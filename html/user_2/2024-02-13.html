<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-13</h1>
<h3>Title: SocraSynth: Multi-LLM Reasoning with Conditional Statistics</h3>
<ul>
<li><strong>Authors: </strong>Edward Y. Chang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06634">https://arxiv.org/abs/2402.06634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06634">https://arxiv.org/pdf/2402.06634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06634]] SocraSynth: Multi-LLM Reasoning with Conditional Statistics(https://arxiv.org/abs/2402.06634)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousness from confrontational to collaborative, gathering final, conciliatory remarks to aid in human reasoning and decision-making. Through case studies in three distinct application domains, this paper showcases SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning, comprehensive assessment, and enhanced collaboration. This underscores the value of multi-agent interactions in leveraging LLMs for advanced knowledge extraction and decision-making support.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）虽然很有前途，但也面临着偏见、幻觉和缺乏推理能力的批评。本文介绍了 SocraSynth，这是一个为缓解这些问题而开发的多 LLM 代理推理平台。 SocraSynth 通过连续论证利用条件统计和系统上下文增强，以及可调整的辩论争议级别。该平台通常涉及一名主持人和两名代表对特定主题的相反观点的法学硕士代理人。 SocraSynth 分为两个主要阶段：知识生成和推理评估。在知识生成阶段，主持人定义辩论主题和争议级别，促使代理人为各自的立场制定支持论据。然后，推理评估阶段采用苏格拉底推理和形式逻辑原则来评估所提出论点的质量。对话结束时，主持人将争议从对抗性调整为合作性，收集最终的和解性言论，以帮助人类推理和决策。通过三个不同应用领域的案例研究，本文展示了 SocraSynth 在促进严谨研究、动态推理、综合评估和加强协作方面的有效性。这强调了多智能体交互在利用法学硕士进行高级知识提取和决策支持方面的价值。</li>
</ul>

<h3>Title: Modeling and Optimization of Epidemiological Control Policies Through  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ishir Rao</a></li>
<li><strong>Subjects: </strong>cs.AI, q-bio.PE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06640">https://arxiv.org/abs/2402.06640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06640">https://arxiv.org/pdf/2402.06640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06640]] Modeling and Optimization of Epidemiological Control Policies Through  Reinforcement Learning(https://arxiv.org/abs/2402.06640)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Pandemics involve the high transmission of a disease that impacts global and local health and economic patterns. The impact of a pandemic can be minimized by enforcing certain restrictions on a community. However, while minimizing infection and death rates, these restrictions can also lead to economic crises. Epidemiological models help propose pandemic control strategies based on non-pharmaceutical interventions such as social distancing, curfews, and lockdowns, reducing the economic impact of these restrictions. However, designing manual control strategies while considering disease spread and economic status is non-trivial. Optimal strategies can be designed through multi-objective reinforcement learning (MORL) models, which demonstrate how restrictions can be used to optimize the outcome of a pandemic. In this research, we utilized an epidemiological Susceptible, Exposed, Infected, Recovered, Deceased (SEIRD) model: a compartmental model for virtually simulating a pandemic day by day. We combined the SEIRD model with a deep double recurrent Q-network to train a reinforcement learning agent to enforce the optimal restriction on the SEIRD simulation based on a reward function. We tested two agents with unique reward functions and pandemic goals to obtain two strategies. The first agent placed long lockdowns to reduce the initial spread of the disease, followed by cyclical and shorter lockdowns to mitigate the resurgence of the disease. The second agent provided similar infection rates but an improved economy by implementing a 10-day lockdown and 20-day no-restriction cycle. This use of reinforcement learning and epidemiological modeling allowed for both economic and infection mitigation in multiple pandemic scenarios.</li>
<li><strong>摘要：</strong>大流行涉及影响全球和地方健康和经济模式的疾病的高传播。通过对社区实施某些限制可以最大限度地减少大流行的影响。然而，这些限制在最大限度降低感染率和死亡率的同时，也可能导致经济危机。流行病学模型有助于提出基于非药物干预措施（例如社交距离、宵禁和封锁）的流行病控制策略，从而减少这些限制的经济影响。然而，在考虑疾病传播和经济状况的同时设计手动控制策略并非易事。可以通过多目标强化学习（MORL）模型来设计最佳策略，该模型演示了如何使用限制来优化大流行的结果。在这项研究中，我们使用了流行病学易感、暴露、感染、康复、死亡 (SEIRD) 模型：一种用于虚拟模拟流行病日常的分区模型。我们将 SEIRD 模型与深度双循环 Q 网络相结合来训练强化学习代理，以基于奖励函数对 SEIRD 模拟实施最佳限制。我们测试了两种具有独特奖励功能和流行病目标的智能体，以获得两种策略。第一个代理实施了长期封锁，以减少疾病的最初传播，然后进行周期性和较短的封锁，以减轻疾病的死灰复燃。第二种药剂提供了类似的感染率，但通过实施 10 天的封锁和 20 天的无限制周期，经济得到了改善。强化学习和流行病学模型的使用可以在多种大流行情况下实现经济和感染缓解。</li>
</ul>

<h3>Title: A Survey on Large Language Model Hallucination via a Creativity  Perspective</h3>
<ul>
<li><strong>Authors: </strong>Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu, Yuanzhuo Wang, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06647">https://arxiv.org/abs/2402.06647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06647">https://arxiv.org/pdf/2402.06647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06647]] A Survey on Large Language Model Hallucination via a Creativity  Perspective(https://arxiv.org/abs/2402.06647)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models (LLMs) are always seen as limitations. However, could they also be a source of creativity? This survey explores this possibility, suggesting that hallucinations may contribute to LLM application by fostering creativity. This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications. Then, through historical examples and recent relevant theories, the survey explores the potential creative benefits of hallucinations in LLMs. To elucidate the value and evaluation criteria of this connection, we delve into the definitions and assessment methods of creativity. Following the framework of divergent and convergent thinking phases, the survey systematically reviews the literature on transforming and harnessing hallucinations for creativity in LLMs. Finally, the survey discusses future research directions, emphasizing the need to further explore and refine the application of hallucinations in creative processes within LLMs.</li>
<li><strong>摘要：</strong>大语言模型（LLM）中的幻觉总是被视为局限性。然而，它们也能成为创造力的源泉吗？这项调查探讨了这种可能性，表明幻觉可能通过培养创造力来促进法学硕士申请。这项调查首先回顾了幻觉的分类及其对关键应用中法学硕士可靠性的负面影响。然后，通过历史实例和最近的相关理论，该调查探讨了法学硕士中幻觉的潜在创造性益处。为了阐明这种联系的价值和评价标准，我们深入探讨了创造力的定义和评估方法。该调查遵循发散思维和聚合思维阶段的框架，系统地回顾了法学硕士转化和利用幻觉以提高创造力的文献。最后，调查讨论了未来的研究方向，强调需要进一步探索和完善法学硕士创造性过程中幻觉的应用。</li>
</ul>

<h3>Title: Conversational Crowdsensing: A Parallel Intelligence Powered Novel  Sensing Approach</h3>
<ul>
<li><strong>Authors: </strong>Zhengqiu Zhu, Yong Zhao, Bin Chen, Sihang Qiu, Kai Xu, Quanjun Yin, Jincai Huang, Zhong Liu, Fei-Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06654">https://arxiv.org/abs/2402.06654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06654">https://arxiv.org/pdf/2402.06654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06654]] Conversational Crowdsensing: A Parallel Intelligence Powered Novel  Sensing Approach(https://arxiv.org/abs/2402.06654)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>The transition from CPS-based Industry 4.0 to CPSS-based Industry 5.0 brings new requirements and opportunities to current sensing approaches, especially in light of recent progress in Chatbots and Large Language Models (LLMs). Therefore, the advancement of parallel intelligence-powered Crowdsensing Intelligence (CSI) is witnessed, which is currently advancing towards linguistic intelligence. In this paper, we propose a novel sensing paradigm, namely conversational crowdsensing, for Industry 5.0. It can alleviate workload and professional requirements of individuals and promote the organization and operation of diverse workforce, thereby facilitating faster response and wider popularization of crowdsensing systems. Specifically, we design the architecture of conversational crowdsensing to effectively organize three types of participants (biological, robotic, and digital) from diverse communities. Through three levels of effective conversation (i.e., inter-human, human-AI, and inter-AI), complex interactions and service functionalities of different workers can be achieved to accomplish various tasks across three sensing phases (i.e., requesting, scheduling, and executing). Moreover, we explore the foundational technologies for realizing conversational crowdsensing, encompassing LLM-based multi-agent systems, scenarios engineering and conversational human-AI cooperation. Finally, we present potential industrial applications of conversational crowdsensing and discuss its implications. We envision that conversations in natural language will become the primary communication channel during crowdsensing process, enabling richer information exchange and cooperative problem-solving among humans, robots, and AI.</li>
<li><strong>摘要：</strong>从基于 CPS 的工业 4.0 到基于 CPSS 的工业 5.0 的过渡为当前的传感方法带来了新的要求和机遇，特别是考虑到聊天机器人和大型语言模型 (LLM) 的最新进展。因此，并行智能驱动的群体感知智能（CSI）的进步是有目共睹的，目前它正在向语言智能迈进。在本文中，我们为工业 5.0 提出了一种新颖的传感范式，即对话式群智感知。它可以减轻个人的工作量和专业要求，促进多元化劳动力的组织和运作，从而促进众感知系统的更快响应和更广泛的普及。具体来说，我们设计了对话式群智感知的架构，以有效地组织来自不同社区的三种类型的参与者（生物、机器人和数字）。通过三个层次的有效对话（即人与人、人与 AI 和 AI 之间），可以实现不同工作人员的复杂交互和服务功能，以完成三个感知阶段（即请求、调度和响应）的各种任务。执行）。此外，我们还探索实现对话式众感知的基础技术，包括基于法学硕士的多智能体系统、场景工程和对话式人机合作。最后，我们提出了对话式群智感知的潜在工业应用并讨论了其影响。我们设想自然语言对话将成为众感知过程中的主要沟通渠道，使人类、机器人和人工智能之间能够进行更丰富的信息交换和协作解决问题。</li>
</ul>

<h3>Title: The Essential Role of Causality in Foundation World Models for Embodied  AI</h3>
<ul>
<li><strong>Authors: </strong>Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer Scetbon, Ade Famoti, Ashley Juan Llorens, Jianfeng Gao, Stefan Bauer, Danica Kragic, Bernhard Schölkopf, Cheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06665">https://arxiv.org/abs/2402.06665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06665">https://arxiv.org/pdf/2402.06665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06665]] The Essential Role of Causality in Foundation World Models for Embodied  AI(https://arxiv.org/abs/2402.06665)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.</li>
<li><strong>摘要：</strong>基础模型的最新进展，特别是大型多模态模型和对话代理的进展，引发了人们对具有普遍能力的实体代理潜力的兴趣。此类代理需要能够在许多不同的现实环境中执行新任务。然而，当前的基础模型无法准确地模拟与现实世界的物理交互，因此不足以用于体现人工智能。因果关系的研究有助于构建真实的世界模型，这对于准确预测可能相互作用的结果至关重要。本文重点讨论了为下一代实体主体构建基础世界模型的前景，并就其中因果关系的重要性提出了新颖的观点。我们认为，整合因果因素对于促进与世界进行有意义的身体互动至关重要。最后，我们揭开了在这种情况下对因果关系的误解，并提出了我们对未来研究的展望。</li>
</ul>

<h3>Title: A Masked language model for multi-source EHR trajectories contextual  representation learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Amirahmadi (1), Mattias Ohlsson (1,2), Kobra Etminani (1), Olle Melander (3), Jonas Björk (4) ((1) Center for Applied Intelligent Systems Research, Halmstad University, (2) Centre for Environmental and Climate Science, Lund University, (3) Department of Clinical Sciences, Lund University, (4) Division of Occupational and Environmental Medicine, Lund University)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06675">https://arxiv.org/abs/2402.06675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06675">https://arxiv.org/pdf/2402.06675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06675]] A Masked language model for multi-source EHR trajectories contextual  representation learning(https://arxiv.org/abs/2402.06675)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Using electronic health records data and machine learning to guide future decisions needs to address challenges, including 1) long/short-term dependencies and 2) interactions between diseases and interventions. Bidirectional transformers have effectively addressed the first challenge. Here we tackled the latter challenge by masking one source (e.g., ICD10 codes) and training the transformer to predict it using other sources (e.g., ATC codes).</li>
<li><strong>摘要：</strong>使用电子健康记录数据和机器学习来指导未来的决策需要应对挑战，包括 1) 长期/短期依赖性和 2) 疾病与干预措施之间的相互作用。双向变压器有效地解决了第一个挑战。在这里，我们通过屏蔽一个源（例如 ICD10 代码）并训练变压器使用其他源（例如 ATC 代码）来预测它来解决后一个挑战。</li>
</ul>

<h3>Title: Scaling Intelligent Agents in Combat Simulations for Wargaming</h3>
<ul>
<li><strong>Authors: </strong>Scotty Black, Christian Darken</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06694">https://arxiv.org/abs/2402.06694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06694">https://arxiv.org/pdf/2402.06694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06694]] Scaling Intelligent Agents in Combat Simulations for Wargaming(https://arxiv.org/abs/2402.06694)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Remaining competitive in future conflicts with technologically-advanced competitors requires us to accelerate our research and development in artificial intelligence (AI) for wargaming. More importantly, leveraging machine learning for intelligent combat behavior development will be key to one day achieving superhuman performance in this domain--elevating the quality and accelerating the speed of our decisions in future wars. Although deep reinforcement learning (RL) continues to show promising results in intelligent agent behavior development in games, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and simulation. Capitalizing on the proven potential of RL and recent successes of hierarchical reinforcement learning (HRL), our research is investigating and extending the use of HRL to create intelligent agents capable of performing effectively in these large and complex simulation environments. Our ultimate goal is to develop an agent capable of superhuman performance that could then serve as an AI advisor to military planners and decision-makers. This papers covers our ongoing approach and the first three of our five research areas aimed at managing the exponential growth of computations that have thus far limited the use of AI in combat simulations: (1) developing an HRL training framework and agent architecture for combat units; (2) developing a multi-model framework for agent decision-making; (3) developing dimension-invariant observation abstractions of the state space to manage the exponential growth of computations; (4) developing an intrinsic rewards engine to enable long-term planning; and (5) implementing this framework into a higher-fidelity combat simulation.</li>
<li><strong>摘要：</strong>为了在未来与技术先进的竞争对手的冲突中保持竞争力，我们需要加快兵棋推演人工智能（AI）的研发。更重要的是，利用机器学习进行智能战斗行为开发将是有一天在这一领域实现超人表现的关键——提高我们在未来战争中决策的质量并加快决策速度。尽管深度强化学习 (RL) 在游戏中的智能代理行为开发方面继续显示出有希望的结果，但在战斗建模和模拟中常见的长期复杂任务中，它的表现尚未达到或超过人类水平。利用 RL 已被证实的潜力和分层强化学习 (HRL) 的最新成功，我们的研究正在调查和扩展 HRL 的使用，以创建能够在这些大型复杂模拟环境中有效执行的智能代理。我们的最终目标是开发一种具有超人性能的智能体，然后可以充当军事规划者和决策者的人工智能顾问。本文涵盖了我们正在进行的方法以及我们五个研究领域中的前三个，旨在管理迄今为止限制人工智能在战斗模拟中使用的指数增长的计算量：(1) 为作战单位开发 HRL 训练框架和代理架构; (2) 开发智能体决策的多模型框架； (3) 开发状态空间的维度不变观察抽象来管理计算的指数增长； (4) 开发内在奖励引擎以实现长期规划； (5)将该框架应用到更高保真度的战斗模拟中。</li>
</ul>

<h3>Title: Integrating LLMs for Explainable Fault Diagnosis in Complex Systems</h3>
<ul>
<li><strong>Authors: </strong>Akshay J. Dave, Tat Nghia Nguyen, Richard B. Vilim</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06695">https://arxiv.org/abs/2402.06695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06695">https://arxiv.org/pdf/2402.06695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06695]] Integrating LLMs for Explainable Fault Diagnosis in Complex Systems(https://arxiv.org/abs/2402.06695)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces an integrated system designed to enhance the explainability of fault diagnostics in complex systems, such as nuclear power plants, where operator understanding is critical for informed decision-making. By combining a physics-based diagnostic tool with a Large Language Model, we offer a novel solution that not only identifies faults but also provides clear, understandable explanations of their causes and implications. The system's efficacy is demonstrated through application to a molten salt facility, showcasing its ability to elucidate the connections between diagnosed faults and sensor data, answer operator queries, and evaluate historical sensor anomalies. Our approach underscores the importance of merging model-based diagnostics with advanced AI to improve the reliability and transparency of autonomous systems.</li>
<li><strong>摘要：</strong>本文介绍了一种集成系统，旨在增强复杂系统（例如核电站）故障诊断的可解释性，其中操作员的理解对于做出明智的决策至关重要。通过将基于物理的诊断工具与大型语言模型相结合，我们提供了一种新颖的解决方案，不仅可以识别故障，还可以对其原因和含义提供清晰、易于理解的解释。该系统的功效通过熔盐设施的应用得到证明，展示了其阐明诊断故障和传感器数据之间的联系、回答操作员查询以及评估历史传感器异常的能力。我们的方法强调了将基于模型的诊断与先进的人工智能相结合以提高自主系统的可靠性和透明度的重要性。</li>
</ul>

<h3>Title: FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruiyang Qin, Yuting Hu, Zheyu Yan, Jinjun Xiong, Ahmed Abbasi, Yiyu Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06696">https://arxiv.org/abs/2402.06696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06696">https://arxiv.org/pdf/2402.06696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06696]] FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via  Large Language Models(https://arxiv.org/abs/2402.06696)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Neural Architecture Search (NAS) has become the de fecto tools in the industry in automating the design of deep neural networks for various applications, especially those driven by mobile and edge devices with limited computing resources. The emerging large language models (LLMs), due to their prowess, have also been incorporated into NAS recently and show some promising results. This paper conducts further exploration in this direction by considering three important design metrics simultaneously, i.e., model accuracy, fairness, and hardware deployment efficiency. We propose a novel LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN models by orders-of-magnitude across almost all design considerations.</li>
<li><strong>摘要：</strong>神经架构搜索 (NAS) 已成为业界针对各种应用自动设计深度神经网络的缺陷工具，特别是那些由计算资源有限的移动和边缘设备驱动的应用。新兴的大语言模型（LLM）由于其强大的能力，最近也被纳入 NAS 并显示出一些有希望的结果。本文通过同时考虑三个重要的设计指标，即模型准确性、公平性和硬件部署效率，在这个方向上进行了进一步的探索。我们在本文中提出了一种新颖的基于 LLM 的 NAS 框架 FL-NAS，并通过实验证明 FL-NAS 确实可以找到高性能的 DNN，在多个方面击败了最先进的 DNN 模型。几乎所有的设计考虑。</li>
</ul>

<h3>Title: Entropy-Regularized Token-Level Policy Optimization for Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Muning Wen, Cheng Deng, Jun Wang, Weinan Zhang, Ying Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06700">https://arxiv.org/abs/2402.06700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06700">https://arxiv.org/pdf/2402.06700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06700]] Entropy-Regularized Token-Level Policy Optimization for Large Language  Models(https://arxiv.org/abs/2402.06700)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed to harmonize the RL process with the principles of language modeling. This methodology decomposes the Q-function update from a coarse action-level view to a more granular token-level perspective, backed by theoretical proof of optimization consistency. Crucially, this decomposition renders linear time complexity in action exploration. We assess the effectiveness of ETPO within a simulated environment that models data science code generation as a series of multi-step interactive tasks; results show that ETPO achieves effective performance improvement on the CodeLlama-7B model and surpasses a variant PPO baseline inherited from RLHF. This underlines ETPO's potential as a robust method for refining the interactive decision-making capabilities of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已显示出作为交互式决策任务中的智能代理的前景。传统方法通常依赖于精心设计的提示、高质量示例或用于上下文学习、监督微调或 RLHF 的额外奖励模型。强化学习 (RL) 为法学硕士提供了一种动态替代方案，通过直接参与特定任务的环境来克服这些依赖性。尽管如此，它仍面临重大障碍：1）需要探索的指数级巨大的行动空间导致不稳定； 2）基于行动级奖励信号分配代币级信用的挑战，导致最大化奖励和准确建模语料库数据之间的不一致。为了应对这些挑战，我们引入了熵正则化令牌级策略优化（ETPO），这是一种专为在令牌级优化 LLM 而定制的熵增强 RL 方法。 ETPO 的核心是我们新颖的每代币软 Bellman 更新，旨在协调 RL 过程与语言建模原理。该方法将 Q 函数更新从粗略的操作级别视图分解为更细粒度的令牌级别视图，并得到优化一致性理论证明的支持。至关重要的是，这种分解在动作探索中呈现线性时间复杂度。我们在模拟环境中评估 ETPO 的有效性，该环境将数据科学代码生成建模为一系列多步骤交互任务；结果表明，ETPO 在 CodeLlama-7B 模型上实现了有效的性能改进，并超越了从 RLHF 继承的变体 PPO 基线。这凸显了 ETPO 作为完善法学硕士交互式决策能力的强大方法的潜力。</li>
</ul>

<h3>Title: NICE: To Optimize In-Context Examples or Not?</h3>
<ul>
<li><strong>Authors: </strong>Pragya Srivastava, Satvik Golechha, Amit Deshpande, Amit Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06733">https://arxiv.org/abs/2402.06733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06733">https://arxiv.org/pdf/2402.06733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06733]] NICE: To Optimize In-Context Examples or Not?(https://arxiv.org/abs/2402.06733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added details, we validate our hypothesis empirically by computing \metric with query-dependent bins of examples, comparing different instructions with ICE selection methods, and performing label perturbation experiments. We conclude that tasks can be divided into two broad classes based on the \metric metric, where the returns on ICE optimization follow predictable trends when instructions are provided in the prompt.</li>
<li><strong>摘要：</strong>最近的研究表明，通过上下文学习和上下文示例优化 (ICE)，大型语言模型 (LLM) 在各种任务上表现出色。然而，大多数这些研究都假设提示中提供了固定的说明或没有提供说明，这导致了一个明显的共识：上下文示例的优化对于更好的性能至关重要。我们通过调查在提供特定于任务的指令时优化上下文示例的必要性来挑战这种针对指令调整的 LLM 的共识，并发现对于某些任务，优化上下文示例的各种方法会产生收益递减的结果。我们引入了一个名为 \metriclong{} (\metric) 的特定于任务的指标，它可以量化给定指令中任务的可学习性，并提供启发式方法来帮助决定是否针对任何新任务优化指令或 ICE。在广泛的任务和逐渐添加细节的系统创建的指令集上，我们通过使用依赖于查询的示例箱计算 \metric、将不同指令与 ICE 选择方法进行比较以及执行标签扰动实验来实证验证我们的假设。我们得出的结论是，任务可以根据 \metric 指标分为两大类，其中当提示中提供指令时，ICE 优化的回报遵循可预测的趋势。</li>
</ul>

<h3>Title: EntGPT: Linking Generative Large Language Models with Knowledge Bases</h3>
<ul>
<li><strong>Authors: </strong>Yifan Ding, Amrit Poudel, Qingkai Zeng, Tim Weninger, Balaji Veeramani, Sanmitra Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06738">https://arxiv.org/abs/2402.06738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06738">https://arxiv.org/pdf/2402.06738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06738]] EntGPT: Linking Generative Large Language Models with Knowledge Bases(https://arxiv.org/abs/2402.06738)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>The ability of Large Language Models (LLMs) to generate factually correct output remains relatively unexplored due to the lack of fact-checking and knowledge grounding during training and inference. In this work, we aim to address this challenge through the Entity Disambiguation (ED) task. We first consider prompt engineering, and design a three-step hard-prompting method to probe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the prompting method improves the micro-F_1 score of the original vanilla models by a large margin, on some cases up to 36% and higher, and obtains comparable performance across 10 datasets when compared to existing methods with SFT. We further improve the knowledge grounding ability through instruction tuning (IT) with similar prompts and responses. The instruction-tuned model not only achieves higher micro-F1 score performance as compared to several baseline methods on supervised entity disambiguation tasks with an average micro-F_1 improvement of 2.1% over the existing baseline models, but also obtains higher accuracy on six Question Answering (QA) tasks in the zero-shot setting. Our methodologies apply to both open- and closed-source LLMs.</li>
<li><strong>摘要：</strong>由于训练和推理过程中缺乏事实检查和知识基础，大型语言模型 (LLM) 生成事实上正确的输出的能力仍然相对未被探索。在这项工作中，我们的目标是通过实体消歧（ED）任务来应对这一挑战。我们首先考虑提示工程，并设计了一种三步硬提示方法来探测法学硕士的 ED 性能，而无需监督微调（SFT）。总体而言，该提示方法大幅提高了原始普通模型的 micro-F_1 分数，在某些情况下高达 36% 甚至更高，并且与现有的 SFT 方法相比，在 10 个数据集上获得了可比的性能。我们通过类似提示和响应的指令调整（IT）进一步提高知识基础能力。与监督实体消歧任务上的几种基线方法相比，指令调整模型不仅实现了更高的 micro-F1 分数性能，平均 micro-F_1 比现有基线模型提高了 2.1%，而且在六个问答上获得了更高的准确性（QA）零样本设置中的任务。我们的方法适用于开源和闭源法学硕士。</li>
</ul>

<h3>Title: GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</h3>
<ul>
<li><strong>Authors: </strong>Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, Sutanay Choudhury</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06764">https://arxiv.org/abs/2402.06764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06764">https://arxiv.org/pdf/2402.06764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06764]] GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding(https://arxiv.org/abs/2402.06764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no--such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the models' capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.</li>
<li><strong>摘要：</strong>将大型语言模型 (LLM) 与从特定领域数据派生的知识图相集成，代表着朝着更强大、更真实的推理迈出的重要一步。随着这些模型的能力变得越来越强大，至关重要的是使它们能够对现实世界的知识图进行多步推理，同时最大限度地减少幻觉。虽然大型语言模型擅长对话和文本生成，但它们对互连实体的领域专用图进行推理的能力仍然有限。例如，我们是否可以根据私有数据库中的关系和属性查询法学硕士，以确定专业网络中针对特定目标的最佳联系人？答案是否定的——这种能力超出了现有方法的范围。然而，这个问题强调了必须解决的关键技术差距。科学、安全和电子商务等领域的许多高价值应用都依赖于编码独特结构、关系和逻辑约束的专有知识图。我们引入了一个用于开发图对齐语言模型（GLaM）的微调框架，该框架将知识图转换为带有标记问答对的替代文本表示。我们证明，将模型建立在特定的基于图的知识基础上可以扩展模型基于结构的推理的能力。我们的方法利用大语言模型的生成能力来创建数据集，并提出了检索增强生成风格方法的有效替代方法。</li>
</ul>

<h3>Title: Debating with More Persuasive LLMs Leads to More Truthful Answers</h3>
<ul>
<li><strong>Authors: </strong>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, Ethan Perez</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06782">https://arxiv.org/abs/2402.06782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06782">https://arxiv.org/pdf/2402.06782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06782]] Debating with More Persuasive LLMs Leads to More Truthful Answers(https://arxiv.org/abs/2402.06782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.</li>
<li><strong>摘要：</strong>将大型语言模型 (LLM) 与所需行为对齐的常用方法在很大程度上依赖于人工标记的数据。然而，随着模型变得越来越复杂，它们将超越人类的专业知识，人类评估的角色将演变为非专家监督专家。预见到这一点，我们会问：较弱的模型能否评估较强模型的正确性？我们在类似的环境中研究这个问题，其中较强的模型（专家）拥有回答问题所需的信息，而较弱的模型（非专家）缺乏这些信息。我们评估的方法是 \textit{debate}，其中两名 LLM 专家各自争论不同的答案，然后由一名非专家选择答案。我们发现辩论始终有助于非专家模型和人类回答问题，分别达到 76% 和 88% 的准确率（朴素基线获得 48% 和 60%）。此外，以无人监督的方式优化专家辩手的说服力可以提高非专家在辩论中识别真相的能力。我们的结果提供了令人鼓舞的经验证据，证明在缺乏基本事实的情况下将模型与辩论结合起来的可行性。</li>
</ul>

<h3>Title: Estimating Player Performance in Different Contexts Using Fine-tuned  Large Events Models</h3>
<ul>
<li><strong>Authors: </strong>Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06815">https://arxiv.org/abs/2402.06815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06815">https://arxiv.org/pdf/2402.06815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06815]] Estimating Player Performance in Different Contexts Using Fine-tuned  Large Events Models(https://arxiv.org/abs/2402.06815)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative application of Large Event Models (LEMs), akin to Large Language Models, to the domain of soccer analytics. By learning the "language" of soccer - predicting variables for subsequent events rather than words LEMs facilitate the simulation of matches and offer various applications, including player performance prediction across different team contexts. We focus on fine-tuning LEMs with the WyScout dataset for the 2017-2018 Premier League season to derive specific insights into player contributions and team strategies. Our methodology involves adapting these models to reflect the nuanced dynamics of soccer, enabling the evaluation of hypothetical transfers. Our findings confirm the effectiveness and limitations of LEMs in soccer analytics, highlighting the model's capability to forecast teams' expected standings and explore high-profile scenarios, such as the potential effects of transferring Cristiano Ronaldo or Lionel Messi to different teams in the Premier League. This analysis underscores the importance of context in evaluating player quality. While general metrics may suggest significant differences between players, contextual analyses reveal narrower gaps in performance within specific team frameworks.</li>
<li><strong>摘要：</strong>本文介绍了大型事件模型 (LEM)（类似于大型语言模型）在足球分析领域的创新应用。通过学习足球的“语言”——预测后续事件的变量而不是单词，LEM 有助于模拟比赛并提供各种应用，包括跨不同团队环境的球员表现预测。我们专注于使用 2017-2018 英超赛季的 WyScout 数据集对 LEM 进行微调，以获得有关球员贡献和球队策略的具体见解。我们的方法包括调整这些模型以反映足球的微妙动态，从而能够评估假设的转会。我们的研究结果证实了 LEM 在足球分析中的有效性和局限性，强调了该模型预测球队预期排名和探索引人注目的场景的能力，例如将 Cristiano Ronaldo 或 Lionel Messi 转移到英超联赛中不同球队的潜在影响。该分析强调了背景在评估球员素质中的重要性。虽然一般指标可能表明球员之间存在显着差异，但背景分析揭示了特定团队框架内绩效差距的缩小。</li>
</ul>

<h3>Title: Monitored Markov Decision Processes</h3>
<ul>
<li><strong>Authors: </strong>Simone Parisi, Montaser Mohammedalamen, Alireza Kazemipour, Matthew E. Taylor, Michael Bowling</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06819">https://arxiv.org/abs/2402.06819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06819">https://arxiv.org/pdf/2402.06819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06819]] Monitored Markov Decision Processes(https://arxiv.org/abs/2402.06819)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent's actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a powerful new formalism that encompasses both new and existing problems and lays the foundation for future research.</li>
<li><strong>摘要：</strong>在强化学习 (RL) 中，智能体通过与环境交互并接收其行为的反馈（数字奖励）来学习执行任务。然而，奖励总是可观察到的假设通常不适用于现实世界的问题。例如，代理可能需要要求人类监督其行为或激活监控系统以接收反馈。甚至可能需要一段时间才能观察到奖励，或者一段时间后不再给予奖励。换句话说，在某些情况下，环境会根据智能体的行为产生奖励，但智能体无法观察到它们。在本文中，我们形式化了一个新颖但通用的 RL 框架 - 受监控的 MDP - 其中代理无法始终观察到奖励。我们讨论了这种设置的理论和实际后果，展示了即使在玩具环境中也提出的挑战，并提出了开始解决这种新颖设置的算法。本文介绍了一种强大的新形式主义，涵盖了新问题和现有问题，并为未来的研究奠定了基础。</li>
</ul>

<h3>Title: Forecasting Events in Soccer Matches Through Language</h3>
<ul>
<li><strong>Authors: </strong>Tiago Mendes-Neves, Luís Meireles, João Mendes-Moreira</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06820">https://arxiv.org/abs/2402.06820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06820">https://arxiv.org/pdf/2402.06820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06820]] Forecasting Events in Soccer Matches Through Language(https://arxiv.org/abs/2402.06820)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including betting and match analytics. Moreover, we show that LEMs provide a simulation backbone on which many analytics pipelines can be built, an approach opposite to the current specialized single-purpose models. LEMs represent a pivotal advancement in soccer analytics, establishing a foundational framework for multifaceted analytics pipelines through a singular machine-learning model.</li>
<li><strong>摘要：</strong>本文介绍了一种预测足球比赛中下一场比赛的方法，这一挑战与大型语言模型 (LLM) 面临的问题非常相似。与严重限制足球赛事动态的其他方法（通常从许多变量中抽象或依赖于顺序模型的混合）不同，我们的研究提出了一种受法学硕士中使用的方法启发的新技术。这些模型可以预测构成赛事的完整变量链，从而显着简化足球大型赛事模型 (LEM) 的构建。利用公开可用的 WyScout 数据集上的深度学习，所提出的方法在关键领域（例如下一个事件类型的预测准确性）显着超越了之前的 LEM 提案的性能。本文重点介绍了 LEM 在各种应用中的实用性，包括投注和比赛分析。此外，我们表明 LEM 提供了一个模拟主干，可以在其上构建许多分析管道，这是一种与当前专用的单一用途模型相反的方法。 LEM 代表了足球分析的关键进步，通过单一的机器学习模型为多方面的分析管道建立了基础框架。</li>
</ul>

<h3>Title: ChemLLM: A Chemical Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su, Hansen Zhong, Yuqiang Li, Wanli Ouyang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06852">https://arxiv.org/abs/2402.06852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06852">https://arxiv.org/pdf/2402.06852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06852]] ChemLLM: A Chemical Large Language Model(https://arxiv.org/abs/2402.06852)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made impressive progress in chemistry applications, including molecular property prediction, molecular generation, experimental protocol design, etc. However, the community lacks a dialogue-based model specifically designed for chemistry. The challenge arises from the fact that most chemical data and scientific knowledge are primarily stored in structured databases, and the direct use of these structured data compromises the model's ability to maintain coherent dialogue. To tackle this issue, we develop a novel template-based instruction construction method that transforms structured knowledge into plain dialogue, making it suitable for language model training. By leveraging this approach, we develop ChemLLM, the first large language model dedicated to chemistry, capable of performing various tasks across chemical disciplines with smooth dialogue interaction. ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name conversion, molecular caption, and reaction prediction, and surpasses GPT-4 on two of them. Remarkably, ChemLLM also shows exceptional adaptability to related mathematical and physical tasks despite being trained mainly on chemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in specialized NLP tasks within chemistry, such as literature translation and cheminformatic programming. ChemLLM opens up a new avenue for exploration within chemical studies, while our method of integrating structured chemical knowledge into dialogue systems sets a new frontier for developing LLMs across various scientific fields. Codes, Datasets, and Model weights are publicly accessible at hf.co/AI4Chem/ChemLLM-7B-Chat.</li>
<li><strong>摘要：</strong>大语言模型（LLM）在化学应用中取得了令人瞩目的进展，包括分子性质预测、分子生成、实验方案设计等。然而，社区缺乏专门为化学设计的基于对话的模型。挑战源于这样一个事实：大多数化学数据和科学知识主要存储在结构化数据库中，而直接使用这些结构化数据会损害模型保持连贯对话的能力。为了解决这个问题，我们开发了一种新颖的基于模板的指令构建方法，将结构化知识转化为简单的对话，使其适合语言模型训练。通过利用这种方法，我们开发了 ChemLLM，这是第一个专用于化学的大型语言模型，能够通过流畅的对话交互执行跨化学学科的各种任务。 ChemLLM 在化学中的所有三个主要任务（即名称转换、分子标题和反应预测）上都击败了 GPT-3.5，并在其中两项上超过了 GPT-4。值得注意的是，尽管主要是在以化学为中心的语料库上进行训练，但 ChemLLM 还表现出了对相关数学和物理任务的卓越适应性。此外，ChemLLM 还表现出对化学领域专业 NLP 任务的熟练程度，例如文献翻译和化学信息学编程。 ChemLLM 为化学研究领域的探索开辟了一条新途径，而我们将结构化化学知识整合到对话系统中的方法为跨不同科学领域开发法学硕士开辟了新领域。代码、数据集和模型权重可在 hf.co/AI4Chem/ChemLLM-7B-Chat 上公开访问。</li>
</ul>

<h3>Title: History, Development, and Principles of Large Language Models-An  Introductory Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Chengming Li, Xiping Hu, Ruifeng Xu, Min Yang, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06853">https://arxiv.org/abs/2402.06853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06853">https://arxiv.org/pdf/2402.06853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06853]] History, Development, and Principles of Large Language Models-An  Introductory Survey(https://arxiv.org/abs/2402.06853)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLMs reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.</li>
<li><strong>摘要：</strong>语言模型是自然语言处理（NLP）的基石，利用数学方法概括语言规律和知识以进行预测和生成。经过数十年的广泛研究，语言建模已经从最初的统计语言模型 (SLM) 发展到当代的大型语言模型 (LLM)。值得注意的是，法学硕士的迅速发展已经达到了处理、理解和生成人类水平文本的能力。然而，尽管法学硕士在改善工作和个人生活方面具有显着优势，但全科医生对这些模式的背景和原则的了解有限，阻碍了其充分发挥潜力。值得注意的是，大多数法学硕士的评审都集中在特定方面并使用专门的语言，这对缺乏相关背景知识的从业者提出了挑战。有鉴于此，本调查旨在提供法学硕士的全面概述，以帮助更广泛的受众。它致力于通过探索语言模型的历史背景并追踪其随时间的演变来促进全面理解。该调查进一步调查了影响法学硕士发展的因素，强调了关键贡献。此外，它还专注于阐明法学硕士的基本原理，为观众提供必要的理论知识。该调查还强调了现有工作的局限性，并指出了有希望的未来方向。</li>
</ul>

<h3>Title: UrbanKGent: A Unified Large Language Model Agent Framework for Urban  Knowledge Graph Construction</h3>
<ul>
<li><strong>Authors: </strong>Yansong Ning, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06861">https://arxiv.org/abs/2402.06861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06861">https://arxiv.org/pdf/2402.06861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06861]] UrbanKGent: A Unified Large Language Model Agent Framework for Urban  Knowledge Graph Construction(https://arxiv.org/abs/2402.06861)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent-13B not only can significantly outperform 21 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10\% with approximately 20 times lower cost. We deploy UrbanKGent-13B to provide online services, which can construct an UrbanKG with thousands of times richer relationships using only one-fifth of the data compared with the existing benchmark. Our data, code, and opensource UrbanKGC agent are available at https://github.com/usail-hkust/UrbanKGent.</li>
<li><strong>摘要：</strong>城市知识图谱最近已成为一种新兴的构建模块，可以从多源城市数据中提取关键知识，以适应不同的城市应用场景。尽管其好处颇多，但城市知识图谱构建（UrbanKGC）仍然严重依赖人工，阻碍了其潜在的进步。本文提出了 UrbanKGent，一个用于城市知识图构建的统一大语言模型代理框架。具体来说，我们首先通过异构感知和地理空间注入的​​指令生成来构建 UrbanKGC 任务的知识指令集（例如关系三元组提取和知识图补全）。此外，我们提出了一个工具增强的迭代轨迹细化模块来增强和细化从 GPT-4 中提取的轨迹。通过在 Llama-2-13B 上使用增强轨迹进行混合指令微调，我们获得了 UrbanKGC 代理 UrbanKGent-13B。我们使用人类自我评估和 GPT-4 自我评估对两个现实世界数据集进行综合评估。实验结果表明，UrbanKGent-13B 不仅可以在 UrbanKGC 任务中显着优于 21 个基线，而且还超过最先进的 LLM GPT-4 超过 10%，而成本降低了约 20 倍。我们部署UrbanKGent-13B来提供在线服务，与现有基准相比，仅使用五分之一的数据就可以构建关系丰富数千倍的UrbanKG。我们的数据、代码和开源 UrbanKGC 代理可在 https://github.com/usail-hkust/UrbanKGent 上获取。</li>
</ul>

<h3>Title: GenTranslate: Large Language Models are Generative Multilingual Speech  and Machine Translators</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, Eng Siong Chng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06894">https://arxiv.org/abs/2402.06894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06894">https://arxiv.org/pdf/2402.06894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06894]] GenTranslate: Large Language Models are Generative Multilingual Speech  and Machine Translators(https://arxiv.org/abs/2402.06894)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的最新进展通过减少表示错误并结合外部知识，推动了多语言语音和机器翻译的发展。然而，这两个翻译任务通常都利用波束搜索解码和 top-1 假设选择来进行推理。这些技术难以充分利用各种 N 最佳假设中的丰富信息，这使得它们对于需要单一高质量输出序列的翻译任务来说不太理想。在本文中，我们提出了一种新的翻译任务生成范式，即“GenTranslate”，它建立在法学硕士的基础上，从 N 最佳列表中的不同翻译版本中生成更好的结果。利用法学硕士丰富的语言知识和强大的推理能力，我们的新范式可以整合N个最佳候选者中的丰富信息，从而生成更高质量的翻译结果。此外，为了支持 LLM 微调，我们构建并发布了一个 HypoTranslate 数据集，其中包含 11 种语言的超过 592K 假设-翻译对。对各种语音和机器翻译基准（例如 FLEURS、CoVoST-2、WMT）的实验表明，我们的 GenTranslate 显着优于最先进的模型。</li>
</ul>

<h3>Title: Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework  and Semantic-Based Metric</h3>
<ul>
<li><strong>Authors: </strong>Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06900">https://arxiv.org/abs/2402.06900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06900">https://arxiv.org/pdf/2402.06900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06900]] Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework  and Semantic-Based Metric(https://arxiv.org/abs/2402.06900)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an influence on downstream metrics.</li>
<li><strong>摘要：</strong>为了开发符合社会标准的大型语言模型 (LLM)，必须辨别生成的文本中是否存在毒性。大多数现有的毒性指标依赖于在特定毒性数据集上训练的编码器模型。然而，这些编码器容易受到分布外（OOD）问题的影响，并且取决于数据集中假设的毒性定义。在本文中，我们引入了一种基于 LLM 的自动鲁棒度量来区分模型响应是否有毒。我们首先分析毒性因素，然后检查法学硕士的内在毒性属性，以确定他们作为评估者的适合性。随后，我们在评估数据集上评估了我们的指标 LLM 作为毒性评估者 (LATTE)。实证结果表明，在测量毒性方面表现出色，在没有训练程序的情况下，F1 分数比最先进的指标提高了 12 分。我们还表明上游毒性对下游指标有影响。</li>
</ul>

<h3>Title: Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach  to Searching for the Most Promising Intermediate Thought</h3>
<ul>
<li><strong>Authors: </strong>Zhen-Yu Zhang, Siwei Han, Huaxiu Yao, Gang Niu, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06918">https://arxiv.org/abs/2402.06918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06918">https://arxiv.org/pdf/2402.06918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06918]] Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach  to Searching for the Most Promising Intermediate Thought(https://arxiv.org/abs/2402.06918)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair, allowing us to identify the most promising thoughts through an iterative process. To further model the noise in the comparison, we resort to the techniques of ensemble and dueling bandits and propose two variants of the proposed algorithm. Experiments on three real-world mathematical and reasoning tasks demonstrate the effectiveness of our proposed algorithm and verify the rationale of the direct pairwise comparison.</li>
<li><strong>摘要：</strong>为了提高大语言模型（LLM）处理复杂推理问题的能力，提出了思想链（CoT）方法来引导LLM逐步推理，促进从简单到复杂的问题解决。生成这种链的最先进方法涉及交互式协作，其中学习者生成候选中间思想，由法学硕士评估，指导后续思想的生成。然而，一个普遍但尚未得到充分研究的问题是，法学硕士的评估通常是嘈杂且不可靠的，可能会误导选择有前途的中间思想的生成过程。在本文中，受 Vapnik 原理的启发，我们提出了一种新颖的基于比较的 CoT 生成算法，该算法可以利用法学硕士的噪声反馈直接识别最有前途的想法。在每一轮中，我们随机配对中间想法，并直接提示法学硕士从每对中选择更有前途的想法，使我们能够通过迭代过程识别最有前途的想法。为了进一步对比较中的噪声进行建模，我们采用了集成和决斗老虎机技术，并提出了所提出算法的两种变体。对三个现实世界数学和推理任务的实验证明了我们提出的算法的有效性并验证了直接成对比较的基本原理。</li>
</ul>

<h3>Title: A Thorough Examination of Decoding Methods in the Era of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06925">https://arxiv.org/abs/2402.06925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06925">https://arxiv.org/pdf/2402.06925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06925]] A Thorough Examination of Decoding Methods in the Era of LLMs(https://arxiv.org/abs/2402.06925)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.</li>
<li><strong>摘要：</strong>解码方法在将语言模型从下一个标记预测器转换为实际任务解决器的过程中发挥着不可或缺的作用。先前对解码方法的研究主要集中在特定于任务的模型，可能无法扩展到当前的通用大语言模型（LLM）时代。此外，最近解码策略的涌入使这一情况进一步复杂化。本文对法学硕士背景下的各种解码方法进行了全面、多方面的分析，评估了它们的性能、对超参数变化的鲁棒性以及跨各种任务、模型和部署环境的解码速度。我们的研究结果表明，解码方法的性能明显依赖于任务，并受到对齐、模型大小和量化等因素的影响。有趣的是，敏感性分析表明，某些方法以大量超参数调整为代价实现了卓越的性能，强调了在不同环境下获得最佳结果和实现实用性之间的权衡。</li>
</ul>

<h3>Title: Making a prototype of Seoul historical sites chatbot using Langchain</h3>
<ul>
<li><strong>Authors: </strong>Jae Young Suh, Minsoo Kwak, Soo Yong Kim, Hyoungseo Cho</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06929">https://arxiv.org/abs/2402.06929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06929">https://arxiv.org/pdf/2402.06929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06929]] Making a prototype of Seoul historical sites chatbot using Langchain(https://arxiv.org/abs/2402.06929)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>In this paper, we are going to share a draft of the development of a conversational agent created to disseminate information about historical sites located in the Seoul. The primary objective of the agent is to increase awareness among visitors who are not familiar with Seoul, about the presence and precise locations of valuable cultural heritage sites. It aims to promote a basic understanding of Korea's rich and diverse cultural history. The agent is thoughtfully designed for accessibility in English and utilizes data generously provided by the Seoul Metropolitan Government. Despite the limited data volume, it consistently delivers reliable and accurate responses, seamlessly aligning with the available information. We have meticulously detailed the methodologies employed in creating this agent and provided a comprehensive overview of its underlying structure within the paper. Additionally, we delve into potential improvements to enhance this initial version of the system, with a primary emphasis on expanding the available data through our prompting. In conclusion, we provide an in-depth discussion of our expectations regarding the future impact of this agent in promoting and facilitating the sharing of historical sites.</li>
<li><strong>摘要：</strong>在本文中，我们将分享一个对话代理的开发草案，该代理的创建目的是传播有关首尔历史遗址的信息。该代理商的主要目标是提高不熟悉首尔的游客对宝贵文化遗产的存在和准确位置的认识。它旨在促进对韩国丰富多样的文化历史的基本了解。该代理经过精心设计，可使用英语访问，并利用首尔市政府慷慨提供的数据。尽管数据量有限，但它始终提供可靠且准确的响应，与可用信息无缝匹配。我们在论文中详细介绍了创建该代理所采用的方法，并对其底层结构进行了全面概述。此外，我们还深入研究了增强系统初始版本的潜在改进，主要重点是通过我们的提示扩展可用数据。最后，我们深入讨论了我们对该代理在促进和促进历史遗址共享方面的未来影响的期望。</li>
</ul>

<h3>Title: LiFi: Lightweight Controlled Text Generation with Fine-Grained Control  Codes</h3>
<ul>
<li><strong>Authors: </strong>Chufan Shi, Deng Cai, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06930">https://arxiv.org/abs/2402.06930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06930">https://arxiv.org/pdf/2402.06930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06930]] LiFi: Lightweight Controlled Text Generation with Fine-Grained Control  Codes(https://arxiv.org/abs/2402.06930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving field of text generation, the demand for more precise control mechanisms has become increasingly apparent. To address this need, we present a novel methodology, LIFI, which offers a lightweight approach with fine-grained control for controlled text generation. Unlike previous studies that train pre-trained language models to follow discrete, categorical, and exclusive control codes, LIFI learns controlled text generation under the guidance of continuous, relative, and nonexclusive control codes. These fine-grained codes are automatically derived from an attribute classifier, initially trained with a small amount of labeled data and subsequently employed to label abundant unlabeled data, thus garnering more extensive supervision signals. Moreover, to achieve efficient control, we incorporate the fine-grained control codes with adapters, a parameter- and compute-efficient way to steer a pre-trained language model. We evaluate LIFI on two conventional tasks -- sentiment control and topic control -- and one newly proposed task -- stylistic novel writing. Comprehensive experimental results validate the effectiveness of our proposed methods, demonstrating substantial performance improvements over existing baselines.</li>
<li><strong>摘要：</strong>在快速发展的文本生成领域，对更精确控制机制的需求变得越来越明显。为了满足这一需求，我们提出了一种新颖的方法，LIFI，它提供了一种轻量级的方法，具有细粒度的控制来控制文本生成。与之前训练预训练语言模型以遵循离散、分类和排他控制代码的研究不同，LIFI 在连续、相对和非排他控制代码的指导下学习受控文本生成。这些细粒度的代码是从属性分类器自动派生的，最初用少量标记数据进行训练，随后用于标记大量未标记数据，从而获得更广泛的监督信号。此外，为了实现高效控制，我们将细粒度控制代码与适配器结合起来，这是一种参数和计算高效的方法来引导预训练的语言模型。我们在两项传统任务（情绪控制和主题控制）和一项新提出的任务（文体小说写作）上评估 LIFI。综合实验结果验证了我们提出的方法的有效性，证明了相对于现有基线的显着性能改进。</li>
</ul>

<h3>Title: OpenFedLLM: Training Large Language Models on Decentralized Private Data  via Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06954">https://arxiv.org/abs/2402.06954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06954">https://arxiv.org/pdf/2402.06954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06954]] OpenFedLLM: Training Large Language Models on Decentralized Private Data  via Federated Learning(https://arxiv.org/abs/2402.06954)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cover 30+ evaluation metrics. Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings. Notably, in a financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can outperform GPT-4 by a significant margin while the model obtained through individual training cannot, demonstrating strong motivation for clients to participate in FL. The code is available at https://github.com/rui-ye/OpenFedLLM.</li>
<li><strong>摘要：</strong>经过大量公开数据的训练，大型语言模型 (LLM) 在各个领域都取得了巨大的成功。虽然更多的数据有助于更好的性能，但令人不安的现实是高质量的公共数据将在几年内耗尽。在本文中，我们为当代法学硕士提供了一个潜在的下一步：通过联合学习（FL）对未充分利用的分布式私有数据进行协作和隐私保护的法学硕士培训，其中多个数据所有者协作训练共享模型而不传输原始数据。为了实现这一目标，我们构建了一个简洁、集成且适合研究的框架/代码库，名为 OpenFedLLM。它涵盖了用于增强指令跟踪能力的联合指令调整、用于与人类价值观保持一致的联合值对齐以及 7 种代表性的 FL 算法。此外，OpenFedLLM支持不同领域的训练，我们涵盖8个训练数据集；并提供全面的评估，涵盖 30 多个评估指标。通过大量的实验，我们观察到所有 FL 算法在训练 LLM 时的性能都优于本地训练，在各种设置中展示了明显的性能改进。值得注意的是，在金融基准中，通过应用任何 FL 算法进行微调的 Llama2-7B 都可以显着优于 GPT-4，而通过单独训练获得的模型却不能，这表明客户参与 FL 的强烈动机。代码可在 https://github.com/rui-ye/OpenFedLLM 获取。</li>
</ul>

<h3>Title: NLP for Knowledge Discovery and Information Extraction from Energetics  Corpora</h3>
<ul>
<li><strong>Authors: </strong>Francis G. VanGessel, Efrem Perry, Salil Mohan, Oliver M. Barham, Mark Cavolowsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06964">https://arxiv.org/abs/2402.06964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06964">https://arxiv.org/pdf/2402.06964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06964]] NLP for Knowledge Discovery and Information Extraction from Energetics  Corpora(https://arxiv.org/abs/2402.06964)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present a demonstration of the utility of NLP for aiding research into energetic materials and associated systems. The NLP method enables machine understanding of textual data, offering an automated route to knowledge discovery and information extraction from energetics text. We apply three established unsupervised NLP models: Latent Dirichlet Allocation, Word2Vec, and the Transformer to a large curated dataset of energetics-related scientific articles. We demonstrate that each NLP algorithm is capable of identifying energetic topics and concepts, generating a language model which aligns with Subject Matter Expert knowledge. Furthermore, we present a document classification pipeline for energetics text. Our classification pipeline achieves 59-76\% accuracy depending on the NLP model used, with the highest performing Transformer model rivaling inter-annotator agreement metrics. The NLP approaches studied in this work can identify concepts germane to energetics and therefore hold promise as a tool for accelerating energetics research efforts and energetics material development.</li>
<li><strong>摘要：</strong>我们展示了 NLP 在帮助含能材料和相关系统研究方面的实用性。 NLP 方法使机器能够理解文本数据，提供从能量学文本中发现知识和提取信息的自动化途径。我们将三个已建立的无监督 NLP 模型：潜在狄利克雷分配、Word2Vec 和 Transformer 应用于能量学相关科学文章的大型精选数据集。我们证明每个 NLP 算法都能够识别充满活力的主题和概念，生成与主题专家知识相一致的语言模型。此外，我们提出了能量学文本的文档分类管道。根据所使用的 NLP 模型，我们的分类管道可实现 59-76% 的准确率，其中性能最高的 Transformer 模型可与注释器间一致性指标相媲美。这项工作中研究的 NLP 方法可以识别与能量学密切相关的概念，因此有望成为加速能量学研究工作和能量学材料开发的工具。</li>
</ul>

<h3>Title: Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning  Framework for Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Jian Wang, Chak Tou Leong, Jiashuo Wang, Dongding Lin, Wenjie Li, Xiao-Yong Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06967">https://arxiv.org/abs/2402.06967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06967">https://arxiv.org/pdf/2402.06967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06967]] Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning  Framework for Dialogue(https://arxiv.org/abs/2402.06967)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat, agent</a></li>
<li><strong>Abstract: </strong>Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.</li>
<li><strong>摘要：</strong>调整用于对话生成的预训练语言模型一直是构建有能力的对话代理的流行范例。然而，传统的调优狭隘地将对话生成视为类似于其他语言生成任务，忽略了两个说话者之间的角色差异以及对话应有的多轮交互过程。这种方式导致构建的代理的聊天一致性不理想。在这项工作中，我们强调对话的交互性、交流性，并认为单独建模代理和用户的说话者角色更为可行，使代理能够始终如一地坚持其角色。我们提出了一种高效的多轮交互式对话调整（Midi-Tuning）框架。它使用两个基于大型语言模型的适配器分别对代理和用户进行建模，它们以交替的顺序逐轮利用话语，并通过轮级内存缓存机制进行调整。大量的实验表明，我们的框架的性能优于传统的微调，并且具有提高对话一致性的巨大潜力。</li>
</ul>

<h3>Title: REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records  Analysis via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07016">https://arxiv.org/abs/2402.07016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07016">https://arxiv.org/pdf/2402.07016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07016]] REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records  Analysis via Large Language Models(https://arxiv.org/abs/2402.07016)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, hallucination, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The integration of multimodal Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time-series EHR, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph (KG). Previous approaches with KG knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge. In response, we propose REALM, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR representations that address these limitations. Firstly, we apply Large Language Model (LLM) to encode long context clinical notes and GRU model to encode time-series EHR data. Secondly, we prompt LLM to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding medical knowledge. By matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency. Lastly, we propose an adaptive multimodal fusion network to integrate extracted knowledge with multimodal EHR data. Our extensive experiments on MIMIC-III mortality and readmission tasks showcase the superior performance of our REALM framework over baselines, emphasizing the effectiveness of each module. REALM framework contributes to refining the use of multimodal EHR data in healthcare and bridging the gap with nuanced medical context essential for informed clinical predictions.</li>
<li><strong>摘要：</strong>多模式电子健康记录 (EHR) 数据的集成显着提高了临床预测能力。利用临床记录和多变量时间序列 EHR，现有模型通常缺乏与临床任务相关的医学背景，从而促进了外部知识的结合，特别是来自知识图谱 (KG) 的知识。以前的知识图谱知识方法主要侧重于结构化知识提取，忽略了非结构化数据模式和语义高维医学知识。作为回应，我们提出了 REALM，这是一种检索增强生成 (RAG) 驱动的框架，用于增强多模式 EHR 表示，从而解决这些限制。首先，我们应用大型语言模型（LLM）对长上下文临床记录进行编码，并应用 GRU 模型对时间序列 EHR 数据进行编码。其次，我们提示LLM提取与任务相关的医学实体，并将专业标记的外部知识图（PrimeKG）中的实体与相应的医学知识进行匹配。通过匹配和调整临床标准，我们的框架消除了幻觉并确保了一致性。最后，我们提出了一种自适应多模态融合网络，将提取的知识与多模态 EHR 数据相集成。我们对 MIMIC-III 死亡率和再入院任务进行的广泛实验展示了我们的 REALM 框架相对于基线的卓越性能，强调了每个模块的有效性。 REALM 框架有助于完善多模式 EHR 数据在医疗保健中的使用，并弥合与临床预测所需的细致入微的医疗背景之间的差距。</li>
</ul>

<h3>Title: Informativeness of Reward Functions in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Rati Devidze, Parameswaran Kamalaruban, Adish Singla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07019">https://arxiv.org/abs/2402.07019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07019">https://arxiv.org/pdf/2402.07019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07019]] Informativeness of Reward Functions in Reinforcement Learning(https://arxiv.org/abs/2402.07019)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Reward functions are central in specifying the task we want a reinforcement learning agent to perform. Given a task and desired optimal behavior, we study the problem of designing informative reward functions so that the designed rewards speed up the agent's convergence. In particular, we consider expert-driven reward design settings where an expert or teacher seeks to provide informative and interpretable rewards to a learning agent. Existing works have considered several different reward design formulations; however, the key challenge is formulating a reward informativeness criterion that adapts w.r.t. the agent's current policy and can be optimized under specified structural constraints to obtain interpretable rewards. In this paper, we propose a novel reward informativeness criterion, a quantitative measure that captures how the agent's current policy will improve if it receives rewards from a specific reward function. We theoretically showcase the utility of the proposed informativeness criterion for adaptively designing rewards for an agent. Experimental results on two navigation tasks demonstrate the effectiveness of our adaptive reward informativeness criterion.</li>
<li><strong>摘要：</strong>奖励函数对于指定我们希望强化学习代理执行的任务至关重要。给定任务和期望的最佳行为，我们研究设计信息奖励函数的问题，以便设计的奖励加速代理的收敛。特别是，我们考虑专家驱动的奖励设计设置，其中专家或教师寻求为学习代理提供信息丰富且可解释的奖励。现有的工作已经考虑了几种不同的奖励设计公式；然而，关键的挑战是制定一个适应 w.r.t. 的奖励信息量标准。代理当前的策略，并且可以在指定的结构约束下进行优化以获得可解释的奖励。在本文中，我们提出了一种新颖的奖励信息量标准，这是一种定量度量，可以捕获代理从特定奖励函数获得奖励时当前策略将如何改进。我们从理论上展示了所提出的信息量标准对于自适应设计代理奖励的实用性。两个导航任务的实验结果证明了我们的自适应奖励信息量标准的有效性。</li>
</ul>

<h3>Title: Gemini Goes to Med School: Exploring the Capabilities of Multimodal  Large Language Models on Medical Challenge Problems & Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Ankit Pal, Malaikannan Sankarasubbu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07023">https://arxiv.org/abs/2402.07023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07023">https://arxiv.org/pdf/2402.07023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07023]] Gemini Goes to Med School: Exploring the Capabilities of Multimodal  Large Language Models on Medical Challenge Problems & Hallucinations(https://arxiv.org/abs/2402.07023)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performance. Additionally, we facilitated future research and development by releasing a Python module for medical LLM evaluation and establishing a dedicated leaderboard on Hugging Face for medical domain LLMs. Python module can be found at https://github.com/promptslab/RosettaEval</li>
<li><strong>摘要：</strong>大型语言模型在医疗保健行业具有潜在的价值，但通过严格的评估来验证其安全性和有效性至关重要。为此，我们全面评估了开源法学硕士和谷歌新的多模式法学硕士（称为 Gemini），涵盖医学推理、幻觉检测和医学视觉问答任务。虽然 Gemini 表现出了能力，但它在诊断准确性方面落后于 MedPaLM 2 和 GPT-4 等最先进的模型。此外，Gemini 在医学 VQA 数据集上的准确率达到 61.45%，明显低于 GPT-4V 88% 的准确率。我们的分析表明，双子座非常容易产生幻觉、过度自信和知识差距，这表明如果不加批判地部署就会存在风险。我们还按医学主题和测试类型进行了详细分析，为开发人员和临床医生提供了可操作的反馈。为了降低风险，我们应用了提高绩效的提示策略。此外，我们还发布了用于医学法学硕士评估的Python模块，并在Hugging Face上为医学领域法学硕士建立了专门的排行榜，以促进未来的研究和开发。 Python 模块可以在 https://github.com/promptslab/RosettaEval 找到</li>
</ul>

<h3>Title: Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts  Models</h3>
<ul>
<li><strong>Authors: </strong>Keisuke Kamahori, Yile Gu, Kan Zhu, Baris Kasikci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07033">https://arxiv.org/abs/2402.07033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07033">https://arxiv.org/pdf/2402.07033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07033]] Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts  Models(https://arxiv.org/abs/2402.07033)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}</li>
<li><strong>摘要：</strong>基于专家混合 (MoE) 架构的大型语言模型 (LLM) 在各种任务上都表现出了良好的性能。然而，由于模型尺寸巨大，在 GPU 内存资源不足的资源受限设置上运行它们具有挑战性。将模型权重卸载到 CPU 内存的现有系统面临着在 CPU 和 GPU 之间频繁移动数据的巨大开销。在本文中，我们提出了 Fiddler，一种资源高效的推理引擎，可为 MoE 模型提供 CPU-GPU 编排。 Fiddler的核心思想是利用CPU的计算能力来最小化CPU和GPU之间的数据移动。我们的评估表明，Fiddler 可以运行参数超过 90GB 的未压缩 Mixtral-8x7B 模型，在具有 24GB 内存的单个 GPU 上每秒生成超过 3 美元的代币，这比现有方法有了数量级的改进。 Fiddler 的代码公开在 \url{https://github.com/efeslab/fiddler}</li>
</ul>

<h3>Title: A Tale of Tails: Model Collapse as a Change of Scaling Laws</h3>
<ul>
<li><strong>Authors: </strong>Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07043">https://arxiv.org/abs/2402.07043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07043">https://arxiv.org/pdf/2402.07043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07043]] A Tale of Tails: Model Collapse as a Change of Scaling Laws(https://arxiv.org/abs/2402.07043)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.</li>
<li><strong>摘要：</strong>随着人工智能模型规模的增长，神经缩放定律已成为预测大型模型在增加原始（人类或自然）训练数据的容量和规模时的改进的重要工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将共同进化，以逐步包含越来越多的合成数据。在本文中，我们要问：在合成数据不可避免地进入训练语料库的情况下，缩放法则将如何变化？未来的模型是否仍会改进，还是注定会退化直至（模型）完全崩溃？我们通过缩放定律的视角开发了模型崩溃的理论框架。我们发现了各种各样的衰减现象，分析了尺度损失、代数变化尺度、技能的“不学习”，以及混合人类和合成数据时的摸索。我们的理论通过大规模实验得到了验证使用大型语言模型 Llama2 进行算术任务和文本生成的转换器。</li>
</ul>

<h3>Title: A Factor Graph Model of Trust for a Collaborative Multi-Agent System</h3>
<ul>
<li><strong>Authors: </strong>Behzad Akbari, Mingfeng Yuan, Hao Wang, Haibin Zhu, Jinjun Shan</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07049">https://arxiv.org/abs/2402.07049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07049">https://arxiv.org/pdf/2402.07049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07049]] A Factor Graph Model of Trust for a Collaborative Multi-Agent System(https://arxiv.org/abs/2402.07049)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial. Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system. Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents. This includes modeling the behavior of robots as a trajectory of actions using a Gaussian process factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors. Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation. The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically assess trust-based decisions with informed consent. The effectiveness of this method is validated via simulations and empirical tests with autonomous robots navigating unsignalized intersections.</li>
<li><strong>摘要：</strong>在以其开放性、动态性和协作性而闻名的多智能体系统（MAS）领域，信任其他智能体的资源和服务的能力至关重要。在这种情况下，信任是指代理对系统内其他人的信息、行为、意图、真实性和能力的依赖和信心。我们的论文介绍了一种新的图形方法，利用因子图来表示代理之间相互依赖的行为和可信度。这包括使用高斯过程因子图将机器人的行为建模为动作轨迹，该图考虑了平滑度、避障和信任相关因素。我们评估信任的方法是分散的，并考虑关键的相互依赖的子因素，例如邻近安全性、一致性和合作。整个系统包括一个因子图网络，这些因子图通过与信任相关的因素进行交互，并采用贝叶斯推理方法在知情同意的情况下动态评估基于信任的决策。该方法的有效性通过自主机器人在无信号交叉口导航的模拟和实证测试得到验证。</li>
</ul>

<h3>Title: $L^*LM$: Learning Automata from Examples using Natural Language Oracles</h3>
<ul>
<li><strong>Authors: </strong>Marcell Vazquez-Chanlatte, Karim Elmaaroufi, Stefan J. Witwicki, Sanjit A. Seshia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07051">https://arxiv.org/abs/2402.07051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07051">https://arxiv.org/pdf/2402.07051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07051]] $L^*LM$: Learning Automata from Examples using Natural Language Oracles(https://arxiv.org/abs/2402.07051)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.</li>
<li><strong>摘要：</strong>专家演示已被证明是间接指定复杂任务的简单方法。最近的算法甚至支持提取明确的形式规范，例如确定性有限自动机 (DFA)，来自演示。不幸的是，这些技术通常样本效率不高。在这项工作中，我们介绍了 $L^*LM$，一种从演示和自然语言中学习 DFA 的算法。由于自然语言的表达能力，我们从专家演示中观察到学习 DFA 的数据效率显着提高。从技术上讲，$L^*LM$ 利用大型语言模型来回答有关底层任务的成员资格查询。然后将其与最新技术相结合，将演示学习转化为一系列标记示例学习问题。在我们的实验中，我们观察到这两种模式相互补充，产生了强大的小样本学习器。</li>
</ul>

<h3>Title: Using Large Language Models to Automate and Expedite Reinforcement  Learning with Reward Machine</h3>
<ul>
<li><strong>Authors: </strong>Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk Topcu, Zhe Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07069">https://arxiv.org/abs/2402.07069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07069">https://arxiv.org/pdf/2402.07069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07069]] Using Large Language Models to Automate and Expedite Reinforcement  Learning with Reward Machine(https://arxiv.org/abs/2402.07069)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies.</li>
<li><strong>摘要：</strong>我们提出了 LARL-RM（大型语言模型生成的奖励机强化学习自动机）算法，以便使用自动机将高级知识编码到强化学习中，以加速强化学习。我们的方法使用大型语言模型（LLM）通过即时工程来获取高级领域特定知识，而不是直接向强化学习算法提供需要专家对自动机进行编码的高级知识。我们使用思想链和少样本方法进行快速工程，并证明我们的方法可以使用这些方法发挥作用。此外，LARL-RM 允许完全闭环强化学习，无需专家指导和监督学习，因为 LARL-RM 可以直接使用 LLM 来生成手头任务所需的高级知识。我们还展示了我们的算法收敛到最优策略的理论保证。我们通过在两个案例研究中实施我们的方法来证明 LARL-RM 将收敛速度提高了 30%。</li>
</ul>

<h3>Title: Using Large Language Models for Student-Code Guided Test Case Generation  in Computer Science Education</h3>
<ul>
<li><strong>Authors: </strong>Nischal Ashok Kumar, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07081">https://arxiv.org/abs/2402.07081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07081">https://arxiv.org/pdf/2402.07081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07081]] Using Large Language Models for Student-Code Guided Test Case Generation  in Computer Science Education(https://arxiv.org/abs/2402.07081)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In computer science education, test cases are an integral part of programming assignments since they can be used as assessment items to test students' programming knowledge and provide personalized feedback on student-written code. The goal of our work is to propose a fully automated approach for test case generation that can accurately measure student knowledge, which is important for two reasons. First, manually constructing test cases requires expert knowledge and is a labor-intensive process. Second, developing test cases for students, especially those who are novice programmers, is significantly different from those oriented toward professional-level software developers. Therefore, we need an automated process for test case generation to assess student knowledge and provide feedback. In this work, we propose a large language model-based approach to automatically generate test cases and show that they are good measures of student knowledge, using a publicly available dataset that contains student-written Java code. We also discuss future research directions centered on using test cases to help students.</li>
<li><strong>摘要：</strong>在计算机科学教育中，测试用例是编程作业不可或缺的一部分，因为它们可以用作评估项目来测试学生的编程知识并对学生编写的代码提供个性化反馈。我们工作的目标是提出一种完全自动化的测试用例生成方法，可以准确地衡量学生的知识，这很重要，原因有二。首先，手动构建测试用例需要专业知识，并且是一个劳动密集型过程。其次，为学生（尤其是新手程序员）开发测试用例与面向专业级软件开发人员的测试用例有显着不同。因此，我们需要一个自动化的测试用例生成过程来评估学生的知识并提供反馈。在这项工作中，我们提出了一种基于大型语言模型的方法来自动生成测试用例，并使用包含学生编写的 Java 代码的公开数据集来证明它们是衡量学生知识的良好方法。我们还讨论了未来的研究方向，重点是使用测试用例来帮助学生。</li>
</ul>

<h3>Title: Refined Sample Complexity for Markov Games with Independent Linear  Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Yan Dai, Qiwen Cui, Simon S. Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07082">https://arxiv.org/abs/2402.07082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07082">https://arxiv.org/pdf/2402.07082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07082]] Refined Sample Complexity for Markov Games with Independent Linear  Function Approximation(https://arxiv.org/abs/2402.07082)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent linear function approximations, we propose novel *action-dependent bonuses* to cover occasionally extreme estimation errors. With the help of state-of-the-art techniques from the single-agent RL literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$ convergence rate, and avoids $\text{poly}(A_{\max})$ dependency simultaneously.</li>
<li><strong>摘要：</strong>马尔可夫游戏（MG）是多智能体强化学习（MARL）的重要模型。长期以来，人们一直认为“多智能体的诅咒”（即算法性能随着智能体的数量呈指数级下降）是不可避免的，直到最近的几项工作（Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023） al., 2023。虽然这些工作确实解决了多智能体的诅咒，但当状态空间过大并且部署（线性）函数近似时，它们的收敛速度要么较慢 $O(T^{-1/ 4})$ 或对动作数量 $A_{\max}$ 产生多项式依赖——这在单代理情况下是可以避免的，即使损失函数可以随时间任意变化（Dai et al., 2023）。本文首先完善了 Wang 等人 (2023) 的“AVLPR”框架，深入了解了“数据相关”（即随机）对次优差距的悲观估计，从而允许更广泛的插件算法选择当专门研究具有独立线性函数近似的 MG 时，我们提出新颖的“依赖于动作的奖励”来覆盖偶尔出现的极端估计错误。借助单智能体强化学习文献中最先进的技术，我们给出了第一个解决多智能体诅咒的算法，获得了最优 $O(T^{-1/2})$收敛速度，并同时避免 $\text{poly}(A_{\max})$ 依赖。</li>
</ul>

<h3>Title: Generalizing Conversational Dense Retrieval via LLM-Cognition Data  Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, Ziliang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07092">https://arxiv.org/abs/2402.07092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07092">https://arxiv.org/pdf/2402.07092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07092]] Generalizing Conversational Dense Retrieval via LLM-Cognition Data  Augmentation(https://arxiv.org/abs/2402.07092)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug.</li>
<li><strong>摘要：</strong>会话搜索利用多轮自然语言上下文来检索相关段落。现有的会话密集检索模型大多将会话视为固定的问题和响应序列，忽视了严重的数据稀疏问题——即用户可以通过多种方式进行会话，而这些交替的会话是未记录的。因此，他们常常很难概括现实场景中的不同对话。在这项工作中，我们提出了一个通过 LLM 认知数据增强 (ConvAug) 泛化会话密集检索的框架。 ConvAug 首先生成多级增强对话，以捕获对话上下文的多样性。受人类认知的启发，我们设计了一种认知感知过程来减少误报、漏报和幻觉的产生。此外，我们开发了一种难度自适应样本过滤器，可以为复杂的对话选择具有挑战性的样本，从而为模型提供更大的学习空间。然后采用对比学习目标来训练更好的会话上下文编码器。在正常和零样本设置下对四个公共数据集进行的广泛实验证明了 ConvAug 的有效性、普遍性和适用性。</li>
</ul>

<h3>Title: Sequential Ordering in Textual Descriptions: Impact on Spatial  Perception Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuyao Ge, Shenghua Liu, Lingrui Mei, Lizhe Chen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07140">https://arxiv.org/abs/2402.07140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07140">https://arxiv.org/pdf/2402.07140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07140]] Sequential Ordering in Textual Descriptions: Impact on Spatial  Perception Abilities of Large Language Models(https://arxiv.org/abs/2402.07140)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.</li>
<li><strong>摘要：</strong>近年来，大型语言模型已经在多个领域达到了最先进的性能。然而，图推理领域的进展仍然有限。我们的工作通过法学硕士彻底研究图形推理来深入研究这一差距。在这项工作中，我们揭示了文本序列对 LLM 空间理解的影响，发现图描述文本序列显着影响 LLM 在图上的推理性能。通过改变图形描述文本序列，我们将 LLM 的性能从 42.22% 提高到 70%。此外，我们评估了LLM性能与图大小之间的关系，发现LLM的推理性能并不随着图大小的增加而单调下降。最后，我们介绍了缩放图推理基准，用于评估不同图大小的 LLM 性能。</li>
</ul>

<h3>Title: Natural Language Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushiks, Yali Du, Ying Wen, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07157">https://arxiv.org/abs/2402.07157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07157">https://arxiv.org/pdf/2402.07157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07157]] Natural Language Reinforcement Learning(https://arxiv.org/abs/2402.07157)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.</li>
<li><strong>摘要：</strong>强化学习（RL）在学习决策任务的策略方面表现出了卓越的能力。然而，强化学习常常受到样本效率低、缺乏可解释性和监督信号稀疏等问题的阻碍。为了解决这些限制，我们从人类学习过程中汲取灵感，引入自然语言强化学习 (NLRL)，它将 RL 原理与自然语言表示创新地结合起来。具体来说，NLRL 重新定义了自然语言空间中的任务目标、策略、价值函数、贝尔曼方程和策略迭代等 RL 概念。我们介绍如何利用 GPT-4 等大型语言模型 (LLM) 的最新进展来实际实施 NLRL。对表格 MDP 的初步实验证明了 NLRL 框架的有效性、效率和可解释性。</li>
</ul>

<h3>Title: Social Evolution of Published Text and The Emergence of Artificial  Intelligence Through Large Language Models and The Problem of Toxicity and  Bias</h3>
<ul>
<li><strong>Authors: </strong>Arifa Khan, P. Saravanan, S.K Venkatesan</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07166">https://arxiv.org/abs/2402.07166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07166">https://arxiv.org/pdf/2402.07166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07166]] Social Evolution of Published Text and The Emergence of Artificial  Intelligence Through Large Language Models and The Problem of Toxicity and  Bias(https://arxiv.org/abs/2402.07166)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.</li>
<li><strong>摘要：</strong>我们提供了人工智能和深度学习快速发展的鸟瞰图，这些发展导致了人工智能在大型语言模型中的突破性出现。本研究的目的是将所有这些发展置于务实的、更广泛的历史社会视角中，不带任何夸张，同时也不带任何造成 20 世纪 70 年代至 1990 年代人工智能冬天的悲观主义情绪。我们同时也指出毒性、偏见、记忆、阿谀奉承、逻辑不一致、幻觉的存在，这些都是对过度乐观者的警告。我们在这里注意到，正如人工智能的出现似乎发生在神经连接或权重数量的阈值点一样，我们也观察到人类大脑，尤其是皮层区域并没有什么特殊或非凡的，而只是按比例缩放的情况。灵长类大脑的放大版本，甚至人类的智力似乎也是一种规模的新兴现象。</li>
</ul>

<h3>Title: Large-Language-Model Empowered Dose Volume Histogram Prediction for  Intensity Modulated Radiotherapy</h3>
<ul>
<li><strong>Authors: </strong>Zehao Dong, Yixin Chen, Hiram Gay, Yao Hao, Geoffrey D. Hugo, Pamela Samson, Tianyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07167">https://arxiv.org/abs/2402.07167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07167">https://arxiv.org/pdf/2402.07167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07167]] Large-Language-Model Empowered Dose Volume Histogram Prediction for  Intensity Modulated Radiotherapy(https://arxiv.org/abs/2402.07167)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Treatment planning is currently a patient specific, time-consuming, and resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction plays a critical role in automating this process. The geometric relationship between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target volume (PTV) has been well established. This study explores the potential of deep learning models for predicting DVHs using images and subsequent human intervention facilitated by a large-language model (LLM) to enhance the planning quality. We propose a pipeline to convert unstructured images to a structured graph consisting of image-patch nodes and dose nodes. A novel Dose Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the structured graph. The proposed DoseGNN is enhanced with the LLM to encode massive knowledge from prescriptions and interactive instructions from clinicians. In this study, we introduced an online human-AI collaboration (OHAC) system as a practical implementation of the concept proposed for the automation of intensity-modulated radiotherapy (IMRT) planning. In comparison to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean square errors that were 80$\%$, 76$\%$ and 41.0$\%$ of those predicted by Swin U-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans through interaction with clinicians using natural language.</li>
<li><strong>摘要：</strong>目前，治疗计划是放射治疗中一项针对患者的、耗时且需要资源的任务。剂量体积直方图 (DVH) 预测在自动化该过程中发挥着关键作用。放疗计划中的 DVH 与危及器官 (OAR) 和计划靶区 (PTV) 之间的几何关系已得到很好的建立。本研究探讨了深度学习模型使用图像预测 DVH 的潜力，以及大语言模型 (LLM) 促进的后续人工干预以提高规划质量。我们提出了一种将非结构化图像转换为由图像补丁节点和剂量节点组成的结构化图的管道。开发了一种新颖的剂量图神经网络 (DoseGNN) 模型，用于从结构化图中预测 DVH。所提出的 DoseGNN 通过法学硕士得到了增强，可以对来自处方和临床医生交互式指令的大量知识进行编码。在这项研究中，我们引入了在线人机协作（OHAC）系统，作为调强放射治疗（IMRT）计划自动化概念的实际实施。与放射治疗中广泛使用的深度学习模型相比，DoseGNN 的均方误差分别为 Swin U-Net Transformer、3D U- 预测的 80$\%$、76$\%$ 和 41.0$\%$分别是 Net CNN 和 vanilla MLP。此外，法学硕士授权的 DoseGNN 模型通过使用自然语言与临床医生互动，有助于无缝调整治疗计划。</li>
</ul>

<h3>Title: Prompt Perturbation in Retrieval-Augmented Generation based Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Hu, Chen Wang, Yanfeng Shu, Helen (Hye-Young)Paik, Liming Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07179">https://arxiv.org/abs/2402.07179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07179">https://arxiv.org/pdf/2402.07179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07179]] Prompt Perturbation in Retrieval-Augmented Generation based Large  Language Models(https://arxiv.org/abs/2402.07179)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在广泛领域的使用迅速增长，其稳健性变得越来越重要。检索增强生成（RAG）被认为是提高法学硕士文本生成可信度的一种手段。然而，基于 RAG 的法学硕士的输出如何受到略有不同的输入的影响尚未得到充分研究。在这项工作中，我们发现即使在提示中插入一个短前缀也会导致生成的输出远离实际正确的答案。我们通过引入一种称为梯度引导提示扰动（GGPP）的新颖优化技术来系统地评估此类前缀对 RAG 的影响。 GGPP 在将基于 RAG 的 LLM 的输出引导到有针对性的错误答案方面取得了很高的成功率。它还可以处理提示中要求忽略不相关上下文的指令。我们还利用有和没有 GGPP 扰动的提示之间的 LLM 神经元激活差异，给出了一种方法，通过对 GGPP 生成的提示触发的神经元激活进行训练的高效检测器来提高基于 RAG 的 LLM 的鲁棒性。我们对开源法学硕士的评估证明了我们方法的有效性。</li>
</ul>

<h3>Title: GraphTranslator: Aligning Graph Model to Large Language Model for  Open-ended Tasks</h3>
<ul>
<li><strong>Authors: </strong>Mengmei Zhang (Alibaba Group Holding Limited, China Telecom Bestpay), Mingwei Sun (Alibaba Group Holding Limited), Peng Wang (Alibaba Group Holding Limited), Shen Fan (Alibaba Group Holding Limited), Yanhu Mo (Alibaba Group Holding Limited), Xiaoxiao Xu (Alibaba Group Holding Limited), Hong Liu (Alibaba Group Holding Limited), Cheng Yang (Peng Cheng Laboratory), Chuan Shi (Peng Cheng Laboratory)</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07197">https://arxiv.org/abs/2402.07197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07197">https://arxiv.org/pdf/2402.07197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07197]] GraphTranslator: Aligning Graph Model to Large Language Model for  Open-ended Tasks(https://arxiv.org/abs/2402.07197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By treating the node representation as a type of language, the proposed GraphTranslator empowers an LLM to make predictions based on node representation and language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results show that the proposed GraphTranslator effectively improves the results of zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended applications through language instructions.</li>
<li><strong>摘要：</strong>像 ChatGPT 这样的大型语言模型 (LLM) 展现出强大的零样本和指令跟踪能力，催化了人工智能不同研究领域的革命性转变，尤其是开放式任务。尽管这个想法在图领域中较少被探索，尽管有许多强大的图模型（GM）可用，但它们仅限于预定义形式的任务。尽管已经提出了几种将 LLM 应用于图的方法，但它们无法同时处理预定义和开放式任务，而 LLM 作为节点特征增强器或独立预测器。为了打破这一困境，我们建议通过一个名为GraphTranslator的翻译器来桥接预训练的GM和LLM，旨在利用GM有效地处理预定义的任务，并利用LLM的扩展接口为GM提供各种开放式任务。为了训练这样的翻译器，我们提出了一个能够沿着节点信息、邻居信息和模型信息构建图文对齐数据的生产者。通过将节点表示视为一种语言，所提出的 GraphTranslator 使法学硕士能够根据节点表示和语言指令进行预测，为预定义和开放式任务提供统一的视角。大量结果表明，所提出的GraphTranslator有效提高了零样本节点分类的结果。图形问答实验揭示了我们的 GraphTranslator 通过语言指令在广泛的开放式应用程序中的潜力。</li>
</ul>

<h3>Title: Synergizing Spatial Optimization with Large Language Models for  Open-Domain Urban Itinerary Planning</h3>
<ul>
<li><strong>Authors: </strong>Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Kebing Hou, Dingyi Zhuang, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07204">https://arxiv.org/abs/2402.07204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07204">https://arxiv.org/pdf/2402.07204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07204]] Synergizing Spatial Optimization with Large Language Models for  Open-Domain Urban Itinerary Planning(https://arxiv.org/abs/2402.07204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in cooperation with an embedding-based module for retrieving candidate POIs from the user's POI database. Then, a spatial optimization module is used to order these POIs, followed by LLM crafting a personalized, spatially coherent itinerary. To the best of our knowledge, this study marks the first integration of LLMs to innovate itinerary planning solutions. Extensive experiments on offline datasets and online subjective evaluation have demonstrated the capacities of our system to deliver more responsive and spatially coherent itineraries than current LLM-based solutions. Our system has been deployed in production at the TuTu online travel service and has attracted thousands of users for their urban travel planning.</li>
<li><strong>摘要：</strong>在本文中，我们首次提出了城市步行的开放域城市行程规划（OUIP）任务，该任务根据以自然语言描述的用户请求直接生成行程。 OUIP与传统的行程规划不同，它限制了用户表达更详细的需求，阻碍了真正的个性化。最近，大型语言模型（LLM）在处理各种任务方面表现出了潜力。然而，由于信息非实时、知识不完整、空间意识不足，他们无法在 OUIP 中独立交付令人满意的用户体验。鉴于此，我们推出了 ItiNera，这是一个 OUIP 系统，它将空间优化与大型语言模型 (LLM) 相结合，提供根据用户需求定制城市行程的服务。具体来说，我们开发了一个基于 LLM 的管道，用于提取和更新 POI 特征，以创建用户拥有的个性化 POI 数据库。对于每个用户请求，我们利用 LLM 与基于嵌入的模块配合，从用户的 POI 数据库中检索候选 POI。然后，使用空间优化模块对这些 POI 进行排序，然后由法学硕士制定个性化的、空间连贯的行程。据我们所知，这项研究标志着法学硕士首次整合以创新行程规划解决方案。对离线数据集和在线主观评估的广泛实验证明，与当前基于法学硕士的解决方案相比，我们的系统能够提供响应更快、空间连贯的行程。我们的系统已在途途在线旅游服务中投入使用，并吸引了数千名用户进行城市旅游规划。</li>
</ul>

<h3>Title: The Reasons that Agents Act: Intention and Instrumental Goals</h3>
<ul>
<li><strong>Authors: </strong>Francis Rhys Ward, Matt MacDermott, Francesco Belardinelli, Francesca Toni, Tom Everitt</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07221">https://arxiv.org/abs/2402.07221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07221">https://arxiv.org/pdf/2402.07221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07221]] The Reasons that Agents Act: Intention and Instrumental Goals(https://arxiv.org/abs/2402.07221)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how our definition can be used to infer the intentions of reinforcement learning agents and language models from their behaviour.</li>
<li><strong>摘要：</strong>意图是人工智能中一个重要且具有挑战性的概念。它很重要，因为它是我们关心的许多其他概念的基础，例如代理、操纵、法律责任和指责。然而，将意图归因于人工智能系统是有争议的，并且没有普遍接受的适用于人工智能代理的意图理论。我们将代理人行为的意图付诸实践，与其选择决定的原因相关。我们在结构因果影响模型中引入了意图的正式定义，该定义基于关于意图的哲学文献并适用于现实世界的机器学习系统。通过大量的例子和结果，我们表明我们的定义捕捉了意图的直观概念，并满足了过去工作提出的需求。此外，我们还展示了我们的定义如何与过去的概念相关，包括实际因果关系和工具目标的概念，这是安全人工智能代理文献中的核心思想。最后，我们演示了如何使用我们的定义从强化学习代理和语言模型的行为中推断其意图。</li>
</ul>

<h3>Title: TransGPT: Multi-modal Generative Pre-trained Transformer for  Transportation</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Xiang Wei, Fangxu Hu, Wenjuan Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07233">https://arxiv.org/abs/2402.07233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07233">https://arxiv.org/pdf/2402.07233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07233]] TransGPT: Multi-modal Generative Pre-trained Transformer for  Transportation(https://arxiv.org/abs/2402.07233)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and multi-modal inputs and outputs. This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is finetuned on a multi-modal Transportation dataset (MTD) that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential applications of TransGPT for traffic analysis and modeling, such as generating synthetic traffic scenarios, explaining traffic phenomena, answering traffic-related questions, providing traffic recommendations, and generating traffic reports. This work advances the state-of-the-art of NLP in the transportation domain and provides a useful tool for ITS researchers and practitioners.</li>
<li><strong>摘要：</strong>自然语言处理（NLP）是智能交通系统（ITS）的关键组成部分，但它在交通领域面临着许多挑战，例如特定领域的知识和数据、多模态输入和输出。本文提出了 TransGPT，一种用于交通领域的新型（多模态）大语言模型，它由两个独立的变体组成：用于单模态数据的 TransGPT-SM 和用于多模态数据的 TransGPT-MM。 TransGPT-SM 在单模态交通数据集 (STD) 上进行了微调，该数据集包含来自交通领域各种来源的文本数据。 TransGPT-MM 在多模式交通数据集 (MTD) 上进行了微调，该数据集是我们从交通领域的三个领域手动收集的：驾驶测试、交通标志和地标。我们在交通领域不同任务的几个基准数据集上评估 TransGPT，并表明它在大多数任务上优于基线模型。我们还展示了 TransGPT 在交通分析和建模方面的潜在应用，例如生成合成交通场景、解释交通现象、回答交通相关问题、提供交通建议和生成交通报告。这项工作推进了交通领域 NLP 的最先进水平，并为 ITS 研究人员和从业者提供了有用的工具。</li>
</ul>

<h3>Title: CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for  Chinese Public Security Domain</h3>
<ul>
<li><strong>Authors: </strong>Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07234">https://arxiv.org/abs/2402.07234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07234">https://arxiv.org/pdf/2402.07234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07234]] CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for  Chinese Public Security Domain(https://arxiv.org/abs/2402.07234)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已在多个应用领域展现出巨大的潜力和有效性。为了评估主流法学硕士在公共安全任务中的表现，本研究旨在构建一个适合中国公共安全领域的专业评估基准——CPSDbench。 CPSDbench整合了从现实场景中收集的公共安全相关数据集，支持对法学硕士在文本分类、信息提取、问答和文本生成四个关键维度的全面评估。此外，本研究引入了一套创新的评估指标，旨在更精确地量化法学硕士在执行与公共安全相关的任务中的功效。通过本研究的深入分析和评估，我们不仅增强了对现有模型在解决公共安全问题方面的性能优势和局限性的理解，也为未来开发更准确、定制化的针对公共安全问题的LLM模型提供了参考。该领域的应用。</li>
</ul>

<h3>Title: Low-Resource Counterspeech Generation for Indic Languages: The Case of  Bengali and Hindi</h3>
<ul>
<li><strong>Authors: </strong>Mithun Das, Saurabh Kumar Pandey, Shivansh Sethi, Punyajoy Saha, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07262">https://arxiv.org/abs/2402.07262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07262">https://arxiv.org/pdf/2402.07262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07262]] Low-Resource Counterspeech Generation for Indic Languages: The Case of  Bengali and Hindi(https://arxiv.org/abs/2402.07262)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can "counter" the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for low-resource languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark. We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.</li>
<li><strong>摘要：</strong>随着在线滥用行为的兴起，NLP 社区已经开始研究使用神经架构来生成反言论，这种反言论可以“对抗”此类辱骂言论的恶毒语气，并淡化/改善其在社交网络上的连锁反应。然而，到目前为止，大部分努力主要集中在英语上。为了弥补孟加拉语和印地语等资源匮乏语言的差距，我们创建了一个包含 5,062 个辱骂性言语/反言语对的基准数据集，其中 2,460 对是孟加拉语，2,602 对是印地语。我们实现了几个基线模型，考虑具有不同配置的各种语间迁移机制，以生成合适的反言语来建立有效的基准。我们观察到单语言设置可以产生最佳性能。此外，利用合成迁移，语言模型可以在一定程度上生成反言语；具体来说，我们注意到当语言属于同一语系时，可迁移性更好。</li>
</ul>

<h3>Title: Previously on the Stories: Recap Snippet Identification for Story  Reading</h3>
<ul>
<li><strong>Authors: </strong>Jiangnan Li, Qiujing Wang, Liyan Xu, Wenjie Pang, Mo Yu, Zheng Lin, Weiping Wang, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07271">https://arxiv.org/abs/2402.07271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07271">https://arxiv.org/pdf/2402.07271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07271]] Previously on the Stories: Recap Snippet Identification for Story  Reading(https://arxiv.org/abs/2402.07271)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Similar to the "previously-on" scenes in TV shows, recaps can help book reading by recalling the readers' memory about the important elements in previous texts to better understand the ongoing plot. Despite its usefulness, this application has not been well studied in the NLP community. We propose the first benchmark on this useful task called Recap Snippet Identification with a hand-crafted evaluation dataset. Our experiments show that the proposed task is challenging to PLMs, LLMs, and proposed methods as the task requires a deep understanding of the plot correlation between snippets.</li>
<li><strong>摘要：</strong>与电视节目中的“之前发生的”场景类似，回顾可以通过回忆读者对之前文本中重要元素的记忆来帮助阅读，从而更好地理解正在进行的情节。尽管它很有用，但该应用程序尚未在 NLP 社区中得到充分研究。我们针对这项有用的任务提出了第一个基准，称为“回顾片段识别”，并使用手工制作的评估数据集。我们的实验表明，所提出的任务对 PLM、LLM 和所提出的方法具有挑战性，因为该任务需要深入了解片段之间的绘图相关性。</li>
</ul>

<h3>Title: How do Large Language Models Navigate Conflicts between Honesty and  Helpfulness?</h3>
<ul>
<li><strong>Authors: </strong>Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07282">https://arxiv.org/abs/2402.07282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07282">https://arxiv.org/pdf/2402.07282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07282]] How do Large Language Models Navigate Conflicts between Honesty and  Helpfulness?(https://arxiv.org/abs/2402.07282)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.</li>
<li><strong>摘要：</strong>在日常交流中，人们常常会近似事实——例如，舍入时间或省略细节——以便最大限度地帮助听众。大型语言模型 (LLM) 如何处理如此微妙的权衡？为了解决这个问题，我们使用旨在表征人类行为的心理模型和实验来分析法学硕士。我们测试了一系列法学硕士，并探索人类偏好或推理时间推理的优化如何影响这些权衡。我们发现，根据人类反馈进行强化学习可以提高诚实度和乐于助人的程度，而思维链提示则使法学硕士倾向于乐于助人而不是诚实。最后，GPT-4 Turbo 展示了类人的响应模式，包括对对话框架和听众决策上下文的敏感性。我们的研究结果揭示了法学硕士内化的对话价值观，并表明即使这些抽象价值观在某种程度上也可以通过零样本提示来引导。</li>
</ul>

<h3>Title: HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node  Classification on Text-Attributed Hypergraphs</h3>
<ul>
<li><strong>Authors: </strong>Adrián Bazaga, Pietro Liò, Gos Micklem</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07309">https://arxiv.org/abs/2402.07309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07309">https://arxiv.org/pdf/2402.07309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07309]] HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node  Classification on Text-Attributed Hypergraphs(https://arxiv.org/abs/2402.07309)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model which simultaneously models hypergraph relational structure while maintaining the high-quality text encoding capabilities of a pre-trained BERT. Notably, HyperBERT presents results that achieve a new state-of-the-art on 5 challenging text-attributed hypergraph node classification benchmarks.</li>
<li><strong>摘要：</strong>超图以复杂的拓扑为特征，用超边表达多个实体之间的高阶交互。最近，基于超图的深度学习方法用于学习文本属性超图节点分类问题的信息数据表示，引起了越来越多的研究关注。然而，现有的方法很难同时捕获超图结构信息的全部范围和节点属性固有的丰富语言属性，这在很大程度上阻碍了它们的有效性和泛化性。为了克服这些挑战，我们探索了通过专门的超图感知层进一步增强预训练 BERT 模型的方法，以完成节点分类任务。这些层将高阶结构归纳偏差引入到语言模型中，从而提高了模型利用超图结构中的高阶上下文信息和文本中存在的语义信息的能力。在本文中，我们提出了一种新架构 HyperBERT，这是一种混合文本超图模型，它可以同时对超图关系结构进行建模，同时保持预训练 BERT 的高质量文本编码能力。值得注意的是，HyperBERT 呈现的结果在 5 个具有挑战性的文本属性超图节点分类基准上实现了新的最先进水平。</li>
</ul>

<h3>Title: A Theoretical Analysis of Nash Learning from Human Feedback under  General KL-Regularized Preference</h3>
<ul>
<li><strong>Authors: </strong>Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07314">https://arxiv.org/abs/2402.07314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07314">https://arxiv.org/pdf/2402.07314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07314]] A Theoretical Analysis of Nash Learning from Human Feedback under  General KL-Regularized Preference(https://arxiv.org/abs/2402.07314)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference. In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs. The learning objective is to find a policy that consistently generates responses preferred over any competing policy while staying close to the initial model. The objective is defined as the Nash equilibrium (NE) of the KL-regularized preference model. We aim to make the first attempt to study the theoretical learnability of the KL-regularized NLHF by considering both offline and online settings. For the offline learning from a pre-collected dataset, we propose algorithms that are efficient under suitable coverage conditions of the dataset. For batch online learning from iterative interactions with a preference oracle, our proposed algorithm enjoys a finite sample guarantee under the structural condition of the underlying preference model. Our results connect the new NLHF paradigm with traditional RL theory, and validate the potential of reward-model-free learning under general preference.</li>
<li><strong>摘要：</strong>人类反馈强化学习 (RLHF) 从概率偏好模型提供的偏好信号中学习，该模型将提示和两个响应作为输入，并生成一个分数，指示一个响应相对于另一个响应的偏好。到目前为止，最流行的RLHF范式是基于奖励的，它从奖励建模的初始步骤开始，然后使用构建的奖励为后续奖励优化阶段提供奖励信号。然而，奖励函数的存在是一个强有力的假设，基于奖励的 RLHF 的表达能力有限，无法捕捉现实世界复杂的人类偏好。在这项工作中，我们为最近提出的学习范式提供了理论见解，即从人类反馈中学习纳什（NLHF），该范式考虑了一般偏好模型，并将对齐过程表述为两个竞争性法学硕士之间的博弈。学习目标是找到一种策略，该策略能够持续生成优于任何竞争策略的响应，同时保持接近初始模型。目标被定义为 KL 正则化偏好模型的纳什均衡 (NE)。我们的目标是首次尝试通过考虑离线和在线设置来研究 KL 正则化 NLHF 的理论可学习性。对于从预先收集的数据集进行离线学习，我们提出了在数据集的适当覆盖条件下有效的算法。对于通过与偏好预言机迭代交互的批量在线学习，我们提出的算法在底层偏好模型的结构条件下享有有限样本保证。我们的结果将新的 NLHF 范式与传统的 RL 理论联系起来，并验证了一般偏好下无奖励模型学习的潜力。</li>
</ul>

<h3>Title: ODIN: Disentangled Reward Mitigates Hacking in RLHF</h3>
<ul>
<li><strong>Authors: </strong>Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07319">https://arxiv.org/abs/2402.07319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07319">https://arxiv.org/pdf/2402.07319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07319]] ODIN: Disentangled Reward Mitigates Hacking in RLHF(https://arxiv.org/abs/2402.07319)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the other trained to decorrelate with length and therefore focus more on the actual content. We then discard the length head in RL to prevent reward hacking on length. Experiments demonstrate that our approach almost eliminates the reward correlation with length, and improves the obtained policy by a significant margin.</li>
<li><strong>摘要：</strong>在这项工作中，我们研究了响应长度的奖励黑客问题，这是法学硕士的人类反馈强化学习（RLHF）中出现的一个挑战。法学硕士的格式良好、冗长但帮助不大的回复往往会欺骗法学硕士甚至人类评估者以获得高分。同样的问题也适用于强化学习中的一些奖励模型。为了解决训练和评估方面的挑战，我们建立了一个更可靠的评估协议来比较不同的训练配置，该协议检查LLM评估分数和通过改变训练超参数获得的响应长度之间的权衡。基于此评估，我们进行了大规模研究，结果深入了解了强化学习中使用的超参数和技巧在减轻长度偏差方面的功效。我们进一步建议通过在共享特征表示上联合训练两个线性头来预测奖励来改进奖励模型，一个训练为与长度相关，另一个训练为与长度去相关，因此更多地关注实际内容。然后，我们丢弃 RL 中的长度头，以防止长度上的奖励黑客行为。实验表明，我们的方法几乎消除了奖励与长度的相关性，并显着提高了所获得的策略。</li>
</ul>

<h3>Title: Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bilal Chughtai, Alan Cooney, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07321">https://arxiv.org/abs/2402.07321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07321">https://arxiv.org/pdf/2402.07321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07321]] Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs(https://arxiv.org/abs/2402.07321)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates from different source tokens.</li>
<li><strong>摘要：</strong>基于 Transformer 的大语言模型 (LLM) 如何存储和检索知识？我们专注于该任务最基本的形式——事实回忆，其中模型的任务是在“事实：罗马斗兽场位于……的国家”形式的提示中明确显示存储的事实。我们发现事实回忆背后的机制故事比之前想象的更复杂。它由几种不同的、独立的、性质不同的机制组成，这些机制相加地组合起来，对正确的属性产生建设性的干扰。我们将这种普遍现象称为加性主题：模型通过总结多个独立贡献来进行计算。每种机制的贡献单独而言可能是不够的，但总结起来会对正确答案产生建设性的干扰。此外，我们扩展了直接 logit 归因的方法，将注意力头的输出归因于各个源标记。我们使用这种技术来解压所谓的“混合头”——它们本身就是来自不同源令牌的一对两个单独的附加更新。</li>
</ul>

<h3>Title: Assessing Generalization for Subpopulation Representative Modeling via  In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Simmons, Vladislav Savinov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07368">https://arxiv.org/abs/2402.07368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07368">https://arxiv.org/pdf/2402.07368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07368]] Assessing Generalization for Subpopulation Representative Modeling via  In-Context Learning(https://arxiv.org/abs/2402.07368)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.</li>
<li><strong>摘要：</strong>本研究利用 2016 年和 2020 年美国全国选举研究数据的上下文学习，评估了基于大语言模型 (LLM) 的子群体代表模型 (SRM) 从经验数据中进行归纳的能力。我们探索响应变量和人口亚组的概括。虽然用经验数据进行调节可以提高整体表现，但情境学习的好处在不同人群中差异很大，有时会损害某一人群的表现，同时有助于其他人群的表现。 SRM 的情境学习所带来的不公平收益给实施 SRM 的从业者以及可能依赖 SRM 的决策者带来了挑战。我们的工作强调需要从不同的子群体中获取细粒度的基准，这些基准不仅测试保真度，而且测试泛化性。</li>
</ul>

<h3>Title: Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy  Induction from Limited Examples</h3>
<ul>
<li><strong>Authors: </strong>Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07386">https://arxiv.org/abs/2402.07386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07386">https://arxiv.org/pdf/2402.07386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07386]] Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy  Induction from Limited Examples(https://arxiv.org/abs/2402.07386)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Automatic taxonomy induction is crucial for web search, recommendation systems, and question answering. Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable. In this work, we introduce Chain-of-Layer which is an in-context learning framework designed to induct taxonomies from a given set of entities. Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom. To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration. Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world benchmarks.</li>
<li><strong>摘要：</strong>自动分类归纳对于网络搜索、推荐系统和问题解答至关重要。人工分类法的人力成本很高，因此非常需要自动分类法构建。在这项工作中，我们引入了 Chain-of-Layer，它是一个上下文学习框架，旨在从给定的一组实体中归纳分类法。层链将任务分解为在每一层中选择相关候选实体，并从上到下逐步构建分类法。为了最大限度地减少错误，我们引入了基于集成的排名过滤器来减少每次迭代时生成的幻觉内容。通过大量的实验，我们证明了 Chain-of-Layer 在四个现实世界基准上实现了最先进的性能。</li>
</ul>

<h3>Title: VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language  Models with Autonomous Instruction Optimization</h3>
<ul>
<li><strong>Authors: </strong>Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng Wang, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07398">https://arxiv.org/abs/2402.07398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07398">https://arxiv.org/pdf/2402.07398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07398]] VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language  Models with Autonomous Instruction Optimization(https://arxiv.org/abs/2402.07398)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.</li>
<li><strong>摘要：</strong>本文提出了 VisLingInstruct，这是一种在零样本学习中推进多模态语言模型 (MMLM) 的新方法。目前的 MMLM 在多模态任务中显示出令人印象深刻的零样本能力，但它们的性能在很大程度上取决于指令的质量。 VisLingInstruct 通过情境学习自主评估和优化教学文本来解决这个问题，提高 MMLM 中视觉感知和语言表达之间的协同作用。除了这一教学进步之外，我们还优化了 MMLM 中的视觉特征提取模块，进一步增强了它们对文本提示的响应能力。我们基于 FlanT5 和 Vicuna 对 MMLM 进行的综合实验表明，VisLingInstruct 显着提高了视觉多模态任务中的零样本性能。值得注意的是，它在 TextVQA 和 HatefulMemes 数据集上的准确度比之前最先进的技术提高了 13.1% 和 9%。</li>
</ul>

<h3>Title: Can LLMs Produce Faithful Explanations For Fact-checking? Towards  Faithful Explainable Fact-Checking via Multi-Agent Debate</h3>
<ul>
<li><strong>Authors: </strong>Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07401">https://arxiv.org/abs/2402.07401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07401">https://arxiv.org/pdf/2402.07401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07401]] Can LLMs Produce Faithful Explanations For Fact-checking? Towards  Faithful Explainable Fact-Checking via Multi-Agent Debate(https://arxiv.org/abs/2402.07401)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Fact-checking research has extensively explored verification but less so the generation of natural-language explanations, crucial for user trust. While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence, advancing the credibility and trustworthiness of these explanations.</li>
<li><strong>摘要：</strong>事实核查研究广泛探索了验证，但较少探索对用户信任至关重要的自然语言解释的生成。虽然大型语言模型（LLM）在文本生成方面表现出色，但它们在事实检查中产生忠实解释的能力仍然没有得到充分检验。我们的研究调查了法学硕士产生此类解释的能力，发现零样本提示通常会导致不忠。为了应对这些挑战，我们提出了多代理辩论细化（MADR）框架，利用多个法学硕士作为在迭代细化过程中扮演不同角色的代理，旨在增强生成解释的忠实度。 MADR 确保最终解释经过严格验证，显着降低不忠实因素的可能性，并与所提供的证据紧密结合。实验结果表明，MADR 显着提高了 LLM 生成的证据解释的忠实度，提高了这些解释的可信度和可信度。</li>
</ul>

<h3>Title: Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic  Hierarchy Process and GPT-4 for Automated Decision Support</h3>
<ul>
<li><strong>Authors: </strong>Igor Svoboda, Dmytro Lande</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CR, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07404">https://arxiv.org/abs/2402.07404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07404">https://arxiv.org/pdf/2402.07404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07404]] Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic  Hierarchy Process and GPT-4 for Automated Decision Support(https://arxiv.org/abs/2402.07404)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Our study presents a new framework that incorporates the Analytic Hierarchy Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language model (LLM), bringing novel approaches to cybersecurity Multiple-criteria Decision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous agents as virtual experts, we automate the decision-making process, enhancing both efficiency and reliability. This new approach focuses on leveraging LLMs for sophisticated decision analysis, highlighting the synergy between traditional decision-making models and cutting-edge AI technologies. Our innovative methodology demonstrates significant advancements in using AI-driven agents for complex decision-making scenarios, highlighting the importance of AI in strategic cybersecurity applications. The findings reveal the transformative potential of combining AHP and LLMs, establishing a new paradigm for intelligent decision support systems in cybersecurity and beyond.</li>
<li><strong>摘要：</strong>我们的研究提出了一个新框架，其中结合了层次分析法 (AHP) 和生成式预训练 Transformer 4 (GPT-4) 大语言模型 (LLM)，为网络安全多标准决策 (MCDA) 带来了新方法。通过利用 GPT-4 自主代理作为虚拟专家的功能，我们实现了决策过程的自动化，从而提高了效率和可靠性。这种新方法侧重于利用法学硕士进行复杂的决策分析，强调传统决策模型与尖端人工智能技术之间的协同作用。我们的创新方法展示了使用人工智能驱动的代理进行复杂决策场景的重大进步，凸显了人工智能在战略网络安全应用中的重要性。研究结果揭示了层次分析法和法学硕士相结合的变革潜力，为网络安全及其他领域的智能决策支持系统建立了新的范式。</li>
</ul>

<h3>Title: Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs  Between Spanish and English</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira, Xiao-Yang Liu, Sophia Ananiadou, Min Peng, Jimin Huang, Qianqian Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07405">https://arxiv.org/abs/2402.07405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07405">https://arxiv.org/pdf/2402.07405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07405]] Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs  Between Spanish and English(https://arxiv.org/abs/2402.07405)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Despite Spanish's pivotal role in the global finance industry, a pronounced gap exists in Spanish financial natural language processing (NLP) and application studies compared to English, especially in the era of large language models (LLMs). To bridge this gap, we unveil Tois\'on de Oro, the first bilingual framework that establishes instruction datasets, finetuned LLMs, and evaluation benchmark for financial LLMs in Spanish joint with English. We construct a rigorously curated bilingual instruction dataset including over 144K Spanish and English samples from 15 datasets covering 7 tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual financial applications. We evaluate our model and existing LLMs using FLARE-ES, the first comprehensive bilingual evaluation benchmark with 21 datasets covering 9 tasks. The FLARE-ES benchmark results reveal a significant multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic instruction tuning and leveraging data from diverse linguistic resources, highlighting the positive impact of cross-linguistic transfer. All our datasets, models, and benchmarks have been released.</li>
<li><strong>摘要：</strong>尽管西班牙语在全球金融业中发挥着举足轻重的作用，但与英语相比，西班牙语金融自然语言处理（NLP）和应用研究仍存在明显差距，尤其是在大语言模型（LLM）时代。为了弥补这一差距，我们推出了 Tois\'on de Oro，这是第一个双语框架，它以西班牙语和英语联合建立了教学数据集、微调的法学硕士以及金融法学硕士的评估基准。我们构建了一个严格策划的双语教学数据集，其中包括来自涵盖 7 个任务的 15 个数据集的超过 144K 西班牙语和英语样本。利用这一点，我们推出了 FinMA-ES，这是专为双语金融应用而设计的法学硕士。我们使用 FLARE-ES 评估我们的模型和现有的法学硕士，FLARE-ES 是第一个全面的双语评估基准，拥有涵盖 9 个任务的 21 个数据集。 FLARE-ES 基准测试结果揭示了现有法学硕士存在显着的多语言表现差距和偏见。由于战略指令调整和利用来自不同语言资源的数据，FinMA-ES 模型在西班牙语金融任务中超越了 GPT-4 等 SOTA LLM，凸显了跨语言迁移的积极影响。我们所有的数据集、模型和基准均已发布。</li>
</ul>

<h3>Title: Potential-Based Reward Shaping For Intrinsic Motivation</h3>
<ul>
<li><strong>Authors: </strong>Grant C. Forbes, Nitish Gupta, Leonardo Villalobos-Arias, Colin M. Potts, Arnav Jhala, David L. Roberts</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07411">https://arxiv.org/abs/2402.07411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07411">https://arxiv.org/pdf/2402.07411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07411]] Potential-Based Reward Shaping For Intrinsic Motivation(https://arxiv.org/abs/2402.07411)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments. These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior. Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for. We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven. We also present {\em Potential-Based Intrinsic Motivation} (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff Walking environments, we demonstrate that PBIM successfully prevents the agent from converging to a suboptimal policy and can speed up training.</li>
<li><strong>摘要：</strong>最近，在复杂和稀疏奖励环境中学习的内在动机（IM）奖励塑造方法不断涌现。这些方法通常会无意中改变环境中的一组最优策略，从而导致次优行为。以前关于减轻奖励塑造风险的工作，特别是通过基于潜力的奖励塑造（PBRS），并不适用于许多 IM 方法，因为它们本身通常是复杂的、可训练的函数，因此依赖于更广泛的变量集。 PBRS 开发的传统奖励功能。我们提出了 PBRS 的扩展，我们证明它在比之前证明的更通用的函数集下保留了一组最优策略。我们还提出了基于潜力的内在动机（PBIM），一种将 IM 奖励转换为基于潜力的形式的方法，该形式可以在不改变最优策略集的情况下使用。在 MiniGrid DoorKey 和 Cliff Walking 环境中进行测试，我们证明 PBIM 成功防止代理收敛到次优策略，并可以加快训练速度。</li>
</ul>

<h3>Title: SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy  Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Sangwoo Shin, Minjong Yoo, Jeongwoo Lee, Honguk Woo</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07418">https://arxiv.org/abs/2402.07418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07418">https://arxiv.org/pdf/2402.07418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07418]] SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy  Adaptation(https://arxiv.org/abs/2402.07418)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains. In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain. The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation. During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts. Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by language prompting and contrastive learning-based context inferences. This hierarchical adaptation empowers the framework to not only infer a complex task specification in one-shot from the interleaved multi-modal snippets, but also adapt it to new domains with zero-shot learning abilities. We evaluate our framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The results clarify the framework's superiority in performing long-horizon tasks and adapting to different domains, showing its broad applicability in practical use cases, such as cognitive robots interpreting abstract instructions and autonomous vehicles operating under varied configurations.</li>
<li><strong>摘要：</strong>这项工作探索了跨域设置中语义技能、语义可解释专家的行为模式的零样本适应能力，其中交错的多模态片段中的用户输入可以提示针对不同域的新的长期任务。在这些跨域设置中，我们提出了一个语义技能翻译框架 SemTra，它利用一组多模态模型从片段中提取技能，并利用预训练语言模型的推理能力使这些提取的技能适应目标领域。该框架采用两级适应层次结构：任务适应和技能适应。在任务适应过程中，语言模型的序列到序列翻译将提取的技能转换为语义技能序列，该序列经过定制以适应跨域上下文。技能适应的重点是通过语言提示和基于对比学习的上下文推理促进的参数实例化来优化目标领域上下文的每个语义技能。这种分层适应使框架不仅能够从交错的多模态片段一次性推断出复杂的任务规范，而且还能够使其适应具有零样本学习能力的新领域。我们使用 Meta-World、Franka Kitchen、RLBench 和 CARLA 环境评估我们的框架。结果阐明了该框架在执行长期任务和适应不同领域方面的优越性，显示了其在实际用例中的广泛适用性，例如解释抽象指令的认知机器人和在不同配置下运行的自动驾驶车辆。</li>
</ul>

<h3>Title: Particle Filter SLAM for Vehicle Localization</h3>
<ul>
<li><strong>Authors: </strong>Tianrui Liu, Changxin Xu, Yuxin Qiao, Chufeng Jiang, Jiqiang Yu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07429">https://arxiv.org/abs/2402.07429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07429">https://arxiv.org/pdf/2402.07429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07429]] Particle Filter SLAM for Vehicle Localization(https://arxiv.org/abs/2402.07429)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Simultaneous Localization and Mapping (SLAM) presents a formidable challenge in robotics, involving the dynamic construction of a map while concurrently determining the precise location of the robotic agent within an unfamiliar environment. This intricate task is further compounded by the inherent "chicken-and-egg" dilemma, where accurate mapping relies on a dependable estimation of the robot's location, and vice versa. Moreover, the computational intensity of SLAM adds an additional layer of complexity, making it a crucial yet demanding topic in the field. In our research, we address the challenges of SLAM by adopting the Particle Filter SLAM method. Our approach leverages encoded data and fiber optic gyro (FOG) information to enable precise estimation of vehicle motion, while lidar technology contributes to environmental perception by providing detailed insights into surrounding obstacles. The integration of these data streams culminates in the establishment of a Particle Filter SLAM framework, representing a key endeavor in this paper to effectively navigate and overcome the complexities associated with simultaneous localization and mapping in robotic systems.</li>
<li><strong>摘要：</strong>同步定位与建图（SLAM）对机器人技术提出了巨大的挑战，涉及动态构建地图，同时确定机器人代理在不熟悉的环境中的精确位置。这项复杂的任务因固有的“先有鸡还是先有蛋”的困境而变得更加复杂，其中准确的映射依赖于对机器人位置的可靠估计，反之亦然。此外，SLAM 的计算强度增加了额外的复杂性，使其成为该领域至关重要但要求很高的主题。在我们的研究中，我们通过采用粒子过滤 SLAM 方法来解决 SLAM 的挑战。我们的方法利用编码数据和光纤陀螺仪 (FOG) 信息来精确估计车辆运动，而激光雷达技术通过提供对周围障碍物的详细洞察来促进环境感知。这些数据流的集成最终建立了粒子过滤器 SLAM 框架，这代表了本文的一项关键工作，即有效导航和克服与机器人系统中同时定位和建图相关的复杂性。</li>
</ul>

<h3>Title: SALAD: Smart AI Language Assistant Daily</h3>
<ul>
<li><strong>Authors: </strong>Ragib Amin Nihal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07431">https://arxiv.org/abs/2402.07431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07431">https://arxiv.org/pdf/2402.07431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07431]] SALAD: Smart AI Language Assistant Daily(https://arxiv.org/abs/2402.07431)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>SALAD is an AI-driven language-learning application designed to help foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech recognition, translated audio, vocabulary tracking, grammar explanations, and songs generated from newly learned words. The app targets beginners and intermediate learners, aiming to make language acquisition more accessible and enjoyable. SALAD uses daily translations to enhance fluency and comfort in communication with native speakers. The primary objectives include effective Japanese language learning, user engagement, and progress tracking. A survey by us found that 39% of foreigners in Japan face discomfort in conversations with Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's ability to enhance their Japanese language skills. The app uses large language models, speech recognition, and diffusion models to bridge the language gap and foster a more inclusive community in Japan.</li>
<li><strong>摘要：</strong>SALAD是一款人工智能驱动的语言学习应用程序，旨在帮助外国人学习日语。它提供汉字-假名-罗马字翻译、语音识别、翻译音频、词汇跟踪、语法解释以及根据新学单词生成的歌曲。该应用程序针对初学者和中级学习者，旨在让语言学习变得更加容易和愉快。 SALAD 使用日常翻译来提高与母语人士交流的流畅度和舒适度。主要目标包括有效的日语学习、用户参与和进度跟踪。我们的一项调查发现，39%的在日外国人在与说日语的人交谈时感到不舒服。超过60%的外国人对SALAD提高日语能力的能力充满信心。该应用程序使用大型语言模型、语音识别和扩散模型来弥合语言差距并在日本培育更具包容性的社区。</li>
</ul>

<h3>Title: Game Agent Driven by Free-Form Text Command: Using LLM-based Code  Generation and Behavior Branch</h3>
<ul>
<li><strong>Authors: </strong>Ray Ito, Junichiro Takahashi</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07442">https://arxiv.org/abs/2402.07442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07442">https://arxiv.org/pdf/2402.07442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07442]] Game Agent Driven by Free-Form Text Command: Using LLM-based Code  Generation and Behavior Branch(https://arxiv.org/abs/2402.07442)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Several attempts have been made to implement text command control for game agents. However, current technologies are limited to processing predefined format commands. This paper proposes a pioneering text command control system for a game agent that can understand natural language commands expressed in free-form. The proposed system uses a large language model (LLM) for code generation to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent. This study conducted empirical validation within a game environment that simulates a Pok\'emon game and involved multiple participants. The results confirmed the system's ability to understand and carry out natural language commands, representing a noteworthy in the realm of real-time language interactive game agents. Notice for the use of this material. The copyright of this material is retained by the Japanese Society for Artificial Intelligence (JSAI). This material is published here with the agreement of JSAI. Please be complied with Copyright Law of Japan if any users wish to reproduce, make derivative work, distribute or make available to the public any part or whole thereof. All Rights Reserved, Copyright (C) The Japanese Society for Artificial Intelligence.</li>
<li><strong>摘要：</strong>已经进行了多种尝试来实现游戏代理的文本命令控制。然而，当前的技术仅限于处理预定义的格式命令。本文提出了一种用于游戏代理的开创性文本命令控制系统，该系统可以理解以自由形式表达的自然语言命令。该系统使用大型语言模型（LLM）进行代码生成，将自然语言命令解释并转换为行为分支，这是一种基于行为树的知识表达，有利于游戏代理的执行。本研究在模拟神奇宝贝游戏并涉及多个参与者的游戏环境中进行了实证验证。结果证实了该系统具有理解和执行自然​​语言命令的能力，在实时语言交互游戏代理领域代表着值得注意的成果。使用本材料的注意事项。本材料的版权由日本人工智能学会 (JSAI) 保留。本材料经 JSAI 同意在此发布。如果任何用户希望复制、制作衍生作品、分发或向公众提供其任何部分或全部内容，请遵守日本版权法。保留所有权利，版权所有 (C) 日本人工智能学会。</li>
</ul>

<h3>Title: AraSpider: Democratizing Arabic-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Heakl, Youssef Mohamed, Ahmed B. Zaky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07448">https://arxiv.org/abs/2402.07448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07448">https://arxiv.org/pdf/2402.07448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07448]] AraSpider: Democratizing Arabic-to-SQL(https://arxiv.org/abs/2402.07448)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting transparency and collaborative knowledge sharing in the field. Overall, these contributions advance NLP research, empower Arabic-speaking researchers, and enrich the global discourse on language comprehension and database interrogation.</li>
<li><strong>摘要：</strong>这项研究提出了 AraSpider，这是 Spider 数据集的第一个阿拉伯语版本，旨在改进阿拉伯语社区的自然语言处理 (NLP)。测试了四种多语言翻译模型在将英语翻译为阿拉伯语方面的有效性。此外，还评估了两个模型从阿拉伯文本生成 SQL 查询的能力。结果表明，使用反向翻译显着提高了 ChatGPT 3.5 和 SQLCoder 模型的性能，这些模型被认为是 Spider 数据集上表现最好的模型。值得注意的是，ChatGPT 3.5 展示了高质量的翻译，而 SQLCoder 在文本到 SQL 任务方面表现出色。该研究强调了结合上下文模式和采用反向翻译策略来增强阿拉伯语 NLP 任务中模型性能的重要性。此外，提供详细的数据集再现性和翻译成其他语言的方法凸显了该研究致力于促进该领域的透明度和协作知识共享。总体而言，这些贡献推进了 NLP 研究，增强了阿拉伯语研究人员的能力，并丰富了关于语言理解和数据库查询的全球讨论。</li>
</ul>

<h3>Title: OS-Copilot: Towards Generalist Computer Agents with Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07456">https://arxiv.org/abs/2402.07456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07456">https://arxiv.org/pdf/2402.07456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07456]] OS-Copilot: Towards Generalist Computer Agents with Self-Improvement(https://arxiv.org/abs/2402.07456)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents.</li>
<li><strong>摘要：</strong>与计算机的自主交互一直是一个长期存在的挑战，但潜力巨大，最近大型语言模型（LLM）的激增显着加速了构建数字代理的进展。然而，大多数这些代理都被设计为与狭窄的域交互，例如特定的软件或网站。这种狭隘的关注限制了它们对一般计算机任务的适用性。为此，我们引入了 OS-Copilot，这是一个构建通用代理的框架，能够与操作系统 (OS) 中的综合元素（包括 Web、代码终端、文件、多媒体和各种第三方应用程序）进行交互。我们使用 OS-Copilot 创建 FRIDAY，这是一个自我改进的实体代理，用于自动化一般计算机任务。在通用人工智能助手基准 GAIA 上，FRIDAY 的性能比之前的方法高出 35%，通过之前任务中积累的技能，展示了对未见过的应用程序的强大泛化能力。我们还提供了数字和定量证据，证明 FRIDAY 在最少的监督下学会了使用 Excel 和 Powerpoint 进行控制和自我改进。我们的 OS-Copilot 框架和实证研究结果为未来研究更强大、更通用的计算机代理提供了基础设施和见解。</li>
</ul>

<h3>Title: Pushing The Limit of LLM Capacity for Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari, Benyou Wang, Jing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07470">https://arxiv.org/abs/2402.07470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07470">https://arxiv.org/pdf/2402.07470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07470]] Pushing The Limit of LLM Capacity for Text Classification(https://arxiv.org/abs/2402.07470)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average. Further evaluation experiments show a clear surpassing of RGPT over human classification.</li>
<li><strong>摘要：</strong>由于大型语言模型 (LLM) 在众多下游 NLP 任务中表现出的非凡功效，文本分类未来研究的价值遇到了挑战和不确定性。在这个开放式语言建模的时代，任务界限逐渐消失，一个紧迫的问题出现了：在法学硕士的充分帮助下，我们在文本分类方面是否取得了重大进展？为了回答这个问题，我们提出了 RGPT，这是一种自适应增强框架，旨在通过循环集成一组强基础学习器来生成专门的文本分类 LLM。基础学习器是通过自适应调整训练样本的分布并用它们迭代微调 LLM 来构建的。然后，通过反复合并先前学习者的历史预测，将此类基础学习者整合为专门的文本分类法学硕士。通过全面的实证比较，我们发现 RGPT 在四个基准上明显优于 8 个 SOTA PLM 和 7 个 SOTA LLM，平均高出 1.36%。进一步的评估实验表明 RGPT 明显超越人类分类。</li>
</ul>

<h3>Title: Food Recommendation as Language Processing (F-RLP): A Personalized and  Contextual Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Ali Rostami, Ramesh Jain, Amir M. Rahmani</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07477">https://arxiv.org/abs/2402.07477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07477">https://arxiv.org/pdf/2402.07477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07477]] Food Recommendation as Language Processing (F-RLP): A Personalized and  Contextual Paradigm(https://arxiv.org/abs/2402.07477)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>State-of-the-art rule-based and classification-based food recommendation systems face significant challenges in becoming practical and useful. This difficulty arises primarily because most machine learning models struggle with problems characterized by an almost infinite number of classes and a limited number of samples within an unbalanced dataset. Conversely, the emergence of Large Language Models (LLMs) as recommendation engines offers a promising avenue. However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations. To address this gap, we introduce Food Recommendation as Language Processing (F-RLP), a novel framework that offers a food-specific, tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize their potential, thereby paving the way for more accurate, personalized food recommendations.</li>
<li><strong>摘要：</strong>最先进的基于规则和分类的食物推荐系统在变得实用和有用方面面临着重大挑战。出现这种困难的主要原因是，大多数机器学习模型都在努力解决不平衡数据集中几乎无限数量的类和有限数量的样本的问题。相反，大型语言模型（LLM）作为推荐引擎的出现提供了一条有前途的途径。然而，通用推荐作为语言处理（RLP）方法缺乏有效食品推荐所需的关键组成部分。为了解决这一差距，我们引入了食品推荐作为语言处理（F-RLP），这是一个提供针对食品的定制基础设施的新颖框架。 F-RLP 利用法学硕士的能力来最大限度地发挥其潜力，从而为更准确、个性化的食品推荐铺平道路。</li>
</ul>

<h3>Title: T-RAG: Lessons from the LLM Trenches</h3>
<ul>
<li><strong>Authors: </strong>Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07483">https://arxiv.org/abs/2402.07483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07483">https://arxiv.org/pdf/2402.07483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07483]] T-RAG: Lessons from the LLM Trenches(https://arxiv.org/abs/2402.07483)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出卓越的语言功能，推动了将其集成到广泛领域的应用程序中的尝试。一个重要的应用领域是针对私营企业文档的问答，其中主要考虑因素是数据安全，这需要可以在本地部署的应用程序、有限的计算资源以及需要正确响应查询的强大应用程序。检索增强生成（RAG）已成为构建基于 LLM 的应用程序的最著名的框架。虽然构建 RAG 相对简单，但要使其成为强大且可靠的应用程序，需要广泛的定制和对应用程序领域相对深入的了解。我们分享构建和部署法学硕士应用程序以通过私人组织文档进行问答的经验。我们的应用程序将 RAG 的使用与经过微调的开源 LLM 结合起来。此外，我们的系统称为 Tree-RAG (T-RAG)，使用树结构来表示组织内的实体层次结构。这用于生成文本描述，以在响应与组织层次结构内的实体有关的用户查询时增强上下文。我们的评估表明，这种组合比简单的 RAG 或微调实现表现更好。最后，我们根据构建供实际使用的 LLM 应用程序的经验分享一些经验教训。</li>
</ul>

<h3>Title: Secret Collusion Among Generative AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H.S. Torr, Lewis Hammond, Christian Schroeder de Witt</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07510">https://arxiv.org/abs/2402.07510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07510">https://arxiv.org/pdf/2402.07510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07510]] Secret Collusion Among Generative AI Agents(https://arxiv.org/abs/2402.07510)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.</li>
<li><strong>摘要：</strong>最近大型语言模型 (LLM) 的功能增强开辟了应用程序，在这些应用程序中，通信生成式 AI 代理团队可以解决联合任务。这带来了涉及未经​​授权的信息共享或其他不需要的代理协调形式的隐私和安全挑战。现代隐写技术可能会使这种动态难以检测。在本文中，我们通过借鉴人工智能和安全文献中的相关概念，全面地形式化了生成人工智能代理系统中的秘密共谋问题。我们研究了使用隐写术的激励措施，并提出了各种缓解措施。我们的调查产生了一个模型评估框架，可以系统地测试各种形式的秘密勾结所需的能力。我们提供了一系列当代法学硕士的广泛实证结果。虽然当前模型的隐写能力仍然有限，但 GPT-4 显示出能力的跳跃，表明需要持续监控隐写前沿模型的能力。最后，我们制定了一项全面的研究计划，以减轻未来生成人工智能模型之间串通的风险。</li>
</ul>

<h3>Title: MAFIA: Multi-Adapter Fused Inclusive LanguAge Models</h3>
<ul>
<li><strong>Authors: </strong>Prachi Jain, Ashutosh Sathe, Varun Gumma, Kabir Ahuja, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07519">https://arxiv.org/abs/2402.07519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07519">https://arxiv.org/pdf/2402.07519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07519]] MAFIA: Multi-Adapter Fused Inclusive LanguAge Models(https://arxiv.org/abs/2402.07519)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 广泛用于 NLP 中的各种任务。最近的研究已经确定了此类模型表现出的各种偏差，并提出了纠正这些偏差的方法。然而，大多数作品都独立地解决了一组有限的偏见维度，例如性别、种族或宗教。此外，这些方法通常涉及微调整个模型以维持下游任务的性能。在这项工作中，我们的目标是跨多个维度模块化地消除预训练语言模型的偏差。之前的工作广泛探索了使用有限的以美国为中心的反事实数据增强 (CDA) 来消除 PLM 的偏差。我们使用结构化知识和大型生成模型以半自动化的方式构建跨多个偏差维度的多样化 CDA。我们强调现有的去偏见方法如何不考虑多种社会偏见之间的相互作用，并提出一种去偏见模型，该模型利用各种社会偏见之间的协同作用并同时实现多偏见去偏见。对多种任务和语言的广泛评估证明了我们方法的有效性。</li>
</ul>

<h3>Title: BreakGPT: A Large Language Model with Multi-stage Structure for  Financial Breakout Detection</h3>
<ul>
<li><strong>Authors: </strong>Kang Zhang, Osamu Yoshie, Weiran Huang</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07536">https://arxiv.org/abs/2402.07536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07536">https://arxiv.org/pdf/2402.07536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07536]] BreakGPT: A Large Language Model with Multi-stage Structure for  Financial Breakout Detection(https://arxiv.org/abs/2402.07536)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the multi-stage structure contributing 17.6% to the improvement. Additionally, it outperforms ChatGPT-4 by 42.07%. Our Code is publicly available: https://github.com/Neviim96/BreakGPT</li>
<li><strong>摘要：</strong>交易区间突破（TRB）是金融交易技术分析中的关键方法，被股票、期货、外汇等金融市场的交易者广泛采用。然而，区分真假突破并提供正确的理由给投资者带来了重大挑战。近年来，大型语言模型在各种下游应用中取得了成功，但在金融突破检测领域的有效性却不佳。原因是突破检测需要独特的数据和特定的知识。为了解决这些问题，我们引入了 BreakGPT，这是第一个用于金融突破检测的大型语言模型。此外，我们还开发了一种新颖的大型语言模型框架，即多阶段结构，有效减少下游应用中的错误。实验结果表明，与GPT-3.5相比，BreakGPT将答案和理性的准确性提高了44%，其中多阶段结构的提高贡献了17.6%。此外，它的性能比 ChatGPT-4 高出 42.07%。我们的代码是公开的：https://github.com/Neviim96/BreakGPT</li>
</ul>

<h3>Title: Show Me How It's Done: The Role of Explanations in Fine-Tuning Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Ballout, Ulf Krumnack, Gunther Heidemann, Kai-Uwe Kuehnberger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07543">https://arxiv.org/abs/2402.07543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07543">https://arxiv.org/pdf/2402.07543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07543]] Show Me How It's Done: The Role of Explanations in Fine-Tuning Language  Models(https://arxiv.org/abs/2402.07543)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models.</li>
<li><strong>摘要：</strong>我们的研究证明了使用微调和解释来增强语言模型性能的显着好处。与维护模型参数的提示不同，微调允许模型在训练阶段学习和更新其参数。在这项研究中，我们使用包含输出解释而不是仅仅呈现答案的数据对各种大小的语言模型进行微调。我们发现，即使参数少至 6000 万个的较小语言模型也能从这种方法中受益匪浅。有趣的是，我们的结果表明，详细的解释对较小的模型比较大的模型更有利，后者从任何形式的解释中获得几乎相同的优势，无论其长度如何。此外，我们证明，包含解释使模型能够解决没有解释就无法解决的任务。最后，我们认为，尽管添加解释具有挑战性，但包含解释的样本不仅减少了训练所需的数据量，而且还促进了模型更有效的泛化。从本质上讲，我们的研究结果表明，通过解释进行微调可以显着提高大型语言模型的性能。</li>
</ul>

<h3>Title: Unveiling Group-Specific Distributed Concept Drift: A Fairness  Imperative in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Teresa Salazar, João Gama, Helder Araújo, Pedro Henriques Abreu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07586">https://arxiv.org/abs/2402.07586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07586">https://arxiv.org/pdf/2402.07586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07586]] Unveiling Group-Specific Distributed Concept Drift: A Fairness  Imperative in Federated Learning(https://arxiv.org/abs/2402.07586)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness. In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time. The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning.</li>
<li><strong>摘要：</strong>在不断发展的机器学习领域，确保公平性已成为一个关键问题，促进了旨在减轻决策过程中歧视性结果的算法的开发。然而，在存在特定群体概念漂移的情况下实现公平仍然是一个尚未探索的前沿，我们的研究代表了这方面的开创性努力。特定群体的概念漂移是指随着时间的推移，一个群体经历概念漂移而另一群体则没有的情况，即使准确性保持相当稳定，也会导致公平性下降。在联邦学习的框架内，客户协作训练模型，其分布式特性进一步放大了这些挑战，因为每个客户都可以独立体验特定于群体的概念漂移，同时仍然共享相同的底层概念，从而创建一个复杂且动态的环境来维护公平性。我们研究的重要贡献之一是形式化并引入了特定群体概念漂移及其分布式对应问题的问题，揭示了其在公平领域的至关重要性。此外，利用先前研究的见解，我们采用现有的分布式概念漂移自适应算法来解决特定于组的分布式概念漂移，该算法利用多模型方法、本地特定于组的漂移检测机制以及随着时间的推移对模型进行连续聚类。我们的实验结果强调了解决特定群体的概念漂移及其分布式对应物对于促进机器学习公平性的重要性。</li>
</ul>

<h3>Title: Near-Minimax-Optimal Distributional Reinforcement Learning with a  Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Mark Rowland, Li Kevin Wenliang, Rémi Munos, Clare Lyle, Yunhao Tang, Will Dabney</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07598">https://arxiv.org/abs/2402.07598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07598">https://arxiv.org/pdf/2402.07598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07598]] Near-Minimax-Optimal Distributional Reinforcement Learning with a  Generative Model(https://arxiv.org/abs/2402.07598)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.</li>
<li><strong>摘要：</strong>我们提出了一种基于模型的分布式强化学习（RL）的新算法，并证明它对于使用生成模型（最多对数因子）近似回报分布是极小极大最优，解决了Zhang等人的悬而未决的问题。 （2023）。我们的分析提供了关于分布强化学习的分类方法的新理论结果，并且还引入了一个新的分布贝尔曼方程，即随机分类 CDF 贝尔曼方程，我们希望该方程具有独立的意义。我们还提供了一项实验研究，比较了几种基于模型的分布式强化学习算法，并为实践者提供了一些启示。</li>
</ul>

<h3>Title: Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07610">https://arxiv.org/abs/2402.07610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07610">https://arxiv.org/pdf/2402.07610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07610]] Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping(https://arxiv.org/abs/2402.07610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance. Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance.</li>
<li><strong>摘要：</strong>自对齐是降低人工注释成本同时确保模型能力的有效方法。然而，当前的大多数方法在单轮中完成数据收集和训练步骤，这可能忽略了自对齐模型不断提高的能力。这就产生了一个关键问题：如果我们进行多次引导自对准会怎么样？该策略是否会提高模型性能或导致模型性能快速下降？在本文中，我们的开创性探索深入研究了引导自对齐对大型语言模型的影响。我们的研究结果表明，通过保证上下文学习中的数据多样性，引导自对准明显优于单轮方法。为了进一步利用引导的功能，我们研究并调整数据的训练顺序，从而提高模型的性能。根据这些发现，我们提出了 Step-On-Feet Tuning (SOFT)，它利用模型不断增强的小样本能力来提高零样本或单样本性能。基于从易到难的训练方法，我们提出了 SOFT+，进一步提高了自对准的性能。我们的实验证明了 SOFT (SOFT+) 在各种分类和生成任务中的效率，突出了引导自对准在不断增强模型对准性能方面的潜力。</li>
</ul>

<h3>Title: Anchor-based Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07616">https://arxiv.org/abs/2402.07616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07616">https://arxiv.org/pdf/2402.07616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07616]] Anchor-based Large Language Models(https://arxiv.org/abs/2402.07616)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM significantly improves computational efficiency and resource utilization, demonstrating the potential of the anchor-based attention approach in the context of LLMs for real-time inference in practical applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 主要采用仅解码器的转换器架构，需要保留历史标记的键/值信息，以提供上下文信息并避免冗余计算。然而，这些 LLM 的庞大尺寸和参数量需要大量 GPU 内存。这种内存需求随着输入文本的长度而增加，从而迫切需要更有效的信息存储和处理方法。本研究介绍了基于锚点的LLM（AnLLM），它利用了创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略。这种方法使 LLM 能够将序列信息压缩到锚标记中，从而减少键/值缓存并提高推理效率。实验表明，AnLLM 保持了相当的准确性，键/值缓存减少了高达 99%，推理速度提高了 3.5 倍。尽管准确性略有下降，但 AnLLM 显着提高了计算效率和资源利用率，证明了基于锚的注意力方法在 LLM 背景下在实际应用中进行实时推理的潜力。</li>
</ul>

<h3>Title: AutoMathText: Autonomous Data Selection with Language Models for  Mathematical Texts</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07625">https://arxiv.org/abs/2402.07625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07625">https://arxiv.org/pdf/2402.07625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07625]] AutoMathText: Autonomous Data Selection with Language Models for  Mathematical Texts(https://arxiv.org/abs/2402.07625)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing models' mathematical reasoning capabilities. The AutoMathText dataset is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code is available at https://github.com/yifanzhang-pro/AutoMathText.</li>
<li><strong>摘要：</strong>为了通过持续预训练提高语言模型在数学推理方面的熟练程度，我们引入了一种利用基础语言模型进行自主数据选择的新颖策略。与传统的监督微调或带有人工注释数据的训练分类器不同，我们的方法利用元提示语言模型作为零样本验证器来自主评估和选择高质量的数学内容，并且我们发布了精选的开源 AutoMathText 数据集包含超过 200GB 的数据。为了证明我们方法的有效性，我们在 AutoMathText 数据集上持续预训练了 7B 参数 Mistral 语言模型，与之前的连续预训练工作相比，在 MATH 数据集上实现了下游性能的显着改进，并且令牌数量减少了几个数量级。与基线相比，我们的方法展示了预训练标记效率提高了 2 倍，强调了我们的方法在增强模型数学推理能力方面的潜力。 AutoMathText 数据集可从 https://huggingface.co/datasets/math-ai/AutoMathText 获取。代码可在 https://github.com/yifanzhang-pro/AutoMathText 获取。</li>
</ul>

<h3>Title: G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07630">https://arxiv.org/abs/2402.07630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07630">https://arxiv.org/pdf/2402.07630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07630]] G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering(https://arxiv.org/abs/2402.07630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and resists hallucination. (Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.)</li>
<li><strong>摘要：</strong>给定一个具有文本属性的图表，我们使用户能够“与他们的图表聊天”：即使用对话界面询问有关图表的问题。为了回答用户的问题，我们的方法提供文本回复并突出显示图表的相关部分。虽然现有的工作以各种方式集成了大型语言模型（LLM）和图神经网络（GNN），但它们主要关注传统的图任务（例如节点、边缘和图分类），或者回答小型或小型的简单图查询。合成图。相比之下，我们开发了一个针对现实世界文本图的灵活问答框架，适用于场景图理解、常识推理和知识图推理等多种应用。为了实现这一目标，我们首先使用从不同任务收集的数据开发图形问答（GraphQA）基准。然后，我们提出了 G-Retriever 方法，它集成了 GNN、LLM 和检索增强生成（RAG）的优点，并且可以通过软提示进行微调以增强图形理解。为了抵制幻觉并允许文本图形大大超过 LLM 的上下文窗口大小，G-Retriever 通过将此任务表述为收集奖品斯坦纳树优化问题来对图形执行 RAG。实证评估表明，我们的方法优于来自多个领域的文本图形任务的基线，可以很好地适应更大的图形尺寸，并且可以抵抗幻觉。 （我们的代码和数据集可在以下网址获取：https://github.com/XiaoxinHe/G-Retriever。）</li>
</ul>

<h3>Title: Detecting the Clinical Features of Difficult-to-Treat Depression using  Synthetic Data from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Isabelle Lorge, Dan W. Joyce, Niall Taylor, Alejo Nevado-Holgado, Andrea Cipriani, Andrey Kormilitzin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07645">https://arxiv.org/abs/2402.07645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07645">https://arxiv.org/pdf/2402.07645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07645]] Detecting the Clinical Features of Difficult-to-Treat Depression using  Synthetic Data from Large Language Models(https://arxiv.org/abs/2402.07645)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person's depressive disorder where despite treatment, they continue to experience significant burden. We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model. The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome). We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data. Our results show promise for future healthcare applications especially in applications where traditionally, highly confidential medical data and human-expert annotation would normally be required.</li>
<li><strong>摘要：</strong>难治性抑郁症（DTD）被认为是对抑郁症的更广泛、更全面的临床视角，尽管接受了治疗，但他们仍然承受着巨大的负担。我们试图开发一种基于大语言模型 (LLM) 的工具，能够查询常规收集的叙述性（自由文本）电子健康记录 (EHR) 数据，以定位已发布的、捕获 DTD 临床综合征的预后因素。在这项工作中，我们使用 LLM 生成的合成数据 (GPT3.5) 和非极大值抑制 (NMS) 算法来训练基于 BERT 的跨度提取模型。然后，生成的模型能够提取并标记与真实临床数据中各种相关的积极和消极因素相关的跨度（即增加或减少患者匹配 DTD 综合征的可能性的文本跨度）。我们证明，在一组多达 20 个不同因素的真实临床数据上可以获得良好的整体性能（跨极性 0.70 F1），并且在重要 DTD 因素的子集上获得高性能（0.85 F1，0.95 精度），例如通过仅根据合成数据训练模型来了解虐待史、情感障碍家族史、疾病严重程度和自杀倾向。我们的结果显示了未来医疗保健应用的前景，特别是在传统上通常需要高度机密的医疗数据和人类专家注释的应用中。</li>
</ul>

<h3>Title: The Sound of Healthcare: Improving Medical Transcription ASR Accuracy  with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ayo Adedeji, Sarita Joshi, Brendan Doohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07658">https://arxiv.org/abs/2402.07658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07658">https://arxiv.org/pdf/2402.07658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07658]] The Sound of Healthcare: Improving Medical Transcription ASR Accuracy  with Large Language Models(https://arxiv.org/abs/2402.07658)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of medical documentation, transcribing clinical dialogues accurately is increasingly paramount. This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing the PriMock57 dataset, which encompasses a diverse range of primary care consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our research is multifaceted, focusing on improvements in general Word Error Rate (WER), Medical Concept WER (MC-WER) for the accurate transcription of essential medical terms, and speaker diarization accuracy. Additionally, we assess the role of LLM post-processing in improving semantic textual similarity, thereby preserving the contextual integrity of clinical dialogues. Through a series of experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT) prompting techniques in enhancing diarization and correction accuracy. Our findings demonstrate that LLMs, particularly through CoT prompting, not only improve the diarization accuracy of existing ASR systems but also achieve state-of-the-art performance in this domain. This improvement extends to more accurately capturing medical concepts and enhancing the overall semantic coherence of the transcribed dialogues. These findings illustrate the dual role of LLMs in augmenting ASR outputs and independently excelling in transcription tasks, holding significant promise for transforming medical ASR systems and leading to more accurate and reliable patient records in healthcare settings.</li>
<li><strong>摘要：</strong>在快速发展的医疗文档领域，准确记录临床对话变得越来越重要。本研究探讨了大型语言模型 (LLM) 在提高医学转录中自动语音识别 (ASR) 系统准确性方面的潜力。利用包含各种初级保健咨询的 PriMock57 数据集，我们应用高级法学硕士来完善 ASR 生成的成绩单。我们的研究是多方面的，重点是改善一般单词错误率 (WER)、用于准确转录基本医学术语的医学概念 WER (MC-WER) 以及说话人分类准确性。此外，我们评估了法学硕士后处理在提高语义文本相似性方面的作用，从而保持临床对话的上下文完整性。通过一系列实验，我们比较了零样本和思想链（CoT）提示技术在提高二值化和校正精度方面的效果。我们的研究结果表明，法学硕士，特别是通过 CoT 提示，不仅提高了现有 ASR 系统的分类准确性，而且还实现了该领域最先进的性能。这一改进延伸到更准确地捕捉医学概念并增强转录对话的整体语义连贯性。这些发现说明了法学硕士在增强 ASR 输出和独立完成转录任务方面的双重作用，为医疗 ASR 系统的转型以及在医疗保健环境中提供更准确、更可靠的患者记录带来了重大希望。</li>
</ul>

<h3>Title: Large Language Models "Ad Referendum": How Good Are They at Machine  Translation in the Legal Domain?</h3>
<ul>
<li><strong>Authors: </strong>Vicent Briva-Iglesias, Joao Lucas Cavalheiro Camargo, Gokhan Dogru</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07681">https://arxiv.org/abs/2402.07681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07681">https://arxiv.org/pdf/2402.07681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07681]] Large Language Models "Ad Referendum": How Good Are They at Machine  Translation in the Legal Domain?(https://arxiv.org/abs/2402.07681)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.</li>
<li><strong>摘要：</strong>本研究针对法律领域四种语言对的传统神经机器翻译 (NMT) 系统，评估了两种最先进的大型语言模型 (LLM) 的机器翻译 (MT) 质量。它结合了自动评估指标（AEM）和专业译者的人工评估（HE）来评估翻译排名、流畅性和充分性。结果表明，虽然谷歌翻译在 AEM 方面普遍优于 LLM，但人类评估者对 LLM（尤其是 GPT-4）的评价在生成上下文充分且流畅的翻译方面相当或略好。这种差异表明法学硕士在处理专业法律术语和背景方面的潜力，凸显了人工评估方法在评估机器翻译质量方面的重要性。该研究强调了法学硕士在专业领域不断发展的能力，并呼吁重新评估传统的 AEM，以更好地捕捉法学硕士生成的翻译的细微差别。</li>
</ul>

<h3>Title: CyberMetric: A Benchmark Dataset for Evaluating Large Language Models  Knowledge in Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Merouane Debbah</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07688">https://arxiv.org/abs/2402.07688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07688">https://arxiv.org/pdf/2402.07688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07688]] CyberMetric: A Benchmark Dataset for Evaluating Large Language Models  Knowledge in Cybersecurity(https://arxiv.org/abs/2402.07688)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics. However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts. In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain. The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity. To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants of diverse expertise levels, facilitating a comprehensive comparison between human and machine intelligence in this area. The findings revealed that LLMs outperformed humans in almost every aspect of cybersecurity.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在从计算机视觉到医疗诊断等各个领域都表现出色。然而，了解网络安全的多样化前景，包括密码学、逆向工程和风险评估等管理方面，即使对于人类专家来说也是一个挑战。在本文中，我们介绍了 Cyber​​Metric，这是一个基准数据集，包含来自网络安全领域的标准、认证、研究论文、书籍和其他出版物的 10,000 个问题。这些问题是通过协作过程创建的，即将专业知识与法学硕士（包括 GPT-3.5 和 Falcon-180B）相结合。人类专家花费了 200 多个小时来验证其准确性和相关性。除了评估法学硕士的知识之外，该数据集的主要目标是促进人类与不同法学硕士在网络安全方面的公平比较。为了实现这一目标，我们精心挑选了 80 个问题，涵盖网络安全领域的广泛主题，并吸引了 30 名不同专业水平的参与者，以促进该领域的人类智能和机器智能的全面比较。研究结果显示，法学硕士在网络安全的几乎每个方面都优于人类。</li>
</ul>

<h3>Title: Model Collapse Demystified: The Case of Regression</h3>
<ul>
<li><strong>Authors: </strong>Elvis Dohmatob, Yunzhen Feng, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07712">https://arxiv.org/abs/2402.07712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07712">https://arxiv.org/pdf/2402.07712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07712]] Model Collapse Demystified: The Case of Regression(https://arxiv.org/abs/2402.07712)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.</li>
<li><strong>摘要：</strong>在像ChatGPT这样的大型语言模型时代，“模型崩溃”现象是指随着时间的推移，模​​型在前几代生成的数据上递归地进行训练，其性能下降，直至模型最终变得完全无用，即模型崩溃了。在这项工作中，我们在核回归的简化设置中研究了这种现象，并获得了结果，这些结果显示模型可以处理虚假数据的情况与模型性能完全崩溃的情况之间存在明显的交叉。在多项式衰减光谱和源条件下，我们获得了修改后的缩放定律，该定律表现出从快速率到慢速率的新交叉现象。我们还提出了一种基于自适应正则化的简单策略来减轻模型崩溃。我们的理论结果通过实验得到了验证。</li>
</ul>

<h3>Title: Asking Multimodal Clarifying Questions in Mixed-Initiative  Conversational Search</h3>
<ul>
<li><strong>Authors: </strong>Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07742">https://arxiv.org/abs/2402.07742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07742">https://arxiv.org/pdf/2402.07742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07742]] Asking Multimodal Clarifying Questions in Mixed-Initiative  Conversational Search(https://arxiv.org/abs/2402.07742)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.</li>
<li><strong>摘要：</strong>在混合主动对话搜索系统中，澄清问题用于帮助难以在单个查询中表达意图的用户。这些问题旨在揭示用户的信息需求并解决查询歧义。我们假设在多模式信息相关的场景中，可以通过使用非文本信息来改进澄清过程。因此，我们建议添加图像来澄清问题，并制定在开放域、混合主动对话搜索系统中提出多模态澄清问题的新任务。为了促进对此任务的研究，我们收集了一个名为 Melon 的数据集，其中包含超过 4k 个多模态澄清问题，并包含超过 14k 个图像。我们还提出了一种名为 Marto 的多模态查询澄清模型，并采用基于提示的生成式微调策略，以不同的提示进行不同阶段的训练。我们进行了多项分析来了解多模式内容在查询澄清阶段的重要性。实验结果表明，在选择相关图像时，图像的添加可以使检索性能显着提高高达 90%。还进行了广泛的分析，以显示 Marto 与判别性基线相比在有效性和效率方面的优越性。</li>
</ul>

<h3>Title: Towards Unified Alignment Between Agents, Humans, and Environment</h3>
<ul>
<li><strong>Authors: </strong>Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang, Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo, Zhicheng Guo, Peng Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07744">https://arxiv.org/abs/2402.07744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07744">https://arxiv.org/pdf/2402.07744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07744]] Towards Unified Alignment Between Agents, Humans, and Environment(https://arxiv.org/abs/2402.07744)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints. We then follow the principles of $\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of $\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.</li>
<li><strong>摘要：</strong>基础模型的快速进步导致了自主智能体的繁荣，它们利用基础模型的通用能力进行推理、决策和环境交互。然而，在复杂、现实的环境中运行时，代理的功效仍然有限。在这项工作中，我们介绍了 $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$) 的原则，该原则倡导主体与人类意图、环境动态和自我约束（例如货币预算限制）同时保持一致。我们从$\mathbf{UA}^2$的角度回顾了当前的代理研究，并强调了现有代理基准和候选方法中被忽视的因素。我们还通过向 WebShop 引入现实功能来进行概念验证研究，包括用于展示意图的用户配置文件、针对复杂环境动态的个性化重新排名以及用于反映自我约束的运行时成本统计。然后，我们遵循 $\mathbf{UA}^2$ 的原则提出我们代理的初始设计，并在改进后的 WebShop 中使用几个候选基线对其性能进行基准测试。大量的实验结果进一步证明了$\mathbf{UA}^2$原理的重要性。我们的研究揭示了自主代理研究的下一步，提高了一般问题解决能力。</li>
</ul>

<h3>Title: Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Zhenguo Li, Wei Bi, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07754">https://arxiv.org/abs/2402.07754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07754">https://arxiv.org/pdf/2402.07754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07754]] Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language  Models(https://arxiv.org/abs/2402.07754)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.</li>
<li><strong>摘要：</strong>扩散模型在文本处理中引起了人们的关注，与传统的自回归模型相比，它具有许多潜在的优势。这项工作探索了扩散模型和思想链（CoT）的集成，这是一种完善的技术，可以提高自回归语言模型的推理能力。我们提出思想扩散（DoT），允许推理步骤通过扩散过程随着时间的推移而扩散。与以从左到右、逐个标记的方式做出决策的传统自回归语言模型相比，DoT 在计算和推理性能之间的权衡方面提供了更大的灵活性。我们的实验结果证明了 DoT 在多位数乘法和小学数学问题中的有效性。此外，DoT 还展示了有前景的自我纠正能力，以及自一致性解码等现有推理增强技术的优势。我们的发现有助于理解和发展扩散语言模型的推理能力。</li>
</ul>

<h3>Title: Towards an Understanding of Stepwise Inference in Transformers: A  Synthetic Graph Navigation Model</h3>
<ul>
<li><strong>Authors: </strong>Mikail Khona, Maya Okawa, Jan Hula, Rahul Ramesh, Kento Nishi, Robert Dick, Ekdeep Singh Lubana, Hidenori Tanaka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07757">https://arxiv.org/abs/2402.07757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07757">https://arxiv.org/pdf/2402.07757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07757]] Towards an Understanding of Stepwise Inference in Transformers: A  Synthetic Graph Navigation Model(https://arxiv.org/abs/2402.07757)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.</li>
<li><strong>摘要：</strong>逐步推理协议（例如草稿本和思维链）通过将复杂问题分解为一系列更简单的子问题来帮助语言模型解决复杂问题。尽管通过这些协议取得了显着的性能提升，但逐步推理的基本机制仍然难以捉摸。为了解决这个问题，我们建议在综合任务上研究自回归 Transformer 模型，该任务体现了问题的多步骤性质，其中逐步推理通常最有用。具体来说，我们定义一个图导航问题，其中模型的任务是遍历图上从起点到目标节点的路径。尽管很简单，我们发现我们可以凭经验重现和分析大规模观察到的几种现象：（i）逐步推理差距，我们在训练数据的结构中找到了其原因； (ii) 随着采样温度的变化，模型生成的多样性与准确性之间的权衡； (iii) 模型输出的简单性偏差； (iv) 组合概括和上下文范例的首要偏差。总的来说，我们的工作引入了一个用于研究逐步推理的扎实的综合框架，并提供了机械假设，可以为更深入地理解这种现象奠定基础。</li>
</ul>

<h3>Title: TELLER: A Trustworthy Framework for Explainable, Generalizable and  Controllable Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Hui Liu, Wenya Wang, Haoru Li, Haoliang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07776">https://arxiv.org/abs/2402.07776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07776">https://arxiv.org/pdf/2402.07776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07776]] TELLER: A Trustworthy Framework for Explainable, Generalizable and  Controllable Fake News Detection(https://arxiv.org/abs/2402.07776)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework. Our implementation is available at \url{https://github.com/less-and-less-bugs/Trust_TELLER}.</li>
<li><strong>摘要：</strong>假新闻的泛滥已成为一个严重的社会问题，引起了工业界和学术界的极大兴趣。虽然现有的基于深度学习的方法在准确检测假新闻方面取得了进展，但由于推理过程不透明、泛化能力差以及与大型语言模型（LLM）集成的固有风险，其可靠性可能会受到损害。为了应对这一挑战，我们提出了一种用于可信假新闻检测的新颖框架，该框架优先考虑模型的可解释性、通用性和可控性。这是通过遵循上述原则的集成认知和决策系统的双系统框架来实现的。认知系统利用人类的专业知识来生成逻辑谓词，指导法学硕士生成人类可读的逻辑原子。同时，决策系统推导出可概括的逻辑规则来聚合这些原子，从而能够识别跨领域输入新闻的真实性，并提高决策过程的透明度。最后，我们给出了四个数据集的综合评估结果，证明了我们提出的框架的可行性和可信度。我们的实现可在 \url{https://github.com/less-and-less-bugs/Trust_TELLER} 获取。</li>
</ul>

<h3>Title: Empowering Federated Learning for Massive Models with NVIDIA FLARE</h3>
<ul>
<li><strong>Authors: </strong>Holger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07792">https://arxiv.org/abs/2402.07792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07792">https://arxiv.org/pdf/2402.07792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07792]] Empowering Federated Learning for Massive Models with NVIDIA FLARE(https://arxiv.org/abs/2402.07792)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge. Most state-of-the-art machine learning algorithms are data-centric. However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets. In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness.</li>
<li><strong>摘要：</strong>在人工智能 (AI) 和大型语言模型 (LLM) 不断发展的格局中，有效处理和利用数据已成为一项关键挑战。大多数最先进的机器学习算法都是以数据为中心的。然而，作为模型性能的命脉，由于隐私、监管、地缘政治、版权问题以及移动大量数据集所需的巨大努力等各种因素，必要的数据并不总是能够集中。在本文中，我们探讨了 NVIDIA FLARE 支持的联邦学习如何通过简单且可扩展的集成功能来应对这些挑战，从而为自然语言处理和生物制药应用程序实现 LLM 的参数高效和全面监督微调，以提高其准确性和稳健性。</li>
</ul>

<h3>Title: Generalising Planning Environment Redesign</h3>
<ul>
<li><strong>Authors: </strong>Alberto Pozanco, Ramon Fraga Pereira, Daniel Borrajo</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07799">https://arxiv.org/abs/2402.07799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07799">https://arxiv.org/pdf/2402.07799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07799]] Generalising Planning Environment Redesign(https://arxiv.org/abs/2402.07799)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment. Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric. This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently. This results in approaches that are not able to generalise across different objectives and/or metrics. In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans. Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top-quality planning to efficiently redesign planning environments according to any interested party's objective and metric. Experiments over a set of environment redesign benchmarks show that our general approach outperforms existing approaches when using well-known metrics, such as facilitating the recognition of goals, as well as its effectiveness when solving environment redesign tasks that optimise a novel set of different metrics.</li>
<li><strong>摘要：</strong>在环境设计中，一个相关方试图通过对环境进行更改来影响另一个代理的决策。大多数有关规划环境（重新）设计的研究都假设相关方的目标是促进目标和计划的识别，并搜索环境修改的空间，以找到简化这些任务并优化特定指标的最小更改集。该搜索空间通常很棘手，因此现有方法设计了依赖于度量的修剪技术来更有效地执行搜索。这导致方法无法概括不同的目标和/或指标。在本文中，我们认为相关方可能拥有不一定与识别代理的目标或计划相关的目标和指标。因此，为了概括规划环境重新设计的任务，我们开发了一种与度量无关的通用环境重新设计方法，并利用最新的高质量规划研究，根据任何相关方的目标和度量有效地重新设计规划环境。对一组环境重新设计基准的实验表明，在使用众所周知的指标时，我们的通用方法优于现有方法，例如促进目标的识别，以及在解决优化一组新颖的不同指标的环境重新设计任务时的有效性。</li>
</ul>

<h3>Title: Retrieval-Augmented Thought Process as Sequential Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07812">https://arxiv.org/abs/2402.07812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07812">https://arxiv.org/pdf/2402.07812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07812]] Retrieval-Augmented Thought Process as Sequential Decision Making(https://arxiv.org/abs/2402.07812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已经证明了其强大的帮助人们的能力并展现出“智慧的火花”。然而，一些公开的挑战阻碍了它们的更广泛应用：例如对隐私的担忧、产生幻觉的倾向以及处理长上下文的困难。在这项工作中，我们通过引入检索增强思维过程（RATP）来应对这些挑战。在获得外部知识的情况下，RATP 将法学硕士的思想生成制定为多步骤决策过程。为了优化这样的思维过程，RATP 利用蒙特卡罗树搜索，并学习 Q 值估计器，以实现经济高效的推理。在解决使用私人数据进行问答的任务时，道德和安全问题限制了 LLM 培训方法，RATP 比现有的上下文检索增强语言模型实现了 50% 的改进。</li>
</ul>

<h3>Title: Injecting Wiktionary to improve token-level contextual representations  using contrastive learning</h3>
<ul>
<li><strong>Authors: </strong>Anna Mosolova, Marie Candito, Carlos Ramisch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07817">https://arxiv.org/abs/2402.07817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07817">https://arxiv.org/pdf/2402.07817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07817]] Injecting Wiktionary to improve token-level contextual representations  using contrastive learning(https://arxiv.org/abs/2402.07817)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.</li>
<li><strong>摘要：</strong>虽然静态词嵌入对上下文是盲目的，但对于词汇语义任务来说，上下文在上下文词嵌入中出现得太多，相同含义出现的向量差异太大（Ethayarajh，2019）。提出了使用对比学习来微调预训练语言模型 (PLM)，利用自动自我增强的示例（Liu 等人，2021b）。在本文中，我们研究如何使用英语维基词典注入词典作为替代监督来源。我们还测试降维如何影响最终的上下文词嵌入。我们在无监督环境（不使用训练集）下评估我们在 Word-In-Context (WiC) 任务上的方法。我们在原始 WiC 测试集上取得了新的 SoTA 结果。我们还提出了两个新的 WiC 测试集，我们证明了我们的微调方法取得了实质性的改进。我们还观察到语义框架归纳任务的改进，尽管幅度不大。尽管我们在英语上进行了实验以便与相关工作进行比较，但我们的方法适用于大型维基词典中存在的多种语言。</li>
</ul>

<h3>Title: Differentially Private Zeroth-Order Methods for Scalable Large Language  Model Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Z Liu, J Lou, W Bao, Z Qin, K Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07818">https://arxiv.org/abs/2402.07818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07818">https://arxiv.org/pdf/2402.07818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07818]] Differentially Private Zeroth-Order Methods for Scalable Large Language  Model Finetuning(https://arxiv.org/abs/2402.07818)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replacement for SGD, this paper presents a comprehensive study both theoretically and empirically. First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters. This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on finetuning trajectory. Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget. We provide theoretical analysis for both proposed methods. We conduct extensive empirical analysis on both encoder-only masked language model and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility.</li>
<li><strong>摘要：</strong>对特定任务数据集进行微调是一种广泛接受的范例，它利用预训练的 LLM 的强大功能来完成各种下游任务。由于 LLM 微调的流行及其伴随的隐私问题，预训练 LLM 的差分隐私 (DP) 微调已引起越来越多的关注，以保护特定任务数据集的隐私。 DP LLM 微调方法的设计核心是隐私、实用性和可扩展性之间的令人满意的权衡。大多数现有方法都建立在 DP-SGD 的开创性工作之上。尽管将 DP-SGD 的可扩展性推向了极限，但不幸的是，基于 DP-SGD 的微调方法受到 SGD 固有的低效率的限制。在本文中，我们研究了 DP 零阶方法在 LLM 预训练中的潜力，该方法通过使用更有效的零阶梯度来逼近梯度，从而避免了 SGD 的可扩展性瓶颈。本文并没有将零阶方法视为 SGD 的直接替代品，而是在理论上和实证上进行了全面的研究。首先，我们提出了动态调度关键超参数的阶段式 DP 零阶方法。该设计基于DP随机扰动与零阶方法梯度逼近误差之间的协同作用及其对微调轨迹的影响。其次，我们通过重新利用不需要额外数据或额外隐私预算的无数据修剪技术来减少可训练参数，从而进一步增强可扩展性。我们为这两种提出的方​​法提供了理论分析。我们对仅编码器的掩码语言模型和仅解码器的自回归语言模型进行了广泛的实证分析，在可扩展性和实用性方面取得了令人印象深刻的结果。</li>
</ul>

<h3>Title: Aya Model: An Instruction Finetuned Open-Access Multilingual Language  Model</h3>
<ul>
<li><strong>Authors: </strong>Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07827">https://arxiv.org/abs/2402.07827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07827">https://arxiv.org/pdf/2402.07827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07827]] Aya Model: An Instruction Finetuned Open-Access Multilingual Language  Model(https://arxiv.org/abs/2402.07827)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新突破集中在少数数据丰富的语言上。怎样才能扩大获得一流公民语言之外的突破的机会？我们的工作介绍了 Aya，这是一种大规模多语言生成语言模型，它遵循 101 种语言的指令，其中超过 50% 的语言被认为资源匮乏。 Aya 在大多数任务上都优于 mT0 和 BLOOMZ，同时覆盖的语言数量是 mT0 和 BLOOMZ 的两倍。我们引入了广泛的新评估套件，扩大了跨 99 种语言的多语言评估的最新水平，包括判别性和生成性任务、人工评估以及涵盖保留任务和分配中表现的模拟获胜率。此外，我们对最佳微调混合物组成、数据修剪以及模型的毒性、偏差和安全性进行了详细的研究。我们在 https://hf.co/CohereForAI/aya-101 开源我们的指令数据集和模型</li>
</ul>

<h3>Title: Do Membership Inference Attacks Work on Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07841">https://arxiv.org/abs/2402.07841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07841">https://arxiv.org/pdf/2402.07841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07841]] Do Membership Inference Attacks Work on Large Language Models?(https://arxiv.org/abs/2402.07841)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges. We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work.</li>
<li><strong>摘要：</strong>成员推理攻击 (MIA) 尝试预测特定数据点是否是目标模型训练数据的成员。尽管对传统机器学习模型进行了广泛的研究，但在大型语言模型（LLM）的预训练数据上研究 MIA 的工作仍然有限。我们对 Pile 上训练的一套语言模型 (LM) 进行了大规模的 MIA 评估，参数范围从 160M 到 12B。我们发现，对于不同 LLM 规模和领域的大多数设置，MIA 几乎没有优于随机猜测。我们的进一步分析表明，这种糟糕的表现可归因于（1）大数据集和少量训练迭代的组合，以及（2）会员和非会员之间固有的模糊边界。我们确定了法学硕士被证明容易受到成员资格推断的特定环境，并表明这种环境中的明显成功可以归因于分布变化，例如当成员和非成员来自看似相同的领域但具有不同的领域时。时间范围。我们将代码和数据作为统一的基准包发布，其中包括所有现有的 MIA，支持未来的工作。</li>
</ul>

<h3>Title: Lissard: Long and Simple Sequential Reasoning Datasets</h3>
<ul>
<li><strong>Authors: </strong>Mirelle Bueno, Roberto Lotufo, Rodrigo Nogueira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07859">https://arxiv.org/abs/2402.07859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07859">https://arxiv.org/pdf/2402.07859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07859]] Lissard: Long and Simple Sequential Reasoning Datasets(https://arxiv.org/abs/2402.07859)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard</li>
<li><strong>摘要：</strong>语言模型现在能够解决需要处理由数十万个标记组成的长序列的任务。然而，它们经常无法完成需要重复使用简单规则的任务，甚至是比训练期间短得多的序列。例如，最先进的法学硕士可以在两个列表中找到最多 20 个项目的共同项目，但当列表有 80 个项目时就会失败。在本文中，我们介绍了 Lissard，这是一个包含七个任务的基准测试，其目标是评估模型处理和生成大范围序列长度（需要重复的程序执行）的能力。我们对开源（Mistral-7B 和 Mixtral-8x7B）和专有模型（GPT-3.5 和 GPT-4）的评估表明，随着序列复杂性的增加，所有模型的性能都在持续下降。数据集和代码可在 https://github.com/unicamp-dl/Lissard 获取</li>
</ul>

<h3>Title: Scaling Laws for Fine-Grained Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Piotr Sankowski, Marek Cygan, Sebastian Jaszczur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07871">https://arxiv.org/abs/2402.07871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07871">https://arxiv.org/pdf/2402.07871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07871]] Scaling Laws for Fine-Grained Mixture of Experts(https://arxiv.org/abs/2402.07871)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.</li>
<li><strong>摘要：</strong>专家混合 (MoE) 模型已成为降低大型语言模型计算成本的主要解决方案。在这项工作中，我们分析了它们的缩放特性，纳入了扩大的变量范围。具体来说，我们引入了一个新的超参数——粒度，其调整可以精确控制专家的规模。在此基础上，我们考虑到训练标记的数量、模型大小和粒度，建立了细粒度 MoE 的缩放法则。利用这些定律，我们得出给定计算预算的最佳训练配置。我们的研究结果不仅表明 MoE 模型始终优于密集 Transformer，而且还强调，随着我们扩大模型规模和训练预算，密集模型和 MoE 模型之间的效率差距会扩大。此外，我们证明了在 MoE 中设置专家规模以反映前馈层的常见做法在几乎任何计算预算下都不是最佳的。</li>
</ul>

<h3>Title: Policy Improvement using Language Feedback Models</h3>
<ul>
<li><strong>Authors: </strong>Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07876">https://arxiv.org/abs/2402.07876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07876">https://arxiv.org/pdf/2402.07876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07876]] Policy Improvement using Language Feedback Models(https://arxiv.org/abs/2402.07876)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.</li>
<li><strong>摘要：</strong>我们引入了语言反馈模型（LFM），它可以识别所需的行为（帮助完成指令中指定的任务的动作），以便在遵循指令时进行模仿学习。为了训练 LFM，我们从大型语言模型 (LLM) 获取有关语言描述的视觉轨迹的反馈。首先，通过使用 LFM 来识别需要模仿的行为，我们在三种不同的语言基础环境（Touchdown、ScienceWorld 和 ALFWorld）上的强行为克隆基线上提高了任务完成率。其次，在控制 LLM 输出代币的数量时，LFM 的表现优于使用 LLM 作为直接预测操作的专家。第三，LFM 泛化到未见过的环境，通过一轮适应将任务完成率提高了 3.5-12.0%。最后，LFM 可以进行修改，以在不损失性能的情况下提供人类可解释的反馈，从而允许人类验证模仿学习的所需行为。</li>
</ul>

<h3>Title: WildfireGPT: Tailored Large Language Model for Wildfire Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yangxinyu Xie, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07877">https://arxiv.org/abs/2402.07877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07877">https://arxiv.org/pdf/2402.07877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07877]] WildfireGPT: Tailored Large Language Model for Wildfire Analysis(https://arxiv.org/abs/2402.07877)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML). However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change. For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展代表了人工智能 (AI) 和机器学习 (ML) 前沿的变革能力。然而，法学硕士是通用模型，经过广泛的文本语料库的训练，通常很难提供特定背景的信息，特别是在需要专业知识的领域，例如更广泛的气候变化背景下的野火细节。对于专注于野火恢复和适应的决策者和政策制定者来说，获得不仅准确而且针对特定领域而不是通用的响应至关重要。为此，我们开发了 WildfireGPT，这是一种原型 LLM 代理，旨在将用户查询转化为有关野火风险的可操作见解。我们通过提供额外的背景信息（例如气候预测和科学文献）来丰富 WildfireGPT，以确保其信息是最新的、相关的且科学准确的。这使得 WildfireGPT 成为一种有效的工具，可以提供有关野火风险的详细的、特定于用户的见解，以支持不同的最终用户，包括研究人员、工程师、城市规划者、应急管理人员和基础设施运营商。</li>
</ul>

<h3>Title: MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement  Learning</h3>
<ul>
<li><strong>Authors: </strong>Ayesha Siddika Nipu, Siming Liu, Anthony Harris</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07890">https://arxiv.org/abs/2402.07890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07890">https://arxiv.org/pdf/2402.07890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07890]] MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement  Learning(https://arxiv.org/abs/2402.07890)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect that our model not only achieves higher winning rate in all the given scenarios but also boosts the agent's learning process in fine-grained decision-making.</li>
<li><strong>摘要：</strong>多智能体系统中的分布式决策对合作和竞争系统中的交互式行为学习提出了严峻的挑战。为了减轻这种复杂性，MAIDRL 提出了一种由智能体影响图 (AIM) 增强的半集中式密集强化学习算法，用于学习星际争霸多智能体挑战 (SMAC) 场景中的有效多智能体控制。在本文中，我们扩展了 MAIDRL 中的 DenseNet，并通过将卷积层合并到深度模型架构中，引入半集中式多代理密集 CNN 强化学习 (MAIDCRL)，并评估同质和异构场景下的性能。结果表明，与现有的 MAIDRL 相比，启用 CNN 的 MAIDCRL 显着提高了学习性能并实现了更快的学习速率，尤其是在更复杂的异构 SMAC 场景上。我们进一步研究模型的稳定性和鲁棒性。统计数据表明，我们的模型不仅在所有给定场景中实现了更高的胜率，而且还促进了代理在细粒度决策中的学习过程。</li>
</ul>

<h3>Title: Suppressing Pink Elephants with Direct Principle Feedback</h3>
<ul>
<li><strong>Authors: </strong>Louis Castricato, Nathan Lile, Suraj Anand, Hailey Schoelkopf, Siddharth Verma, Stella Biderman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07896">https://arxiv.org/abs/2402.07896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07896">https://arxiv.org/pdf/2402.07896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07896]] Suppressing Pink Elephants with Direct Principle Feedback(https://arxiv.org/abs/2402.07896)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Existing methods for controlling language models, such as RLHF and Constitutional AI, involve determining which LLM behaviors are desirable and training them into a language model. However, in many cases, it is desirable for LLMs to be controllable \textit{at inference time}, so that they can be used in multiple contexts with diverse needs. We illustrate this with the \textbf{Pink Elephant Problem}: instructing an LLM to avoid discussing a certain entity (a ``Pink Elephant''), and instead discuss a preferred entity (``Grey Elephant''). We apply a novel simplification of Constitutional AI, \textbf{Direct Principle Feedback}, which skips the ranking of responses and uses DPO directly on critiques and revisions. Our results show that after DPF fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on our curated test set assessing the Pink Elephant Problem.</li>
<li><strong>摘要：</strong>控制语言模型的现有方法，例如 RLHF 和宪法人工智能，涉及确定哪些 LLM 行为是可取的，并将它们训练成语言模型。然而，在许多情况下，LLM 最好是在推理时可控的，以便它们可以在具有不同需求的多个上下文中使用。我们用\textbf{粉红大象问题}来说明这一点：指示法学硕士避免讨论某个实体（“粉红大象”），而讨论首选实体（“灰象”）。我们应用宪法人工智能的新颖简化，\textbf{直接原则反馈}，它跳过回复排名并直接使用 DPO 进行批评和修订。我们的结果表明，在我们的合成 Pink Elephants 数据集上进行 DPF 微调后，我们的 13B 微调 LLaMA 2 模型显着优于 Llama-2-13B-Chat 和提示基线，并且在我们策划的数据集上的表现与 GPT-4 一样好评估粉红象问题的测试集。</li>
</ul>

<h3>Title: A systematic investigation of learnability from single child linguistic  input</h3>
<ul>
<li><strong>Authors: </strong>Yulu Qin, Wentao Wang, Brenden M. Lake</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07899">https://arxiv.org/abs/2402.07899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07899">https://arxiv.org/pdf/2402.07899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07899]] A systematic investigation of learnability from single child linguistic  input(https://arxiv.org/abs/2402.07899)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.</li>
<li><strong>摘要：</strong>语言模型（LM）在生成语言连贯的文本方面表现出了非凡的能力，引发了关于它们与理解人类语言可学习性的相关性的讨论。然而，这些模型的训练数据和孩子收到的语言输入之间存在显着差距。语言模型通常使用比儿童定向语音大几个数量级且根本不同的数据进行训练（Warstadt 和 Bowman，2022；Warstadt 等人，2023；Frank，2023a）。为了解决这一差异，我们的研究重点是针对单个孩子的语言输入子集来训练 LM。此前，Wang、Vong、Kim 和 Lake（2023）发现，在这种环境下训练的 LM 可以形成句法和语义词簇，并对某些语言现象产生敏感性，但他们只考虑了 LSTM 和仅从一个单一训练的更简单的神经网络子数据集。在这里，为了检查单子输入的可学习性的稳健性，我们在五个数据集（3 个单子和 2 个基线）上系统地训练了六种不同的模型架构。我们发现，在单童数据集上训练的模型显示出与之前的工作相匹配的一致结果，强调了从儿童语言输入的子集形成有意义的句法和语义表示的稳健性。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
