<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-11</h1>
<h3>Title: Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Itay Itzhak, Yonatan Belinkov, Gabriel Stanovsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07186">https://arxiv.org/abs/2507.07186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07186">https://arxiv.org/pdf/2507.07186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07186]] Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs(https://arxiv.org/abs/2507.07186)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decision-making, similar to those seen in humans. Prior work has found that these biases vary across models and can be amplified by instruction tuning. However, it remains unclear if these differences in biases stem from pretraining, finetuning, or even random noise due to training stochasticity. We propose a two-step causal experimental approach to disentangle these factors. First, we finetune models multiple times using different random seeds to study how training randomness affects over $30$ cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping instruction datasets between models to isolate bias sources. This swap uses datasets that led to different bias patterns, directly testing whether biases are dataset-dependent. Our findings reveal that while training randomness introduces some variability, biases are mainly shaped by pretraining: models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data. These insights suggest that understanding biases in finetuned models requires considering their pretraining origins beyond finetuning effects. This perspective can guide future efforts to develop principled strategies for evaluating and mitigating bias in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）表现出认知偏见 - 非理性决策的系统倾向，类似于人类中的决策。先前的工作发现，这些偏见在各个模型之间各不相同，并且可以通过指令调整来放大。但是，目前尚不清楚这些偏见的差异是否源于训练随机性引起的训练，填充甚至随机噪声。我们提出了一种两步的因果实验方法，以解开这些因素。首先，我们多次使用不同的随机种子来研究训练随机性如何影响$ 30 $的认知偏见。其次，我们介绍\ emph {交叉调整}  - 在模型之间交换指令数据集以隔离偏置源。此交换使用导致不同偏差模式的数据集，直接测试偏差是否取决于数据集。我们的发现表明，尽管训练随机性引入了一些可变性，但偏见主要是由预处理形成的：具有相同验证的骨架的模型表现出比仅共享Finetunting数据的模型更相似的偏差模式。这些见解表明，理解固定模型中的偏见需要考虑其预训练的起源，而没有鉴定效果。这种观点可以指导未来的努力，以制定评估和减轻LLM中偏见的原则策略。</li>
</ul>

<h3>Title: Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses</h3>
<ul>
<li><strong>Authors: </strong>Jens Rupprecht (1), Georg Ahnert (1), Markus Strohmaier (1 and 2 and 3) ((1) University of Mannheim, (2) GESIS - Leibniz Institute for the Social Sciences, (3) Complexity Science Hub)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07188">https://arxiv.org/abs/2507.07188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07188">https://arxiv.org/pdf/2507.07188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07188]] Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses(https://arxiv.org/abs/2507.07188)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used as proxies for human subjects in social science surveys, but their reliability and susceptibility to known response biases are poorly understood. This paper investigates the response robustness of LLMs in normative survey contexts -- we test nine diverse LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of 11 perturbations to both question phrasing and answer option structure, resulting in over 167,000 simulated interviews. In doing so, we not only reveal LLMs' vulnerabilities to perturbations but also reveal that all tested models exhibit a consistent \textit{recency bias} varying in intensity, disproportionately favoring the last-presented answer option. While larger models are generally more robust, all models remain sensitive to semantic variations like paraphrasing and to combined perturbations. By applying a set of perturbations, we reveal that LLMs partially align with survey response biases identified in humans. This underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用作社会科学调查中人类受试者的代理，但是对已知反应偏见的可靠性和易感性知之甚少。本文调查了LLM在规范调查环境中的响应鲁棒性 - 我们在世界价值调查（WVS）的问题上测试了9种不同的LLMS（WVS），应用了一组全面的11个扰动，以提出问题措辞和答案选项结构，导致超过167,000个模拟访谈。在此过程中，我们不仅揭示了LLMS对扰动的脆弱性，而且还揭示了所有经过测试的模型表现出一致的\ textit {recenty偏见}的强度变化，不成比例地赞成最后一个呈现的答案选项。虽然较大的模型通常更健壮，但所有模型均对诸如释义和组合扰动等语义变化敏感。通过应用一组扰动，我们揭示了LLM部分与人类确定的调查响应偏见一致。这强调了使用LLMS生成合成调查数据时及时设计和鲁棒性测试的至关重要性。</li>
</ul>

<h3>Title: SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains</h3>
<ul>
<li><strong>Authors: </strong>Krithika Ramesh, Daniel Smolyak, Zihao Zhao, Nupoor Gandhi, Ritu Agarwal, Margrét Bjarnadóttir, Anjalie Field</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07229">https://arxiv.org/abs/2507.07229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07229">https://arxiv.org/pdf/2507.07229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07229]] SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains(https://arxiv.org/abs/2507.07229)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present SynthTextEval, a toolkit for conducting comprehensive evaluations of synthetic text. The fluency of large language model (LLM) outputs has made synthetic text potentially viable for numerous applications, such as reducing the risks of privacy violations in the development and deployment of AI systems in high-stakes domains. Realizing this potential, however, requires principled consistent evaluations of synthetic data across multiple dimensions: its utility in downstream systems, the fairness of these systems, the risk of privacy leakage, general distributional differences from the source text, and qualitative feedback from domain experts. SynthTextEval allows users to conduct evaluations along all of these dimensions over synthetic data that they upload or generate using the toolkit's generation module. While our toolkit can be run over any data, we highlight its functionality and effectiveness over datasets from two high-stakes domains: healthcare and law. By consolidating and standardizing evaluation metrics, we aim to improve the viability of synthetic text, and in-turn, privacy-preservation in AI development.</li>
<li><strong>摘要：</strong>我们提出了Synthtexteval，这是一种用于进行合成文本的全面评估的工具包。大语言模型（LLM）输出的流利度使综合文本对众多应用可能可行，例如在高风险域中降低AI系统的开发和部署中侵犯隐私的风险。但是，意识到这种潜力需要对多个维度跨多个维度的合成数据进行原则评估：其在下游系统中的效用，这些系统的公平性，隐私泄漏的风险，对源文本的一般分布差异以及域专家的定性反馈。 Synthtexteval允许用户对使用工具包的生成模块上传或生成的合成数据进行所有这些维度进行评估。虽然我们的工具包可以通过任何数据运行，但我们强调了其在两个高风险领域的数据集（医疗保健和法律）上的功能和有效性。通过巩固和标准化评估指标，我们旨在提高合成文本的生存能力，并在AI开发中进行隐私保护。</li>
</ul>

<h3>Title: Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings</h3>
<ul>
<li><strong>Authors: </strong>Minseon Kim, Jean-Philippe Corbeil, Alessandro Sordoni, Francois Beaulieu, Paul Vozila</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07248">https://arxiv.org/abs/2507.07248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07248">https://arxiv.org/pdf/2507.07248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07248]] Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings(https://arxiv.org/abs/2507.07248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As the performance of large language models (LLMs) continues to advance, their adoption is expanding across a wide range of domains, including the medical field. The integration of LLMs into medical applications raises critical safety concerns, particularly due to their use by users with diverse roles, e.g. patients and clinicians, and the potential for model's outputs to directly affect human health. Despite the domain-specific capabilities of medical LLMs, prior safety evaluations have largely focused only on general safety benchmarks. In this paper, we introduce a safety evaluation protocol tailored to the medical domain in both patient user and clinician user perspectives, alongside general safety assessments and quantitatively analyze the safety of medical LLMs. We bridge a gap in the literature by building the PatientSafetyBench containing 466 samples over 5 critical categories to measure safety from the perspective of the patient. We apply our red-teaming protocols on the MediPhi model collection as a case study. To our knowledge, this is the first work to define safety evaluation criteria for medical LLMs through targeted red-teaming taking three different points of view - patient, clinician, and general user - establishing a foundation for safer deployment in medical domains.</li>
<li><strong>摘要：</strong>随着大语言模型（LLM）的性能继续前进，其采用正在扩大到包括医疗领域在内的广泛领域。将LLM集成到医疗应用中引起了关键的安全问题，特别是由于用户使用各种角色，例如患者和临床医生，以及模型产量直接影响人类健康的潜力。尽管医疗LLM具有特定领域的功能，但先前的安全评估主要仅集中在一般安全基准上。在本文中，我们介绍了针对患者用户和临床用户观点的医疗领域量身定制的安全评估协议，以及一般安全评估，并定量分析医疗LLM的安全性。我们通过在5个关键类别中包含466个样本的患者使用患者来弥合文献中的差距，从而从患者的角度衡量安全性。我们将红色团队的协议应用于Mediphi模型收集，作为案例研究。据我们所知，这是通过以三种不同的观点（患者，临床医生和普通用户）为目标的红线来定义医疗LLMS安全评估标准的第一项工作，为在医疗领域中的安全部署建立了基础。</li>
</ul>

<h3>Title: The Impact of Background Speech on Interruption Detection in Collaborative Groups</h3>
<ul>
<li><strong>Authors: </strong>Mariah Bradford, Nikhil Krishnaswamy, Nathaniel Blanchard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07280">https://arxiv.org/abs/2507.07280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07280">https://arxiv.org/pdf/2507.07280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07280]] The Impact of Background Speech on Interruption Detection in Collaborative Groups(https://arxiv.org/abs/2507.07280)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Interruption plays a crucial role in collaborative learning, shaping group interactions and influencing knowledge construction. AI-driven support can assist teachers in monitoring these interactions. However, most previous work on interruption detection and interpretation has been conducted in single-conversation environments with relatively clean audio. AI agents deployed in classrooms for collaborative learning within small groups will need to contend with multiple concurrent conversations -- in this context, overlapping speech will be ubiquitous, and interruptions will need to be identified in other ways. In this work, we analyze interruption detection in single-conversation and multi-group dialogue settings. We then create a state-of-the-art method for interruption identification that is robust to overlapping speech, and thus could be deployed in classrooms. Further, our work highlights meaningful linguistic and prosodic information about how interruptions manifest in collaborative group interactions. Our investigation also paves the way for future works to account for the influence of overlapping speech from multiple groups when tracking group dialog.</li>
<li><strong>摘要：</strong>中断在协作学习，塑造小组互动和影响知识构建方面起着至关重要的作用。 AI驱动的支持可以帮助教师监视这些互动。但是，大多数关于中断检测和解释的研究都在相对干净的音频中进行了。在课堂上部署在小组中的协作学习中的AI代理将需要与多次并发对话抗衡 - 在这种情况下，重叠的语音将无处不在，并且需要以其他方式识别中断。在这项工作中，我们分析了单转交和多组对话设置中的中断检测。然后，我们创建了一种最先进的方法，用于中断识别，该方法可以重叠语音，因此可以部署在教室中。此外，我们的工作强调了有关中断如何在协作小组互动中表现出的有意义的语言和韵律信息。我们的调查还为未来的工作铺平了道路，以说明在跟踪小组对话框时多个小组重叠的语音的影响。</li>
</ul>

<h3>Title: Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Anirban Saha Anik, Xiaoying Song, Elliott Wang, Bryan Wang, Bengisu Yarimbas, Lingzi Hong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07307">https://arxiv.org/abs/2507.07307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07307">https://arxiv.org/pdf/2507.07307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07307]] Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation(https://arxiv.org/abs/2507.07307)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) incorporated with Retrieval-Augmented Generation (RAG) have demonstrated powerful capabilities in generating counterspeech against misinformation. However, current studies rely on limited evidence and offer less control over final outputs. To address these challenges, we propose a Multi-agent Retrieval-Augmented Framework to generate counterspeech against health misinformation, incorporating multiple LLMs to optimize knowledge retrieval, evidence enhancement, and response refinement. Our approach integrates both static and dynamic evidence, ensuring that the generated counterspeech is relevant, well-grounded, and up-to-date. Our method outperforms baseline approaches in politeness, relevance, informativeness, and factual accuracy, demonstrating its effectiveness in generating high-quality counterspeech. To further validate our approach, we conduct ablation studies to verify the necessity of each component in our framework. Furthermore, human evaluations reveal that refinement significantly enhances counterspeech quality and obtains human preference.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）与检索演示的生成（RAG）合并，已经证明了在产生反对错误信息的强大功能。但是，当前的研究依靠有限的证据，对最终产出的控制权更少。为了应对这些挑战，我们提出了一个多代理检索型框架，以生成反对健康错误信息的反语，并结合了多个LLM，以优化知识检索，增强证据和响应改进。我们的方法既集成静态和动态证据，从而确保生成的反语是相关的，良好的和最新的。我们的方法在礼貌，相关性，信息性和事实准确性上优于基线方法，这表明了其在产生高质量的反语方面的有效性。为了进一步验证我们的方法，我们进行消融研究以验证框架中每个组件的必要性。此外，人类的评估表明，精致可显着提高反语质量并获得人类偏好。</li>
</ul>

<h3>Title: GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation</h3>
<ul>
<li><strong>Authors: </strong>Fardin Rastakhiz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07414">https://arxiv.org/abs/2507.07414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07414">https://arxiv.org/pdf/2507.07414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07414]] GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation(https://arxiv.org/abs/2507.07414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Time, cost, and energy efficiency are critical considerations in Deep-Learning (DL), particularly when processing long texts. Transformers, which represent the current state of the art, exhibit quadratic computational complexity relative to input length, making them inefficient for extended documents. This study introduces a novel model architecture that combines Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated with a real-time, end-to-end graph generation mechanism. The model processes compact batches of character-level inputs without requiring padding or truncation. To enhance performance while maintaining high speed and efficiency, the model incorporates information from Large Language Models (LLMs), such as token embeddings and sentiment polarities, through efficient dictionary lookups. It captures local contextual patterns using CNNs, expands local receptive fields via lattice-based graph structures, and employs small-world graphs to aggregate document-level information. The generated graphs exhibit structural properties indicative of meaningful semantic organization, with an average clustering coefficient of approximately 0.45 and an average shortest path length ranging between 4 and 5. The model is evaluated across multiple text classification tasks, including sentiment analysis and news-categorization, and is compared against state-of-the-art models. Experimental results confirm the proposed model's efficiency and competitive performance.</li>
<li><strong>摘要：</strong>时间，成本和能源效率是深度学习（DL）的关键考虑，尤其是在处理长文本时。代表当前最新状态的变压器相对于输入长度表现出二次计算复杂性，使得它们效率低为扩展文档。这项研究介绍了一种新型的模型结构，该模型结构结合了图形神经网络（GNN）和卷积神经网络（CNN），并与实时的，端到端的图形生成机制集成在一起。模型处理字符级输入的紧凑批次，而无需填充或截断。为了在保持高速和效率的同时提高性能，该模型通过有效的词典查找结合了大型语言模型（LLM）的信息，例如令牌嵌入和情感极性。它使用CNN捕获本地上下文模式，通过基于晶格的图形结构扩展本地接收场，并采用小世界图来汇总文档级信息。生成的图具有指示有意义的语义组织的结构特性，平均聚类系数约为0.45，平均最短路径长度在4到5之间。在多个文本分类任务中评估了该模型，包括情感分析和新闻类别，并比较了与较为直觉的模型相比。实验结果证实了所提出的模型的效率和竞争性能。</li>
</ul>

<h3>Title: MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning</h3>
<ul>
<li><strong>Authors: </strong>Hieu Tran, Zonghai Yao, Won Seok Jang, Sharmin Sultana, Allen Chang, Yuan Zhang, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07419">https://arxiv.org/abs/2507.07419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07419">https://arxiv.org/pdf/2507.07419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07419]] MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning(https://arxiv.org/abs/2507.07419)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Generative AI has demonstrated strong potential in healthcare, from clinical decision support to patient-facing chatbots that improve outcomes. A critical challenge for deployment is effective human-AI communication, where content must be both personalized and understandable. We introduce MedReadCtrl, a readability-controlled instruction tuning framework that enables LLMs to adjust output complexity without compromising meaning. Evaluations of nine datasets and three tasks across medical and general domains show that MedReadCtrl achieves significantly lower readability instruction-following errors than GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples). Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low literacy levels. These gains reflect MedReadCtrl's ability to restructure clinical content into accessible, readability-aligned language while preserving medical intent, offering a scalable solution to support patient education and expand equitable access to AI-enabled care.</li>
<li><strong>摘要：</strong>从临床决策支持到面向患者的聊天机器人，生成的AI在医疗保健方面具有强大的潜力。部署的关键挑战是有效的人类交流，其中的内容必须是个性化的，又可以理解的。我们介绍了MedreadCtrl，这是一种可读性控制的指令调谐框架，使LLMS能够在不影响意义的情况下调整输出复杂性。对医学和一般领域的九个数据集和三个任务的评估表明，MedreadCtrl的可读性指令遵循错误的误差明显低于GPT-4（例如，Readme，p <0.001）的1.39 vs. 1.59，并且在未经看去的临床任务上递减了可观的增长（例如，sarie sari sari sari sari shime）。专家一直偏爱MedreadCtrl（71.7％比23.3％），尤其是在低识字水平下。这些收益反映了MedreadCtrl将临床内容重组为可访问的，可读性与一致性的语言的能力，同时保留医学意图，提供可扩展的解决方案，以支持患者教育并扩大对AI支持的护理的公平访问。</li>
</ul>

<h3>Title: SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data</h3>
<ul>
<li><strong>Authors: </strong>Zonghai Yao, Youxia Zhao, Avijit Mitra, David A. Levy, Emily Druhl, Jack Tsai, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07421">https://arxiv.org/abs/2507.07421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07421">https://arxiv.org/pdf/2507.07421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07421]] SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data(https://arxiv.org/abs/2507.07421)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Eviction is a significant yet understudied social determinants of health (SDoH), linked to housing instability, unemployment, and mental health. While eviction appears in unstructured electronic health records (EHRs), it is rarely coded in structured fields, limiting downstream applications. We introduce SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop annotation, and automated prompt optimization (APO) to extract eviction statuses from clinical notes. Using this pipeline, we created the largest public eviction-related SDoH dataset to date, comprising 14 fine-grained categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%), GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling cost-effective deployment across various model sizes. The pipeline reduces annotation effort by over 80%, accelerates dataset creation, enables scalable eviction detection, and generalizes to other information extraction tasks.</li>
<li><strong>摘要：</strong>驱逐是健康的重要社会决定因素（SDOH），与住房不稳定，失业和心理健康有关。尽管驱逐出现在非结构化的电子健康记录（EHR）中，但很少在结构化领域编码，从而限制了下游应用。我们介绍了Synthehr-Eviction，可扩展的管道，结合了LLMS，人体注释和自动化及时优化（APO），以从临床注释中提取驱逐状态。使用此管道，我们迄今为止创建了最大的公共驱逐相关数据集，其中包括14个细粒类别。对SyntheHR-Empiction培训的微调LLM（例如Qwen2.5，Llama3）的宏F1得分为88.8％（驱逐）和90.3％（其他SDOH）（其他SDOH），对人类验证的数据，超过GPT-4O-4O-APO（87.8％，87.8％，87.3％），GPT-4O。 Biobert（60.7％，68.3％），同时在各种型号范围内实现了成本效益的部署。该管道将​​注释工作减少了80％以上，加速了数据集创建，实现可扩展的驱逐检测，并将其推广到其他信息提取任务。</li>
</ul>

<h3>Title: Towards Interpretable Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Boileau, Philippe Helluy, Jeremy Pawlus, Svitlana Vyetrenko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07439">https://arxiv.org/abs/2507.07439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07439">https://arxiv.org/pdf/2507.07439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07439]] Towards Interpretable Time Series Foundation Models(https://arxiv.org/abs/2507.07439)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the distillation of time series reasoning capabilities into small, instruction-tuned language models as a step toward building interpretable time series foundation models. Leveraging a synthetic dataset of mean-reverting time series with systematically varied trends and noise levels, we generate natural language annotations using a large multimodal model and use these to supervise the fine-tuning of compact Qwen models. We introduce evaluation metrics that assess the quality of the distilled reasoning - focusing on trend direction, noise intensity, and extremum localization - and show that the post-trained models acquire meaningful interpretive capabilities. Our results highlight the feasibility of compressing time series understanding into lightweight, language-capable models suitable for on-device or privacy-sensitive deployment. This work contributes a concrete foundation toward developing small, interpretable models that explain temporal patterns in natural language.</li>
<li><strong>摘要：</strong>在本文中，我们研究了时间序列推理功能的蒸馏，以构建可解释的时间序列基础模型的一步。利用均值趋势和噪声水平的均值回复时间序列的合成数据集，我们使用大型的多模型模型生成自然语言注释，并使用它们来监督紧凑型QWEN模型的微调。我们介绍评估指标，以评估蒸馏推理的质量 - 专注于趋势方向，噪声强度和极值定位 - 并表明受过训练的模型具有有意义的解释能力。我们的结果突出了将时间序列理解理解为轻巧，具有语言能力的模型的可行性，适合于设备或对隐私敏感的部署。这项工作为开发小型，可解释的模型提供了具体的基础，这些模型可以解释自然语言的时间模式。</li>
</ul>

<h3>Title: SAND: Boosting LLM Agents with Self-Taught Action Deliberation</h3>
<ul>
<li><strong>Authors: </strong>Yu Xia, Yiran Jenny Shen, Junda Wu, Tong Yu, Sungchul Kim, Ryan A. Rossi, Lina Yao, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07441">https://arxiv.org/abs/2507.07441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07441">https://arxiv.org/pdf/2507.07441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07441]] SAND: Boosting LLM Agents with Self-Taught Action Deliberation(https://arxiv.org/abs/2507.07441)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）代理通常会通过对反应式专家轨迹的监督填充或对成对推广的优化优化进行调节。这些方法中的大多数都集中在模仿特定的专家行为或促进被拒绝的推理思想和行动。但是，如果没有推理和比较替代作用，则使用这些方法进行了审核的LLM代理可能会因有限的动作空间探索而过于承认看似合理但次优的行动。为了解决这个问题，在本文中，我们提出了自学成才的行动审议（SAND）框架，使LLM代理商能够在承诺之前明确考虑候选行动。为了应对何时以及何时以及何时考虑何时以及在较大的行动空间和步骤级别的行动评估中应对什么，我们结合了自洽行动采样和执行引导的行动批评，以帮助使用LLM代理的基本模型综合逐步的行动审议思想。然后，以迭代方式，将审议轨迹用于修复LLM代理本身。评估两种代表性的交互式代理任务，Sand比初始监督的登录平均提高了20％，并且表现优于最先进的代理调整方法。</li>
</ul>

<h3>Title: RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, Guorui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07451">https://arxiv.org/abs/2507.07451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07451">https://arxiv.org/pdf/2507.07451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07451]] RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning(https://arxiv.org/abs/2507.07451)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) for large language models is an energy-intensive endeavor: training can be unstable, and the policy may gradually drift away from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement Learning with Experience rePlay\, -- \,a two-phase framework that first collects verified trajectories and then replays them during subsequent training. At every update step, the policy is optimized on mini-batches that blend newly generated rollouts with these replayed successes. By replaying high-quality examples, RLEP steers the model away from fruitless exploration, focuses learning on promising reasoning paths, and delivers both faster convergence and stronger final performance. On the Qwen2.5-Math-7B base model, RLEP reaches baseline peak accuracy with substantially fewer updates and ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%, on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our code, datasets, and checkpoints are publicly available at this https URL to facilitate reproducibility and further research.</li>
<li><strong>摘要：</strong>大型语言模型的强化学习（RL）是一项能源密集型的努力：培训可能是不稳定的，并且该政策可能会逐渐摆脱验证的重量。我们提出\ emph {rlep} \， -  \，具有经验重播\， -  \的增强学习，这是一个两阶段的框架，首先收集经过验证的轨迹，然后在随后的培训中重播它们。在每个更新步骤中，该策略均可在将新生成的推出与这些重播成功融合在一起的迷你批次上进行了优化。通过重播高质量的示例，RLEP将模型从毫无结果的探索中转移到了有希望的推理路径上，并提供更快的融合和更强的最终表现。在QWEN2.5-MATH-7B基本模型上，RLEP达到基线峰准确性，最终更新少得多，最终超过了它，将AIME-2024的准确性从38.2％提高到39.9％，将AIME-2025上的AIME-2025从19.8％到22.3％提高到22.3％，并从AMC-2023上提高到77..0.0.0.0.0.0.0. d.82.0％。我们的代码，数据集和检查站在此HTTPS URL上公开可用，以促进可重复性和进一步的研究。</li>
</ul>

<h3>Title: Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaiqu Liang, Haimin Hu, Xuandong Zhao, Dawn Song, Thomas L. Griffiths, Jaime Fernández Fisac</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07484">https://arxiv.org/abs/2507.07484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07484">https://arxiv.org/pdf/2507.07484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07484]] Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models(https://arxiv.org/abs/2507.07484)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. We conduct empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. Our results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. We also observe prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. Our findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior.</li>
<li><strong>摘要：</strong>哲学家哈里·法兰克福（Harry Frankfurt）概念化了胡说八道，是指不考虑其真相价值的陈述。虽然先前的工作探索了大型语言模型（LLM）幻觉和摇摇欲坠，但我们提出了机器胡说八道作为一个总体的概念框架，可以使研究人员能够表征LLMS中出现真实性的更广泛现象，并在其基本机制上阐明。我们介绍了废话指数，这是一种新颖的指标，量化了LLMS对真理的冷漠，并提出了一种补充分类法，分析了四种定性形式的废话：空的修辞，触摸，蠕虫词和未验证的主张。我们在市场数据集，政治中立数据集和我们的新的BullShiteVal基准（跨越100个AI助手的情况）上进行了经验评估，该基准是明确设计用于评估机器胡说八道的。我们的结果表明，通过从人类反馈（RLHF）学习的增强学习的模型显着加剧了胡说八道和推理时间链链（COT），促使人们显着放大了特定的胡说八道形式，尤其是空的修辞和partering。我们还以政治背景下观察到普遍的机器胡说八道，鼬鼠词是主要的策略。我们的发现突出了AI对齐中的系统挑战，并为更真实的LLM行为提供了新的见解。</li>
</ul>

<h3>Title: PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Mihir Parmar, Palash Goyal, Xin Liu, Yiwen Song, Mingyang Ling, Chitta Baral, Hamid Palangi, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07495">https://arxiv.org/abs/2507.07495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07495">https://arxiv.org/pdf/2507.07495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07495]] PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving(https://arxiv.org/abs/2507.07495)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, decomposing complex problems into simple subtasks--a crucial part of human-like natural planning--to solve the given problem has significantly boosted the performance of large language models (LLMs). However, leveraging such planning structures during post-training to boost the performance of smaller open-source LLMs remains underexplored. Motivated by this, we introduce PLAN-TUNING, a unified post-training framework that (i) distills synthetic task decompositions (termed "planning trajectories") from large-scale LLMs and (ii) fine-tunes smaller models via supervised and reinforcement-learning objectives designed to mimic these planning processes to improve complex reasoning. On GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by an average $\sim7\%$. Furthermore, plan-tuned models show better generalization capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$ performance improvements on OlympiadBench and AIME 2024, respectively. Our detailed analysis demonstrates how planning trajectories improves complex reasoning capabilities, showing that PLAN-TUNING is an effective strategy for improving task-specific performance of smaller LLMs.</li>
<li><strong>摘要：</strong>最近，将复杂的问题分解为简单的子任务（类似人类的自然规划的关键部分）解决给定的问题已大大提高了大语言模型（LLMS）的性能。但是，在培训后利用此类计划结构以提高较小的开源LLMS的性能仍然没有被忽视。在此激励的情况下，我们引入了计划调整，这是一个统一的训练后框架，（i）（i）将合成的任务分解（称为“计划轨迹”）从大型LLMS和（ii）通过有监督的和增强型的目的旨在模仿这些计划的目标来改善复杂的理由的目标。在GSM8K和数学基准上，计划调整的型号的表现平均$ \ sim7 \％$优于强基线。此外，计划调整的模型在室外数据集上显示出更好的概括功能，分别为$ \ sim10 \％$和$ \ sim12 \％$ $ $ $ $ $ $ $ $ $。我们的详细分析表明，规划轨迹如何提高复杂的推理能力，表明计划调整是改善较小LLM的任务特定性能的有效策略。</li>
</ul>

<h3>Title: Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code</h3>
<ul>
<li><strong>Authors: </strong>Keqin Bao, Nuo Chen, Xiaoyuan Li, Binyuan Hui, Bowen Yu, Fuli Feng, Junyang Lin, Xiangnan He, Dayiheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07498">https://arxiv.org/abs/2507.07498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07498">https://arxiv.org/pdf/2507.07498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07498]] Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code(https://arxiv.org/abs/2507.07498)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Enhancing reasoning capabilities remains a central focus in the LLM reasearch community. A promising direction involves requiring models to simulate code execution step-by-step to derive outputs for given inputs. However, as code is often designed for large-scale systems, direct application leads to over-reliance on complex data structures and algorithms, even for simple cases, resulting in overfitting to algorithmic patterns rather than core reasoning structures. To address this, we propose TeaR, which aims at teaching LLMs to reason better. TeaR leverages careful data curation and reinforcement learning to guide models in discovering optimal reasoning paths through code-related tasks, thereby improving general reasoning abilities. We conduct extensive experiments using two base models and three long-CoT distillation models, with model sizes ranging from 1.5 billion to 32 billion parameters, and across 17 benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results consistently show significant performance improvements. Notably, TeaR achieves a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.</li>
<li><strong>摘要：</strong>增强推理能力仍然是LLM Reasearch社区的核心重点。一个有希望的方向涉及要求模型逐步模拟代码执行，以得出给定输入的输出。但是，由于代码通常是为大型系统设计的，因此直接应用程序会导致对复杂数据结构和算法的过度依赖，即使对于简单的情况，也导致过度适合算法模式，而不是核心推理结构。为了解决这个问题，我们提出了撕裂，旨在教授LLMS更好地推理。撕裂利用仔细的数据策划和强化学习来指导模型通过与代码相关的任务发现最佳推理路径，从而提高一般推理能力。我们使用两种基本模型和三个长期蒸馏模型进行了广泛的实验，模型尺寸从15亿到320亿个参数不等，以及跨越数学，知识，代码和逻辑推理的17个基准测试。结果始终显示出显着的性能改善。值得注意的是，撕裂可在QWEN2.5-7B上提高35.9％，R1-DISTISTILD-7B提高了5.9％。</li>
</ul>

<h3>Title: Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Varin Sikka, Vishal Sikka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07505">https://arxiv.org/abs/2507.07505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07505">https://arxiv.org/pdf/2507.07505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07505]] Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models(https://arxiv.org/abs/2507.07505)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>With widespread adoption of transformer-based language models in AI, there is significant interest in the limits of LLMs capabilities, specifically so-called hallucinations, occurrences in which LLMs provide spurious, factually incorrect or nonsensical information when prompted on certain subjects. Furthermore, there is growing interest in agentic uses of LLMs - that is, using LLMs to create agents that act autonomously or semi-autonomously to carry out various tasks, including tasks with applications in the real world. This makes it important to understand the types of tasks LLMs can and cannot perform. We explore this topic from the perspective of the computational complexity of LLM inference. We show that LLMs are incapable of carrying out computational and agentic tasks beyond a certain complexity, and further that LLMs are incapable of verifying the accuracy of tasks beyond a certain complexity. We present examples of both, then discuss some consequences of this work.</li>
<li><strong>摘要：</strong>随着AI中基于变压器的语言模型的广泛采用，人们对LLMS功能的限制，特别是所谓的幻觉引起了浓厚的兴趣，在某些主题上提示LLMS提供了虚假的，事实上的错误或荒谬的信息。此外，对LLMS的代理使用的兴趣越来越大 - 也就是说，使用LLMS来创建自主或半自主行动以执行各种任务，包括具有现实世界中应用程序的任务。这使得了解LLMS所能做到的任务类型非常重要。我们从LLM推断的计算复杂性的角度探讨了这个主题。我们表明，LLMs无法执行超出一定复杂性的计算和代理任务，进一步说，LLMs无法验证超出一定复杂性的任务的准确性。我们介绍两者的例子，然后讨论这项工作的一些后果。</li>
</ul>

<h3>Title: Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System</h3>
<ul>
<li><strong>Authors: </strong>Yuanchen Shi, Longyin Zhang, Fang Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07509">https://arxiv.org/abs/2507.07509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07509">https://arxiv.org/pdf/2507.07509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07509]] Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System(https://arxiv.org/abs/2507.07509)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>The growing need for psychological support due to increasing pressures has exposed the scarcity of relevant datasets, particularly in non-English languages. To address this, we propose a framework that leverages limited real-world data and expert knowledge to fine-tune two large language models: Dialog Generator and Dialog Modifier. The Generator creates large-scale psychological counseling dialogues based on predefined paths, which guide system response strategies and user interactions, forming the basis for effective support. The Modifier refines these dialogues to align with real-world data quality. Through both automated and manual review, we construct the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K dialogues across 13 groups, 16 psychological problems, 13 causes, and 12 support focuses. Additionally, we introduce the Comprehensive Agent Dialogue Support System (CADSS), where a Profiler analyzes user characteristics, a Summarizer condenses dialogue history, a Planner selects strategies, and a Supporter generates empathetic responses. The experimental results of the Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate that CADSS achieves state-of-the-art performance on both CPsDD and ESConv datasets.</li>
<li><strong>摘要：</strong>由于压力的增加，人们对心理支持的需求日益增长，已经揭示了相关数据集的稀缺性，尤其是在非英语语言中。为了解决这个问题，我们提出了一个框架，该框架利用有限的现实世界数据和专家知识来微调两个大语言模型：对话框生成器和对话框修饰符。发电机基于预定义的路径创建了大规模的心理咨询对话，这些对话指导系统响应策略和用户互动，从而构成了有效支持的基础。修饰符完善了这些对话，以与现实世界的数据质量保持一致。通过自动化和手动审查，我们构建了中国心理支持对话数据集（CPSDD），其中包含13个组，16个心理问题，13个原因和12个支持重点的68K对话。此外，我们介绍了综合代理对话支持系统（CADSS），其中探查者分析用户特征，摘要器凝结对话历史记录，计划者选择策略，而支持者会产生善解人意的响应。战略预测和情感支持对话（ESC）任务的实验结果表明，CADSS在CPSDD和ESCONV数据集上都实现了最先进的性能。</li>
</ul>

<h3>Title: CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text</h3>
<ul>
<li><strong>Authors: </strong>Akram Elbouanani, Evan Dufraisse, Aboubacar Tuo, Adrian Popescu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07539">https://arxiv.org/abs/2507.07539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07539">https://arxiv.org/pdf/2507.07539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07539]] CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text(https://arxiv.org/abs/2507.07539)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper presents a competitive approach to multilingual subjectivity detection using large language models (LLMs) with few-shot prompting. We participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation campaign. We show that LLMs, when paired with carefully designed prompts, can match or outperform fine-tuned smaller language models (SLMs), particularly in noisy or low-quality data settings. Despite experimenting with advanced prompt engineering techniques, such as debating LLMs and various example selection strategies, we found limited benefit beyond well-crafted standard few-shot prompts. Our system achieved top rankings across multiple languages in the CheckThat! 2025 subjectivity detection task, including first place in Arabic and Polish, and top-four finishes in Italian, English, German, and multilingual tracks. Notably, our method proved especially robust on the Arabic dataset, likely due to its resilience to annotation inconsistencies. These findings highlight the effectiveness and adaptability of LLM-based few-shot learning for multilingual sentiment tasks, offering a strong alternative to traditional fine-tuning, particularly when labeled data is scarce or inconsistent.</li>
<li><strong>摘要：</strong>本文使用大型语言模型（LLM）提出了一种竞争性的多语言主观性检测方法。我们参加了任务1：检查的主观性！ 2025评估活动。我们表明，LLMS与精心设计的提示配对时，可以匹配或胜过微调的较小语言模型（SLM），尤其是在嘈杂或低质量的数据设置中。尽管尝试了高级及时的工程技术，例如辩论LLM和各种示例选择策略，但我们发现超出了精心制作的标准次数提示以外的好处有限。我们的系统在CheckThat中跨多种语言获得了最高排名！ 2025年的主观性检测任务，包括阿拉伯语和波兰的第一名，以及意大利语，英语，德语和多语言曲目的前四名。值得注意的是，我们的方法在阿拉伯数据集上特别强大，这可能是由于其对注释不一致的韧性所致。这些发现突出了基于LLM的几次学习对多语言情感任务的有效性和适应性，为传统的微调提供了强有力的替代方案，尤其是当标记的数据稀缺或不一致时。</li>
</ul>

<h3>Title: The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora</h3>
<ul>
<li><strong>Authors: </strong>Chen Amiraz, Yaroslav Fyodorov, Elad Haramaty, Zohar Karnin, Liane Lewin-Eytan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07543">https://arxiv.org/abs/2507.07543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07543">https://arxiv.org/pdf/2507.07543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07543]] The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora(https://arxiv.org/abs/2507.07543)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Cross-lingual retrieval-augmented generation (RAG) is a critical capability for retrieving and generating answers across languages. Prior work in this context has mostly focused on generation and relied on benchmarks derived from open-domain sources, most notably Wikipedia. In such settings, retrieval challenges often remain hidden due to language imbalances, overlap with pretraining data, and memorized content. To address this gap, we study Arabic-English RAG in a domain-specific setting using benchmarks derived from real-world corporate datasets. Our benchmarks include all combinations of languages for the user query and the supporting document, drawn independently and uniformly at random. This enables a systematic study of multilingual retrieval behavior. Our findings reveal that retrieval is a critical bottleneck in cross-lingual domain-specific scenarios, with significant performance drops occurring when the user query and supporting document languages differ. A key insight is that these failures stem primarily from the retriever's difficulty in ranking documents across languages. Finally, we propose a simple retrieval strategy that addresses this source of failure by enforcing equal retrieval from both languages, resulting in substantial improvements in cross-lingual and overall performance. These results highlight meaningful opportunities for improving multilingual retrieval, particularly in practical, real-world RAG applications.</li>
<li><strong>摘要：</strong>跨语性检索型生成（RAG）是跨语言检索和生成答案的关键能力。在这种情况下，先前的工作主要集中在发电上，并依赖于开放域来源（尤其是Wikipedia）得出的基准。在这种情况下，由于语言失衡，与预处理的数据重叠以及记忆的内容，检索挑战常常保持隐藏。为了解决这一差距，我们使用来自现实世界的企业数据集衍生的基准在特定于域的设置中研究阿拉伯语 - 英语抹布。我们的基准包括用户查询的所有语言组合和支持文档，并随机独立且均匀地绘制。这使得对多语言检索行为进行系统研究。我们的发现表明，在特定于跨语言领域的方案中，检索是一个关键的瓶颈，当用户查询和支持文档语言不同时，出现大量的性能下降。一个关键的见解是，这些失败主要源于猎犬在跨语言中对文档进行排名的困难。最后，我们提出了一种简单的检索策略，该策略通过从两种语言中相同的检索实施同等的检索来解决这种失败的来源，从而实现了跨语言和整体性能的实质性改善。这些结果凸显了改善多语言检索的有意义的机会，尤其是在实际的现实抹布应用中。</li>
</ul>

<h3>Title: The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs</h3>
<ul>
<li><strong>Authors: </strong>Jierun Chen, Tiezheng Yu, Haoli Bai, Lewei Yao, Jiannan Wu, Kaican Li, Fei Mi, Chaofan Tao, Lei Zhu, Manyi Zhang, Xiaohui Li, Lu Hou, Lifeng Shang, Qun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07562">https://arxiv.org/abs/2507.07562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07562">https://arxiv.org/pdf/2507.07562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07562]] The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs(https://arxiv.org/abs/2507.07562)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large vision-language models (VLMs) increasingly adopt post-training techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL) to elicit sophisticated reasoning. While these methods exhibit synergy in language-only models, their joint effectiveness in VLMs remains uncertain. We present a systematic investigation into the distinct roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning benchmarks. We find that SFT improves performance on difficult questions by in-depth, structured reasoning, but introduces verbosity and degrades performance on simpler ones. In contrast, RL promotes generalization and brevity, yielding consistent improvements across all difficulty levels, though the improvements on the hardest questions are less prominent compared to SFT. Surprisingly, combining them through two-staged, interleaved, or progressive training strategies, as well as data mixing and model merging, all fails to produce additive benefits, instead leading to trade-offs in accuracy, reasoning style, and response length. This ``synergy dilemma'' highlights the need for more seamless and adaptive approaches to unlock the full potential of combined post-training techniques for reasoning VLMs.</li>
<li><strong>摘要：</strong>大型视觉模型（VLMS）越来越多地采用训练后技术，例如长期经营链（COT）监督的微调（SFT）和加固学习（RL）来引起复杂的推理。尽管这些方法在仅语言模型中表现出协同作用，但它们在VLMS中的共同有效性仍然不确定。我们对多个多模式推理基准的长期SFT和RL的不同作用和相互作用进行了系统的研究。我们发现，SFT通过深入的结构化推理提高了难题的绩效，但引入了冗长的性能并降低了更简单的绩效。相比之下，RL促进了概括和简洁，尽管与SFT相比，最难的问题的改进不那么突出，但在所有难度水平上都会取得一致的改善。令人惊讶的是，将它们通过两阶段，交错或渐进式培训策略以及数据混合和模型合并结合在一起，所有这些都无法产生添加优势，而是导致精确度，推理样式和响应长度的权衡。这种``协同难题''强调了需要采用更多无缝和适应性方法来解锁对推理VLM的全面培训技术的全部潜力。</li>
</ul>

<h3>Title: Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Yupu Liang, Yaping Zhang, Zhiyang Zhang, Yang Zhao, Lu Xiang, Chengqing Zong, Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07572">https://arxiv.org/abs/2507.07572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07572">https://arxiv.org/pdf/2507.07572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07572]] Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation(https://arxiv.org/abs/2507.07572)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Document Image Machine Translation (DIMT) aims to translate text within document images, facing generalization challenges due to limited training data and the complex interplay between visual and textual information. To address these challenges, we introduce M4Doc, a novel single-to-mix modality alignment framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an image-only encoder with the multimodal representations of an MLLM, pre-trained on large-scale document image datasets. This alignment enables a lightweight DIMT model to learn crucial visual-textual correlations during training. During inference, M4Doc bypasses the MLLM, maintaining computational efficiency while benefiting from its multimodal knowledge. Comprehensive experiments demonstrate substantial improvements in translation quality, especially in cross-domain generalization and challenging document image scenarios.</li>
<li><strong>摘要：</strong>文档图像机器翻译（DIMT）旨在在文档图像中翻译文本，面临概括挑战，这是由于训练数据有限以及视觉和文本信息之间的复杂相互作用所致。为了应对这些挑战，我们介绍了M4DOC，这是一种新型的单一单一模式对齐框架，利用多模式大语言模型（MLLM）。 M4DOC将仅图像编码器与MLLM的多模式表示，并在大规模文档图像数据集上进行了预训练。这种对齐使一个轻巧的DIMT模型能够在训练过程中学习至关重要的视觉文本相关性。在推断期间，M4DOC绕过MLLM，保持计算效率，同时受益于其多模式知识。全面的实验表明了翻译质量的实质性改善，尤其是跨域概括和具有挑战性的文档图像方案。</li>
</ul>

<h3>Title: Bayesian Discrete Diffusion Beats Autoregressive Perplexity</h3>
<ul>
<li><strong>Authors: </strong>Cooper Doyle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07586">https://arxiv.org/abs/2507.07586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07586">https://arxiv.org/pdf/2507.07586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07586]] Bayesian Discrete Diffusion Beats Autoregressive Perplexity(https://arxiv.org/abs/2507.07586)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We reveal a hidden Bayesian core of discrete-diffusion language models by showing that the expected denoiser output under the forward masking distribution recovers the exact posterior over clean tokens. Under minimal assumptions, Monte Carlo marginalization over K independent corruptions converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of consistency and finite-sample error bounds. Building on this insight, we introduce a lightweight inference-time ensemble that averages K mask-and-denoise passes to obtain posterior-aware token probabilities and uncertainty estimates at no extra training cost. On WikiText-2, our method achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite using a model of comparable size. Code is available at this https URL.</li>
<li><strong>摘要：</strong>我们通过表明向前掩蔽分布下的预期Denoiser输出可在清洁令牌上恢复确切的后验，从而揭示了一个隐藏的贝叶斯核心。在最小的假设下，k独立腐败的蒙特卡洛边缘化以o（1/sqrt（k））的速率收敛到这一后验，从而简单地证明了一致性和有限样本误差范围。在这种见解的基础上，我们引入了轻巧的推理时间合奏，该合奏平均k面具和denoise通行证以获得后意识的令牌概率和不确定性估计，而无需额外的培训费用。在Wikitext-2上，我们的方法达到了测试的困惑8.8，k = 8，尽管使用了相当大小的模型，但GPT-2小的测试困惑为20.3。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks</h3>
<ul>
<li><strong>Authors: </strong>Joyeeta Datta, Niclas Doll, Qusai Ramadan, Zeyd Boukhers</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07630">https://arxiv.org/abs/2507.07630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07630">https://arxiv.org/pdf/2507.07630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07630]] Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks(https://arxiv.org/abs/2507.07630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated outstanding performance across a range of NLP tasks, however, their computational demands hinder their deployment in real-world, resource-constrained environments. This work investigates the extent to which LLMs can be compressed using Knowledge Distillation (KD) while maintaining strong performance on Question Answering (QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5 families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot prompting conditions. Results show that student models retain over 90% of their teacher models' performance while reducing parameter counts by up to 57.1%. Furthermore, one-shot prompting yields additional performance gains over zero-shot setups for both model families. These findings underscore the trade-off between model efficiency and task performance, demonstrating that KD, combined with minimal prompting, can yield compact yet capable QA systems suitable for resource-constrained applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在一系列NLP任务中表现出出色的性能，但是，它们的计算要求阻碍了他们在现实世界中的，资源约束的环境中的部署。这项工作研究了可以使用知识蒸馏（KD）压缩LLM的程度，同时保持在问答（QA）任务上的强劲绩效。我们在两个QA基准测试基准（小队和MLQA）上评估了从Pythia和Qwen2.5家族提取的学生模型，并在零射门和一次性提示条件下进行。结果表明，学生模型保留了教师模型的90％以上的表现，同时将参数计数降低了57.1％。此外，一杆促使两个模型系列的零射击设置可获得额外的性能增长。这些发现强调了模型效率和任务绩效之间的权衡，表明KD加上最小的提示，可以产生紧凑型但功能强大的质量质量质量检查系统，适用于资源受限的应用程序。</li>
</ul>

<h3>Title: FrugalRAG: Learning to retrieve and reason for multi-hop QA</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Java, Srivathsan Koundinyan, Nagarajan Natarajan, Amit Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07634">https://arxiv.org/abs/2507.07634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07634">https://arxiv.org/pdf/2507.07634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07634]] FrugalRAG: Learning to retrieve and reason for multi-hop QA(https://arxiv.org/abs/2507.07634)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, retrieval-augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-of-thought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples).</li>
<li><strong>摘要：</strong>我们考虑回答复杂问题的问题，允许访问大型的非结构化文档语料库。解决问题的事实上的方法是利用（迭代）通过检索的文档检索和推理的语言模型，直到该模型具有足够的信息来生成答案为止。尝试改进这种方法的尝试专注于检索功能的产生（RAG）指标，例如准确性和召回率，可以分为两种类型：（a）对大型问题答案（QA）数据集进行微调，并通过对RL链链的良好调查技术进行了增强，并利用基于RL的良好调查技术，这些技术依靠问题限制了依赖问题的信息。但是，检索搜索的效率是同等重要的指标，它受到了较少的关注。在这项工作中，我们表明：（1）不需要大规模的微调来改善抹布指标，这与最近文献中流行的主张相反。具体而言，具有改进提示的标准反应管道可以在基准（例如HOTPOTQA）上胜过最先进的方法。 （2）受监督和基于RL的微调可以从节俭的角度来帮助抹布，即由于推理时间的搜索数量而导致的延迟。例如，我们表明我们可以使用相同的基础模型和小型培训成本（1000个示例），以几乎一半的成本（就搜索数量而言）实现竞争性的破布指标（就搜索数量而言）。</li>
</ul>

<h3>Title: Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement</h3>
<ul>
<li><strong>Authors: </strong>Haotan Guo, Jianfei He, Jiayuan Ma, Hongbin Na, Zimu Wang, Haiyang Zhang, Qi Chen, Wei Wang, Zijing Shi, Tao Shen, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07640">https://arxiv.org/abs/2507.07640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07640">https://arxiv.org/pdf/2507.07640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07640]] Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement(https://arxiv.org/abs/2507.07640)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Phonetic Cloaking Replacement (PCR), defined as the deliberate use of homophonic or near-homophonic variants to hide toxic intent, has become a major obstacle to Chinese content moderation. While this problem is well-recognized, existing evaluations predominantly rely on rule-based, synthetic perturbations that ignore the creativity of real users. We organize PCR into a four-way surface-form taxonomy and compile \ours, a dataset of 500 naturally occurring, phonetically cloaked offensive posts gathered from the RedNote platform. Benchmarking state-of-the-art LLMs on this dataset exposes a serious weakness: the best model reaches only an F1-score of 0.672, and zero-shot chain-of-thought prompting pushes performance even lower. Guided by error analysis, we revisit a Pinyin-based prompting strategy that earlier studies judged ineffective and show that it recovers much of the lost accuracy. This study offers the first comprehensive taxonomy of Chinese PCR, a realistic benchmark that reveals current detectors' limits, and a lightweight mitigation technique that advances research on robust toxicity detection.</li>
<li><strong>摘要：</strong>语音掩盖替换（PCR）被定义为故意使用谐音或近杂种变体来隐藏有毒意图，已成为中国内容适度的主要障碍。尽管该问题得到了很好的认可，但现有的评估主要依赖于基于规则的合成扰动，这些扰动忽略了真实用户的创造力。我们将PCR整理成一个四向表面形式的分类法和编译\我们的cm，该数据集由500个自然出现的，语音掩盖的进攻帖子，从Rednote平台收集。该数据集上的最先进的LLM基准测试揭示了一个严重的弱点：最佳模型仅达到0.672的F1得分，而零投机链的零链链促使促使性能提高了性能甚至更低。在错误分析的指导下，我们重新审视了一种基于拼音的提示策略，该策略早期研究判断无效，并表明它恢复了许多损失的准确性。这项研究提供了中国PCR的第一个全面分类法，这是一种现实的基准，揭示了当前的检测器的限制，以及一种轻巧的缓解技术，可在鲁棒毒性检测方面进行研究。</li>
</ul>

<h3>Title: An Automated Length-Aware Quality Metric for Summarization</h3>
<ul>
<li><strong>Authors: </strong>Andrew D. Foland</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07653">https://arxiv.org/abs/2507.07653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07653">https://arxiv.org/pdf/2507.07653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07653]] An Automated Length-Aware Quality Metric for Summarization(https://arxiv.org/abs/2507.07653)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This paper proposes NOrmed Index of Retention (NOIR), a quantitative objective metric for evaluating summarization quality of arbitrary texts that relies on both the retention of semantic meaning and the summary length compression. This gives a measure of how well the recall-compression tradeoff is managed, the most important skill in summarization. Experiments demonstrate that NOIR effectively captures the token-length / semantic retention tradeoff of a summarizer and correlates to human perception of sumarization quality. Using a language model-embedding to measure semantic similarity, it provides an automated alternative for assessing summarization quality without relying on time-consuming human-generated reference summaries. The proposed metric can be applied to various summarization tasks, offering an automated tool for evaluating and improving summarization algorithms, summarization prompts, and synthetically-generated summaries.</li>
<li><strong>摘要：</strong>本文提出了定期保留指数（NOIR），这是一种定量目标度量标准，用于评估依赖语义含义和摘要长度压缩的任意文本的摘要质量。这可以衡量召回压缩权权衡的管理程度，这是总结中最重要的技能。实验表明，黑色有效地捕获了摘要的令牌 /语义保留权的权衡，并与人类对总和质量的看法相关。它使用语言模型装饰来衡量语义相似性，它提供了一种自动选择，用于评估摘要质量，而无需依赖耗时的人类生成的参考摘要。提出的指标可以应用于各种摘要任务，提供一种自动化工具，用于评估和改进摘要算法，汇总提示和合成生成的摘要。</li>
</ul>

<h3>Title: KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities</h3>
<ul>
<li><strong>Authors: </strong>Hruday Markondapatnaikuni, Basem Suleiman, Abdelkarim Erradi, Shijing Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07695">https://arxiv.org/abs/2507.07695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07695">https://arxiv.org/pdf/2507.07695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07695]] KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities(https://arxiv.org/abs/2507.07695)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Fine-tuning is an immensely resource-intensive process when retraining Large Language Models (LLMs) to incorporate a larger body of knowledge. Although many fine-tuning techniques have been developed to reduce the time and computational cost involved, the challenge persists as LLMs continue to grow in size and complexity. To address this, a new approach to knowledge expansion in LLMs is needed. Retrieval-Augmented Generation (RAG) offers one such alternative by storing external knowledge in a database and retrieving relevant chunks to support question answering. However, naive implementations of RAG face significant limitations in scalability and answer accuracy. This paper introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome these limitations. Inspired by the divide-and-conquer paradigm, K2RAG integrates dense and sparse vector search, knowledge graphs, and text summarization to improve retrieval quality and system efficiency. The framework also includes a preprocessing step that summarizes the training data, significantly reducing the training time. K2RAG was evaluated using the MultiHopRAG dataset, where the proposed pipeline was trained on the document corpus and tested on a separate evaluation set. Results demonstrated notable improvements over common naive RAG implementations. K2RAG achieved the highest mean answer similarity score of 0.57, and reached the highest third quartile (Q3) similarity of 0.82, indicating better alignment with ground-truth answers. In addition to improved accuracy, the framework proved highly efficient. The summarization step reduced the average training time of individual components by 93%, and execution speed was up to 40% faster than traditional knowledge graph-based RAG systems. K2RAG also demonstrated superior scalability, requiring three times less VRAM than several naive RAG implementations tested in this study.</li>
<li><strong>摘要：</strong>当对大型语言模型（LLMS）进行重新融合时，微调是一个非常有资源密集的过程。尽管已经开发了许多微调技术来减少涉及的时间和计算成本，但随着LLM的规模和复杂性的增长，挑战仍然存在。为了解决这个问题，需要在LLMS中进行知识扩展的新方法。检索演示的生成（RAG）通过将外部知识存储在数据库中并检索相关的块来支持问答答案，从而提供了一种替代方案。但是，纯天真的抹布实现面临可伸缩性的重大限制和回答准确性。本文介绍了KeyKnowledGerag（K2RAG），这是一个旨在克服这些局限性的新型框架。受划分范围范式的启发，K2RAG集成了密集和稀疏的矢量搜索，知识图和文本摘要，以提高检索质量和系统效率。该框架还包括一个预处理步骤，该步骤总结了训练数据，从而大大减少了培训时间。使用Multihoprag数据集对K2RAG进行了评估，在该数据集中，在文档语料库上对所提出的管道进行了培训，并在单独的评估集上进行了测试。结果表明，对常见的幼稚抹布实现有了显着改善。 K2RAG达到了0.57的最高平均答案相似性评分，并达到了0.82的最高第三四分位数（Q3）相似性，表明与地面真正的答案更好。除了提高精度外，该框架还证明了高效。汇总步骤使单个组件的平均训练时间减少了93％，并且执行速度比传统知识基于图形的抹布系统快40％。 K2RAG还表现出较高的可伸缩性，比本研究中测试的几个幼稚的抹布实现所需的VRAM少三倍。</li>
</ul>

<h3>Title: Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhijin Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07725">https://arxiv.org/abs/2507.07725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07725">https://arxiv.org/pdf/2507.07725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07725]] Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization(https://arxiv.org/abs/2507.07725)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Post-training alignment of large language models (LLMs) is a critical challenge, as not all tokens contribute equally to model performance. This paper introduces a selective alignment strategy that prioritizes high-impact tokens within preference pairs, leveraging token-level log-probability differences between the current policy and a reference model. By focusing on these informative tokens, our approach reduces computational overhead and enhances alignment fidelity. We further explore the role of reference model quality, demonstrating that stronger reference models significantly improve token selection accuracy and overall optimization effectiveness. Comprehensive experiments on benchmarks such as Arena-Hard and MT-Bench validate the superiority of our Selective-DPO method over standard DPO and distillation-based baselines. Our findings highlight the importance of token-level optimization and reference model selection in advancing preference alignment for LLMs. The code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的训练后一致性是一个关键的挑战，因为并非所有代币都对模型性能产生了同等的贡献。本文介绍了一种选择性的对齐策略，该策略优先考虑优先级的高影响令，从而利用了当前策略和参考模型之间的令牌级对数概率差异。通过专注于这些内容丰富的令牌，我们的方法可以减少计算开销并增强对齐保真度。我们进一步探讨了参考模型质量的作用，表明更强大的参考模型可显着提高令牌选择准确性和整体优化效果。基准（例如竞技场）和MT板凳等基准的综合实验验证了我们选择性DPO方法比标准DPO和基于蒸馏的基准的优越性。我们的发现突出了令牌级优化和参考模型选择在推进LLMS偏好对齐方面的重要性。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance</h3>
<ul>
<li><strong>Authors: </strong>Peizhang Shao, Linrui Xu, Jinxi Wang, Wei Zhou, Xingyu Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07748">https://arxiv.org/abs/2507.07748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07748">https://arxiv.org/pdf/2507.07748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07748]] When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance(https://arxiv.org/abs/2507.07748)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>This paper establishes the first comprehensive review of Large Language Models (LLMs) applied within the legal domain. It pioneers an innovative dual lens taxonomy that integrates legal reasoning frameworks and professional ontologies to systematically unify historical research and contemporary breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such as contextual reasoning and generative argumentation, surmount traditional limitations by dynamically capturing legal semantics and unifying evidence reasoning. Significant progress is documented in task generalization, reasoning formalization, workflow integration, and addressing core challenges in text processing, knowledge integration, and evaluation rigor via technical innovations like sparse attention mechanisms and mixture-of-experts architectures. However, widespread adoption of LLM introduces critical challenges: hallucination, explainability deficits, jurisdictional adaptation difficulties, and ethical asymmetry. This review proposes a novel taxonomy that maps legal roles to NLP subtasks and computationally implements the Toulmin argumentation framework, thus systematizing advances in reasoning, retrieval, prediction, and dispute resolution. It identifies key frontiers including low-resource systems, multimodal evidence integration, and dynamic rebuttal handling. Ultimately, this work provides both a technical roadmap for researchers and a conceptual framework for practitioners navigating the algorithmic future, laying a robust foundation for the next era of legal artificial intelligence. We have created a GitHub repository to index the relevant papers: this https URL.</li>
<li><strong>摘要：</strong>本文建立了对法律领域中应用的大语言模型（LLM）的首次全面综述。它开创了创新的双镜头分类法，该分类法将法律推理框架和专业本体论整合到系统地统一历史研究和当代突破。基于变压器的LLM表现出紧急的能力，例如上下文推理和生成论证，通过动态捕获法律语义并统一证据推理来克服传统的限制。在任务概括，形式化，工作流程集成以及通过技术创新（例如稀疏的注意机制和混合式体系结构）中严格的评估方面的核心挑战中记录了重大进展。但是，LLM的广泛采用引入了关键挑战：幻觉，解释性赤字，管辖权适应困难和道德不对称性。这篇综述提出了一种新颖的分类法，该分类法将法律角色映射到NLP子任务，并在计算上实现了Toulmin论证框架，从而使推理，检索，预测和争议解决方案的进步系统化。它确定了关键边界，包括低资源系统，多模式证据集成和动态反驳处理。最终，这项工作为研究人员提供了技术路线图，也为从业者浏览算法未来的概念框架，为下一个法律人工智能时代奠定了强大的基础。我们创建了一个GitHub存储库来索引相关论文：此HTTPS URL。</li>
</ul>

<h3>Title: StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Shoutao Guo, Xiang Li, Shaolei Zhang, Mengge Liu, Wei Chen, Yang Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07803">https://arxiv.org/abs/2507.07803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07803">https://arxiv.org/pdf/2507.07803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07803]] StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model(https://arxiv.org/abs/2507.07803)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Streaming speech translation (StreamST) requires determining appropriate timing, known as policy, to generate translations while continuously receiving source speech inputs, balancing low latency with high translation quality. However, existing StreamST methods typically operate on sentence-level speech segments, referred to as simultaneous speech translation (SimulST). In practice, they require collaboration with segmentation models to accomplish StreamST, where the truncated speech segments constrain SimulST models to make policy decisions and generate translations based on limited contextual information. Moreover, SimulST models struggle to learn effective policies due to the complexity of speech inputs and cross-lingual generation. To address these challenges, we propose StreamUni, which achieves StreamST through a unified Large Speech-Language Model (LSLM). Specifically, StreamUni incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate multi-stage outputs. Leveraging these multi-stage outputs, StreamUni simultaneously accomplishes speech segmentation, policy decision, and translation generation, completing StreamST without requiring massive policy-specific training. Additionally, we propose a streaming CoT training method that enhances low-latency policy decisions and generation capabilities using limited CoT data. Experiments demonstrate that our approach achieves state-of-the-art performance on StreamST tasks.</li>
<li><strong>摘要：</strong>流语音翻译（StreamST）要求确定适当的时机（称为策略），以生成翻译，同时连续接收源语音输入，从而平衡低潜伏期与高翻译质量。但是，现有的StreamST方法通常在句子级的语音段上运行，称为同时语音翻译（Simulst）。在实践中，他们需要与细分模型的合作来完成流，在此截断的语音段限制了Simulst模型以制定策略决策并基于有限的上下文信息生成翻译。此外，由于语音投入和跨语言产生的复杂性，Simulst模型很难学习有效的政策。为了应对这些挑战，我们提出了溪流，该流通过统一的大型语音语言模型（LSLM）实现了流。具体而言，Streamuni将语音链（COT）纳入引导LSLM以生成多阶段输出。利用这些多阶段的输出，同时完成语音细分，政策决策和翻译生成，完成流st，而无需进行大规模的政策特定培训。此外，我们提出了一种流式婴儿床培训方法，该方法使用有限的COT数据增强了低延迟的政策决策和发电能力。实验表明，我们的方法在StreamST任务上实现了最先进的性能。</li>
</ul>

<h3>Title: Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Nhi Hoai Doan, Tatsuya Hiraoka, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07810">https://arxiv.org/abs/2507.07810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07810">https://arxiv.org/pdf/2507.07810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07810]] Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning(https://arxiv.org/abs/2507.07810)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper investigates the relationship between large language models' (LLMs) ability to recognize repetitive input patterns and their performance on in-context learning (ICL). In contrast to prior work that has primarily focused on attention heads, we examine this relationship from the perspective of skill neurons, specifically repetition neurons. Our experiments reveal that the impact of these neurons on ICL performance varies depending on the depth of the layer in which they reside. By comparing the effects of repetition neurons and induction heads, we further identify strategies for reducing repetitive outputs while maintaining strong ICL capabilities.</li>
<li><strong>摘要：</strong>本文调查了大语言模型（LLMS）识别重复输入模式及其在信封学习（ICL）上的表现的能力之间的关系。与主要集中于注意力头的先前工作相反，我们从技能神经元的角度（特别是重复神经元）研究了这种关系。我们的实验表明，这些神经元对ICL性能的影响因其居住层的深度而异。通过比较重复神经元和诱导头的影响，我们进一步确定了减少重复输出的策略，同时保持强大的ICL能力。</li>
</ul>

<h3>Title: On the Effect of Instruction Tuning Loss on Generalization</h3>
<ul>
<li><strong>Authors: </strong>Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07817">https://arxiv.org/abs/2507.07817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07817">https://arxiv.org/pdf/2507.07817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07817]] On the Effect of Instruction Tuning Loss on Generalization(https://arxiv.org/abs/2507.07817)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Instruction Tuning has emerged as a pivotal post-training paradigm that enables pre-trained language models to better follow user instructions. Despite its significance, little attention has been given to optimizing the loss function used. A fundamental, yet often overlooked, question is whether the conventional auto-regressive objective - where loss is computed only on response tokens, excluding prompt tokens - is truly optimal for instruction tuning. In this work, we systematically investigate the impact of differentially weighting prompt and response tokens in instruction tuning loss, and propose Weighted Instruction Tuning (WIT) as a better alternative to conventional instruction tuning. Through extensive experiments on five language models of different families and scale, three finetuning datasets of different sizes, and five diverse evaluation benchmarks, we show that the standard instruction tuning loss often yields suboptimal performance and limited robustness to input prompt variations. We find that a low-to-moderate weight for prompt tokens coupled with a moderate-to-high weight for response tokens yields the best-performing models across settings and also serve as better starting points for the subsequent preference alignment training. These findings highlight the need to reconsider instruction tuning loss and offer actionable insights for developing more robust and generalizable models. Our code is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>指令调整已成为一个关键的训练后训练范式，该范式使训练训练的语言模型可以更好地遵循用户说明。尽管具有重要意义，但很少关注优化所使用的损失函数。一个基本的，但经常被忽视的问题是，常规自动回归目标 - 仅在响应令牌上计算损失（不包括及时令牌）是否确实是指导调整的最佳选择。在这项工作中，我们系统地研究了差异加权提示和响应令牌在教学调整损失中的影响，并提出加权指令调整（WIT），作为传统教学调整的更好替代方法。通过对不同家庭和规模的五个语言模型进行的大规模实验，三个不同大小的填充数据集以及五个不同的评估基准，我们表明，标准指令调谐损失通常会产生次优性能和对输入及时变化的鲁棒性。我们发现，迅速令牌的低到中度重量以及中等重量的响应令牌可产生跨环境的表现最佳的模型，并为随后的偏好比对训练提供更好的起点。这些发现凸显了需要重新考虑指导调整损失并提供可行的见解，以开发更健壮和可推广的模型。我们的代码在此HTTPS URL上开源。</li>
</ul>

<h3>Title: Conditional Unigram Tokenization with Parallel Data</h3>
<ul>
<li><strong>Authors: </strong>Gianluca Vico, Jindřinch Libovický</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07824">https://arxiv.org/abs/2507.07824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07824">https://arxiv.org/pdf/2507.07824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07824]] Conditional Unigram Tokenization with Parallel Data(https://arxiv.org/abs/2507.07824)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce conditional unigram tokenization, a novel approach that extends unigram tokenization by conditioning target token probabilities on source-language tokens from parallel data. Given a fixed source tokenizer, our method learns a target tokenizer that maximizes cross-lingual semantic alignment. We evaluate our tokenizer on four language pairs across different families and resource levels, examining intrinsic properties and downstream performance on machine translation and language modeling. While our conditional tokenizer maintains comparable statistical properties to standard unigram tokenizers, results are mixed: we observe no improvements in machine translation quality, but find consistent perplexity reductions in language modeling. We hypothesize that quadratic scaling of conditional probability estimation with respect to the vocabulary size creates a data efficiency bottleneck. Our findings suggest that alternative parameterizations may be necessary for practical cross-lingual tokenization.</li>
<li><strong>摘要：</strong>我们介绍了条件杂物令牌化，这是一种新颖的方法，通过从平行数据中调节源语言令牌的目标令牌概率来扩展杂物令牌。给定固定源令牌，我们的方法学习了一个目标令牌，可最大程度地提高跨语义语义对齐。我们在不同家庭和资源水平的四个语言对中评估了我们的令牌，并检查了机器翻译和语言建模的内在属性和下游性能。虽然我们的条件代币仪保持与标准杂物标记器的可比统计属性，但结果是混合的：我们观察到机器翻译质量没有改善，但是发现语言建模的一致性降低。我们假设有关词汇大小的条件概率估计的二次缩放会产生数据效率瓶颈。我们的发现表明，对于实际的跨语性令牌化，可能需要替代参数化。</li>
</ul>

<h3>Title: From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems</h3>
<ul>
<li><strong>Authors: </strong>Youngjoon Jang, Seongtae Hong, Junyoung Son, Sungjin Park, Chanjun Park, Heuiseok Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07847">https://arxiv.org/abs/2507.07847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07847">https://arxiv.org/pdf/2507.07847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07847]] From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems(https://arxiv.org/abs/2507.07847)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in natural language processing (NLP), improving factual consistency and reducing hallucinations by integrating external document retrieval with large language models (LLMs). However, the effectiveness of RAG is often hindered by coreferential complexity in retrieved documents, introducing ambiguity that disrupts in-context learning. In this study, we systematically investigate how entity coreference affects both document retrieval and generative performance in RAG-based systems, focusing on retrieval relevance, contextual understanding, and overall response quality. We demonstrate that coreference resolution enhances retrieval effectiveness and improves question-answering (QA) performance. Through comparative analysis of different pooling strategies in retrieval tasks, we find that mean pooling demonstrates superior context capturing ability after applying coreference resolution. In QA tasks, we discover that smaller models benefit more from the disambiguation process, likely due to their limited inherent capacity for handling referential ambiguity. With these findings, this study aims to provide a deeper understanding of the challenges posed by coreferential complexity in RAG, providing guidance for improving retrieval and generation in knowledge-intensive AI applications.</li>
<li><strong>摘要：</strong>检索增强的一代（RAG）已成为自然语言处理（NLP）的关键框架，通过将外部文档检索与大语言模型（LLMS）集成，从而提高了事实的一致性并减少了幻觉。但是，在检索的文档中，核心的复杂性通常会阻碍抹布的有效性，从而引入了破坏中文学习的歧义。在这项研究中，我们系统地研究了实体核心如何影响基于抹布的系统中的文档检索和生成性能，重点是检索相关性，上下文理解和整体响应质量。我们证明，核心分辨率提高了检索效率并提高了提问（QA）的绩效。通过对检索任务中不同合并策略的比较分析，我们发现平均池在应用核心分辨率后表现出了出色的上下文捕获能力。在质量检查任务中，我们发现较小的模型从歧义过程中受益更多，这可能是由于它们固有的处理能力有限。通过这些发现，这项研究旨在更深入地了解RAG中核心复杂性带来的挑战，从而为改善知识密集型AI应用中的检索和产生提供了指导。</li>
</ul>

<h3>Title: DocCHA: Towards LLM-Augmented Interactive Online diagnosis System</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Liu, Dachun Sun, Yi R. Fung, Dilek Hakkani-Tür, Tarek Abdelzaher</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07870">https://arxiv.org/abs/2507.07870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07870">https://arxiv.org/pdf/2507.07870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07870]] DocCHA: Towards LLM-Augmented Interactive Online diagnosis System(https://arxiv.org/abs/2507.07870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Despite the impressive capabilities of Large Language Models (LLMs), existing Conversational Health Agents (CHAs) remain static and brittle, incapable of adaptive multi-turn reasoning, symptom clarification, or transparent decision-making. This hinders their real-world applicability in clinical diagnosis, where iterative and structured dialogue is essential. We propose DocCHA, a confidence-aware, modular framework that emulates clinical reasoning by decomposing the diagnostic process into three stages: (1) symptom elicitation, (2) history acquisition, and (3) causal graph construction. Each module uses interpretable confidence scores to guide adaptive questioning, prioritize informative clarifications, and refine weak reasoning links. Evaluated on two real-world Chinese consultation datasets (IMCS21, DX), DocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5, GPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and over 30 percent improvement in symptom recall, with only modest increase in dialogue turns. These results demonstrate the effectiveness of DocCHA in enabling structured, transparent, and efficient diagnostic conversations -- paving the way for trustworthy LLM-powered clinical assistants in multilingual and resource-constrained settings.</li>
<li><strong>摘要：</strong>尽管大语言模型（LLM）具有令人印象深刻的能力，但现有的对话卫生代理商（CHAS）仍然保持静态和脆性，无法自适应多转弯推理，症状澄清或透明决策。这阻碍了他们在临床诊断中的现实适用性，在临床诊断中，迭代和结构化对话至关重要。我们提出了DOCCHA，这是一种具有信心的，模块化的框架，通过将诊断过程分解为三个阶段来模拟临床推理：（1）症状引起，（2）（2）历史记录获取和（3）因果图构造。每个模块都使用可解释的置信度得分来指导自适应质疑，优先考虑信息性澄清并完善弱推理链接。 Evaluated on two real-world Chinese consultation datasets (IMCS21, DX), DocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5, GPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and over 30 percent improvement in symptom recall, with only modest increase in dialogue turns.这些结果证明了DOCCHA在实现结构化，透明和有效的诊断对话方面的有效性 - 为可信赖的LLM驱动的临床助理铺平了道路，以进行多语言和资源约束的设置。</li>
</ul>

<h3>Title: Automating MD simulations for Proteins using Large language Models: NAMD-Agent</h3>
<ul>
<li><strong>Authors: </strong>Achuth Chandrasekhar, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07887">https://arxiv.org/abs/2507.07887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07887">https://arxiv.org/pdf/2507.07887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07887]] Automating MD simulations for Proteins using Large language Models: NAMD-Agent(https://arxiv.org/abs/2507.07887)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Molecular dynamics simulations are an essential tool in understanding protein structure, dynamics, and function at the atomic level. However, preparing high quality input files for MD simulations can be a time consuming and error prone process. In this work, we introduce an automated pipeline that leverages Large Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with python scripting and Selenium based web automation to streamline the generation of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based interface for preparing simulation-ready inputs for NAMD. By integrating Gemini's code generation and iterative refinement capabilities, simulation scripts are automatically written, executed, and revised to navigate CHARMM GUI, extract appropriate parameters, and produce the required NAMD input files. Post processing is performed using additional software to further refine the simulation outputs, thereby enabling a complete and largely hands free workflow. Our results demonstrate that this approach reduces setup time, minimizes manual errors, and offers a scalable solution for handling multiple protein systems in parallel. This automated framework paves the way for broader application of LLMs in computational structural biology, offering a robust and adaptable platform for future developments in simulation automation.</li>
<li><strong>摘要：</strong>分子动力学模拟是理解原子水平上蛋白质结构，动力学和功能的重要工具。但是，为MD模拟准备高质量的输入文件可能是一个耗时和易于错误的过程。在这项工作中，我们引入了一条自动化管道，该管道利用大型语言模型（LLMS），特别是Gemini 2.0 Flash，并结合Python脚本和基于Selenium的Web自动化，以简化MD输入文件的生成。该管道利用Charmm GUI的全面基于Web的界面来准备NAMD的模拟输入。通过集成Gemini的代码生成和迭代改进功能，模拟脚本会自动编写，执行和修订以导航Charmm GUI，提取适当的参数并产生所需的NAMD输入文件。使用其他软件执行后处理，以进一步完善模拟输出，从而实现完整且在很大程度上免费的工作流程。我们的结果表明，这种方法减少了设置时间，最大程度地减少了手动误差，并为处理多个蛋白质系统并联提供了可扩展的解决方案。该自动化框架为LLM在计算结构生物学中的更广泛应用铺平了道路，为未来的模拟自动化提供了强大且适应性的平台。</li>
</ul>

<h3>Title: DTECT: Dynamic Topic Explorer & Context Tracker</h3>
<ul>
<li><strong>Authors: </strong>Suman Adhya, Debarshi Kumar Sanyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07910">https://arxiv.org/abs/2507.07910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07910">https://arxiv.org/pdf/2507.07910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07910]] DTECT: Dynamic Topic Explorer & Context Tracker(https://arxiv.org/abs/2507.07910)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, chat</a></li>
<li><strong>Abstract: </strong>The explosive growth of textual data over time presents a significant challenge in uncovering evolving themes and trends. Existing dynamic topic modeling techniques, while powerful, often exist in fragmented pipelines that lack robust support for interpretation and user-friendly exploration. We introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end system that bridges the gap between raw textual data and meaningful temporal insights. DTECT provides a unified workflow that supports data preprocessing, multiple model architectures, and dedicated evaluation metrics to analyze the topic quality of temporal topic models. It significantly enhances interpretability by introducing LLM-driven automatic topic labeling, trend analysis via temporally salient words, interactive visualizations with document-level summarization, and a natural language chat interface for intuitive data querying. By integrating these features into a single, cohesive platform, DTECT empowers users to more effectively track and understand thematic dynamics. DTECT is open-source and available at this https URL.</li>
<li><strong>摘要：</strong>随着时间的流逝，文本数据的爆炸性增长在发现不断发展的主题和趋势方面提出了重大挑战。现有的动态主题建模技术虽然强大，但通常存在于零散的管道中，这些管道缺乏对解释和用户友好探索的强大支持。我们介绍了dtect（动态主题资源管理器和上下文跟踪器），这是一个端到端系统，它弥合了原始文本数据和有意义的时间洞察力之间的差距。 DTECT提供了一个统一的工作流，该工作流支持数据预处理，多个模型体系结构和专用评估指标，以分析时间主题模型的主题质量。它通过引入LLM驱动的自动主题标签，通过时间显着的单词，具有文档级摘要的交互式可视化以及用于直观数据查询的自然语言聊天界面来显着增强可解释性。通过将这些功能集成到一个凝聚力的平台中，DTECT使用户能够更有效地跟踪和理解主题动态。 DTECT是开源的，可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment</h3>
<ul>
<li><strong>Authors: </strong>Guoxin Zang, Xue Li, Donglin Di, Lanshun Nie, Dechen Zhan, Yang Song, Lei Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07939">https://arxiv.org/abs/2507.07939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07939">https://arxiv.org/pdf/2507.07939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07939]] SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment(https://arxiv.org/abs/2507.07939)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While Vision-Language Models (VLMs) have shown promising progress in general multimodal tasks, they often struggle in industrial anomaly detection and reasoning, particularly in delivering interpretable explanations and generalizing to unseen categories. This limitation stems from the inherently domain-specific nature of anomaly detection, which hinders the applicability of existing VLMs in industrial scenarios that require precise, structured, and context-aware analysis. To address these challenges, we propose SAGE, a VLM-based framework that enhances anomaly reasoning through Self-Guided Fact Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE integrates domain-specific knowledge into visual reasoning via fact extraction and fusion, while E-DPO aligns model outputs with expert preferences using entropy-aware optimization. Additionally, we introduce AD-PL, a preference-optimized dataset tailored for industrial anomaly reasoning, consisting of 28,415 question-answering instances with expert-ranked responses. To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation (MLE), a quantitative framework analyzing model logic and consistency. SAGE demonstrates superior performance on industrial anomaly datasets under zero-shot and one-shot settings. The code, model and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>尽管视觉模型（VLM）在一般的多模式任务中表现出了有希望的进步，但它们经常在工业异常检测和推理中挣扎，尤其是在提供可解释的解释和推广到看不见的类别方面。这种限制源于异常检测的固有特定于域特定的性质，这阻碍了现有VLM在需要精确，结构化和上下文感知分析的工业场景中的适用性。为了应对这些挑战，我们提出了SAGE，这是一种基于VLM的框架，通过自我引导的事实增强（SFE）和熵感知直接偏好优化（E-DPO）来增强异常推理。 SFE通过事实提取和融合将特定于域的知识集成到视觉推理中，而E-DPO将模型输出与熵感知优化的专家偏好相结合。此外，我们介绍了AD-PL，这是一种针对工业异常推理的偏好优化数据集，由28,415个带有专家级回答的问题的问题组成。为了评估异常推理模型，我们开发了多尺度逻辑评估（MLE），这是一个定量框架，分析了模型逻辑和一致性。 Sage在零射门和单次设置下的工业异常数据集上表现出了出色的性能。代码，模型和数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: MIRIX: Multi-Agent Memory System for LLM-Based Agents</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Xi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07957">https://arxiv.org/abs/2507.07957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07957">https://arxiv.org/pdf/2507.07957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07957]] MIRIX: Multi-Agent Memory System for LLM-Based Agents(https://arxiv.org/abs/2507.07957)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the field's most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy.</li>
<li><strong>摘要：</strong>尽管AI代理的记忆能力正在引起人们的关注，但现有的解决方案在根本上仍然有限。大多数人依靠平坦，狭窄的内存组件，限制其个性化，抽象和可靠地回忆用户特定信息的能力。为此，我们介绍了Mirix，这是一种模块化的多代理记忆系统，通过解决该领域最关键的挑战来重新定义AI内存的未来：使语言模型能够真正记住。与先前的方法不同，Mirix超越了文本，以拥抱丰富的视觉和多模式体验，使记忆在现实世界中确实有用。 Mirix由六种不同的，仔细的内存类型组成：核心，情节，语义，程序，资源存储器和知识库，再加上一个多代理框架，该框架动态控制和坐标更新和检索。该设计使代理商能够按大规模持续，推理并准确检索多样化的长期用户数据。我们在两个苛刻的设置中验证Mirix。首先，在ScreenShotVQA上，这是一个具有挑战性的多模式基准，包括每个序列近20,000个高分辨率的计算机屏幕截图，需要深层的上下文理解，并且在没有应用现有的内存系统的情况下，Mirix的准确性比RAG基线高35％，同时将存储需求降低99.9％。其次，在Locomo上，具有单模式文本输入的长期对话基准，Mirix的最新性能为85.4％，超过了现有的基线。这些结果表明，Mirix为内存启动的LLM代理设定了新的性能标准。为了允许用户体验我们的内存系统，我们提供了由Mirix提供动力的包装应用程序。它实时监视屏幕，建立个性化的内存基础，并提供直观的可视化和安全的本地存储空间以确保隐私。</li>
</ul>

<h3>Title: Why is Your Language Model a Poor Implicit Reward Model?</h3>
<ul>
<li><strong>Authors: </strong>Noam Razin, Yong Lin, Jiarui Yao, Sanjeev Arora</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07981">https://arxiv.org/abs/2507.07981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07981">https://arxiv.org/pdf/2507.07981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07981]] Why is Your Language Model a Poor Implicit Reward Model?(https://arxiv.org/abs/2507.07981)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reward models are key to language model post-training and inference pipelines. Conveniently, recent work showed that every language model defines an implicit reward model (IM-RM), without requiring any architectural changes. However, such IM-RMs tend to generalize worse, especially out-of-distribution, compared to explicit reward models (EX-RMs) that apply a dedicated linear head over the hidden representations of a language model. The existence of a generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They can be trained using the same data, loss function, and language model, and differ only in how the reward is computed. Towards a fundamental understanding of the implicit biases underlying different reward model types, we investigate the root cause of this gap. Our main finding, backed by theory and experiments, is that IM-RMs rely more heavily on superficial token-level cues. Consequently, they often generalize worse than EX-RMs under token-level distribution shifts, as well as in-distribution. Furthermore, we provide evidence against alternative hypotheses for the generalization gap. Most notably, we challenge the intuitive claim that IM-RMs struggle in tasks where generation is harder than verification because they can operate both as a verifier and a generator. Taken together, our results highlight that seemingly minor design choices can substantially impact the generalization behavior of reward models.</li>
<li><strong>摘要：</strong>奖励模型是语言模型训练后和推理管道的关键。方便的，最近的工作表明，每个语言模型都定义了隐性奖励模型（IM-RM），而无需任何建筑更改。但是，与明确的奖励模型（EX-RMS）相比，这种IM-RMS倾向于概括更糟，尤其是分发量，这些模型（EX-RMS）在语言模型的隐藏表示上应用了专用的线性头。概括差距的存在令人困惑，因为EX-RMS和IM-RM几乎相同。可以使用相同的数据，损失函数和语言模型对它们进行培训，并且只能在计算奖励的方式上有所不同。为了对不同奖励模型类型的隐性偏见有基本的理解，我们研究了这一差距的根本原因。在理论和实验的支持下，我们的主要发现是IM-RMS更严重地依赖于表面令牌级别的提示。因此，它们通常在令牌级别的分布变化和分布情况下概括比EX-RMS。此外，我们提供了反对概括差距的替代假设的证据。最值得注意的是，我们挑战了IM-RMS在发电比验证更难的任务中挣扎的直觉主张，因为它们可以作为验证者和发电机运行。综上所述，我们的结果表明，看似较小的设计选择可以大大影响奖励模型的概括行为。</li>
</ul>

<h3>Title: Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology</h3>
<ul>
<li><strong>Authors: </strong>Sabine Felde, Rüdiger Buchkremer, Gamal Chehab, Christian Thielscher, Jörg HW Distler, Matthias Schneider, Jutta G. Richter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07983">https://arxiv.org/abs/2507.07983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07983">https://arxiv.org/pdf/2507.07983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07983]] Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology(https://arxiv.org/abs/2507.07983)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show promise for supporting clinical decision-making in complex fields such as rheumatology. Our evaluation shows that smaller language models (SLMs), combined with retrieval-augmented generation (RAG), achieve higher diagnostic and therapeutic performance than larger models, while requiring substantially less energy and enabling cost-efficient, local deployment. These features are attractive for resource-limited healthcare. However, expert oversight remains essential, as no model consistently reached specialist-level accuracy in rheumatology.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）显示出支持在风湿病等复杂领域的临床决策的希望。我们的评估表明，较小的语言模型（SLM）与检索功能增强的生成（RAG）相比，获得更高的诊断和治疗性能，同时需要更少的能量并实现了较少的能量并实现了成本效益的本地部署。这些功能对资源有限的医疗保健有吸引力。但是，专家的监督仍然是必不可少的，因为没有模型始终达到风湿病学专家级的准确性。</li>
</ul>

<h3>Title: Automating Expert-Level Medical Reasoning Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuang Zhou, Wenya Xie, Jiaxi Li, Zaifu Zhan, Meijia Song, Han Yang, Cheyenna Espinoza, Lindsay Welton, Xinnie Mai, Yanwei Jin, Zidu Xu, Yuen-Hei Chung, Yiyun Xing, Meng-Han Tsai, Emma Schaffer, Yucheng Shi, Ninghao Liu, Zirui Liu, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07988">https://arxiv.org/abs/2507.07988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07988">https://arxiv.org/pdf/2507.07988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07988]] Automating Expert-Level Medical Reasoning Evaluation of Large Language Models(https://arxiv.org/abs/2507.07988)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into clinical decision-making, ensuring transparent and trustworthy reasoning is essential. However, existing evaluation strategies of LLMs' medical reasoning capability either suffer from unsatisfactory assessment or poor scalability, and a rigorous benchmark remains lacking. To address this, we introduce MedThink-Bench, a benchmark designed for rigorous, explainable, and scalable assessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging questions across ten medical domains, each annotated with expert-crafted step-by-step rationales. Building on this, we propose LLM-w-Ref, a novel evaluation framework that leverages fine-grained rationales and LLM-as-a-Judge mechanisms to assess intermediate reasoning with expert-level fidelity while maintaining scalability. Experiments show that LLM-w-Ref exhibits a strong positive correlation with expert judgments. Benchmarking twelve state-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can surpass larger proprietary counterparts (e.g., OpenAI-o3). Overall, MedThink-Bench offers a foundational tool for evaluating LLMs' medical reasoning, advancing their safe and responsible deployment in clinical practice.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLMS）越来越多地整合到临床决策中，确保透明和值得信赖的推理至关重要。但是，LLMS医学推理能力的现有评估策略要么遭受评估不足或可伸缩性不佳，而且仍然缺乏严格的基准。为了解决这个问题，我们介绍了Medthink Bench，这是一种基准，旨在对LLMS的医学推理进行严格，可解释和可扩展的评估。 Medthink Bench包括在十个医疗领域的500个具有挑战性的问题，每个问题都用专家制作的逐步理性注释。在此基础上，我们提出了LLM-W-REF，这是一个新型的评估框架，利用细粒度的理由和LLM-AS-A-A-Gudge机制来评估具有专家级保真度的中级推理，同时保持可伸缩性。实验表明，LLM-W-REF与专家判断表现出很强的正相关。通过基准进行十二个最先进的LLM，我们发现较小的模型（例如Medgemma-27b）可以超越较大的专有对应物（例如OpenAI-O3）。总体而言，Medthink-Bench提供了一种基础工具，用于评估LLMS的医疗推理，从而推进其在临床实践中的安全和负责任的部署。</li>
</ul>

<h3>Title: PyVision: Agentic Vision with Dynamic Tooling</h3>
<ul>
<li><strong>Authors: </strong>Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.07998">https://arxiv.org/abs/2507.07998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.07998">https://arxiv.org/pdf/2507.07998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.07998]] PyVision: Agentic Vision with Dynamic Tooling(https://arxiv.org/abs/2507.07998)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.</li>
<li><strong>摘要：</strong>LLM越来越多地部署为代理，能够计划，推理和动态调用外部工具的系统。但是，在视觉推理中，先验方法在很大程度上仍然受到预定义的工作流和静态工具集的限制。在本报告中，我们提出了Pyvision，这是一个交互式，多转变的框架，使MLLM可以自主生成，执行和完善基于Python的工具，该工具量身定制了手头任务，解释了灵活且可解释的问题解决方案。我们开发了Pyvision创建的工具的分类法，并在各种基准测试中分析它们的用法。数量上，Pyvision在V*上实现了一致的性能增长，在V*上将GPT-4.1提高 +7.8％，而Claude-4.0-Sonnet在VLMSAREBLIND MINI上提高了 +31.1％。这些结果表明，更广泛的转变：动态工具不仅可以使用工具，还可以使用它们来发明它们，朝着更具代理的视觉推理前进。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
