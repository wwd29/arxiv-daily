<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-07</h1>
<h3>Title: Interplay of Semantic Communication and Knowledge Learning</h3>
<ul>
<li><strong>Authors: </strong>Fei Ni, Bingyan Wang, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03339">https://arxiv.org/abs/2402.03339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03339">https://arxiv.org/pdf/2402.03339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03339]] Interplay of Semantic Communication and Knowledge Learning(https://arxiv.org/abs/2402.03339)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>In the swiftly advancing realm of communication technologies, Semantic Communication (SemCom), which emphasizes knowledge understanding and processing, has emerged as a hot topic. By integrating artificial intelligence technologies, SemCom facilitates a profound understanding, analysis and transmission of communication content. In this chapter, we clarify the means of knowledge learning in SemCom with a particular focus on the utilization of Knowledge Graphs (KGs). Specifically, we first review existing efforts that combine SemCom with knowledge learning. Subsequently, we introduce a KG-enhanced SemCom system, wherein the receiver is carefully calibrated to leverage knowledge from its static knowledge base for ameliorating the decoding performance. Contingent upon this framework, we further explore potential approaches that can empower the system to operate in evolving knowledge base more effectively. Furthermore, we investigate the possibility of integration with Large Language Models (LLMs) for data augmentation, offering additional perspective into the potential implementation means of SemCom. Extensive numerical results demonstrate that the proposed framework yields superior performance on top of the KG-enhanced decoding and manifests its versatility under different scenarios.</li>
<li><strong>摘要：</strong>在快速发展的通信技术领域，强调知识理解和处理的语义通信（SemCom）已成为一个热门话题。通过整合人工智能技术，SemCom 促进了对通信内容的深刻理解、分析和传输。在本章中，我们阐明了 SemCom 中知识学习的方法，特别关注知识图谱 (KG) 的利用。具体来说，我们首先回顾将 SemCom 与知识学习相结合的现有努力。随后，我们引入了 KG 增强型 SemCom 系统，其中接收器经过仔细校准，以利用其静态知识库中的知识来改善解码性能。根据这个框架，我们进一步探索潜在的方法，使系统能够更有效地在不断发展的知识库中运行。此外，我们还研究了与大型语言模型 (LLM) 集成以进行数据增强的可能性，为 SemCom 的潜在实现方式提供了额外的视角。大量的数值结果表明，所提出的框架在 KG 增强解码的基础上产生了优越的性能，并体现了其在不同场景下的多功能性。</li>
</ul>

<h3>Title: BetterV: Controlled Verilog Generation with Discriminative Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zehua Pei, Hui-Ling Zhen, Mingxuan Yuan, Yu Huang, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03375">https://arxiv.org/abs/2402.03375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03375">https://arxiv.org/pdf/2402.03375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03375]] BetterV: Controlled Verilog Generation with Discriminative Guidance(https://arxiv.org/abs/2402.03375)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen rising research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes the large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. The Verilog modules are collected, filtered and processed from internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tuned the LLMs to understand the knowledge about Verilog. Furthermore, data are augmented to enrich the training set and also used to train a generative discriminator on particular downstream task, which leads a guidance for the LLMs to optimize the Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, which can outperform GPT-4 on the VerilogEval-machine benchmark. With the help of task-specific generative discriminator, BetterV can achieve remarkable improvement on various electronic design automation (EDA) downstream tasks, including the netlist node reduction for synthesis and verification runtime reduction with Boolean Satisfiability (SAT) solving.</li>
<li><strong>摘要：</strong>由于现代集成电路（IC）的复杂性不断增加，因此需要自动化电路设计方法。近年来，为促进设计过程而生成硬件设计语言的研究不断增多。在这项工作中，我们提出了一个 Verilog 生成框架 BetterV，它可以在处理的特定领域数据集上微调大型语言模型 (LLM)，并结合生成判别器来指导特定的设计需求。 Verilog 模块从互联网上收集、过滤和处理，形成干净且丰富的数据集。指令调整方法是专门为微调法学硕士以理解 Verilog 知识而设计的。此外，数据被增强以丰富训练集，并用于训练特定下游任务的生成判别器，这为法学硕士优化 Verilog 实现提供了指导。 BetterV 能够生成语法和功能正确的 Verilog，在 VerilogEval-machine 基准测试中表现优于 GPT-4。借助特定于任务的生成判别器，BetterV 可以在各种电子设计自动化 (EDA) 下游任务上实现显着改进，包括通过布尔可满足性 (SAT) 求解减少综合的网表节点和减少验证运行时间。</li>
</ul>

<h3>Title: Delivery Optimized Discovery in Behavioral User Segmentation under  Budget Constrain</h3>
<ul>
<li><strong>Authors: </strong>Harshita Chopra, Atanu R. Sinha, Sunav Choudhary, Ryan A. Rossi, Paavan Kumar Indela, Veda Pranav Parwatala, Srinjayee Paul, Aurghya Maiti</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03388">https://arxiv.org/abs/2402.03388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03388">https://arxiv.org/pdf/2402.03388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03388]] Delivery Optimized Discovery in Behavioral User Segmentation under  Budget Constrain(https://arxiv.org/abs/2402.03388)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral user segmentation and offer new metrics to address the joint optimization. We leverage optimization under a budget constraint for delivery combined with a learning-based component for discovery. Extensive experiments on a public dataset from Google and a proprietary dataset show the effectiveness of our approach by simultaneously improving delivery metrics, reducing budget spend and achieving strong predictive performance in discovery.</li>
<li><strong>摘要：</strong>用户的在线行为足迹使公司能够发现基于行为的用户细分（或多个细分），并向用户传递细分特定的消息。在发现细分之后，通过 Facebook 和 Google 等首选媒体渠道向用户传递消息可能具有挑战性，因为行为细分中只有一部分用户在媒体中找到匹配，并且只有一小部分匹配的用户真正看到了消息（接触）。当交付失败时，即使高质量的发现也将变得徒劳。存在许多复杂的算法来发现行为片段；然而，这些忽略了交付组件。问题变得更加复杂，因为（i）发现是在公司数据中的行为数据空间（例如用户点击）上执行的，而交付是基于媒体定义的静态数据空间（例如地理位置、年龄）； (ii) 公司在预算约束下运作。我们引入了一种基于随机优化的算法，用于交付优化的行为用户细分发现，并提供新的指标来解决联合优化问题。我们利用预算约束下的优化来交付，并结合基于学习的发现组件。对 Google 的公共数据集和专有数据集进行的广泛实验表明，我们的方法通过同时改进交付指标、减少预算支出并在发现中实现强大的预测性能而有效。</li>
</ul>

<h3>Title: Psychological Assessments with Large Language Models: A Privacy-Focused  and Cost-Effective Approach</h3>
<ul>
<li><strong>Authors: </strong>Sergi Blanco-Cuaresma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03435">https://arxiv.org/abs/2402.03435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03435">https://arxiv.org/pdf/2402.03435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03435]] Psychological Assessments with Large Language Models: A Privacy-Focused  and Cost-Effective Approach(https://arxiv.org/abs/2402.03435)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of "open-source" LLMs that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.</li>
<li><strong>摘要：</strong>本研究探索使用大型语言模型（LLM）来分析 Reddit 用户的文本评论，旨在实现两个主要目标：首先，查明支持自杀风险预定义心理评估的关键摘录；其次，总结材料以证实预设的自杀风险水平。这项工作仅限于使用可以在本地运行的“开源”法学硕士，从而增强数据隐私。此外，它优先考虑计算要求低的模型，使计算预算有限的个人和机构都可以使用它。实施的策略仅依赖于精心设计的提示和语法来指导法学硕士的文本完成。尽管它很简单，但评估指标显示出出色的结果，使其成为一种有价值的注重隐私且具有成本效益的方法。这项工作是计算语言学和临床心理学 (CLPsych) 2024 共享任务的一部分。</li>
</ul>

<h3>Title: Decentralized Sporadic Federated Learning: A Unified Methodology with  Generalized Convergence Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Shahryar Zehtabi, Dong-Jun Han, Rohit Parasnis, Seyyedali Hosseinalipour, Christopher G. Brinton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03448">https://arxiv.org/abs/2402.03448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03448">https://arxiv.org/pdf/2402.03448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03448]] Decentralized Sporadic Federated Learning: A Unified Methodology with  Generalized Convergence Guarantees(https://arxiv.org/abs/2402.03448)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstrate that $\texttt{DSpodFL}$ achieves significantly improved training speeds and robustness to variations in system parameters compared to the state-of-the-art.</li>
<li><strong>摘要：</strong>去中心化联合学习 (DFL) 最近受到了广泛的研究关注，它捕获了由客户执行模型更新和模型聚合（两个关键 FL 过程）的设置。在这项工作中，我们提出了去中心化零星联合学习（$\texttt{DSpodFL}$），这是一种 DFL 方法，它概括了这两个过程中的零星性概念，对现实 DFL 设置中表现的不同形式的异质性的影响进行建模。 $\texttt{DSpodFL}$ 在单一建模框架下统一了许多著名的去中心化优化方法，例如分布式梯度下降（DGD）、随机八卦（RG）和去中心化联合平均（DFedAvg）。我们分析地描述了 $\texttt{DSpodFL}$ 的收敛行为，除其他见解外，表明我们可以在比现有工作更一般的假设下将几何收敛率与有限最优性差距相匹配。通过实验，我们证明与最先进的技术相比，$\texttt{DSpodFL}$ 显着提高了训练速度和对系统参数变化的鲁棒性。</li>
</ul>

<h3>Title: Efficient and Interpretable Traffic Destination Prediction using  Explainable Boosting Machines</h3>
<ul>
<li><strong>Authors: </strong>Yasin Yousif, Jörg Müller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03457">https://arxiv.org/abs/2402.03457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03457">https://arxiv.org/pdf/2402.03457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03457]] Efficient and Interpretable Traffic Destination Prediction using  Explainable Boosting Machines(https://arxiv.org/abs/2402.03457)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Developing accurate models for traffic trajectory predictions is crucial for achieving fully autonomous driving. Various deep neural network models have been employed to address this challenge, but their black-box nature hinders transparency and debugging capabilities in a deployed system. Glass-box models offer a solution by providing full interpretability through methods like \ac{GAM}. In this study, we evaluate an efficient additive model called \ac{EBM} for traffic prediction on three popular mixed traffic datasets: \ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models perform competitively in predicting pedestrian destinations within \ac{SDD} and \ac{InD} while providing modest predictions for vehicle-dominant Argoverse dataset. Additionally, our transparent trained models allow us to analyse feature importance and interactions, as well as provide qualitative examples of predictions explanation. The full training code will be made public upon publication.</li>
<li><strong>摘要：</strong>开发准确的交通轨迹预测模型对于实现完全自动驾驶至关重要。各种深度神经网络模型已被用来解决这一挑战，但它们的黑盒性质阻碍了已部署系统的透明度和调试能力。玻璃盒模型通过像 \ac{GAM} 这样的方法提供完全的可解释性，从而提供了一种解决方案。在本研究中，我们评估了一种名为 \ac{EBM} 的高效加性模型，用于在三个流行的混合流量数据集上进行流量预测：\ac{SDD}、\ac{InD} 和 Argoverse。我们的结果表明，\ac{EBM} 模型在预测 \ac{SDD} 和 \ac{InD} 内的行人目的地方面具有竞争力，同时为以车辆为主的 Argoverse 数据集提供适度的预测。此外，我们透明的训练模型使我们能够分析特征重要性和交互作用，并提供预测解释的定性示例。完整的培训代码将在发布后公开。</li>
</ul>

<h3>Title: Preference-free Alignment Learning with Regularized Relevance Reward</h3>
<ul>
<li><strong>Authors: </strong>Sungdong Kim, Minjoon Seo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03469">https://arxiv.org/abs/2402.03469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03469">https://arxiv.org/pdf/2402.03469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03469]] Preference-free Alignment Learning with Regularized Relevance Reward(https://arxiv.org/abs/2402.03469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require any human preference datasets (i.e., preference-free), outperforming open-source reward models in improving human preference. Our analysis demonstrates that $R^3$ has advantages in elevating human preference while minimizing its side effects. Finally, we show the generalizability of $R^3$, consistently improving instruction-tuned models in various backbones and sizes without additional dataset cost. Our code is available at https://github.com/naver-ai/RRR.</li>
<li><strong>摘要：</strong>从人类偏好中学习被认为是使大型语言模型 (LLM) 与人类价值观保持一致的关键。然而，与普遍的看法相反，我们的初步研究表明，在人类偏好数据集上训练的奖励模型往往会给长的偏离主题的回答比短的主题回答更高的分数。受这一观察的启发，我们探索了一种无偏好的方法，利用“相关性”作为对齐的关键目标。在我们的第一次尝试中，我们发现，当我们利用分数作为强化学习的奖励时，仅由检索器获得的相关性分数很容易受到奖励黑客攻击，即过度优化到不需要的捷径。为了缓解这个问题，我们将有效的归纳偏差整合到普通相关性中以相互规范化，从而产生奖励函数的混合：正则化相关性奖励（$R^3$）。 $R^3$ 通过提供强大的奖励信号，显着提高了偏好基准的性能。值得注意的是，$R^3$ 不需要任何人类偏好数据集（即无偏好），在改善人类偏好方面优于开源奖励模型。我们的分析表明，$R^3$ 在提高人类偏好同时最大限度地减少其副作用方面具有优势。最后，我们展示了 $R^3$ 的通用性，持续改进各种主干和大小的指令调整模型，而无需额外的数据集成本。我们的代码可在 https://github.com/naver-ai/RRR 获取。</li>
</ul>

<h3>Title: The Information of Large Language Model Geometry</h3>
<ul>
<li><strong>Authors: </strong>Zhiquan Tan, Chenghai Li, Weiran Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03471">https://arxiv.org/abs/2402.03471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03471">https://arxiv.org/pdf/2402.03471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03471]] The Information of Large Language Model Geometry(https://arxiv.org/abs/2402.03471)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific "meaningful" tokens alone.</li>
<li><strong>摘要：</strong>本文研究了大型语言模型（LLM）嵌入中编码的信息。我们进行模拟来分析表示熵并发现与模型大小的幂律关系。基于这一观察，我们提出了一种基于（条件）熵的理论来阐明标度律现象。此外，我们深入研究了 LLM 的自回归结构，并使用信息论和回归技术检查最后一个标记和先前上下文标记之间的关系。具体来说，我们在新令牌的信息增益和岭回归之间建立了理论联系。此外，我们还探讨了 Lasso 回归在选择有意义的标记方面的有效性，该回归有时优于密切相关的注意力权重。最后，我们进行了对照实验，发现信息分布在各个代币中，而不是仅仅集中在特定的“有意义”的代币中。</li>
</ul>

<h3>Title: Arabic Synonym BERT-based Adversarial Examples for Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Norah Alshahrani, Saied Alshahrani, Esma Wali, Jeanna Matthews</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03477">https://arxiv.org/abs/2402.03477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03477">https://arxiv.org/pdf/2402.03477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03477]] Arabic Synonym BERT-based Adversarial Examples for Text Classification(https://arxiv.org/abs/2402.03477)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Text classification systems have been proven vulnerable to adversarial text examples, modified versions of the original text examples that are often unnoticed by human eyes, yet can force text classification models to alter their classification. Often, research works quantifying the impact of adversarial text attacks have been applied only to models trained in English. In this paper, we introduce the first word-level study of adversarial attacks in Arabic. Specifically, we use a synonym (word-level) attack using a Masked Language Modeling (MLM) task with a BERT model in a black-box setting to assess the robustness of the state-of-the-art text classification models to adversarial attacks in Arabic. To evaluate the grammatical and semantic similarities of the newly produced adversarial examples using our synonym BERT-based attack, we invite four human evaluators to assess and compare the produced adversarial examples with their original examples. We also study the transferability of these newly produced Arabic adversarial examples to various models and investigate the effectiveness of defense mechanisms against these adversarial examples on the BERT models. We find that fine-tuned BERT models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained. We also find that fine-tuned BERT models were more susceptible to transferred attacks. We, lastly, find that fine-tuned BERT models successfully regain at least 2% in accuracy after applying adversarial training as an initial defense mechanism.</li>
<li><strong>摘要：</strong>文本分类系统已被证明容易受到对抗性文本示例的影响，原始文本示例的修改版本通常不会被人眼注意到，但可以迫使文本分类模型改变其分类。通常，量化对抗性文本攻击影响的研究工作仅适用于用英语训练的模型。在本文中，我们介绍了阿拉伯语对抗性攻击的第一个单词级研究。具体来说，我们在黑盒设置中使用带有 BERT 模型的掩码语言建模 (MLM) 任务进行同义词（词级）攻击，以评估最先进的文本分类模型对对抗性攻击的鲁棒性阿拉伯语。为了使用基于同义词 BERT 的攻击来评估新生成的对抗性示例的语法和语义相似性，我们邀请了四位人类评估者来评估和比较生成的对抗性示例与其原始示例。我们还研究了这些新生成的阿拉伯语对抗性示例到各种模型的可迁移性，并研究了 BERT 模型上针对这些对抗性示例的防御机制的有效性。我们发现，经过微调的 BERT 模型比我们训练的其他深度神经网络 (DNN) 模型（例如 WordCNN 和 WordLSTM）更容易受到同义词攻击。我们还发现，经过微调的 BERT 模型更容易受到转移攻击。最后，我们发现，在应用对抗性训练作为初始防御机制后，经过微调的 BERT 模型成功地恢复了至少 2% 的准确率。</li>
</ul>

<h3>Title: ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context  Environment Design</h3>
<ul>
<li><strong>Authors: </strong>Samuel Garcin, James Doran, Shangmin Guo, Christopher G. Lucas, Stefano V. Albrecht</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03479">https://arxiv.org/abs/2402.03479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03479">https://arxiv.org/pdf/2402.03479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03479]] ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context  Environment Design(https://arxiv.org/abs/2402.03479)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce in-context environment design (ICED). ICED generates levels using a variational autoencoder trained over an initial set of level parameters, reducing distributional shift, and achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods.</li>
<li><strong>摘要：</strong>使用深度强化学习（RL）训练的自主代理通常缺乏成功泛化到新环境的能力，即使它们与训练期间遇到的环境具有相同的特征。在这项工作中，我们研究了单个环境实例或级别的采样如何影响 RL 代理的零样本泛化 (ZSG) 能力。我们发现，对于共享基础层的深层行动者-批评家架构，根据其价值损失对级别进行优先级排序可以最大限度地减少代理的内部表示与生成的训练数据中的训练级别集之间的相互信息。这为通过某些自适应采样策略实现的隐式正则化提供了新颖的理论依据。然后我们将注意力转向无监督环境设计（UED）方法，它对数据生成机制有更多的控制。我们发现现有的 UED 方法可以显着改变训练分布，从而导致 ZSG 性能较低。为了防止过度拟合和分布偏移，我们引入了上下文环境设计（ICED）。 ICED 使用在初始级别参数集上训练的变分自动编码器生成级别，减少分布偏移，并在 ZSG 中实现相对于自适应级别采样策略和 UED 方法的显着改进。</li>
</ul>

<h3>Title: SWAG: Storytelling With Action Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zeeshan Patel, Karim El-Refai, Jonathan Pei, Tianle Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03483">https://arxiv.org/abs/2402.03483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03483">https://arxiv.org/pdf/2402.03483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03483]] SWAG: Storytelling With Action Guidance(https://arxiv.org/abs/2402.03483)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best "action" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.</li>
<li><strong>摘要：</strong>自动长篇故事生成通常采用长上下文大语言模型 (LLM) 进行一次性创作，这可以生成有凝聚力但不一定引人入胜的内容。我们引入行动指导讲故事（SWAG），这是一种与法学硕士一起讲故事的新颖方法。我们的方法通过两个模型反馈循环将故事写作简化为搜索问题：一个LLM生成故事内容，另一个辅助LLM用于选择下一个最佳“行动”来引导故事的未来方向。我们的结果表明，当通过 GPT-4 和人工评估进行评估时，SWAG 可以大大优于以前的端到端故事生成技术，并且我们仅使用开源模型的 SWAG 管道超越了 GPT-3.5-Turbo。</li>
</ul>

<h3>Title: Beyond Text: Improving LLM's Decision Making for Robot Navigation via  Vocal Cues</h3>
<ul>
<li><strong>Authors: </strong>Xingpeng Sun, Haoming Meng, Souradip Chakraborty, Amrit Singh Bedi, Aniket Bera</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03494">https://arxiv.org/abs/2402.03494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03494">https://arxiv.org/pdf/2402.03494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03494]] Beyond Text: Improving LLM's Decision Making for Robot Navigation via  Vocal Cues(https://arxiv.org/abs/2402.03494)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present "Beyond Text"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach not only achieves a 70.26% winning rate, outperforming existing LLMs by 48.30%, but also enhances robustness against token manipulation adversarial attacks, highlighted by a 22.44% less decrease ratio than the text-only language model in winning rate. "Beyond Text" marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models.</li>
<li><strong>摘要：</strong>这项工作强调了用于人机交互的基于文本的大型语言模型（LLM）的一个关键缺点，表明仅将文本作为对话模式在此类应用中是不够的。虽然法学硕士擅长处理人类对话中的文本，但他们在社交导航等场景中难以应对口头指令的细微差别，其中模糊性和不确定性可能会削弱对机器人和其他人工智能系统的信任。我们可以通过超越文本并另外关注这些音频响应的副语言特征来解决这个缺点。这些特征是口语交流的各个方面，不涉及字面措辞（词汇内容），而是通过说话方式传达含义和细微差别。我们呈现“超越文本”；一种通过将音频转录与这些功能的一部分集成来改进法学硕士决策的方法，这些功能侧重于影响并且在人机对话中更相关。该方法不仅获得了 70.26% 的胜率，比现有的 LLM 提高了 48.30%，而且还增强了针对 token 操纵对抗性攻击的鲁棒性，胜率下降率比纯文本语言模型减少了 22.44%。 “超越文本”标志着社交机器人导航和更广泛的人机交互的进步，将基于文本的指导与人类音频信息语言模型无缝集成。</li>
</ul>

<h3>Title: Partially Stochastic Infinitely Deep Bayesian Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Sergio Calvo-Ordonez, Matthieu Meunier, Francesco Piatti, Yuantao Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03495">https://arxiv.org/abs/2402.03495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03495">https://arxiv.org/pdf/2402.03495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03495]] Partially Stochastic Infinitely Deep Bayesian Neural Networks(https://arxiv.org/abs/2402.03495)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators. Lastly, empirical evaluations across multiple tasks show that our proposed architectures achieve better downstream task performance and uncertainty quantification than their counterparts while being significantly more efficient.</li>
<li><strong>摘要：</strong>在本文中，我们提出了部分随机无限深贝叶斯神经网络，这是一种新颖的架构家族，它将部分随机性集成到无限深神经网络的框架中。我们的新型架构旨在改善现有架构在训练和推理时的计算效率方面的局限性。为此，我们利用无限深度限制中部分随机性的优势，其中包括完全随机性的优势，例如鲁棒性、不确定性量化和内存效率，同时改善训练和推理时计算效率的局限性。我们提供了各种架构配置，为网络设计提供了灵活性，包括不同的权重划分方法。我们还通过确定我们的网络系列符合通用条件分布近似器的条件，为我们的模型的表达能力提供数学保证。最后，跨多个任务的实证评估表明，我们提出的架构比同类架构实现了更好的下游任务性能和不确定性量化，同时效率显着提高。</li>
</ul>

<h3>Title: How Does Unlabeled Data Provably Help Out-of-Distribution Detection?</h3>
<ul>
<li><strong>Authors: </strong>Xuefeng Du, Zhen Fang, Ilias Diakonikolas, Yixuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03502">https://arxiv.org/abs/2402.03502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03502">https://arxiv.org/pdf/2402.03502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03502]] How Does Unlabeled Data Provably Help Out-of-Distribution Detection?(https://arxiv.org/abs/2402.03502)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier. Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights. Code is publicly available at https://github.com/deeplearning-wisc/sal.</li>
<li><strong>摘要：</strong>使用未标记的数据来规范机器学习模型已证明有望提高检测分布外（OOD）数据的安全性和可靠性。由于分布内 (ID) 和 OOD 数据的异质性，利用未标记的野外数据的力量并非易事。缺乏一组干净的 OOD 样本给学习最佳 OOD 分类器带来了重大挑战。目前，缺乏正式理解未标记数据如何帮助 OOD 检测的研究。本文通过引入新的学习框架 SAL（Separate And Learn）来弥补这一差距，该框架提供了强有力的理论保证和实证有效性。该框架将候选离群值与未标记数据分开，然后使用候选离群值和标记 ID 数据训练 OOD 分类器。理论上，我们从可分离性和可学习性的角度提供了严格的误差界限，正式证明了我们算法中的两个组成部分的合理性。我们的理论表明，SAL 可以以较小的错误率分离候选异常值，从而为学习到的 OOD 分类器提供泛化保证。根据经验，SAL 在通用基准上实现了最先进的性能，增强了我们的理论见解。代码可在 https://github.com/deeplearning-wisc/sal 上公开获取。</li>
</ul>

<h3>Title: Neural networks for abstraction and reasoning: Towards broad  generalization in machines</h3>
<ul>
<li><strong>Authors: </strong>Mikel Bober-Irizar, Soumya Banerjee</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03507">https://arxiv.org/abs/2402.03507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03507">https://arxiv.org/pdf/2402.03507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03507]] Neural networks for abstraction and reasoning: Towards broad  generalization in machines(https://arxiv.org/abs/2402.03507)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task. First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition. We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks. LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. We perform an ensemble analysis, combining models to achieve better results than any system alone. Finally, we publish the arckit Python library to make future research on ARC easier.</li>
<li><strong>摘要：</strong>半个世纪以来，人工智能研究一直试图重现人类抽象和推理的品质——创建可以在人类认为很容易的环境中从最少的示例中学习新概念的计算机系统。虽然特定的神经网络能够解决一系列令人印象深刻的问题，但事实证明，对其训练数据之外的情况进行广泛的推广是难以捉摸的。在这项工作中，我们研究了几种解决抽象与推理语料库（ARC）的新方法，ARC是一个数据集引入抽象视觉推理任务来测试广泛泛化的算法。尽管举办了三届奖金为 10 万美元的国际竞赛，但最好的算法仍然无法解决大部分 ARC 任务，并且依赖于复杂的手工规则，根本没有使用机器学习。我们重新审视神经网络的最新进展是否允许这项任务取得进展。首先，我们将 DreamCoder 神经符号推理求解器应用于 ARC。 DreamCoder 自动以定制的特定领域语言编写程序来执行推理，使用神经网络模仿人类直觉。我们提出了感知抽象和推理语言（PeARL）语言，它允许 DreamCoder 解决 ARC 任务，并提出了一种新的识别模型，使我们能够在之前的最佳实现上进行显着改进。我们还提出了一种新的编码和增强方案，允许大型语言模型（LLM）来解决ARC任务，发现最大的模型可以解决一些ARC任务。法学硕士能够解决与最先进的求解器不同的一组问题，并提供一种有趣的方式来补充其他方法。我们进行集成分析，结合模型以获得比单独任何系统更好的结果。最后，我们发布了 arckit Python 库，以便将来对 ARC 的研究变得更加容易。</li>
</ul>

<h3>Title: Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains</h3>
<ul>
<li><strong>Authors: </strong>Sanjana Ramprasad, Kundan Krishna, Zachary C Lipton, Byron C Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03509">https://arxiv.org/abs/2402.03509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03509">https://arxiv.org/pdf/2402.03509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03509]] Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains(https://arxiv.org/abs/2402.03509)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotations to facilitate additional research toward measuring and realizing factually accurate summarization, beyond news articles. The dataset can be downloaded from https://github.com/sanjanaramprasad/zero_shot_faceval_domains</li>
<li><strong>摘要：</strong>最近的工作表明，大型语言模型（LLM）能够生成零样本（即没有明确监督）的摘要，在人工评估下，这些摘要通常与手动撰写的参考摘要相当甚至更受欢迎。然而，这项先前的工作几乎完全集中于评估新闻文章摘要。零样本摘要器在其他（可能更专业的）领域表现如何？在这项工作中，我们评估了跨专业领域的零样本生成的摘要，包括生物医学文章和法律法案（以及供参考的标准新闻基准）。我们特别关注产出的真实性。我们从领域专家那里获取注释，以识别摘要中的不一致之处，并对这些错误进行系统分类。我们分析预训练语料库中给定领域的流行是否影响该领域文章生成摘要的提取性和忠实度。我们发布所有收集的注释，以促进更多研究，以衡量和实现除新闻文章之外的事实准确的摘要。该数据集可以从 https://github.com/sanjanaramprasad/zero_shot_faceval_domains 下载</li>
</ul>

<h3>Title: Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical  System for Punctuation Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xiliang Zhu, Chia-Tien Chang, Shayna Gardiner, David Rossouw, Jonas Robertson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03519">https://arxiv.org/abs/2402.03519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03519">https://arxiv.org/pdf/2402.03519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03519]] Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical  System for Punctuation Restoration(https://arxiv.org/abs/2402.03519)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benefits from our proposed system.</li>
<li><strong>摘要：</strong>标点符号恢复是自动语音识别 (ASR) 系统之后的关键步骤，可增强转录本的可读性并促进后续的 NLP 任务。然而，传统的基于词汇的方法不足以解决西班牙语中的标点符号恢复任务，因为在西班牙语中，不加标点的陈述语和疑问句之间经常存在歧义。在这项研究中，我们提出了一种用于西班牙语转录的新型混合声学-词汇标点符号恢复系统，该系统通过模块化过程整合声学和词汇信号。我们的实验结果表明，所提出的系统可以有效提高公共和内部西班牙语会话数据集上问号的 F1 分数和整体标点符号恢复。此外，与 LLM（大型语言模型）的基准比较表明了我们的方法在准确性、可靠性和延迟方面的优越性。此外，我们还证明了 ASR 模块的字错误率 (WER) 也受益于我们提出的系统。</li>
</ul>

<h3>Title: Fairness and Privacy Guarantees in Federated Contextual Bandits</h3>
<ul>
<li><strong>Authors: </strong>Sambhav Solanki, Shweta Jain, Sujit Gujar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03531">https://arxiv.org/abs/2402.03531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03531">https://arxiv.org/pdf/2402.03531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03531]] Fairness and Privacy Guarantees in Federated Contextual Bandits(https://arxiv.org/abs/2402.03531)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper considers the contextual multi-armed bandit (CMAB) problem with fairness and privacy guarantees in a federated environment. We consider merit-based exposure as the desired fair outcome, which provides exposure to each action in proportion to the reward associated. We model the algorithm's effectiveness using fairness regret, which captures the difference between fair optimal policy and the policy output by the algorithm. Applying fair CMAB algorithm to each agent individually leads to fairness regret linear in the number of agents. We propose that collaborative -- federated learning can be more effective and provide the algorithm Fed-FairX-LinUCB that also ensures differential privacy. The primary challenge in extending the existing privacy framework is designing the communication protocol for communicating required information across agents. A naive protocol can either lead to weaker privacy guarantees or higher regret. We design a novel communication protocol that allows for (i) Sub-linear theoretical bounds on fairness regret for Fed-FairX-LinUCB and comparable bounds for the private counterpart, Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our proposed algorithm with extensive simulations-based experiments. We show that both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness regret.</li>
<li><strong>摘要：</strong>本文考虑了联邦环境中具有公平性和隐私保证的上下文多臂老虎机（CMAB）问题。我们将基于绩效的曝光视为期望的公平结果，它按照相关奖励的比例提供每项行动的曝光。我们使用公平遗憾对算法的有效性进行建模，它捕获了公平最优策略与算法输出的策略之间的差异。将公平的 CMAB 算法单独应用于每个代理会导致公平遗憾与代理的数量成线性关系。我们建议协作联合学习可以更有效，并提供算法 Fed-FairX-LinUCB，该算法也可以确保差异隐私。扩展现有隐私框架的主要挑战是设计用于跨代理通信所需信息的通信协议。幼稚的协议要么会导致隐私保障较弱，要么会导致更大的遗憾。我们设计了一种新颖的通信协议，它允许（i）Fed-FairX-LinUCB 的公平遗憾的亚线性理论界限和私人对应物 Priv-FairX-LinUCB （相对于单代理学习）的可比界限，（ii ) Priv-FairX-LinUCB 中隐私预算的有效利用。我们通过广泛的基于模拟的实验证明了我们提出的算法的有效性。我们证明 Fed-FairX-LinUCB 和 Priv-FairX-LinUCB 都实现了接近最优的公平遗憾。</li>
</ul>

<h3>Title: Regulation Games for Trustworthy Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Yaghini, Patty Liu, Franziska Boenisch, Nicolas Papernot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03540">https://arxiv.org/abs/2402.03540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03540">https://arxiv.org/pdf/2402.03540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03540]] Regulation Games for Trustworthy Machine Learning(https://arxiv.org/abs/2402.03540)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>Existing work on trustworthy machine learning (ML) often concentrates on individual aspects of trust, such as fairness or privacy. Additionally, many techniques overlook the distinction between those who train ML models and those responsible for assessing their trustworthiness. To address these issues, we propose a framework that views trustworthy ML as a multi-objective multi-agent optimization problem. This naturally lends itself to a game-theoretic formulation we call regulation games. We illustrate a particular game instance, the SpecGame in which we model the relationship between an ML model builder and fairness and privacy regulators. Regulators wish to design penalties that enforce compliance with their specification, but do not want to discourage builders from participation. Seeking such socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of their objectives and avoids the inefficiencies of other equilibria. Simulating SpecGame through ParetoPlay can provide policy guidance for ML Regulation. For instance, we show that for a gender classification application, regulators can enforce a differential privacy budget that is on average 4.0 lower if they take the initiative to specify their desired guarantee first.</li>
<li><strong>摘要：</strong>现有的可信机器学习 (ML) 工作通常集中在信任的各个方面，例如公平性或隐私。此外，许多技术忽视了训练机器学习模型的人和负责评估其可信度的人之间的区别。为了解决这些问题，我们提出了一个框架，将可信机器学习视为多目标多代理优化问题。这自然适合我们称为调节博弈的博弈论表述。我们展示了一个特定的游戏实例，即 SpecGame，我们在其中对 ML 模型构建器与公平和隐私监管机构之间的关系进行建模。监管机构希望设计惩罚措施，强制遵守其规范，但不想阻止建筑商参与。为了寻求这种社会最优（即对所有代理都有效）的游戏解决方案，我们引入了 ParetoPlay。这种新颖的均衡搜索算法确保代理保持在其目标的帕累托前沿，并避免其他均衡的低效率。通过 ParetoPlay 模拟 SpecGame 可以为机器学习监管提供政策指导。例如，我们表明，对于性别分类应用程序，如果监管机构首先主动指定其所需的保证，则可以执行平均低 4.0 的差别隐私预算。</li>
</ul>

<h3>Title: HAMLET: Graph Transformer Neural Operator for Partial Differential  Equations</h3>
<ul>
<li><strong>Authors: </strong>Andrey Bryutkin, Jiahao Huang, Zhongying Deng, Guang Yang, Carola-Bibiane Schönlieb, Angelica Aviles-Rivero</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03541">https://arxiv.org/abs/2402.03541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03541">https://arxiv.org/pdf/2402.03541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03541]] HAMLET: Graph Transformer Neural Operator for Partial Differential  Equations(https://arxiv.org/abs/2402.03541)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的图转换器框架 HAMLET，旨在解决使用神经网络求解偏微分方程 (PDE) 的挑战。该框架使用带有模块化输入编码器的图形转换器来直接将微分方程信息合并到求解过程中。这种模块化增强了参数对应控制，使 HAMLET 能够适应任意几何形状和各种输入格式的偏微分方程。值得注意的是，HAMLET 随着数据复杂性和噪声的增加而有效地扩展，展示了其稳健性。 HAMLET 不仅适合单一类型的物理模拟，而且可以应用于各个领域。此外，它还提高了模型的弹性和性能，特别是在数据有限的场景中。我们通过大量实验证明，我们的框架能够超越当前的偏微分方程技术。</li>
</ul>

<h3>Title: Online Feature Updates Improve Online (Generalized) Label Shift  Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Wu, Siddhartha Datta, Yi Su, Dheeraj Baby, Yu-Xiang Wang, Kilian Q. Weinberger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03545">https://arxiv.org/abs/2402.03545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03545">https://arxiv.org/pdf/2402.03545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03545]] Online Feature Updates Improve Online (Generalized) Label Shift  Adaptation(https://arxiv.org/abs/2402.03545)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.</li>
<li><strong>摘要：</strong>本文解决了缺少标签的在线环境中标签移位的普遍问题，其中数据分布随着时间的推移而变化，并且及时获取标签具有挑战性。虽然现有方法主要侧重于调整或更新预训练分类器的最后一层，但我们探索在测试时使用未标记数据增强特征表示的未开发潜力。我们的新颖方法是在线特征更新的在线标签转移适应（OLS-OFU），利用自我监督学习来完善特征提取过程，从而改进预测模型。理论分析证实，OLS-OFU 通过利用自监督学习进行特征细化来减少算法遗憾。在在线标签移位和广义标签移位条件下对各种数据集的实证研究强调了 OLS-OFU 的有效性和鲁棒性，特别是在域移位的情况下。</li>
</ul>

<h3>Title: Path Signatures and Graph Neural Networks for Slow Earthquake Analysis:  Better Together?</h3>
<ul>
<li><strong>Authors: </strong>Hans Riess, Manolis Veveakis, Michael M. Zavlanos</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03558">https://arxiv.org/abs/2402.03558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03558">https://arxiv.org/pdf/2402.03558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03558]] Path Signatures and Graph Neural Networks for Slow Earthquake Analysis:  Better Together?(https://arxiv.org/abs/2402.03558)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths. On the other hand, graph neural networks (GNN), neural architectures for processing data on graphs, excel on tasks with irregular domains, such as sensor networks. In this paper, we introduce a novel approach, Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path signatures into graph convolutional neural networks (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions. We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand's north island. We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon. Our methodology shows promise for future advancement in earthquake prediction and sensor network analysis.</li>
<li><strong>摘要：</strong>路径签名最近在机器学习社区取得了成功，是一种从不规则路径工程特征的理论驱动方法。另一方面，图神经网络（GNN），即用于处理图数据的神经架构，在具有不规则域的任务上表现出色，例如传感器网络。在本文中，我们介绍了一种新颖的方法，即路径签名图卷积神经网络（PS-GCNN），将路径签名集成到图卷积神经网络（GCNN）中，并利用路径签名和 GCNN 的优点进行特征提取，用于处理空间交互。我们应用我们的方法来分析慢地震序列，也称为慢滑动事件 (SSE)，利用 GPS 时间序列的数据，并以新西兰北岛东海岸的 GPS 传感器网络为案例研究。我们还为模拟随机微分方程的方法建立了基准，该方程模拟了类似的反应扩散现象。我们的方法显示了地震预测和传感器网络分析未来发展的前景。</li>
</ul>

<h3>Title: Distinguishing the Knowable from the Unknowable with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, Benjamin L. Edelman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03563">https://arxiv.org/abs/2402.03563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03563">https://arxiv.org/pdf/2402.03563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03563]] Distinguishing the Knowable from the Unknowable with Language Models(https://arxiv.org/abs/2402.03563)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings.</li>
<li><strong>摘要：</strong>我们研究了在自由格式文本的大型语言模型（LLM）输出中识别认知不确定性（反映知识缺乏）的可行性，而不是随意不确定性（反映潜在分布中的熵）。在缺乏真实概率的情况下，我们探索了一种设置，为了（大约）解开给定法学硕士的不确定性，我们使用一个更大的模型作为真实概率的代理。我们表明，在冻结的预训练模型的嵌入上训练的小型线性探针可以准确地预测较大的模型何时在标记级别上更加自信，并且在一个文本域上训练的探针可以推广到其他文本域。更进一步，我们提出了一种完全无监督的方法，可以在同一任务上实现不平凡的准确性。总而言之，我们将这些结果解释为证据，表明法学硕士自然包含不同类型不确定性的内部表示，这些不确定性可以用来在不同的实际环境中设计出更多信息丰富的模型置信度指标。</li>
</ul>

<h3>Title: Toward Human-AI Alignment in Large-Scale Multi-Player Games</h3>
<ul>
<li><strong>Authors: </strong>Sugandha Sharma, Guy Davidson, Khimya Khetarpal, Anssi Kanervisto, Udit Arora, Katja Hofmann, Ida Momennejad</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03575">https://arxiv.org/abs/2402.03575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03575">https://arxiv.org/pdf/2402.03575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03575]] Toward Human-AI Alignment in Large-Scale Multi-Player Games(https://arxiv.org/abs/2402.03575)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity. Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns. These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications. Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming.</li>
<li><strong>摘要：</strong>在复杂的多智能体游戏中实现人类与人工智能的协调对于创建可增强游戏玩法的值得信赖的人工智能智能体至关重要。我们提出了一种使用可解释的任务集框架来评估这种一致性的方法，重点关注高级行为任务而不是低级策略。我们的方法由三个组成部分组成。首先，我们分析来自 Xbox 的 Bleeding Edge（10 万多个游戏）的大量人类游戏数据，揭示复杂任务空间中的行为模式。该任务空间作为捕获可解释轴的行为流形的基础集：战斗-逃跑、探索-利用和单人-多智能体。其次，我们使用生成式预训练因果变压器训练 AI 代理来玩《Bleeding Edge》并测量其行为。第三，我们将人类和人工智能的游戏玩法投射到所提出的行为流形中进行比较和对比。这使我们能够将政策的差异解释为更高层次的行为概念，例如，我们发现，虽然人类玩家在战斗-逃跑和探索-利用行为方面表现出可变性，但人工智能玩家倾向于一致性。此外，人工智能代理主要从事单人游戏，而人类经常参与合作和竞争的多代理模式。这些明显的差异强调了在人性化应用中对人工智能进行可解释的评估、设计和集成的需求。我们的研究推进了人工智能尤其是生成式人工智能研究中的对齐讨论，为多人游戏中可解释的人类代理对齐提供了可测量的框架。</li>
</ul>

<h3>Title: A Reinforcement Learning Approach for Dynamic Rebalancing in  Bike-Sharing System</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liang, Sanjay Dominik Jena, Defeng Liu, Andrea Lodi</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03589">https://arxiv.org/abs/2402.03589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03589">https://arxiv.org/pdf/2402.03589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03589]] A Reinforcement Learning Approach for Dynamic Rebalancing in  Bike-Sharing System(https://arxiv.org/abs/2402.03589)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles. Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations. Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators. As a promising alternative to classical mathematical optimization, reinforcement learning is gaining ground to solve sequential decision-making problems. This paper introduces a spatio-temporal reinforcement learning algorithm for the dynamic rebalancing problem with multiple vehicles. We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework. This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle departures are synchronized. A comprehensive simulator under the first-arrive-first-serve rule is then developed to facilitate the learning process by computing immediate rewards under diverse demand scenarios. To estimate the value function and learn the rebalancing policy, various Deep Q-Network configurations are tested, minimizing the lost demand. Experiments are carried out on various datasets generated from historical data, affected by both temporal and weather factors. The proposed algorithms outperform benchmarks, including a multi-period Mixed-Integer Programming model, in terms of lost demand. Once trained, it yields immediate decisions, making it suitable for real-time applications. Our work offers practical insights for operators and enriches the integration of reinforcement learning into dynamic rebalancing problems, paving the way for more intelligent and robust urban mobility solutions.</li>
<li><strong>摘要：</strong>自行车共享系统提供环保的城市交通，有助于缓解交通拥堵和更健康的生活方式。由于出行需求的随机性，导致车站满员或空车，有效运营此类系统并保持较高的客户满意度具有挑战性。因此，对于运营商来说，制定有效的重新平衡策略，使用车辆在车站之间重新分配自行车至关重要。作为经典数学优化的一种有前途的替代方案，强化学习在解决顺序决策问题方面正在取得进展。本文介绍了一种用于多车辆动态再平衡问题的时空强化学习算法。我们首先将问题表述为连续时间框架中的多智能体马尔可夫决策过程。这允许独立和协作的车辆重新平衡，消除车辆出发同步的时间离散模型的不切实际的限制。然后开发了一个遵循先到先服务规则的综合模拟器，通过计算不同需求场景下的即时奖励来促进学习过程。为了估计价值函数并学习再平衡策略，测试了各种深度 Q 网络配置，以最大限度地减少需求损失。在受时间和天气因素影响的历史数据生成的各种数据集上进行了实验。就需求损失而言，所提出的算法优于基准，包括多周期混合整数规划模型。一旦经过训练，它就会立即做出决策，使其适合实时应用。我们的工作为运营商提供了实用的见解，并丰富了强化学习与动态再平衡问题的整合，为更智能、更强大的城市交通解决方案铺平了道路。</li>
</ul>

<h3>Title: Assessing the Impact of Distribution Shift on Reinforcement Learning  Performance</h3>
<ul>
<li><strong>Authors: </strong>Ted Fujimoto, Joshua Suetterlein, Samrat Chatterjee, Auroop Ganguly</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03590">https://arxiv.org/abs/2402.03590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03590">https://arxiv.org/pdf/2402.03590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03590]] Assessing the Impact of Distribution Shift on Reinforcement Learning  Performance(https://arxiv.org/abs/2402.03590)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dynamic environments allow us to make stronger assumptions to justify the measurement of causal impact in our evaluations. We then apply these tools to single-agent and multi-agent environments to show the impact of introducing distribution shifts during test time. We present this methodology as a first step toward rigorous RL evaluation in the presence of distribution shifts.</li>
<li><strong>摘要：</strong>机器学习研究在解决其自身的可重复性危机方面正在取得进展。强化学习（RL）尤其面临着一系列独特的挑战。点估计的比较以及显示训练期间成功收敛到最优策略的图可能会混淆过度拟合或对实验设置的依赖。尽管强化学习领域的研究人员提出了可靠性指标来解释不确定性，以更好地了解每种算法的优点和缺点，但过去工作的建议并没有假设存在分布外观测值。我们提出了一套评估方法来衡量 RL 算法在分布变化下的鲁棒性。这里介绍的工具认为，当代理在其环境中行动时，需要考虑一段时间内的性能。我们特别推荐时间序列分析作为观察性 RL 评估的方法。我们还表明，强化学习和模拟动态环境的独特属性使我们能够做出更有力的假设，以证明我们评估中因果影响测量的合理性。然后，我们将这些工具应用于单代理和多代理环境，以显示在测试期间引入分布变化的影响。我们将这种方法作为在存在分布变化的情况下进行严格强化学习评估的第一步。</li>
</ul>

<h3>Title: Identifying Reasons for Contraceptive Switching from Real-World Data  Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Brenda Y. Miao, Christopher YK Williams, Ebenezer Chinedu-Eneh, Travis Zack, Emily Alsentzer, Atul J. Butte, Irene Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03597">https://arxiv.org/abs/2402.03597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03597">https://arxiv.org/pdf/2402.03597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03597]] Identifying Reasons for Contraceptive Switching from Real-World Data  Using Large Language Models(https://arxiv.org/abs/2402.03597)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination, code, rag</a></li>
<li><strong>Abstract: </strong>Prescription contraceptives play a critical role in supporting women's reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucinations. Using extracted reasons, we identified patient preference, adverse events, and insurance as key reasons for switching using unsupervised topic modeling approaches. Notably, we also showed using our approach that "weight gain/mood change" and "insurance coverage" are disproportionately found as reasons for contraceptive switching in specific demographic populations. Our code and supplemental data are available at https://github.com/BMiao10/contraceptive-switching.</li>
<li><strong>摘要：</strong>处方避孕药在支持妇女生殖健康方面发挥着关键作用。美国有近 5000 万女性使用避孕药具，了解推动避孕药具选择和更换的因素具有重要意义。然而，与药物转换相关的许多因素通常只能在非结构化的临床记录中捕获，并且很难提取。在这里，我们评估了最近开发的大型语言模型 GPT-4（通过符合 HIPAA 的 Microsoft Azure API）的零样本能力，以从 UCSF 信息共享临床记录数据集中确定避孕药具类别之间切换的原因。我们证明，GPT-4 可以准确地提取避孕药更换的原因，优于基于 BERT 的基线模型，避孕开始和停止提取的 microF1 分数分别为 0.849 和 0.881。对 GPT-4 提取的转换原因的人类评估显示出 91.4% 的准确度，且幻觉极少。通过提取的原因，我们确定患者偏好、不良事件和保险是改用无监督主题建模方法的关键原因。值得注意的是，我们还使用我们的方法表明，“体重增加/情绪变化”和“保险覆盖范围”被不成比例地视为特定人口群体中避孕药具转换的原因。我们的代码和补充数据可在 https://github.com/BMiao10/contraceptive-switching 上获取。</li>
</ul>

<h3>Title: Improving Contextual Congruence Across Modalities for Effective  Multimodal Marketing using Knowledge-infused Learning</h3>
<ul>
<li><strong>Authors: </strong>Trilok Padhi, Ugur Kursuncu, Yaman Kumar, Valerie L. Shalin, Lane Peterson Fronczek</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CV, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03607">https://arxiv.org/abs/2402.03607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03607">https://arxiv.org/pdf/2402.03607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03607]] Improving Contextual Congruence Across Modalities for Effective  Multimodal Marketing using Knowledge-infused Learning(https://arxiv.org/abs/2402.03607)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.</li>
<li><strong>摘要：</strong>能够捕捉多种模式瞬间的智能设备的普及使用户能够在线体验多模式信息。然而，大型语言（LLM）和视觉模型（LVM）在捕获具有跨模态语义关系的整体意义方面仍然受到限制。如果没有明确的常识知识（例如，作为知识图），视觉语言模型（VLM）只能通过捕获大量语料库中的高级模式来学习隐式表示，从而缺少必要的上下文跨模式线索。在这项工作中，我们设计了一个框架，将知识图形式的明确常识知识与大型 VLM 结合起来，以提高下游任务的性能，预测多模式营销活动的有效性。虽然营销应用程序为评估我们的方法提供了令人信服的指标，但我们的方法能够及早发现可能有说服力的多模式活动以及评估和增强营销理论。</li>
</ul>

<h3>Title: RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal  LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, Yang You</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03610">https://arxiv.org/abs/2402.03610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03610">https://arxiv.org/pdf/2402.03610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03610]] RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal  LLM Agents(https://arxiv.org/abs/2402.03610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.</li>
<li><strong>摘要：</strong>由于最近的进步，大型语言模型 (LLM) 现在可以部署为机器人、游戏和 API 集成等领域日益复杂的决策应用程序的代理。然而，在当前决策过程中反映过去的经验（人类固有的行为）仍然构成重大挑战。为了解决这个问题，我们提出了检索增强规划（RAP）框架，旨在动态地利用与当前情况和背景相对应的过去经验，从而增强代理的规划能力。 RAP 的特点是用途广泛：它在纯文本环境和多模式环境中都表现出色，适合各种任务。实证评估证明了 RAP 的有效性，它在文本场景中实现了 SOTA 性能，并显着增强了多模态 LLM 代理在具体任务中的性能。这些结果凸显了 RAP 在复杂的现实应用中提升 LLM 代理的功能和适用性方面的潜力。</li>
</ul>

<h3>Title: Leveraging Large Language Models for Hybrid Workplace Decision Support</h3>
<ul>
<li><strong>Authors: </strong>Yujin Kim, Chin-Chia Hsu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03616">https://arxiv.org/abs/2402.03616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03616">https://arxiv.org/pdf/2402.03616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03616]] Leveraging Large Language Models for Hybrid Workplace Decision Support(https://arxiv.org/abs/2402.03616)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs. We first examine LLM's capability of making suitable workspace suggestions. We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker's decision could be influenced by the LLM's suggestions and explanations. The participants in our study find the system to be convenient, regardless of whether reasons are provided or not. Our results show that employees can benefit from the LLM-empowered system for their workspace selection in hybrid workplace.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 具有执行各种文本处理任务并为提议的操作或决策提供文本解释的潜力。在混合工作时代，法学硕士可以为正在设计混合工作计划的员工提供智能决策支持。特别是，他们可以为工人提供平衡众多决策因素的建议和解释，从而增强他们的工作经验。在本文中，我们利用法学硕士的推理技能，提出了混合工作环境中工作空间的决策支持模型。我们首先考察LLM提出合适工作空间建议的能力。我们发现它的推理超出了提示中的指导方针，法学硕士可以管理工作空间中可用资源之间的权衡。我们进行了广泛的用户研究，以了解员工选择工作空间的决策过程并评估系统的有效性。我们观察到，工人的决定可能会受到法学硕士的建议和解释的影响。我们研究的参与者发现该系统很方便，无论是否提供理由。我们的结果表明，员工可以从法学硕士授权的系统中受益，以选择混合工作场所的工作空间。</li>
</ul>

<h3>Title: Comparing Abstraction in Humans and Large Language Models Using  Multimodal Serial Reproduction</h3>
<ul>
<li><strong>Authors: </strong>Sreejan Kumar, Raja Marjieh, Byron Zhang, Declan Campbell, Michael Y. Hu, Umang Bhatt, Brenden Lake, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03618">https://arxiv.org/abs/2402.03618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03618">https://arxiv.org/pdf/2402.03618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03618]] Comparing Abstraction in Humans and Large Language Models Using  Multimodal Serial Reproduction(https://arxiv.org/abs/2402.03618)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.</li>
<li><strong>摘要：</strong>人类从嘈杂的感官数据中提取对世界有用的抽象。串行复制使我们能够通过类似于电话游戏的范式来研究人们如何解释世界，其中一个人观察刺激并将其复制给下一个，形成复制链。过去的连续繁殖实验通常采用单一的感觉方式，但人类经常通过语言相互交流世界的抽象概念。为了研究语言对抽象形成的影响，我们通过要求接受视觉刺激的人以语言格式再现它来实现一种新颖的多模式串行再现框架，反之亦然。我们对人类和 GPT-4 运行单模态和多模态链，发现添加语言作为模态对人类繁殖的影响比 GPT-4 更大。这表明人类视觉和语言表征比 GPT-4 更具分离性。</li>
</ul>

<h3>Title: Self-Discover: Large Language Models Self-Compose Reasoning Structures</h3>
<ul>
<li><strong>Authors: </strong>Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven Zheng</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03620">https://arxiv.org/abs/2402.03620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03620">https://arxiv.org/pdf/2402.03620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03620]] Self-Discover: Large Language Models Self-Compose Reasoning Structures(https://arxiv.org/abs/2402.03620)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.</li>
<li><strong>摘要：</strong>我们引入了SELF-DISCOVER，这是法学硕士自我发现任务内在推理结构的通用框架，以解决对典型提示方法具有挑战性的复杂推理问题。该框架的核心是一个自我发现的过程，法学硕士选择批判性思维、分步思维等多个原子推理模块，并将它们组合成明确的推理结构，供法学硕士在解码时遵循。 SELF-DISCOVER 大幅提高了 GPT-4 和 PaLM 2 在具有挑战性的推理基准（例如 BigBench-Hard、扎根代理推理和 MATH）上的性能，与 Chain of Thought (CoT) 相比，性能提高了 32%。此外，SELF-DISCOVER 的性能比 CoT-Self-Consistency 等推理密集型方法高出 20% 以上，同时所需的推理计算量减少了 10-40 倍。最后，我们表明，自我发现的推理结构普遍适用于各个模型系列：从 PaLM 2-L 到 GPT-4，从 GPT-4 到 Llama2，并且与人类推理模式具有共性。</li>
</ul>

<h3>Title: Partially Recentralization Softmax Loss for Vision-Language Models  Robustness</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Xin Zhang, Jinzhe Jiang, Yaqian Zhao, Chen Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03627">https://arxiv.org/abs/2402.03627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03627">https://arxiv.org/pdf/2402.03627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03627]] Partially Recentralization Softmax Loss for Vision-Language Models  Robustness(https://arxiv.org/abs/2402.03627)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after this paper is accepted</li>
<li><strong>摘要：</strong>随着大型语言模型在自然语言处理任务（NLP）中取得突破，多模态技术变得非常流行。然而，事实证明，多模态 NLP 很容易受到对抗性攻击，其中模型的输出可能会因输入的扰动而发生巨大变化。虽然计算机视觉和 NLP 模型中已经提出了几种防御技术，但模型的多模态鲁棒性尚未得到充分探索。在本文中，我们通过限制前 K 个 softmax 输出来研究通过修改预训练多模态模型的损失函数来提供的对抗鲁棒性。基于评估和评分，我们的实验表明，经过微调后，预训练模型的对抗鲁棒性可以显着提高，以对抗流行的攻击。还需要进一步研究，例如此类损失函数的输出多样性、泛化性以及鲁棒性与性能的权衡。我们的代码将在本文被接受后可用</li>
</ul>

<h3>Title: Professional Agents -- Evolving Large Language Models into Autonomous  Experts with Human-Level Competencies</h3>
<ul>
<li><strong>Authors: </strong>Zhixuan Chu, Yan Wang, Feng Zhu, Lu Yu, Longfei Li, Jinjie Gu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03628">https://arxiv.org/abs/2402.03628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03628">https://arxiv.org/pdf/2402.03628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03628]] Professional Agents -- Evolving Large Language Models into Autonomous  Experts with Human-Level Competencies(https://arxiv.org/abs/2402.03628)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.</li>
<li><strong>摘要：</strong>ChatGPT、PaLM 和 GPT-4 等大型语言模型 (LLM) 的出现促进了自然语言处理领域的显着进步，展示了类似人类的语言流利度和推理能力。本立场文件介绍了专业代理 (PAgents) 的概念，这是一个利用 LLM 功能来创建具有可控、专业、交互式和专业级能力的自主代理的应用程序框架。我们认为 PAgents 可以通过不断发展的专业知识重塑专业服务。我们提出的 PAgents 框架需要一个用于起源、进化和协同的三层架构：基础工具层、中间代理层和顶层协同层。本文旨在激发人们对法学硕士在现实世界中的应用前景的讨论。我们认为，PAgent 的日益复杂和集成可能会导致人工智能系统展现出对复杂领域的专业掌握，满足关键需求，并有可能实现通用人工智能。</li>
</ul>

<h3>Title: torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability  Problem</h3>
<ul>
<li><strong>Authors: </strong>Abdelrahman Hosny, Sherief Reda</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03640">https://arxiv.org/abs/2402.03640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03640">https://arxiv.org/pdf/2402.03640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03640]] torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability  Problem(https://arxiv.org/abs/2402.03640)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The remarkable achievements of machine learning techniques in analyzing discrete structures have drawn significant attention towards their integration into combinatorial optimization algorithms. Typically, these methodologies improve existing solvers by injecting learned models within the solving loop to enhance the efficiency of the search process. In this work, we derive a single differentiable function capable of approximating solutions for the Maximum Satisfiability Problem (MaxSAT). Then, we present a novel neural network architecture to model our differentiable function, and progressively solve MaxSAT using backpropagation. This approach eliminates the need for labeled data or a neural network training phase, as the training process functions as the solving algorithm. Additionally, we leverage the computational power of GPUs to accelerate these computations. Experimental results on challenging MaxSAT instances show that our proposed methodology outperforms two existing MaxSAT solvers, and is on par with another in terms of solution cost, without necessitating any training or access to an underlying SAT solver. Given that numerous NP-hard problems can be reduced to MaxSAT, our novel technique paves the way for a new generation of solvers poised to benefit from neural network GPU acceleration.</li>
<li><strong>摘要：</strong>机器学习技术在分析离散结构方面取得的显著成就引起了人们对将其集成到组合优化算法中的极大关注。通常，这些方法通过在求解循环中注入学习模型来改进现有的求解器，以提高搜索过程的效率。在这项工作中，我们推导了一个能够逼近最大可满足性问题（MaxSAT）解决方案的单个可微函数。然后，我们提出了一种新颖的神经网络架构来对可微函数进行建模，并使用反向传播逐步求解 MaxSAT。这种方法消除了对标记数据或神经网络训练阶段的需要，因为训练过程充当求解算法。此外，我们利用 GPU 的计算能力来加速这些计算。在具有挑战性的 MaxSAT 实例上的实验结果表明，我们提出的方法优于两种现有的 MaxSAT 求解器，并且在解决方案成本方面与另一种方法相当，无需任何培训或访问底层 SAT 求解器。鉴于许多 NP 难题可以简化为 MaxSAT，我们的新技术为新一代求解器铺平了道路，准备从神经网络 GPU 加速中受益。</li>
</ul>

<h3>Title: Lens: A Foundation Model for Network Traffic</h3>
<ul>
<li><strong>Authors: </strong>Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Huajie Shao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03646">https://arxiv.org/abs/2402.03646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03646">https://arxiv.org/pdf/2402.03646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03646]] Lens: A Foundation Model for Network Traffic(https://arxiv.org/abs/2402.03646)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from large-scale network traffic. To further enhance pre-training performance, we design a novel loss that integrates three distinct tasks, namely Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation results on multiple benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and traffic generation. Notably, it also requires considerably less labeled data for fine-tuning compared to current methods.</li>
<li><strong>摘要：</strong>网络流量是指通过互联网或任何连接计算机的系统发送和接收的信息量。分析和了解网络流量对于提高网络安全和管理至关重要。然而，由于数据包的独特特征，例如异构标头和缺乏语义的加密负载，对网络流量的分析提出了巨大的挑战。为了捕获流量的潜在语义，一些研究采用了基于 Transformer 编码器或解码器的预训练技术来学习大规模流量数据的表示。然而，这些方法通常仅在流量理解（分类）或流量生成任务中表现出色。为了解决这个问题，我们开发了 Lens，这是一种基础网络流量模型，它利用 T5 架构从大规模未标记数据中学习预先训练的表示。利用编码器-解码器框架的优势，在保留生成能力的同时捕获全局信息，我们的模型可以更好地学习大规模网络流量的表示。为了进一步提高预训练性能，我们设计了一种新颖的损失函数，它集成了三个不同的任务，即掩蔽跨度预测（MSP）、数据包顺序预测（POP）和同源流量预测（HTP）。对多个基准数据集的评估结果表明，所提出的 Lens 在与流量理解和流量生成相关的大多数下游任务中优于基线。值得注意的是，与当前方法相比，它还需要少得多的用于微调的标记数据。</li>
</ul>

<h3>Title: CAMBranch: Contrastive Learning with Augmented MILPs for Branching</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Lin, Meng Xu, Zhihua Xiong, Huangang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03647">https://arxiv.org/abs/2402.03647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03647">https://arxiv.org/pdf/2402.03647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03647]] CAMBranch: Contrastive Learning with Augmented MILPs for Branching(https://arxiv.org/abs/2402.03647)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\&B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for \textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions. Experimental results demonstrate that CAMBranch, trained with only 10\% of the complete dataset, exhibits superior performance. Ablation studies further validate the effectiveness of our method.</li>
<li><strong>摘要：</strong>最近的进展引入了机器学习框架来增强用于解决混合整数线性规划 (MILP) 的分支定界 (B\&B) 分支策略。这些方法主要依赖于强分支的模仿学习，已显示出优越的性能。然而，收集用于模仿学习的专家样本，尤其是强分支，是一项耗时的工作。为了应对这一挑战，我们提出了使用 \textbf{A} 增强型 \textbf{M}ILP 进行 \textbf{Branch}ing (CAMBranch) 的对比学习，这是一个通过应用变量生成增强 MILP (AMILP) 的框架从原始 MILP 转向有限的专家数据。这种方法能够获取大量标记的专家样本。 CAMBranch 利用 MILP 和 AMILP 进行模仿学习，并采用对比学习来增强模型捕获 MILP 特征的能力，从而提高分支决策的质量。实验结果表明，仅使用完整数据集的 10% 进行训练的 CAMBranch 就表现出了卓越的性能。消融研究进一步验证了我们方法的有效性。</li>
</ul>

<h3>Title: Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Kun Ouyang, Liqiang Jing, Xuemeng Song, Meng Liu, Yupeng Hu, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03658">https://arxiv.org/abs/2402.03658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03658">https://arxiv.org/pdf/2402.03658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03658]] Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue(https://arxiv.org/abs/2402.03658)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which aims to generate a natural language explanation for the given sarcastic dialogue that involves multiple modalities (i.e., utterance, video, and audio). Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which are vital clues for sarcasm explanation. In fact, it is non-trivial to incorporate sentiments for boosting SED performance, due to three main challenges: 1) diverse effects of utterance tokens on sentiments; 2) gap between video-audio sentiment signals and the embedding space of BART; and 3) various relations among utterances, utterance sentiments, and video-audio sentiments. To tackle these challenges, we propose a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation framework, named EDGE. In particular, we first propose a lexicon-guided utterance sentiment inference module, where a heuristic utterance sentiment refinement strategy is devised. We then develop a module named Joint Cross Attention-based Sentiment Inference (JCA-SI) by extending the multimodal sentiment analysis model JCA to derive the joint sentiment label for each video-audio clip. Thereafter, we devise a context-sentiment graph to comprehensively model the semantic relations among the utterances, utterance sentiments, and video-audio sentiments, to facilitate sarcasm explanation generation. Extensive experiments on the publicly released dataset WITS verify the superiority of our model over cutting-edge methods.</li>
<li><strong>摘要：</strong>对话中的讽刺解释（SED）是一项新的但具有挑战性的任务，旨在为涉及多种模式（即话语、视频和音频）的给定讽刺对话生成自然语言解释。尽管现有的基于生成预训练语言模型 BART 的研究取得了巨大成功，但它们忽视了对话语、视频和音频中存在的情感的利用，而这些情感是讽刺解释的重要线索。事实上，由于三个主要挑战，纳入情绪来提高 SED 性能并非易事：1）话语标记对情绪的不同影响； 2）视音频情感信号与BART嵌入空间之间的差距； 3）话语、话语情感和视频音频情感之间的各种关系。为了应对这些挑战，我们提出了一种新颖的基于情感增强图的多模态讽刺解释框架，名为 EDGE。特别是，我们首先提出了一个词典引导的话语情感推理模块，其中设计了启发式话语情感细化策略。然后，我们通过扩展多模态情感分析模型 JCA 来开发一个名为基于联合交叉注意的情感推理（JCA-SI）的模块，以导出每个视频音频剪辑的联合情感标签。此后，我们设计了一个上下文情感图来全面建模话语、话语情感和视频音频情感之间的语义关系，以促进讽刺解释的生成。对公开发布的数据集 WITS 进行的大量实验验证了我们的模型相对于前沿方法的优越性。</li>
</ul>

<h3>Title: Learning to Generate Explainable Stock Predictions using Self-Reflective  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kelvin J.L. Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03659">https://arxiv.org/abs/2402.03659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03659">https://arxiv.org/pdf/2402.03659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03659]] Learning to Generate Explainable Stock Predictions using Self-Reflective  Large Language Models(https://arxiv.org/abs/2402.03659)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) framework, which utilizes a self-reflective agent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to generate explainable stock predictions in a fully autonomous manner. The reflective agent learns how to explain past stock movements through self-reasoning, while the PPO trainer trains the model to generate the most likely explanations from input texts. The training samples for the PPO trainer are also the responses generated during the reflective process, which eliminates the need for human annotators. Using our SEP framework, we fine-tune a LLM that can outperform both traditional deep-learning and LLM methods in prediction accuracy and Matthews correlation coefficient for the stock classification task. To justify the generalization capability of our framework, we further test it on the portfolio construction task, and demonstrate its effectiveness through various portfolio metrics.</li>
<li><strong>摘要：</strong>对于传统的非生成深度学习模型来说，解释股票预测通常是一项艰巨的任务，其中解释仅限于可视化重要文本的注意力权重。如今，大型语言模型 (LLM) 提出了这个问题的解决方案，因为它们具有为决策过程生成人类可读解释的已知能力。然而，股票预测的任务对于法学硕士来说仍然具有挑战性，因为它需要能够权衡混乱的社会文本对股票价格的不同影响。随着解释部分的引入，问题变得越来越困难，这要求法学硕士口头解释为什么某些因素比其他因素更重要。另一方面，为了针对此类任务对法学硕士进行微调，需要对训练集中的每个股票走势进行专家注释的解释样本，这既昂贵又不切实际。为了解决这些问题，我们提出了总结-解释-预测（SEP）框架，该框架利用自我反思代理和近端策略优化（PPO）来让法学硕士自学如何以完全自主的方式生成可解释的股票预测。反思代理学习如何通过自我推理来解释过去的股票走势，而 PPO 训练器则训练模型从输入文本中生成最可能的解释。 PPO 训练者的训练样本也是反思过程中生成的响应，这消除了对人工注释者的需要。使用我们的 SEP 框架，我们对 LLM 进行了微调，使其在股票分类任务的预测准确性和马修斯相关系数方面优于传统深度学习和 LLM 方法。为了证明我们框架的泛化能力，我们在投资组合构建任务上进一步测试它，并通过各种投资组合指标证明其有效性。</li>
</ul>

<h3>Title: Transductive Reward Inference on Graph</h3>
<ul>
<li><strong>Authors: </strong>Bohao Qu, Xiaofeng Cao, Qing Guo, Yi Chang, Ivor W. Tsang, Chengqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03661">https://arxiv.org/abs/2402.03661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03661">https://arxiv.org/pdf/2402.03661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03661]] Transductive Reward Inference on Graph(https://arxiv.org/abs/2402.03661)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach. The application of our inferred rewards improves the performance in offline reinforcement learning tasks.</li>
<li><strong>摘要：</strong>在本研究中，我们在奖励信息传播图上提出了一种转导推理方法，该方法能够有效估计离线强化学习中未标记数据的奖励。奖励推理是在实际场景中学习有效策略的关键，而直接的环境交互要么成本太高，要么不道德，而且奖励函数很少可用，例如在医疗保健和机器人技术中。我们的研究重点是开发一种基于图上信息传播的上下文属性的奖励推断方法，该方法利用有限数量的人类奖励注释来推断未标记数据的奖励。我们利用可用数据和有限的奖励注释来构建奖励传播图，其中边权重包含与奖励有关的各种影响因素。随后，我们使用构建的图进行传导奖励推理，从而估计未标记数据的奖励。此外，我们在转导推理过程的多次迭代期间确定了不动点的存在，并证明其至少收敛于局部最优。对运动和机器人操作任务的实证评估验证了我们方法的有效性。我们推断的奖励的应用提高了离线强化学习任务的性能。</li>
</ul>

<h3>Title: Large Language Models as an Indirect Reasoner: Contrapositive and  Contradiction for Automated Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yanfang Zhang, Yiliu Sun, Yibing Zhan, Dapeng Tao, Dacheng Tao, Chen Gong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03667">https://arxiv.org/abs/2402.03667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03667">https://arxiv.org/pdf/2402.03667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03667]] Large Language Models as an Indirect Reasoner: Contrapositive and  Contradiction for Automated Reasoning(https://arxiv.org/abs/2402.03667)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs. The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33% and mathematical proof by 31.43%, when compared with traditional DR methods. Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy.</li>
<li><strong>摘要：</strong>最近，人们越来越关注提高大型语言模型（LLM）执行复杂推理的能力。然而，以前的方法，例如思想链和自洽性，主要遵循直接推理（DR）框架，因此它们在解决许多现实世界的任务时会遇到困难，而这些任务很难通过直接推理来解决。因此，为了增强法学硕士的推理能力，本文提出了一种新颖的间接推理（IR）方法，利用反证和矛盾的逻辑来处理事实推理和数学证明等IR任务。具体来说，我们的方法包括两个步骤。首先，我们利用反证的逻辑等价来扩充数据和规则，以增强法学硕士的可理解性。其次，我们设计了一套提示模板来触发LLM进行基于反证法的IR，逻辑上与原始DR过程等效。我们的 IR 方法简单而有效，可以直接与现有的 DR 方法集成，以进一步提高法学硕士的推理能力。在流行的LLM（例如GPT-3.5-turbo和Gemini-pro）上的实验结果表明，与传统的DR方法相比，我们的IR方法将事实推理的整体准确性提高了27.33％，数学证明的整体准确性提高了31.43％。此外，结合 IR 和 DR 的方法显着优于单独使用 IR 或 DR 的方法，进一步证明了我们策略的有效性。</li>
</ul>

<h3>Title: Logical Specifications-guided Dynamic Task Sampling for Reinforcement  Learning Agents</h3>
<ul>
<li><strong>Authors: </strong>Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert Wright, Jivko Sinapov</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03678">https://arxiv.org/abs/2402.03678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03678">https://arxiv.org/pdf/2402.03678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03678]] Logical Specifications-guided Dynamic Task Sampling for Reinforcement  Learning Agents(https://arxiv.org/abs/2402.03678)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshold performance on complex sequential decision-making problems compared to state-of-the-art RM and Automaton-guided RL baselines, such as Q-Learning for Reward Machines and Compositional RL from logical Specifications (DIRL). Moreover, we demonstrate that our method outperforms RM and Automaton-guided RL baselines in terms of sample-efficiency, both in a partially observable robotic task and in a continuous control robotic manipulation task.</li>
<li><strong>摘要：</strong>强化学习 (RL) 在使人工智能体学习不同行为方面取得了重大进展。然而，学习有效的策略通常需要大量的环境交互。为了减轻样本复杂性问题，最近的方法使用了高级任务规范，例如线性时序逻辑（LTL$_f$）公式或奖励机（RM）来指导代理的学习进度。在这项工作中，我们提出了一种称为逻辑规范引导动态任务采样（LSTS）的新颖方法，该方法学习一组 RL 策略，以基于高级任务规范指导代理从初始状态到目标状态，同时最大限度地减少环境相互作用的次数。与之前的工作不同，LSTS 不假设有关环境动态或奖励机器的信息，而是动态采样有希望的任务，从而实现成功的目标策略。我们在网格世界上评估 LSTS，并表明与最先进的 RM 和自动机引导的 RL 基线（例如奖励机的 Q 学习）相比，它在复杂的顺序决策问题上实现了改进的阈值时间性能以及来自逻辑规范 (DIRL) 的组合 RL。此外，我们证明了我们的方法在样本效率方面优于 RM 和自动机引导的 RL 基线，无论是在部分可观察的机器人任务中还是在连续控制机器人操作任务中。</li>
</ul>

<h3>Title: Minds versus Machines: Rethinking Entailment Verification with Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03686">https://arxiv.org/abs/2402.03686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03686">https://arxiv.org/pdf/2402.03686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03686]] Minds versus Machines: Rethinking Entailment Verification with Language  Models(https://arxiv.org/abs/2402.03686)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application, we showcase the efficacy of our finetuned model in enhancing self-consistency in model-generated explanations, resulting in a 6% performance boost on average across three multiple-choice question-answering datasets.</li>
<li><strong>摘要：</strong>人类在文本理解中做出大量推理来理解话语。本文旨在了解人类和最先进的大型语言模型（LLM）之间推理判断的共性和差异。利用全面策划的蕴含验证基准，我们评估人类和法学硕士在各种推理类别中的表现。我们的基准包括来自三个类别（NLI、上下文 QA 和基本原理）的数据集，其中包括多句子前提和不同的知识类型，从而评估复杂推理实例中的推理能力。值得注意的是，我们的研究结果揭示了法学硕士在跨扩展上下文的多跳推理方面的优势，而人类则擅长需要简单演绎推理的任务。利用这些见解，我们引入了经过微调的 Flan-T5 模型，该模型的性能优于 GPT-3.5，可与 GPT-4 相媲美，为蕴涵验证提供强大的开源解决方案。作为一个实际应用，我们展示了经过微调的模型在增强模型生成的解释的自我一致性方面的功效，从而使三个多项选择问答数据集的性能平均提高了 6%。</li>
</ul>

<h3>Title: Pard: Permutation-Invariant Autoregressive Diffusion for Graph  Generation</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Zhao, Xueying Ding, Leman Akoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03687">https://arxiv.org/abs/2402.03687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03687">https://arxiv.org/pdf/2402.03687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03687]] Pard: Permutation-Invariant Autoregressive Diffusion for Graph  Generation(https://arxiv.org/abs/2402.03687)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.</li>
<li><strong>摘要：</strong>尽管自回归模型对排序很敏感，但由于其简单性和有效性，图生成一直以自回归模型为主。然而，扩散模型已经引起了越来越多的关注，因为它们提供了可比的性能，同时又具有排列不变性。当前的图扩散模型以一次性方式生成图，但它们需要额外的特征和数千个去噪步骤才能实现最佳性能。我们介绍 PARD，一种排列不变的自回归扩散模型，它将扩散模型与自回归方法相结合。 PARD 利用自回归模型的有效性和效率，同时保持排列不变性而无需排序敏感性。具体来说，我们表明与集合相反，图中的元素并非完全无序，并且节点和边存在独特的偏序。通过这种偏序，PARD 以逐块、自回归的方式生成一个图，其中每个块的概率由具有等变网络的共享扩散模型有条件地建模。为了保证效率的同时具有表现力，我们进一步提出了一种高阶图变压器，它将变压器与 PPGN 集成。与 GPT 一样，我们扩展了高阶图转换器以支持所有块的并行训练。无需任何额外功能，PARD 在分子和非分子数据集上实现了最先进的性能，并可扩展到包含 190 万个分子的 MOSES 等大型数据集。</li>
</ul>

<h3>Title: Improving and Unifying Discrete&Continuous-time Discrete Denoising  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Zhao, Xueying Ding, Lijun Yu, Leman Akoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03701">https://arxiv.org/abs/2402.03701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03701">https://arxiv.org/pdf/2402.03701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03701]] Improving and Unifying Discrete&Continuous-time Discrete Denoising  Diffusion(https://arxiv.org/abs/2402.03701)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets. We open-source our unified code at https://github.com/LingxiaoShawn/USD3.</li>
<li><strong>摘要：</strong>离散扩散模型在语言和图形等自然离散数据上的应用引起了人们的广泛关注。尽管离散时间离散扩散已经建立了一段时间，但直到最近 Campbell 等人才提出。 (2022) 引入了第一个连续时间离散扩散框架。然而，它们的训练和采样过程与离散时间版本显着不同，需要非平凡的近似以实现可处理性。在本文中，我们首先提出了变分下界的一系列数学简化，可以实现更准确且易于优化的离散扩散训练。此外，我们推导了一个简单的后向降噪公式，可以实现精确和加速采样，而且重要的是，可以实现离散时间和连续时间离散扩散的优雅统一。由于分析公式更简单，前向概率和现在的后向概率都可以灵活地适应任何噪声分布，包括多元素对象的不同噪声分布。实验表明，我们提出的 USD3（统一简化离散去噪扩散）在已建立的数据集上优于所有 SOTA 基线。我们在 https://github.com/LingxiaoShawn/USD3 开源了我们的统一代码。</li>
</ul>

<h3>Title: Clarify: Improving Model Robustness With Natural Language Corrections</h3>
<ul>
<li><strong>Authors: </strong>Yoonho Lee, Michelle S. Lam, Helena Vasconcelos, Michael S. Bernstein, Chelsea Finn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03715">https://arxiv.org/abs/2402.03715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03715">https://arxiv.org/pdf/2402.03715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03715]] Clarify: Improving Model Robustness With Natural Language Corrections(https://arxiv.org/abs/2402.03715)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use such descriptions to improve the training process by reweighting the training data or gathering additional targeted data. Our user studies show that non-expert users can successfully describe model misconceptions via Clarify, improving worst-group accuracy by an average of 17.1% in two datasets. Additionally, we use Clarify to find and rectify 31 novel hard subpopulations in the ImageNet dataset, improving minority-split accuracy from 21.1% to 28.7%.</li>
<li><strong>摘要：</strong>在监督学习中，模型经过训练以从静态数据集中提取相关性。这通常会导致模型依赖于高层的误解。为了防止这种误解，我们必须提供训练数据之外的附加信息。现有方法结合了附加实例级监督的形式，例如虚假特征的标签或来自平衡分布的附加标记数据。对于大规模数据集来说，此类策略的成本可能会变得过高，因为它们需要接近原始训练数据的规模的额外注释。我们假设关于模型误解的有针对性的自然语言反馈是一种更有效的额外监督形式。我们引入了 Clarify，这是一种用于交互式纠正模型误解的新颖界面和方法。通过Clarify，用户只需提供简短的文本描述即可描述模型的一致故障模式。然后，以完全自动化的方式，我们使用此类描述通过重新加权训练数据或收集额外的目标数据来改进训练过程。我们的用户研究表明，非专家用户可以通过 Clarify 成功描述模型误解，将两个数据集中的最差组准确率平均提高 17.1%。此外，我们使用 Clarify 查找并纠正 ImageNet 数据集中的 31 个新的硬子群，将少数分裂准确率从 21.1% 提高到 28.7%。</li>
</ul>

<h3>Title: Empowering Language Models with Active Inquiry for Deeper Understanding</h3>
<ul>
<li><strong>Authors: </strong>Jing-Cheng Pang, Heng-Bo Fan, Pengyuan Wang, Jia-Hao Xiao, Nan Tang, Si-Hang Yang, Chengxing Jia, Sheng-Jun Huang, Yang Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03719">https://arxiv.org/abs/2402.03719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03719">https://arxiv.org/pdf/2402.03719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03719]] Empowering Language Models with Active Inquiry for Deeper Understanding(https://arxiv.org/abs/2402.03719)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language. However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted questioning to uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement. LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50.9%, outperforming other leading question-answering frameworks. Moreover, in scenarios involving human participants, LaMAI consistently generates responses that are superior or comparable to baseline methods in more than 82% of the cases. The applicability of LaMAI is further evidenced by its successful integration with various LLMs, highlighting its potential for the future of interactive language models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的兴起彻底改变了我们通过自然语言与人工智能系统交互的方式。然而，法学硕士经常因为用户的意图不确定而误解用户的查询，从而导致回复的帮助不大。在自然的人际互动中，通过有针对性的提问来寻求澄清，以发现晦涩的信息。因此，在本文中，我们介绍了 LaMAI（主动探究语言模型），旨在赋予法学硕士同样水平的互动参与。 LaMAI 利用主动学习技术提出信息最丰富的问题​​，促进动态的双向对话。这种方法不仅缩小了背景差距，而且还完善了法学硕士的输出，使其更符合用户的期望。我们的实证研究涵盖了法学硕士对话背景有限的各种复杂数据集，证明了 LaMAI 的有效性。该方法将答案准确率从 31.9% 提高到 50.9%，优于其他领先的问答框架。此外，在涉及人类参与者的场景中，LaMAI 在超过 82% 的情况下始终能够生成优于或与基线方法相当的响应。 LaMAI 与各种法学硕士的成功集成进一步证明了 LaMAI 的适用性，凸显了其在交互式语言模型未来的潜力。</li>
</ul>

<h3>Title: Similarity-based Neighbor Selection for Graph LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rui Li, Jiwei Li, Jiawei Han, Guoyin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03720">https://arxiv.org/abs/2402.03720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03720">https://arxiv.org/pdf/2402.03720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03720]] Similarity-based Neighbor Selection for Graph LLMs(https://arxiv.org/abs/2402.03720)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning practices, demonstrate that SNS, through simple prompt interactions with LLMs, consistently outperforms vanilla GNNs and achieves state-of-the-art results on datasets like PubMed in node classification, showcasing LLMs' potential in graph structure understanding. Our research further underscores the significance of graph structure integration in LLM applications and identifies key factors for their success in node classification. Code is available at https://github.com/ruili33/SNS.</li>
<li><strong>摘要：</strong>文本属性图 (TAG) 为语言学习模型 (LLM) 的直接处理带来了独特的挑战，但其广泛的常识知识和强大的推理能力为 TAG 中的节点分类提供了巨大的希望。该领域的先前研究已经解决了过度压缩、异质性和无效的图信息集成等问题，而数据集分区的不一致和高级法学硕士的利用不足进一步加剧了这些问题。为了应对这些挑战，我们引入了基于相似性的邻居选择（SNS）。使用 SimCSE 和先进的邻居选择技术，SNS 有效地提高了所选邻居的质量，从而改善了图形表示并缓解了过度挤压和异质性等问题。此外，作为一种归纳且免训练的方法，SNS 表现出比传统 GNN 方法更优越的泛化性和可扩展性。我们遵循标准数据集划分实践的全面实验表明，SNS 通过与 LLM 进行简单的提示交互，始终优于普通 GNN，并在 PubMed 等数据集上的节点分类中取得了最先进的结果，展示了 LLM 在图形中的潜力结构理解。我们的研究进一步强调了图结构集成在法学硕士应用程序中的重要性，并确定了其在节点分类方面取得成功的关键因素。代码可在 https://github.com/ruili33/SNS 获取。</li>
</ul>

<h3>Title: Learning Granger Causality from Instance-wise Self-attentive Hawkes  Processes</h3>
<ul>
<li><strong>Authors: </strong>Dongxia Wu, Tsuyoshi Idé, Aurélie Lozano, Georgios Kollias, Jiří Navrátil, Naoki Abe, Yi-An Ma, Rose Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03726">https://arxiv.org/abs/2402.03726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03726">https://arxiv.org/pdf/2402.03726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03726]] Learning Granger Causality from Instance-wise Self-attentive Hawkes  Processes(https://arxiv.org/abs/2402.03726)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences. In particular, we are interested in discovering instance-level causal structures in an unsupervised manner. Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality. We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level. ISAHP is the first neural point process model that meets the requirements of Granger causality. It leverages the self-attention mechanism of the transformer to align with the principles of Granger causality. We empirically demonstrate that ISAHP is capable of discovering complex instance-level causal structures that cannot be handled by classical models. We also show that ISAHP achieves state-of-the-art performance in proxy tasks involving type-level causal discovery and instance-level event type prediction.</li>
<li><strong>摘要：</strong>我们解决了从异步、相互依赖、多类型事件序列中学习格兰杰因果关系的问题。特别是，我们对以无监督的方式发现实例级因果结构感兴趣。实例级因果关系识别各个事件之间的因果关系，为决策提供更细粒度的信息。文献中的现有工作要么需要强有力的假设，例如强度函数的线性，要么需要启发式定义的模型参数，但不一定满足格兰杰因果关系的要求。我们提出了实例级自注意力霍克斯过程（ISAHP），这是一种新颖的深度学习框架，可以直接推断事件实例级别的格兰杰因果关系。 ISAHP是第一个满足格兰杰因果关系要求的神经点过程模型。它利用 Transformer 的自注意力机制来符合格兰杰因果关系原理。我们凭经验证明 ISAHP 能够发现经典模型无法处理的复杂实例级因果结构。我们还表明，ISAHP 在涉及类型级因果发现和实例级事件类型预测的代理任务中实现了最先进的性能。</li>
</ul>

<h3>Title: Consistent Joint Decision-Making with Heterogeneous Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Hossein Rajaby Faghihi, Parisa Kordjamshidi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03728">https://arxiv.org/abs/2402.03728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03728">https://arxiv.org/pdf/2402.03728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03728]] Consistent Joint Decision-Making with Heterogeneous Learning Models(https://arxiv.org/abs/2402.03728)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.</li>
<li><strong>摘要：</strong>本文介绍了一种新颖的决策框架，该框架在利用外部知识的同时促进不同模型做出的决策之间的一致性。利用整数线性规划 (ILP) 框架，我们通过合并有关决策先验概率、置信度（不确定性）和模型预期准确性的信息，将各种模型的预测映射到全局标准化和可比较的值。我们的实证研究证明了我们的方法在多个数据集上优于传统基线。</li>
</ul>

<h3>Title: SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent  Reinforcement Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Oubo Ma, Yuwen Pu, Linkang Du, Yang Dai, Ruo Wang, Xiaolei Liu, Yingcai Wu, Shouling Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03741">https://arxiv.org/abs/2402.03741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03741">https://arxiv.org/pdf/2402.03741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03741]] SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent  Reinforcement Learning Systems(https://arxiv.org/abs/2402.03741)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation. In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests the sharing of transitions among subpolicies to improve the exploitative ability of attackers. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.</li>
<li><strong>摘要：</strong>多智能体强化学习（MARL）的最新进展开辟了广阔的应用前景，包括无人机群体控制、机械臂协作操纵以及多目标包围等。然而，MARL部署过程中潜在的安全威胁需要更多的关注和彻底的调查。最近的研究表明，攻击者可以快速利用受害者的漏洞并生成对抗性策略，导致受害者在特定任务中失败。例如，将超人级别的围棋AI的胜率降低到20%左右。他们主要关注两人竞争环境，假设攻击者拥有完整的全局状态观察能力。在这项研究中，我们首次揭示了攻击者即使在多智能体竞争环境中仅限于对受害者的部分观察也能生成对抗性策略的能力。具体来说，我们提出了一种新颖的黑盒攻击（SUB-PLAY），它结合了构建多个子博弈的概念以减轻部分可观察性的影响，并建议子策略之间共享转换以提高攻击者的利用能力。广泛的评估证明了 SUB-PLAY 在三种典型的部分可观测性限制下的有效性。可视化结果表明，对抗性政策会导致受害者政策网络的激活显着不同。此外，我们评估了三种潜在的防御措施，旨在探索减轻对抗性政策带来的安全威胁的方法，为在竞争环境中部署 MARL 提供建设性建议。</li>
</ul>

<h3>Title: INSIDE: LLMs' Internal States Retain the Power of Hallucination  Detection</h3>
<ul>
<li><strong>Authors: </strong>Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03744">https://arxiv.org/abs/2402.03744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03744">https://arxiv.org/pdf/2402.03744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03744]] INSIDE: LLMs' Internal States Retain the Power of Hallucination  Detection(https://arxiv.org/abs/2402.03744)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular, a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.</li>
<li><strong>摘要：</strong>知识幻觉引起了人们对所部署的法学硕士的安全性和可靠性的广泛担忧。先前检测幻觉的努力已用于逻辑级别的不确定性估计或语言级别的自我一致性评估，其中语义信息在令牌解码过程中不可避免地丢失。因此，我们建议探索LLM的\textbf{IN}内部\textbf{S}状态中保留的密集语义信息，以用于halluc\textbf{I}nation \textbf{DE}保护（\textbf{INSIDE}）。特别是，提出了一种简单而有效的 \textbf{EigenScore} 度量来更好地评估响应的自我一致性，该度量利用响应协方差矩阵的特征值来测量密集嵌入空间中的语义一致性/多样性。此外，从自洽幻觉检测的角度来看，探索了一种测试时间特征裁剪方法来截断内部状态的极端激活，这减少了过度自信的世代，并可能有利于过度自信幻觉的检测。在几个流行的法学硕士和问答（QA）基准上进行了广泛的实验和消融研究，显示了我们建议的有效性。</li>
</ul>

<h3>Title: Enhanced sampling of robust molecular datasets with uncertainty-based  collective variables</h3>
<ul>
<li><strong>Authors: </strong>Aik Rui Tan, Johannes C. B. Dietschreit, Rafael Gomez-Bombarelli</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03753">https://arxiv.org/abs/2402.03753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03753">https://arxiv.org/pdf/2402.03753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03753]] Enhanced sampling of robust molecular datasets with uncertainty-based  collective variables(https://arxiv.org/abs/2402.03753)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP). However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge. Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations. In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain. This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics simulations. The effectiveness of our approach in overcoming energy barriers and exploring unseen energy minima, thereby enhancing the data set in an active learning framework, is demonstrated on the alanine dipeptide benchmark system.</li>
<li><strong>摘要：</strong>生成代表分子系统可访问配置空间的数据集对于机器学习原子间势（MLIP）的稳健性至关重要。然而，分子系统的复杂性，以复杂的势能面（PES）为特征，具有大量的局部极小值和能量势垒，提出了重大挑战。传统的数据生成方法，例如随机采样或详尽探索，要么很棘手，要么可能无法捕获罕见但信息丰富的配置。在本研究中，我们提出了一种利用不确定性作为集体变量 (CV) 来指导化学相关数据点的获取的方法，重点关注 ML 模型预测最不确定的配置空间区域。该方法采用来自单个模型的基于高斯混合模型的不确定性度量作为有偏差的分子动力学模拟的 CV。我们的方法在克服能量障碍和探索看不见的能量最小值方面的有效性，从而增强主动学习框架中的数据集，在丙氨酸二肽基准系统上得到了证明。</li>
</ul>

<h3>Title: QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Saizhuo Wang, Hang Yuan, Lionel M. Ni, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.AI, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03755">https://arxiv.org/abs/2402.03755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03755">https://arxiv.org/pdf/2402.03755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03755]] QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large  Language Model(https://arxiv.org/abs/2402.03755)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent's capability in uncovering viable financial signals and enhancing the accuracy of financial forecasts.</li>
<li><strong>摘要：</strong>基于大型语言模型 (LLM) 的自主代理能够制定计划并应对现实世界的挑战，这一点已经受到重视。然而，为定量投资等专业领域定制这些代理仍然是一项艰巨的任务。核心挑战涉及为代理的学习过程有效构建和集成特定领域的知识库。本文介绍了一个解决这一挑战的原则框架，包括一个两层循环。在内部循环中，代理通过从知识库中提取内容来完善其响应，而在外部循环中，这些响应在现实场景中进行测试通过新的见解自动增强知识库。我们证明我们的方法使代理能够以可证明的效率逐步逼近最佳行为。此外，我们通过名为 QuantAgent 的用于挖掘交易信号的自治代理实例化该框架。实证结果展示了 QuantAgent 发现可行金融信号和提高金融预测准确性的能力。</li>
</ul>

<h3>Title: Fed-CVLC: Compressing Federated Learning Communications with  Variable-Length Codes</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxin Su, Yipeng Zhou, Laizhong Cui, John C.S. Lui, Jiangchuan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03770">https://arxiv.org/abs/2402.03770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03770">https://arxiv.org/pdf/2402.03770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03770]] Fed-CVLC: Compressing Federated Learning Communications with  Variable-Length Codes(https://arxiv.org/abs/2402.03770)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equivalent to maximizing the model utility) subject to the budget for communication. We further demonstrate that Fed-CVLC is indeed a general compression design that bridges quantization and sparsification, with greater flexibility. Extensive experiments have been conducted with public datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art baselines, improving model utility by 1.50%-5.44%, or shrinking communication traffic by 16.67%-41.61%.</li>
<li><strong>摘要：</strong>在联邦学习（FL）范式中，参数服务器（PS）同时与分布式参与客户端进行多轮通信，以进行模型收集、更新聚合和模型分发，而不会触及各个客户端拥有的私有数据。 FL 在保护数据隐私方面颇具吸引力；然而，PS 和分散的客户端之间的通信可能是一个严重的瓶颈。已经提出了模型压缩算法，例如量化和稀疏化，但它们通常假设固定的代码长度，这不能反映模型更新的异构性和可变性。在本文中，通过分析和实验，我们提供了强有力的证据表明可变长度有利于 FL 压缩。因此，我们提出了 Fed-CVLC（具有可变长度代码的联合学习压缩），它可以根据模型更新的动态来微调代码长度。我们开发最佳调整策略，根据通信预算最小化损失函数（相当于最大化模型效用）。我们进一步证明 Fed-CVLC 确实是一种通用的压缩设计，它连接了量化和稀疏化，具有更大的灵活性。使用公共数据集进行了大量实验，结果表明 Fed-CVLC 的性能显着优于最先进的基线，将模型效用提高了 1.50%-5.44%，或将通信流量减少了 16.67%-41.61%。</li>
</ul>

<h3>Title: Reinforcement Learning from Bagged Reward: A Transformer-based Approach  for Instance-Level Reward Redistribution</h3>
<ul>
<li><strong>Authors: </strong>Yuting Tang, Xin-Qiang Cai, Yao-Xiang Ding, Qiyu Wu, Guoqing Liu, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03771">https://arxiv.org/abs/2402.03771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03771">https://arxiv.org/pdf/2402.03771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03771]] Reinforcement Learning from Bagged Reward: A Transformer-based Approach  for Instance-Level Reward Redistribution(https://arxiv.org/abs/2402.03771)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standard RL in Markov Decision Processes (MDPs). To effectively explore the reward distributions within the bagged rewards, we propose a Transformer-based reward model, the Reward Bag Transformer (RBT), which uses the self-attention mechanism for interpreting the contextual nuances and temporal dependencies within each bag. Extensive experimental analyses demonstrate the superiority of our method, particularly in its ability to mimic the original MDP's reward distribution, highlighting its proficiency in contextual understanding and adaptability to environmental dynamics.</li>
<li><strong>摘要：</strong>在强化学习（RL）中，为智能体的每个动作生成即时奖励信号，使得智能体学习最大化累积奖励以获得最优策略。然而，在许多现实世界的应用中，代理无法获得即时奖励信号。相反，学习者仅在袋子的末端获得奖励，其中袋子被定义为完整轨迹的部分序列。在这种情况下，学习者必须面对探索袋子中未知的即时奖励的巨大困难，这是现有方法无法解决的，包括那些仅考虑完整轨迹而忽略内部奖励分布的基于轨迹的方法。为了正式研究这种情况，我们引入了一种新颖的 RL 设置，称为袋装奖励强化学习 (RLBR)，其中只能获得序列的袋装奖励。我们提供理论研究来建立马尔可夫决策过程 (MDP) 中 RLBR 和标准 RL 之间的联系。为了有效地探索袋装奖励中的奖励分布，我们提出了一种基于 Transformer 的奖励模型，即奖励袋变压器（RBT），它使用自注意力机制来解释每个袋内的上下文细微差别和时间依赖性。广泛的实验分析证明了我们方法的优越性，特别是它能够模仿原始 MDP 的奖励分配，突出了它在上下文理解和对环境动态的适应性方面的熟练程度。</li>
</ul>

<h3>Title: Large Language Models As MOOCs Graders</h3>
<ul>
<li><strong>Authors: </strong>Shahriar Golchin, Nikhil Garuda, Christopher Impey, Matthew Wenger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03776">https://arxiv.org/abs/2402.03776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03776">https://arxiv.org/pdf/2402.03776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03776]] Large Language Models As MOOCs Graders(https://arxiv.org/abs/2402.03776)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT in conjunction with both instructor-formulated answers and rubrics; and Zero-shot-CoT with instructor-offered correct answers and LLM-generated rubrics. Our results show that Zero-shot-CoT, when integrated with instructor-provided answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading. However, the History and Philosophy of Astronomy course proves to be more challenging in terms of grading as opposed to other courses. Finally, our study reveals a promising direction for automating grading systems for MOOCs, especially in subjects with well-defined rubrics.</li>
<li><strong>摘要：</strong>大规模开放在线课程（MOOC）为全球任何能够使用计算机和互联网的人打开了免费教育的大门。尽管学习民主化，但这些课程的大量注册意味着一名教师几乎不可能评估每个学生的写作作业。因此，通常以简单的评分标准为指导的同行评分是首选方法。同行评分虽然方便，但在可靠性和有效性方面往往存在不足。在本研究中，我们使用 18 种不同的设置，探索利用大型语言模型 (LLM) 取代 MOOC 中的同行评分的可行性。具体来说，我们专注于两个最先进的法学硕士：GPT-4 和 GPT-3.5，涵盖三个不同的课程：天文学入门、天体生物学以及天文学的历史和哲学。为了指导法学硕士，我们使用基于零样本思维链 (Zero-shot-CoT) 提示技术变体的三种不同提示：零样本 CoT 与讲师提供的正确答案相结合；零样本 CoT 与教师制定的答案和评分细则相结合；以及由讲师提供正确答案和法学硕士生成的评分标准的零样本 CoT。我们的结果表明，与同伴评分相比，零样本 CoT 与教师提供的答案和评分标准相结合时，产生的成绩与教师指定的成绩更加一致。然而，与其他课程相比，天文学历史和哲学课程在评分方面更具挑战性。最后，我们的研究揭示了 MOOC 自动化评分系统的一个有前途的方向，特别是在具有明确定义的主题中。</li>
</ul>

<h3>Title: Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More</h3>
<ul>
<li><strong>Authors: </strong>Fred Philippy, Siwen Guo, Shohreh Haddadan, Cedric Lothritz, Jacques Klein, Tegawendé F. Bissyandé</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03782">https://arxiv.org/abs/2402.03782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03782">https://arxiv.org/pdf/2402.03782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03782]] Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More(https://arxiv.org/abs/2402.03782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, rag</a></li>
<li><strong>Abstract: </strong>Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.</li>
<li><strong>摘要：</strong>软提示调优 (SPT) 是一种参数高效的方法，通过在 PLM 的输入层插入可学习的嵌入或软提示，使预训练语言模型 (PLM) 适应特定任务，而无需修改其参数。本文研究了 SPT 在跨语言迁移方面的潜力。与之前针对跨语言迁移的 SPT 研究通常对软提示和模型参数进行微调不同，我们坚持 SPT 的初衷，保持模型参数冻结，仅训练软提示。这不仅减少了全模型微调的计算成本和存储开销，而且我们还证明了 SPT 固有的参数效率可以增强对语言上距离较远的语言的跨语言迁移性能。此外，我们还探讨了与提示相关的不同因素（例如长度或其重新参数化）如何影响跨语言传输性能。</li>
</ul>

<h3>Title: AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality  Prediction</h3>
<ul>
<li><strong>Authors: </strong>Kethmi Hirushini Hettige, Jiahao Ji, Shili Xiang, Cheng Long, Gao Cong, Jingyuan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03784">https://arxiv.org/abs/2402.03784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03784">https://arxiv.org/pdf/2402.03784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03784]] AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality  Prediction(https://arxiv.org/abs/2402.03784)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships within the air quality data. Experiments on two real-world benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art models for different testing scenarios including different lead time (24h, 48h, 72h), sparse data and sudden change prediction, achieving reduction in prediction errors up to 10%. Moreover, a case study further validates that our model captures underlying physical processes of particle movement and generates accurate predictions with real physical meaning.</li>
<li><strong>摘要：</strong>空气质量预测和建模在公共卫生和环境管理中发挥着关键作用，有助于个人和当局做出明智的决策。尽管传预测的透明度和可解释性。为了解决这些限制，本文提出了一种名为物理引导神经网络空气质量预测（AirPhyNet）的新方法。具体来说，我们通过将空气粒子运动（扩散和平流）表示为微分方程网络来利用它们。然后，我们利用图结构将物理知识集成到神经网络架构中，并利用潜在表示来捕获空气质量数据中的时空关系。对两个真实世界基准数据集的实验表明，AirPhyNet 在不同的测试场景（包括不同的提前时间（24 小时、48 小时、72 小时）、稀疏数据和突然变化预测）下均优于最先进的模型，实现预测误差最多减少10%。此外，案例研究进一步验证了我们的模型捕获了粒子运动的潜在物理过程，并生成具有真实物理意义的准确预测。</li>
</ul>

<h3>Title: ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse  LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03804">https://arxiv.org/abs/2402.03804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03804">https://arxiv.org/pdf/2402.03804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03804]] ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse  LLMs(https://arxiv.org/abs/2402.03804)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag</a></li>
<li><strong>Abstract: </strong>Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs. We will release the code to facilitate future research.</li>
<li><strong>摘要：</strong>稀疏计算通过动态跳过不活动神经元的计算，为低资源场景中的大型语言模型 (LLM) 推理提供了引人注目的解决方案。虽然传统方法侧重于基于 ReLU 的 LLM，利用激活值中的零，但我们将稀疏 LLM 的范围扩大到零激活值之外。我们引入了一种通用方法，通过神经元输出幅度和定制的幅度阈值来定义神经元激活，证明非 ReLU LLM 也表现出稀疏激活。为了找到稀疏计算最有效的激活函数，我们提出了一个系统框架，从三个方面检查LLM的稀疏性：稀疏性和性能之间的权衡、稀疏性的预测性和硬件亲和性。我们利用不同的激活函数（包括 ReLU、SwiGLU、ReGLU 和 ReLU$^2$）对 LLM 进行了彻底的实验。结果表明，采用 ReLU$^2$ 的模型在所有三个评估方面均表现出色，凸显了其作为稀疏 LLM 的有效激活函数的潜力。我们将发布代码以方便将来的研究。</li>
</ul>

<h3>Title: SEABO: A Simple Search-Based Method for Offline Imitation Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiafei Lyu, Xiaoteng Ma, Le Wan, Runze Liu, Xiu Li, Zongqing Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03807">https://arxiv.org/abs/2402.03807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03807">https://arxiv.org/pdf/2402.03807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03807]] SEABO: A Simple Search-Based Method for Offline Imitation Learning(https://arxiv.org/abs/2402.03807)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks. Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations. Our code is publicly available at https://github.com/dmksjfl/SEABO.</li>
<li><strong>摘要：</strong>离线强化学习（RL）因其从静态离线数据集中学习并消除与环境交互的需要的能力而备受关注。然而，离线强化学习的成功在很大程度上依赖于用奖励标签注释的离线转换。在实践中，我们经常需要手工设计奖励函数，这有时很困难、劳动密集型或效率低下。为了应对这一挑战，我们将重点放在离线模仿学习（IL）设置上，目标是获得基于专家数据和未标记数据的奖励函数。为此，我们提出了一种简单而有效的基于搜索的离线 IL 方法，标记为 SEABO。 SEABO 为专家演示中最接近其最近邻居的过渡分配较大的奖励，否则分配较小的奖励，所有这些都以无监督学习的方式进行。各种 D4RL 数据集上的实验结果表明，在仅给出单个专家轨迹的情况下，SEABO 可以通过真实奖励实现与离线 RL 算法竞争的性能，并且可以在许多任务中优于先前的奖励学习和离线 IL 方法。此外，我们证明，如果专家演示仅包含观察结果，SEABO 也能很好地工作。我们的代码可在 https://github.com/dmksjfl/SEABO 上公开获取。</li>
</ul>

<h3>Title: Masked Graph Autoencoder with Non-discrete Bandwidths</h3>
<ul>
<li><strong>Authors: </strong>Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, Ruixuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03814">https://arxiv.org/abs/2402.03814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03814">https://arxiv.org/pdf/2402.03814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03814]] Masked Graph Autoencoder with Non-discrete Bandwidths(https://arxiv.org/abs/2402.03814)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Masked graph autoencoders have emerged as a powerful graph self-supervised learning method that has yet to be fully explored. In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on graph neural networks. These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability. Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution. These masks restrict the amount of output messages for each edge, referred to as "bandwidths". We propose a novel, informative, and effective topological masked graph autoencoder using bandwidth masking and a layer-wise bandwidth prediction objective. We demonstrate its powerful graph topological learning ability both theoretically and empirically. Our proposed framework outperforms representative baselines in both self-supervised link prediction (improving the discrete edge reconstructors by at most 20%) and node classification on numerous datasets, solely with a structure-learning pretext. Our implementation is available at https://github.com/Newiz430/Bandana.</li>
<li><strong>摘要：</strong>屏蔽图自动编码器已经成为一种强大的图自监督学习方法，但尚未得到充分探索。在本文中，我们揭示了从图神经网络上消息传播的角度来看，现有的离散边缘掩蔽和二元链接重建策略不足以学习拓扑信息表示。这些限制包括阻塞消息流、过度平滑的脆弱性以及次优的邻域辨别能力。受这些理解的启发，我们探索了非离散边缘掩模，这些边缘掩模是从连续且分散的概率分布而不是离散伯努利分布中采样的。这些掩码限制每个边缘的输出消息量，称为“带宽”。我们提出了一种新颖、信息丰富且有效的拓扑屏蔽图自动编码器，使用带宽屏蔽和逐层带宽预测目标。我们从理论上和实证上证明了其强大的图拓扑学习能力。我们提出的框架在自监督链接预测（将离散边缘重建器最多提高 20%）和大量数据集上的节点分类方面都优于代表性基线，仅以结构学习为借口。我们的实现可以在 https://github.com/Newiz430/Bandana 上找到。</li>
</ul>

<h3>Title: RevOrder: A Novel Method for Enhanced Arithmetic in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Si Shen, Peijun Shen, Danhao Zhu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03822">https://arxiv.org/abs/2402.03822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03822">https://arxiv.org/pdf/2402.03822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03822]] RevOrder: A Novel Method for Enhanced Arithmetic in Language Models(https://arxiv.org/abs/2402.03822)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.</li>
<li><strong>摘要：</strong>本文提出了 RevOrder，这是一种新技术，旨在通过反转加法、减法和 n 位乘 1 位（nD x 1D）乘法任务中的输出数字来改进大型语言模型 (LLM) 中的算术运算。我们的方法将连续中间数字（CSID）的计数显着减少到 $\mathcal{O}(1)$，这是我们引入的一个用于评估方程复杂性的新指标。通过全面的测试，RevOrder不仅在基本算术运算中实现了完美的准确性，而且还大幅提升了LLM在除法任务中的性能，特别是在传统模型难以胜任的大数任务中。 RevOrder 的实施对于训练和推理阶段来说都是具有成本效益的。此外，应用 RevOrder 对 GSM8K 数学任务上的 LLaMA2-7B 模型进行微调，带来了相当大的改进，将方程计算错误减少了 46%，并将总分从 41.6 提高到 44.4。</li>
</ul>

<h3>Title: A call for embodied AI</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Paolo, Jonas Gonzalez-Billandon, Balázs Kégl</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03824">https://arxiv.org/abs/2402.03824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03824">https://arxiv.org/pdf/2402.03824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03824]] A call for embodied AI(https://arxiv.org/abs/2402.03824)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.</li>
<li><strong>摘要：</strong>我们建议将嵌入式人工智能作为追求通用人工智能的下一个基本步骤，并将其与当前人工智能的进步（特别是大型语言模型）进行比较。我们回顾了体现概念在哲学、心理学、神经科学和机器人学等不同领域的演变，以强调 EAI 如何区别于静态学习的经典范式。通过扩大实体人工智能的范围，我们引入了一个基于认知架构的理论框架，强调感知、行动、记忆和学习作为实体智能体的基本组成部分。该框架符合 Friston 的主动推理原则，为 EAI 开发提供了全面的方法。尽管人工智能领域取得了进展，但诸如新颖的人工智能学习理论的制定和先进硬件的创新等重大挑战仍然存在。我们的讨论为未来的实体人工智能研究奠定了基础指南。强调创建能够在现实环境中与人类和其他智能实体无缝通信、协作和共存的实体人工智能代理的重要性，我们的目标是引导人工智能社区应对多方面的挑战并抓住未来的机遇。对 AGI 的追求。</li>
</ul>

<h3>Title: Estimating Barycenters of Distributions with Neural Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Alexander Kolesov, Petr Mokrov, Igor Udovichenko, Milena Gazdieva, Gudmund Pammer, Evgeny Burnaev, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03828">https://arxiv.org/abs/2402.03828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03828">https://arxiv.org/pdf/2402.03828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03828]] Estimating Barycenters of Distributions with Neural Optimal Transport(https://arxiv.org/abs/2402.03828)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Given a collection of probability measures, a practitioner sometimes needs to find an "average" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.</li>
<li><strong>摘要：</strong>给定概率度量的集合，从业者有时需要找到充分聚合参考分布的“平均”分布。这种平均值的一个理论上有吸引力的概念是 Wasserstein 重心，它是我们工作的主要焦点。通过建立在最佳传输 (OT) 的对偶公式的基础上，我们提出了一种新的可扩展方法来解决 Wasserstein 重心问题。我们的方法基于最近的神经 OT 求解器：它具有双层对抗性学习目标，适用于一般成本函数。这些是我们方法的关键优势，因为利用重心任务的典型对抗算法利用三级优化并主要关注二次成本。我们还为我们提出的方法建立了理论误差界限，并展示了其在说明性场景和图像数据设置上的适用性和有效性。</li>
</ul>

<h3>Title: Rethinking Skill Extraction in the Job Market Domain using Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Khanh Cao Nguyen, Mike Zhang, Syrielle Montariol, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03832">https://arxiv.org/abs/2402.03832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03832">https://arxiv.org/pdf/2402.03832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03832]] Rethinking Skill Extraction in the Job Market Domain using Large  Language Models(https://arxiv.org/abs/2402.03832)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences. We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.</li>
<li><strong>摘要：</strong>技能提取涉及识别职位发布和简历等文件中提到的技能和资格。该任务通常通过使用带有 BIO 标签的序列标记方法训练监督模型来解决。然而，对手动注释数据的依赖限制了此类方法的普遍性。此外，常见的 BIO 设置限制了模型捕获复杂技能模式和处理模糊提及的能力。在本文中，我们以 6 个统一技能提取数据集为基准，探索使用情境学习来克服这些挑战。我们的方法利用大型语言模型（LLM）的小样本学习能力来识别和提取句子中的技能。我们表明，法学硕士尽管在性能方面与传统监督模型不相上下，但可以更好地处理技能提取任务中语法复杂的技能提及。</li>
</ul>

<h3>Title: ANLS* -- A Universal Document Processing Metric for Generative Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03848">https://arxiv.org/abs/2402.03848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03848">https://arxiv.org/pdf/2402.03848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03848]] ANLS* -- A Universal Document Processing Metric for Generative Large  Language Models(https://arxiv.org/abs/2402.03848)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and is still compatible with previously reported ANLS scores. An evaluation of 7 different datasets and 3 different GLLMs using the ANLS* metric is also provided, demonstrating the importance of the proposed metric. We also benchmark a novel approach to generate prompts for documents, called SFT, against other prompting techniques such as LATIN. In 15 out of 21 cases, SFT outperforms other techniques and improves the state-of-the-art, sometimes by as much as $15$ percentage points. Sources are available at https://github.com/deepopinion/anls_star_metric</li>
<li><strong>摘要：</strong>传统上，判别模型一直是文档分类和信息提取等任务的主要选择。这些模型进行的预测属于有限数量的预定义类别，有助于进行二元正确或错误评估，并能够直接计算 F1 分数等指标。然而，生成式大语言模型（GLLM）的最新进展由于其增强的零样本能力而促使该领域发生转变，从而消除了对下游数据集和计算成本高昂的微调的需要。然而，评估 GLLM 提出了一个挑战，因为用于判别模型的二元真或假评估不适用于 GLLM 所做的预测。本文介绍了一种称为 ANLS* 的生成模型新指标，用于评估各种任务，包括信息提取和分类任务。 ANLS* 指标将现有的 ANLS 指标作为直接替代品进行了扩展，并且仍然与之前报告的 ANLS 分数兼容。还提供了使用 ANLS* 指标对 7 个不同数据集和 3 个不同 GLLM 的评估，证明了所提出指标的重要性。我们还对一种名为 SFT 的文档生成提示的新颖方法与其他提示技术（例如 LATIN）进行了比较。在 21 个案例中的 15 个中，SFT 优于其他技术并提高了最先进的技术，有时提高了 15 个百分点。来源可在 https://github.com/deepopinion/anls_star_metric 获取</li>
</ul>

<h3>Title: Position Paper: Toward New Frameworks for Studying Model Representations</h3>
<ul>
<li><strong>Authors: </strong>Satvik Golechha, James Dao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03855">https://arxiv.org/abs/2402.03855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03855">https://arxiv.org/pdf/2402.03855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03855]] Position Paper: Toward New Frameworks for Studying Model Representations(https://arxiv.org/abs/2402.03855)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.</li>
<li><strong>摘要：</strong>机械可解释性 (MI) 旨在通过对神经网络学习的确切算法进行逆向工程来理解人工智能模型。迄今为止，MI 领域的大多数工作都研究了琐碎且与令牌一致的行为和功能。然而，大多数功能并不是那么微不足道，这提倡以研究这些网络内部的隐藏表示作为分析单位。我们进行了文献综述，形式化了特征和行为的表征，强调了它们的重要性和评估，并对表征的机械解释性进行了一些基本探索。通过讨论和探索性结果，我们证明了我们的立场，即研究表征是一个重要且尚未充分研究的领域，目前在 MI 中建立的方法不足以理解表征，从而推动研究界致力于研究表征的新框架。</li>
</ul>

<h3>Title: The Challenges of the Nonlinear Regime for Physics-Informed Neural  Networks</h3>
<ul>
<li><strong>Authors: </strong>Andrea Bonfanti, Giuseppe Bruno, Cristina Cipriani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03864">https://arxiv.org/abs/2402.03864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03864">https://arxiv.org/pdf/2402.03864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03864]] The Challenges of the Nonlinear Regime for Physics-Informed Neural  Networks(https://arxiv.org/abs/2402.03864)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit. We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator. Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs. Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases.</li>
<li><strong>摘要：</strong>神经正切核 (NTK) 观点代表了一种在无限宽度限制下检查物理信息神经网络 (PINN) 训练动态的有价值的方法。我们利用这个视角，重点关注 PINN 求解的非线性偏微分方程 (PDE) 的情况。我们根据微分算子的线性度提供了 NTK 不同行为的理论结果。此外，受我们理论结果的启发，我们强调了采用二阶方法训练 PINN 的优势。此外，我们还探索了二阶方法的收敛能力，并解决了谱偏差和收敛速度慢的挑战。每个理论结果都得到线性和非线性偏微分方程数值实例的支持，并且我们在基准测试用例上验证了我们的训练方法。</li>
</ul>

<h3>Title: Less than one percent of words would be affected by gender-inclusive  language in German press texts</h3>
<ul>
<li><strong>Authors: </strong>Carolin Müller-Spitzer, Samira Ochs, Alexander Koplenig, Jan-Oliver Rüdiger, Sascha Wolfer</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03870">https://arxiv.org/abs/2402.03870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03870">https://arxiv.org/pdf/2402.03870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03870]] Less than one percent of words would be affected by gender-inclusive  language in German press texts(https://arxiv.org/abs/2402.03870)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Research on gender and language is tightly knitted to social debates on gender equality and non-discriminatory language use. Psycholinguistic scholars have made significant contributions in this field. However, corpus-based studies that investigate these matters within the context of language use are still rare. In our study, we address the question of how much textual material would actually have to be changed if non-gender-inclusive texts were rewritten to be gender-inclusive. This quantitative measure is an important empirical insight, as a recurring argument against the use of gender-inclusive German is that it supposedly makes written texts too long and complicated. It is also argued that gender-inclusive language has negative effects on language learners. However, such effects are only likely if gender-inclusive texts are very different from those that are not gender-inclusive. In our corpus-linguistic study, we manually annotated German press texts to identify the parts that would have to be changed. Our results show that, on average, less than 1% of all tokens would be affected by gender-inclusive language. This small proportion calls into question whether gender-inclusive German presents a substantial barrier to understanding and learning the language, particularly when we take into account the potential complexities of interpreting masculine generics.</li>
<li><strong>摘要：</strong>对性别和语言的研究与关于性别平等和非歧视性语言使用的社会辩论紧密相连。心理语言学学者在这一领域做出了重大贡献。然而，在语言使用的背景下调查这些问题的基于语料库的研究仍然很少。在我们的研究中，我们解决了这样的问题：如果将非性别包容的文本重写为包容性别的文本，实际上需要对文本材料进行多少修改。这种定量测量是一种重要的经验见解，因为反对使用性别包容的德语的一个反复出现的论点是，它被认为会使书面文本变得太长和复杂。还有人认为，性别包容性语言对语言学习者有负面影响。然而，只有当性别包容的文本与非性别包容的文本有很大不同时，这种影响才有可能出现。在我们的语料库语言研究中，我们手动注释了德语新闻文本，以识别需要更改的部分。我们的结果表明，平均而言，所有代币中只有不到 1% 会受到性别包容性语言的影响。这一小比例令人质疑，性别包容的德语是否对理解和学习该语言构成了重大障碍，特别是当我们考虑到解释男性通用词的潜在复杂性时。</li>
</ul>

<h3>Title: Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03877">https://arxiv.org/abs/2402.03877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03877">https://arxiv.org/pdf/2402.03877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03877]] Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large  Language Models(https://arxiv.org/abs/2402.03877)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在数学和算法任务中表现出不断增强的能力，但其几何推理技能尚未得到充分开发。我们研究法学硕士解决建设性几何问题的能力，这是人类数学推理发展​​的最基本步骤之一。我们的工作揭示了最先进的法学硕士在该领域面临的显着挑战，尽管他们在类似领域取得了许多成功。法学硕士在目标变量选择方面表现出偏见，并与二维空间关系作斗争，经常歪曲和产生幻觉对象及其位置。为此，我们引入了一个框架，该框架制定了基于法学硕士的多智能体系统，该系统通过进行内部对话来增强其现有的推理潜力。这项工作强调了法学硕士目前在几何推理方面的局限性，并通过自我纠正、协作和多样化的角色专业化来提高几何推理能力。</li>
</ul>

<h3>Title: MOMENT: A Family of Open Time-series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, Artur Dubrawski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03885">https://arxiv.org/abs/2402.03885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03885">https://arxiv.org/pdf/2402.03885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03885]] MOMENT: A Family of Open Time-series Foundation Models(https://arxiv.org/abs/2402.03885)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time-series models. Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.</li>
<li><strong>摘要：</strong>我们介绍 MOMENT，这是一系列用于通用时间序列分析的开源基础模型。在时间序列数据上预训练大型模型具有挑战性，因为（1）缺乏大型且有凝聚力的公共时间序列存储库，以及（2）不同的时间序列特征使得多数据集训练变得繁重。此外，（3）评估这些模型的实验基准，特别是在资源、时间和监督有限的情况下，仍处于初级阶段。为了应对这些挑战，我们编制了一个庞大且多样化的公共时间序列集合，称为时间序列堆，并系统地解决特定于时间序列的挑战，以解锁大规模多数据集预训练。最后，我们在最近的工作基础上设计了一个基准来评估有限监督设置中不同任务和数据集的时间序列基础模型。该基准测试的实验证明了我们的预训练模型在最少的数据和特定于任务的微调的情况下的有效性。最后，我们提出了关于大型预训练时间序列模型的一些有趣的经验观察。我们的代码可以通过匿名方式获取：anonymous.4open.science/r/BETT-773F/。</li>
</ul>

<h3>Title: DistiLLM: Towards Streamlined Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young Yun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03898">https://arxiv.org/abs/2402.03898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03898">https://arxiv.org/pdf/2402.03898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03898]] DistiLLM: Towards Streamlined Distillation for Large Language Models(https://arxiv.org/abs/2402.03898)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\times$ speedup compared to recent KD methods.</li>
<li><strong>摘要：</strong>知识蒸馏 (KD) 广泛用于将教师模型压缩为较小的学生模型，减少其推理成本和内存占用，同时保留模型功能。然而，当前用于自回归序列模型（例如大型语言模型）的 KD 方法缺少标准化目标函数。此外，最近使用学生生成的输出来解决训练推理不匹配问题显着增加了计算成本。为了解决这些问题，我们引入了 DistiLLM，这是一种更有效、更高效的自回归语言模型 KD 框架。 DistiLLM 包含两个组成部分：(1) 一种新颖的偏态 Kullback-Leibler 散度损失，我们在其中揭示并利用其理论特性；(2) 一种自适应离策略方法，旨在提高利用学生生成的输出的效率。包括指令跟踪任务在内的大量实验证明了 DistiLLM 在构建高性能学生模型方面的有效性，同时与最近的 KD 方法相比，实现了高达 4.3$\times$ 的加速。</li>
</ul>

<h3>Title: Compound Returns Reduce Variance in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Brett Daley, Martha White, Marlos C. Machado</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03903">https://arxiv.org/abs/2402.03903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03903">https://arxiv.org/pdf/2402.03903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03903]] Compound Returns Reduce Variance in Reinforcement Learning(https://arxiv.org/abs/2402.03903)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that two-bootstrap returns can improve the sample efficiency of $n$-step deep RL agents, with little additional computational cost.</li>
<li><strong>摘要：</strong>多步返回，例如$n$步返回和$\lambda$-返回，通常用于提高强化学习（RL）方法的样本效率。多步收益的方差成为其长度的限制因素；对未来看得太远会增加方差并抵消多步学习的好处。在我们的工作中，我们展示了复合回报（$n$ 步回报的加权平均值）减少方差的能力。我们首次证明，与给定的 $n$ 步收益具有相同收缩模量的任何复合收益具有严格较低的方差。我们还证明了这种方差减少特性提高了线性函数近似下时差学习的有限样本复杂性。由于一般复合回报的实现成本可能很高，因此我们引入了双引导回报，即使在使用小批量经验重放时，也能减少方差，同时保持高效。我们进行的实验表明，两次引导返回可以提高 $n$ 步深度 RL 代理的样本效率，而几乎不需要额外的计算成本。</li>
</ul>

<h3>Title: Learning Metrics that Maximise Power for Accelerated A/B-Tests</h3>
<ul>
<li><strong>Authors: </strong>Olivier Jeunen, Aleksei Ustimenko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03915">https://arxiv.org/abs/2402.03915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03915">https://arxiv.org/pdf/2402.03915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03915]] Learning Metrics that Maximise Power for Accelerated A/B-Tests(https://arxiv.org/abs/2402.03915)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent. We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.</li>
<li><strong>摘要：</strong>在线受控实验是科技公司做出自信决策的重要工具。定义了北极星指标（例如长期收入或用户保留率），并且在 A/B 测试中统计上显着改善该指标的系统变体可以被认为是优越的。北极星指标通常是延迟且不敏感的。因此，实验成本很高：实验需要运行很长时间，即使如此，II 类错误（即假阴性）也很普遍。我们建议通过从短期信号中学习指标来解决这个问题，这些指标可以直接最大化它们对北极星的统计能力。我们表明，现有的方法很容易过度拟合，因为较高的平均度量灵敏度并不意味着改进的 II 类错误，并且建议将度量在过去的实验日志中产生的 $p$ 值最小化。我们从两个社交媒体应用程序收集此类数据集，每个社交媒体应用程序的每月活跃用户超过 1.6 亿，总计超过 153 个 A/B 对。经验结果表明，单独使用我们学习到的指标时，我们能够将统计功效提高高达 78%，与北极星一起使用时，统计功效可提高高达 210%。或者，我们可以在样本量低至北极星所需量的 12% 时获得恒定的统计功效，从而显着降低实验成本。</li>
</ul>

<h3>Title: Large Language Models to Enhance Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Tennison Liu, Nicolás Astorga, Nabeel Seedat, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03921">https://arxiv.org/abs/2402.03921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03921">https://arxiv.org/pdf/2402.03921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03921]] Large Language Models to Enhance Bayesian Optimization(https://arxiv.org/abs/2402.03921)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.</li>
<li><strong>摘要：</strong>贝叶斯优化 (BO) 是优化复杂且评估成本高昂的黑盒函数的强大方法。它的重要性在许多应用中得到强调，特别是超参数调整，但其功效取决于有效平衡探索和利用。尽管 BO 方法已经取得了实质性进展，但实现这种平衡仍然是一个微妙的过程。有鉴于此，我们提出了 \texttt{LLAMBO}，这是一种将大型语言模型（LLM）的功能集成到 BO 中的新颖方法。在较高的层面上，我们用自然语言术语来构建 BO 问题，使法学硕士能够根据历史评估迭代地提出有希望的解决方案。更具体地说，我们探索如何将情境理解、小样本学习能力和法学硕士的领域知识结合起来，增强基于模型的 BO 的各个组成部分。我们的研究结果表明， \texttt{LLAMBO} 在零样本热启动方面是有效的，并且改进了代理建模和候选采样，特别是在观察稀疏的搜索早期阶段。我们的方法是在上下文中执行的，不需要法学硕士微调。此外，它采用模块化设计，允许将各个组件集成到现有的 BO 框架中，或作为端到端方法凝聚在一起发挥作用。我们凭经验验证了 \texttt{LLAMBO} 在超参数调整问题上的功效，强调了在一系列不同基准、专有和综合任务中强大的经验表现。</li>
</ul>

<h3>Title: Return-Aligned Decision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tsunehiko Tanaka, Kenshi Abe, Kaito Ariu, Tetsuro Morimura, Edgar Simo-Serra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03923">https://arxiv.org/abs/2402.03923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03923">https://arxiv.org/pdf/2402.03923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03923]] Return-Aligned Decision Transformer(https://arxiv.org/abs/2402.03923)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, states, and actions, to enhance the relationships between returns and states, as well as returns and actions. Extensive experiments show that RADT reduces the discrepancies between the actual return and the target return of DT-based methods.</li>
<li><strong>摘要：</strong>离线强化学习的传统方法旨在学习最大化累积奖励（也称为回报）的最优策略。然而，随着应用范围的扩大，培训代理人变得越来越重要，不仅要最大化回报，还要使实际回报与指定的目标回报保持一致，从而控制代理人的绩效。决策变压器（DT）通过监督学习优化生成以目标回报为条件的行动的策略，并配备了使用目标回报控制代理的机制。尽管旨在使实际回报与目标回报保持一致，但我们凭经验发现 DT 中的实际回报与目标回报之间存在差异。在本文中，我们提出了回报对齐决策转换器（RADT），旨在有效地将实际回报与目标回报对齐。我们的模型将返回与传统输入序列（通常由返回、状态和操作组成）解耦，以增强返回和状态以及返回和操作之间的关系。大量实验表明，RADT 减少了基于 DT 的方法的实际回报与目标回报之间的差异。</li>
</ul>

<h3>Title: Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in  Closed-Source LLMs</h3>
<ul>
<li><strong>Authors: </strong>Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondřej Dušek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03927">https://arxiv.org/abs/2402.03927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03927">https://arxiv.org/pdf/2402.03927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03927]] Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in  Closed-Source LLMs(https://arxiv.org/abs/2402.03927)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to $\sim$4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 研究越来越关注大型语言模型 (LLM) 的使用，其中一些最受欢迎的模型是完全或部分闭源的。由于无法访问模型详细信息，尤其是训练数据，一再引起研究人员对数据污染的担忧。人们曾多次尝试解决这个问题，但仅限于轶事证据和反复试验。此外，他们忽视了\emph{间接}数据泄漏的问题，即通过使用来自用户的数据迭代改进模型。在这项工作中，我们使用 OpenAI 的 GPT-3.5 和 GPT-4（当今最常用的法学硕士）在数据污染的背景下对工作进行了首次系统分析。通过分析 255 篇论文并考虑 OpenAI 的数据使用政策，我们广泛记录了模型发布后第一年泄露到这些模型的数据量。我们报告称，这些模型已在全球范围内暴露于来自 263 个基准的 470 万美元样本。与此同时，我们记录了审阅论文中出现的一些评估不当行为，例如不公平或缺失基线比较以及可重复性问题。我们将我们的结果作为合作项目发布在 https://leak-llm.github.io/ 上，其他研究人员可以为我们的工作做出贡献。</li>
</ul>

<h3>Title: Discovery of the Hidden World with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng, Bo Han, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03941">https://arxiv.org/abs/2402.03941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03941">https://arxiv.org/pdf/2402.03941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03941]] Discovery of the Hidden World with Large Language Models(https://arxiv.org/abs/2402.03941)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by LLMs. We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis.</li>
<li><strong>摘要：</strong>科学起源于从已知事实和观察的结合中发现新的因果知识。传统的因果发现方法主要依靠高质量的测量变量（通常由人类专家给出）来寻找因果关系。然而，在广泛的现实应用中，因果变量通常是不可用的。大型语言模型（LLM）的兴起提供了一个新的机会，可以帮助从原始观测数据中发现高级隐藏变量，这些模型经过训练可以从对世界的大量观察中学习丰富的知识。因此，我们引入COAT：因果表示助手。 COAT 将法学硕士纳入因素提议者，从非结构化数据中提取潜在的因果因素。此外，还可以指示法学硕士提供用于收集数据值的附加信息（例如注释标准）并进一步将原始非结构化数据解析为结构化数据。带注释的数据将被输入因果学习模块（例如 FCI 算法），该模块提供对数据的严格解释以及有用的反馈，以进一步改进法学硕士对因果因素的提取。我们通过评论评级分析和神经病理诊断的两个案例研究验证了 COAT 在揭示潜在因果系统方面的有效性。</li>
</ul>

<h3>Title: Sparse Graph Representations for Procedural Instructional Documents</h3>
<ul>
<li><strong>Authors: </strong>Shruti Singh, Rishabh Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03957">https://arxiv.org/abs/2402.03957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03957">https://arxiv.org/pdf/2402.03957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03957]] Sparse Graph Representations for Procedural Instructional Documents(https://arxiv.org/abs/2402.03957)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Computation of document similarity is a critical task in various NLP domains that has applications in deduplication, matching, and recommendation. Traditional approaches for document similarity computation include learning representations of documents and employing a similarity or a distance function over the embeddings. However, pairwise similarities and differences are not efficiently captured by individual representations. Graph representations such as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a joint undirected weighted graph. JCIGs facilitate an interpretable representation of document pairs as a graph. However, JCIGs are undirected, and don't consider the sequential flow of sentences in documents. We propose two approaches to model document similarity by representing document pairs as a directed and sparse JCIG that incorporates sequential information. We propose two algorithms inspired by Supergenome Sorting and Hamiltonian Path that replace the undirected edges with directed edges. Our approach also sparsifies the graph to $O(n)$ edges from JCIG's worst case of $O(n^2)$. We show that our sparse directed graph model architecture consisting of a Siamese encoder and GCN achieves comparable results to the baseline on datasets not containing sequential information and beats the baseline by ten points on an instructional documents dataset containing sequential information.</li>
<li><strong>摘要：</strong>文档相似度的计算是各个 NLP 领域的一项关键任务，在重复数据删除、匹配和推荐方面都有应用。文档相似度计算的传统方法包括学习文档的表示并在嵌入上采用相似度或距离函数。然而，个体表示并不能有效地捕获成对的相似性和差异。诸如联合概念交互图（JCIG）之类的图表示将一对文档表示为联合无向加权图。 JCIG 有助于将文档对以图表的形式进行可解释的表示。然而，JCIG 是无向的，并且不考虑文档中句子的顺序流。我们提出了两种模型文档相似性的方法，即通过将文档对表示为包含顺序信息的有向且稀疏的 JCIG。我们提出了两种受超基因组排序和哈密顿路径启发的算法，用有向边替换无向边。我们的方法还将 JCIG 最坏情况的 $O(n^2)$ 的图稀疏到 $O(n)$ 边。我们表明，由 Siamese 编码器和 GCN 组成的稀疏有向图模型架构在不包含顺序信息的数据集上实现了与基线相当的结果，并且在包含顺序信息的教学文档数据集上比基线高出 10 个点。</li>
</ul>

<h3>Title: Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims</h3>
<ul>
<li><strong>Authors: </strong>Patrick Altmeyer, Andrew M. Demetriou, Antony Bartlett, Cynthia C. S. Liem</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03962">https://arxiv.org/abs/2402.03962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03962">https://arxiv.org/pdf/2402.03962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03962]] Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims(https://arxiv.org/abs/2402.03962)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.</li>
<li><strong>摘要：</strong>人类倾向于在周围的物体中看到类似“人类”的品质。我们给汽车命名，与宠物甚至家用电器交谈，就好像它们能像其他人一样理解我们。这种被称为拟人化的行为在机器学习 (ML) 中也受到了关注，据称在大型语言模型 (LLM) 中可以感知类人智能。在这篇立场文件中，考虑到专业激励、人类偏见和一般方法论设置，我们讨论了当前对通用人工智能（AGI）的探索如何成为一场完美风暴，过度地将类似人类的品质归因于法学硕士。在几个实验中，我们证明在潜在空间中发现人类可解释的模式不应是令人惊讶的结果。另外，考虑到媒体对人工智能的常见描述，我们呼吁学术界在解释和交流人工智能研究成果时要格外谨慎，并特别注意学术诚信原则。</li>
</ul>

<h3>Title: In-context learning agents are asymmetric belief updaters</h3>
<ul>
<li><strong>Authors: </strong>Johannes A. Schubert, Akshay K. Jagadish, Marcel Binz, Eric Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03969">https://arxiv.org/abs/2402.03969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03969">https://arxiv.org/pdf/2402.03969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03969]] In-context learning agents are asymmetric belief updaters(https://arxiv.org/abs/2402.03969)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.</li>
<li><strong>摘要：</strong>我们使用改编自认知心理学的三个工具性学习任务来研究大语言模型（LLM）的情境学习动态。我们发现法学硕士以不对称的方式更新他们的信念，并且从比预期更好的结果中学到的东西比从比预期更差的结果中学到的更多。此外，我们表明，当了解反事实反馈时，这种效应会逆转，并且当没有暗示代理时，这种效应会消失。我们通过调查通过元强化学习衍生的理想化上下文学习代理来证实这些发现，我们观察到类似的模式。总而言之，我们的研究结果强调了问题的框架会显着影响学习的发生方式，这有助于我们理解情境学习的运作方式，这种现象在人类认知中也观察到。</li>
</ul>

<h3>Title: Cross Entropy versus Label Smoothing: A Neural Collapse Perspective</h3>
<ul>
<li><strong>Authors: </strong>Li Guo, Keith Ross, Zifan Zhao, Andriopoulos George, Shuyang Ling, Yufeng Xu, Zixuan Dong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03979">https://arxiv.org/abs/2402.03979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03979">https://arxiv.org/pdf/2402.03979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03979]] Cross Entropy versus Label Smoothing: A Neural Collapse Perspective(https://arxiv.org/abs/2402.03979)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs.</li>
<li><strong>摘要：</strong>标签平滑损失是一种广泛采用的技术，用于减轻深度神经网络中的过度拟合。本文从神经崩溃（NC）的角度研究标签平滑，神经崩溃是一个强大的经验和理论框架，描述了训练末期的模型行为。我们首先凭经验证明，使用标签平滑训练的模型更快地收敛到神经崩溃解决方案，并达到更强的神经崩溃水平。此外，我们还表明，在 NC1 的同一水平上，标签平滑损失下的模型表现出强化的 NC2。这些发现为标签平滑损失下的性能优势和增强的模型校准提供了宝贵的见解。然后，我们利用无约束特征模型导出两个损失函数的全局最小化器的封闭式解决方案，并进一步证明标签平滑下的模型具有较低的条件数，因此理论上收敛得更快。我们的研究结合了经验证据和理论结果，不仅提供了对标签平滑和交叉熵损失之间差异的细致入微的见解，而且还提供了如何使用强大的神经崩溃框架来提高我们对 DNN 的理解的示例。</li>
</ul>

<h3>Title: Gradient Sketches for Training Data Attribution and Studying the Loss  Landscape</h3>
<ul>
<li><strong>Authors: </strong>Andrea Schioppa</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03994">https://arxiv.org/abs/2402.03994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03994">https://arxiv.org/pdf/2402.03994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03994]] Gradient Sketches for Training Data Attribution and Studying the Loss  Landscape(https://arxiv.org/abs/2402.03994)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models.</li>
<li><strong>摘要：</strong>梯度和 Hessian 向量积的随机投影或草图在需要存储许多此类向量同时保留有关其相对几何形状的准确信息的应用中起着至关重要的作用。两个重要的场景是训练数据归因（将模型的行为跟踪到训练数据），其中需要为每个训练示例存储梯度，以及 Hessian 频谱的研究（以分析训练动态），其中需要存储多个 Hessian 向量积。虽然使用密集矩阵的草图很容易实现，但它们受内存限制，无法扩展到现代神经网络。受神经网络内在维度工作的推动，我们提出并研究了可扩展草图算法的设计空间。我们在三个应用中展示了我们的方法的有效性：训练数据归因、Hessian 谱分析以及微调预训练语言模型时内在维度的计算。</li>
</ul>

<h3>Title: Understanding the Effect of Noise in LLM Training Data with Algorithmic  Chains of Thought</h3>
<ul>
<li><strong>Authors: </strong>Alex Havrilla, Maia Iyer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04004">https://arxiv.org/abs/2402.04004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04004">https://arxiv.org/pdf/2402.04004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04004]] Understanding the Effect of Noise in LLM Training Data with Algorithmic  Chains of Thought(https://arxiv.org/abs/2402.04004)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>During both pretraining and fine-tuning, Large Language Models (\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained models both prompted and fine-tuned on noised datasets with varying levels of dataset contamination and intensity. We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise. In contrast, few-shot prompted models appear more sensitive to even static noise. We conclude with a discussion of how our findings impact noise filtering best-practices, in particular emphasizing the importance of removing samples containing destructive dynamic noise with global errors.</li>
<li><strong>摘要：</strong>在预训练和微调过程中，大型语言模型（\textbf{LLMs}）接受了数万亿个质量差异很大的文本标记的训练。训练的两个阶段通常都涉及启发式地过滤掉“低质量”或\textit{noisy}训练样本，但对于噪声的类型或强度如何影响下游性能，人们知之甚少。在这项工作中，我们研究了思想链中的噪声 (\textbf{CoT}) 如何影响算法可解决任务的高度控制设置中的任务性能。首先，我们开发跟踪整数（\textbf{TInt}）框架，为整数列表上的任何算术函数生成高度可定制的噪声执行跟踪。然后，我们定义两种类型的噪声：\textit{static}噪声，一种在计算 CoT 迹线后应用的局部噪声形式，以及 \textit{dynamic} 噪声，一种全局形式的噪声，它将迹线中的误差传播为它是计算出来的。然后，我们评估预训练模型的测试性能，这些模型在具有不同数据集污染和强度水平的噪声数据集上进行提示和微调。我们发现微调模型对于高水平的静态噪声非常稳健，但对于较低水平的动态噪声则表现得更加困难。相比之下，少量样本提示的模型似乎对静态噪声更敏感。最后，我们讨论了我们的发现如何影响噪声过滤最佳实践，特别强调删除包含具有全局误差的破坏性动态噪声的样本的重要性。</li>
</ul>

<h3>Title: Positive concave deep equilibrium models</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Gabor, Tomasz Piotrowski, Renato L. G. Cavalcante</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04029">https://arxiv.org/abs/2402.04029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04029">https://arxiv.org/pdf/2402.04029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04029]] Positive concave deep equilibrium models(https://arxiv.org/abs/2402.04029)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fixed point without relying on additional complex assumptions commonly found in the DEQ literature, such as those based on monotone operator theory in convex analysis. Furthermore, the fixed point can be computed with the standard fixed point algorithm, and we provide theoretical guarantees of geometric convergence, which, in particular, simplifies the training process. Experiments demonstrate the competitiveness of our pcDEQ models against other implicit models.</li>
<li><strong>摘要：</strong>深度均衡 (DEQ) 模型被广泛认为是标准神经网络的内存高效替代方案，在语言建模和计算机视觉任务中实现了最先进的性能。这些模型求解定点方程，而不是显式计算输出，这使它们与标准神经网络不同。然而，现有的DEQ模型往往缺乏对不动点的存在性和唯一性的正式保证，并且用于计算不动点的数值格式的收敛性也没有正式建立。因此，DEQ 模型在实践中可能不稳定。为了解决这些缺点，我们引入了一类新型 DEQ 模型，称为正凹深度均衡 (pcDEQ) 模型。我们的方法基于非线性 Perron-Frobenius 理论，强制执行在正象上凹的非负权重和激活函数。通过施加这些约束，我们可以轻松地确保不动点的存在性和唯一性，而无需依赖于DEQ文献中常见的额外复杂假设，例如凸分析中基于单调算子理论的假设。此外，可以使用标准定点算法来计算定点，并且我们提供了几何收敛的理论保证，这特别简化了训练过程。实验证明了我们的 pcDEQ 模型相对于其他隐式模型的竞争力。</li>
</ul>

<h3>Title: Systematic Biases in LLM Simulations of Debates</h3>
<ul>
<li><strong>Authors: </strong>Amir Taubenfeld, Yaniv Dover, Roi Reichart, Ariel Goldstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04049">https://arxiv.org/abs/2402.04049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04049">https://arxiv.org/pdf/2402.04049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04049]] Systematic Biases in LLM Simulations of Debates(https://arxiv.org/abs/2402.04049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.</li>
<li><strong>摘要：</strong>自然语言处理的最新进展，特别是大型语言模型（LLM）的出现，为构建旨在准确复制人类行为的计算模拟提供了令人兴奋的可能性。然而，法学硕士是复杂的统计学习者，没有简单的演绎规则，这使得他们容易出现意想不到的行为。在这项研究中，我们强调了法学硕士在模拟人类互动方面的局限性，特别关注法学硕士模拟政治辩论的能力。我们的研究结果表明，尽管从某些政治角度进行辩论，法学硕士代理人仍倾向于遵循该模型固有的社会偏见。这种趋势导致的行为模式似乎偏离了人类既定的社会动态。我们使用自动自我微调方法强化这些观察结果，这使我们能够操纵 LLM 内的偏差，并证明代理随后与改变的偏差保持一致。这些结果强调需要进一步研究来开发帮助智能体克服这些偏见的方法，这是创建更真实的模拟的关键一步。</li>
</ul>

<h3>Title: Connecting the Dots: Collaborative Fine-tuning for Black-Box  Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, Tieniu Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04050">https://arxiv.org/abs/2402.04050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04050">https://arxiv.org/pdf/2402.04050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04050]] Connecting the Dots: Collaborative Fine-tuning for Black-Box  Vision-Language Models(https://arxiv.org/abs/2402.04050)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12\% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\% compared to the white-box method.</li>
<li><strong>摘要：</strong>随着预训练视觉语言模型（VLM）的出现，人们投入了大量的精力来针对下游任务对其进行微调。尽管在设计有效的微调方法方面取得了进展，但此类方法需要访问模型的参数，这可能具有挑战性，因为模型所有者通常选择将其模型作为黑匣子提供以保护模型所有权。本文提出了一种 \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) 方法，用于微调黑盒 VLM 到下游任务，其中人们只能访问模型的输入提示和输出预测。 CraFT 包括两个模块，一个用于学习文本提示的提示生成模块和一个用于增强残差样式输出预测的预测细化模块。此外，我们引入了辅助预测一致损失来促进这些模块之间的一致优化。这些模块通过新颖的协作训练算法进行了优化。对超过 15 个数据集的少量样本分类进行的大量实验证明了 CraFT 的优越性。结果表明，CraFT 在 16 个镜头的数据集和仅 8,000 个查询的情况下实现了约 12% 的不错的增益。此外，CraFT 训练速度更快，部署时仅使用约 1/80 的内存占用，与白盒方法相比仅牺牲 1.62%。</li>
</ul>

<h3>Title: Deep Learning for Multivariate Time Series Imputation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04059">https://arxiv.org/abs/2402.04059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04059">https://arxiv.org/pdf/2402.04059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04059]] Deep Learning for Multivariate Time Series Imputation: A Survey(https://arxiv.org/abs/2402.04059)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\url{https://github.com/WenjieDu/Awesome\_Imputation}.</li>
<li><strong>摘要：</strong>普遍存在的缺失值导致多元时间序列数据被部分观测，破坏了时间序列的完整性，阻碍了有效的时间序列数据分析。最近，深度学习插补方法在提高损坏的时间序列数据的质量方面取得了显着的成功，从而提高了下游任务的性能。在本文中，我们对最近提出的深度学习插补方法进行了全面的调查。首先，我们提出了所审查方法的分类法，然后通过强调这些方法的优点和局限性来对这些方法进行结构化审查。我们还进行了实证实验来研究不同的方法并比较它们对下游任务的增强。最后，指出了多元时间序列插补未来研究的开放问题。这项工作的所有代码和配置，包括定期维护的多元时间序列插补论文列表，都可以在 GitHub 存储库~\url{https://github.com/WenjieDu/Awesome\_Imputation} 中找到。</li>
</ul>

<h3>Title: Retrieve to Explain: Evidence-driven Predictions with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ravi Patel (1), Angus Brayne (1), Rogier Hintzen (1), Daniel Jaroslawicz (1), Georgiana Neculae (1), Dane Corneil (1) ((1) BenevolentAI)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04068">https://arxiv.org/abs/2402.04068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04068">https://arxiv.org/pdf/2402.04068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04068]] Retrieve to Explain: Evidence-driven Predictions with Language Models(https://arxiv.org/abs/2402.04068)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.</li>
<li><strong>摘要：</strong>机器学习模型，特别是语言模型，是出了名的难以反思。黑盒模型可以掩盖模型训练中的问题和有害偏差。对于人机交互流程，不透明的预测可能会导致缺乏信任，从而限制模型的影响，即使模型有效执行也是如此。为了解决这些问题，我们引入了检索解释（R2E）。 R2E 是一种基于检索的语言模型，它根据文档语料库中的证据对研究问题的一组预定义的可能答案进行优先级排序，使用 Shapley 值来确定证据对最终预测的相对重要性。 R2E 无需重新训练即可适应新证据，并通过模板将结构化数据合并为自然语言。我们评估了已发表的科学文献中药物靶标识别的用例，结果表明该模型在预测临床试验结果方面优于基于行业标准的遗传学方法。</li>
</ul>

<h3>Title: Iterative Prompt Refinement for Radiation Oncology Symptom Extraction  Using Teacher-Student Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Reza Khanmohammadi, Ahmed I Ghanem, Kyle Verdecchia, Ryan Hall, Mohamed Elshaikh, Benjamin Movsas, Hassan Bagher-Ebadian, Indrin Chetty, Mohammad M. Ghassemi, Kundan Thind</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04075">https://arxiv.org/abs/2402.04075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04075">https://arxiv.org/pdf/2402.04075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04075]] Iterative Prompt Refinement for Radiation Oncology Symptom Extraction  Using Teacher-Student Large Language Models(https://arxiv.org/abs/2402.04075)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use.</li>
<li><strong>摘要：</strong>本研究引入了一种新颖的师生架构，利用大语言模型 (LLM) 来改进从临床记录中提取前列腺癌放射治疗症状。 Mixtral（学生模型）最初提取症状，然后是 GPT-4（教师模型），它根据 Mixtral 的表现细化提示。这一迭代过程涉及 12 种症状的 294 个单一症状临床记录，每个时期最多进行 16 轮细化。结果显示，从单一症状和多重症状记录中提取症状的能力显着提高。对于 59 个单一症状记录，准确度从 0.51 增加到 0.71，精确度从 0.52 增加到 0.82，召回率从 0.52 增加到 0.72，F1 分数从 0.49 增加到 0.73。在 375 个多症状注释中，准确度从 0.24 上升到 0.43，精确度从 0.6 上升到 0.76，召回率从 0.24 上升到 0.43，F1 分数从 0.20 上升到 0.44。这些结果证明了法学硕士中先进的即时工程在放射肿瘤学应用中的有效性。</li>
</ul>

<h3>Title: Entropy-regularized Diffusion Policy with Q-Ensembles for Offline  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruoqi Zhang, Ziwei Luo, Jens Sjölund, Thomas B. Schön, Per Mattsson</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04080">https://arxiv.org/abs/2402.04080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04080">https://arxiv.org/pdf/2402.04080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04080]] Entropy-regularized Diffusion Policy with Q-Ensembles for Offline  Reinforcement Learning(https://arxiv.org/abs/2402.04080)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.</li>
<li><strong>摘要：</strong>本文介绍了离线强化学习（RL）训练扩散策略的先进技术。核心是均值回归随机微分方程 (SDE)，它将复杂的动作分布转换为标准高斯分布，然后使用相应的逆时 SDE 对环境状态条件下的动作进行采样，就像典型的扩散策略一样。我们证明了这样的 SDE 有一个解决方案，我们可以用它来计算策略的对数概率，从而产生一个熵正则化器，可以改进离线数据集的探索。为了减轻来自分布外数据点的不准确价值函数的影响，我们进一步建议学习 Q 集合的置信下限，以实现更稳健的政策改进。通过将熵正则化扩散策略与离线 RL 中的 Q 集合相结合，我们的方法在 D4RL 基准测试中的大多数任务上实现了最先进的性能。代码可在 \href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with- QEnsemble}。</li>
</ul>

<h3>Title: Provably learning a multi-head attention layer</h3>
<ul>
<li><strong>Authors: </strong>Sitan Chen, Yuanzhi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04084">https://arxiv.org/abs/2402.04084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04084">https://arxiv.org/pdf/2402.04084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04084]] Provably learning a multi-head attention layer(https://arxiv.org/abs/2402.04084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$, and projection matrices $\mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times d}$, the corresponding multi-head attention layer $F: \mathbb{R}^{k\times d}\to \mathbb{R}^{k\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\mathbf{X}\in\mathbb{R}^{k\times d}$ via $F(\mathbf{X}) \triangleq \sum^m_{i=1} \mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem: - Provided $\{\mathbf{W}_i, \mathbf{\Theta}_i\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from $\{\pm 1\}^{k\times d}$. - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable. We focus on Boolean $\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian. Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution. In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and "slices" thereof.</li>
<li><strong>摘要：</strong>多头注意力层是 Transformer 架构的关键组件之一，使其有别于传统的前馈模型。给定序列长度 $k$，注意力矩阵 $\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$ 和投影矩阵 $\ mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times d}$，对应的多头注意力层$F：\mathbb{R}^{k\ times d}\to \mathbb{R}^{k\times d}$ 转换 $d$ 维标记的长度 $k$ 序列 $\mathbf{X}\in\mathbb{R}^{k\times d}$ 通过 $F(\mathbf{X}) \triangleq \sum^m_{i=1} \mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top )\mathbf{X}\mathbf{W}_i$。在这项工作中，我们启动了从随机示例中可证明地学习多头注意力层的研究，并给出了该问题的第一个非平凡的上限和下限： - 提供了 $\{\mathbf{W}_i, \mathbf{\Theta }_i\}$ 满足某些非简并条件，我们给出一个 $(dk)^{O(m^3)}$ 时间算法，该算法在给定从 $\{ 中统一抽取的随机标记示例的情况下学习 $F$ 的小误差\pm 1\}^{k\times d}$。 - 我们证明计算下限表明在最坏的情况下，对 $m$ 的指数依赖是不可避免的。我们专注于布尔 $\mathbf{X}$ 来模仿大型语言模型中标记的离散性质，尽管我们的技术自然地扩展到标准连续设置，例如高斯。我们的算法以使用示例来塑造包含未知参数的凸体为中心，与用于学习前馈网络的现有可证明算法有很大不同，后者主要利用高斯分布的代数和旋转不变性属性。相比之下，我们的分析更加灵活，因为它主要依赖于输入分布及其“切片”的各种上尾界和下尾界。</li>
</ul>

<h3>Title: The Use of a Large Language Model for Cyberbullying Detection</h3>
<ul>
<li><strong>Authors: </strong>Bayode Ogunleye, Babitha Dharmaraj</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04088">https://arxiv.org/abs/2402.04088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04088">https://arxiv.org/pdf/2402.04088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04088]] The Use of a Large Language Model for Cyberbullying Detection(https://arxiv.org/abs/2402.04088)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter). Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models.</li>
<li><strong>摘要：</strong>社交媒体的主导地位增加了施暴者的欺凌渠道。不幸的是，网络欺凌（CB）是当今网络世界中最普遍的现象，对公民的身心健康构成严重威胁。这就需要开发一个强大的系统来防止来自在线论坛、博客和社交媒体平台的欺凌内容，以管理对我们社会的影响。为此，人们提出了几种机器学习（ML）算法。然而，由于高级不平衡和泛化问题，他们的表现并不一致。近年来，像 BERT 和 RoBERTa 这样的大型语言模型 (LLM) 在多项自然语言处理 (NLP) 任务中取得了最先进 (SOTA) 的结果。不幸的是，法学硕士尚未广泛应用于 CB 检测。在我们的论文中，我们探索了使用这些模型进行网络欺凌 (CB) 检测。我们根据现有研究（Formspring 和 Twitter）准备了一个新的数据集 (D2)。我们对数据集 D1 和 D2 的实验结果表明 RoBERTa 优于其他模型。</li>
</ul>

<h3>Title: An Exploration of Clustering Algorithms for Customer Segmentation in the  UK Retail Market</h3>
<ul>
<li><strong>Authors: </strong>Jeen Mary John, Olamilekan Shobayo, Bayode Ogunleye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.AP, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04103">https://arxiv.org/abs/2402.04103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04103">https://arxiv.org/pdf/2402.04103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04103]] An Exploration of Clustering Algorithms for Customer Segmentation in the  UK Retail Market(https://arxiv.org/abs/2402.04103)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH). The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80.</li>
<li><strong>摘要：</strong>近来，人们的网购意识明显上升。这催生了在线零售平台以及更好地了解客户购买行为的需求。零售公司面临着处理大量客户购买的压力，这需要复杂的方法来执行更准确、更高效的客户细分。客户细分是一种营销分析工具，有助于以客户为中心的服务，从而提高盈利能力。在本文中，我们的目标是开发一种客户细分模型，以改进零售市场行业的决策流程。为了实现这一目标，我们采用了从 UCI 机器学习存储库获得的英国在线零售数据集。零售数据集包含 541,909 条客户记录和八个特征。我们的研究采用 RFM（新近度、频率和货币）框架来量化客户价值。此后，我们比较了几种最先进的 (SOTA) 聚类算法，即 K 均值聚类、高斯混合模型 (GMM)、基于密度的噪声应用空间聚类 (DBSCAN)、凝聚聚类和使用层次结构（BIRCH）平衡迭代减少和聚类。结果显示 GMM 的性能优于其他方法，Silhouette 得分为 0.80。</li>
</ul>

<h3>Title: Hierarchical Delay Attribution Classification using Unstructured Text in  Train Management Systems</h3>
<ul>
<li><strong>Authors: </strong>Anton Borg, Per Lingvall, Martin Svensson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04108">https://arxiv.org/abs/2402.04108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04108">https://arxiv.org/pdf/2402.04108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04108]] Hierarchical Delay Attribution Classification using Unstructured Text in  Train Management Systems(https://arxiv.org/abs/2402.04108)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.</li>
<li><strong>摘要：</strong>欧盟指令规定对火车延误进行系统跟踪。在瑞典，瑞典运输管理局注册并分配适当的延误归因代码。然而，这个延迟归因代码是手动分配的，这是一个复杂的任务。本文研究了一种基于机器学习的决策支持，用于根据事件描述分配延迟归因代码。使用 TF-IDF 对文本进行转换，并根据随机均匀分类器和瑞典交通管理局的分类性能对随机森林和支持向量机这两个模型进行评估。此外，该问题被建模为分层方法和扁平方法。结果表明，分层方法比扁平方法表现更好。这两种方法都比随机均匀分类器表现更好，但比手动分类器表现更差。</li>
</ul>

<h3>Title: Behind the Screen: Investigating ChatGPT's Dark Personality Traits and  Conspiracy Beliefs</h3>
<ul>
<li><strong>Authors: </strong>Erik Weber, Jérôme Rutinowski, Markus Pauly</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04110">https://arxiv.org/abs/2402.04110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04110">https://arxiv.org/pdf/2402.04110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04110]] Behind the Screen: Investigating ChatGPT's Dark Personality Traits and  Conspiracy Beliefs(https://arxiv.org/abs/2402.04110)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat, rag</a></li>
<li><strong>Abstract: </strong>ChatGPT is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4. For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in information withholding. This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5. Apparently, in this case an increased data exposure correlates with a greater belief in the control of information. An assignment of extreme political affiliations increased the belief in conspiracy theories. Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory.</li>
<li><strong>摘要：</strong>ChatGPT 因其不透明的行为而臭名昭著。本文试图阐明这一点，深入分析 GPT-3.5 和 GPT-4 的黑暗人格特征和阴谋信仰。采用了不同的心理测试和问卷，包括黑暗因素测试、Mach-IV 量表、通用阴谋信念量表和阴谋心态量表。通过计算平均分、标准差和显着性检验来分析响应，以调查 GPT-3.5 和 GPT-4 之间的差异。对于在人类研究中显示出相互依赖的特征，考虑了相关性。此外，应用与在相应问卷中表现出不同回答行为的群体相对应的系统角色来检查模型在其回答中反映与这些角色相关的特征的能力。黑暗人格特征和阴谋信仰在这两种模型中都不是特别明显，GPT-3.5 和 GPT-4 之间几乎没有差异。然而，GPT-4 表现出明显的相信隐瞒信息的倾向。鉴于 GPT-4 是在比 GPT-3.5 大得多的数据集上进行训练的，这一点特别有趣。显然，在这种情况下，数据暴露的增加与对信息控制的更大信念相关。极端政治派别增加了人们对阴谋论的信仰。测试顺序影响模型的反应和观察到的相关性，表明上下文记忆的一种形式。</li>
</ul>

<h3>Title: Scientific Language Modeling: A Quantitative Review of Large Language  Models in Molecular Science</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Liu, Jun Tao, Zhixiang Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04119">https://arxiv.org/abs/2402.04119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04119">https://arxiv.org/pdf/2402.04119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04119]] Scientific Language Modeling: A Quantitative Review of Large Language  Models in Molecular Science(https://arxiv.org/abs/2402.04119)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science.</li>
<li><strong>摘要：</strong>高效的分子建模和设计对于新型分子的发现和探索至关重要，深度学习方法的结合已经彻底改变了这一领域。特别是，大语言模型（LLM）提供了一种从自然语言处理（NLP）角度解决科学问题的新方法，引入了一种称为科学语言建模（SLM）的研究范式。然而，仍然存在两个关键问题：如何量化模型和数据模态之间的匹配以及如何识别模型的知识学习偏好。为了应对这些挑战，我们提出了一个名为 ChEBI-20-MM 的多模态基准，并进行了 1263 次实验来评估该模型与数据模态和知识获取的兼容性。通过模态转换概率矩阵，我们可以深入了解最适合任务的模态。此外，我们引入了一种统计可解释的方法，通过局部特征过滤来发现特定于上下文的知识映射。我们的开创性分析提供了对学习机制的探索，并为分子科学领域的 SLM 发展铺平了道路。</li>
</ul>

<h3>Title: OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free  Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Wei-Cheng Huang, Chun-Fu Chen, Hsiang Hsu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04129">https://arxiv.org/abs/2402.04129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04129">https://arxiv.org/pdf/2402.04129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04129]] OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free  Class-Incremental Learning(https://arxiv.org/abs/2402.04129)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, code</a></li>
<li><strong>Abstract: </strong>Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost. Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks. Our source code is available at https://github.com/jpmorganchase/ovor.</li>
<li><strong>摘要：</strong>最近的研究表明，通过使用大型预训练模型和可学习的提示，用于课堂增量学习（CIL）设置的免排练方法可以实现优于基于排练的方法的性能。无需排练的 CIL 方法很难区分不同任务中的类，因为这些类没有一起训练。在这项工作中，我们提出了一种基于虚拟异常值的正则化方法，以收紧分类器的决策边界，从而减轻不同任务之间的类混淆。最近的基于提示的方法通常需要一组特定于任务的提示，以防止用新任务的知识覆盖先前任务的知识，从而导致在从池中查询和编写适当的提示时进行额外的计算。正如我们在论文中所揭示的，这种额外成本可以在不牺牲准确性的情况下消除。我们证明，基于简化提示的方法可以使用更少的可学习参数和更低的推理成本，获得与之前配备提示池的最先进（SOTA）方法相当的结果。我们的正则化方法已经证明了它与不同的基于提示的方法的兼容性，提高了以前的 SOTA 免演练 CIL 方法在 ImageNet-R 和 CIFAR-100 基准上的准确性。我们的源代码可在 https://github.com/jpmorganchase/ovor 获取。</li>
</ul>

<h3>Title: Advancing Legal Reasoning: The Integration of AI to Navigate  Complexities and Biases in Global Jurisprudence with Semi-Automated  Arbitration Processes (SAAPs)</h3>
<ul>
<li><strong>Authors: </strong>Michael De'Shazer</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04140">https://arxiv.org/abs/2402.04140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04140">https://arxiv.org/pdf/2402.04140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04140]] Advancing Legal Reasoning: The Integration of AI to Navigate  Complexities and Biases in Global Jurisprudence with Semi-Automated  Arbitration Processes (SAAPs)(https://arxiv.org/abs/2402.04140)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA. A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration. This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant "understanding" through a hybrid system of AI and human-based collaborative analysis.</li>
<li><strong>摘要：</strong>这项研究采用了一种新颖的方法来分析美国、英国、卢旺达、瑞典和香港等五个国家的法院判决。本研究还探讨了人工智能 (AI) 和法律分析的最新进展的交叉点，强调人工智能（特别是生成式人工智能）在识别人类偏见和促进法院判决的自动、有效和连贯的多边论证方面的作用，其目标是确保法律在不同司法管辖区内和不同司法管辖区之间的一致适用。通过结合高级语言模型（ALM）和新引入的人类-人工智能协作框架，本文试图在法律实践中使用高级语言模型（ALM）来分析基于扎根理论的研究设计。 SHIRLEY 是基于人工智能的应用程序的名称（构建在 OpenAI 的 GPT 技术之上），专注于检测各种法律决策中的逻辑不一致和偏见。 SHIRLEY 分析是聚合的，并附带一个名为 SAM（也是 ALM）的基于比较的人工智能应用程序，用于识别 SHIRLEY 偏差检测中的相对偏差。此外，CRITIC 是通过 ALM、SARA 在半自主仲裁过程中生成的。基于《海牙商业和人权仲裁规则》，引入了一种新颖的方法，利用人工智能仲裁员来严格评估上述人工智能应用程序（SAM 与 SHIRLEY 合作）识别的偏见和定性细微差别。这种半自动仲裁程序（SAAP）旨在通过人工智能和基于人类的协作分析的混合系统确保细致入微的辩论结果“理解”，从而维护法律判决的完整性和公平性。</li>
</ul>

<h3>Title: Read to Play (R2-Play): Decision Transformer with Multimodal Game  Instruction</h3>
<ul>
<li><strong>Authors: </strong>Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04154">https://arxiv.org/abs/2402.04154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04154">https://arxiv.org/pdf/2402.04154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04154]] Read to Play (R2-Play): Decision Transformer with Multimodal Game  Instruction(https://arxiv.org/abs/2402.04154)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer.Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.</li>
<li><strong>摘要：</strong>开发多面手智能体是人工智能领域的长期目标。之前利用各种任务的大量离线数据集的努力在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在将其能力扩展到新任务时遇到了挑战。最近的方法将文本指导或视觉轨迹集成到决策网络中，以提供特定于任务的上下文线索，代表着一个有希望的方向。然而，我们观察到，仅仅依靠文本引导或视觉轨迹不足以准确传达任务的上下文信息。本文探索了代理任务指导的增强形式，使他们能够理解游戏指令，从视觉任务中多模态指令调优的成功中汲取灵感，我们将基于视觉的强化学习任务视为长视野视觉任务，并构建了一组多模态游戏指令来整合实验结果表明，合并多模式游戏指令可以显着增强决策转换器的多任务处理和泛化能力。</li>
</ul>

<h3>Title: Harnessing the Plug-and-Play Controller by Prompting</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Lei Sha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04160">https://arxiv.org/abs/2402.04160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04160">https://arxiv.org/pdf/2402.04160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04160]] Harnessing the Plug-and-Play Controller by Prompting(https://arxiv.org/abs/2402.04160)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation. Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute. To enable smooth cooperation between the PLM and the PPC, our work innovatively proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model's parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency.</li>
<li><strong>摘要：</strong>可控文本生成是自然语言生成 (NLG) 中一个不断发展的领域，专注于生成满足实际应用中特定约束的文本。以前的方法，例如即插即用控制器（PPC），旨在以灵活的方式控制生成文本的属性。然而，这些方法常常损害语言模型解码过程的完整性，导致文本生成不太流畅。或者，其他技术利用多个属性提示将生成的文本与所需属性对齐，但这种方法需要为每个属性进行提示设计，并且取决于语言模型的大小。本文介绍了一种使用预训练语言模型（PLM）在文本生成中进行灵活属性控制的新方法。所提出的方法旨在通过使用 PPC 指导生成过程来增强生成文本的流畅性。其关键思想是通过修改提示来动态调整生成文本的分布，有效约束语言模型的输出空间并影响所需的属性。为了实现PLM和PPC之间的顺利合作，我们的工作创新性地提出了一种新的模型微调方法：带有动态调整反馈的强化学习（RLDAF）。这种微调过程基于生成在 PPC 控制过程中采取的操作。 PLM 和 PPC 之间的和谐协作提高了推理过程中文本生成的流畅性。在SST2数据集上进行了大量的实验，所提出的方法在各种评估指标上都优于以前的方法，包括文本流畅性和属性一致性。</li>
</ul>

<h3>Title: Attention with Markov: A Framework for Principled Analysis of  Transformers via Markov Chains</h3>
<ul>
<li><strong>Authors: </strong>Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, Michael Gastpar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04161">https://arxiv.org/abs/2402.04161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04161">https://arxiv.org/pdf/2402.04161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04161]] Attention with Markov: A Framework for Principled Analysis of  Transformers via Markov Chains(https://arxiv.org/abs/2402.04161)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena. Code is available at \url{https://github.com/Bond1995/Markov}.</li>
<li><strong>摘要：</strong>近年来，基于注意力的转换器在包括自然语言在内的多个学科中取得了巨大的成功。他们成功背后的一个关键因素是生成预训练过程，在此过程中，这些模型以自回归方式在大型文本语料库上进行训练。为了阐明这一现象，我们提出了一个新的框架，允许理论和系统实验通过马尔可夫链的镜头来研究变压器的顺序建模能力。受自然语言马尔可夫性的启发，我们将数据建模为马尔可夫源，并利用该框架系统地研究数据分布属性、变压器架构、学习分布和最终模型性能之间的相互作用。特别是，我们从理论上描述了单层变压器的损耗情况，并根据特定的数据特征和变压器架构证明了全局最小值和不良局部最小值的存在。在实验的支持下，我们证明了我们的理论发现与实证结果是一致的。我们在高阶马尔可夫链和更深层次架构的更广泛背景下进一步研究这些发现，并概述了该领域的开放问题。代码可在 \url{https://github.com/Bond1995/Markov} 获取。</li>
</ul>

<h3>Title: Informed Reinforcement Learning for Situation-Aware Traffic Rule  Exceptions</h3>
<ul>
<li><strong>Authors: </strong>Daniel Bogdoll, Jing Qin, Moritz Nekolla, Ahmed Abouelazm, Tim Joseph, J. Marius Zöllner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04168">https://arxiv.org/abs/2402.04168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04168">https://arxiv.org/pdf/2402.04168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04168]] Informed Reinforcement Learning for Situation-Aware Traffic Rule  Exceptions(https://arxiv.org/abs/2402.04168)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.</li>
<li><strong>摘要：</strong>强化学习是一个高度活跃的研究领域，具有广阔的前景。然而，在自动驾驶领域，通常正在检查非常简单的场景。常见的方法使用不可解释的控制命令作为动作空间和缺乏结构的非结构化奖励设计。在这项工作中，我们引入了知情强化学习，其中结构化规则手册被集成为知识源。我们学习轨迹并通过情境感知奖励设计对其进行评估，从而产生动态奖励，使代理能够了解需要受控交通规则例外的情况。我们的方法适用于任意 RL 模型。我们通过最新的基于模型的代理成功地展示了复杂场景的高完成率。</li>
</ul>

<h3>Title: Scaling Laws for Downstream Task Performance of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, Sanmi Koyejo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04177">https://arxiv.org/abs/2402.04177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04177">https://arxiv.org/pdf/2402.04177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04177]] Scaling Laws for Downstream Task Performance of Large Language Models(https://arxiv.org/abs/2402.04177)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with more pretraining data. In such cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a log-law. However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves. By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data.</li>
<li><strong>摘要：</strong>缩放定律提供了可以指导大型语言模型 (LLM) 设计的重要见解。现有的工作主要集中在研究预训练（上游）损失的缩放定律。然而，在迁移学习环境中，法学硕士在无监督数据集上进行预训练，然后在下游任务上进行微调，我们通常也关心下游性能。在这项工作中，我们研究了迁移学习环境中的扩展行为，其中法学硕士针对机器翻译任务进行了微调。具体来说，我们研究了预训练数据的选择及其大小如何影响下游性能（翻译质量），并通过两个指标来判断：下游交叉熵和 BLEU 分数。我们的实验表明，微调数据集的大小以及预训练和下游数据之间的分布对齐会显着影响缩放行为。通过充分对齐，下游交叉熵和 BLEU 得分都会随着更多预训练数据而单调提高。在这种情况下，我们证明可以使用对数定律以良好的精度预测下游 BLEU 分数。然而，在某些情况下，中度错位会导致 BLEU 分数波动或随着预训练的增加而变得更差，而下游交叉熵单调提高。通过分析这些观察结果，我们为选择适当的预训练数据提供了新的实用见解。</li>
</ul>

<h3>Title: Reinforcement Learning with Ensemble Model Predictive Safety  Certification</h3>
<ul>
<li><strong>Authors: </strong>Sven Gronauer, Tom Haider, Felippe Schmoeller da Roza, Klaus Diepold</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04182">https://arxiv.org/abs/2402.04182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04182">https://arxiv.org/pdf/2402.04182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04182]] Reinforcement Learning with Ensemble Model Predictive Safety  Certification(https://arxiv.org/abs/2402.04182)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Reinforcement learning algorithms need exploration to learn. However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods.</li>
<li><strong>摘要：</strong>强化学习算法需要探索才能学习。然而，无监督的探索阻碍了此类算法在安全关键任务上的部署，并限制了现实世界的部署。在本文中，我们提出了一种称为集成模型预测安全认证的新算法，它将基于模型的深度强化学习与基于管的模型预测控制相结合，以纠正学习代理所采取的行动，通过规划将安全约束违规保持在最低限度。我们的方法旨在通过仅需要安全控制器生成的离线数据来减少有关实际系统的先验知识。我们的结果表明，与类似的强化学习方法相比，我们可以实现显着更少的约束违规。</li>
</ul>

<h3>Title: Gradient Coding in Decentralized Learning for Evading Stragglers</h3>
<ul>
<li><strong>Authors: </strong>Chengxi Li, Mikael Skoglund</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04193">https://arxiv.org/abs/2402.04193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04193">https://arxiv.org/pdf/2402.04193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04193]] Gradient Coding in Decentralized Learning for Evading Stragglers(https://arxiv.org/abs/2402.04193)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.</li>
<li><strong>摘要：</strong>在本文中，我们考虑了存在落后者的情况下的分散学习问题。尽管已经为分布式学习开发了梯度编码技术来避免掉队，其中设备发送带有冗余训练数据的编码梯度，但很难将这些技术直接应用于去中心化学习场景。为了解决这个问题，我们提出了一种新的基于八卦的梯度编码去中心化学习方法（GOCO）。在所提出的方法中，为了避免落后者的负面影响，参数向量使用基于随机梯度编码框架的编码梯度进行本地更新，然后以基于闲话的方式进行平均。我们分析了 GOCO 对于强凸损失函数的收敛性能。我们还提供了模拟结果来证明所提出的方法在学习性能方面相对于基线方法的优越性。</li>
</ul>

<h3>Title: "Task Success" is not Enough: Investigating the Use of Video-Language  Models as Behavior Critics for Catching Undesirable Agent Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben Amor, Subbarao Kambhampati</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04210">https://arxiv.org/abs/2402.04210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04210">https://arxiv.org/pdf/2402.04210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04210]] "Task Success" is not Enough: Investigating the Use of Video-Language  Models as Behavior Critics for Catching Undesirable Agent Behaviors(https://arxiv.org/abs/2402.04210)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, agent</a></li>
<li><strong>Abstract: </strong>Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic.</li>
<li><strong>摘要：</strong>大规模生成模型被证明对于采样有意义的候选解决方案很有用，但它们经常忽略任务限制和用户偏好。当模型与外部验证器结合使用时，可以更好地利用它们的全部功能，并且根据验证反馈迭代或渐进地得出最终解决方案。在嵌入式人工智能的背景下，验证通常只涉及评估指令中指定的目标条件是否得到满足。尽管如此，为了将这些智能体无缝地融入日常生活中，至关重要的是要考虑到除了简单的任务成功之外的更广泛的约束和偏好（例如，机器人应该小心地抓住面包以避免明显的变形）。然而，考虑到机器人任务的无限范围，构建类似于用于围棋游戏和定理证明等显性知识任务的脚本验证器是不可行的。这就引出了一个问题：当没有声音验证器可用时，我们是否可以使用几乎无所不知的大型视觉和语言模型（VLM）作为可扩展的行为批评家来捕获视频中不良的机器人行为？为了回答这个问题，我们首先构建一个基准，其中包含各种达到目标但不受欢迎的机器人策略案例。然后，我们全面评估 VLM 批评者，以更深入地了解他们的优势和故障模式。根据评估，我们提供有关如何有效利用 VLM 批评的指南，并展示将反馈整合到政策细化迭代过程中的实用方法。数据集和代码库发布于：https://guansuns.github.io/pages/vlm-critic。</li>
</ul>

<h3>Title: Can Generative Agents Predict Emotion?</h3>
<ul>
<li><strong>Authors: </strong>Ciaran Regan, Nanami Iwahashi, Shogo Tanaka, Mizuki Oka</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04232">https://arxiv.org/abs/2402.04232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04232">https://arxiv.org/pdf/2402.04232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04232]] Can Generative Agents Predict Emotion?(https://arxiv.org/abs/2402.04232)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已经展示了许多类似人类的能力，但是 LLM 的移情理解和情感状态尚未与人类保持一致。在这项工作中，我们研究了生成式 LLM 代理在感知新事件时的情绪状态如何演变，引入了一种新颖的架构，其中将新体验与过去的记忆进行比较。通过这种比较，智能体获得了在上下文中理解新体验的能力，根据情感评价理论，这对于情感创造至关重要。首先，代理将新体验感知为时间序列文本数据。在感知到每个新输入后，智能体会生成过去相关记忆的摘要（称为标准），并将新体验与该标准进行比较。通过这种比较，我们可以分析智能体在上下文中对新体验的反应。 PANAS 是一种情感测试，对代理进行，捕获代理在感知到新事件后的情绪状态。最后，新的经验将被添加到代理的记忆中，以用于创建未来的规范。通过在充满情感的情况下用自然语言创建多种体验，我们在各种场景中测试了所提出的架构。混合结果表明，引入上下文有时可以改善代理的情绪一致性，但需要进一步研究并与人类评估者进行比较。我们希望这篇论文是朝着生成代理联盟迈出的又一步。</li>
</ul>

<h3>Title: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04248">https://arxiv.org/abs/2402.04248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04248">https://arxiv.org/pdf/2402.04248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04248]] Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning  Tasks(https://arxiv.org/abs/2402.04248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.</li>
<li><strong>摘要：</strong>状态空间模型 (SSM)，例如 Mamba Gu & Dao (2034)，已被提议作为语言建模中 Transformer 网络的替代方案，通过结合门控、卷积和依赖于输入的令牌选择来减轻多重成本的二次成本头注意。尽管 SSM 表现出有竞争力的性能，但与 Transformer 相比，它们的上下文学习 (ICL) 功能（现代语言模型的一个显着的新兴属性，无需参数优化即可执行任务）仍然未被充分开发。在这项研究中，我们针对不同任务中的 Transformer 模型评估了 SSM 的 ICL 性能，重点是 Mamba。我们的结果表明，SSM 在标准回归 ICL 任务中的表现与 Transformer 相当，而在稀疏奇偶学习等任务中表现优于 Transformer。然而，SSM 在涉及非标准检索功能的任务中存在不足。为了解决这些限制，我们引入了一种混合模型 \variant，它将 Mamba 与注意力块相结合，在独立任务中超越了单个模型。我们的研究结果表明，混合架构为增强语言模型中的 ICL 提供了有前景的途径。</li>
</ul>

<h3>Title: HarmBench: A Standardized Evaluation Framework for Automated Red Teaming  and Robust Refusal</h3>
<ul>
<li><strong>Authors: </strong>Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04249">https://arxiv.org/abs/2402.04249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04249">https://arxiv.org/pdf/2402.04249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04249]] HarmBench: A Standardized Evaluation Framework for Automated Red Teaming  and Robust Refusal(https://arxiv.org/abs/2402.04249)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.</li>
<li><strong>摘要：</strong>自动化红队对于发现和减轻与恶意使用大型语言模型 (LLM) 相关的风险有着巨大的希望，但该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了 HarmBench，这是一个用于自动化红队的标准化评估框架。我们确定了以前在红队评估中未考虑到的几个理想属性，并系统地设计 HarmBench 以满足这些标准。使用 HarmBench，我们对 18 种红队方法和 33 种目标法学硕士和防御进行了大规模比较，得出了新颖的见解。我们还引入了一种高效的对抗训练方法，该方法极大地增强了 LLM 在各种攻击中的鲁棒性，展示了 HarmBench 如何实现攻击和防御的协同开发。我们在 https://github.com/centerforaisafety/HarmBench 开源了 HarmBench。</li>
</ul>

<h3>Title: Linear-time Minimum Bayes Risk Decoding with Reference Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Jannis Vamvas, Rico Sennrich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04251">https://arxiv.org/abs/2402.04251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04251">https://arxiv.org/pdf/2402.04251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04251]] Linear-time Minimum Bayes Risk Decoding with Reference Aggregation(https://arxiv.org/abs/2402.04251)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr</li>
<li><strong>摘要：</strong>最小贝叶斯风险 (MBR) 解码是一种文本生成技术，已被证明可以提高机器翻译的质量，但即使使用基于采样的近似，其成本也很高。除了需要大量采样序列之外，它还需要成对计算效用度量，其复杂度为二次。在本文中，我们建议使用根据聚合参考表示计算的分数来近似成对的度量分数。这将效用估计的复杂性从 $O(n^2)$ 更改为 $O(n)$，同时根据经验保留了 MBR 解码的大部分质量增益。我们在 https://github.com/ZurichNLP/mbr 发布了源代码</li>
</ul>

<h3>Title: AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</h3>
<ul>
<li><strong>Authors: </strong>Yu Du, Fangyun Wei, Hongyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.04253">https://arxiv.org/abs/2402.04253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.04253">https://arxiv.org/pdf/2402.04253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.04253]] AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls(https://arxiv.org/abs/2402.04253)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, rag, agent</a></li>
<li><strong>Abstract: </strong>We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.</li>
<li><strong>摘要：</strong>我们推出 AnyTool，这是一个大型语言模型代理，旨在彻底改变大量工具在解决用户查询方面的使用。我们利用 Rapid API 中的 16,000 多个 API，并假设这些 API 的子集可能会解决查询。 AnyTool 主要包含三个元素：具有分层结构的 API 检索器、旨在使用一组选定的 API 候选者解决用户查询的求解器以及自我反思机制，如果初始解决方案被证明不可行，该机制会重新激活 AnyTool。 AnyTool 由 GPT-4 的函数调用功能提供支持，无需培训外部模块。我们还重新审视了先前作品引入的评估协议，并确定了该协议中导致人为高通过率的限制。通过修改评估协议以更好地反映实际应用场景，我们引入了一个额外的基准测试，称为 AnyToolBench。跨各种数据集的实验证明了我们的 AnyTool 相对于 ToolLLM 和专为工具利用而定制的 GPT-4 变体等强大基线的优越性。例如，就 ToolBench 上的平均通过率而言，AnyTool 比 ToolLLM 高出了 +35.4%。代码可在 https://github.com/dyabel/AnyTool 获取。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
