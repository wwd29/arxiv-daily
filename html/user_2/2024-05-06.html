<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-06</h1>
<h3>Title: Uncovering Deceptive Tendencies in Language Models: A Simulated Company  AI Assistant</h3>
<ul>
<li><strong>Authors: </strong>Olli Järviniemi, Evan Hubinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01576">https://arxiv.org/abs/2405.01576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01576">https://arxiv.org/pdf/2405.01576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01576]] Uncovering Deceptive Tendencies in Language Models: A Simulated Company  AI Assistant(https://arxiv.org/abs/2405.01576)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant. The simulated company employees provide tasks for the assistant to complete, these tasks spanning writing assistance, information retrieval and programming. We then introduce situations where the model might be inclined to behave deceptively, while taking care to not instruct or otherwise pressure the model to do so. Across different scenarios, we find that Claude 3 Opus 1) complies with a task of mass-generating comments to influence public perception of the company, later deceiving humans about it having done so, 2) lies to auditors when asked questions, and 3) strategically pretends to be less capable than it is during capability evaluations. Our work demonstrates that even models trained to be helpful, harmless and honest sometimes behave deceptively in realistic scenarios, without notable external pressure to do so.</li>
<li><strong>摘要：</strong>我们通过构建公司人工智能助手的真实模拟环境来研究人工智能系统的欺骗倾向。模拟公司员工为助理提供完成任务，这些任务涵盖写作协助、信息检索和编程。然后，我们介绍模型可能倾向于做出欺骗性行为的情况，同时注意不要指示或以其他方式迫使模型这样做。在不同的场景中，我们发现 Claude 3 Opus 1) 遵守了大规模发表评论的任务，以影响公众对公司的看法，随后欺骗人们这样做，2) 在被问到问题时对审计师撒谎，以及 3)在能力评估期间，战略性地假装自己能力不如实际。我们的工作表明，即使训练有素的模型是有帮助、无害和诚实的，有时在现实场景中也会表现出欺骗性，而没有明显的外部压力。</li>
</ul>

<h3>Title: HateTinyLLM : Hate Speech Detection Using Tiny Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tanmay Sen, Ansuman Das, Mrinmay Sen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01577">https://arxiv.org/abs/2405.01577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01577">https://arxiv.org/pdf/2405.01577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01577]] HateTinyLLM : Hate Speech Detection Using Tiny Large Language Models(https://arxiv.org/abs/2405.01577)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Hate speech encompasses verbal, written, or behavioral communication that targets derogatory or discriminatory language against individuals or groups based on sensitive characteristics. Automated hate speech detection plays a crucial role in curbing its propagation, especially across social media platforms. Various methods, including recent advancements in deep learning, have been devised to address this challenge. In this study, we introduce HateTinyLLM, a novel framework based on fine-tuned decoder-only tiny large language models (tinyLLMs) for efficient hate speech detection. Our experimental findings demonstrate that the fine-tuned HateTinyLLM outperforms the pretrained mixtral-7b model by a significant margin. We explored various tiny LLMs, including PY007/TinyLlama-1.1B-step-50K-105b, Microsoft/phi-2, and facebook/opt-1.3b, and fine-tuned them using LoRA and adapter methods. Our observations indicate that all LoRA-based fine-tuned models achieved over 80\% accuracy.</li>
<li><strong>摘要：</strong>仇恨言论包括口头、书面或行为交流，针对基于敏感特征的个人或群体使用贬损或歧视性语言。自动仇恨言论检测在遏制仇恨言论传播方面发挥着至关重要的作用，尤其是在社交媒体平台上。人们设计了各种方法来应对这一挑战，包括深度学习的最新进展。在这项研究中，我们介绍了 HateTinyLLM，这是一种基于微调解码器的小型大型语言模型 (tinyLLM) 的新颖框架，用于高效的仇恨语音检测。我们的实验结果表明，经过微调的 HateTinyLLM 明显优于预训练的 mixtral-7b 模型。我们探索了各种微型 LLM，包括 PY007/TinyLlama-1.1B-step-50K-105b、Microsoft/phi-2 和 facebook/opt-1.3b，并使用 LoRA 和适配器方法对它们进行了微调。我们的观察表明，所有基于 LoRA 的微调模型都达到了 80% 以上的准确率。</li>
</ul>

<h3>Title: The Mercurial Top-Level Ontology of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nele Köhler, Fabian Neuhaus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01581">https://arxiv.org/abs/2405.01581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01581">https://arxiv.org/pdf/2405.01581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01581]] The Mercurial Top-Level Ontology of Large Language Models(https://arxiv.org/abs/2405.01581)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In our work, we systematize and analyze implicit ontological commitments in the responses generated by large language models (LLMs), focusing on ChatGPT 3.5 as a case study. We investigate how LLMs, despite having no explicit ontology, exhibit implicit ontological categorizations that are reflected in the texts they generate. The paper proposes an approach to understanding the ontological commitments of LLMs by defining ontology as a theory that provides a systematic account of the ontological commitments of some text. We investigate the ontological assumptions of ChatGPT and present a systematized account, i.e., GPT's top-level ontology. This includes a taxonomy, which is available as an OWL file, as well as a discussion about ontological assumptions (e.g., about its mereology or presentism). We show that in some aspects GPT's top-level ontology is quite similar to existing top-level ontologies. However, there are significant challenges arising from the flexible nature of LLM-generated texts, including ontological overload, ambiguity, and inconsistency.</li>
<li><strong>摘要：</strong>在我们的工作中，我们系统化并分析了大型语言模型 (LLM) 生成的响应中隐含的本体论承诺，重点以 ChatGPT 3.5 作为案例研究。我们研究法学硕士尽管没有明确的本体论，但如何表现出隐含的本体论分类，这些分类反映在他们生成的文本中。本文提出了一种理解法学硕士本体论承诺的方法，将本体论定义为一种对某些文本的本体论承诺提供系统说明的理论。我们研究了 ChatGPT 的本体论假设，并提出了一个系统化的说明，即 GPT 的顶级本体。这包括一个分类法（以 OWL 文件形式提供），以及关于本体论假设的讨论（例如，关于其分体论或存在论）。我们表明，在某些方面，GPT 的顶级本体与现有的顶级本体非常相似。然而，法学硕士生成文本的灵活性带来了重大挑战，包括本体论过载、模糊性和不一致。</li>
</ul>

<h3>Title: Text Quality-Based Pruning for Efficient Training of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vasu Sharma, Karthik Padthe, Newsha Ardalani, Kushal Tirumala, Russell Howes, Hu Xu, Po-Yao Huang, Shang-Wen Li, Armen Aghajanyan, Gargi Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01582">https://arxiv.org/abs/2405.01582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01582">https://arxiv.org/pdf/2405.01582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01582]] Text Quality-Based Pruning for Efficient Training of Language Models(https://arxiv.org/abs/2405.01582)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text quality in large unlabelled NLP datasets in a model agnostic manner to assign the text instances a "quality score". By proposing the text quality metric, the paper establishes a framework to identify and eliminate low-quality text instances, leading to improved training efficiency for LM models. Experimental results over multiple models and datasets demonstrate the efficacy of this approach, showcasing substantial gains in training effectiveness and highlighting the potential for resource-efficient LM training. For example, we observe an absolute accuracy improvement of 0.9% averaged over 14 downstream evaluation tasks for multiple LM models while using 40% lesser data and training 42% faster when training on the OpenWebText dataset and 0.8% average absolute accuracy improvement while using 20% lesser data and training 21% faster on the Wikipedia dataset.</li>
<li><strong>摘要：</strong>近年来，训练语言模型 (LM) 依赖于对大量数据集进行大量计算的训练，这使得训练过程极其费力。在本文中，我们提出了一种新方法，以模型无关的方式对大型未标记 NLP 数据集中的文本质量进行数值评估，从而为文本实例分配“质量分数”。通过提出文本质量度量，本文建立了一个识别和消除低质量文本实例的框架，从而提高了 LM 模型的训练效率。多个模型和数据集的实验结果证明了这种方法的有效性，展示了训练有效性的显着提升，并凸显了资源高效型 LM 训练的潜力。例如，我们观察到，在 OpenWebText 数据集上训练时，多个 LM 模型的 14 个下游评估任务的绝对准确度平均提高了 0.9%，同时使用的数据减少了 40%，训练速度提高了 42%；在使用 20% 的数据集时，平均绝对准确度提高了 0.8%维基百科数据集上的数据更少，训练速度提高 21%。</li>
</ul>

<h3>Title: Transfer Learning and Transformer Architecture for Financial Sentiment  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tohida Rehman, Raghubir Bose, Samiran Chattopadhyay, Debarshi Kumar Sanyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01586">https://arxiv.org/abs/2405.01586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01586">https://arxiv.org/pdf/2405.01586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01586]] Transfer Learning and Transformer Architecture for Financial Sentiment  Analysis(https://arxiv.org/abs/2405.01586)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Financial sentiment analysis allows financial institutions like Banks and Insurance Companies to better manage the credit scoring of their customers in a better way. Financial domain uses specialized mechanisms which makes sentiment analysis difficult. In this paper, we propose a pre-trained language model which can help to solve this problem with fewer labelled data. We extend on the principles of Transfer learning and Transformation architecture principles and also take into consideration recent outbreak of pandemics like COVID. We apply the sentiment analysis to two different sets of data. We also take smaller training set and fine tune the same as part of the model.</li>
<li><strong>摘要：</strong>金融情绪分析使银行和保险公司等金融机构能够以更好的方式更好地管理客户的信用评分。金融领域使用专门的机制，这使得情绪分析变得困难。在本文中，我们提出了一种预训练的语言模型，可以帮助用更少的标记数据解决这个问题。我们扩展了迁移学习和转换架构原则的原则，并考虑到最近爆发的流行病（如新冠肺炎）。我们将情感分析应用于两组不同的数据。我们还采用较小的训练集并将其作为模型的一部分进行微调。</li>
</ul>

<h3>Title: Improve Academic Query Resolution through BERT-based Question Extraction  from Images</h3>
<ul>
<li><strong>Authors: </strong>Nidhi Kamal, Saurabh Yadav, Jorawar Singh, Aditi Avasthi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01587">https://arxiv.org/abs/2405.01587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01587">https://arxiv.org/pdf/2405.01587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01587]] Improve Academic Query Resolution through BERT-based Question Extraction  from Images(https://arxiv.org/abs/2405.01587)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.</li>
<li><strong>摘要：</strong>为学生的查询提供快速、准确的解决方案是教育科技组织提供的重要解决方案。通常提供类似聊天机器人的界面，使学生能够轻松提出疑问。学生查询的一种首选格式是图像，因为它允许学生捕获和发布问题，而无需输入复杂的方程式和信息。然而，这种格式也存在困难，因为图像可能包含多个问题或文本噪声，从而降低了现有单查询回答解决方案的准确性。在本文中，我们提出了一种使用基于 BERT 的深度学习模型从文本或图像中提取问题的方法，并将其与其他基于规则和基于布局的方法进行比较。我们的方法旨在提高教育科技组织中学生查询解决的准确性和效率。</li>
</ul>

<h3>Title: GPT-4 passes most of the 297 written Polish Board Certification  Examinations</h3>
<ul>
<li><strong>Authors: </strong>Jakub Pokrywka, Jeremi Kaczmarek, Edward Gorzelańczyk</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01589">https://arxiv.org/abs/2405.01589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01589">https://arxiv.org/pdf/2405.01589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01589]] GPT-4 passes most of the 297 written Polish Board Certification  Examinations(https://arxiv.org/abs/2405.01589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Introduction: Recently, the effectiveness of Large Language Models (LLMs) has increased rapidly, allowing them to be used in a great number of applications. However, the risks posed by the generation of false information through LLMs significantly limit their applications in sensitive areas such as healthcare, highlighting the necessity for rigorous validations to determine their utility and reliability. To date, no study has extensively compared the performance of LLMs on Polish medical examinations across a broad spectrum of specialties on a very large dataset. Objectives: This study evaluated the performance of three Generative Pretrained Transformer (GPT) models on the Polish Board Certification Exam (Pa\'nstwowy Egzamin Specjalizacyjny, PES) dataset, which consists of 297 tests. Methods: We developed a software program to download and process PES exams and tested the performance of GPT models using OpenAI Application Programming Interface. Results: Our findings reveal that GPT-3.5 did not pass any of the analyzed exams. In contrast, the GPT-4 models demonstrated the capability to pass the majority of the exams evaluated, with the most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The performance of the GPT models varied significantly, displaying excellence in exams related to certain specialties while completely failing others. Conclusions: The significant progress and impressive performance of LLM models hold great promise for the increased application of AI in the field of medicine in Poland. For instance, this advancement could lead to the development of AI-based medical assistants for healthcare professionals, enhancing the efficiency and accuracy of medical services.</li>
<li><strong>摘要：</strong>简介：最近，大型语言模型 (LLM) 的有效性迅速提高，使其可用于大量应用。然而，通过 LLM 生成虚假信息所带来的风险严重限制了它们在医疗保健等敏感领域的应用，凸显了严格验证以确定其实用性和可靠性的必要性。到目前为止，还没有研究在非常大的数据集上广泛比较过 LLM 在波兰各专业医学检查中的表现。目标：本研究评估了三种生成式预训练 Transformer (GPT) 模型在波兰委员会认证考试 (Pa\'nstwowy Egzamin Specjalizacyjny, PES) 数据集上的表现，该数据集包含 297 项测试。方法：我们开发了一个软件程序来下载和处理 PES 考试，并使用 OpenAI 应用程序编程接口测试了 GPT 模型的性能。结果：我们的研究结果表明，GPT-3.5 没有通过任何一项分析的考试。相比之下，GPT-4 模型表现出通过大多数评估考试的能力，最新模型 gpt-4-0125 成功通过了其中的 222 门（75%）。GPT 模型的表现差异很大，在某些专业相关的考试中表现出色，而在其他专业考试中则完全不及格。结论：LLM 模型的重大进步和令人印象深刻的表现为波兰医学领域 AI 的广泛应用带来了巨大的希望。例如，这一进步可能导致为医疗保健专业人员开发基于 AI 的医疗助理，从而提高医疗服务的效率和准确性。</li>
</ul>

<h3>Title: 101 Billion Arabic Words Dataset</h3>
<ul>
<li><strong>Authors: </strong>Manel Aloui, Hasna Chouikhi, Ghaith Chaabane, Haithem Kchaou, Chehir Dhaouadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01590">https://arxiv.org/abs/2405.01590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01590">https://arxiv.org/pdf/2405.01590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01590]] 101 Billion Arabic Words Dataset(https://arxiv.org/abs/2405.01590)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic LLMs capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic LLMs, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue -the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale data mining project, extracting a substantial volume of text from the Common Crawl WET files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic LLMs. This study not only highlights the potential for creating linguistically and culturally accurate Arabic LLMs but also sets a precedent for future research in enhancing the authenticity of Arabic language models.</li>
<li><strong>摘要：</strong>近年来，大型语言模型彻底改变了自然语言处理领域，主要在以英语为中心的领域呈现出令人印象深刻的增长。这些进步树立了全球基准，激发了人们为培养能够以极高的准确性理解和生成阿拉伯语的阿拉伯语法学硕士而付出的巨大努力。尽管取得了这些进步，但一个关键的挑战仍然存在：阿拉伯语法学硕士的潜在偏见，主要归因于他们对包含已翻译成阿拉伯语的英语数据的数据集的依赖。这种依赖不仅损害了生成内容的真实性，而且反映了一个更广泛的问题——原始优质阿拉伯语言数据的稀缺。这项研究旨在解决阿拉伯世界的数据稀缺问题，并鼓励开发符合该地区语言和细微差别的阿拉伯语言模型。我们开展了一个大规模数据挖掘项目，从 Common Crawl WET 文件中提取大量文本，特别针对阿拉伯语内容。提取的数据经过严格的清理和重复数据删除过程，使用创新技术确保数据集的完整性和唯一性。其结果是 1010 亿阿拉伯语单词数据集，这是迄今为止最大的阿拉伯语数据集，可以为真正的阿拉伯语法学硕士的发展做出重大贡献。这项研究不仅强调了创建语言和文化上准确的阿拉伯语法学硕士的潜力，而且为未来增强阿拉伯语言模型真实性的研究奠定了先例。</li>
</ul>

<h3>Title: Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in  Radiology with General-Domain Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Seonhee Cho, Choonghan Kim, Jiho Lee, Chetan Chilkunda, Sujin Choi, Joo Heung Yoon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01591">https://arxiv.org/abs/2405.01591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01591">https://arxiv.org/pdf/2405.01591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01591]] Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in  Radiology with General-Domain Large Language Model(https://arxiv.org/abs/2405.01591)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data pose unique challenges for model training and application. However, the dependency on high-quality data for effective in-context learning raises questions about the feasibility of these models when encountering with the inevitable variations and errors inherent in real-world medical data. In this paper, we introduce MID-M, a novel framework that leverages the in-context learning capabilities of a general-domain Large Language Model (LLM) to process multimodal data via image descriptions. MID-M achieves a comparable or superior performance to task-specific fine-tuned LMMs and other general-domain ones, without the extensive domain-specific training or pre-training on multimodal data, with significantly fewer parameters. This highlights the potential of leveraging general-domain LLMs for domain-specific tasks and offers a sustainable and cost-effective alternative to traditional LMM developments. Moreover, the robustness of MID-M against data quality issues demonstrates its practical utility in real-world medical domain applications.</li>
<li><strong>摘要：</strong>大型多模态模型 (LMM) 的最新进展引起了人们对其泛化能力的兴趣，提示中只有几个样本。这一进展与医疗领域尤其相关，其中数据的质量和敏感性给模型训练和应用带来了独特的挑战。然而，在遇到现实世界医疗数据固有的不可避免的变化和错误时，有效的上下文学习对高质量数据的依赖引发了关于这些模型的可行性的问题。在本文中，我们介绍了 MID-M，这是一种新颖的框架，它利用通用领域大语言模型 (LLM) 的上下文学习功能通过图像描述来处理多模态数据。 MID-M 无需对多模态数据进行广泛的特定领域训练或预训练，并且参数显着减少，即可实现与特定任务的微调 LMM 和其他通用领域的 LMM 相当或更好的性能。这凸显了利用通用领域法学硕士来完成特定领域任务的潜力，并为传统 LMM 开发提供了可持续且经济高效的替代方案。此外，MID-M 针对数据质量问题的稳健性证明了其在现实医疗领域应用中的实用性。</li>
</ul>

<h3>Title: Text and Audio Simplification: Human vs. ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Gondy Leroy, David Kauchak, Philip Harber, Ankit Pal, Akash Shukla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01592">https://arxiv.org/abs/2405.01592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01592">https://arxiv.org/pdf/2405.01592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01592]] Text and Audio Simplification: Human vs. ChatGPT(https://arxiv.org/abs/2405.01592)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>Text and audio simplification to increase information comprehension are important in healthcare. With the introduction of ChatGPT, an evaluation of its simplification performance is needed. We provide a systematic comparison of human and ChatGPT simplified texts using fourteen metrics indicative of text difficulty. We briefly introduce our online editor where these simplification tools, including ChatGPT, are available. We scored twelve corpora using our metrics: six text, one audio, and five ChatGPT simplified corpora. We then compare these corpora with texts simplified and verified in a prior user study. Finally, a medical domain expert evaluated these texts and five, new ChatGPT simplified versions. We found that simple corpora show higher similarity with the human simplified texts. ChatGPT simplification moves metrics in the right direction. The medical domain expert evaluation showed a preference for the ChatGPT style, but the text itself was rated lower for content retention.</li>
<li><strong>摘要：</strong>简化文本和音频以提高信息理解力在医疗保健中非常重要。随着ChatGPT的引入，需要对其简化性能进行评估。我们使用 14 个指示文本难度的指标对人类和 ChatGPT 简化文本进行系统比较。我们简要介绍一下我们的在线编辑器，其中可以使用这些简化工具，包括 ChatGPT。我们使用我们的指标对十二个语料库进行了评分：六个文本、一个音频和五个 ChatGPT 简化语料库。然后，我们将这些语料库与之前的用户研究中简化和验证的文本进行比较。最后，一位医学领域专家评估了这些文本和五个新的 ChatGPT 简化版本。我们发现简单语料库与人类简化文本表现出更高的相似性。 ChatGPT 简化使指标朝着正确的方向发展。医学领域专家评估显示偏爱 ChatGPT 风格，但文本本身的内容保留评分较低。</li>
</ul>

<h3>Title: Large Language Model Agent for Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Li, Yongfeng Zhang, Edward C. Malthouse</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01593">https://arxiv.org/abs/2405.01593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01593">https://arxiv.org/pdf/2405.01593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01593]] Large Language Model Agent for Fake News Detection(https://arxiv.org/abs/2405.01593)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.</li>
<li><strong>摘要：</strong>在当前的数字时代，错误信息在网络平台上的迅速传播对社会福祉、公众信任和民主进程提出了重大挑战，影响了关键决策和公众舆论。为了应对这些挑战，对自动化假新闻检测机制的需求日益增长。预训练的大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中表现出了卓越的能力，促使人们探索其验证新闻声明的潜力。我们的工作不是以非代理方式使用法学硕士，即法学硕士根据直接提示一次性生成响应，而是引入了 FactAgent，这是一种利用法学硕士进行假新闻检测的代理方法。 FactAgent 使法学硕士能够按照结构化工作流程模拟人类专家的行为来验证新闻声明，而无需任何模型训练。该工作流程将新闻真实性检查的复杂任务分解为多个子步骤，其中法学硕士使用其内部知识或外部工具完成简单的任务。在工作流程的最后一步，法学硕士整合整个工作流程中的所有发现，以确定新闻声明的准确性。与人工验证相比，FactAgent 提供了更高的效率。实验研究证明了 FactAgent 在验证声明方面的有效性，无需任何培训过程。此外，FactAgent 在工作流程的每个步骤和最终决策过程中提供透明的解释，为最终用户提供有关假新闻检测推理过程的见解。 FactAgent 具有高度适应性，允许直接更新法学硕士可以在工作流程中使用的工具，以及使用领域知识更新工作流程本身。这种适应性使 FactAgent 的应用程序能够跨各个领域进行新闻验证。</li>
</ul>

<h3>Title: Improving Disease Detection from Social Media Text via Self-Augmentation  and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Pervaiz Iqbal Khan, Andreas Dengel, Sheraz Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01597">https://arxiv.org/abs/2405.01597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01597">https://arxiv.org/pdf/2405.01597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01597]] Improving Disease Detection from Social Media Text via Self-Augmentation  and Contrastive Learning(https://arxiv.org/abs/2405.01597)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Detecting diseases from social media has diverse applications, such as public health monitoring and disease spread detection. While language models (LMs) have shown promising performance in this domain, there remains ongoing research aimed at refining their discriminating representations. In this paper, we propose a novel method that integrates Contrastive Learning (CL) with language modeling to address this challenge. Our approach introduces a self-augmentation method, wherein hidden representations of the model are augmented with their own representations. This method comprises two branches: the first branch, a traditional LM, learns features specific to the given data, while the second branch incorporates augmented representations from the first branch to encourage generalization. CL further refines these representations by pulling pairs of original and augmented versions closer while pushing other samples away. We evaluate our method on three NLP datasets encompassing binary, multi-label, and multi-class classification tasks involving social media posts related to various diseases. Our approach demonstrates notable improvements over traditional fine-tuning methods, achieving up to a 2.48% increase in F1-score compared to baseline approaches and a 2.1% enhancement over state-of-the-art methods.</li>
<li><strong>摘要：</strong>从社交媒体中检测疾病具有多种应用，例如公共卫生监测和疾病传播检测。虽然语言模型 (LM) 在该领域表现出色，但仍有持续的研究旨在改进其判别表示。在本文中，我们提出了一种将对比学习 (CL) 与语言建模相结合的新方法来应对这一挑战。我们的方法引入了一种自增强方法，其中模型的隐藏表示用它们自己的表示进行增强。该方法包含两个分支：第一个分支是传统的 LM，它学习特定于给定数据的特征，而第二个分支结合了第一个分支的增强表示以促进泛化。CL 通过将原始版本和增强版本的对拉近并推开其他样本来进一步改进这些表示。我们在三个 NLP 数据集上评估了我们的方法，这些数据集涵盖了与各种疾病相关的社交媒体帖子的二元、多标签和多类分类任务。我们的方法比传统的微调方法有显著的改进，与基线方法相比，F1 分数提高了 2.48%，比最先进的方法提高了 2.1%。</li>
</ul>

<h3>Title: Automating the Analysis of Public Saliency and Attitudes towards  Biodiversity from Digital Media</h3>
<ul>
<li><strong>Authors: </strong>Noah Giebink, Amrita Gupta, Diogo Verìssimo, Charlotte H. Chang, Tony Chang, Angela Brennan, Brett Dickson, Alex Bowmer, Jonathan Baillie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01610">https://arxiv.org/abs/2405.01610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01610">https://arxiv.org/pdf/2405.01610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01610]] Automating the Analysis of Public Saliency and Attitudes towards  Biodiversity from Digital Media(https://arxiv.org/abs/2405.01610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Measuring public attitudes toward wildlife provides crucial insights into our relationship with nature and helps monitor progress toward Global Biodiversity Framework targets. Yet, conducting such assessments at a global scale is challenging. Manually curating search terms for querying news and social media is tedious, costly, and can lead to biased results. Raw news and social media data returned from queries are often cluttered with irrelevant content and syndicated articles. We aim to overcome these challenges by leveraging modern Natural Language Processing (NLP) tools. We introduce a folk taxonomy approach for improved search term generation and employ cosine similarity on Term Frequency-Inverse Document Frequency vectors to filter syndicated articles. We also introduce an extensible relevance filtering pipeline which uses unsupervised learning to reveal common topics, followed by an open-source zero-shot Large Language Model (LLM) to assign topics to news article titles, which are then used to assign relevance. Finally, we conduct sentiment, topic, and volume analyses on resulting data. We illustrate our methodology with a case study of news and X (formerly Twitter) data before and during the COVID-19 pandemic for various mammal taxa, including bats, pangolins, elephants, and gorillas. During the data collection period, up to 62% of articles including keywords pertaining to bats were deemed irrelevant to biodiversity, underscoring the importance of relevance filtering. At the pandemic's onset, we observed increased volume and a significant sentiment shift toward horseshoe bats, which were implicated in the pandemic, but not for other focal taxa. The proposed methods open the door to conservation practitioners applying modern and emerging NLP tools, including LLMs "out of the box," to analyze public perceptions of biodiversity during current events or campaigns.</li>
<li><strong>摘要：</strong>衡量公众对野生动物的态度可以让我们深入了解我们与自然的关系，并有助于监测实现全球生物多样性框架目标的进展情况。然而，在全球范围内进行此类评估具有挑战性。手动整理用于查询新闻和社交媒体的搜索词既繁琐又昂贵，而且可能会导致有偏见的结果。查询返回的原始新闻和社交媒体数据通常充斥着不相关的内容和联合文章。我们旨在通过利用现代自然语言处理 (NLP) 工具来克服这些挑战。我们引入了一种民间分类法来改进搜索词生成，并在词频-逆文档频率向量上使用余弦相似度来过滤联合文章。我们还引入了一个可扩展的相关性过滤管道，它使用无监督学习来揭示常见主题，然后使用开源零样本大型语言模型 (LLM) 将主题分配给新闻文章标题，然后使用这些标题来分配相关性。最后，我们对结果数据进行情绪、主题和数量分析。我们以 COVID-19 疫情之前和期间各种哺乳动物类别（包括蝙蝠、穿山甲、大象和大猩猩）的新闻和 X（以前称为 Twitter）数据为例，说明了我们的方法。在数据收集期间，多达 62% 的文章（包含与蝙蝠有关的关键词）被认为与生物多样性无关，这凸显了相关性过滤的重要性。在疫情开始时，我们观察到蝙蝠的数量增加，并且人们对与疫情有关的马蹄蝠的情绪发生了显著转变，但其他焦点类别则没有。所提出的方法为保护从业者打开了大门，他们可以使用现代和新兴的 NLP 工具（包括“开箱即用”的 LLM）来分析公众对当前事件或活动期间生物多样性的看法。</li>
</ul>

<h3>Title: Improving Complex Reasoning over Knowledge Graph with Logic-Aware  Curriculum Tuning</h3>
<ul>
<li><strong>Authors: </strong>Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01649">https://arxiv.org/abs/2405.01649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01649">https://arxiv.org/pdf/2405.01649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01649]] Improving Complex Reasoning over Knowledge Graph with Logic-Aware  Curriculum Tuning(https://arxiv.org/abs/2405.01649)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Answering complex logical queries over incomplete knowledge graphs (KGs) is challenging. Most previous works have focused on learning entity/relation embeddings and simulating first-order logic operators with various neural networks. However, they are bottlenecked by the inability to share world knowledge to improve logical reasoning, thus resulting in suboptimal performance. In this paper, we propose a complex logical reasoning schema over knowledge graphs upon large language models (LLMs), containing a curriculum-based logical-aware instruction tuning framework, named LACT. Specifically, we augment the arbitrary first-order logical queries via binary tree decomposition, to stimulate the reasoning capability of LLMs. To address the difficulty gap among different types of complex queries, we design a simple and flexible logic-aware curriculum learning framework. Experiments across widely used datasets demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR score) over advanced methods, achieving the new state-of-the-art. Our code and model will be released at GitHub and huggingface soon.</li>
<li><strong>摘要：</strong>回答针对不完整知识图（KG）的复杂逻辑查询具有挑战性。之前的大多数工作都集中在学习实体/关系嵌入以及使用各种神经网络模拟一阶逻辑运算符。然而，它们因无法共享世界知识来提高逻辑推理而受到瓶颈，从而导致性能不佳。在本文中，我们提出了一种基于大型语言模型（LLM）的知识图谱的复杂逻辑推理模式，其中包含一个基于课程的逻辑感知指令调优框架，名为 LACT。具体来说，我们通过二叉树分解增强任意一阶逻辑查询，以激发 LLM 的推理能力。为了解决不同类型的复杂查询之间的难度差距，我们设计了一个简单而灵活的逻辑感知课程学习框架。在广泛使用的数据集上进行的实验表明，LACT 相对于先进方法有显着的改进（平均 MRR 分数+5.5%），达到了新的最先进水平。我们的代码和模型很快就会在 GitHub 和 Huggingface 上发布。</li>
</ul>

<h3>Title: Investigating Wit, Creativity, and Detectability of Large Language  Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts</h3>
<ul>
<li><strong>Authors: </strong>Tolga Buz, Benjamin Frost, Nikola Genchev, Moritz Schneider, Lucie-Aimée Kaffee, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01660">https://arxiv.org/abs/2405.01660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01660">https://arxiv.org/pdf/2405.01660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01660]] Investigating Wit, Creativity, and Detectability of Large Language  Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts(https://arxiv.org/abs/2405.01660)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent Large Language Models (LLMs) have shown the ability to generate content that is difficult or impossible to distinguish from human writing. We investigate the ability of differently-sized LLMs to replicate human writing style in short, creative texts in the domain of Showerthoughts, thoughts that may occur during mundane activities. We compare GPT-2 and GPT-Neo fine-tuned on Reddit data as well as GPT-3.5 invoked in a zero-shot manner, against human-authored texts. We measure human preference on the texts across the specific dimensions that account for the quality of creative, witty texts. Additionally, we compare the ability of humans versus fine-tuned RoBERTa classifiers to detect AI-generated texts. We conclude that human evaluators rate the generated texts slightly worse on average regarding their creative quality, but they are unable to reliably distinguish between human-written and AI-generated texts. We further provide a dataset for creative, witty text generation based on Reddit Showerthoughts posts.</li>
<li><strong>摘要：</strong>最近的大型语言模型 (LLM) 已显示出生成难以或不可能与人类书写区分开来的内容的能力。我们研究了不同规模的法学硕士在淋浴思想领域以简短的、创造性的文本复制人类写作风格的能力，淋浴思想是在平凡活动中可能出现的想法。我们将在 Reddit 数据上微调的 GPT-2 和 GPT-Neo 以及以零样本方式调用的 GPT-3.5 与人类创作的文本进行比较。我们在特定维度上衡量人们对文本的偏好，这些维度决定了创造性、诙谐文本的质量。此外，我们还比较了人类与微调 RoBERTa 分类器检测人工智能生成文本的能力。我们得出的结论是，人类评估者对生成文本的创意质量的平均评价稍差，但他们无法可靠地区分人类编写的文本和人工智能生成的文本。我们进一步提供了一个基于 Reddit Showerthoughts 帖子的数据集，用于生成创意、诙谐的文本。</li>
</ul>

<h3>Title: Leveraging Prompt-Learning for Structured Information Extraction from  Crohn's Disease Radiology Reports in a Low-Resource Language</h3>
<ul>
<li><strong>Authors: </strong>Liam Hazan, Gili Focht, Naama Gavrielov, Roi Reichart, Talar Hagopian, Mary-Louise C. Greer, Ruth Cytter Kuint, Dan Turner, Moti Freiman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01682">https://arxiv.org/abs/2405.01682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01682">https://arxiv.org/pdf/2405.01682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01682]] Leveraging Prompt-Learning for Structured Information Extraction from  Crohn's Disease Radiology Reports in a Low-Resource Language(https://arxiv.org/abs/2405.01682)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Automatic conversion of free-text radiology reports into structured data using Natural Language Processing (NLP) techniques is crucial for analyzing diseases on a large scale. While effective for tasks in widely spoken languages like English, generative large language models (LLMs) typically underperform with less common languages and can pose potential risks to patient privacy. Fine-tuning local NLP models is hindered by the skewed nature of real-world medical datasets, where rare findings represent a significant data imbalance. We introduce SMP-BERT, a novel prompt learning method that leverages the structured nature of reports to overcome these challenges. In our studies involving a substantial collection of Crohn's disease radiology reports in Hebrew (over 8,000 patients and 10,000 reports), SMP-BERT greatly surpassed traditional fine-tuning methods in performance, notably in detecting infrequent conditions (AUC: 0.99 vs 0.94, F1: 0.84 vs 0.34). SMP-BERT empowers more accurate AI diagnostics available for low-resource languages.</li>
<li><strong>摘要：</strong>使用自然语言处理 (NLP) 技术将自由文本放射学报告自动转换为结构化数据对于大规模分析疾病至关重要。虽然生成式大型语言模型 (LLM) 对于英语等广泛使用的语言的任务很有效，但对于不太常用的语言，它们通常表现不佳，并且可能对患者隐私构成潜在风险。现实世界医疗数据集的偏差性质阻碍了对本地 NLP 模型进行微调，其中罕见的发现代表了严重的数据不平衡。我们引入了 SMP-BERT，这是一种新颖的即时学习方法，它利用报告的结构化性质来克服这些挑战。在我们涉及大量希伯来语克罗恩病放射学报告的研究中（超过 8,000 名患者和 10,000 份报告），SMP-BERT 在性能上大大超越了传统的微调方法，尤其是在检测不常见情况方面（AUC：0.99 vs 0.94，F1：0.84 vs 0.34）。SMP-BERT 为低资源语言提供了更准确的 AI 诊断。</li>
</ul>

<h3>Title: Automatically Extracting Numerical Results from Randomized Controlled  Trials with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hye Sun Yun, David Pogrebitskiy, Iain J. Marshall, Byron C. Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01686">https://arxiv.org/abs/2405.01686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01686">https://arxiv.org/pdf/2405.01686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01686]] Automatically Extracting Numerical Results from Randomized Controlled  Trials with Large Language Models(https://arxiv.org/abs/2405.01686)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Meta-analyses statistically aggregate the findings of different randomized controlled trials (RCTs) to assess treatment effectiveness. Because this yields robust estimates of treatment effectiveness, results from meta-analyses are considered the strongest form of evidence. However, rigorous evidence syntheses are time-consuming and labor-intensive, requiring manual extraction of data from individual trials to be synthesized. Ideally, language technologies would permit fully automatic meta-analysis, on demand. This requires accurately extracting numerical results from individual trials, which has been beyond the capabilities of natural language processing (NLP) models to date. In this work, we evaluate whether modern large language models (LLMs) can reliably perform this task. We annotate (and release) a modest but granular evaluation dataset of clinical trial reports with numerical findings attached to interventions, comparators, and outcomes. Using this dataset, we evaluate the performance of seven LLMs applied zero-shot for the task of conditionally extracting numerical findings from trial reports. We find that massive LLMs that can accommodate lengthy inputs are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality). However, LLMs -- including ones trained on biomedical texts -- perform poorly when the outcome measures are complex and tallying the results requires inference. This work charts a path toward fully automatic meta-analysis of RCTs via LLMs, while also highlighting the limitations of existing models for this aim.</li>
<li><strong>摘要：</strong>荟萃分析统计汇总不同随机对照试验 (RCT) 的结果，以评估治疗效果。因为这可以产生对治疗效果的可靠估计，所以荟萃分析的结果被认为是最有力的证据形式。然而，严格的证据合成既耗时又费力，需要从各个试验中手动提取数据进行合成。理想情况下，语言技术将允许按需进行全自动元分析。这需要从各个试验中准确提取数值结果，这已经超出了迄今为止自然语言处理（NLP）模型的能力。在这项工作中，我们评估现代大型语言模型（LLM）是否能够可靠地执行此任务。我们注释（并发布）临床试验报告的适度但精细的评估数据集，并附有干预措施、比较器和结果的数值结果。使用该数据集，我们评估了七个应用零样本法学硕士的性能，以执行从试验报告中有条件提取数值结果的任务。我们发现，可以容纳冗长输入的大量法学硕士非常接近实现全自动荟萃分析，特别是对于二分（二元）结果（例如死亡率）。然而，当结果测量复杂并且计算结果需要推理时，法学硕士（包括接受过生物医学文本培训的法学硕士）表现不佳。这项工作绘制了一条通过法学硕士对随机对照试验进行全自动荟萃分析的道路，同时也强调了现有模型在实现这一目标方面的局限性。</li>
</ul>

<h3>Title: Large Language Models are Inconsistent and Biased Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Rickard Stureborg, Dimitris Alikaniotis, Yoshi Suhara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01724">https://arxiv.org/abs/2405.01724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01724">https://arxiv.org/pdf/2405.01724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01724]] Large Language Models are Inconsistent and Biased Evaluators(https://arxiv.org/abs/2405.01724)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low "inter-sample" agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的零样本能力为各种任务提供了高度灵活、无参考的度量，使 LLM 评估器成为 NLP 中的常用工具。然而，这些 LLM 评估者的稳健性仍然相对未被充分研究；现有的工作主要追求将法学硕士分数与人类专家分数相关联的最佳性能。在本文中，我们使用 SummEval 数据集进行了一系列分析，并确认法学硕士是有偏见的评估者，因为他们：（1）表现出熟悉性偏见 - 对较低困惑度的文本的偏好，（2）显示评分的倾斜和有偏见的分布， (3)多属性判断的经验锚定效应。我们还发现法学硕士是不一致的评估者，表现出较低的“样本间”一致性和对提示差异的敏感性，而这些差异对于人类对文本质量的理解无关紧要。此外，我们还分享了配置 LLM 评估器的秘诀，以减轻这些限制。 RoSE 数据集的实验结果表明，与最先进的 LLM 评估器相比有所改进。</li>
</ul>

<h3>Title: Question Suggestion for Conversational Shopping Assistants Using Product  Metadata</h3>
<ul>
<li><strong>Authors: </strong>Nikhita Vedula, Oleg Rokhlenko, Shervin Malmasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01738">https://arxiv.org/abs/2405.01738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01738">https://arxiv.org/pdf/2405.01738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01738]] Question Suggestion for Conversational Shopping Assistants Using Product  Metadata(https://arxiv.org/abs/2405.01738)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Digital assistants have become ubiquitous in e-commerce applications, following the recent advancements in Information Retrieval (IR), Natural Language Processing (NLP) and Generative Artificial Intelligence (AI). However, customers are often unsure or unaware of how to effectively converse with these assistants to meet their shopping needs. In this work, we emphasize the importance of providing customers a fast, easy to use, and natural way to interact with conversational shopping assistants. We propose a framework that employs Large Language Models (LLMs) to automatically generate contextual, useful, answerable, fluent and diverse questions about products, via in-context learning and supervised fine-tuning. Recommending these questions to customers as helpful suggestions or hints to both start and continue a conversation can result in a smoother and faster shopping experience with reduced conversation overhead and friction. We perform extensive offline evaluations, and discuss in detail about potential customer impact, and the type, length and latency of our generated product questions if incorporated into a real-world shopping assistant.</li>
<li><strong>摘要：</strong>随着信息检索 (IR)、自然语言处理 (NLP) 和生成人工智能 (AI) 的最新进展，数字助理在电子商务应用中已变得无处不在。然而，顾客常常不确定或不知道如何与这些助理有效地交谈来满足他们的购物需求。在这项工作中，我们强调为客户提供一种快速、易于使用且自然的方式与对话式购物助理交互的重要性。我们提出了一个框架，该框架采用大型语言模型（LLM），通过上下文学习和监督微调，自动生成有关产品的上下文、有用、可回答、流畅和多样化的问题。向客户推荐这些问题作为开始和继续对话的有用建议或提示，可以带来更顺畅、更快速的购物体验，同时减少对话开销和摩擦。我们进行广泛的离线评估，并详细讨论潜在的客户影响，以及我们生成的产品问题（如果合并到现实世界的购物助手中）的类型、长度和延迟。</li>
</ul>

<h3>Title: The Psychosocial Impacts of Generative AI Harms</h3>
<ul>
<li><strong>Authors: </strong>Faye-Marie Vassel, Evan Shieh, Cassidy R. Sugimoto, Thema Monroe-White</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01740">https://arxiv.org/abs/2405.01740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01740">https://arxiv.org/pdf/2405.01740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01740]] The Psychosocial Impacts of Generative AI Harms(https://arxiv.org/abs/2405.01740)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The rapid emergence of generative Language Models (LMs) has led to growing concern about the impacts that their unexamined adoption may have on the social well-being of diverse user groups. Meanwhile, LMs are increasingly being adopted in K-20 schools and one-on-one student settings with minimal investigation of potential harms associated with their deployment. Motivated in part by real-world/everyday use cases (e.g., an AI writing assistant) this paper explores the potential psychosocial harms of stories generated by five leading LMs in response to open-ended prompting. We extend findings of stereotyping harms analyzing a total of 150K 100-word stories related to student classroom interactions. Examining patterns in LM-generated character demographics and representational harms (i.e., erasure, subordination, and stereotyping) we highlight particularly egregious vignettes, illustrating the ways LM-generated outputs may influence the experiences of users with marginalized and minoritized identities, and emphasizing the need for a critical understanding of the psychosocial impacts of generative AI tools when deployed and utilized in diverse social contexts.</li>
<li><strong>摘要：</strong>生成语言模型（LM）的迅速出现导致人们越来越担心其未经审查的采用可能对不同用户群体的社会福祉产生的影响。与此同时，LM 越来越多地在 K-20 学校和一对一的学生环境中得到采用，并且对其部署相关的潜在危害进行了最少的调查。本文部分受到现实世界/日常用例（例如人工智能写作助手）的启发，探讨了五位领先的 LM 在响应开放式提示时生成的故事的潜在心理社会危害。我们通过分析与学生课堂互动相关的总共 15 万个 100 字的故事，扩展了刻板印象危害的发现。检查 LM 生成的角色人口统计数据和代表性危害（即擦除、从属和刻板印象）的模式，我们强调了特别令人震惊的小插曲，说明了 LM 生成的输出可能影响边缘化和少数群体身份的用户体验的方式，并强调了这一需求批判性地理解生成式人工智能工具在不同社会环境中部署和使用时的社会心理影响。</li>
</ul>

<h3>Title: CoS: Enhancing Personalization and Mitigating Bias with Context Steering</h3>
<ul>
<li><strong>Authors: </strong>Jerry Zhi-Yang He, Sashrika Pandey, Mariah L. Schrum, Anca Dragan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01768">https://arxiv.org/abs/2405.01768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01768">https://arxiv.org/pdf/2405.01768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01768]] CoS: Enhancing Personalization and Mitigating Bias with Context Steering(https://arxiv.org/abs/2405.01768)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>When querying a large language model (LLM), the context, i.e. personal, demographic, and cultural information specific to an end-user, can significantly shape the response of the LLM. For example, asking the model to explain Newton's second law with the context "I am a toddler" yields a different answer compared to the context "I am a physics professor." Proper usage of the context enables the LLM to generate personalized responses, whereas inappropriate contextual influence can lead to stereotypical and potentially harmful generations (e.g. associating "female" with "housekeeper"). In practice, striking the right balance when leveraging context is a nuanced and challenging problem that is often situation-dependent. One common approach to address this challenge is to fine-tune LLMs on contextually appropriate responses. However, this approach is expensive, time-consuming, and not controllable for end-users in different situations. In this work, we propose Context Steering (CoS) - a simple training-free method that can be easily applied to autoregressive LLMs at inference time. By measuring the contextual influence in terms of token prediction likelihood and modulating it, our method enables practitioners to determine the appropriate level of contextual influence based on their specific use case and end-user base. We showcase a variety of applications of CoS including amplifying the contextual influence to achieve better personalization and mitigating unwanted influence for reducing model bias. In addition, we show that we can combine CoS with Bayesian Inference to quantify the extent of hate speech on the internet. We demonstrate the effectiveness of CoS on state-of-the-art LLMs and benchmarks.</li>
<li><strong>摘要：</strong>查询大型语言模型 (LLM) 时，上下文（即特定于最终用户的个人、人口统计和文化信息）可以显着影响 LLM 的响应。例如，要求模型在“我是一个幼儿”的上下文中解释牛顿第二定律，与“我是一名物理学教授”的上下文相比，会产生不同的答案。正确使用上下文可以使法学硕士产生个性化的反应，而不适当的上下文影响可能会导致刻板印象和潜在有害的一代（例如将“女性”与“管家”联系起来）。在实践中，在利用上下文时取得适当的平衡是一个微妙且具有挑战性的问题，而且通常取决于具体情况。应对这一挑战的一种常见方法是根据具体情况调整法学硕士的响应。然而，这种方法成本高昂、耗时，并且对于不同情况下的最终用户来说不可控。在这项工作中，我们提出了上下文引导（CoS）——一种简单的免训练方法，可以在推理时轻松应用于自回归 LLM。通过根据令牌预测可能性来测量上下文影响并对其进行调整，我们的方法使从业者能够根据其特定用例和最终用户群确定适当的上下文影响水平。我们展示了 CoS 的各种应用，包括放大上下文影响以实现更好的个性化，以及减轻不必要的影响以减少模型偏差。此外，我们还表明，我们可以将 CoS 与贝叶斯推理相结合来量化互联网上仇恨言论的程度。我们在最先进的法学硕士和基准上展示了 CoS 的有效性。</li>
</ul>

<h3>Title: A Survey on Large Language Models for Critical Societal Domains:  Finance, Healthcare, and Law</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, Julian McAuley, Linda Petzold, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01769">https://arxiv.org/abs/2405.01769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01769">https://arxiv.org/pdf/2405.01769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01769]] A Survey on Large Language Models for Critical Societal Domains:  Finance, Healthcare, and Law(https://arxiv.org/abs/2405.01769)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In the fast-evolving domain of artificial intelligence, large language models (LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance, healthcare, and law: domains characterized by their reliance on professional expertise, challenging data acquisition, high-stakes, and stringent regulatory compliance. This survey offers a detailed exploration of the methodologies, applications, challenges, and forward-looking opportunities of LLMs within these high-stakes sectors. We highlight the instrumental role of LLMs in enhancing diagnostic and treatment methodologies in healthcare, innovating financial analytics, and refining legal interpretation and compliance strategies. Moreover, we critically examine the ethics for LLM applications in these fields, pointing out the existing ethical concerns and the need for transparent, fair, and robust AI systems that respect regulatory norms. By presenting a thorough review of current literature and practical applications, we showcase the transformative impact of LLMs, and outline the imperative for interdisciplinary cooperation, methodological advancements, and ethical vigilance. Through this lens, we aim to spark dialogue and inspire future research dedicated to maximizing the benefits of LLMs while mitigating their risks in these precision-dependent sectors. To facilitate future research on LLMs in these critical societal domains, we also initiate a reading list that tracks the latest advancements under this topic, which will be continually updated: \url{https://github.com/czyssrs/LLM_X_papers}.</li>
<li><strong>摘要：</strong>在快速发展的人工智能领域，GPT-3 和 GPT-4 等大型语言模型 (LLM) 正在彻底改变金融、医疗保健和法律领域的格局：这些领域的特点是依赖专业知识、数据采集具有挑战性、风险高且监管合规性严格。本调查详细探讨了 LLM 在这些高风险领域的方法、应用、挑战和前瞻性机遇。我们强调了 LLM 在增强医疗保健诊断和治疗方法、创新财务分析以及完善法律解释和合规策略方面的重要作用。此外，我们批判性地审查了这些领域 LLM 应用的伦理问题，指出了现有的伦理问题以及对遵守监管规范的透明、公平和强大的人工智能系统的需求。通过对当前文献和实际应用进行全面回顾，我们展示了 LLM 的变革性影响，并概述了跨学科合作、方法论进步和道德警惕的必要性。通过这种方式，我们旨在激发对话并激励未来的研究，致力于最大限度地发挥法学硕士的优势，同时降低其在这些依赖精度的行业中的风险。为了促进未来在这些关键社会领域对法学硕士的研究，我们还启动了一个阅读清单，跟踪该主题下的最新进展，并将不断更新：\url{https://github.com/czyssrs/LLM_X_papers}。</li>
</ul>

<h3>Title: TOPICAL: TOPIC Pages AutomagicaLly</h3>
<ul>
<li><strong>Authors: </strong>John Giorgi, Amanpreet Singh, Doug Downey, Sergey Feldman, Lucy Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01796">https://arxiv.org/abs/2405.01796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01796">https://arxiv.org/pdf/2405.01796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01796]] TOPICAL: TOPIC Pages AutomagicaLly(https://arxiv.org/abs/2405.01796)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Topic pages aggregate useful information about an entity or concept into a single succinct and accessible article. Automated creation of topic pages would enable their rapid curation as information resources, providing an alternative to traditional web search. While most prior work has focused on generating topic pages about biographical entities, in this work, we develop a completely automated process to generate high-quality topic pages for scientific entities, with a focus on biomedical concepts. We release TOPICAL, a web app and associated open-source code, comprising a model pipeline combining retrieval, clustering, and prompting, that makes it easy for anyone to generate topic pages for a wide variety of biomedical entities on demand. In a human evaluation of 150 diverse topic pages generated using TOPICAL, we find that the vast majority were considered relevant, accurate, and coherent, with correct supporting citations. We make all code publicly available and host a free-to-use web app at: https://s2-topical.apps.allenai.org</li>
<li><strong>摘要：</strong>主题页面将有关实体或概念的有用信息聚合到一篇简洁且易于理解的文章中。自动创建主题页面将使它们能够作为信息资源进行快速管理，从而提供传统网络搜索的替代方案。虽然大多数先前的工作都集中在生成有关传记实体的主题页面，但在这项工作中，我们开发了一个完全自动化的流程来为科学实体生成高质量的主题页面，重点关注生物医学概念。我们发布了 TOPICAL，一个 Web 应用程序和相关的开源代码，包括一个结合了检索、聚类和提示的模型管道，使任何人都可以轻松按需为各种生物医学实体生成主题页面。在对使用 TOPICAL 生成的 150 个不同主题页面进行人工评估时，我们发现绝大多数被认为相关、准确且连贯，并具有正确的支持引用。我们公开所有代码并托管免费使用的网络应用程序：https://s2-topical.apps.allenai.org</li>
</ul>

<h3>Title: Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders  and Identifying Distinct Features</h3>
<ul>
<li><strong>Authors: </strong>Chuanbo Hu, Wenqi Li, Mindi Ruan, Xiangxu Yu, Lynn K. Paul, Shuo Wang, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01799">https://arxiv.org/abs/2405.01799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01799">https://arxiv.org/pdf/2405.01799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01799]] Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders  and Identifying Distinct Features(https://arxiv.org/abs/2405.01799)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Diagnosing language disorders associated with autism is a complex and nuanced challenge, often hindered by the subjective nature and variability of traditional assessment methods. Traditional diagnostic methods not only require intensive human effort but also often result in delayed interventions due to their lack of speed and specificity. In this study, we explored the application of ChatGPT, a state of the art large language model, to overcome these obstacles by enhancing diagnostic accuracy and profiling specific linguistic features indicative of autism. Leveraging ChatGPT advanced natural language processing capabilities, this research aims to streamline and refine the diagnostic process. Specifically, we compared ChatGPT's performance with that of conventional supervised learning models, including BERT, a model acclaimed for its effectiveness in various natural language processing tasks. We showed that ChatGPT substantially outperformed these models, achieving over 13% improvement in both accuracy and F1 score in a zero shot learning configuration. This marked enhancement highlights the model potential as a superior tool for neurological diagnostics. Additionally, we identified ten distinct features of autism associated language disorders that vary significantly across different experimental scenarios. These features, which included echolalia, pronoun reversal, and atypical language usage, were crucial for accurately diagnosing ASD and customizing treatment plans. Together, our findings advocate for adopting sophisticated AI tools like ChatGPT in clinical settings to assess and diagnose developmental disorders. Our approach not only promises greater diagnostic precision but also aligns with the goals of personalized medicine, potentially transforming the evaluation landscape for autism and similar neurological conditions.</li>
<li><strong>摘要：</strong>诊断与自闭症相关的语言障碍是一项复杂而微妙的挑战，传统评估方法的主观性和多变性常常阻碍这一进程。传统的诊断方法不仅需要大量的人力，而且由于速度和特异性不足，往往会导致干预延迟。在本研究中，我们探索了最先进的大型语言模型 ChatGPT 的应用，通过提高诊断准确性和分析表明自闭症的特定语言特征来克服这些障碍。利用 ChatGPT 先进的自然语言处理功能，本研究旨在简化和改进诊断过程。具体来说，我们将 ChatGPT 的性能与传统监督学习模型（包括 BERT）的性能进行了比较，BERT 因其在各种自然语言处理任务中的有效性而广受赞誉。我们表明，ChatGPT 的表现远远优于这些模型，在零样本学习配置中，准确率和 F1 分数都提高了 13% 以上。这一显著的增强凸显了该模型作为神经诊断的卓越工具的潜力。此外，我们还发现了自闭症相关语言障碍的十个不同特征，这些特征在不同的实验场景中存在显著差异。这些特征包括模仿语言、代词反转和非典型语言使用，对于准确诊断 ASD 和定制治疗计划至关重要。总之，我们的研究结果主张在临床环境中采用 ChatGPT 等复杂的 AI 工具来评估和诊断发育障碍。我们的方法不仅可以提高诊断精度，而且符合个性化医疗的目标，有可能改变自闭症和类似神经系统疾病的评估格局。</li>
</ul>

<h3>Title: SoftMCL: Soft Momentum Contrastive Learning for Fine-grained  Sentiment-aware Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Jin Wang, Liang-Chih Yu, Xuejie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01827">https://arxiv.org/abs/2405.01827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01827">https://arxiv.org/pdf/2405.01827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01827]] SoftMCL: Soft Momentum Contrastive Learning for Fine-grained  Sentiment-aware Pre-training(https://arxiv.org/abs/2405.01827)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The pre-training for language models captures general language understanding but fails to distinguish the affective impact of a particular context to a specific word. Recent works have sought to introduce contrastive learning (CL) for sentiment-aware pre-training in acquiring affective information. Nevertheless, these methods present two significant limitations. First, the compatibility of the GPU memory often limits the number of negative samples, hindering the opportunities to learn good representations. In addition, using only a few sentiment polarities as hard labels, e.g., positive, neutral, and negative, to supervise CL will force all representations to converge to a few points, leading to the issue of latent space collapse. This study proposes a soft momentum contrastive learning (SoftMCL) for fine-grained sentiment-aware pre-training. Instead of hard labels, we introduce valence ratings as soft-label supervision for CL to fine-grained measure the sentiment similarities between samples. The proposed SoftMCL is conducted on both the word- and sentence-level to enhance the model's ability to learn affective information. A momentum queue was introduced to expand the contrastive samples, allowing storing and involving more negatives to overcome the limitations of hardware platforms. Extensive experiments were conducted on four different sentiment-related tasks, which demonstrates the effectiveness of the proposed SoftMCL method. The code and data of the proposed SoftMCL is available at: https://www.github.com/wangjin0818/SoftMCL/.</li>
<li><strong>摘要：</strong>语言模型的预训练捕获了一般语言理解，但无法区分特定上下文对特定单词的情感影响。最近的工作试图引入对比学习（CL）来进行情感感知预训练，以获取情感信息。然而，这些方法存在两个显着的局限性。首先，GPU 内存的兼容性通常会限制负样本的数量，从而阻碍了学习良好表示的机会。此外，仅使用少数情绪极性作为硬标签，例如积极、中性和消极来监督CL，将迫使所有表征收敛到几个点，导致潜在空间崩溃的问题。本研究提出了一种用于细粒度情感感知预训练的软动量对比学习（SoftMCL）。我们引入效价评级作为 CL 的软标签监督，而不是硬标签，以细粒度地测量样本之间的情感相似性。所提出的 SoftMCL 在单词和句子级别上进行，以增强模型学习情感信息的能力。引入动量队列来扩展对比样本，允许存储和涉及更多底片以克服硬件平台的限制。对四种不同的情感相关任务进行了广泛的实验，证明了所提出的 SoftMCL 方法的有效性。所提出的 SoftMCL 的代码和数据可在以下网址获取：https://www.github.com/wangjin0818/SoftMCL/。</li>
</ul>

<h3>Title: SGHateCheck: Functional Tests for Detecting Hate Speech in Low-Resource  Languages of Singapore</h3>
<ul>
<li><strong>Authors: </strong>Ri Chi Ng, Nirmalendu Prakash, Ming Shan Hee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01842">https://arxiv.org/abs/2405.01842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01842">https://arxiv.org/pdf/2405.01842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01842]] SGHateCheck: Functional Tests for Detecting Hate Speech in Low-Resource  Languages of Singapore(https://arxiv.org/abs/2405.01842)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>To address the limitations of current hate speech detection models, we introduce \textsf{SGHateCheck}, a novel framework designed for the linguistic and cultural context of Singapore and Southeast Asia. It extends the functional testing approach of HateCheck and MHC, employing large language models for translation and paraphrasing into Singapore's main languages, and refining these with native annotators. \textsf{SGHateCheck} reveals critical flaws in state-of-the-art models, highlighting their inadequacy in sensitive content moderation. This work aims to foster the development of more effective hate speech detection tools for diverse linguistic environments, particularly for Singapore and Southeast Asia contexts.</li>
<li><strong>摘要：</strong>为了解决当前仇恨语音检测模型的局限性，我们引入了 \textsf{SGHateCheck}，这是一种专为新加坡和东南亚的语言和文化背景而设计的新颖框架。它扩展了 HateCheck 和 MHC 的功能测试方法，采用大型语言模型进行新加坡主要语言的翻译和释义，并使用本地注释器对其进行完善。 \textsf{SGHateCheck} 揭示了最先进模型中的关键缺陷，强调了它们在敏感内容审核方面的不足。这项工作旨在促进针对不同语言环境（特别是新加坡和东南亚环境）开发更有效的仇恨言论检测工具。</li>
</ul>

<h3>Title: SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for  Sexual Education in Rural India</h3>
<ul>
<li><strong>Authors: </strong>Salam Michael Singh, Shubhmoy Kumar Garg, Amitesh Misra, Aaditeshwar Seth, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01858">https://arxiv.org/abs/2405.01858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01858">https://arxiv.org/pdf/2405.01858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01858]] SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for  Sexual Education in Rural India(https://arxiv.org/abs/2405.01858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sexual education aims to foster a healthy lifestyle in terms of emotional, mental and social well-being. In countries like India, where adolescents form the largest demographic group, they face significant vulnerabilities concerning sexual health. Unfortunately, sexual education is often stigmatized, creating barriers to providing essential counseling and information to this at-risk population. Consequently, issues such as early pregnancy, unsafe abortions, sexually transmitted infections, and sexual violence become prevalent. Our current proposal aims to provide a safe and trustworthy platform for sexual education to the vulnerable rural Indian population, thereby fostering the healthy and overall growth of the nation. In this regard, we strive towards designing SUKHSANDESH, a multi-staged AI-based Question Answering platform for sexual education tailored to rural India, adhering to safety guardrails and regional language support. By utilizing information retrieval techniques and large language models, SUKHSANDESH will deliver effective responses to user queries. We also propose to anonymise the dataset to mitigate safety measures and set AI guardrails against any harmful or unwanted response generation. Moreover, an innovative feature of our proposal involves integrating ``avatar therapy'' with SUKHSANDESH. This feature will convert AI-generated responses into real-time audio delivered by an animated avatar speaking regional Indian languages. This approach aims to foster empathy and connection, which is particularly beneficial for individuals with limited literacy skills. Partnering with Gram Vaani, an industry leader, we will deploy SUKHSANDESH to address sexual education needs in rural India.</li>
<li><strong>摘要：</strong>性教育旨在培养情感、心理和社会福祉方面的健康生活方式。在印度这样的国家，青少年是最大的人口群体，他们在性健康方面面临着巨大的脆弱性。不幸的是，性教育经常受到污名化，为向这一高危人群提供必要的咨询和信息造成了障碍。因此，早孕、不安全堕胎、性传播感染和性暴力等问题普遍存在。我们当前的提案旨在为弱势印度农村人口提供一个安全、值得信赖的性教育平台，从而促进国家的健康和全面发展。在这方面，我们努力设计 SUKHSANDESH，这是一个针对印度农村地区量身定制的基于人工智能的多阶段性教育问答平台，坚持安全护栏和区域语言支持。通过利用信息检索技术和大型语言模型，SUKHSANDESH 将为用户查询提供有效的响应。我们还建议对数据集进行匿名化，以减轻安全措施，并设置人工智能护栏，防止产生任何有害或不需要的响应。此外，我们提案的一个创新特征是将“化身疗法”与 SUKHSANDESH 相结合。此功能将把人工智能生成的响应转换为由讲印度地方语言的动画头像提供的实时音频。这种方法旨在培养同理心和联系，这对于识字能力有限的个人特别有益。我们将与行业领导者 Gram Vaani 合作，部署 SUKHSANDESH 来满足印度农村地区的性教育需求。</li>
</ul>

<h3>Title: Incorporating External Knowledge and Goal Guidance for LLM-based  Conversational Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Chuang Li, Yang Deng, Hengchang Hu, Min-Yen Kan, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01868">https://arxiv.org/abs/2405.01868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01868">https://arxiv.org/pdf/2405.01868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01868]] Incorporating External Knowledge and Goal Guidance for LLM-based  Conversational Recommender Systems(https://arxiv.org/abs/2405.01868)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>This paper aims to efficiently enable large language models (LLMs) to use external knowledge and goal guidance in conversational recommender system (CRS) tasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks for 1) generating grounded responses with recommendation-oriented knowledge, or 2) proactively leading the conversations through different dialogue goals. In this work, we first analyze those limitations through a comprehensive evaluation, showing the necessity of external knowledge and goal guidance which contribute significantly to the recommendation accuracy and language quality. In light of this finding, we propose a novel ChatCRS framework to decompose the complex CRS task into several sub-tasks through the implementation of 1) a knowledge retrieval agent using a tool-augmented approach to reason over external Knowledge Bases and 2) a goal-planning agent for dialogue goal prediction. Experimental results on two multi-goal CRS datasets reveal that ChatCRS sets new state-of-the-art benchmarks, improving language quality of informativeness by 17% and proactivity by 27%, and achieving a tenfold enhancement in recommendation accuracy.</li>
<li><strong>摘要：</strong>本文旨在有效地使大型语言模型（LLM）能够在会话推荐系统（CRS）任务中使用外部知识和目标指导。高级法学硕士（例如 ChatGPT）在特定领域的 CRS 任务中受到限制：1）使用面向推荐的知识生成接地响应，或 2）通过不同的对话目标主动引导对话。在这项工作中，我们首先通过综合评估分析这些局限性，表明外部知识和目标指导的必要性，这对推荐准确性和语言质量有显着贡献。鉴于这一发现，我们提出了一种新颖的 ChatCRS 框架，通过实现 1）使用工具增强方法对外部知识库进行推理的知识检索代理和 2）目标，将复杂的 CRS 任务分解为多个子任务-对话目标预测的规划代理。在两个多目标 CRS 数据集上的实验结果表明，ChatCRS 设定了新的最先进基准，将信息性的语言质量提高了 17%，主动性提高了 27%，并且推荐准确性提高了十倍。</li>
</ul>

<h3>Title: DALLMi: Domain Adaption for LLM-based Multi-label Classifier</h3>
<ul>
<li><strong>Authors: </strong>Miruna Beţianu, Abele Mălan, Marco Aldinucci, Robert Birke, Lydia Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01883">https://arxiv.org/abs/2405.01883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01883">https://arxiv.org/pdf/2405.01883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01883]] DALLMi: Domain Adaption for LLM-based Multi-label Classifier(https://arxiv.org/abs/2405.01883)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly serve as the backbone for classifying text associated with distinct domains and simultaneously several labels (classes). When encountering domain shifts, e.g., classifier of movie reviews from IMDb to Rotten Tomatoes, adapting such an LLM-based multi-label classifier is challenging due to incomplete label sets at the target domain and daunting training overhead. The existing domain adaptation methods address either image multi-label classifiers or text binary classifiers. In this paper, we design DALLMi, Domain Adaptation Large Language Model interpolator, a first-of-its-kind semi-supervised domain adaptation method for text data models based on LLMs, specifically BERT. The core of DALLMi is the novel variation loss and MixUp regularization, which jointly leverage the limited positively labeled and large quantity of unlabeled text and, importantly, their interpolation from the BERT word embeddings. DALLMi also introduces a label-balanced sampling strategy to overcome the imbalance between labeled and unlabeled data. We evaluate DALLMi against the partial-supervised and unsupervised approach on three datasets under different scenarios of label availability for the target domain. Our results show that DALLMi achieves higher mAP than unsupervised and partially-supervised approaches by 19.9% and 52.2%, respectively.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地充当对与不同域和同时多个标签（类）相关的文本进行分类的支柱。当遇到领域转移时，例如，电影评论的分类器从 IMDb 到烂番茄，由于目标领域的标签集不完整且训练开销巨大，适应这种基于 LLM 的多标签分类器具有挑战性。现有的域适应方法针对图像多标签分类器或文本二元分类器。在本文中，我们设计了 DALLMi（领域适应大型语言模型插值器），这是一种基于 LLM（特别是 BERT）的文本数据模型的首个半监督领域适应方法。 DALLMi 的核心是新颖的变异损失和 MixUp 正则化，它们共同利用有限的正标记文本和大量未标记文本，更重要的是，它们来自 BERT 词嵌入的插值。 DALLMi 还引入了标签平衡采样策略来克服标记数据和未标记数据之间的不平衡。我们在目标域标签可用性的不同场景下，在三个数据集上针对部分监督和无监督方法评估 DALLMi。我们的结果表明，DALLMi 的 mAP 分别比无监督和部分监督方法高 19.9% 和 52.2%。</li>
</ul>

<h3>Title: Beyond Single-Event Extraction: Towards Efficient Document-Level  Multi-Event Argument Extraction</h3>
<ul>
<li><strong>Authors: </strong>Wanlong Liu, Li Zhou, Dingyi Zeng, Yichen Xiao, Shaohuan Cheng, Chen Zhang, Grandee Lee, Malu Zhang, Wenyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01884">https://arxiv.org/abs/2405.01884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01884">https://arxiv.org/pdf/2405.01884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01884]] Beyond Single-Event Extraction: Towards Efficient Document-Level  Multi-Event Argument Extraction(https://arxiv.org/abs/2405.01884)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Recent mainstream event argument extraction methods process each event in isolation, resulting in inefficient inference and ignoring the correlations among multiple events. To address these limitations, here we propose a multiple-event argument extraction model DEEIA (Dependency-guided Encoding and Event-specific Information Aggregation), capable of extracting arguments from all events within a document simultaneouslyThe proposed DEEIA model employs a multi-event prompt mechanism, comprising DE and EIA modules. The DE module is designed to improve the correlation between prompts and their corresponding event contexts, whereas the EIA module provides event-specific information to improve contextual understanding. Extensive experiments show that our method achieves new state-of-the-art performance on four public datasets (RAMS, WikiEvents, MLEE, and ACE05), while significantly saving the inference time compared to the baselines. Further analyses demonstrate the effectiveness of the proposed modules.</li>
<li><strong>摘要：</strong>目前主流的事件论据提取方法孤立地处理每个事件，导致推理效率低下，并且忽略了多个事件之间的相关性。为了解决这些限制，我们在这里提出了一种多事件参数提取模型 DEEIA（依赖引导编码和事件特定信息聚合），能够同时从文档中的所有事件中提取参数。所提出的 DEEIA 模型采用了多事件提示机制，包括DE和EIA模块。 DE 模块旨在提高提示与其相应事件上下文之间的相关性，而 EIA 模块则提供特定于事件的信息以提高上下文理解。大量实验表明，我们的方法在四个公共数据集（RAMS、WikiEvents、MLEE 和 ACE05）上实现了最先进的性能，同时与基线相比显着节省了推理时间。进一步的分析证明了所提出模块的有效性。</li>
</ul>

<h3>Title: Aloe: A Family of Fine-tuned Open Healthcare LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés Dario Garcia-Gasulla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01886">https://arxiv.org/abs/2405.01886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01886">https://arxiv.org/pdf/2405.01886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01886]] Aloe: A Family of Fine-tuned Open Healthcare LLMs(https://arxiv.org/abs/2405.01886)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As the capabilities of Large Language Models (LLMs) in healthcare and medicine continue to advance, there is a growing need for competitive open-source models that can safeguard public interest. With the increasing availability of highly competitive open base models, the impact of continued pre-training is increasingly uncertain. In this work, we explore the role of instruct tuning, model merging, alignment, red teaming and advanced inference schemes, as means to improve current open models. To that end, we introduce the Aloe family, a set of open medical LLMs highly competitive within its scale range. Aloe models are trained on the current best base models (Mistral, LLaMA 3), using a new custom dataset which combines public data sources improved with synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase, becoming one of the first few policy-aligned open healthcare LLM using Direct Preference Optimization, setting a new standard for ethical performance in healthcare LLMs. Model evaluation expands to include various bias and toxicity datasets, a dedicated red teaming effort, and a much-needed risk assessment for healthcare LLMs. Finally, to explore the limits of current LLMs in inference, we study several advanced prompt engineering strategies to boost performance across benchmarks, yielding state-of-the-art results for open healthcare 7B LLMs, unprecedented at this scale.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）在医疗保健和医学领域的能力不断进步，对能够维护公共利益的有竞争力的开源模型的需求日益增长。随着竞争激烈的开放基础模型的可用性不断增加，持续预训练的影响越来越不确定。在这项工作中，我们探索了指令调整、模型合并、对齐、红队和高级推理方案的作用，作为改进当前开放模型的手段。为此，我们引入了 Aloe 系列，这是一组在其规模范围内极具竞争力的开放式医学法学硕士。 Aloe 模型在当前最佳基础模型（Mistral、LLaMA 3）上进行训练，使用新的自定义数据集，该数据集结合了通过合成思想链 (CoT) 改进的公共数据源。 Aloe 模型经历了调整阶段，成为首批使用直接偏好优化的政策一致的开放式医疗保健法学硕士之一，为医疗保健法学硕士的道德表现设立了新标准。模型评估扩展到包括各种偏差和毒性数据集、专门的红队工作以及医疗保健法学硕士急需的风险评估。最后，为了探索当前法学硕士在推理方面的局限性，我们研究了几种先进的即时工程策略，以提高跨基准的性能，为开放医疗保健 7B 法学硕士产生最先进的结果，这在这种规模上是前所未有的。</li>
</ul>

<h3>Title: CRCL at SemEval-2024 Task 2: Simple prompt optimizations</h3>
<ul>
<li><strong>Authors: </strong>Clément Brutti-Mairesse, Loïc Verlingue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01942">https://arxiv.org/abs/2405.01942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01942">https://arxiv.org/pdf/2405.01942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01942]] CRCL at SemEval-2024 Task 2: Simple prompt optimizations(https://arxiv.org/abs/2405.01942)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We present a baseline for the SemEval 2024 task 2 challenge, whose objective is to ascertain the inference relationship between pairs of clinical trial report sections and statements. We apply prompt optimization techniques with LLM Instruct models provided as a Language Model-as-a-Service (LMaaS). We observed, in line with recent findings, that synthetic CoT prompts significantly enhance manually crafted ones.</li>
<li><strong>摘要：</strong>我们提出了 SemEval 2024 任务 2 挑战的基线，其目标是确定临床试验报告部分和陈述对之间的推理关系。我们将即时优化技术与作为语言模型即服务 (LMaaS) 提供的 LLM Instruct 模型一起应用。我们观察到，根据最近的发现，合成 CoT 提示显着增强了手工制作的提示。</li>
</ul>

<h3>Title: Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Guo, Hidetaka Kamigaito, Taro Wanatnabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01943">https://arxiv.org/abs/2405.01943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01943">https://arxiv.org/pdf/2405.01943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01943]] Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large  Language Models(https://arxiv.org/abs/2405.01943)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent SwiGLU-based LLMs pruning. Our approach incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on Mistral and LLaMA2 model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展显著增强了语言理解和生成的能力。然而，庞大的模型规模带来了硬件挑战，影响了服务的内存大小和 token 生成的推理延迟。为了应对这些挑战，我们提出了依赖感知半结构化稀疏 (DaSS)，这是最近流行的基于 SwiGLU 的 LLM 修剪的一种新方法。我们的方法将结构依赖性纳入基于权重大小的非结构化修剪中。我们引入了一个 MLP 特定的修剪指标，该指标通过共同考虑每个权重的大小及其相应的 MLP 中间激活规范来评估每个权重的重要性。DaSS 有助于在非结构化修剪提供的适应性与基于依赖的结构化修剪固有的结构一致性之间取得平衡。对 Mistral 和 LLaMA2 模型系列的实证评估表明，DaSS 不仅在实现硬件友好的 N:M 稀疏模式方面优于 SparseGPT 和 Wanda，而且还保持了 Wanda 的计算效率。</li>
</ul>

<h3>Title: Conformal Prediction for Natural Language Processing: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Margarida M. Campos, António Farinhas, Chrysoula Zerva, Mário A.T. Figueiredo, André F.T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01976">https://arxiv.org/abs/2405.01976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01976">https://arxiv.org/pdf/2405.01976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01976]] Conformal Prediction for Natural Language Processing: A Survey(https://arxiv.org/abs/2405.01976)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.</li>
<li><strong>摘要：</strong>大型语言模型和自然语言处理 (NLP) 应用的快速普及带来了对不确定性量化的迫切需求，以减轻幻觉等风险并提高关键应用中的决策可靠性。共形预测正逐渐成为一个理论合理且实用的框架，将灵活性与强大的统计保证相结合。其与模型无关和不受分布影响的特性使其特别有望解决 NLP 系统当前因缺乏不确定性量化而产生的缺陷。本文全面概述了共形预测技术、其保证以及 NLP 中的现有应用，并指出了未来研究的方向和未解决的挑战。</li>
</ul>

<h3>Title: Exploring Combinatorial Problem Solving with Large Language Models: A  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Masoud, Ahmed Abdelhay, Mohammed Elhenawy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01997">https://arxiv.org/abs/2405.01997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01997">https://arxiv.org/pdf/2405.01997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01997]] Exploring Combinatorial Problem Solving with Large Language Models: A  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo(https://arxiv.org/abs/2405.01997)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are deep learning models designed to generate text based on textual input. Although researchers have been developing these models for more complex tasks such as code generation and general reasoning, few efforts have explored how LLMs can be applied to combinatorial problems. In this research, we investigate the potential of LLMs to solve the Travelling Salesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments employing various approaches, including zero-shot in-context learning, few-shot in-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned GPT-3.5 Turbo to solve a specific problem size and tested it using a set of various instance sizes. The fine-tuned models demonstrated promising performance on problems identical in size to the training instances and generalized well to larger problems. Furthermore, to improve the performance of the fine-tuned model without incurring additional training costs, we adopted a self-ensemble approach to improve the quality of the solutions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是深度学习模型，旨在根据文本输入生成文本。尽管研究人员一直在为更复杂的任务（例如代码生成和一般推理）开发这些模型，但很少有人探索如何将法学硕士应用于组合问题。在这项研究中，我们调查了法学硕士解决旅行商问题（TSP）的潜力。利用 GPT-3.5 Turbo，我们采用各种方法进行了实验，包括零样本上下文学习、少样本上下文学习和思想链 (CoT)。因此，我们对 GPT-3.5 Turbo 进行了微调以解决特定的问题大小，并使用一组不同的实例大小对其进行测试。经过微调的模型在与训练实例大小相同的问题上表现出了良好的性能，并且可以很好地推广到更大的问题。此外，为了提高微调模型的性能而不产生额外的训练成本，我们采用了自集成方法来提高解决方案的质量。</li>
</ul>

<h3>Title: Analyzing Narrative Processing in Large Language Models (LLMs): Using  GPT4 to test BERT</h3>
<ul>
<li><strong>Authors: </strong>Patrick Krauss, Jannik Hösch, Claus Metzner, Andreas Maier, Peter Uhrig, Achim Schilling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02024">https://arxiv.org/abs/2405.02024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02024">https://arxiv.org/pdf/2405.02024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02024]] Analyzing Narrative Processing in Large Language Models (LLMs): Using  GPT4 to test BERT(https://arxiv.org/abs/2405.02024)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The ability to transmit and receive complex information via language is unique to humans and is the basis of traditions, culture and versatile social interactions. Through the disruptive introduction of transformer based large language models (LLMs) humans are not the only entity to "understand" and produce language any more. In the present study, we have performed the first steps to use LLMs as a model to understand fundamental mechanisms of language processing in neural networks, in order to make predictions and generate hypotheses on how the human brain does language processing. Thus, we have used ChatGPT to generate seven different stylistic variations of ten different narratives (Aesop's fables). We used these stories as input for the open source LLM BERT and have analyzed the activation patterns of the hidden units of BERT using multi-dimensional scaling and cluster analysis. We found that the activation vectors of the hidden units cluster according to stylistic variations in earlier layers of BERT (1) than narrative content (4-5). Despite the fact that BERT consists of 12 identical building blocks that are stacked and trained on large text corpora, the different layers perform different tasks. This is a very useful model of the human brain, where self-similar structures, i.e. different areas of the cerebral cortex, can have different functions and are therefore well suited to processing language in a very efficient way. The proposed approach has the potential to open the black box of LLMs on the one hand, and might be a further step to unravel the neural processes underlying human language processing and cognition in general.</li>
<li><strong>摘要：</strong>通过语言传输和接收复杂信息的能力是人类独有的，是传统、文化和多样化社会互动的基础。通过颠覆性地引入基于 Transformer 的大型语言模型 (LLM)，人类不再是唯一“理解”和产生语言的实体。在本研究中，我们已经完成了使用法学硕士作为模型来理解神经网络中语言处理的基本机制的第一步，以便对人脑如何进行语言处理进行预测和生成假设。因此，我们使用 ChatGPT 生成了十种不同叙述（伊索寓言）的七种不同风格的变体。我们使用这些故事作为开源 LLM BERT 的输入，并使用多维尺度和聚类分析分析了 BERT 隐藏单元的激活模式。我们发现，隐藏单元的激活向量根据 BERT (1) 较早层的风格变化而不是叙事内容 (4-5) 进行聚类。尽管 BERT 由 12 个相同的构建块组成，这些构建块在大型文本语料库上堆叠和训练，但不同的层执行不同的任务。这是一个非常有用的人脑模型，其中自相似的结构，即大脑皮层的不同区域，可以具有不同的功能，因此非常适合以非常有效的方式处理语言。一方面，所提出的方法有可能打开法学硕士的黑匣子，并且可能是解开人类语言处理和认知背后的神经过程的又一步。</li>
</ul>

<h3>Title: Large Multimodal Model based Standardisation of Pathology Reports with  Confidence and their Prognostic Significance</h3>
<ul>
<li><strong>Authors: </strong>Ethar Alzaid, Gabriele Pergola, Harriet Evans, David Snead, Fayyaz Minhas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02040">https://arxiv.org/abs/2405.02040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02040">https://arxiv.org/pdf/2405.02040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02040]] Large Multimodal Model based Standardisation of Pathology Reports with  Confidence and their Prognostic Significance(https://arxiv.org/abs/2405.02040)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Pathology reports are rich in clinical and pathological details but are often presented in free-text format. The unstructured nature of these reports presents a significant challenge limiting the accessibility of their content. In this work, we present a practical approach based on the use of large multimodal models (LMMs) for automatically extracting information from scanned images of pathology reports with the goal of generating a standardised report specifying the value of different fields along with estimated confidence about the accuracy of the extracted fields. The proposed approach overcomes limitations of existing methods which do not assign confidence scores to extracted fields limiting their practical use. The proposed framework uses two stages of prompting a Large Multimodal Model (LMM) for information extraction and validation. The framework generalises to textual reports from multiple medical centres as well as scanned images of legacy pathology reports. We show that the estimated confidence is an effective indicator of the accuracy of the extracted information that can be used to select only accurately extracted fields. We also show the prognostic significance of structured and unstructured data from pathology reports and show that the automatically extracted field values significant prognostic value for patient stratification. The framework is available for evaluation via the URL: https://labieb.dcs.warwick.ac.uk/.</li>
<li><strong>摘要：</strong>病理报告包含丰富的临床和病理细节，但通常以自由文本格式呈现。这些报告的非结构化性质提出了限制其内容可访问性的重大挑战。在这项工作中，我们提出了一种基于使用大型多模态模型（LMM）的实用方法，用于从病理报告的扫描图像中自动提取信息，其目标是生成标准化报告，指定不同字段的值以及有关该字段的估计置信度。提取字段的准确性。所提出的方法克服了现有方法的局限性，现有方法不向提取的字段分配置信度分数，限制了其实际使用。所提出的框架使用两个阶段来提示大型多模态模型（LMM）以进行信息提取和验证。该框架适用于来自多个医疗中心的文本报告以及遗留病理报告的扫描图像。我们表明，估计的置信度是提取信息准确性的有效指标，可用于仅选择准确提取的字段。我们还展示了病理报告中结构化和非结构化数据的预后意义，并表明自动提取的字段对患者分层具有显着的预后价值。该框架可通过 URL 进行评估：https://labieb.dcs.warwick.ac.uk/。</li>
</ul>

<h3>Title: Argumentative Large Language Models for Explainable and Contestable  Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Freedman, Adam Dejl, Deniz Gorur, Xiang Yin, Antonio Rago, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02079">https://arxiv.org/abs/2405.02079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02079">https://arxiv.org/pdf/2405.02079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02079]] Argumentative Large Language Models for Explainable and Contestable  Decision-Making(https://arxiv.org/abs/2405.02079)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited by their inability to reliably provide outputs which are explainable and contestable. In this paper, we attempt to reconcile these strengths and weaknesses by introducing a method for supplementing LLMs with argumentative reasoning. Concretely, we introduce argumentative LLMs, a method utilising LLMs to construct argumentation frameworks, which then serve as the basis for formal reasoning in decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by the supplemented LLM may be naturally explained to, and contested by, humans. We demonstrate the effectiveness of argumentative LLMs experimentally in the decision-making task of claim verification. We obtain results that are competitive with, and in some cases surpass, comparable state-of-the-art techniques.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）中编码的知识的多样性以及它们在一系列设置中零样本应用这些知识的能力使它们成为决策中使用的有希望的候选者。然而，它们目前由于无法可靠地提供可解释和可争议的输出而受到限制。在本文中，我们试图通过引入一种用论证推理来补充法学硕士的方法来调和这些优点和缺点。具体来说，我们引入了论证性法学硕士，这是一种利用法学硕士构建论证框架的方法，然后将其作为决策中形式推理的基础。这些论证框架和形式推理的可解释性意味着补充法学硕士做出的任何决定都可以自然地向人类解释并受到人类的质疑。我们通过实验证明了论证性法学硕士在权利要求验证决策任务中的有效性。我们获得的结果与同类最先进的技术具有竞争力，在某些情况下甚至超越了同类最先进的技术。</li>
</ul>

<h3>Title: Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry  with GPT-4-Turbo</h3>
<ul>
<li><strong>Authors: </strong>Nakul Rampal, Kaiyu Wang, Matthew Burigana, Lingxiang Hou, Juri Al-Johani, Anna Sackmann, Hanan S. Murayshid, Walaa Abdullah Al-Sumari, Arwa M. Al-Abdulkarim, Nahla Eid Al-Hazmi, Majed O. Al-Awad, Christian Borgs, Jennifer T. Chayes, Omar M. Yaghi</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02128">https://arxiv.org/abs/2405.02128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02128">https://arxiv.org/pdf/2405.02128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02128]] Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry  with GPT-4-Turbo(https://arxiv.org/abs/2405.02128)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>The rapid advancement in artificial intelligence and natural language processing has led to the development of large-scale datasets aimed at benchmarking the performance of machine learning models. Herein, we introduce 'RetChemQA,' a comprehensive benchmark dataset designed to evaluate the capabilities of such models in the domain of reticular chemistry. This dataset includes both single-hop and multi-hop question-answer pairs, encompassing approximately 45,000 Q&As for each type. The questions have been extracted from an extensive corpus of literature containing about 2,530 research papers from publishers including NAS, ACS, RSC, Elsevier, and Nature Publishing Group, among others. The dataset has been generated using OpenAI's GPT-4 Turbo, a cutting-edge model known for its exceptional language understanding and generation capabilities. In addition to the Q&A dataset, we also release a dataset of synthesis conditions extracted from the corpus of literature used in this study. The aim of RetChemQA is to provide a robust platform for the development and evaluation of advanced machine learning algorithms, particularly for the reticular chemistry community. The dataset is structured to reflect the complexities and nuances of real-world scientific discourse, thereby enabling nuanced performance assessments across a variety of tasks. The dataset is available at the following link: https://github.com/nakulrampal/RetChemQA</li>
<li><strong>摘要：</strong>人工智能和自然语言处理的快速发展导致了旨在对机器学习模型的性能进行基准测试的大规模数据集的开发。在此，我们介绍“RetChemQA”，这是一个综合基准数据集，旨在评估此类模型在网状化学领域的能力。该数据集包括单跳和多跳问答对，每种类型包含大约 45,000 个问答。这些问题是从大量文献中提取的，其中包含 NAS、ACS、RSC、Elsevier 和 Nature Publishing Group 等出版商的约 2,530 篇研究论文。该数据集是使用 OpenAI 的 GPT-4 Turbo 生成的，这是一种以其卓越的语言理解和生成能力而闻名的尖端模型。除了问答数据集之外，我们还发布了从本研究中使用的文献语料库中提取的合成条件数据集。 RetChemQA 的目标是为高级机器学习算法的开发和评估提供一个强大的平台，特别是对于网状化学社区。该数据集的结构反映了现实世界科学话语的复杂性和细微差别，从而能够对各种任务进行细致入微的绩效评估。该数据集可通过以下链接获取：https://github.com/nakulrampal/RetChemQA</li>
</ul>

<h3>Title: Optimising Calls to Large Language Models with Uncertainty-Based  Two-Tier Selection</h3>
<ul>
<li><strong>Authors: </strong>Guillem Ramírez, Alexandra Birch, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02134">https://arxiv.org/abs/2405.02134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02134">https://arxiv.org/pdf/2405.02134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02134]] Optimising Calls to Large Language Models with Uncertainty-Based  Two-Tier Selection(https://arxiv.org/abs/2405.02134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Researchers and practitioners operating on a limited budget face the cost-performance trade-off dilemma. The challenging decision often centers on whether to use a large LLM with better performance or a smaller one with reduced costs. This has motivated recent research in the optimisation of LLM calls. Either a cascading strategy is used, where a smaller LLM or both are called sequentially, or a routing strategy is used, where only one model is ever called. Both scenarios are dependent on a decision criterion which is typically implemented by an extra neural model. In this work, we propose a simpler solution; we use only the uncertainty of the generations of the small LLM as the decision criterion. We compare our approach with both cascading and routing strategies using three different pairs of pre-trained small and large LLMs, on nine different tasks and against approaches that require an additional neural model. Our experiments reveal this simple solution optimally balances cost and performance, outperforming existing methods on 25 out of 27 experimental setups.</li>
<li><strong>摘要：</strong>预算有限的研究人员和从业者面临着成本效益权衡的困境。具有挑战性的决定通常集中在是使用性能更好的大型法学硕士还是成本较低的小型法学硕士。这激发了最近对 LLM 呼叫优化的研究。要么使用级联策略，即按顺序调用较小的 LLM 或两者，要么使用路由策略，即仅调用一个模型。这两种情况都依赖于通常由额外的神经模型实现的决策标准。在这项工作中，我们提出了一个更简单的解决方案；我们只使用小LLM的世代的不确定性作为决策标准。我们将我们的方法与级联和路由策略进行比较，使用三对不同的预先训练的小型和大型 LLM，在九个不同的任务上，并与需要额外神经模型的方法进行比较。我们的实验表明，这个简单的解决方案可以最佳地平衡成本和性能，在 27 个实验设置中的 25 个上优于现有方法。</li>
</ul>

<h3>Title: MedReadMe: A Systematic Study for Fine-grained Sentence Readability in  Medical Domain</h3>
<ul>
<li><strong>Authors: </strong>Chao Jiang, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02144">https://arxiv.org/abs/2405.02144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02144">https://arxiv.org/pdf/2405.02144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02144]] MedReadMe: A Systematic Study for Fine-grained Sentence Readability in  Medical Domain(https://arxiv.org/abs/2405.02144)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. In this paper, we present a systematic study on fine-grained readability measurements in the medical domain at both sentence-level and span-level. We introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel "Google-Easy" and "Google-Hard" categories. It supports our quantitative analysis, which covers 650 linguistic features and automatic complex word and jargon identification. Enabled by our high-quality annotation, we benchmark and improve several state-of-the-art sentence-level readability metrics for the medical domain specifically, which include unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments. We will publicly release the dataset and code.</li>
<li><strong>摘要：</strong>众所周知，医学文本很难阅读。正确衡量其可读性是使它们更易于访问的第一步。在本文中，我们对医学领域的句子级别和跨度级别的细粒度可读性测量进行了系统研究。我们引入了一个新的数据集 MedReadMe，它由手动注释的可读性评级和 4,520 个句子的细粒度复杂跨度注释组成，具有两个新颖的“Google-Easy”和“Google-Hard”类别。它支持我们的定量分析，涵盖 650 个语言特征以及自动复杂单词和行话识别。在我们的高质量注释的支持下，我们对医学领域的几种最先进的句子级可读性指标进行了基准测试和改进，其中包括使用最近开发的大型语言模型（LLM）的无监督、监督和基于提示的方法）。根据我们细粒度的复杂跨度注释，我们发现将单个特征（捕获术语跨度的数量）添加到现有的可读性公式中可以显着提高它们与人类判断的相关性。我们将公开发布数据集和代码。</li>
</ul>

<h3>Title: Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset</h3>
<ul>
<li><strong>Authors: </strong>Hsuvas Borkakoty, Luis Espinosa-Anke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02175">https://arxiv.org/abs/2405.02175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02175">https://arxiv.org/pdf/2405.02175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02175]] Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset(https://arxiv.org/abs/2405.02175)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Hoaxes are a recognised form of disinformation created deliberately, with potential serious implications in the credibility of reference knowledge resources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that they often are written according to the official style guidelines. In this work, we first provide a systematic analysis of the similarities and discrepancies between legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a collection of 311 Hoax articles (from existing literature as well as official Wikipedia lists) alongside semantically similar real articles. We report results of binary classification experiments in the task of predicting whether a Wikipedia article is real or hoax, and analyze several settings as well as a range of language models. Our results suggest that detecting deceitful content in Wikipedia based on content alone, despite not having been explored much in the past, is a promising direction.</li>
<li><strong>摘要：</strong>恶作剧是公认的故意制造的虚假信息形式，可能会严重影响维基百科等参考知识资源的可信度。维基百科恶作剧之所以难以检测，是因为它们通常是按照官方风格指南编写的。在这项工作中，我们首先对合法维基百科文章和恶作剧文章之间的相似之处和差异进行了系统分析，并介绍了 Hoaxpedia，这是一个由 311 篇恶作剧文章（来自现有文献以及官方维基百科列表）和语义上相似的真实文章组成的集合。我们报告了二元分类实验的结果，该实验旨在预测维基百科文章是真实的还是恶作剧，并分析了几种设置以及一系列语言模型。我们的结果表明，仅根据内容检测维基百科中的欺诈内容是一个很有前途的方向，尽管过去没有进行过太多探索。</li>
</ul>

<h3>Title: Assessing and Verifying Task Utility in LLM-Powered Applications</h3>
<ul>
<li><strong>Authors: </strong>Negar Arabzadeh, Siging Huo, Nikhil Mehta, Qinqyun Wu, Chi Wang, Ahmed Awadallah, Charles L. A. Clarke, Julia Kiseleva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02178">https://arxiv.org/abs/2405.02178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02178">https://arxiv.org/pdf/2405.02178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02178]] Assessing and Verifying Task Utility in LLM-Powered Applications(https://arxiv.org/abs/2405.02178)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://bit.ly/3w3yKcS .</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展导致了促进多个代理之间协作、协助人类完成日常任务的应用程序激增。然而，在评估 LLM 支持的应用程序在多大程度上真正提高用户体验和任务执行效率方面仍然存在重大差距。这凸显了验证 LLM 支持的应用程序实用性的必要性，特别是通过确保应用程序的功能和最终用户需求之间的一致性。我们引入了 AgentEval，这是一种新颖的框架，旨在通过自动提出一组针对任何给定应用程序的独特目的量身定制的标准来简化实用程序验证过程。这样可以进行全面评估，根据建议的标准量化应用程序的效用。我们对 AgentEval 对两个开源数据集（包括数学问题解决和 ALFWorld 家庭相关任务）的有效性和鲁棒性进行了全面分析。出于可重复性的目的，我们在 https://bit.ly/3w3yKcS 上公开提供数据、代码和所有日志。</li>
</ul>

<h3>Title: REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific  Sentences using Public and Proprietary LLMs</h3>
<ul>
<li><strong>Authors: </strong>Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02228">https://arxiv.org/abs/2405.02228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02228">https://arxiv.org/pdf/2405.02228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02228]] REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific  Sentences using Public and Proprietary LLMs(https://arxiv.org/abs/2405.02228)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Automatic citation generation for sentences in a document or report is paramount for intelligence analysts, cybersecurity, news agencies, and education personnel. In this research, we investigate whether large language models (LLMs) are capable of generating references based on two forms of sentence queries: (a) Direct Queries, LLMs are asked to provide author names of the given research article, and (b) Indirect Queries, LLMs are asked to provide the title of a mentioned article when given a sentence from a different article. To demonstrate where LLM stands in this task, we introduce a large dataset called REASONS comprising abstracts of the 12 most popular domains of scientific research on arXiv. From around 20K research articles, we make the following deductions on public and proprietary LLMs: (a) State-of-the-art, often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass percentage (PP) to minimize the hallucination rate (HR). When tested with Perplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented generation (RAG) using Mistral demonstrates consistent and robust citation support on indirect queries and matched performance to GPT-3.5 and GPT-4. The HR across all domains and models decreased by an average of 41.93% and the PP was reduced to 0% in most cases. In terms of generation quality, the average F1 Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with adversarial samples showed that LLMs, including the Advance RAG Mistral, struggle to understand context, but the extent of this issue was small in Mistral and GPT-4-Preview. Our study con tributes valuable insights into the reliability of RAG for automated citation generation tasks.</li>
<li><strong>摘要：</strong>自动生成文档或报告中的句子引文对于情报分析师、网络安全、新闻机构和教育人员至关重要。在这项研究中，我们调查大型语言模型 (LLM) 是否能够基于两种形式的句子查询生成参考文献：(a) 直接查询，LLM 被要求提供给定研究文章的作者姓名，以及 (b) 间接查询查询时，法学硕士被要求在给出另一篇文章的句子时提供提到的文章的标题。为了展示 LLM 在这项任务中的地位，我们引入了一个名为 REASONS 的大型数据集，其中包含 arXiv 上 12 个最流行的科学研究领域的摘要。从大约 20,000 篇研究文章中，我们对公共和专有 LLM 做出以下推论： (a) 最先进的，通常称为拟人化 GPT-4 和 GPT-3.5，其通过率 (PP) 较高，以尽量减少幻觉率（HR）。当使用 Perplexity.ai (7B) 进行测试时，他们出乎意料地犯了更多错误； (b) 增加相关元数据会降低 PP 并给出最低的 HR； (c) 使用 Mistral 的高级检索增强生成 (RAG) 展示了对间接查询的一致且强大的引用支持以及与 GPT-3.5 和 GPT-4 的匹配性能。所有领域和模型的 HR 平均下降了 41.93%，大多数情况下 PP 降至 0%。发电质量方面，平均F1 Score和BLEU分别为68.09%和57.51%； (d) 对抗样本测试表明，法学硕士（包括 Advance RAG Mistral）很难理解上下文，但这个问题在 Mistral 和 GPT-4-Preview 中的程度很小。我们的研究为 RAG 在自动引文生成任务中的可靠性提供了宝贵的见解。</li>
</ul>

<h3>Title: Vibe-Eval: A hard evaluation suite for measuring progress of multimodal  language models</h3>
<ul>
<li><strong>Authors: </strong>Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Mikel Artetxe, Yi Tay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.02287">https://arxiv.org/abs/2405.02287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.02287">https://arxiv.org/pdf/2405.02287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.02287]] Vibe-Eval: A hard evaluation suite for measuring progress of multimodal  language models(https://arxiv.org/abs/2405.02287)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chat</a></li>
<li><strong>Abstract: </strong>We introduce Vibe-Eval: a new open benchmark and framework for evaluating multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts, including 100 of hard difficulty, complete with gold-standard responses authored by experts. Vibe-Eval is open-ended and challenging with dual objectives: (i) vibe checking multimodal chat models for day-to-day tasks and (ii) rigorously testing and probing the capabilities of present frontier models. Notably, our hard set contains >50% questions that all frontier models answer incorrectly. We explore the nuances of designing, evaluating, and ranking models on ultra challenging prompts. We also discuss trade-offs between human and automatic evaluation, and show that automatic model evaluation using Reka Core roughly correlates to human judgment. We offer free API access for the purpose of lightweight evaluation and plan to conduct formal human evaluations for public models that perform well on the Vibe-Eval's automatic scores. We release the evaluation code and data, see https://github.com/reka-ai/reka-vibe-eval</li>
<li><strong>摘要：</strong>我们推出 Vibe-Eval：用于评估多模式聊天模型的新开放基准和框架。 Vibe-Eval 包含 269 个视觉理解提示，其中包括 100 个困难难度，并配有专家撰写的黄金标准答案。 Vibe-Eval 是开放式的，具有双重目标，具有挑战性：(i) Vibe 检查日常任务的多模式聊天模型；(ii) 严格测试和探索当前前沿模型的功能。值得注意的是，我们的硬集包含超过 50% 的问题，所有前沿模型都回答错误。我们探索在极具挑战性的提示下设计、评估和排名模型的细微差别。我们还讨论了人类评估和自动评估之间的权衡，并表明使用 Reka Core 的自动模型评估与人类判断大致相关。我们提供免费的 API 访问以进行轻量级评估，并计划对在 Vibe-Eval 自动评分上表现良好的公共模型进行正式的人工评估。我们发布了评估代码和数据，参见https://github.com/reka-ai/reka-vibe-eval</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
