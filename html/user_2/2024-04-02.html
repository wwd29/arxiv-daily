<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-02</h1>
<h3>Title: Classifying Conspiratorial Narratives At Scale: False Alarms and  Erroneous Connections</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Diab, Rr. Nefriana, Yu-Ru Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00141">https://arxiv.org/abs/2404.00141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00141">https://arxiv.org/pdf/2404.00141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00141]] Classifying Conspiratorial Narratives At Scale: False Alarms and  Erroneous Connections(https://arxiv.org/abs/2404.00141)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>Online discussions frequently involve conspiracy theories, which can contribute to the proliferation of belief in them. However, not all discussions surrounding conspiracy theories promote them, as some are intended to debunk them. Existing research has relied on simple proxies or focused on a constrained set of signals to identify conspiracy theories, which limits our understanding of conspiratorial discussions across different topics and online communities. This work establishes a general scheme for classifying discussions related to conspiracy theories based on authors' perspectives on the conspiracy belief, which can be expressed explicitly through narrative elements, such as the agent, action, or objective, or implicitly through references to known theories, such as chemtrails or the New World Order. We leverage human-labeled ground truth to train a BERT-based model for classifying online CTs, which we then compared to the Generative Pre-trained Transformer machine (GPT) for detecting online conspiratorial content. Despite GPT's known strengths in its expressiveness and contextual understanding, our study revealed significant flaws in its logical reasoning, while also demonstrating comparable strengths from our classifiers. We present the first large-scale classification study using posts from the most active conspiracy-related Reddit forums and find that only one-third of the posts are classified as positive. This research sheds light on the potential applications of large language models in tasks demanding nuanced contextual comprehension.</li>
<li><strong>摘要：</strong>网上讨论经常涉及阴谋论，这可能会导致人们对阴谋论的信仰扩散。然而，并非所有围绕阴谋论的讨论都在宣扬阴谋论，因为有些讨论的目的是揭穿阴谋论。现有的研究依赖于简单的代理或专注于一组有限的信号来识别阴谋论，这限制了我们对不同主题和在线社区的阴谋讨论的理解。这项工作根据作者对阴谋信仰的观点建立了一个对与阴谋论相关的讨论进行分类的总体方案，可以通过代理人、行动或目标等叙事元素明确表达，也可以通过引用已知理论隐式表达。例如化学尾迹或新世界秩序。我们利用人类标记的真实数据来训练基于 BERT 的模型，用于对在线 CT 进行分类，然后将其与用于检测在线阴谋内容的生成式预训练 Transformer 机 (GPT) 进行比较。尽管 GPT 在表达能力和语境理解方面具有众所周知的优势，但我们的研究揭示了其逻辑推理的重大缺陷，同时也证明了我们的分类器的类似优势。我们使用来自最活跃的阴谋相关 Reddit 论坛的帖子进行了首次大规模分类研究，发现只有三分之一的帖子被归类为正面帖子。这项研究揭示了大型语言模型在需要细致入微的上下文理解的任务中的潜在应用。</li>
</ul>

<h3>Title: On-the-fly Definition Augmentation of LLMs for Biomedical NER</h3>
<ul>
<li><strong>Authors: </strong>Monica Munnangi, Sergey Feldman, Byron C Wallace, Silvio Amir, Tom Hope, Aakanksha Naik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00152">https://arxiv.org/abs/2404.00152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00152">https://arxiv.org/pdf/2404.00152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00152]] On-the-fly Definition Augmentation of LLMs for Biomedical NER(https://arxiv.org/abs/2404.00152)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs. For example, it leads to a relative improvement of 15\% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM performance, allowing them to outperform fine-tuned language models in few-shot settings. To facilitate future research in this direction, we release our code at https://github.com/allenai/beacon.</li>
<li><strong>摘要：</strong>尽管法学硕士具有一般能力，但他们在生物医学 NER 任务上仍然举步维艰，由于专业术语的存在和训练数据的缺乏，这些任务很困难。在这项工作中，我们着手通过一种新的知识增强方法，在有限的数据设置中提高生物医学 NER 的法学硕士表现，该方法结合了动态相关概念的定义。在此过程中，为了提供知识增强的试验台，我们对提示策略进行了全面的探索。我们的实验表明，定义增强对于开源和封闭式法学硕士都很有用。例如，它使我们所有（六个）测试数据集的 GPT-4 性能 (F1) 相对提高了 15%（平均）。我们进行了广泛的消融和分析，以证明我们的性能改进源于添加相关的定义知识。我们发现，仔细的提示策略也可以提高 LLM 的性能，使它们能够在少量设置中超越微调的语言模型。为了促进这个方向的未来研究，我们在 https://github.com/allenai/beacon 发布了我们的代码。</li>
</ul>

<h3>Title: DataAgent: Evaluating Large Language Models' Ability to Answer  Zero-Shot, Natural Language Queries</h3>
<ul>
<li><strong>Authors: </strong>Manit Mishra, Abderrahman Braham, Charles Marsom, Bryan Chung, Gavin Griffin, Dakshesh Sidnerlikar, Chatanya Sarin, Arjun Rajaram</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00188">https://arxiv.org/abs/2404.00188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00188">https://arxiv.org/pdf/2404.00188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00188]] DataAgent: Evaluating Large Language Models' Ability to Answer  Zero-Shot, Natural Language Queries(https://arxiv.org/abs/2404.00188)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data Scientist" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis.</li>
<li><strong>摘要：</strong>分析数据集和提取有意义信息的传统过程通常既耗时又费力。之前的工作已将手动、重复编码和数据收集确定为阻碍数据科学家开展更细致的工作和高水平项目的主要障碍。为了解决这个问题，我们将 OpenAI 的 GPT-3.5 评估为“语言数据科学家”（LDS），它可以从给定的数据集中推断出关键发现，包括相关性和基本信息。该模型在一组不同的基准数据集上进行了测试，以评估其跨多个标准的性能，包括涉及 NumPy、Pandas、Scikit-Learn 和 TensorFlow 等库的基于数据科学代码生成的任务，并且在正确回答问题方面取得了广泛成功。给定与基准数据集相关的数据科学查询。 LDS 使用各种新颖的提示工程技术来有效回答给定问题，包括思想链强化和 SayCan 提示工程。我们的研究结果表明，利用大型语言模型进行低级、零样本数据分析具有巨大潜力。</li>
</ul>

<h3>Title: GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream  Neural Network Enhancement with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Jiawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00189">https://arxiv.org/abs/2404.00189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00189">https://arxiv.org/pdf/2404.00189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00189]] GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream  Neural Network Enhancement with LLMs(https://arxiv.org/abs/2404.00189)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study introduces GPTA, a Large Language Model assistance training framework, that enhances the training of downstream task models via prefix prompt. By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and LLMs with the novel ``dialogue gradient''. The framework not only demonstrates significant improvements in model performance across six NLP benchmark datasets, but also reduces overfitting in low-resource scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with LLM support.</li>
<li><strong>摘要：</strong>本研究引入了大型语言模型辅助训练框架GPTA，通过前缀提示增强下游任务模型的训练。通过最大限度地减少 LLM 的数据暴露，该框架解决了在下游任务模型训练中应用 LLM 的安全和法律挑战。 GPTA 采用新的协同训练方法，通过参数梯度优化下游模型，并通过新颖的“对话梯度”优化法学硕士。该框架不仅在六个 NLP 基准数据集上展示了模型性能的显着改进，而且还有效减少了低资源场景下的过度拟合。详细的分析进一步验证了我们的先驱框架为在法学硕士支持下的下游任务模型训练提供了一种经济高效且适应性强的方法。</li>
</ul>

<h3>Title: Conceptual and Unbiased Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ben Zhou, Hongming Zhang, Sihao Chen, Dian Yu, Hongwei Wang, Baolin Peng, Dan Roth, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00205">https://arxiv.org/abs/2404.00205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00205">https://arxiv.org/pdf/2404.00205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00205]] Conceptual and Unbiased Reasoning in Language Models(https://arxiv.org/abs/2404.00205)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual reasoning on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods. We then discuss how models can improve since high-level abstract reasoning is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying reasoning paths and asking models to perform self-refinement. Experiments show that our proposed techniques improve models' conceptual reasoning performance by 8% to 11%, achieving a more robust reasoning system that relies less on inductive biases.</li>
<li><strong>摘要：</strong>概念推理，即从抽象和高层次角度进行推理的能力，是人类认知泛化的关键。然而，对大型语言模型执行概念推理的能力的研究有限。在这项工作中，我们弥合了这一差距，并提出了一种新颖的概念化框架，该框架迫使模型对抽象问题进行概念推理，并在可验证的符号空间中生成解决方案。使用这个框架作为分析工具，我们发现现有的大型语言模型在概念推理方面存在不足，与直接推理方法相比，在各种基准测试中下降了 9% 到 28%。然后我们讨论模型如何改进，因为高级抽象推理是公正和可概括决策的关键。我们提出了两种技术来添加可信的归纳信号，即通过生成具有相似底层推理路径的熟悉问题并要求模型执行自我改进。实验表明，我们提出的技术将模型的概念推理性能提高了 8% 到 11%，实现了更少依赖归纳偏差的更稳健的推理系统。</li>
</ul>

<h3>Title: Causal Inference for Human-Language Model Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Bohan Zhang, Yixin Wang, Paramveer S. Dhillon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00207">https://arxiv.org/abs/2404.00207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00207">https://arxiv.org/pdf/2404.00207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00207]] Causal Inference for Human-Language Model Collaboration(https://arxiv.org/abs/2404.00207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (ISE) -- which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop CausalCollab, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that CausalCollab effectively reduces confounding and significantly improves counterfactual estimation over a set of competitive baselines.</li>
<li><strong>摘要：</strong>在本文中，我们研究了人类和语言模型 (LM) 之间的协作动态，其中交互通常涉及 LM 提出文本片段以及人类编辑或响应这些建议。在这种情况下，人类与语言模型的有效互动需要从人类与语言模型的历史交互中辨别出有效的基于文本的交互策略，例如编辑和响应风格。这一目标本质上是因果关系，由反事实的“假设”问题驱动：如果人类采用不同的文本编辑/细化策略，协作的结果将如何变化？回答这个因果推理问题的一个关键挑战是制定适当的因果估计值：传统的平均治疗效果（ATE）估计值由于其高维度而不适用于基于文本的治疗。为了解决这个问题，我们引入了一种新的因果估计量——增量文体效应（ISE）——它描述了将文本无限地转向特定风格（例如增加正式性）的平均影响。我们建立了ISE非参数辨识的条件。在此基础上，我们开发了 CausalCollab，一种旨在估计动态人类与LM 协作中各种交互策略的 ISE 的算法。我们对三种不同的人类与 LM 协作场景的实证调查表明，CausalCollab 有效减少了混杂因素，并显着改进了对一组竞争基线的反事实估计。</li>
</ul>

<h3>Title: EventGround: Narrative Reasoning by Grounding to Eventuality-centric  Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Cheng Jiayang, Lin Qiu, Chunkit Chan, Xin Liu, Yangqiu Song, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00209">https://arxiv.org/abs/2404.00209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00209">https://arxiv.org/pdf/2404.00209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00209]] EventGround: Narrative Reasoning by Grounding to Eventuality-centric  Knowledge Graphs(https://arxiv.org/abs/2404.00209)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsity problems. We provide simple yet effective parsing and partial information extraction methods to tackle these problems. Experimental results demonstrate that our approach consistently outperforms baseline models when combined with graph neural network (GNN) or large language model (LLM) based graph reasoning models. Our framework, incorporating grounded knowledge, achieves state-of-the-art performance while providing interpretable evidence.</li>
<li><strong>摘要：</strong>叙事推理依赖于对故事背景中可能发生的事件的理解，这需要丰富的背景世界知识。为了帮助机器利用这些知识，现有的解决方案可以分为两类。有些人专注于通过预训练具有偶发性感知目标的语言模型 (LM) 来隐式建模偶发性知识。然而，这种方法破坏了知识结构并且缺乏可解释性。其他人明确地将有关偶发事件的世界知识收集到结构化的以偶发事件为中心的知识图（KG）中。然而，现有的利用这些知识源获取自由文本的研究是有限的。在这项工作中，我们提出了一个名为 EventGround 的初步综合框架，旨在解决将自由文本扎根于以事件为中心的知识图谱以进行情境化叙事推理的问题。我们在这个方向上确定了两个关键问题：事件表示和稀疏性问题。我们提供简单而有效的解析和部分信息提取方法来解决这些问题。实验结果表明，当与基于图神经网络（GNN）或大语言模型（LLM）的图推理模型相结合时，我们的方法始终优于基线模型。我们的框架结合了扎实的知识，在提供可解释的证据的同时实现了最先进的性能。</li>
</ul>

<h3>Title: Multi-Conditional Ranking with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pouya Pezeshkpour, Estevam Hruschka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00211">https://arxiv.org/abs/2404.00211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00211">https://arxiv.org/pdf/2404.00211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00211]] Multi-Conditional Ranking with Large Language Models(https://arxiv.org/abs/2404.00211)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the items (EXSIR). Our extensive experiments show that this decomposed reasoning method enhances LLMs' performance significantly, achieving up to a 12% improvement over existing LLMs. We also provide a detailed analysis of LLMs performance across various condition categories, and examine the effectiveness of decomposition step. Furthermore, we compare our method with existing approaches such as Chain-of-Thought and an encoder-type ranking model, demonstrating the superiority of our approach and complexity of MCR task. We released our dataset and code.</li>
<li><strong>摘要：</strong>利用大型语言模型 (LLM) 对一组项目进行排名已成为推荐和检索系统中的常见方法。通常，这些系统专注于根据给定查询以单调顺序对大量文档进行排序。然而，现实世界的场景通常会带来不同的挑战：对相对较小的一组项目进行排名，但要根据各种不同且偶尔发生冲突的条件。在本文中，我们通过引入 MCRank 来定义和探索多条件排名的任务，MCRank 是一个为评估各种项目类型和条件的多条件排名而定制的基准。我们使用 MCRank 对法学硕士的分析表明，随着项目和条件的数量和复杂性的增加，其表现会显着下降。为了克服这个限制，我们提出了一种新颖的分解推理方法，包括对条件进行提取和排序，然后对项目进行迭代排名（EXSIR）。我们的大量实验表明，这种分解推理方法显着提高了法学硕士的表现，比现有法学硕士的性能提高了 12%。我们还对各种条件类别的法学硕士表现进行了详细分析，并检查了分解步骤的有效性。此外，我们将我们的方法与现有的方法（例如思想链和编码器类型排序模型）进行比较，证明了我们的方法的优越性和 MCR 任务的复杂性。我们发布了我们的数据集和代码。</li>
</ul>

<h3>Title: Injecting New Knowledge into Large Language Models via Supervised  Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar Reddy Yannam, Tolga Aktas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00213">https://arxiv.org/abs/2404.00213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00213">https://arxiv.org/pdf/2404.00213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00213]] Injecting New Knowledge into Large Language Models via Supervised  Fine-Tuning(https://arxiv.org/abs/2404.00213)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts. We present a novel dataset generation process that leads to more effective knowledge ingestion through SFT, and our results show considerable performance improvements in Q&A tasks related to out-of-domain knowledge. This study contributes to the understanding of domain adaptation for LLMs and highlights the potential of SFT in enhancing the factuality of LLM responses in specific knowledge domains.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLM）在生成类人文本方面表现出了卓越的性能，被证明是跨各种应用程序的宝贵资产。然而，调整这些模型以纳入新的、领域外的知识仍然是一个挑战，特别是对于模型知识截止日期之后发生的事实和事件。本文研究了监督微调（SFT）作为法学硕士知识注入方法的有效性，特别关注最近的体育赛事领域。我们比较不同的数据集生成策略（基于标记和基于事实的缩放），以创建帮助模型学习新信息的训练数据。我们在 GPT-4 上的实验表明，虽然基于令牌的扩展可以提高问答准确性，但它可能无法提供新知识的统一覆盖。另一方面，基于事实的扩展提供了一种更系统的方法来确保均匀覆盖所有事实。我们提出了一种新颖的数据集生成过程，可以通过 SFT 更有效地获取知识，我们的结果表明，与域外知识相关的问答任务的性能得到了显着提高。这项研究有助于理解法学硕士的领域适应，并强调了 SFT 在增强特定知识领域法学硕士回答的真实性方面的潜力。</li>
</ul>

<h3>Title: Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge  Editing Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00216">https://arxiv.org/abs/2404.00216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00216">https://arxiv.org/pdf/2404.00216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00216]] Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge  Editing Benchmark(https://arxiv.org/abs/2404.00216)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展使它们能够以更类似于人类的方式传达事实知识。通过修改具有事实解码功能的法学硕士，人们做出了广泛的努力来减少事实幻觉。然而，它们也带来了阻碍知识更新的风险，因为它们使模型对已知事实过于自信。在这项工作中，我们首先重新审视当前的事实解码方法，并验证它们在提高事实准确性方面的有效性。随后，我们在知识编辑基准上对几种强事实性解码方法进行了进一步评估。与原始解码相比，所有这些解码方法都显着降低了 llama2 模型的性能，最大降幅达到惊人的 81.3%。这进一步表明，当前现有的解码方法仍然无法完美解决事实幻觉，因为它们忽视了保留知识编辑灵活性的重要性。因此，我们的工作表明，对事实对齐的研究应同时关注知识编辑的有效性。</li>
</ul>

<h3>Title: Classification and Clustering of Sentence-Level Embeddings of Scientific  Articles Generated by Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Bartz Guedes, Ana Estela Antunes da Silva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00224">https://arxiv.org/abs/2404.00224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00224">https://arxiv.org/pdf/2404.00224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00224]] Classification and Clustering of Sentence-Level Embeddings of Scientific  Articles Generated by Contrastive Learning(https://arxiv.org/abs/2404.00224)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Scientific articles are long text documents organized into sections, each describing aspects of the research. Analyzing scientific production has become progressively challenging due to the increase in the number of available articles. Within this scenario, our approach consisted of fine-tuning transformer language models to generate sentence-level embeddings from scientific articles, considering the following labels: background, objective, methods, results, and conclusion. We trained our models on three datasets with contrastive learning. Two datasets are from the article's abstracts in the computer science and medical domains. Also, we introduce PMC-Sents-FULL, a novel dataset of sentences extracted from the full texts of medical articles. We compare the fine-tuned and baseline models in clustering and classification tasks to evaluate our approach. On average, clustering agreement measures values were five times higher. For the classification measures, in the best-case scenario, we had an average improvement in F1-micro of 30.73\%. Results show that fine-tuning sentence transformers with contrastive learning and using the generated embeddings in downstream tasks is a feasible approach to sentence classification in scientific articles. Our experiment codes are available on GitHub.</li>
<li><strong>摘要：</strong>科学文章是按部分组织的长文本文档，每个部分描述研究的各个方面。由于可用文章数量的增加，分析科学成果变得越来越具有挑战性。在这种情况下，我们的方法包括微调 Transformer 语言模型，以从科学文章生成句子级嵌入，并考虑以下标签：背景、目标、方法、结果和结论。我们通过对比学习在三个数据集上训练我们的模型。两个数据集来自计算机科学和医学领域的文章摘要。此外，我们还介绍了 PMC-Sents-FULL，这是一个从医学文章全文中提取的新颖句子数据集。我们比较聚类和分类任务中的微调模型和基线模型来评估我们的方法。平均而言，聚类一致性度量值高出五倍。对于分类措施，在最好的情况下，我们在 F1-micro 上的平均改进为 30.73%。结果表明，通过对比学习微调句子转换器并在下游任务中使用生成的嵌入是科学文章中句子分类的可行方法。我们的实验代码可以在 GitHub 上找到。</li>
</ul>

<h3>Title: DeFT: Flash Tree-attention with IO-Awareness for Efficient  Tree-search-based LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00242">https://arxiv.org/abs/2404.00242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00242">https://arxiv.org/pdf/2404.00242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00242]] DeFT: Flash Tree-attention with IO-Awareness for Efficient  Tree-search-based LLM Inference(https://arxiv.org/abs/2404.00242)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculation: we calculate partial attention of each QKV groups in a fused kernel then apply a Tree-topology-aware Global Reduction strategy to get final attention. Thanks to a reduction in KV cache IO by 3.6-4.5$\times$, along with an additional reduction in IO for $\mathbf{Q} \mathbf{K}^\top$ and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup of 1.7-2.4$\times$ in end-to-end latency across two practical reasoning tasks over the SOTA attention algorithms.</li>
<li><strong>摘要：</strong>使用树搜索进行解码可以极大地提高基于 Transformer 的大型语言模型 (LLM) 的推理质量。根据引导信号，它通过形成 LLM 输出来搜索树中从根到叶的最佳路径，以提高可控性、推理能力、对齐等。然而，由于计算、内存占用和内存访问方面的冗余，当前的树解码策略及其推理系统不能很好地适应，从而导致推理效率低下。为了解决这个问题，我们提出了 DeFT，一种 IO 感知的树注意力算法，它分两个阶段保持内存高效的注意力计算和低内存占用：（1）QKV 准备：我们提出了一种 KV 引导树分割策略来明智地对 QKV 进行分组为了提高GPU的利用率，并尽可能减少GPU全局内存和片上共享内存之间的KV缓存的内存读写； (2)注意力计算：我们计算融合内核中每个QKV组的部分注意力，然后应用树拓扑感知的全局缩减策略来获得最终的注意力。由于 KV 缓存 IO 减少了 3.6-4.5$\times$，同时 $\mathbf{Q} \mathbf{K}^\top$ 的 IO 进一步减少，Softmax 相当于总 KV 的 25%与 SOTA 注意力算法相比，DeFT 在两个实际推理任务中的端到端延迟可以实现 1.7-2.4$\times$ 的加速。</li>
</ul>

<h3>Title: Your Co-Workers Matter: Evaluating Collaborative Capabilities of  Language Models in Blocks World</h3>
<ul>
<li><strong>Authors: </strong>Guande Wu, Chen Zhao, Claudio Silva, He He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00246">https://arxiv.org/abs/2404.00246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00246">https://arxiv.org/pdf/2404.00246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00246]] Your Co-Workers Matter: Evaluating Collaborative Capabilities of  Language Models in Blocks World(https://arxiv.org/abs/2404.00246)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state and identify and correct execution errors. Both human-machine and machine-machine experiments show that LLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric.</li>
<li><strong>摘要：</strong>独立与世界交互的语言代理在自动化数字任务方面具有巨大的潜力。虽然大型语言模型 (LLM) 代理在理解和执行文本游戏和网页控制等任务方面取得了进展，但许多现实世界的任务还需要与人类或其他同等角色的 LLM 协作，这涉及意图理解、任务协调和沟通。为了测试 LLM 的协作能力，我们设计了一个块世界环境，其中两个代理（每个代理都有独特的目标和技能）一起构建目标结构。为了完成目标，他们可以在世界中行动并用自然语言进行交流。在这种环境下，我们设计了越来越具有挑战性的设置来评估不同的协作视角，从独立任务到更复杂的依赖任务。我们进一步采用思想链提示，其中包括中间推理步骤来对合作伙伴的状态进行建模并识别和纠正执行错误。人机和机器机实验都表明LLM代理具有很强的接地能力，并且我们的方法显着提高了评估指标。</li>
</ul>

<h3>Title: DiLM: Distilling Dataset into Language Model for Text-level Dataset  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Aru Maekawa, Satoshi Kosugi, Kotaro Funakoshi, Manabu Okumura</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00264">https://arxiv.org/abs/2404.00264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00264">https://arxiv.org/pdf/2404.00264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00264]] DiLM: Distilling Dataset into Language Model for Text-level Dataset  Distillation(https://arxiv.org/abs/2404.00264)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at https://github.com/arumaekawa/DiLM.</li>
<li><strong>摘要：</strong>数据集蒸馏的目的是通过创建少量信息丰富的合成样本来压缩训练数据集，以便在它们上训练的神经网络的性能与在原始训练数据集上训练的神经网络一样好。当前的文本数据集蒸馏方法将每个合成样本创建为一系列词嵌入而不是文本，以应用基于梯度的优化；然而，这种嵌入级蒸馏数据集不能用于训练其词嵌入权重与用于蒸馏的模型不同的其他模型。为了解决这个问题，我们提出了一种新颖的文本数据集蒸馏方法，称为将数据集蒸馏成语言模型（DiLM），它训练语言模型以生成信息丰富的合成训练样本作为文本数据，而不是直接优化合成样本。我们在各种文本分类数据集上评估了 DiLM，结果表明，从 DiLM 提取的合成数据集优于当前核心集选择方法的数据集。 DiLM 在训练不同类型的模型和大型语言模型的上下文学习方面取得了显着的泛化性能。我们的代码将在 https://github.com/arumaekawa/DiLM 上提供。</li>
</ul>

<h3>Title: Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal  Traits</h3>
<ul>
<li><strong>Authors: </strong>Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan Wen, Ala Tak, Fred Morstatter, Morteza Dehghani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00267">https://arxiv.org/abs/2404.00267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00267">https://arxiv.org/pdf/2404.00267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00267]] Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal  Traits(https://arxiv.org/abs/2404.00267)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not fully diminish the predictive power of authors' linguistic patterns over their personal traits. We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when LLMs are used in the writing process. Our findings have important implications for the study of linguistic markers of personal traits in the age of LLMs.</li>
<li><strong>摘要：</strong>先前的研究已经建立了个人语言使用与其个人特质之间的联系。我们的语言模式揭示了我们的性格、情绪状态和信仰的信息。然而，随着大型语言模型 (LLM) 在日常写作中越来越多地采用写作助手，一个关键问题出现了：当 LLM 参与写作过程时，作者的语言模式是否仍然可以预测他们的个人特征？我们研究了法学硕士对人口和心理特征的语言标记的影响，特别研究了三个法学硕士 - GPT3.5、Llama 2 和 Gemini - 涵盖六种不同的特征：性别、年龄、政治倾向、个性、同理心和道德。我们的研究结果表明，虽然法学硕士的使用稍微降低了语言模式对作者个人特质的预测能力，但显着的变化并不常见，而且法学硕士的使用并没有完全削弱作者语言模式对其个人特质的预测能力。我们还注意到，当法学硕士用于写作过程时，一些理论上建立的基于词汇的语言标记失去了作为预测因子的可靠性。我们的研究结果对于法学硕士时代个人特质的语言标记研究具有重要意义。</li>
</ul>

<h3>Title: A Comprehensive Study on NLP Data Augmentation for Hate Speech  Detection: Legacy Methods, BERT, and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Md Saroar Jahan, Mourad Oussalah, Djamila Romaissa Beddia, Jhuma kabir Mim, Nabil Arhab</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00303">https://arxiv.org/abs/2404.00303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00303">https://arxiv.org/pdf/2404.00303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00303]] A Comprehensive Study on NLP Data Augmentation for Hate Speech  Detection: Legacy Methods, BERT, and LLMs(https://arxiv.org/abs/2404.00303)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.</li>
<li><strong>摘要：</strong>由于需要解决仇恨言论领域带来的挑战、社交媒体词汇的动态性质以及需要大量训练数据的大规模神经网络的需求，推动了 NLP 领域对数据增强的兴趣激增。然而，数据增强中词汇替换的普遍使用引起了人们的担忧，因为它可能会无意中改变预期含义，从而影响监督机器学习模型的有效性。为了寻求合适的数据增强方法，本研究探索了既定的传统方法和当代实践，例如大型语言模型 (LLM)，包括仇恨言论检测中的 GPT。此外，我们提出了基于 BERT 的编码器模型的优化利用和上下文余弦相似度过滤，暴露了先前同义词替换方法的显着局限性。我们的比较分析涵盖五种流行的增强技术：WordNet 和 Fast-Text 同义词替换、反向翻译、BERT-mask 上下文增强和法学硕士。我们对五个基准数据集的分析表明，虽然反向翻译等传统方法显示出较低的标签更改率（0.3-1.5%），而基于 BERT 的上下文同义词替换提供了句子多样性，但代价是较高的标签更改率（超过 6%） ）。我们提出的基于 BERT 的上下文余弦相似度过滤将标签更改显着减少到仅 0.05%，证明了其 F1 性能提高 0.7% 的功效。然而，使用 GPT-3 增强数据不仅避免了过度拟合，数据增加了多达七倍，而且与传统方法相比，嵌入空间覆盖率提高了 15%，分类 F1 分数提高了 1.4%，比我们的方法提高了 0.8%。</li>
</ul>

<h3>Title: Can LLMs Master Math? Investigating Large Language Models on Math Stack  Exchange</h3>
<ul>
<li><strong>Authors: </strong>Ankit Satpute, Noah Giessing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00344">https://arxiv.org/abs/2404.00344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00344">https://arxiv.org/pdf/2404.00344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00344]] Can LLMs Master Math? Investigating Large Language Models on Math Stack  Exchange(https://arxiv.org/abs/2404.00344)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning. We make our code and findings publicly available for research: \url{https://github.com/gipplab/LLM-Investig-MathStackExchange}</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种自然语言任务中表现出了卓越的能力，通常能够实现超越人类的表现。尽管取得了这些进步，数学领域仍然面临着独特的挑战，这主要是由于其特殊的结构和所需的精度。在这项研究中，我们采用了两步法来调查法学硕士回答数学问题的熟练程度。首先，我们聘用了最有效的法学硕士（根据其在数学问答基准上的表现来确定），从 Math Stack Exchange (MSE) 中生成 78 个问题的答案。其次，对表现最好的LLM进行案例分析，通过人工评估重点关注其答案的质量和准确性。我们发现，在针对回答数学问题进行微调的现有法学硕士中，GPT-4 表现最佳（nDCG 为 0.48，P@10 为 0.37），并且考虑到 P@10，其性能优于 ArqMATH3 Task1 上当前的最佳方法。我们的案例分析表明，虽然 GPT-4 在某些情况下可以生成相关响应，但它并不能始终如一地准确回答所有问题。本文探讨了法学硕士目前在解决复杂数学问题方面的局限性。通过案例分析，我们揭示了法学硕士在数学方面能力的差距，从而为人工智能驱动的数学推理的未来研究和进步奠定了基础。我们公开我们的代码和研究结果以供研究：\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}</li>
</ul>

<h3>Title: Controllable and Diverse Data Augmentation with Large Language Model for  Low-Resource Open-Domain Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhenhua Liu, Tong Zhu, Jianxiang Xiang, Wenliang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00361">https://arxiv.org/abs/2404.00361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00361">https://arxiv.org/pdf/2404.00361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00361]] Controllable and Diverse Data Augmentation with Large Language Model for  Low-Resource Open-Domain Dialogue Generation(https://arxiv.org/abs/2404.00361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Data augmentation (DA) is crucial to mitigate model training instability and over-fitting problems in low-resource open-domain dialogue generation. However, traditional DA methods often neglect semantic data diversity, restricting the overall quality. Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem, we propose \textbf{S}ummary-based \textbf{D}ialogue \textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability of LLM by using dialogue summaries as a planning tool. Based on summaries, SDA can generate high-quality and diverse dialogue data even with a small seed dataset. To evaluate the efficacy of data augmentation methods for open-domain dialogue, we designed a clustering-based metric to characterize the semantic diversity of the augmented dialogue data. The experimental results show that SDA can augment high-quality and semantically diverse dialogues given a small seed dataset and an LLM, and the augmented data can boost the performance of open-domain dialogue models.</li>
<li><strong>摘要：</strong>数据增强（DA）对于缓解低资源开放域对话生成中的模型训练不稳定和过度拟合问题至关重要。然而，传统的DA方法往往忽视语义数据的多样性，限制了整体质量。最近，大语言模型（LLM）已被用于DA来生成多样化的对话。然而，它们的可控性有限，并且与种子对话相比，往往会生成具有分布变化的对话。为了最大化增强多样性并解决可控性问题，我们提出基于 \textbf{S}ummary \textbf{D}ialogue \textbf{A} 的 LLM (SDA) 增强。我们的方法通过使用对话摘要作为规划工具来增强法学硕士的可控性。基于摘要，即使种子数据集很小，SDA 也可以生成高质量和多样化的对话数据。为了评估开放域对话的数据增强方法的有效性，我们设计了一种基于聚类的度量来表征增强对话数据的语义多样性。实验结果表明，在给定小型种子数据集和 LLM 的情况下，SDA 可以增强高质量和语义多样化的对话，并且增强的数据可以提高开放域对话模型的性能。</li>
</ul>

<h3>Title: Small Language Models Learn Enhanced Reasoning Skills from Medical  Textbooks</h3>
<ul>
<li><strong>Authors: </strong>Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee, Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00376">https://arxiv.org/abs/2404.00376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00376">https://arxiv.org/pdf/2404.00376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00376]] Small Language Models Learn Enhanced Reasoning Skills from Medical  Textbooks(https://arxiv.org/abs/2404.00376)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notably, it surpassed the passing threshold of the United States Medical Licensing Examination (USMLE) for the first time for a 7B-parameter model. Additionally, our system offered more detailed free-form responses to clinical queries compared to existing 7B and 13B models, approaching the performance level of GPT-3.5. This significantly narrows the performance gap with large LMs, showcasing its effectiveness in addressing complex medical challenges.</li>
<li><strong>摘要：</strong>虽然商业大语言模型（LM）的最新进展在医疗任务中显示出了有希望的结果，但其闭源性质带来了严重的隐私和安全问题，阻碍了它们在医疗领域的广泛使用。尽管努力创建开源模型，但其有限的参数通常会导致解决复杂医疗问题所需的多步推理能力不足。为了解决这个问题，我们推出了 Meerkat-7B，这是一种拥有 70 亿个参数的新型医疗人工智能系统。 Meerkat-7B 使用我们的新合成数据集进行训练，该数据集包含源自 18 本医学教科书的高质量思维链推理路径以及各种指令跟踪数据集。我们的系统在七个医疗基准上取得了显着的准确性，超过了 GPT-3.5 13.1%，并且分别超过了之前最好的 7B 模型（如 MediTron-7B 和 BioMistral-7B）13.4% 和 9.8%。值得注意的是，它的 7B 参数模型首次超过了美国医师执照考试（USMLE）的通过门槛。此外，与现有的 7B 和 13B 模型相比，我们的系统对临床查询提供了更详细的自由格式响应，接近 GPT-3.5 的性能水平。这显着缩小了与大型 LM 的性能差距，展示了其在解决复杂医疗挑战方面的有效性。</li>
</ul>

<h3>Title: Aurora-M: The First Open Source Multilingual Language Model Red-teamed  according to the U.S. Executive Order</h3>
<ul>
<li><strong>Authors: </strong>Taishi Nakamura, Mayank Mishra, Simone Tedeschi, Yekun Chai, Jason T Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao, Adalberto Junior, Alpay Ariyak, Aleksandr Drozd, Jordan Clive, Kshitij Gupta, Liangyu Chen, Qi Sun, Ken Tsui, Noah Persaud, Nour Fahmy, Tianlong Chen, Mohit Bansal, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Huu Nguyen, Sampo Pyysalo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00399">https://arxiv.org/abs/2404.00399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00399">https://arxiv.org/pdf/2404.00399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00399]] Aurora-M: The First Open Source Multilingual Language Model Red-teamed  according to the U.S. Executive Order(https://arxiv.org/abs/2404.00399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Aurora-M is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations. To promote responsible open-source LLM development, Aurora-M and its variants are released at https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 .</li>
<li><strong>摘要：</strong>预训练的语言模型支撑着多种人工智能应用程序，但其训练的高计算成本限制了可访问性。 BLOOM 和 StarCoder 等举措旨在使预训练模型的访问民主化，以促进协作社区开发。然而，此类现有模型面临着挑战：多语言能力有限、持续预训练会导致灾难性遗忘，而从头开始预训练的计算成本很高，并且需要遵守人工智能安全和开发法律。本文介绍了 Aurora-M，这是一种 15B 参数多语言开源模型，经过英语、芬兰语、印地语、日语、越南语和代码训练。通过 StarCoderPlus 对 4350 亿个额外令牌的持续预训练，Aurora-M 的训练令牌总数超过了 2 万亿个令牌。它是第一个根据人工审查的安全指令进行微调的开源多语言模型，因此其开发不仅符合传统的红队考虑因素，而且符合拜登-哈里斯安全行政命令中阐述的具体问题，安全、值得信赖的人工智能开发和使用。 Aurora-M 在各种任务和语言中经过了严格的评估，展示了针对灾难性遗忘的稳健性，并且在多语言环境中（特别是在安全评估方面）优于替代方案。为了促进负责任的开源 LLM 开发，Aurora-M 及其变体在 https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 上发布。</li>
</ul>

<h3>Title: UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion  Cause</h3>
<ul>
<li><strong>Authors: </strong>Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Hasti Seifi, Jiayuan Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00403">https://arxiv.org/abs/2404.00403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00403">https://arxiv.org/pdf/2404.00403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00403]] UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion  Cause(https://arxiv.org/abs/2404.00403)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Multimodal emotion recognition in conversation (MERC) and multimodal emotion-cause pair extraction (MECPE) has recently garnered significant attention. Emotions are the expression of affect or feelings; responses to specific events, thoughts, or situations are known as emotion causes. Both are like two sides of a coin, collectively describing human behaviors and intents. However, most existing works treat MERC and MECPE as separate tasks, which may result in potential challenges in integrating emotion and cause in real-world applications. In this paper, we propose a Unified Multimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC) to explore the causality and complementarity between emotion and emotion cause. Concretely, UniMEEC reformulates the MERC and MECPE tasks as two mask prediction problems, enhancing the interaction between emotion and cause. Meanwhile, UniMEEC shares the prompt learning among modalities for probing modality-specific knowledge from the Pre-trained model. Furthermore, we propose a task-specific hierarchical context aggregation to control the information flow to the task. Experiment results on four public benchmark datasets verify the model performance on MERC and MECPE tasks and achieve consistent improvements compared with state-of-the-art methods.</li>
<li><strong>摘要：</strong>对话中的多模态情感识别（MERC）和多模态情感原因对提取（MECPE）最近引起了人们的广泛关注。情绪是情感或感受的表达；对特定事件、想法或情况的反应被称为情绪原因。两者就像硬币的两面，共同描述人类的行为和意图。然而，大多数现有工作将 MERC 和 MECPE 视为单独的任务，这可能会导致在现实应用中整合情感和原因的潜在挑战。在本文中，我们提出了统一多模态情感识别和情感原因分析框架（UniMEEC）来探索情感与情感原因之间的因果关系和互补性。具体来说，UniMEEC 将 MERC 和 MECPE 任务重新表述为两个掩模预测问题，增强了情感与原因之间的互动。同时，UniMEEC 分享了各种模式之间的即时学习，用于从预训练模型中探索特定模式的知识。此外，我们提出了一种特定于任务的分层上下文聚合来控制流向任务的信息流。四个公共基准数据集的实验结果验证了模型在 MERC 和 MECPE 任务上的性能，并与最先进的方法相比取得了一致的改进。</li>
</ul>

<h3>Title: CoDa: Constrained Generation based Data Augmentation for Low-Resource  NLP</h3>
<ul>
<li><strong>Authors: </strong>Chandra Kiran Reddy Evuru, Sreyan Ghosh, Sonal Kumar, Ramaneswaran S, Utkarsh Tyagi, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00415">https://arxiv.org/abs/2404.00415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00415">https://arxiv.org/pdf/2404.00415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00415]] CoDa: Constrained Generation based Data Augmentation for Low-Resource  NLP(https://arxiv.org/abs/2404.00415)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We present CoDa (Constrained Generation based Data Augmentation), a controllable, effective, and training-free data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control over the augmentation generation process, thereby also allowing easy adaptation to several domains. We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3 low-resource settings. CoDa outperforms all our baselines, qualitatively and quantitatively, with improvements of 0.12%-7.19%. Code is available here: https://github.com/Sreyan88/CoDa</li>
<li><strong>摘要：</strong>我们提出了 CoDa（基于约束生成的数据增强），这是一种用于低资源（数据稀缺）NLP 的可控、有效且免训练的数据增强技术。我们的方法基于提示现成的遵循大型语言模型（LLM）指令来生成满足一组约束的文本。准确地说，我们从低资源数据集中的每个实例中提取一组简单的约束，并将它们用语言表达出来，以促使法学硕士生成新颖且多样化的训练实例。我们的研究结果表明，遵循下游数据集中简单约束的合成数据可以作为高效的增强，而 CoDa 可以实现这一点，而无需复杂的解码时间受限生成技术或使用复杂算法进行微调，最终使模型偏向于小数训练实例数。此外，CoDa 是第一个为用户提供对增强生成过程的显式控制的框架，从而还可以轻松适应多个领域。我们在涵盖 3 个任务和 3 个低资源设置的 11 个数据集上展示了 CoDa 的有效性。 CoDa 在质量和数量上都优于我们的所有基线，提高了 0.12%-7.19%。代码可在此处获取：https://github.com/Sreyan88/CoDa</li>
</ul>

<h3>Title: Planning and Editing What You Retrieve for Enhanced Tool Learning</h3>
<ul>
<li><strong>Authors: </strong>Tenghao Huang, Dongwon Jung, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00450">https://arxiv.org/abs/2404.00450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00450">https://arxiv.org/pdf/2404.00450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00450]] Planning and Editing What You Retrieve for Enhanced Tool Learning(https://arxiv.org/abs/2404.00450)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&R)'' and ``Edit-and-Ground (E\&G)'' paradigms. The P\&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall and NDCG in tool retrieval tasks, significantly surpassing current state-of-the-art models.</li>
<li><strong>摘要：</strong>最近在将外部工具与大型语言模型 (LLM) 集成方面取得的进展开辟了新的领域，在数学推理、代码生成器和智能助手方面都有应用。然而，现有的方法依赖于简单的一次性检索策略，无法有效、准确地筛选相关工具。本文介绍了一种新颖的 \modelname (\modelmeaning) 方法，包括“计划和检索（P\&R）”和“编辑和基础（E\&G）”范式。 P\&R 范例由用于筛选相关工具的神经检索模块和基于 LLM 的查询规划器组成，该查询规划器将复杂的查询分解为可操作的任务，从而提高工具利用的有效性。 E\&G 范式利用 LLM 根据用户场景丰富工具描述，弥合用户查询和工具功能之间的差距。实验结果表明，这些范式显着提高了工具检索任务中的召回率和 NDCG，显着超越了当前最先进的模型。</li>
</ul>

<h3>Title: MetaIE: Distilling a Meta Model from LLM for All Kinds of Information  Extraction Tasks</h3>
<ul>
<li><strong>Authors: </strong>Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00457">https://arxiv.org/abs/2404.00457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00457">https://arxiv.org/pdf/2404.00457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00457]] MetaIE: Distilling a Meta Model from LLM for All Kinds of Information  Extraction Tasks(https://arxiv.org/abs/2404.00457)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract "important information", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of "important information". We evaluate the meta-model under the few-shot adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.</li>
<li><strong>摘要：</strong>信息提取 (IE) 是自然语言处理的一个基本领域，其中提示大型语言模型 (LLM)，即使有上下文示例，也无法击败在非常小的 IE 数据集上调整的小型 LM。我们观察到 IE 任务，例如命名实体识别和关系提取，都专注于提取重要信息，这些信息可以形式化为标签到跨度匹配。在本文中，我们提出了一种新颖的框架MetaIE，通过学习提取“重要信息”（即IE的元理解）来构建小型LM作为元模型，从而使该元模型可以适应所有类型的IE 有效且高效地执行任务。具体来说，MetaIE 按照标签到跨度方案通过 LLM 的符号蒸馏获得小型 LM。我们通过从语言模型预训练数据集（例如我们实现中的 OpenWebText）中采样句子来构建蒸馏数据集，并提示 LLM 识别“重要信息”的类型跨度。我们在少样本适应设置下评估元模型。来自 6 个 IE 任务的 13 个数据集的广泛结果证实，MetaIE 可以为 IE 数据集上的小样本调整提供更好的起点，并且优于其他元模型（1）普通语言模型预训练，（2）多 IE-使用人工注释进行任务预训练，以及 (3) 来自 LLM 的单 IE 任务符号蒸馏。此外，我们还提供了 MetaIE 的全面分析，例如蒸馏数据集的大小、元模型架构和元模型的大小。</li>
</ul>

<h3>Title: NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid Karlinsky, Assaf Arbelle</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00459">https://arxiv.org/abs/2404.00459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00459">https://arxiv.org/pdf/2404.00459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00459]] NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning(https://arxiv.org/abs/2404.00459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of "42", we suggest using "{2:42}" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.</li>
<li><strong>摘要：</strong>语言模型在处理数字数据和执行算术运算方面遇到了困难。我们假设这种限制部分归因于非直观的文本数字表示。当因果语言模型读取或生成数字时，在处理整个数字之前，它不知道其位置值（例如千与百）。为了解决这个问题，我们建议对数字的表示方式进行简单的调整，即在每个数字之前包含位数。例如，我们建议使用“{2:42}”作为新格式，而不是“42”。这种方法，我们称之为数字逻辑，通过充当思想链（CoT），在数字生成方面提供了额外的优势。通过要求模型首先考虑位数，它增强了生成实际数字之前的推理过程。我们使用算术任务来演示数字逻辑格式化的有效性。我们进一步证明了 NumeroLogic 对一般自然语言建模的适用性，提高了 MMLU 基准中的语言理解性能。</li>
</ul>

<h3>Title: Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model  unless you have access to GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Aryo Pradipta Gema, Giwon Hong, Pasquale Minervini, Luke Daines, Beatrice Alex</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00484">https://arxiv.org/abs/2404.00484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00484">https://arxiv.org/pdf/2404.00484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00484]] Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model  unless you have access to GPT-4(https://arxiv.org/abs/2404.00484)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies, including Chain-of-Thought, In-Context Learning, and Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the consistency of LLMs by merging adapters that were fine-tuned separately using triplet and language modelling objectives. We found that merging the two PEFT adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs. However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks joint-first in the competition with 0.8328. Finally, our contamination analysis with GPT-4 indicates that there was no test data leakage.</li>
<li><strong>摘要：</strong>NLI4CT 任务评估自然语言推理系统，以预测假设是否包含或与临床试验报告中的证据相矛盾。在本研究中，我们使用多种策略评估各种大型语言模型 (LLM)，包括思想链、上下文学习和参数高效微调 (PEFT)。我们提出了一种 PEFT 方法，通过合并使用三元组和语言建模目标分别微调的适配器来提高 LLM 的一致性。我们发现合并两个 PEFT 适配器可以提高 LLM 的 F1 分数 (+0.0346) 和一致性 (+0.152)。然而，就忠实度和一致性而言，我们的新方法并没有产生比 GPT-4 更准确的结果。对三个指标进行平均后，GPT-4 以 0.8328 在竞赛中并列第一。最后，我们对 GPT-4 的污染分析表明没有测试数据泄漏。</li>
</ul>

<h3>Title: Dialectical Alignment: Resolving the Tension of 3H and Security Threats  of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00486">https://arxiv.org/abs/2404.00486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00486">https://arxiv.org/pdf/2404.00486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00486]] Dialectical Alignment: Resolving the Tension of 3H and Security Threats  of LLMs(https://arxiv.org/abs/2404.00486)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional prompt engineering or prior declaration of ``you may be attacked`` to the LLMs' context window.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的兴起，确保它们体现有益、诚实和无害 (3H) 的原则（即人类对齐）变得至关重要。虽然 RLHF、DPO 等现有的对齐方法可以有效地微调 LLM 以匹配偏好数据集中的偏好，但它们通常会导致 LLM 高度接受人类输入和外部证据，即使这些信息被毒化。当外部证据与其参数记忆发生冲突时，这导致法学硕士倾向于成为适应性变色龙。这加剧了LLM受到外部中毒数据攻击的风险，从而对检索增强生成（RAG）等LLM系统应用带来重大安全风险。为了应对这一挑战，我们提出了一个新颖的框架：辩证对齐（DA），它（1）利用人工智能反馈来确定法学硕士的最佳策略，以在上下文窗口（即上下文窗口中的不同外部证据）中导航上下文间冲突和上下文记忆冲突。 ，不同比例的中毒事实上下文）； （2）根据上述AI反馈和策略构建SFT数据集以及偏好数据集； (3)使用上述数据集进行LLM对齐，以防御中毒上下文攻击，同时保留上下文知识编辑的有效性。我们的实验表明，辩证对齐模型将中毒数据攻击防御能力提高了 20，并且不需要任何额外的提示工程或事先向 LLM 上下文窗口声明“您可能会受到攻击”。</li>
</ul>

<h3>Title: Noise-Aware Training of Layout-Aware Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ritesh Sarkhel, Xiaoqi Ren, Lauro Beltrao Costa, Guolong Su, Vincent Perot, Yanan Xie, Emmanouil Koukoumidis, Arnab Nandi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00488">https://arxiv.org/abs/2404.00488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00488">https://arxiv.org/pdf/2404.00488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00488]] Noise-Aware Training of Layout-Aware Language Models(https://arxiv.org/abs/2404.00488)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degradation in the model's quality due to noisy, weakly labeled samples, NAT estimates the confidence of each training sample and incorporates it as uncertainty measure during training. We train multiple state-of-the-art extractor models using NAT. Experiments on a number of publicly available and in-house datasets show that NAT-trained models are not only robust in performance -- it outperforms a transfer-learning baseline by up to 6% in terms of macro-F1 score, but it is also more label-efficient -- it reduces the amount of human-effort required to obtain comparable performance by up to 73%.</li>
<li><strong>摘要：</strong>视觉丰富的文档 (VRD) 利用视觉特征和语言线索来传播信息。训练从文档中识别命名实体的自定义提取器需要大量以文本和视觉方式注释的目标文档类型实例。这是企业场景中一个昂贵的瓶颈，我们希望以可扩展的方式为数千种不同的文档类型训练自定义提取器。在目标文档类型的未标记实例上预训练提取器模型，然后对人工标记实例进行微调步骤在这些场景中不起作用，因为它超出了为提取器分配的最大允许训练时间。我们通过在本文中提出噪声感知训练方法或 NAT 来解决这种情况。 NAT 不是获取昂贵的人工标记文档，而是利用弱标记文档以可扩展的方式训练提取器。为了避免由于噪声、弱标记样本而导致模型质量下降，NAT 会估计每个训练样本的置信度，并将其作为训练期间的不确定性度量。我们使用 NAT 训练多个最先进的提取器模型。对许多公开可用和内部数据集的实验表明，经过 NAT 训练的模型不仅性能稳健——在宏观 F1 分数方面比迁移学习基线高出 6%，而且标签效率更高——它可以将获得可比性能所需的人力减少多达 73%。</li>
</ul>

<h3>Title: PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt  Compression</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00489">https://arxiv.org/abs/2404.00489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00489">https://arxiv.org/pdf/2404.00489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00489]] PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt  Compression(https://arxiv.org/abs/2404.00489)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform. Experimental evaluation using benchmark datasets shows that prompts compressed by PROMPT-SAW are not only better in terms of readability, but they also outperform the best-performing baseline models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic settings while compressing the original prompt text by 33.0 and 56.7.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在多种不同的自然语言处理任务中表现出了卓越的能力。虽然提示是 LLM 推理的重要工具，但我们观察到，过长的提示会带来巨大的成本。现有的压缩冗长提示的尝试导致压缩提示的可读性和可解释性方面的结果不合标准，从而对提示实用性产生不利影响。为了解决这个问题，我们提出了 PROMPT-SAW：通过关系感知图进行提示压缩，这是一种针对与任务无关和任务感知的提示进行提示压缩的有效策略。 PROMPT-SAW 使用提示的文本信息构建图表，然后提取图表中的关键信息元素以得出压缩的提示。我们还提出了 GSM8K-AUG，即现有 GSM8k 基准测试的扩展版本，用于与任务无关的提示，以提供全面的评估平​​台。使用基准数据集的实验评估表明，PROMPT-SAW 压缩的提示不仅在可读性方面更好，而且在压缩时的任务感知和任务无关设置方面，它们的性能分别比性能最佳的基线模型高出 14.3 和 13.7原始提示文本为 33.0 和 56.7。</li>
</ul>

<h3>Title: Multi-hop Question Answering under Temporal Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00492">https://arxiv.org/abs/2404.00492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00492">https://arxiv.org/pdf/2404.00492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00492]] Multi-hop Question Answering under Temporal Knowledge Editing(https://arxiv.org/abs/2404.00492)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal scopes.</li>
<li><strong>摘要：</strong>知识编辑（KE）下的多跳问答（MQA）在大语言模型时代引起了极大的关注。然而，现有的 KE 下的 MQA 模型在处理包含显式时间上下文的问题时表现不佳。为了解决这个限制，我们提出了一个新的框架，即临时知识增强多跳问答（TEMPLE-MQA）。与以前的方法不同，TEMPLE-MQA 首先构建时间感知图（TAG）以结构化方式存储编辑知识。然后，通过我们提出的推理路径、结构检索和联合推理阶段，TEMPLE-MQA 有效地识别问题查询中的时间上下文。基准数据集上的实验表明 TEMPLE-MQA 显着优于基准模型。此外，我们还贡献了一个新的数据集，即 TKEMQA，它作为专门为具有时间范围的 MQA 量身定制的首个基准。</li>
</ul>

<h3>Title: Configurable Safety Tuning of Language Models with Synthetic Preference  Data</h3>
<ul>
<li><strong>Authors: </strong>Victor Gallego</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00495">https://arxiv.org/abs/2404.00495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00495">https://arxiv.org/pdf/2404.00495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00495]] Configurable Safety Tuning of Language Models with Synthetic Preference  Data(https://arxiv.org/abs/2404.00495)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning</li>
<li><strong>摘要：</strong>最先进的语言模型微调技术，例如直接偏好优化 (DPO)，通过将预定义行为硬编码到模型中来限制用户控制。为了解决这个问题，我们提出了一种新方法，可配置安全调整（CST），它使用合成偏好数据增强 DPO，以促进 LLM 在推理时的灵活安全配置。 CST 通过引入指定安全配置的系统提示来克服普通 DPO 的限制，使 LLM 部署者能够根据自己的需要禁用/启用安全首选项，只需更改系统提示即可。我们的实验评估表明，CST 成功管理了不同的安全配置并保留了 LLM 的原始功能，这表明它是一种稳健的可配置部署方法。数据和模型可在 https://github.com/vicgalle/configurable-safety-tuning 获取</li>
</ul>

<h3>Title: MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in  Conversations with Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zebang Cheng, Fuqiang Niu, Yuxiang Lin, Zhi-Qi Cheng, Bowen Zhang, Xiaojiang Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00511">https://arxiv.org/abs/2404.00511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00511">https://arxiv.org/pdf/2404.00511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00511]] MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in  Conversations with Multimodal Language Models(https://arxiv.org/abs/2404.00511)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git</li>
<li><strong>摘要：</strong>本文介绍了我们向 SemEval 2024 任务 3 的子任务 2 提交的获奖作品，该任务涉及对话中的多模态情感原因分析。我们提出了一种新颖的多模态情感识别和多模态情感原因提取（MER-MCE）框架，该框架使用专门的情感编码器集成了文本、音频和视觉模态。我们的方法通过利用特定于模态的功能来增强情感理解和因果推理，从而使自己与表现最佳的团队区分开来。实验评估证明了我们多模式方法的优势，我们提交的作品获得了 0.3435 的竞争加权 F1 分数，排名第三，仅落后第一队 0.0339，落后第二队 0.0025。项目：https://github.com/MIPS-COLT/MER-MCE.git</li>
</ul>

<h3>Title: Comparing Bad Apples to Good Oranges: Aligning Large Language Models via  Joint Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00530">https://arxiv.org/abs/2404.00530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00530">https://arxiv.org/pdf/2404.00530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00530]] Comparing Bad Apples to Good Oranges: Aligning Large Language Models via  Joint Preference Optimization(https://arxiv.org/abs/2404.00530)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint instruction-response preference data using DOVE outperforms the LLM trained with DPO by 5.2% and 3.3% win-rate for the summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available at https://github.com/Hritikbansal/dove.</li>
<li><strong>摘要：</strong>对齐大型语言模型 (LLM) 的常用技术依赖于通过比较固定上下文条件下的多个代来获取人类偏好。当各代被放置在相同的上下文中时，这仅利用成对比较。然而，这种有条件的排名往往无法捕捉人类偏好的复杂和多维方面。在这项工作中，我们重新审视了偏好获取的传统范式，并提出了一个基于联合诱导指令-响应对偏好的新轴。虽然先验偏好优化是为条件排名协议（例如 DPO）设计的，但我们提出的偏好获取协议引入了 DOVE，这是一种新的偏好优化目标，它增加了所选指令-响应对相对于被拒绝的指令-响应对的联合概率。有趣的是，我们发现使用 DOVE 联合指令-响应偏好数据训练的法学硕士在摘要和开放式对话数据集上的胜率分别比使用 DPO 训练的法学硕士高出 5.2% 和 3.3%。我们的研究结果表明，通过利用更广泛的人类偏好启发，对指令和响应对的联合偏好可以显着增强法学硕士的一致性。数据和代码可在 https://github.com/Hritikbansal/dove 获取。</li>
</ul>

<h3>Title: DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented  Dialogue Representations</h3>
<ul>
<li><strong>Authors: </strong>Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, Weiran Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00557">https://arxiv.org/abs/2404.00557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00557">https://arxiv.org/pdf/2404.00557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00557]] DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented  Dialogue Representations(https://arxiv.org/abs/2404.00557)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.</li>
<li><strong>摘要：</strong>在一般文本上预训练的语言模型在不同领域取得了令人印象深刻的成果。然而，与一般文本相比，任务导向对话（TOD）的独特语言特征限制了现有语言模型的实际用途。当前面向任务的对话预训练方法忽略了对话的一对多属性，在给定相同的对话上下文的情况下，多个响应可能是适当的。在本文中，我们提出了一种名为 DivTOD 的新型对话预训练模型，该模型与法学硕士合作学习各种面向任务的对话表示。 DivTOD 指导法学硕士将多样化的知识转移到较小的模型中，同时删除与面向任务的对话相矛盾的领域知识。实验表明，我们的模型在各种下游对话任务上优于强大的 TOD 基线，并学习了面向任务的对话的内在多样性。</li>
</ul>

<h3>Title: ParaICL: Towards Robust Parallel In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00570">https://arxiv.org/abs/2404.00570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00570">https://arxiv.org/pdf/2404.00570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00570]] ParaICL: Towards Robust Parallel In-Context Learning(https://arxiv.org/abs/2404.00570)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已成为自然语言处理（NLP）领域的常态，以其卓越的能力在少样本上下文学习（ICL）方面表现出色。尽管如此，ICL 的成功在很大程度上取决于少量演示示例的选择，这使得选择过程变得越来越重要。现有方法已深入研究优化这些示例的数量和语义相似性，以提高 ICL 性能。然而，我们的初步实验表明 ICL 的有效性受到输入上下文长度的限制。此外，少量演示示例的不同组合可以显着提高不同测试样本的准确性。为了解决这个问题，我们提出了一种名为并行上下文学习（ParaICL）的新方法，该方法有效地利用所有演示示例，而不会超过可管理的输入上下文长度。 ParaICL 采用并行批处理，根据演示中的问题与测试问题的语义相似性将演示示例分配到不同的批次中。然后，它计算每个批次的标准化批次语义分数。应用受自适应合理性约束的加权平均语义目标来选择最合适的标记。通过大量实验，我们验证了 ParaICL 的有效性并进行消融研究以强调其设计原理。我们进一步证明 ParaICL 可以与现有方法无缝集成。</li>
</ul>

<h3>Title: EvoCodeBench: An Evolving Code Generation Benchmark Aligned with  Real-World Code Repositories</h3>
<ul>
<li><strong>Authors: </strong>Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, Zhi Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00599">https://arxiv.org/abs/2404.00599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00599">https://arxiv.org/pdf/2404.00599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00599]] EvoCodeBench: An Evolving Code Generation Benchmark Aligned with  Real-World Code Repositories(https://arxiv.org/abs/2404.00599)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.</li>
<li><strong>摘要：</strong>如何在代码生成中评估大型语言模型（LLM）是一个悬而未决的问题。现有的基准测试与现实世界的代码存储库的一致性较差，不足以评估法学硕士的编码能力。为了解决上述问题，本文提出了一种新的基准测试——EvoCodeBench，它具有三个主要优点。 (1) EvoCodeBench 在多个维度上与现实世界的存储库保持一致，例如代码分布和依赖分布。 (2) EvoCodeBench 提供全面的注释（例如需求、参考代码和参考依赖项）和强大的评估指标（例如 Pass@k 和 Recall@k）。 (3) EvoCodeBench 是一个不断发展的基准，以避免数据泄漏。我们构建了一个自动管道来从最新的存储库更新 EvoCodeBench。我们发布了第一个版本 - EvoCodeBench-2403，包含来自 25 个现实存储库的 275 个样本。基于 EvoCodeBench，我们提出存储库级代码生成并评估 10 个流行的 LLM（例如 gpt-4、gpt-3.5、DeepSeek Coder、StarCoder 2、CodeLLaMa、Gemma 和 Qwen 1.5）。我们的实验揭示了这些法学硕士在现实世界存储库中的编码能力。例如，在我们的实验中，仅 gpt-4 的最高 Pass@1 为 20.73%。我们还分析了失败的案例并总结了 EvoCodeBench 中现有 LLM 的缺点。我们发布了 EvoCodeBench、所有提示以及法学硕士的完成情况，以供进一步的社区分析。</li>
</ul>

<h3>Title: Extensive Self-Contrast Enables Feedback-Free Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Xixuan Song, Yuxiao Dong, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00604">https://arxiv.org/abs/2404.00604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00604">https://arxiv.org/pdf/2404.00604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00604]] Extensive Self-Contrast Enables Feedback-Free Language Model Alignment(https://arxiv.org/abs/2404.00604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform SFT and standard DPO training by large margins. And as the number of self-generated negatives increases, the performance of Self-Contrast continues to grow. Code and data are available at https://github.com/THUDM/Self-Contrast.</li>
<li><strong>摘要：</strong>来自人类反馈的强化学习（RLHF）一直是最近大型语言模型（LLM）对齐的核心技术。然而，它严重依赖昂贵的人力或法学硕士作为法官的偏好反馈可能会阻碍其更广泛的应用。在这项工作中，我们引入了 Self-Contrast，这是一种通过利用广泛的自我生成的负数来实现的无反馈大语言模型对齐方法。仅通过监督微调 (SFT) 目标，Self-Contrast 利用法学硕士本身生成大量多样化的候选者，并利用预先训练的嵌入模型根据文本相似性过滤多个负数。从理论上讲，我们说明在这种情况下，仅缩放负面响应仍然可以有效地近似具有更平衡的正面和负面偏好注释的情况。我们在三个数据集上进行的直接偏好优化 (DPO) 实验表明，Self-Contrast 始终能够大幅优于 SFT 和标准 DPO 训练。随着自我生成底片数量的增加，自我对比的性能不断增长。代码和数据可在 https://github.com/THUDM/Self-Contrast 获取。</li>
</ul>

<h3>Title: RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00610">https://arxiv.org/abs/2404.00610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00610">https://arxiv.org/pdf/2404.00610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00610]] RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation(https://arxiv.org/abs/2404.00610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出卓越的功能，但容易产生不准确或幻觉的响应。这种限制源于它们对大量预训练数据集的依赖，这使得它们在未见过的场景中容易出错。为了应对这些挑战，检索增强生成（RAG）通过将外部相关文档纳入响应生成过程来解决这个问题，从而利用非参数知识以及法学硕士的情境学习能力。然而，现有的 RAG 实现主要关注上下文检索的初始输入，忽略了模糊或复杂查询的细微差别，这些查询需要进一步澄清或分解才能获得准确的响应。为此，我们在本文中建议学习 Refine Query for Retrieval Augmented Generation (RQ-RAG)，努力通过配备显式重写、分解和消歧功能来增强模型。我们的实验结果表明，当应用于 7B Llama2 模型时，我们的方法在三个单跳 QA 数据集上平均超越了之前最先进的 (SOTA) 1.9\%，并且还展示了在处理复杂的多跳 QA 数据集。我们的代码可在 https://github.com/chanchimin/RQ-RAG 获取。</li>
</ul>

<h3>Title: Learning to Plan for Language Modeling from Unlabeled Data</h3>
<ul>
<li><strong>Authors: </strong>Nathan Cornille, Marie-Francine Moens, Florian Mai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00614">https://arxiv.org/abs/2404.00614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00614">https://arxiv.org/pdf/2404.00614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00614]] Learning to Plan for Language Modeling from Unlabeled Data(https://arxiv.org/abs/2404.00614)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.</li>
<li><strong>摘要：</strong>通过训练预测未标记语料库中的下一个标记，大型语言模型可以学习在没有任何标记数据的情况下执行许多任务。然而，他们的下一个令牌预测目标可以说限制了他们在需要规划的场景中的表现，例如撰写一篇连贯的文章。在本文中，我们训练了一个通过自我监督学习目标来规划未来写作过程的模块。通过以生成的潜在计划为条件，我们的模型以无监督的方式将成功的语言模型公式扩展到更抽象的计划。根据经验，我们证明我们的方法总体上提高了语言建模性能，特别是在文本结构方面。由于我们的框架使用不受监督且位于语言模型外部的规划器模块，因此可以大规模训练新的规划器模块并轻松与社区共享。</li>
</ul>

<h3>Title: Against The Achilles' Heel: A Survey on Red Teaming for Generative  Models</h3>
<ul>
<li><strong>Authors: </strong>Lizhi Lin, Honglin Mu, Zenan Zhai, Minghan Wang, Yuxia Wang, Renxi Wang, Junjie Gao, Yixuan Zhang, Wanxiang Che, Timothy Baldwin, Xudong Han, Haonan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00629">https://arxiv.org/abs/2404.00629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00629">https://arxiv.org/pdf/2404.00629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00629]] Against The Achilles' Heel: A Survey on Red Teaming for Generative  Models(https://arxiv.org/abs/2404.00629)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new areas of research.</li>
<li><strong>摘要：</strong>生成模型迅速普及并被集成到日常应用中，随着各种漏洞的暴露，引发了人们对其安全问题的担忧。面对这个问题，红队领域正在经历快速增长，这凸显了需要一个覆盖整个管道并解决社区新兴主题的综合组织。我们进行了广泛的调查，审查了 120 多篇论文，引入了基于语言模型固有功能的细粒度攻击策略的分类。此外，我们还开发了统一各种自动红队方法的搜索器框架。此外，我们的调查涵盖了新的领域，包括多模式攻击和防御、多语言模型的风险、无害查询的过度杀伤以及下游应用程序的安全性。我们希望这项调查能够提供该领域的系统视角并开辟新的研究领域。</li>
</ul>

<h3>Title: WavLLM: Towards Robust and Adaptive Speech Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00656">https://arxiv.org/abs/2404.00656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00656">https://arxiv.org/pdf/2404.00656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00656]] WavLLM: Towards Robust and Adaptive Speech Large Language Model(https://arxiv.org/abs/2404.00656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{aka.ms/wavllm}.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的最新进展彻底改变了自然语言处理领域，逐渐将其范围扩大到多模态感知和生成。然而，将听力能力有效地整合到法学硕士中提出了重大挑战，特别是在跨不同背景进行概括和执行复杂的听觉任务方面。在这项工作中，我们介绍了 WavLLM，一种具有双编码器的鲁棒自适应语音大语言模型，以及通过两阶段课程学习方法优化的提示感知 LoRA 权重适配器。利用双编码器，我们解耦不同类型的语音信息，利用 Whisper 编码器处理语音的语义内容，并利用 WavLM 编码器捕获说话者身份的独特特征。在课程学习框架内，WavLLM 首先通过优化混合基本单一任务来构建其基础能力，然后针对更复杂的任务（例如基本任务的组合）进行高级多任务训练。为了增强对不同任务和指令的灵活性和依从性，在第二个高级多任务训练阶段引入了提示感知 LoRA 重量适配器。我们在通用语音基准（包括 ASR、ST、SV、ER 等任务）上验证了所提出的模型，并将其应用于专门的数据集，例如用于 SQA 的高考英语听力理解集和语音思想链（CoT）评估集。实验表明，所提出的模型在相同模型大小的一系列语音任务中实现了最先进的性能，在使用 CoT 方法执行复杂任务时表现出强大的泛化能力。此外，我们的模型无需专门训练即可成功完成高考任务。代码、模型、音频和高考评估集可以在 \url{aka.ms/wavllm} 访问。</li>
</ul>

<h3>Title: CoUDA: Coherence Evaluation via Unified Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Dawei Zhu, Wenhao Wu, Yifan Song, Fangwei Zhu, Ziqiang Cao, Sujian Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00681">https://arxiv.org/abs/2404.00681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00681">https://arxiv.org/pdf/2404.00681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00681]] CoUDA: Coherence Evaluation via Unified Data Augmentation(https://arxiv.org/abs/2404.00681)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse. Extensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.</li>
<li><strong>摘要：</strong>连贯性评估旨在评估话语的组织和结构，即使在大型语言模型时代，这仍然具有挑战性。由于注释数据的稀缺，数据增强通常用于训练一致性评估模型。然而，之前对该任务的增强主要依赖于启发式规则，缺乏设计标准作为指导。在本文中，我们从语篇结构的语言学理论中汲取灵感，提出了一种名为 CoUDA 的数据增强框架。 CoUDA 将话语连贯性分解为全局和局部两个方面，并分别为这两个方面设计增强策略。特别是对于局部一致性，我们提出了一种用于构建增强样本的新颖生成策略，其中涉及对生成模型进行后预训练并应用两种控制机制来控制生成样本的难度。在推理过程中，CoUDA 还联合评估全局和局部方面，以综合评估话语的整体连贯性。一致性评估方面的大量实验表明，CoUDA 仅用 2.33 亿个参数，在逐点评分和成对排名任务中实现了最先进的性能，甚至超越了最近基于 GPT-3.5 和 GPT-4 的指标。</li>
</ul>

<h3>Title: How Much are LLMs Contaminated? A Comprehensive Survey and the  LLMSanitize Library</h3>
<ul>
<li><strong>Authors: </strong>Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00699">https://arxiv.org/abs/2404.00699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00699">https://arxiv.org/pdf/2404.00699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00699]] How Much are LLMs Contaminated? A Comprehensive Survey and the  LLMSanitize Library(https://arxiv.org/abs/2404.00699)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address contamination, or a clear consensus on prevention, mitigation and classification of contamination. In this paper, we survey all recent work on contamination with LLMs, and help the community track contamination levels of LLMs by releasing an open-source Python library named LLMSanitize implementing major contamination detection algorithms, which link is: https://github.com/ntunlp/LLMSanitize.</li>
<li><strong>摘要：</strong>近年来，随着大型语言模型（LLM）的兴起，新的机遇不断涌现，但也带来了新的挑战，污染很快变得至关重要。人工智能领域的商业应用和筹款已经达到了这样的规模，在流行的问答基准上提高几个百分点就可能转化为数千万美元，这给模型的完整性带来了巨大的压力。与此同时，跟踪法学硕士所看到的数据变得越来越困难；如果 GPT-4 和 Claude-3 等闭源模型不是不可能的话，就不会泄露有关训练集的任何信息。因此，污染成为一个关键问题：法学硕士的表现可能不再可靠，因为高性能可能至少部分归因于他们之前接触过的数据。这种限制危及了自然语言处理领域的整体进展，然而，仍然缺乏如何有效解决污染的方法，或者在污染的预防、缓解和分类方面缺乏明确的共识。在本文中，我们调查了最近所有关于 LLM 污染的工作，并通过发布一个名为 LLMSanitize 的开源 Python 库来帮助社区跟踪 LLM 的污染水平，该库实现了主要的污染检测算法，链接为：https://github.com /ntunlp/LLMSanitize。</li>
</ul>

<h3>Title: A Controlled Reevaluation of Coreference Resolution Models</h3>
<ul>
<li><strong>Authors: </strong>Ian Porada, Xiyuan Zou, Jackie Chi Kit Cheung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00727">https://arxiv.org/abs/2404.00727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00727">https://arxiv.org/pdf/2404.00727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00727]] A Controlled Reevaluation of Coreference Resolution Models(https://arxiv.org/abs/2404.00727)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of the increase in F1 score reported in the past five years.</li>
<li><strong>摘要：</strong>所有最先进的共指消解（CR）模型都涉及微调预训练的语言模型。由于缺乏标准化的实验设置，很难或不可能确定一种 CR 模型相对于另一种模型的优越性能是由于语言模型的选择还是其他因素（例如特定于任务的架构）造成的。为了解决这种歧义，我们系统地评估了五个 CR 模型并控制某些设计决策，包括每个模型使用的预训练语言模型。在控制语言模型大小时，基于编码器的 CR 模型在准确性和推理速度方面都优于最新的基于解码器的模型。令人惊讶的是，在基于编码器的 CR 模型中，更新的模型并不总是更准确，而且我们测试的最旧的 CR 模型将最好的推广到域外文本类型。我们的结论是，控制语言模型的选择会减少过去五年中报告的 F1 分数的大部分增长，但不是全部。</li>
</ul>

<h3>Title: Can Language Models Recognize Convincing Arguments?</h3>
<ul>
<li><strong>Authors: </strong>Paula Rescala, Manoel Horta Ribeiro, Tiancheng Hu, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00750">https://arxiv.org/abs/2404.00750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00750">https://arxiv.org/pdf/2404.00750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00750]] Can Language Models Recognize Convincing Arguments?(https://arxiv.org/abs/2404.00750)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus & Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with this paper contribute to the crucial ongoing effort of continuously evaluating and monitoring the rapidly evolving capabilities and potential impact of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 卓越且不断增强的功能引发了人们对其可能被滥用来创建个性化、令人信服的错误信息和宣传的担忧。为了在不直接参与人类实验的情况下深入了解法学硕士的说服能力，我们建议研究他们在检测令人信服的论点的相关任务中的表现。我们扩展了 Durmus & Cardie (2018) 的数据集，包含辩论、投票和用户特征，并提出了衡量法学硕士能力的任务：(1) 区分强论点和弱论点，(2) 根据信仰和人口特征预测立场，以及(3) 根据个人的特征来确定论点对个人的吸引力。我们表明，法学硕士在这些任务中的表现与人类相当，并且结合不同法学硕士的预测可以产生显着的性能提升，甚至超越人类的表现。本文发布的数据和代码有助于持续评估和监控法学硕士快速发展的能力和潜在影响的关键持续努力。</li>
</ul>

<h3>Title: From Robustness to Improved Generalization and Calibration in  Pre-trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Josip Jukić, Jan Šnajder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00758">https://arxiv.org/abs/2404.00758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00758">https://arxiv.org/pdf/2404.00758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00758]] From Robustness to Improved Generalization and Calibration in  Pre-trained Language Models(https://arxiv.org/abs/2404.00758)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperforming unregularized fine-tuning and other similar regularization methods.</li>
<li><strong>摘要：</strong>增强预训练语言模型 (PLM) 的泛化性和不确定性量化对于其有效性和可靠性至关重要。机器学习研究确立了稳健性对于提高泛化能力的重要性，在此基础上，我们研究了通过 Jacobian 和 Hessian 正则化实现的表示平滑度在增强 PLM 性能方面的作用。尽管此类正则化方法已被证明在计算机视觉中有效，但它们在自然语言处理 (NLP) 中的应用（其中 PLM 输入源自离散域）带来了独特的挑战。我们引入了一种新颖的两阶段正则化方法 JacHess，该方法可最小化 PLM 中间表示中雅可比矩阵和 Hessian 矩阵相对于其输入的范数。我们使用 GLUE 基准进行的评估表明，JacHess 显着改进了 PLM 中的域内泛化和校准，优于非正则化微调和其他类似的正则化方法。</li>
</ul>

<h3>Title: Extracting Social Determinants of Health from Pediatric Patient Notes  Using Large Language Models: Novel Corpus and Methods</h3>
<ul>
<li><strong>Authors: </strong>Yujuan Fu, Giridhar Kaushik Ramachandran, Nicholas J Dobbins, Namu Park, Michael Leu, Abby R. Rosenberg, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00826">https://arxiv.org/abs/2404.00826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00826">https://arxiv.org/pdf/2404.00826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00826]] Extracting Social Determinants of Health from Pediatric Patient Notes  Using Large Language Models: Novel Corpus and Methods(https://arxiv.org/abs/2404.00826)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1. Our proposed fine-tuning LLM-based extractors achieve high performance at 78.4 F1 for event arguments. In-context learning approaches with GPT-4 demonstrate promise for reliable SDoH extraction with limited annotated examples, with extraction performance at 82.3 F1 for event triggers.</li>
<li><strong>摘要：</strong>健康的社会决定因素 (SDoH) 在塑造健康结果方面发挥着关键作用，特别是对于干预措施可能产生长期影响的儿科人群。 SDoH 在电子健康记录 (EHR) 中经常被研究，它为不同的患者数据提供了丰富的存储库。在这项工作中，我们提出了一种新颖的注释语料库，即儿科社会历史注释语料库（PedSHAC），并使用大型语言模型（LLM）的微调和上下文学习方法来评估详细 SDoH 表示的自动提取。 PedSHAC 包含来自华盛顿大学 (UW) 医院系统内儿科患者的 1,260 份临床记录的带注释的社会历史部分。 PedSHAC 采用基于事件的注释方案，捕获了 10 个不同的健康决定因素，包括生活和经济稳定性、既往创伤、教育机会、药物使用史和心理健康，总体注释器一致性为 81.9 F1。我们提出的基于 LLM 的微调提取器在事件参数方面实现了 78.4 F1 的高性能。使用 GPT-4 的上下文学习方法证明了通过有限的注释示例进行可靠的 SDoH 提取的前景，事件触发器的提取性能为 82.3 F1。</li>
</ul>

<h3>Title: PID Control-Based Self-Healing to Improve the Robustness of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00828">https://arxiv.org/abs/2404.00828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00828">https://arxiv.org/pdf/2404.00828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00828]] PID Control-Based Self-Healing to Improve the Robustness of Large  Language Models(https://arxiv.org/abs/2404.00828)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states and future dynamical trends, respectively. We leverage the geometrical properties of the training data to design effective linear PID controllers. This approach reduces the computational cost to that of using just the P controller, instead of the full PID control. Further, we introduce an analytical method for approximating the optimal control solutions, enhancing the real-time inference capabilities of this controlled system. Moreover, we conduct a theoretical error analysis of the analytic solution in a simplified setting. The proposed PID control-based self-healing is a low cost framework that improves the robustness of pre-trained large language models, whether standard or robustly trained, against a wide range of perturbations. A detailed implementation can be found in:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.</li>
<li><strong>摘要：</strong>尽管深度神经网络在许多自然语言处理应用中都很有效，但最近的研究结果暴露了这些语言模型在引入微小扰动时的脆弱性。虽然在语义上人类无法区分，但这些扰动会显着降低训练有素的语言模型的性能，引发人们对在安全关键情况下部署它们的可靠性的担忧。在这项工作中，我们构建了一个计算高效的自我修复过程，以纠正在线推理过程中对输入数据施加扰动时出现的不良模型行为。这被表述为轨迹优化问题，其中使用 PID（比例积分微分）控制机制自动校正神经网络层的内部状态。 P 控制器的目标是即时状态调整，而 I 和 D 控制器分别考虑过去的状态和未来的动态趋势。我们利用训练数据的几何特性来设计有效的线性 PID 控制器。这种方法将计算成本降低到仅使用 P 控制器而不是完整 PID 控制的计算成本。此外，我们引入了一种近似最优控制解决方案的分析方法，增强了该受控系统的实时推理能力。此外，我们在简化的设置下对解析解进行了理论误差分析。所提出的基于 PID 控制的自我修复是一种低成本框架，可提高预训练大型语言模型（无论是标准模型还是经过鲁棒训练）针对各种扰动的鲁棒性。详细实现可以参见：https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models。</li>
</ul>

<h3>Title: Returning to the Start: Generating Narratives with Related Endpoints</h3>
<ul>
<li><strong>Authors: </strong>Anneliese Brei, Chao Zhao, Snigdha Chaturvedi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00829">https://arxiv.org/abs/2404.00829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00829">https://arxiv.org/pdf/2404.00829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00829]] Returning to the Start: Generating Narratives with Related Endpoints(https://arxiv.org/abs/2404.00829)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Human writers often bookend their writing with ending sentences that relate back to the beginning sentences in order to compose a satisfying narrative that "closes the loop." Motivated by this observation, we propose RENarGen, a controllable story-generation paradigm that generates narratives by ensuring the first and last sentences are related and then infilling the middle sentences. Our contributions include an initial exploration of how various methods of bookending from Narratology affect language modeling for stories. Automatic and human evaluations indicate RENarGen produces better stories with more narrative closure than current autoregressive models.</li>
<li><strong>摘要：</strong>人类作家经常用与开头句子相关的结尾句子来结束他们的写作，以便撰写一个令人满意的“闭合循环”的叙述。受这一观察的启发，我们提出了 RENarGen，这是一种可控的故事生成范式，通过确保第一个和最后一个句子相关，然后填充中间句子来生成叙述。我们的贡献包括初步探索叙事学的各种书尾方法如何影响故事的语言建模。自动和人工评估表明，与当前的自回归模型相比，RENarGen 能够生成更好的故事，具有更多的叙事闭合性。</li>
</ul>

<h3>Title: Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie  Embedding</h3>
<ul>
<li><strong>Authors: </strong>Lung-Chuan Chen, Zong-Ru Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00862">https://arxiv.org/abs/2404.00862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00862">https://arxiv.org/pdf/2404.00862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00862]] Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie  Embedding(https://arxiv.org/abs/2404.00862)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chinese, we conduct secondary pre-training on Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our proposed zip-tie embedding initialization. The resulting model called Bailong, which stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie embeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B optimized for multi-turn dialogue scenarios. Recognizing the inadequacy of benchmark datasets in Traditional Chinese, we further introduce Bailong-bench to assess the alignment of models with human preferences and the capability to follow instructions in both Traditional Chinese and English tasks. In our evaluation, Bailong-instruct 7B exhibits competitive performance on Bailong-bench and other benchmark datasets when compared to other open-source models of similar or even larger parameter sizes. Bailong-instruct 7B and Bailong-bench are publicly available with the aim of empowering the community to build upon our efforts.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种 NLP 应用中表现出了卓越的性能。然而，大多数现有的开源法学硕士主要是根据英语数据进行预训练的，其他语言的数据很少。当应用于可用资源较少的语言时，多语言训练数据的这种缺陷会导致性能不佳。此外，通过使用附加数据进行全参数微调来提高法学硕士在低资源语言上的性能需要大量的计算资源，这给研究组织和个人研究人员带来了计算障碍。因此，人们提出了参数高效调整和高级嵌入初始化等多种技术来解决这些挑战。在这项工作中，我们将它们结合起来，以促进以英语为主的开源法学硕士的跨语言转移。为了有效提高模型对繁体中文的熟练程度，我们利用 QLoRA 和我们提出的 zip-tie 嵌入初始化，对 Llama 2 7B 进行了繁体中文数据的二次预训练。由此产生的模型称为 Bailong，它代表基于 qLOra 和 zip-tie embedding 的双语迁移学习。我们推出了Bailong-instruct 7B，这是Bailong 7B的微调版本，针对多回合对话场景进行了优化。认识到繁体中文基准数据集的不足，我们进一步引入 Bailong-bench 来评估模型与人类偏好的一致性以及在繁体中文和英文任务中遵循指令的能力。在我们的评估中，与具有类似甚至更大参数大小的其他开源模型相比，Bailong-instruct 7B 在 Bailong-bench 和其他基准数据集上表现出具有竞争力的性能。 Bailong-instruct 7B 和 Bailong-bench 是公开的，目的是让社区能够在我们的努力的基础上再接再厉。</li>
</ul>

<h3>Title: Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei He, Shichun Liu, Jun Zhao, Yiwen Ding, Yi Lu, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00884">https://arxiv.org/abs/2404.00884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00884">https://arxiv.org/pdf/2404.00884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00884]] Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large  Language Models(https://arxiv.org/abs/2404.00884)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已经显示出有前途的上下文学习（ICL）能力，只需几次演示即可快速适应新任务。然而，当前的小样本方法在很大程度上依赖于高质量的、特定于查询的演示，而这些演示往往是缺乏的。当面临演示外 (OOD) 查询时，依赖手工制作的演示或外部检索器的方法可能会失败。为了弥合有限演示和 OOD 查询之间的差距，我们提出了 Self-Demos，这是一种新颖的提示方法，通过查询感知演示生成来引发法学硕士固有的通用性。生成的演示策略性地在现有演示和给定查询之间进行插值，将查询从 OOD 转换为 ID。为了评估我们方法的有效性，我们手动构建了 OOD-Toolset，这是一个工具使用场景中的数据集，包含 300 多个真实 API 和 1000 个实例，每个实例包含三个工具用例作为演示和一个 OOD 查询。对我们的数据集和两个公共数学基准进行的彻底实验表明，我们的方法可以超越 OOD 设置中最先进的基线。此外，我们还进行了一系列分析来验证自我演示的概括性并提供更多见解。</li>
</ul>

<h3>Title: TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary  Detection for Human-Machine Mixed Text</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Qu, Xiangfeng Meng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00899">https://arxiv.org/abs/2404.00899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00899">https://arxiv.org/pdf/2404.00899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00899]] TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary  Detection for Human-Machine Mixed Text(https://arxiv.org/abs/2404.00899)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the increasing prevalence of text generated by large language models (LLMs), there is a growing concern about distinguishing between LLM-generated and human-written texts in order to prevent the misuse of LLMs, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content. This paper explores LLMs' ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of LLMs achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of LLMs in detecting boundaries within mixed texts, including the incorporation of extra layers on top of LLMs, combination of segmentation loss, and the impact of pretraining. Our findings aim to provide valuable insights for future research in this area.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）生成的文本越来越流行，人们越来越关注区分 LLM 生成的文本和人类编写的文本，以防止 LLM 的滥用，例如传播误导性信息和学术不诚实。 。先前的研究主要集中于将文本分类为完全由人类编写或由法学硕士生成，忽略了包含这两种类型内容的混合文本的检测。本文探讨了法学硕士识别人类书写和机器生成的混合文本中边界的能力。我们通过将其转化为标记分类问题来处理此任务，并将标签转折点视为边界。值得注意的是，我们的法学硕士集成模型在 SemEval'24 竞赛任务 8 的“人机混合文本检测”子任务中获得了第一名。此外，我们还研究了影响法学硕士检测混合文本中边界的能力的因素，包括在 LLM 之上加入额外层、分割损失的组合以及预训练的影响。我们的研究结果旨在为该领域的未来研究提供有价值的见解。</li>
</ul>

<h3>Title: Token-Efficient Leverage Learning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuanhao Zeng, Min Wang, Yihang Wang, Yingxia Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00914">https://arxiv.org/abs/2404.00914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00914">https://arxiv.org/pdf/2404.00914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00914]] Token-Efficient Leverage Learning in Large Language Models(https://arxiv.org/abs/2404.00914)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypothesis and explore its promising potential through empirical testing.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在各种任务中表现出色，但在高资源场景中表现更好，这在低资源场景中提出了挑战。数据稀缺和法学硕士适应特定任务的固有困难加剧了这一挑战。为了解决这两个障碍，我们引入了 \textbf{杠杆学习}。我们提出了这种方法的简化实现，称为代币高效杠杆学习（TELL）。 TELL 展示了杠杆学习的潜力，展示了各种法学硕士和低资源任务的有效性，代币范围从 10^4$ 到 10^6$ 不等。与传统的监督微调 (SFT) 相比，它可将任务数据需求减少近一个数量级，同时提供具有竞争力的性能。在相同数量的任务数据下，与 SFT 相比，TELL 在提高任务性能方面领先。我们讨论了杠杆学习的机制，表明它与量化假设相一致，并通过实证测试探索其有希望的潜力。</li>
</ul>

<h3>Title: A Survey on Multilingual Large Language Models: Corpora, Alignment, and  Bias</h3>
<ul>
<li><strong>Authors: </strong>Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00929">https://arxiv.org/abs/2404.00929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00929">https://arxiv.org/pdf/2404.00929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00929]] A Survey on Multilingual Large Language Models: Corpora, Alignment, and  Bias(https://arxiv.org/abs/2404.00929)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs including its category and evaluation metrics, and summarize the existing debiasing techniques. Finally, we discuss existing challenges and point out promising research directions. By demonstrating these aspects, this paper aims to facilitate a deeper understanding of MLLMs and their potentiality in various domains.</li>
<li><strong>摘要：</strong>在大语言模型（LLM）的基础上，开发了多语言大语言模型（MLLM）来解决多语言自然语言处理任务的挑战，希望实现从高资源语言到低资源语言的知识迁移。然而，仍然存在重大限制和挑战，例如语言不平衡、多语言对齐和固有偏见。在本文中，我们的目标是对 MLLM 进行全面分析，深入探讨围绕这些关键问题的讨论。首先，我们首先概述 MLLM，涵盖其演变、关键技术和多语言能力。其次，我们探索了广泛用于 MLLM 训练的多语言语料库和面向下游任务的多语言数据集，这对于增强 MLLM 的跨语言能力至关重要。第三，我们调查了现有的多语言表示研究，并研究了当前的 MLLM 是否可以学习通用语言表示。第四，我们讨论了 MLLM 的偏差，包括其类别和评估指标，并总结了现有的去偏差技术。最后，我们讨论了现有的挑战并指出了有前途的研究方向。通过展示这些方面，本文旨在促进人们更深入地了解 MLLM 及其在各个领域的潜力。</li>
</ul>

<h3>Title: PSYDIAL: Personality-based Synthetic Dialogue Generation using Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, Kyung-Ah Sohn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00930">https://arxiv.org/abs/2404.00930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00930">https://arxiv.org/pdf/2404.00930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00930]] PSYDIAL: Personality-based Synthetic Dialogue Generation using Large  Language Models(https://arxiv.org/abs/2404.00930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chat</a></li>
<li><strong>Abstract: </strong>We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conversational AI in Korean and potentially other languages. Our code is publicly available at https://github.com/jiSilverH/psydial.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的基于个性的端到端合成对话数据生成管道，专门设计用于通过提示从大型语言模型中引出响应。我们设计提示时，考虑到用户与聊天机器人互动时的真实场景，生成更人性化的对话。我们介绍 PSYDIAL，这是第一个专注于基于个性的对话的韩国对话数据集，使用我们提出的管道进行策划。值得注意的是，我们的研究重点是大五人格模型的外向维度。实验结果表明，虽然预先训练的模型和使用闲聊数据集进行微调的模型很难生成反映个性的响应，但使用 PSYDIAL 训练的模型显示出显着的改进。我们管道的多功能性超出了对话任务的范围，为其他非对话相关的应用程序提供了潜力。这项研究为韩语和其他语言的更细致、个性驱动的对话人工智能打开了大门。我们的代码可在 https://github.com/jiSilverH/psydial 上公开获取。</li>
</ul>

<h3>Title: ChatGLM-RLHF: Practices of Aligning Large Language Models with Human  Feedback</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Hou, Yiin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00934">https://arxiv.org/abs/2404.00934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00934">https://arxiv.org/pdf/2404.00934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00934]] ChatGLM-RLHF: Practices of Aligning Large Language Models with Human  Feedback(https://arxiv.org/abs/2404.00934)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\% more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations.</li>
<li><strong>摘要：</strong>ChatGLM 是一项免费使用的 AI 服务，由 ChatGLM 大语言模型 (LLM) 系列提供支持。在本文中，我们提出了 ChatGLM-RLHF 管道——一种来自人类反馈的强化学习 (RLHF) 系统——旨在增强 ChatGLM 与人类偏好的一致性。 ChatGLM-RLHF 包含三个主要部分：人类偏好数据的收集、奖励模型的训练和策略的优化。在将 ChatGLM-RLHF 集成到生产中的整个过程中，我们遇到并解决了一些前所未有的挑战。我们引入了减轻稳定大规模训练奖励方差的策略，通过融合梯度下降实现模型并行性，并设计正则化约束以避免法学硕士中的灾难性遗忘。实验表明，与 ChatGLM 的监督微调 (SFT) 版本相比，ChatGLM-RLHF 在对齐任务方面带来了显着改进。例如，在中文对齐任务中，它比 ChatGLM-SFT 平均多取得 15% 的胜利。这项工作展示了我们使法学硕士与人类偏好保持一致的实践，提供了对 RLHF 实施中的挑战和解决方案的见解。</li>
</ul>

<h3>Title: Evaluating the Factuality of Large Language Models using Large-Scale  Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Xiaoze Liu, Feijie Wu, Tianyang Xu, Zhuo Chen, Yichi Zhang, Xiaoqian Wang, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00942">https://arxiv.org/abs/2404.00942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00942">https://arxiv.org/pdf/2404.00942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00942]] Evaluating the Factuality of Large Language Models using Large-Scale  Knowledge Graphs(https://arxiv.org/abs/2404.00942)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM performance across different metrics and highlight the potential for future improvements in ensuring the factual integrity of LLM outputs. The code is publicly available at https://github.com/xz-liu/GraphEval.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现极大地改变了人工智能领域，增强了机器学习和人工智能能力。事实性问题是法学硕士的一个关键问题，因为他们可能会产生事实上不正确的答案。在本文中，我们提出 GraphEval 使用相当大的测试数据集来评估法学硕士的表现。具体来说，测试数据集是从包含超过 1000 万个事实的大型知识图中检索的，无需昂贵的人工操作。与根据生成的答案评估法学硕士的传统方法不同，GraphEval 通过创建判断模型来估计法学硕士给出的答案的正确性，从而简化了评估过程。我们的实验表明，法官模型的事实评估与法学硕士生成输出的正确性密切相关，同时也大大降低了评估成本。此外，我们的研究结果为跨不同指标的法学硕士表现提供了宝贵的见解，并强调了未来在确保法学硕士输出的事实完整性方面的改进潜力。该代码可在 https://github.com/xz-liu/GraphEval 上公开获取。</li>
</ul>

<h3>Title: Evalverse: Unified and Accessible Library for Large Language Model  Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, Chanjun Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00943">https://arxiv.org/abs/2404.00943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00943">https://arxiv.org/pdf/2404.00943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00943]] Evalverse: Unified and Accessible Library for Large Language Model  Evaluation(https://arxiv.org/abs/2404.00943)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.</li>
<li><strong>摘要：</strong>本文介绍了 Evalverse，这是一个新颖的库，它通过将不同的评估工具统一到一个用户友好的框架中来简化大型语言模型 (LLM) 的评估。通过与 Slack 等通信平台的集成，Evalverse 使人工智能知识有限的个人能够轻松请求 LLM 评估并接收详细报告。因此，Evalverse 是法学硕士综合评估的强大工具，为研究人员和从业者提供了一个集中且易于访问的评估框架。最后，我们还提供了 Evalverse 的演示视频，以两分钟的形式展示了其功能和实现。</li>
</ul>

<h3>Title: AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for  Detecting Multi-generator Machine-generated Text</h3>
<ul>
<li><strong>Authors: </strong>Renhua Gu, Xiangfeng Meng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00950">https://arxiv.org/abs/2404.00950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00950">https://arxiv.org/pdf/2404.00950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00950]] AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for  Detecting Multi-generator Machine-generated Text(https://arxiv.org/abs/2404.00950)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>SemEval-2024 Task 8 provides a challenge to detect human-written and machine-generated text. There are 3 subtasks for different detection scenarios. This paper proposes a system that mainly deals with Subtask B. It aims to detect if given full text is written by human or is generated by a specific Large Language Model (LLM), which is actually a multi-class text classification task. Our team AISPACE conducted a systematic study of fine-tuning transformer-based models, including encoderonly, decoder-only and encoder-decoder models. We compared their performance on this task and identified that encoder-only models performed exceptionally well. We also applied a weighted Cross Entropy loss function to address the issue of data imbalance of different class samples. Additionally, we employed softvoting strategy over multi-models ensemble to enhance the reliability of our predictions. Our system ranked top 1 in Subtask B, which sets a state-of-the-art benchmark for this new challenge.</li>
<li><strong>摘要：</strong>SemEval-2024 任务 8 提出了检测人类编写和机器生成的文本的挑战。针对不同的检测场景，有3个子任务。本文提出了一个主要处理子任务B的系统。它的目的是检测给定的全文是由人类编写的还是由特定的大语言模型（LLM）生成的，这实际上是一个多类文本分类任务。我们的团队 AISPACE 对基于 Transformer 的模型进行了系统的微调研究，包括仅编码器、仅解码器和编码器-解码器模型。我们比较了他们在这项任务上的表现，并发现仅编码器的模型表现得非常好。我们还应用了加权交叉熵损失函数来解决不同类样本的数据不平衡问题。此外，我们在多模型集成上采用软投票策略来增强我们预测的可靠性。我们的系统在子任务 B 中排名第一，这为这一新挑战设定了最先进的基准。</li>
</ul>

<h3>Title: Prior Constraints-based Reward Model Training for Aligning Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hang Zhou, Chenglong Wang, Yimin Hu, Tong Xiao, Chunliang Zhang, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00978">https://arxiv.org/abs/2404.00978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00978">https://arxiv.org/pdf/2404.00978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00978]] Prior Constraints-based Reward Model Training for Aligning Large  Language Models(https://arxiv.org/abs/2404.00978)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward score scaling. As another bonus, our method is easily integrated into arbitrary rank-based alignment methods, such as direct preference optimization, and can yield consistent improvement.</li>
<li><strong>摘要：</strong>利用人类反馈来对齐大型语言模型 (LLM) 的强化学习通常使用比较对的排名损失来训练奖励模型。然而，训练过程存在一个固有的问题：由于缺乏本文提出了一种基于先验约束的奖励模型（PCRM）训练方法来缓解这个问题。 PCRM 在奖励模型训练期间结合了先前的约束，特别是每个比较对的输出之间的长度比和余弦相似度，以调节优化幅度和控制分数裕度。我们通过检查 PCRM 与人类偏好的排名相关性及其通过 RL 调整 LLM 的有效性来全面评估 PCRM。实验结果表明，PCRM 通过有效限制奖励分数缩放，显着提高了对齐性能。作为另一个好处，我们的方法可以轻松集成到任意基于排名的对齐方法中，例如直接偏好优化，并且可以产生一致的改进。</li>
</ul>

<h3>Title: Exploring the Nexus of Large Language Models and Legal Systems: A Short  Survey</h3>
<ul>
<li><strong>Authors: </strong>Weicong Qin, Zhongxiang Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00990">https://arxiv.org/abs/2404.00990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00990">https://arxiv.org/pdf/2404.00990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00990]] Exploring the Nexus of Large Language Models and Legal Systems: A Short  Survey(https://arxiv.org/abs/2404.00990)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), there is a profound transformation occurring in the realm of natural language processing tasks within the legal domain. The capabilities of LLMs are increasingly demonstrating unique roles in the legal sector, bringing both distinctive benefits and various challenges. This survey delves into the synergy between LLMs and the legal system, such as their applications in tasks like legal text comprehension, case retrieval, and analysis. Furthermore, this survey highlights key challenges faced by LLMs in the legal domain, including bias, interpretability, and ethical considerations, as well as how researchers are addressing these issues. The survey showcases the latest advancements in fine-tuned legal LLMs tailored for various legal systems, along with legal datasets available for fine-tuning LLMs in various languages. Additionally, it proposes directions for future research and development.</li>
<li><strong>摘要：</strong>随着人工智能（AI）和大型语言模型（LLM）的进步，法律领域内的自然语言处理任务领域正在发生深刻的转变。法学硕士的能力在法律领域日益展现出独特的作用，既带来了独特的好处，也带来了各种挑战。这项调查深入研究了法学硕士与法律体系之间的协同作用，例如它们在法律文本理解、案例检索和分析等任务中的应用。此外，这项调查还强调了法学硕士在法律领域面临的主要挑战，包括偏见、可解释性和道德考虑，以及研究人员如何解决这些问题。该调查展示了针对各种法律体系量身定制的法律法学硕士微调的最新进展，以及可用于各种语言微调法学硕士的法律数据集。此外，它还提出了未来研究和开发的方向。</li>
</ul>

<h3>Title: LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report  Generation</h3>
<ul>
<li><strong>Authors: </strong>Zilong Wang, Xufang Luo, Xinyang Jiang, Dongsheng Li, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00998">https://arxiv.org/abs/2404.00998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00998">https://arxiv.org/pdf/2404.00998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00998]] LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report  Generation(https://arxiv.org/abs/2404.00998)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced and accessible.</li>
<li><strong>摘要：</strong>评估生成的放射学报告对于放射学人工智能的发展至关重要，但现有指标无法反映该任务的临床要求。本研究提出了一种新颖的评估框架，使用大语言模型（LLM）来比较放射学报告以进行评估。我们比较了各种法学硕士的表现，并证明，当使用 GPT-4 时，我们提出的指标实现了接近放射科医生的评估一致性。此外，为了降低成本并提高可访问性，使该方法实用，我们使用LLM评估结果构建数据集并进行知识蒸馏以训练较小的模型。蒸馏模型达到了与 GPT-4 相当的评估能力。我们的框架和蒸馏模型为放射学报告的生成提供了一种可访问且有效的评估方法，促进了更多临床相关模型的开发。该模型将进一步开源和可访问。</li>
</ul>

<h3>Title: PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison</h3>
<ul>
<li><strong>Authors: </strong>ChaeHun Park, Minseok Choi, Dohyun Lee, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01015">https://arxiv.org/abs/2404.01015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01015">https://arxiv.org/pdf/2404.01015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01015]] PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison(https://arxiv.org/abs/2404.01015)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in detecting common failures from open-domain dialogue systems, including repetition and speaker insensitivity.</li>
<li><strong>摘要：</strong>对于开放域对话系统来说，建立可靠且自动化的评估指标是一个必要但具有挑战性的问题。最近的研究提出了评估指标，通过考虑生成的响应与之前对话历史的相关性来评估生成的响应。尽管有效，但这些指标直接评估个人响应，而不是考虑其与其他响应相比的相对质量。为了解决这个问题，我们提出了 PairEval，这是一种新颖的对话评估指标，用于通过将响应的质量与不同对话中的响应进行比较来评估响应。 PairEval 建立在开源和中等规模的语言模型之上，我们使它们专门用于对话响应之间的成对比较。对多个基准的大量实验表明，与基线指标相比，我们的指标与人类判断的相关性更高。我们还发现，所提出的比较指标在检测开放域对话系统的常见故障（包括重复和说话者不敏感）方面更加稳健。</li>
</ul>

<h3>Title: Source-Aware Training Enables Knowledge Attribution in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01019">https://arxiv.org/abs/2404.01019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01019">https://arxiv.org/pdf/2404.01019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01019]] Source-Aware Training Enables Knowledge Attribution in Language Models(https://arxiv.org/abs/2404.01019)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can enable faithful attribution to the pretraining data without a substantial impact on the model's quality compared to standard pretraining. Our results also highlight the importance of data augmentation in achieving attribution.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在预训练过程中学习大量知识，但它们常常忘记这些知识的来源。我们研究了内在源引用的问题，其中法学硕士需要引用支持生成的响应的预训练源。内在来源引用可以提高法学硕士的透明度、可解释性和可验证性。为了赋予法学硕士这样的能力，我们探索了源感知培训——一种预培训方法，其中包括（i）培训法学硕士将唯一的源文档标识符与每个文档中的知识相关联，然后（ii）进行指令调整以进行教学法学硕士在出现提示时引用支持性预训练来源。源感知培训可以轻松应用于现成的预培训法学硕士，并且与现有预培训/微调框架的差异极小。通过对精心策划的数据进行实验，我们证明了与标准预训练相比，我们的训练方法可以忠实地归因于预训练数据，而不会对模型的质量产生重大影响。我们的结果还强调了数据增强在实现归因方面的重要性。</li>
</ul>

<h3>Title: ARAGOG: Advanced RAG Output Grading</h3>
<ul>
<li><strong>Authors: </strong>Matouš Eibich, Shivay Nagpal, Alexander Fred-Ojala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01037">https://arxiv.org/abs/2404.01037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01037">https://arxiv.org/pdf/2404.01037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01037]] ARAGOG: Advanced RAG Output Grading(https://arxiv.org/abs/2404.01037)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 对于将外部知识集成到大型语言模型 (LLM) 输出中至关重要。虽然有关 RAG 的文献不断增加，但它主要侧重于对最新最先进 (SoTA) 技术与其前身技术的系统回顾和比较，在广泛的实验比较方面存在差距。本研究通过评估各种 RAG 方法对检索精度和答案相似性的影响来解决这一差距。我们发现假设文档嵌入（HyDE）和 LLM 重新排名显着提高了检索精度。然而，与基线 Naive RAG 系统相比，最大边际相关性 (MMR) 和 Cohere 重新排序并未表现出明显的优势，并且多查询方法表现不佳。句子窗口检索成为检索精度最有效的方法，尽管它在答案相似性方面的表现各不相同。该研究证实了文档摘要索引作为有效检索方法的潜力。与本研究相关的所有资源均可通过我们的 GitHub 存储库 ARAGOG (https://github.com/predlico/ARAGOG) 公开访问，以便进一步调查。我们欢迎社区进一步开展 RAG 系统的探索性研究。</li>
</ul>

<h3>Title: Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language  Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01054">https://arxiv.org/abs/2404.01054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01054">https://arxiv.org/pdf/2404.01054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01054]] Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language  Model Alignment(https://arxiv.org/abs/2404.01054)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially when the proxy reward model has a low correlation with the true objective.</li>
<li><strong>摘要：</strong>采用奖励模型的 Best-of-N (BoN) 采样已被证明是在解码时使大型语言模型 (LLM) 与人类偏好保持一致的有效策略。 BoN 抽样很容易受到奖励黑客问题的影响。由于奖励模型是真实目标的不完美代理，因此过度优化其价值可能会损害其在真实目标上的表现。在偏好学习技术中防止奖励黑客攻击的常见解决方案是使用邻近正则化（例如 KL 正则化）来优化奖励，这可确保语言模型保持接近参考模型。在这项研究中，我们提出了正则化 Best-of-N (RBoN)，这是 BoN 的一种变体，旨在通过在响应选择中纳入邻近项来减轻奖励黑客行为，类似于偏好学习技术。我们在 AlpacaFarm 数据集上评估了 RBoN 的两个变体，发现它们的表现优于 BoN，特别是当代理奖励模型与真实目标的相关性较低时。</li>
</ul>

<h3>Title: Efficient Prompting Methods for Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01077">https://arxiv.org/abs/2404.01077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01077">https://arxiv.org/pdf/2404.01077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01077]] Efficient Prompting Methods for Large Language Models: A Survey(https://arxiv.org/abs/2404.01077)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks. While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs. As a result, the LLM field has seen a remarkable surge in efficient prompting methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design. The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization. We present the basic concepts of prompting, review the advances for efficient prompting, and highlight future research directions.</li>
<li><strong>摘要：</strong>提示已成为使大型语言模型（LLM）适应特定自然语言处理任务的主流范例。虽然这种方法为法学硕士的上下文学习打开了大门，但它带来了模型推理的额外计算负担和手动设计提示的人力，特别是在使用冗长而复杂的提示来指导和控制法学硕士的行为时。因此，法学硕士领域的有效激励方法显着激增。在本文中，我们对这些方法进行了全面的概述。在高层次上，高效的提示方法大致可以分为两种方法：高效计算提示和高效设计提示。前者涉及各种压缩提示的方式，后者采用自动提示优化技术。我们介绍了提示的基本概念，回顾了有效提示的进展，并强调了未来的研究方向。</li>
</ul>

<h3>Title: AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer  Models for Lateral Thinking Puzzles</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Panagiotopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01084">https://arxiv.org/abs/2404.01084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01084">https://arxiv.org/pdf/2404.01084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01084]] AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer  Models for Lateral Thinking Puzzles(https://arxiv.org/abs/2404.01084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.</li>
<li><strong>摘要：</strong>在本文中，我们概述了 SemEval-2024 任务 9 竞赛的提交内容：“BRAINTEASER：一项违反常识的新颖任务”。我们参与两个子任务：子任务 A 句谜题和子任务 B 词谜题。我们通过微调评估大量不同大小的基于预训练的基于 Transformer 的语言模型。随后，我们对他们的分数和反应进行分析，以帮助未来的研究人员有效地理解和利用这些模型。我们表现​​最好的方法在这两个子任务的竞赛排行榜上确保了竞争地位。在评估阶段，我们的最佳提交在句子谜题中获得了 81.7% 的平均准确度分数，在单词谜题中获得了 85.4% 的平均准确度分数，分别显着优于最佳神经基线 (ChatGPT) 20% 和 30% 以上。</li>
</ul>

<h3>Title: SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework  with Sentiment-guided Textual Similarity</h3>
<ul>
<li><strong>Authors: </strong>Jaemin Kim, Yohan Na, Kangmin Kim, Sang Rak Lee, Dong-Kyu Chae</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01104">https://arxiv.org/abs/2404.01104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01104">https://arxiv.org/pdf/2404.01104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01104]] SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework  with Sentiment-guided Textual Similarity(https://arxiv.org/abs/2404.01104)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, sentiment-aware pre-trained language models (PLMs) demonstrate impressive results in downstream sentiment analysis tasks. However, they neglect to evaluate the quality of their constructed sentiment representations; they just focus on improving the fine-tuning performance, which overshadows the representation quality. We argue that without guaranteeing the representation quality, their downstream performance can be highly dependent on the supervision of the fine-tuning data rather than representation quality. This problem would make them difficult to foray into other sentiment-related domains, especially where labeled data is scarce. We first propose Sentiment-guided Textual Similarity (SgTS), a novel metric for evaluating the quality of sentiment representations, which is designed based on the degree of equivalence in sentiment polarity between two sentences. We then propose SentiCSE, a novel Sentiment-aware Contrastive Sentence Embedding framework for constructing sentiment representations via combined word-level and sentence-level objectives, whose quality is guaranteed by SgTS. Qualitative and quantitative comparison with the previous sentiment-aware PLMs shows the superiority of our work. Our code is available at: https://github.com/nayohan/SentiCSE</li>
<li><strong>摘要：</strong>最近，情感感知预训练语言模型（PLM）在下游情感分析任务中展示了令人印象深刻的结果。然而，他们忽视了评估其构建的情感表征的质量；他们只是专注于提高微调性能，这掩盖了表示质量。我们认为，在不保证表示质量的情况下，它们的下游性能可能高度依赖于微调数据的监督，而不是表示质量。这个问题将使他们难以涉足其他与情感相关的领域，特别是在标记数据稀缺的领域。我们首先提出了情感引导文本相似度（SgTS），这是一种评估情感表示质量的新指标，它是基于两个句子之间情感极性的等价程度而设计的。然后，我们提出了 SentiCSE，一种新颖的情感感知对比句子嵌入框架，用于通过组合词级和句子级目标构建情感表示，其质量由 SgTS 保证。与之前的情感感知 PLM 的定性和定量比较显示了我们工作的优越性。我们的代码位于：https://github.com/nayohan/SentiCSE</li>
</ul>

<h3>Title: Structured Information Matters: Incorporating Abstract Meaning  Representation into LLMs for Improved Open-Domain Dialogue Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01129">https://arxiv.org/abs/2404.01129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01129">https://arxiv.org/pdf/2404.01129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01129]] Structured Information Matters: Incorporating Abstract Meaning  Representation into LLMs for Improved Open-Domain Dialogue Evaluation(https://arxiv.org/abs/2404.01129)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs) with LLMs. The SLMs can explicitly incorporate Abstract Meaning Representation (AMR) graph information of the dialogue through a gating mechanism for enhanced semantic representation learning. The evaluation result of SLMs and AMR graph information are plugged into the prompt of LLM, for the enhanced in-context learning performance. Experimental results on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to a wide range of state-of-the-art baselines, especially in discriminating adversarial negative responses. Our code is available at https://github.com/Bernard-Yang/SIMAMR.</li>
<li><strong>摘要：</strong>自动开放域对话评估越来越受到关注。可训练的评估指标通常使用真正的正面和随机选择的负面响应进行训练，导致他们倾向于为与给定上下文具有较高内容相似性的响应分配较高的分数。然而，对抗性的负面反应与上下文具有高度的内容相似性，但在语义上却有所不同。因此，现有的评估指标不够稳健，无法评估此类响应，导致与人类判断的相关性较低。虽然最近的研究表明利用大型语言模型（LLM）进行开放域对话评估具有一定的功效，但它们在有效处理对抗性负面例子方面仍然遇到挑战。在本文中，我们提出了一个简单而有效的开放域对话评估框架，它将特定领域语言模型（SLM）与LLM结合起来。 SLM 可以通过用于增强语义表示学习的门控机制明确地合并对话的抽象含义表示（AMR）图信息。将SLM的评估结果和AMR图信息插入到LLM的提示中，以增强情境学习性能。开放域对话评估任务的实验结果证明了我们的方法与各种最先进的基线相比的优越性，特别是在区分对抗性负面反应方面。我们的代码可在 https://github.com/Bernard-Yang/SIMAMR 获取。</li>
</ul>

<h3>Title: Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case  Study on Reddit</h3>
<ul>
<li><strong>Authors: </strong>Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak, Sarah Masud Preum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01147">https://arxiv.org/abs/2404.01147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01147">https://arxiv.org/pdf/2404.01147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01147]] Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case  Study on Reddit(https://arxiv.org/abs/2404.01147)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已被证明能够熟练地正确回答在线对话中的问题。然而，利用法学硕士对事实驱动的社交媒体问题进行类人回答的研究仍处于探索之中。在这项工作中，我们研究了法学硕士如何对在几个特定主题的 Reddit 社区或子 Reddit 上提出的事实驱动问题的各种人类答案进行建模。我们收集并发布了一个数据集，其中包含来自 15 个 r/Ask{Topic} 社区的 409 个事实驱动问题和 7,534 个不同的人工评分答案，涵盖 3 个类别：职业、社会身份和地理位置。我们发现法学硕士在模拟此类问题的高评价人类答案方面要好得多，而不是低评价人类答案。我们根据初步发现提出了未来研究的几个方向。</li>
</ul>

<h3>Title: Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade  Offs in Large Language Model Training</h3>
<ul>
<li><strong>Authors: </strong>Vivian Liu, Yiqiao Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01157">https://arxiv.org/abs/2404.01157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01157">https://arxiv.org/pdf/2404.01157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01157]] Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade  Offs in Large Language Model Training(https://arxiv.org/abs/2404.01157)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We argue for the training of LLMs in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions. Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs. Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance.</li>
<li><strong>摘要：</strong>自然语言处理领域的杰出工作长期以来一直试图通过改进以前的模型训练方法、改变模型架构以及开发更深入的数据集以提高其性能来创建新的创新模型。然而，随着自然语言处理领域的快速发展，温室气体排放量也随之增加，人们对培训法学硕士造成的环境破坏产生了担忧。全面了解与人工智能相关的各种成本，特别是与环境方面相关的成本，是确保人工智能模型安全的基础。目前，对人工智能模型的二氧化碳排放量的调查仍然是一个新兴的研究领域，因此，在本文中，我们评估了著名的大型语言模型的二氧化碳排放量，这些模型由于其大量的碳足迹而具有特别高的碳足迹。模型参数。我们主张通过提出减少碳排放的措施，以负责任和可持续的方式对法学硕士进行培训。此外，我们通过对比两种广泛使用的 GPU 模型训练期间的二氧化碳排放量，讨论了硬件的选择如何影响二氧化碳排放量。根据我们的结果，我们提出了我们提出的解决方案的优点和缺点，并论证了在不牺牲其稳健性和性能的情况下训练更环保的人工智能模型的可能性。</li>
</ul>

<h3>Title: LITE: Modeling Environmental Ecosystems with Multimodal Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01165">https://arxiv.org/abs/2404.01165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01165">https://arxiv.org/pdf/2404.01165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01165]] LITE: Modeling Environmental Ecosystems with Multimodal Large Language  Models(https://arxiv.org/abs/2404.01165)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities. During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the distribution shift is handled by incorporating multi-granularity information from past observations. Finally, guided by domain instructions, a language model is employed to fuse the multimodal representations for the prediction. Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25% reduction in prediction error. This justifies its effectiveness. Our data and code are available at https://github.com/hrlics/LITE.</li>
<li><strong>摘要：</strong>环境生态系统的建模在地球的可持续管理中发挥着关键作用。准确预测时空关键环境变量有助于制定明智的政策和决策，从而改善民生。最近，基于深度学习的方法在建模时空关系以预测环境变量方面显示出了前景。然而，这些方法通常在处理不完整的特征和分布变化方面存在不足，由于数据收集的巨大成本和测量仪器的故障，这些问题在环境数据中很常见。为了解决这些问题，我们提出了 LITE——一种用于环境生态系统建模的多模式大语言模型。具体来说，LITE 通过将不同的环境变量转换为自然语言描述和线图图像来统一它们。然后，LITE 利用统一编码器来捕获不同模态的时空动态和相关性。在此步骤中，不完整的特征由稀疏的专家混合框架估算，并且通过合并来自过去观察的多粒度信息来处理分布偏移。最后，在领域指令的指导下，采用语言模型来融合多模态表示以进行预测。我们的实验表明，与最佳基线相比，LITE 显着增强了不同领域的环境时空预测性能，预测误差降低了 41.25%。这证明了其有效性。我们的数据和代码可在 https://github.com/hrlics/LITE 获取。</li>
</ul>

<h3>Title: Generating Faithful and Complete Hospital-Course Summaries from the  Electronic Health Record</h3>
<ul>
<li><strong>Authors: </strong>Griffin Adams</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01189">https://arxiv.org/abs/2404.01189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01189">https://arxiv.org/pdf/2404.01189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01189]] Generating Faithful and Complete Hospital-Course Summaries from the  Electronic Health Record(https://arxiv.org/abs/2404.01189)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>The rapid adoption of Electronic Health Records (EHRs) has been instrumental in streamlining administrative tasks, increasing transparency, and enabling continuity of care across providers. An unintended consequence of the increased documentation burden, however, has been reduced face-time with patients and, concomitantly, a dramatic rise in clinician burnout. In this thesis, we pinpoint a particularly time-intensive, yet critical, documentation task: generating a summary of a patient's hospital admissions, and propose and evaluate automated solutions. In Chapter 2, we construct a dataset based on 109,000 hospitalizations (2M source notes) and perform exploratory analyses to motivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we address faithfulness from a modeling perspective by revising noisy references [EMNLP 2022] and, to reduce the reliance on references, directly calibrating model outputs to metrics [ACL 2023]. These works relied heavily on automatic metrics as human annotations were limited. To fill this gap, in Chapter 4, we conduct a fine-grained expert annotation of system errors in order to meta-evaluate existing metrics and better understand task-specific issues of domain adaptation and source-summary alignments. To learn a metric less correlated to extractiveness (copy-and-paste), we derive noisy faithfulness labels from an ensemble of existing metrics and train a faithfulness classifier on these pseudo labels [MLHC 2023]. Finally, in Chapter 5, we demonstrate that fine-tuned LLMs (Mistral and Zephyr) are highly prone to entity hallucinations and cover fewer salient entities. We improve both coverage and faithfulness by performing sentence-level entity planning based on a set of pre-computed salient entities from the source text, which extends our work on entity-guided news summarization [ACL, 2023], [EMNLP, 2023].</li>
<li><strong>摘要：</strong>电子健康记录 (EHR) 的快速采用有助于简化管理任务、提高透明度并实现跨服务提供者的护理连续性。然而，文件负担增加的一个意想不到的后果是与患者会面的时间减少，随之而来的是临床医生倦怠的急剧增加。在本文中，我们指出了一项特别耗时但至关重要的文档任务：生成患者入院情况的摘要，并提出和评估自动化解决方案。在第 2 章中，我们基于 109,000 例住院治疗（2M 源注释）构建了一个数据集，并进行探索性分析，以推动未来的建模和评估工作 [NAACL 2021]。在第 3 章中，我们通过修改噪声参考 [EMNLP 2022] 从建模角度解决忠实度问题，并为了减少对参考的依赖，直接将模型输出校准为指标 [ACL 2023]。由于人工注释有限，这些工作严重依赖自动指标。为了填补这一空白，在第 4 章中，我们对系统错误进行了细粒度的专家注释，以便对现有指标进行元评估，并更好地理解领域适应和源摘要对齐的特定于任务的问题。为了学习与提取性（复制粘贴）相关性较小的指标，我们从现有指标的集合中导出嘈杂的忠诚度标签，并在这些伪标签上训练忠诚度分类器 [MLHC 2023]。最后，在第 5 章中，我们证明了经过微调的 LLM（Mistral 和 Zephyr）很容易产生实体幻觉，并且覆盖的显着实体较少。我们通过基于源文本中一组预先计算的显着实体执行句子级实体规划来提高覆盖率和可信度，这扩展了我们在实体引导新闻摘要方面的工作[ACL，2023]，[EMNLP，2023]。</li>
</ul>

<h3>Title: The Fine Line: Navigating Large Language Model Pretraining with  Down-streaming Capability Analysis</h3>
<ul>
<li><strong>Authors: </strong>Chen Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, Jiaheng Liu, Stephen W. Huang, Shawn Yue, Wenhu Chen, Jie Fu, Ge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01204">https://arxiv.org/abs/2404.01204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01204">https://arxiv.org/pdf/2404.01204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01204]] The Fine Line: Navigating Large Language Model Pretraining with  Down-streaming Capability Analysis(https://arxiv.org/abs/2404.01204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.</li>
<li><strong>摘要：</strong>发现反映最终模型性能的早期指标是大规模预训练的核心原则之一。现有的缩放定律证明了预训练损失和训练失败之间的幂律相关性，这是大型语言模型当前训练状态的重要指标。但该原理只关注模型对训练数据的压缩特性，导致与下游任务的能力提升不一致。一些后续工作试图将标度律扩展到更复杂的指标（例如超参数），但仍然缺乏对预训练期间各种能力之间的动态差异的全面分析。为了解决上述局限性，本文对各种预训练中间检查点的模型能力进行了全面比较。通过此分析，我们确认特定下游指标在不同规模（最多 670 亿个参数）的模型中表现出相似的训练动态。除了我们的核心发现之外，我们还复制了 Amber 和 OpenLLaMA，并发布了它们的中间检查点。该举措为研究界提供了宝贵的资源，并促进了开源研究人员对法学硕士预训练的验证和探索。此外，我们还提供实证总结，包括不同模型和能力的性能比较，以及不同训练阶段关键指标的教学。基于这些发现，我们提供了一种更加用户友好的策略来评估优化状态，为建立稳定的预训练过程提供指导。</li>
</ul>

<h3>Title: AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for  hallucination detection and analysis</h3>
<ul>
<li><strong>Authors: </strong>Natalia Griogoriadou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01210">https://arxiv.org/abs/2404.01210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01210">https://arxiv.org/pdf/2404.01210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01210]] AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for  hallucination detection and analysis(https://arxiv.org/abs/2404.01210)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>In this paper, we present our team's submissions for SemEval-2024 Task-6 - SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The participants were asked to perform binary classification to identify cases of fluent overgeneration hallucinations. Our experimentation included fine-tuning a pre-trained model on hallucination detection and a Natural Language Inference (NLI) model. The most successful strategy involved creating an ensemble of these models, resulting in accuracy rates of 77.8% and 79.9% on model-agnostic and model-aware datasets respectively, outperforming the organizers' baseline and achieving notable results when contrasted with the top-performing results in the competition, which reported accuracies of 84.7% and 81.3% correspondingly.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了我们团队提交的 SemEval-2024 Task-6 - SHROOM，这是一项关于幻觉和相关可观察到的过度生成错误的共享任务。参与者被要求进行二元分类，以识别流畅的过度生成幻觉的案例。我们的实验包括微调幻觉检测的预训练模型和自然语言推理（NLI）模型。最成功的策略是创建这些模型的集合，在与模型无关和模型感知的数据集上的准确率分别达到 77.8% 和 79.9%，超越了组织者的基线，与表现最好的结果相比，取得了显着的结果在比赛中，准确率分别为 84.7% 和 81.3%。</li>
</ul>

<h3>Title: Stable Code Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, Nathan Cooper</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01226">https://arxiv.org/abs/2404.01226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01226">https://arxiv.org/pdf/2404.01226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01226]] Stable Code Technical Report(https://arxiv.org/abs/2404.01226)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>We introduce Stable Code, the first in our new-generation of code language models series, which serves as a general-purpose base code language model targeting code completion, reasoning, math, and other software engineering-based tasks. Additionally, we introduce an instruction variant named Stable Code Instruct that allows conversing with the model in a natural chat interface for performing question-answering and instruction-based tasks. In this technical report, we detail the data and training procedure leading to both models. Their weights are available via Hugging Face for anyone to download and use at https://huggingface.co/stabilityai/stable-code-3b and https://huggingface.co/stabilityai/stable-code-instruct-3b. This report contains thorough evaluations of the models, including multilingual programming benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of its release, Stable Code is the state-of-the-art open model under 3B parameters and even performs comparably to larger models of sizes 7 billion and 15 billion parameters on the popular Multi-PL benchmark. Stable Code Instruct also exhibits state-of-the-art performance on the MT-Bench coding tasks and on Multi-PL completion compared to other instruction tuned models. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.</li>
<li><strong>摘要：</strong>我们推出了稳定代码，这是新一代代码语言模型系列中的第一个，它作为通用基础代码语言模型，针对代码完成、推理、数学和其他基于软件工程的任务。此外，我们还引入了一种名为 Stable Code Instruct 的指令变体，它允许在自然聊天界面中与模型进行对话，以执行问答和基于指令的任务。在这份技术报告中，我们详细介绍了这两种模型的数据和训练过程。它们的权重可通过 Hugging Face 供任何人下载和使用：https://huggingface.co/stabilityai/stable-code-3b 和 https://huggingface.co/stabilityai/stable-code-instruct-3b。该报告包含对模型的全面评估，包括多语言编程基准和侧重于多轮对话的 MT 基准。在发布时，Stable Code 是 3B 参数下最先进的开放模型，甚至在流行的 Multi-PL 基准上的性能可与 70 亿和 150 亿参数大小的较大模型相媲美。与其他指令调整模型相比，Stable Code Instruct 在 MT-Bench 编码任务和 Multi-PL 完成方面也表现出了最先进的性能。鉴于其较小的尺寸，我们还提供了许多边缘设备的吞吐量测量。此外，我们还开源了几个量化检查点，并提供了它们与原始模型相比的性能指标。</li>
</ul>

<h3>Title: LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01230">https://arxiv.org/abs/2404.01230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01230">https://arxiv.org/pdf/2404.01230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01230]] LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language  Models(https://arxiv.org/abs/2404.01230)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.</li>
<li><strong>摘要：</strong>本文对大型语言模型（LLM）在战略推理中的现状和机遇进行了全面调查，战略推理是一种复杂的推理形式，需要理解和预测多智能体环境中的对手行为，同时相应地调整策略。战略推理的特点是关注多主体之间相互作用的动态和不确定性，其中理解环境和预测他人的行为至关重要。我们与法学硕士探讨了与战略推理相关的范围、应用、方法和评估指标，强调了该领域的蓬勃发展以及提高其决策绩效的跨学科方法。它旨在系统化和澄清有关该主题的分散文献，提供系统综述，强调战略推理作为一种关键认知能力的重要性，并提供对未来研究方向和潜在改进的见解。</li>
</ul>

<h3>Title: Open-Vocabulary Federated Learning with Multimodal Prototyping</h3>
<ul>
<li><strong>Authors: </strong>Huimin Zeng, Zhenrui Yue, Dong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01232">https://arxiv.org/abs/2404.01232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01232">https://arxiv.org/pdf/2404.01232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01232]] Open-Vocabulary Federated Learning with Multimodal Prototyping(https://arxiv.org/abs/2404.01232)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechanism. Fed-MP exploits the knowledge learned from the seen classes, and robustifies the adapted VLM to unseen categories. Our empirical evaluation on various datasets validates the effectiveness of Fed-MP.</li>
<li><strong>摘要：</strong>现有的联邦学习（FL）研究通常假设训练标签空间和测试标签空间是相同的。然而，在现实世界的应用中，这种假设过于理想，不可能成立。新用户可能会提出涉及未见过的类的数据的查询，而这种开放词汇表查询将直接缺陷此类 FL 系统。因此，在这项工作中，我们明确关注 FL 中尚未探索的开放词汇挑战。也就是说，对于新用户，全局服务器应理解她/他涉及任意未知类的查询。为了解决这个问题，我们利用预先训练的视觉语言模型（VLM）。特别是，我们提出了一种在 FL 背景下为 VLM 量身定制的新颖适应框架，称为联合多模态原型（Fed-MP）。 Fed-MP 基于轻量级客户端残差自适应聚合本地模型权重，并基于新颖的多模态原型机制进行预测。 Fed-MP 利用从已见类别中学到的知识，并将适应的 VLM 增强到未见过的类别。我们对各种数据集的实证评估验证了 Fed-MP 的有效性。</li>
</ul>

<h3>Title: Effectively Prompting Small-sized Language Models for Cross-lingual  Tasks via Winning Tickets</h3>
<ul>
<li><strong>Authors: </strong>Mingqi Li, Feng Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01242">https://arxiv.org/abs/2404.01242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01242">https://arxiv.org/pdf/2404.01242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01242]] Effectively Prompting Small-sized Language Models for Cross-lingual  Tasks via Winning Tickets(https://arxiv.org/abs/2404.01242)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts. The LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime. Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective. Then, we prepend soft prompts to the original pre-trained language model and only update the selected parameters together with prompt-related parameters when adapting to the downstream tasks. We verify the effectiveness of our LTP framework on cross-lingual tasks, specifically targeting low-resource languages. Our approach outperforms the baselines by only updating 20\% of the original parameters.</li>
<li><strong>摘要：</strong>当前的软提示方法在应用于小型模型（少于十亿个参数）时产生有限的性能。深度提示调整需要在每一层预先设置参数以增强效率，它提供了一种提示小型模型的解决方案，尽管需要精心设计的实现。在本文中，我们介绍了将中奖彩票与软提示集成在一起的彩票提示学习（LTP）框架。 LTP 提供了一种更简单的实现，并且只需要一次性执行。我们在跨语言任务上展示了 LTP，之前的工作依赖于外部工具，如人工设计的多语言模板和双语词典，这在资源匮乏的情况下可能不可行。具体来说，我们选择在使用掩码语言建模目标进行微调期间变化最大的参数子集。然后，我们将软提示添加到原始预训练语言模型中，并且在适应下游任务时仅更新所选参数以及与提示相关的参数。我们验证了 LTP 框架在跨语言任务上的有效性，特别是针对低资源语言。我们的方法仅更新 20% 的原始参数，性能优于基线。</li>
</ul>

<h3>Title: An image speaks a thousand words, but can everyone listen? On  translating images for cultural relevance</h3>
<ul>
<li><strong>Authors: </strong>Simran Khanuja, Sathyanarayanan Ramamoorthy, Yueqi Song, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01247">https://arxiv.org/abs/2404.01247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01247">https://arxiv.org/pdf/2404.01247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01247]] An image speaks a thousand words, but can everyone listen? On  translating images for cultural relevance(https://arxiv.org/abs/2404.01247)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://github.com/simran-khanuja/image-transcreation.</li>
<li><strong>摘要：</strong>鉴于多媒体内容的兴起，人工翻译人员越来越注重文化上的调整，不仅是文字，还有图像等其他形式，以传达相同的含义。虽然一些应用程序将从中受益，但机器翻译系统仍然仅限于处理语音和文本中的语言。在这项工作中，我们迈出了翻译图像以使其具有文化相关性的第一步。首先，我们构建三个由最先进的生成模型组成的管道来完成任务。接下来，我们构建一个由两部分组成的评估数据集：i) 概念：包含 600 个跨文化连贯的图像，重点关注每个图像的单个概念；ii) 应用程序：包含从现实世界应用程序中精选的 100 个图像。我们对翻译图像进行多方面的人工评估，以评估文化相关性和意义保存。我们发现，截至目前，图像编辑模型无法完成这项任务，但可以通过在循环中利用 LLM 和检索器来改进。最佳管道只能翻译较简单的概念数据集中某些国家/地区的 5% 图像，而应用程序数据集中的某些国家/地区没有翻译成功，这凸显了该任务的挑战性。我们的代码和数据发布在这里：https://github.com/simran-khanuja/image-transcreation。</li>
</ul>

<h3>Title: UniArk: Improving Generalisation and Consistency for Factual Knowledge  Extraction through Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Yijun Yang, Jie He, Pinzhen Chen, Víctor Gutiérrez-Basulto, Jeff Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01253">https://arxiv.org/abs/2404.01253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01253">https://arxiv.org/pdf/2404.01253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01253]] UniArk: Improving Generalisation and Consistency for Factual Knowledge  Extraction through Debiasing(https://arxiv.org/abs/2404.01253)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the inconsistency and out-of-domain generation of models. Further, ParaTrex offers a reference method for constructing paraphrased datasets using large language models.</li>
<li><strong>摘要：</strong>最近的几篇论文研究了语言模型作为知识库的潜力，以及提取事实知识时是否存在严重偏差。在这项工作中，我们重点关注调优过程中看不见的提示的事实探测性能，并使用概率视图展示了用于探测知识的语言模型中预训练和下游调优目标之间固有的不一致。我们假设同时消除这些目标的偏差可能是对看不见的提示进行概括的关键。我们提出了一个基于适配器的框架 UniArk，用于通过简单的方法提取通用且一致的事实知识，而无需引入额外的参数。大量实验表明，UniArk可以显着提高模型的域外泛化能力以及各种提示下的一致性。此外，我们构建了 ParaTrex，一个大规模且多样化的数据集，用于测量模型的不一致和域外生成。此外，ParaTrex 提供了一种使用大型语言模型构建释义数据集的参考方法。</li>
</ul>

<h3>Title: FABLES: Evaluating faithfulness and content selection in book-length  summarization</h3>
<ul>
<li><strong>Authors: </strong>Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01261">https://arxiv.org/abs/2404.01261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01261">https://arxiv.org/pdf/2404.01261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01261]] FABLES: Evaluating faithfulness and content selection in book-length  summarization(https://arxiv.org/abs/2404.01261)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.</li>
<li><strong>摘要：</strong>虽然长上下文大语言模型（LLM）可以在技术上总结书本长度的文档（> 100K token），但文档的长度和复杂性迄今为止禁止对依赖于输入的方面（如忠实度）进行评估。在本文中，我们对法学硕士生成的虚构书籍摘要进行了首次大规模的人类忠实度评估和内容选择。我们的研究通过关注 2023 年或 2024 年出版的书籍摘要来缓解数据污染问题，并且我们聘请在注释任务之前充分阅读每本书的注释者，以最大限度地减少成本和认知负担。我们收集了 FABLES，这是一个对 26 本书的 LLM 生成的摘要中的 3,158 条声明进行注释的数据集，花费了 5200 美元，这使我们能够根据忠实度对 LLM 总结者进行排名：Claude-3-Opus 显着优于所有封闭式 -源 LLM，而开源 Mixtral 与 GPT-3.5-Turbo 相当。对注释的分析表明，大多数不忠实的主张与事件和人物状态有关，并且它们通常需要对叙述进行间接推理才能无效。虽然基于 LLM 的自动评估器在其他环境中已被证明在事实性和连贯性方面是可靠的，但我们实施了几个 LLM 忠诚度评估器，发现没有一个与人类注释有很强的相关性，特别是在检测不忠实的声明方面。我们的实验表明，检测不忠实的主张不仅是摘要评估的重要未来方向，而且也是长上下文理解的测试平台。最后，我们通过探索全书摘要中的内容选择错误来超越忠实性：我们开发了与关键叙述元素相关的遗漏错误的类型，并且还确定了对本书结尾处发生的事件的系统性过度强调。</li>
</ul>

<h3>Title: Artificial Intelligence and the Spatial Documentation of Languages</h3>
<ul>
<li><strong>Authors: </strong>Hakam Ghanim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01263">https://arxiv.org/abs/2404.01263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01263">https://arxiv.org/pdf/2404.01263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01263]] Artificial Intelligence and the Spatial Documentation of Languages(https://arxiv.org/abs/2404.01263)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>The advancement in technology has made interdisciplinary research more accessible. Particularly the breakthrough in Artificial Intelligence AI has given huge advantages to researchers working in interdisciplinary and multidisciplinary fields. This study investigates the ability of AI models, particularly GPT4 and GPT Data Analyst in creating language maps for language documentation. The study Integrates documentary linguistics linguistic geography and AI by showcasing how AI models facilitate the spatial documentation of languages through the creation of language maps with minimal cartographic expertise. The study is conducted using a CSV file and a GeoJSON file both obtained from HDX and from the researchers fieldwork. The study data is then applied in realtime conversations with the AI models in order to generate the language distribution maps. The study highlights the two AI models capabilities in generating highquality static and interactive web maps and streamlining the mapmaking process, despite facing challenges like inconsistencies and difficulties in adding legends. The findings suggest a promising future for AI in generating language maps and enhancing the work of documentary linguists as they collect their data in the field pointing towards the need for further development to fully harness AI potential in this field.</li>
<li><strong>摘要：</strong>技术的进步使得跨学科研究变得更加容易。特别是人工智能AI的突破，给跨学科、多学科领域的研究人员带来了巨大的优势。本研究调查了 AI 模型，特别是 GPT4 和 GPT 数据分析师在为语言文档创建语言地图方面的能力。该研究将文献语言学、语言地理学和人工智能相结合，展示人工智能模型如何通过以最少的制图专业知识创建语言地图来促进语言的空间记录。该研究使用从 HDX 和研究人员实地工作获得的 CSV 文件和 GeoJSON 文件进行。然后将研究数据应用于与人工智能模型的实时对话，以生成语言分布图。该研究强调了两种人工智能模型在生成高质量静态和交互式网络地图以及简化地图制作过程方面的能力，尽管面临着不一致和添加图例困难等挑战。研究结果表明，人工智能在生成语言地图和加强文献语言学家在该领域收集数据的工作方面有着广阔的前景，这表明需要进一步开发以充分利用人工智能在该领域的潜力。</li>
</ul>

<h3>Title: Mapping the Increasing Use of LLMs in Scientific Papers</h3>
<ul>
<li><strong>Authors: </strong>Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01268">https://arxiv.org/abs/2404.01268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01268">https://arxiv.org/pdf/2404.01268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01268]] Mapping the Increasing Use of LLMs in Scientific Papers(https://arxiv.org/abs/2404.01268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.</li>
<li><strong>摘要：</strong>科学出版通过传播研究成果、促进合作、鼓励可重复性以及确保科学知识可获取、可验证和长期积累来奠定科学的基础。最近，人们纷纷猜测有多少人在学术写作中使用像 ChatGPT 这样的大型语言模型 (LLM)，以及该工具可能在多大程度上对全球科学实践产生影响。然而，我们缺乏对法学硕士大幅修改或创作的学术写作比例的精确衡量。为了解决这一差距，我们对 2020 年 1 月至 2024 年 2 月期间在 arXiv、bioRxiv 和 Nature 组合期刊上发表的 950,965 篇论文进行了首次系统性、大规模的分析，使用人口层面的统计框架来衡量 LLM 的流行程度。随着时间的推移修改内容。我们的统计估计在语料库级别上进行，比对单个实例的推断更稳健。我们的研究结果显示，LLM 的使用量稳步增长，其中计算机科学论文的增长幅度最大且最快（高达 17.5%）。相比之下，数学论文和自然作品集的 LLM 修改最少（高达 6.3%）。此外，在总体水平上，我们的分析表明，较高水平的法学硕士修改与第一作者更频繁地发表预印本的论文、更拥挤的研究领域的论文以及篇幅较短的论文相关。我们的研究结果表明，法学硕士正在科学著作中广泛使用。</li>
</ul>

<h3>Title: Large Language Models are Capable of Offering Cognitive Reappraisal, if  Guided</h3>
<ul>
<li><strong>Authors: </strong>Hongli Zhan, Allen Zheng, Yoon Kyung Lee, Jina Suh, Junyi Jessy Li, Desmond C. Ong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01288">https://arxiv.org/abs/2404.01288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01288">https://arxiv.org/pdf/2404.01288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01288]] Large Language Models are Capable of Offering Cognitive Reappraisal, if  Guided(https://arxiv.org/abs/2404.01288)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have offered new opportunities for emotional support, and recent work has shown that they can produce empathic responses to people in distress. However, long-term mental well-being requires emotional self-regulation, where a one-time empathic response falls short. This work takes a first step by engaging with cognitive reappraisals, a strategy from psychology practitioners that uses language to targetedly change negative appraisals that an individual makes of the situation; such appraisals is known to sit at the root of human emotional experience. We hypothesize that psychologically grounded principles could enable such advanced psychology capabilities in LLMs, and design RESORT which consists of a series of reappraisal constitutions across multiple dimensions that can be used as LLM instructions. We conduct a first-of-its-kind expert evaluation (by clinical psychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability to generate cognitive reappraisal responses to medium-length social media messages asking for support. This fine-grained evaluation showed that even LLMs at the 7B scale guided by RESORT are capable of generating empathic responses that can help users reappraise their situations.</li>
<li><strong>摘要：</strong>大语言模型（LLM）为情感支持提供了新的机会，最近的研究表明它们可以对陷入困境的人产生同理心反应。然而，长期的心理健康需要情绪的自我调节，而一次性的共情反应是不够的。这项工作迈出了第一步，涉及认知重新评估，这是心理学从业者的一种策略，利用语言有针对性地改变个人对情况的负面评估；众所周知，这种评价是人类情感体验的根源。我们假设基于心理学的原则可以在法学硕士中实现如此先进的心理学能力，并设计RESORT，它由一系列跨多个维度的重新评估构成组成，可以用作法学硕士指令。我们对法学硕士的零样本能力进行了首次专家评估（由拥有硕士或博士学位的临床心理学家进行），以对寻求支持的中等长度社交媒体消息产生认知重新评估反应。这种细粒度的评估表明，即使是 RESORT 指导下的 7B 级别的法学硕士也能够产生移情反应，帮助用户重新评估他们的情况。</li>
</ul>

<h3>Title: Towards Safety and Helpfulness Balanced Responses via Controllable Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yi-Lin Tuan, Xilun Chen, Eric Michael Smith, Louis Martin, Soumya Batra, Asli Celikyilmaz, William Yang Wang, Daniel M. Bikel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01295">https://arxiv.org/abs/2404.01295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01295">https://arxiv.org/pdf/2404.01295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01295]] Towards Safety and Helpfulness Balanced Responses via Controllable Large  Language Models(https://arxiv.org/abs/2404.01295)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.</li>
<li><strong>摘要：</strong>如今，随着大型语言模型 (LLM) 变得越来越容易访问，安全性和实用性之间的权衡可能会显着影响用户体验。优先考虑安全的模型将导致用户感觉参与度和帮助较少，而优先考虑帮助性则可能会造成伤害。可能的危害包括教人们如何制造炸弹、让青少年接触不适当的内容以及损害用户的心理健康。在这项工作中，我们建议通过控制法学硕士中的两个属性来平衡不同用例中的安全性和有用性。我们探索不需要额外人工注释的免培训和微调方法，并分析法学硕士控制安全性和有用性的挑战。我们的实验表明，我们的方法可以倒带学习模型并解锁其可控性。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
