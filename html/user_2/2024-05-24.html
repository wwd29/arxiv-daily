<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-24</h1>
<h3>Title: Title:
          An Assessment of Model-On-Model Deception</h3>
<ul>
<li><strong>Authors: </strong>Julius Heitkoetter, Michael Gerovitch, Laker Newhouse</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          An Assessment of Model-On-Model Deception(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>The trustworthiness of highly capable language models is put at risk when they are able to produce deceptive outputs. Moreover, when models are vulnerable to deception it undermines reliability. In this paper, we introduce a method to investigate complex, model-on-model deceptive scenarios. We create a dataset of over 10,000 misleading explanations by asking Llama-2 7B, 13B, 70B, and GPT-3.5 to justify the wrong answer for questions in the MMLU. We find that, when models read these explanations, they are all significantly deceived. Worryingly, models of all capabilities are successful at misleading others, while more capable models are only slightly better at resisting deception. We recommend the development of techniques to detect and defend against deception.</li>
<li><strong>摘要：</strong>当高性能语言模型能够产生欺骗性输出时，其可信度就会受到威胁。此外，当模型容易受到欺骗时，可靠性也会受到损害。在本文中，我们介绍了一种研究复杂模型对模型欺骗场景的方法。我们通过要求 Llama-2 7B、13B、70B 和 GPT-3.5 证明 MMLU 中问题的错误答案，创建了一个包含 10,000 多个误导性解释的数据集。我们发现，当模型阅读这些解释时，它们都会受到严重欺骗。令人担忧的是，所有能力的模型都能成功误导他人，而能力更强的模型在抵抗欺骗方面只略胜一筹。我们建议开发检测和防御欺骗的技术。</li>
</ul>

<h3>Title: Title:
          RAGE Against the Machine: Retrieval-Augmented LLM Explanations</h3>
<ul>
<li><strong>Authors: </strong>Joel Rorseth, Parke Godfrey, Lukasz Golab, Divesh Srivastava, Jaroslaw Szlichta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RAGE Against the Machine: Retrieval-Augmented LLM Explanations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper demonstrates RAGE, an interactive tool for explaining Large Language Models (LLMs) augmented with retrieval capabilities; i.e., able to query external sources and pull relevant information into their input context. Our explanations are counterfactual in the sense that they identify parts of the input context that, when removed, change the answer to the question posed to the LLM. RAGE includes pruning methods to navigate the vast space of possible explanations, allowing users to view the provenance of the produced answers.</li>
<li><strong>摘要：</strong>本文演示了 RAGE，一种交互式工具，用于解释具有检索功能的大型语言模型 (LLM)；即，能够查询外部源并将相关信息提取到其输入上下文中。我们的解释是反事实的，因为它们识别了输入上下文的某些部分，当删除这些部分时，会改变向法学硕士提出的问题的答案。 RAGE 包括修剪方法来导航可能的解释的广阔空间，允许用户查看生成的答案的出处。</li>
</ul>

<h3>Title: Title:
          Large Language Models for Education: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Hanyi Xu, Wensheng Gan, Zhenlian Qi, Jiayang Wu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models for Education: A Survey(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) has a profound impact on traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, computer vision, speech recognition, and autonomous driving. LLMs have also been applied in many fields, including recommendation, finance, government, education, legal affairs, and finance. As powerful auxiliary tools, LLMs incorporate various technologies such as deep learning, pre-training, fine-tuning, and reinforcement learning. The use of LLMs for smart education (LLMEdu) has been a significant strategic direction for countries worldwide. While LLMs have shown great promise in improving teaching quality, changing education models, and modifying teacher roles, the technologies are still facing several challenges. In this paper, we conduct a systematic review of LLMEdu, focusing on current technologies, challenges, and future developments. We first summarize the current state of LLMEdu and then introduce the characteristics of LLMs and education, as well as the benefits of integrating LLMs into education. We also review the process of integrating LLMs into the education industry, as well as the introduction of related technologies. Finally, we discuss the challenges and problems faced by LLMEdu, as well as prospects for future optimization of LLMEdu.</li>
<li><strong>摘要：</strong>人工智能（AI）对传统教育产生深远影响。近年来，大语言模型（LLM）越来越多地应用于自然语言处理、计算机视觉、语音识别和自动驾驶等各种应用中。 LLM也被应用于许多领域，包括推荐、金融、政府、教育、法律事务和金融。作为强大的辅助工具，法学硕士融合了深度学习、预训练、微调、强化学习等多种技术。利用法学硕士进行智慧教育（LLMEdu）一直是世界各国的重要战略方向。尽管法学硕士在提高教学质量、改变教育模式和改变教师角色方面表现出了巨大的希望，但这些技术仍然面临着一些挑战。在本文中，我们对 LLMEdu 进行了系统回顾，重点关注当前技术、挑战和未来发展。我们首先总结了LLMEdu的现状，然后介绍了LLM和教育的特点，以及LLM融入教育的好处。我们还回顾了LLM融入教育行业的过程，以及相关技术的引入。最后，我们讨论了LLMEdu面临的挑战和问题，以及LLMEdu未来优化的展望。</li>
</ul>

<h3>Title: Title:
          DuetRAG: Collaborative Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DuetRAG: Collaborative Retrieval-Augmented Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 方法通过相关检索段落增强大型语言模型 (LLM) 的输入，从而减少知识密集型任务中的事实错误。然而，由于缺乏相应的领域知识，当代 RAG 方法在复杂领域问题（例如 HotPot QA）中遇到了不相关的知识检索问题，导致生成质量低下。为了解决这个问题，我们提出了一种新颖的协作检索增强生成框架，DuetRAG。我们的 bootstrapping 理念是同时集成领域优化和 RAG 模型，以提高知识检索质量，从而提高生成质量。最后，我们在 HotPot QA 上演示 DuetRAG 与人类专家研究人员的匹配。</li>
</ul>

<h3>Title: Title:
          MathDivide: Improved mathematical reasoning by large language models</h3>
<ul>
<li><strong>Authors: </strong>Saksham Sahai Srivastava, Ashutosh Gandhi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MathDivide: Improved mathematical reasoning by large language models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models have been proven to be capable of handling complex linguistic and cognitive tasks. Therefore their usage has been extended to tasks requiring logical reasoning ability such as Mathematics. In this paper, we propose a prompting technique called MathDivide that breaks down the mathematical problem into simpler subproblems. Each of the subproblems is formulated as an algebraic expression whose value is evaluated by the Python code generated by the LLM for the corresponding algebraic expression. The values fed to the Python code are the numerical values provided in the problem statement. The solutions for the subproblems are composed together to obtain the final answer for the problem statement. Finally, the final answer is compared to the correct answer. If the final answer matches the correct answer, it is produced as output else a refinement prompt is fed to the LLM. We experiment with this prompting technique on both closed-source LLM models and open-source LLM models using GSM8K dataset. The results obtained demonstrate that MathDivide was able to significantly outperform the leading prompting technique called Math-prompter.</li>
<li><strong>摘要：</strong>大型语言模型已被证明能够处理复杂的语言和认知任务。因此，它们的用途已扩展到需要逻辑推理能力的任务，例如数学。在本文中，我们提出了一种名为 MathDivide 的提示技术，它将数学问题分解为更简单的子问题。每个子问题都被表述为一个代数表达式，其值由 LLM 为相应代数表达式生成的 Python 代码计算。提供给 Python 代码的值是问题陈述中提供的数值。将子问题的解决方案组合在一起以获得问题陈述的最终答案。最后将最终答案与正确答案进行比较。如果最终答案与正确答案匹配，则会将其作为输出生成，否则将向 LLM 提供细化提示。我们使用 GSM8K 数据集在闭源 LLM 模型和开源 LLM 模型上试验了这种提示技术。获得的结果表明 MathDivide 能够显着优于称为 Math-prompter 的领先提示技术。</li>
</ul>

<h3>Title: Title:
          Understanding the Rare Inflammatory Disease Using Large Language Models and Social Media Data</h3>
<ul>
<li><strong>Authors: </strong>Nan Miles Xi, Hong-Long Ji, Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Understanding the Rare Inflammatory Disease Using Large Language Models and Social Media Data(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Sarcoidosis is a rare inflammatory disease characterized by the formation of granulomas in various organs. The disease presents diagnostic and treatment challenges due to its diverse manifestations and unpredictable nature. In this study, we employed a Large Language Model (LLM) to analyze sarcoidosis-related discussions on the social media platform Reddit. Our findings underscore the efficacy of LLMs in accurately identifying sarcoidosis-related content. We discovered a wide array of symptoms reported by patients, with fatigue, swollen lymph nodes, and shortness of breath as the most prevalent. Prednisone was the most prescribed medication, while infliximab showed the highest effectiveness in improving prognoses. Notably, our analysis revealed disparities in prognosis based on age and gender, with women and younger patients experiencing good and polarized outcomes, respectively. Furthermore, unsupervised clustering identified three distinct patient subgroups (phenotypes) with unique symptom profiles, prognostic outcomes, and demographic distributions. Finally, sentiment analysis revealed a moderate negative impact on patients' mental health post-diagnosis, particularly among women and younger individuals. Our study represents the first application of LLMs to understand sarcoidosis through social media data. It contributes to understanding the disease by providing data-driven insights into its manifestations, treatments, prognoses, and impact on patients' lives. Our findings have direct implications for improving personalized treatment strategies and enhancing the quality of care for individuals living with sarcoidosis.</li>
<li><strong>摘要：</strong>结节病是一种罕见的炎症性疾病，其特征是各种器官中形成肉芽肿。由于该疾病的表现形式多样且性质难以预测，因此诊断和治疗都具有挑战性。在本研究中，我们采用大型语言模型 (LLM) 分析社交媒体平台 Reddit 上与结节病相关的讨论。我们的研究结果强调了 LLM 在准确识别结节病相关内容方面的有效性。我们发现患者报告的症状多种多样，其中最常见的是疲劳、淋巴结肿大和呼吸急促。泼尼松是最常用的处方药，而英夫利昔单抗在改善预后方面表现出最高的效果。值得注意的是，我们的分析揭示了基于年龄和性别的预后差异，女性和年轻患者分别经历了良好和两极分化的结果。此外，无监督聚类确定了三个不同的患者亚组（表型），具有独特的症状特征、预后结果和人口分布。最后，情绪分析显示，诊断后患者心理健康受到中等程度的负面影响，尤其是在女性和年轻人中。我们的研究代表了 LLM 首次通过社交媒体数据了解结节病。它通过提供数据驱动的洞察力，了解疾病的表现、治疗、预后和对患者生活的影响，有助于了解疾病。我们的研究结果对改善个性化治疗策略和提高结节病患者的护理质量具有直接影响。</li>
</ul>

<h3>Title: Title:
          News Recommendation with Category Description by a Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yuki Yada, Hayato Yamana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          News Recommendation with Category Description by a Large Language Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Personalized news recommendations are essential for online news platforms to assist users in discovering news articles that match their interests from a vast amount of online content. Appropriately encoded content features, such as text, categories, and images, are essential for recommendations. Among these features, news categories, such as tv-golden-globe, finance-real-estate, and news-politics, play an important role in understanding news content, inspiring us to enhance the categories' descriptions. In this paper, we propose a novel method that automatically generates informative category descriptions using a large language model (LLM) without manual effort or domain-specific knowledge and incorporates them into recommendation models as additional information. In our comprehensive experimental evaluations using the MIND dataset, our method successfully achieved 5.8% improvement at most in AUC compared with baseline approaches without the LLM's generated category descriptions for the state-of-the-art content-based recommendation models including NAML, NRMS, and NPA. These results validate the effectiveness of our approach. The code is available at this https URL.</li>
<li><strong>摘要：</strong>个性化的新闻推荐对于网络新闻平台来说至关重要，帮助用户从海量的网络内容中发现符合自己兴趣的新闻文章。适当编码的内容特征（例如文本、类别和图像）对于推荐至关重要。其中，新闻类别，如电视-金球奖、金融-房地产、新闻-政治等，在理解新闻内容方面发挥着重要作用，启发我们加强类别的描述。在本文中，我们提出了一种新颖的方法，该方法可以使用大型语言模型（LLM）自动生成信息丰富的类别描述，而无需手动操作或特定领域的知识，并将它们作为附加信息合并到推荐模型中。在我们使用 MIND 数据集进行的综合实验评估中，与没有法学硕士为最先进的基于内容的推荐模型（包括 NAML、NRMS、和NPA。这些结果验证了我们方法的有效性。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Title:
          Control Token with Dense Passage Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Juhwan Lee, Jisu Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Control Token with Dense Passage Retrieval(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This study addresses the hallucination problem in large language models (LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a technique that involves embedding relevant information in the prompt to obtain accurate answers. However, RAG also faced inherent issues in retrieving correct information. To address this, we employed the Dense Passage Retrieval(DPR) (Karpukhin et al., 2020) model for fetching domain-specific documents related to user queries. Despite this, the DPR model still lacked accuracy in document retrieval. We enhanced the DPR model by incorporating control tokens, achieving significantly superior performance over the standard DPR model, with a 13% improvement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.</li>
<li><strong>摘要：</strong>这项研究解决了大型语言模型（LLM）中的幻觉问题。我们采用了检索增强生成（RAG）（Lewis et al., 2020），这种技术涉及在提示中嵌入相关信息以获得准确的答案。然而，RAG 在检索正确信息方面也面临着固有的问题。为了解决这个问题，我们采用密集通道检索（DPR）（Karpukhin 等人，2020）模型来获取与用户查询相关的特定领域文档。尽管如此，DPR模型在文档检索方面仍然缺乏准确性。我们通过合并控制令牌增强了 DPR 模型，实现了比标准 DPR 模型显着优越的性能，Top-1 准确率提高了 13%，Top-20 准确率提高了 4%。</li>
</ul>

<h3>Title: Title:
          METAREFLECTION: Learning Instructions for Language Agents using Past Reflections</h3>
<ul>
<li><strong>Authors: </strong>Priyanshu Gupta, Shashank Kirtania, Ananya Singha, Sumit Gulwani, Arjun Radhakrishna, Sherry Shi, Gustavo Soares</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          METAREFLECTION: Learning Instructions for Language Agents using Past Reflections(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Despite the popularity of Large Language Models (LLMs), crafting specific prompts for LLMs to perform particular tasks remains challenging. Users often engage in multiple conversational turns with an LLM-based agent to accomplish their intended task. Recent studies have demonstrated that linguistic feedback, in the form of self-reflections generated by the model, can work as reinforcement during these conversations, thus enabling quicker convergence to the desired outcome. Motivated by these findings, we introduce METAREFLECTION, a novel technique that learns general prompt instructions for a specific domain from individual self-reflections gathered during a training phase. We evaluate our technique in two domains: Infrastructure as Code (IAC) vulnerability detection and question-answering (QA) using REACT and COT. Our results demonstrate a notable improvement, with METARELECTION outperforming GPT-4 by 16.82% (IAC), 31.33% (COT), and 15.42% (REACT), underscoring the potential of METAREFLECTION as a viable method for enhancing the efficiency of LLMs.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 很受欢迎，但为 LLM 执行特定任务制定具体提示仍然具有挑战性。用户经常与基于法学硕士的代理进行多次对话，以完成他们的预期任务。最近的研究表明，模型生成的自我反思形式的语言反馈可以在这些对话中起到强化作用，从而能够更快地收敛到期望的结果。受这些发现的启发，我们引入了 METAREFLECTION，这是一种新技术，可以从训练阶段收集的个人自我反思中学习特定领域的一般提示指令。我们在两个领域评估我们的技术：基础设施即代码 (IAC) 漏洞检测和使用 REACT 和 COT 的问答 (QA)。我们的结果显示出显着的改进，METAREFLECTION 的性能比 GPT-4 高出 16.82% (IAC)、31.33% (COT) 和 15.42% (REACT)，这凸显了 METAREFLECTION 作为提高 LLM 效率的可行方法的潜力。</li>
</ul>

<h3>Title: Title:
          UCCIX: Irish-eXcellence Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          UCCIX: Irish-eXcellence Large Language Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The development of Large Language Models (LLMs) has predominantly focused on high-resource languages, leaving extremely low-resource languages like Irish with limited representation. This work presents UCCIX, a pioneering effort on the development of an open-source Irish-based LLM. We propose a novel framework for continued pre-training of LLMs specifically adapted for extremely low-resource languages, requiring only a fraction of the textual data typically needed for training LLMs according to scaling laws. Our model, based on Llama 2-13B, outperforms much larger models on Irish language tasks with up to 12% performance improvement, showcasing the effectiveness and efficiency of our approach. We also contribute comprehensive Irish benchmarking datasets, including IrishQA, a question-answering dataset, and Irish version of MT-bench. These datasets enable rigorous evaluation and facilitate future research in Irish LLM systems. Our work aims to preserve and promote the Irish language, knowledge, and culture of Ireland in the digital era while providing a framework for adapting LLMs to other indigenous languages.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的开发主要集中在高资源语言上，而爱尔兰语等资源极少的语言的代表性有限。这项工作介绍了 UCCIX，这是开发爱尔兰开源法学硕士的一项开创性努力。我们提出了一种新的框架，用于继续对法学硕士进行预训练，专门适用于资源极少的语言，仅需要根据缩放法则训练法学硕士通常所需的文本数据的一小部分。我们的模型基于 Llama 2-13B，在爱尔兰语言任务上的性能优于更大的模型，性能提升高达 12%，展示了我们方法的有效性和效率。我们还贡献了全面的爱尔兰基准测试数据集，包括 IrishQA、问答数据集和爱尔兰版本的 MT-bench。这些数据集可以进行严格的评估，并促进爱尔兰法学硕士系统的未来研究。我们的工作旨在在数字时代保护和促进爱尔兰的爱尔兰语言、知识和文化，同时为法学硕士适应其他土著语言提供框架。</li>
</ul>

<h3>Title: Title:
          Divergent Creativity in Humans and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Antoine Bellemare-Pepin (1 and 2), François Lespinasse (3), Philipp Thölke (1), Yann Harel (1), Kory Mathewson (4), Jay A. Olson (5), Yoshua Bengio (4 and 6), Karim Jerbi (1, 4 and 7) ((1) CoCo Lab, Psychology department, Université de Montréal, Montreal, QC, Canada, (2) Music department, Concordia University, Montreal, QC, Canada, (3) Sociology and Anthropology department, Concordia University, Montreal, QC, Canada, (4) Mila (Quebec AI research Institute), Montreal, QC, Canada, (5) Department of Psychology, University of Toronto Mississauga, Mississauga, ON, Canada, (6) Department of Computer Science and Operations Research, Université de Montréal, Montreal, QC, Canada, (7) UNIQUE Center (Quebec Neuro-AI research Center), QC, Canada)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Divergent Creativity in Humans and Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The recent surge in the capabilities of Large Language Models (LLMs) has led to claims that they are approaching a level of creativity akin to human capabilities. This idea has sparked a blend of excitement and apprehension. However, a critical piece that has been missing in this discourse is a systematic evaluation of LLM creativity, particularly in comparison to human divergent thinking. To bridge this gap, we leverage recent advances in creativity science to build a framework for in-depth analysis of divergent creativity in both state-of-the-art LLMs and a substantial dataset of 100,000 humans. We found evidence suggesting that LLMs can indeed surpass human capabilities in specific creative tasks such as divergent association and creative writing. Our quantitative benchmarking framework opens up new paths for the development of more creative LLMs, but it also encourages more granular inquiries into the distinctive elements that constitute human inventive thought processes, compared to those that can be artificially generated.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）能力的激增导致人们声称它们正在接近类似于人类能力的创造力水平。这个想法引起了人们的兴奋和担忧。然而，本文中缺失的一个关键部分是对法学硕士创造力的系统评估，特别是与人类发散思维的比较。为了弥补这一差距，我们利用创造力科学的最新进展构建了一个框架，用于深入分析最先进的法学硕士和 100,000 人的大量数据集的不同创造力。我们发现的证据表明，法学硕士确实可以在特定的创造性任务（例如发散性联想和创造性写作）中超越人类的能力。我们的定量基准框架为更具创造性的法学硕士的发展开辟了新的道路，但与人工生成的元素相比，它也鼓励对构成人类创造性思维过程的独特元素进行更细致的探究。</li>
</ul>

<h3>Title: Title:
          QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Wang, Zhaowei Li, Qi Xu, Yiqing Cai, Hang Song, Qi Qi, Ran Zhou, Zhida Huang, Tao Wang, Li Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) poses challenges in terms of resource limitations and inference efficiency. To address these challenges, recent research has focused on using smaller task-specific language models, which are enhanced by distilling the knowledge rationales generated by LLMs. However, previous works mostly emphasize the effectiveness of positive knowledge, while overlooking the knowledge noise and the exploration of negative knowledge. In this paper, we first propose a general approach called quality-guided contrastive rationale distillation for reasoning capacity learning, considering contrastive learning perspectives. For the learning of positive knowledge, we collect positive rationales through self-consistency to denoise the LLM rationales generated by temperature sampling. For the negative knowledge distillation, we generate negative rationales using temperature sampling for the iteration-before smaller language models themselves. Finally, a contrastive loss is designed to better distill the positive and negative rationales into the smaller language model, where an online-update discriminator is used to judge the qualities of rationales and assign weights for better optimizing the training process. Through extensive experiments on multiple reasoning tasks, we demonstrate that our method consistently outperforms the previous distillation methods and produces higher-quality rationales.</li>
<li><strong>摘要：</strong>部署大型语言模型 (LLM) 在资源限制和推理效率方面面临挑战。为了应对这些挑战，最近的研究集中于使用较小的任务特定语言模型，这些模型通过提炼 LLM 生成的知识原理得到增强。然而，以前的研究大多强调积极知识的有效性，而忽视了知识噪音和消极知识的探索。在本文中，我们首先提出了一种称为质量引导对比原理提炼的通用方法，用于推理能力学习，考虑对比学习的观点。对于积极知识的学习，我们通过自洽收集积极原理，以对温度采样生成的 LLM 原理进行去噪。对于负面知识提炼，我们使用温度采样为迭代之前较小的语言模型本身生成负面原理。最后，设计对比损失以更好地将积极和消极原理提炼到较小的语言模型中，其中使用在线更新鉴别器来判断原理的质量并分配权重以更好地优化训练过程。通过对多个推理任务进行大量实验，我们证明我们的方法始终优于以前的提炼方法并产生更高质量的理论依据。</li>
</ul>

<h3>Title: Title:
          Assisted Debate Builder with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Elliot Faugier, Frédéric Armetta, Angela Bonifati, Bruno Yun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Assisted Debate Builder with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce ADBL2, an assisted debate builder tool. It is based on the capability of large language models to generalise and perform relation-based argument mining in a wide-variety of domains. It is the first open-source tool that leverages relation-based mining for (1) the verification of pre-established relations in a debate and (2) the assisted creation of new arguments by means of large language models. ADBL2 is highly modular and can work with any open-source large language models that are used as plugins. As a by-product, we also provide the first fine-tuned Mistral-7B large language model for relation-based argument mining, usable by ADBL2, which outperforms existing approaches for this task with an overall F1-score of 90.59% across all domains.</li>
<li><strong>摘要：</strong>我们推出 ADBL2，一种辅助辩论生成器工具。它基于大型语言模型在各种领域中泛化和执行基于关系的参数挖掘的能力。它是第一个利用基于关系的挖掘来实现（1）验证辩论中预先建立的关系以及（2）通过大型语言模型辅助创建新论点的开源工具。 ADBL2 是高度模块化的，可以与任何用作插件的开源大型语言模型一起使用。作为副产品，我们还提供了第一个经过微调的 Mistral-7B 大型语言模型，用于基于关系的参数挖掘，可供 ADBL2 使用，该模型优于该任务的现有方法，在所有领域的总体 F1 分数为 90.59% 。</li>
</ul>

<h3>Title: Title:
          A Systematic Analysis on the Temporal Generalization of Language Models in Social Media</h3>
<ul>
<li><strong>Authors: </strong>Asahi Ushio, Jose Camacho-Collados</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Systematic Analysis on the Temporal Generalization of Language Models in Social Media(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time, and they can become obsolete due to the dynamism and evolving nature of online content. This paper focuses on temporal shifts in social media and, in particular, Twitter. We propose a unified evaluation scheme to assess the performance of language models (LMs) under temporal shift on standard social media tasks. LMs are tested on five diverse social media NLP tasks under different temporal settings, which revealed two important findings: (i) the decrease in performance under temporal shift is consistent across different models for entity-focused tasks such as named entity recognition or disambiguation, and hate speech detection, but not significant in the other tasks analysed (i.e., topic and sentiment classification); and (ii) continuous pre-training on the test period does not improve the temporal adaptability of LMs.</li>
<li><strong>摘要：</strong>在机器学习中，当训练和测试划分在时间上存在差异时，就会发生时间变化。对于新闻或社交媒体等流数据，模型通常是在一段时间内的固定语料库上进行训练的，并且由于在线内容的动态性和不断发展的性质，它们可能会变得过时。本文重点关注社交媒体（尤其是 Twitter）的时间变化。我们提出了一个统一的评估方案来评估语言模型（LM）在标准社交媒体任务的时间转换下的表现。在不同时间设置下的五种不同的社交媒体 NLP 任务上对 LM 进行了测试，结果揭示了两个重要的发现：（i）在以实体为中心的任务（例如命名实体识别或消歧）的不同模型中，时间转移下的性能下降是一致的，并且仇恨言论检测，但在其他分析任务（即主题和情感分类）中并不重要； (ii) 在测试期间持续进行预训练并不能提高 LM 的时间适应性。</li>
</ul>

<h3>Title: Title:
          A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Khoshnoodi, Vinija Jain, Mingye Gao, Malavika Srikanth, Aman Chadha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the crucial importance of accelerating text generation in large language models (LLMs) for efficiently producing content, the sequential nature of this process often leads to high inference latency, posing challenges for real-time applications. Various techniques have been proposed and developed to address these challenges and improve efficiency. This paper presents a comprehensive survey of accelerated generation techniques in autoregressive language models, aiming to understand the state-of-the-art methods and their applications. We categorize these techniques into several key areas: speculative decoding, early exiting mechanisms, and non-autoregressive methods. We discuss each category's underlying principles, advantages, limitations, and recent advancements. Through this survey, we aim to offer insights into the current landscape of techniques in LLMs and provide guidance for future research directions in this critical area of natural language processing.</li>
<li><strong>摘要：</strong>尽管加速大型语言模型 (LLM) 中的文本生成对于高效生成内容至关重要，但该过程的顺序性通常会导致较高的推理延迟，对实时应用构成挑战。已经提出和开发了各种技术来应对这些挑战并提高效率。本文全面介绍了自回归语言模型中的加速生成技术，旨在了解最先进的方法及其应用。我们将这些技术分为几个关键领域：推测解码、早期退出机制和非自回归方法。我们讨论了每个类别的基本原理、优势、局限性和最新进展。通过这次调查，我们旨在深入了解 LLM 中当前的技术状况，并为这一自然语言处理关键领域的未来研究方向提供指导。</li>
</ul>

<h3>Title: Title:
          Using Combinatorial Optimization to Design a High quality LLM Solution</h3>
<ul>
<li><strong>Authors: </strong>Samuel Ackerman, Eitan Farchi, Rami Katan, Orna Raz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Using Combinatorial Optimization to Design a High quality LLM Solution(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>We introduce a novel LLM based solution design approach that utilizes combinatorial optimization and sampling. Specifically, a set of factors that influence the quality of the solution are identified. They typically include factors that represent prompt types, LLM inputs alternatives, and parameters governing the generation and design alternatives. Identifying the factors that govern the LLM solution quality enables the infusion of subject matter expert knowledge. Next, a set of interactions between the factors are defined and combinatorial optimization is used to create a small subset $P$ that ensures all desired interactions occur in $P$. Each element $p \in P$ is then developed into an appropriate benchmark. Applying the alternative solutions on each combination, $p \in P$ and evaluating the results facilitate the design of a high quality LLM solution pipeline. The approach is especially applicable when the design and evaluation of each benchmark in $P$ is time-consuming and involves manual steps and human evaluation. Given its efficiency the approach can also be used as a baseline to compare and validate an autoML approach that searches over the factors governing the solution.</li>
<li><strong>摘要：</strong>我们引入了一种新颖的基于 LLM 的解决方案设计方法，该方法利用组合优化和采样。具体来说，确定了一组影响解决方案质量的因素。它们通常包括代表提示类型的因素、LLM 输入替代方案以及控制生成和设计替代方案的参数。确定控制 LLM 解决方案质量的因素可以注入主题专家知识。接下来，定义因素之间的一组相互作用，并使用组合优化来创建一个小子集 $P$，以确保所有所需的相互作用都发生在 $P$ 中。然后将 P$ 中的每个元素 $p \in 开发为适当的基准。将替代解决方案应用于每个组合 $p \in P$ 并评估结果有助于设计高质量的 LLM 解决方案管道。当 $P$ 中每个基准的设计和评估非常耗时并且涉及手动步骤和人工评估时，该方法尤其适用。鉴于其效率，该方法还可以用作比较和验证自动机器学习方法的基线，该方法搜索控制解决方案的因素。</li>
</ul>

<h3>Title: Title:
          IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues</h3>
<ul>
<li><strong>Authors: </strong>Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Although the Retrieval-Augmented Generation (RAG) paradigms can use external knowledge to enhance and ground the outputs of Large Language Models (LLMs) to mitigate generative hallucinations and static knowledge base problems, they still suffer from limited flexibility in adopting Information Retrieval (IR) systems with varying capabilities, constrained interpretability during the multi-round retrieval process, and a lack of end-to-end optimization. To address these challenges, we propose a novel LLM-centric approach, IM-RAG, that integrates IR systems with LLMs to support multi-round RAG through learning Inner Monologues (IM, i.e., the human inner voice that narrates one's thoughts). During the IM process, the LLM serves as the core reasoning model (i.e., Reasoner) to either propose queries to collect more information via the Retriever or to provide a final answer based on the conversational context. We also introduce a Refiner that improves the outputs from the Retriever, effectively bridging the gap between the Reasoner and IR modules with varying capabilities and fostering multi-round communications. The entire IM process is optimized via Reinforcement Learning (RL) where a Progress Tracker is incorporated to provide mid-step rewards, and the answer prediction is further separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive experiments with the HotPotQA dataset, a popular benchmark for retrieval-based, multi-step question-answering. The results show that our approach achieves state-of-the-art (SOTA) performance while providing high flexibility in integrating IR modules as well as strong interpretability exhibited in the learned inner monologues.</li>
<li><strong>摘要：</strong>尽管检索增强生成（RAG）范式可以使用外部知识来增强和基础大型语言模型（LLM）的输出，以减轻生成幻觉和静态知识库问题，但它们在采用信息检索（IR）方面仍然受到灵活性有限的影响系统具有不同的功能，多轮检索过程中的可解释性受到限制，并且缺乏端到端优化。为了应对这些挑战，我们提出了一种以法学硕士为中心的新颖方法 IM-RAG，它将 IR 系统与法学硕士相结合，通过学习内心独白（IM，即人类内心的声音来叙述自己的想法）来支持多轮 RAG。在 IM 过程中，LLM 充当核心推理模型（即 Reasoner），通过检索器提出查询以收集更多信息，或者根据对话上下文提供最终答案。我们还引入了 Refiner，它可以提高 Retriever 的输出，有效地弥合 Reasoner 和具有不同功能的 IR 模块之间的差距，并促进多轮通信。整个 IM 过程通过强化学习 (RL) 进行优化，其中包含进度跟踪器以提供中间步骤奖励，并且答案预测通过监督微调 (SFT) 进一步单独优化。我们使用 HotPotQA 数据集进行了广泛的实验，这是基于检索的多步骤问答的流行基准。结果表明，我们的方法实现了最先进的 (SOTA) 性能，同时在集成 IR 模块方面提供了高度灵活性，并且在学习的内心独白中表现出了很强的可解释性。</li>
</ul>

<h3>Title: Title:
          LLMs can learn self-restraint through iterative self-reflection</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Piché, Aristides Milios, Dzmitry Bahdanau, Chris Pal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLMs can learn self-restraint through iterative self-reflection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>In order to be deployed safely, Large Language Models (LLMs) must be capable of dynamically adapting their behavior based on their level of knowledge and uncertainty associated with specific topics. This adaptive behavior, which we refer to as self-restraint, is non-trivial to teach since it depends on the internal knowledge of an LLM. By default, LLMs are trained to maximize the next token likelihood, which does not teach the model to modulate its answer based on its level of uncertainty. In order to learn self-restraint, we devise a utility function that can encourage the model to produce responses only when it is confident in them. This utility function can be used to score generation of different length and abstention. To optimize this function, we introduce ReSearch, a process of ``self-reflection'' consisting of iterative self-prompting and self-evaluation. We use the ReSearch algorithm to generate synthetic data on which we finetune our models. Compared to their original versions, our resulting models generate fewer \emph{hallucinations} overall at no additional inference cost, for both known and unknown topics, as the model learns to selectively restrain itself. In addition, our method elegantly incorporates the ability to abstain by augmenting the samples generated by the model during the search procedure with an answer expressing abstention.</li>
<li><strong>摘要：</strong>为了安全部署，大型语言模型 (LLM) 必须能够根据其知识水平和与特定主题相关的不确定性动态调整其行为。这种适应性行为，我们称之为自我约束，教学起来并不简单，因为它取决于法学硕士的内部知识。默认情况下，LLM 被训练为最大化下一个标记的可能性，这不会教导模型根据其不确定性水平来调整其答案。为了学习自我约束，我们设计了一个效用函数，可以鼓励模型仅在对响应有信心时才产生响应。该效用函数可用于对不同长度和弃权的生成进行评分。为了优化这个功能，我们引入了ReSearch，一个由迭代的自我提示和自我评估组成的“自我反思”过程。我们使用 ReSearch 算法生成合成数据，并根据这些数据微调我们的模型。与原始版本相比，我们的结果模型在没有额外推理成本的情况下，对于已知和未知的主题，总体上产生了更少的 \emph{幻觉}，因为模型学会了选择性地约束自己。此外，我们的方法巧妙地结合了弃权的能力，通过在搜索过程中增加模型生成的样本并表达弃权的答案。</li>
</ul>

<h3>Title: Title:
          Intelligent Tutor: Leveraging ChatGPT and Microsoft Copilot Studio to Deliver a Generative AI Student Support and Feedback System within Teams</h3>
<ul>
<li><strong>Authors: </strong>Wei-Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Intelligent Tutor: Leveraging ChatGPT and Microsoft Copilot Studio to Deliver a Generative AI Student Support and Feedback System within Teams(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>This study explores the integration of the ChatGPT API with GPT-4 model and Microsoft Copilot Studio on the Microsoft Teams platform to develop an intelligent tutoring system. Designed to provide instant support to students, the system dynamically adjusts educational content in response to the learners' progress and feedback. Utilizing advancements in natural language processing and machine learning, it interprets student inquiries, offers tailored feedback, and facilitates the educational journey. Initial implementation highlights the system's potential in boosting students' motivation and engagement, while equipping educators with critical insights into the learning process, thus promoting tailored educational experiences and enhancing instructional effectiveness.</li>
<li><strong>摘要：</strong>本研究探讨了在 Microsoft Teams 平台上将 ChatGPT API 与 GPT-4 模型和 Microsoft Copilot Studio 集成来开发智能辅导系统。该系统旨在为学生提供即时支持，根据学习者的进步和反馈动态调整教育内容。它利用自然语言处理和机器学习方面的进步，解释学生的疑问，提供量身定制的反馈，并促进教育之旅。初步实施凸显了该系统在提高学生积极性和参与度方面的潜力，同时为教育工作者提供对学习过程的批判性见解，从而促进量身定制的教育体验并提高教学效果。</li>
</ul>

<h3>Title: Title:
          A survey on fairness of large language models in e-commerce: progress, application, and challenge</h3>
<ul>
<li><strong>Authors: </strong>Qingyang Ren, Zilin Jiang, Jinghan Cao, Sijia Li, Chiqu Li, Yiyang Liu, Shuning Huo, Tiange He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A survey on fairness of large language models in e-commerce: progress, application, and challenge(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This survey explores the fairness of large language models (LLMs) in e-commerce, examining their progress, applications, and the challenges they face. LLMs have become pivotal in the e-commerce domain, offering innovative solutions and enhancing customer experiences. This work presents a comprehensive survey on the applications and challenges of LLMs in e-commerce. The paper begins by introducing the key principles underlying the use of LLMs in e-commerce, detailing the processes of pretraining, fine-tuning, and prompting that tailor these models to specific needs. It then explores the varied applications of LLMs in e-commerce, including product reviews, where they synthesize and analyze customer feedback; product recommendations, where they leverage consumer data to suggest relevant items; product information translation, enhancing global accessibility; and product question and answer sections, where they automate customer support. The paper critically addresses the fairness challenges in e-commerce, highlighting how biases in training data and algorithms can lead to unfair outcomes, such as reinforcing stereotypes or discriminating against certain groups. These issues not only undermine consumer trust, but also raise ethical and legal concerns. Finally, the work outlines future research directions, emphasizing the need for more equitable and transparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate biases and improve the fairness of these systems, ensuring they serve diverse global markets effectively and ethically. Through this comprehensive analysis, the survey provides a holistic view of the current landscape of LLMs in e-commerce, offering insights into their potential and limitations, and guiding future endeavors in creating fairer and more inclusive e-commerce environments.</li>
<li><strong>摘要：</strong>这项调查探讨了电子商务中大型语言模型 (LLM) 的公平性，研究了它们的进展、应用以及它们面临的挑战。法学硕士已成为电子商务领域的关键，提供创新的解决方案并增强客户体验。这项工作对法学硕士在电子商务中的应用和挑战进行了全面的调查。本文首先介绍了法学硕士在电子商务中使用的关键原则，详细介绍了根据特定需求定制这些模型的预训练、微调和提示的过程。然后探讨了法学硕士在电子商务中的各种应用，包括产品评论，他们在其中综合和分析客户反馈；产品推荐，他们利用消费者数据来推荐相关商品；产品信息翻译，增强全球可访问性；以及产品问答部分，他们在其中自动化客户支持。该论文批判性地解决了电子商务中的公平挑战，强调了训练数据和算法中的偏见如何导致不公平的结果，例如强化刻板印象或歧视某些群体。这些问题不仅损害了消费者的信任，还引起了道德和法律方面的担忧。最后，该工作概述了未来的研究方向，强调电子商务领域需要更加公平和透明的法学硕士。它主张持续努力减少偏见并提高这些系统的公平性，确保它们有效且符合道德地服务于多元化的全球市场。通过这种全面的分析，该调查提供了电子商务法学硕士当前状况的整体视图，深入了解其潜力和局限性，并指导未来创造更公平、更具包容性的电子商务环境的努力。</li>
</ul>

<h3>Title: Title:
          DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>User Simulators play a pivotal role in training and evaluating task-oriented dialogue systems. Traditional user simulators typically rely on human-engineered agendas, resulting in generated responses that often lack diversity and spontaneity. Although large language models (LLMs) exhibit a remarkable capacity for generating coherent and contextually appropriate utterances, they may fall short when tasked with generating responses that effectively guide users towards their goals, particularly in dialogues with intricate constraints and requirements. This paper introduces DuetSim, a novel framework designed to address the intricate demands of task-oriented dialogues by leveraging LLMs. DuetSim stands apart from conventional approaches by employing two LLMs in tandem: one dedicated to response generation and the other focused on verification. This dual LLM approach empowers DuetSim to produce responses that not only exhibit diversity but also demonstrate accuracy and are preferred by human users. We validate the efficacy of our method through extensive experiments conducted on the MultiWOZ dataset, highlighting improvements in response quality and correctness, largely attributed to the incorporation of the second LLM. Our code is accessible at: this https URL.</li>
<li><strong>摘要：</strong>用户模拟器在训练和评估面向任务的对话系统中发挥着关键作用。传统的用户模拟器通常依赖于人为设计的议程，导致生成的响应通常缺乏多样性和自发性。尽管大型语言模型 (LLM) 在生成连贯且上下文适当的话语方面表现出卓越的能力，但在生成有效引导用户实现其目标的响应时，它们可能会表现不佳，特别是在具有复杂约束和要求的对话中。本文介绍了 DuetSim，这是一种新颖的框架，旨在利用法学硕士来解决面向任务的对话的复杂需求。 DuetSim 与传统方法不同，它同时采用两个法学硕士：一个致力于响应生成，另一个专注于验证。这种双重 LLM 方法使 DuetSim 能够生成不仅表现出多样性而且表现出准确性的响应，并且受到人类用户的青睐。我们通过在 MultiWOZ 数据集上进行的广泛实验验证了我们方法的有效性，突出了响应质量和正确性的改进，这在很大程度上归功于第二个法学硕士的纳入。我们的代码可通过以下网址访问：此 https URL。</li>
</ul>

<h3>Title: Title:
          Crowdsourcing with Enhanced Data Quality Assurance: An Efficient Approach to Mitigate Resource Scarcity Challenges in Training Large Language Models for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>P. Barai, G. Leroy, P. Bisht, J. M. Rothman, S. Lee, J. Andrews, S. A. Rice, A. Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Crowdsourcing with Enhanced Data Quality Assurance: An Efficient Approach to Mitigate Resource Scarcity Challenges in Training Large Language Models for Healthcare(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated immense potential in artificial intelligence across various domains, including healthcare. However, their efficacy is hindered by the need for high-quality labeled data, which is often expensive and time-consuming to create, particularly in low-resource domains like healthcare. To address these challenges, we propose a crowdsourcing (CS) framework enriched with quality control measures at the pre-, real-time-, and post-data gathering stages. Our study evaluated the effectiveness of enhancing data quality through its impact on LLMs (Bio-BERT) for predicting autism-related symptoms. The results show that real-time quality control improves data quality by 19 percent compared to pre-quality control. Fine-tuning Bio-BERT using crowdsourced data generally increased recall compared to the Bio-BERT baseline but lowered precision. Our findings highlighted the potential of crowdsourcing and quality control in resource-constrained environments and offered insights into optimizing healthcare LLMs for informed decision-making and improved patient care.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在包括医疗保健在内的各个领域的人工智能中展现出了巨大的潜力。然而，它们的功效受到对高质量标记数据的需求的阻碍，这些数据的创建通常既昂贵又耗时，特别是在医疗保健等资源匮乏的领域。为了应对这些挑战，我们提出了一个众包（CS）框架，在数据收集前、实时和数据收集后阶段丰富了质量控制措施。我们的研究通过对法学硕士（Bio-BERT）的影响来评估提高数据质量的有效性，以预测自闭症相关症状。结果表明，与预质量控制相比，实时质量控制将数据质量提高了 19%。与 Bio-BERT 基线相比，使用众包数据微调 Bio-BERT 通常会提高召回率，但会降低精度。我们的研究结果强调了资源有限环境中众包和质量控制的潜力，并提供了优化医疗保健法学硕士以做出明智决策和改善患者护理的见解。</li>
</ul>

<h3>Title: Title:
          Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality</h3>
<ul>
<li><strong>Authors: </strong>Jiahuan Pei, Irene Viola, Haochen Huang, Junxiao Wang, Moonisa Ahsan, Fanghua Ye, Jiang Yiming, Yao Sai, Di Wang, Zhumin Chen, Pengjie Ren, Pablo Cesar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Autonomous artificial intelligence (AI) agents have emerged as promising protocols for automatically understanding the language-based environment, particularly with the exponential development of large language models (LLMs). However, a fine-grained, comprehensive understanding of multimodal environments remains under-explored. This work designs an autonomous workflow tailored for integrating AI agents seamlessly into extended reality (XR) applications for fine-grained training. We present a demonstration of a multimodal fine-grained training assistant for LEGO brick assembly in a pilot XR environment. Specifically, we design a cerebral language agent that integrates LLM with memory, planning, and interaction with XR tools and a vision-language agent, enabling agents to decide their actions based on past experiences. Furthermore, we introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset synthesized automatically in the workflow served by a commercial LLM. This dataset comprises multimodal instruction manuals, conversations, XR responses, and vision question answering. Last, we present several prevailing open-resource LLMs as benchmarks, assessing their performance with and without fine-tuning on the proposed dataset. We anticipate that the broader impact of this workflow will advance the development of smarter assistants for seamless user interaction in XR environments, fostering research in both AI and HCI communities.</li>
<li><strong>摘要：</strong>自主人工智能 (AI) 代理已成为自动理解基于语言的环境的有前途的协议，特别是随着大型语言模型 (LLM) 的指数级发展。然而，对多模式环境的细粒度、全面的理解仍有待探索。这项工作设计了一个专为将人工智能代理无缝集成到扩展现实（XR）应用程序中以进行细粒度训练而定制的自主工作流程。我们展示了用于在试点 XR 环境中组装乐高积木的多模式细粒度培训助手。具体来说，我们设计了一种大脑语言代理，它将 LLM 与记忆、规划以及与 XR 工具和视觉语言代理的交互相结合，使代理能够根据过去的经验决定他们的行动。此外，我们还引入了 LEGO-MRTA，这是一个在商业法学硕士服务的工作流程中自动合成的多模式细粒度装配对话数据集。该数据集包括多模式说明手册、对话、XR 响应和视觉问答。最后，我们提出了几个流行的开源法学硕士作为基准，评估它们在对提议的数据集进行微调和不进行微调的情况下的表现。我们预计该工作流程将产生更广泛的影响，推动更智能的助手的开发，以实现 XR 环境中的无缝用户交互，从而促进人工智能和人机交互社区的研究。</li>
</ul>

<h3>Title: Title:
          Can formal argumentative reasoning enhance LLMs performances?</h3>
<ul>
<li><strong>Authors: </strong>Federico Castagna, Isabel Sassoon, Simon Parsons</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can formal argumentative reasoning enhance LLMs performances?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent years witnessed significant performance advancements in deep-learning-driven natural language models, with a strong focus on the development and release of Large Language Models (LLMs). These improvements resulted in better quality AI-generated output but rely on resource-expensive training and upgrading of models. Although different studies have proposed a range of techniques to enhance LLMs without retraining, none have considered computational argumentation as an option. This is a missed opportunity since computational argumentation is an intuitive mechanism that formally captures agents' interactions and the information conflict that may arise during such interplays, and so it seems well-suited for boosting the reasoning and conversational abilities of LLMs in a seamless manner. In this paper, we present a pipeline (MQArgEng) and preliminary study to evaluate the effect of introducing computational argumentation semantics on the performance of LLMs. Our experiment's goal was to provide a proof-of-concept and a feasibility analysis in order to foster (or deter) future research towards a fully-fledged argumentation engine plugin for LLMs. Exploratory results using the MT-Bench indicate that MQArgEng provides a moderate performance gain in most of the examined topical categories and, as such, show promise and warrant further research.</li>
<li><strong>摘要：</strong>近年来，深度学习驱动的自然语言模型的性能取得了显着的进步，特别是大型语言模型（LLM）的开发和发布。这些改进导致人工智能生成的输出质量更高，但依赖于资源昂贵的培训和模型升级。尽管不同的研究提出了一系列无需再培训即可增强法学硕士的技术，但没有一个研究考虑将计算论证作为一种选择。这是一个错失的机会，因为计算论证是一种直观的机制，可以正式捕获代理的交互以及交互过程中可能出现的信息冲突，因此它似乎非常适合以无缝方式提高法学硕士的推理和会话能力。在本文中，我们提出了一个管道（MQArgEng）和初步研究来评估引入计算论证语义对法学硕士性能的影响。我们实验的目标是提供概念验证和可行性分析，以促进（或阻止）未来针对法学硕士的成熟论证引擎插件的研究。使用 MT-Bench 的探索性结果表明，MQArgEng 在大多数检查的主题类别中提供了适度的性能增益，因此，显示出前景并值得进一步研究。</li>
</ul>

<h3>Title: Title:
          Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation</h3>
<ul>
<li><strong>Authors: </strong>Cheng Niu, Xingguang Wang, Xuxin Cheng, Juntong Song, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data.</li>
<li><strong>摘要：</strong>对话状态跟踪（DST）旨在监控对话中不断变化的对话状态，在开发面向任务的对话系统中发挥着关键作用。然而，获取 DST 任务的注释数据通常是一项成本高昂的工作。在本文中，我们重点关注利用法学硕士生成对话数据，以减少对话收集和注释成本。具体来说，GPT-4 用于模拟用户和代理交互，生成数千个带有 DST 标签注释的对话。然后对生成的数据和真实数据进行 LLaMA 2 的两阶段微调以进行 DST 预测。两个公共 DST 基准的实验结果表明，使用生成的对话数据，我们的模型比仅在真实数据上训练的基线表现更好。此外，我们的方法还能够适应现实场景中的动态需求，快速生成新领域的对话。在用相应的生成对话片段替换任何领域中的对话片段后，该模型实现了与在真实数据上训练的模型相当的性能。</li>
</ul>

<h3>Title: Title:
          Surgical Feature-Space Decomposition of LLMs: Why, When and How?</h3>
<ul>
<li><strong>Authors: </strong>Arnav Chavan, Nahush Lele, Deepak Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Surgical Feature-Space Decomposition of LLMs: Why, When and How?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Low-rank approximations, of the weight and feature space can enhance the performance of deep learning models, whether in terms of improving generalization or reducing the latency of inference. However, there is no clear consensus yet on \emph{how}, \emph{when} and \emph{why} these approximations are helpful for large language models (LLMs). In this work, we empirically study the efficacy of weight and feature space decomposition in transformer-based LLMs. We demonstrate that surgical decomposition not only provides critical insights into the trade-off between compression and language modelling performance, but also sometimes enhances commonsense reasoning performance of LLMs. Our empirical analysis identifies specific network segments that intrinsically exhibit a low-rank structure. Furthermore, we extend our investigation to the implications of low-rank approximations on model bias. Overall, our findings offer a novel perspective on optimizing LLMs, presenting the low-rank approximation not only as a tool for performance enhancements, but also as a means to potentially rectify biases within these models. Our code is available at \href{this https URL}{GitHub}.</li>
<li><strong>摘要：</strong>权重和特征空间的低秩近似可以提高深度学习模型的性能，无论是提高泛化能力还是减少推理延迟。然而，对于 \emph{how}、\emph{when} 和 \emph{why} 这些近似值对大型语言模型 (LLM) 有帮助，目前还没有明确的共识。在这项工作中，我们实证研究了基于 Transformer 的 LLM 中权重和特征空间分解的功效。我们证明，手术分解不仅为压缩和语言建模性能之间的权衡提供了重要的见解，而且有时还增强了法学硕士的常识推理性能。我们的实证分析确定了本质上表现出低等级结构的特定网络段。此外，我们将研究扩展到低秩近似对模型偏差的影响。总体而言，我们的研究结果为优化 LLM 提供了一种新颖的视角，表明低秩近似不仅可以作为性能增强的工具，而且还可以作为潜在纠正这些模型中偏差的一种手段。我们的代码可在 \href{此 https URL}{GitHub} 获取。</li>
</ul>

<h3>Title: Title:
          Assessing Political Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Luca Rettenberger, Markus Reischl, Mark Schutera</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Assessing Political Bias in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The assessment of societal biases within Large Language Models (LLMs) has emerged as a critical concern in the contemporary discourse surrounding Artificial Intelligence (AI) ethics and their impact. Especially, recognizing and considering political biases is important for practical applications to gain a deeper understanding of the possibilities and behaviors and to prevent unwanted statements. As the upcoming elections of the European Parliament will not remain unaffected by LLMs, we evaluate the bias of the current most popular open-source models concerning political issues within the European Union (EU) from a German perspective. To do so, we use the "Wahl-O-Mat", a voting advice application used in Germany, to determine which political party is the most aligned for the respective LLM. We show that larger models, such as Llama3-70B, tend to align more closely with left-leaning political parties like GRÜNE and Volt, while smaller models often remain neutral, particularly in English. This highlights the nuanced behavior of LLMs and the importance of language in shaping their political stances. Our findings underscore the importance of rigorously assessing and addressing societal bias in LLMs to safeguard the integrity and fairness of applications that employ the power of modern machine learning methods.</li>
<li><strong>摘要：</strong>对大型语言模型 (LLM) 中的社会偏见的评估已成为当代围绕人工智能 (AI) 伦理及其影响的讨论中的一个关键问题。特别是，认识和考虑政治偏见对于实际应用非常重要，可以更深入地了解可能性和行为并防止不必要的陈述。由于即将到来的欧洲议会选举不会不受法学硕士的影响，我们从德国的角度评估当前最流行的关于欧盟（EU）内部政治问题的开源模型的偏见。为此，我们使用德国使用的投票建议应用程序“Wahl-O-Mat”来确定哪个政党最适合相应的法学硕士。我们表明，较大的模型（例如 Llama3-70B）往往与 GRÜNE 和 Volt 等左倾政党更紧密地结盟，而较小的模型通常保持中立，尤其是在英语中。这凸显了法学硕士的微妙行为以及语言在塑造其政治立场方面的重要性。我们的研究结果强调了严格评估和解决法学硕士中的社会偏见的重要性，以维护利用现代机器学习方法的力量的应用程序的完整性和公平性。</li>
</ul>

<h3>Title: Title:
          Case-Based Reasoning Approach for Solving Financial Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yikyung Kim, Jay-Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Case-Based Reasoning Approach for Solving Financial Question Answering(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Measuring a machine's understanding of human language often involves assessing its reasoning skills, i.e. logical process of deriving answers to questions. While recent language models have shown remarkable proficiency in text based tasks, their efficacy in complex reasoning problems involving heterogeneous information such as text, tables, and numbers remain uncertain. Addressing this gap, FinQA introduced a numerical reasoning dataset for financial documents and simultaneously proposed a program generation approach . Our investigation reveals that half of the errors (48%) stem from incorrect operations being generated. To address this issue, we propose a novel approach to tackle numerical reasoning problems using case based reasoning (CBR), an artificial intelligence paradigm that provides problem solving guidance by offering similar cases (i.e. similar questions and corresponding logical programs). Our model retrieves relevant cases to address a given question, and then generates an answer based on the retrieved cases and contextual information. Through experiments on the FinQA dataset, we demonstrate competitive performance of our approach and additionally show that by expanding case repository, we can help solving complex multi step programs which FinQA showed weakness of.</li>
<li><strong>摘要：</strong>衡量机器对人类语言的理解通常涉及评估其推理能力，即得出问题答案的逻辑过程。虽然最近的语言模型在基于文本的任务中表现出了显着的熟练程度，但它们在涉及文本、表格和数字等异构信息的复杂推理问题上的功效仍然不确定。为了解决这一差距，FinQA 引入了金融文档的数值推理数据集，同时提出了程序生成方法。我们的调查显示，一半的错误 (48%) 源于生成的错误操作。为了解决这个问题，我们提出了一种使用基于案例的推理（CBR）来解决数字推理问题的新方法，这是一种人工智能范式，通过提供类似的案例（即类似的问题和相应的逻辑程序）来提供问题解决指导。我们的模型检索相关案例来解决给定的问题，然后根据检索到的案例和上下文信息生成答案。通过在 FinQA 数据集上的实验，我们展示了我们的方法的竞争性能，并且还表明，通过扩展案例库，我们可以帮助解决 FinQA 所表现出的弱点的复杂的多步骤程序。</li>
</ul>

<h3>Title: Title:
          LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions</h3>
<ul>
<li><strong>Authors: </strong>Victor Agostinelli, Sanghyun Hong, Lizhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A promising approach to preserving model performance in linearized transformers is to employ position-based re-weighting functions. However, state-of-the-art re-weighting functions rely heavily on target sequence lengths, making it difficult or impossible to apply them to autoregressive and simultaneous tasks, where the target and sometimes even the input sequence length are unknown. To address this issue, we propose Learned Proportions (LeaP) and LeaPformers. Our contribution is built on two major components. First, we generalize the dependence on explicit positional representations and sequence lengths into dependence on sequence proportions for re-weighting. Second, we replace static positional representations with dynamic proportions derived via a compact module, enabling more flexible attention concentration patterns. We evaluate LeaPformer against eight representative efficient transformers on the Long-Range Arena benchmark, showing that LeaPformer achieves the best quality-throughput trade-off, as well as LeaPformer to Wikitext-103 autoregressive language modeling and simultaneous speech-to-text translation for two language pairs, achieving competitive results.</li>
<li><strong>摘要：</strong>在线性化变压器中保持模型性能的一种有前途的方法是采用基于位置的重新加权函数。然而，最先进的重新加权函数严重依赖于目标序列长度，使得它们很难或不可能应用于自回归和同步任务，其中目标，有时甚至输入序列长度都是未知的。为了解决这个问题，我们提出了学习比例（LeaP）和 LeaPformers。我们的贡献基于两个主要组成部分。首先，我们将对显式位置表示和序列长度的依赖概括为对重新加权的序列比例的依赖。其次，我们用通过紧凑模块导出的动态比例取代静态位置表示，从而实现更灵活的注意力集中模式。我们在 Long-Range Arena 基准上对 LeaPformer 与八个具有代表性的高效 Transformer 进行了评估，结果表明 LeaPformer 实现了最佳的质量-吞吐量权衡，以及 LeaPformer 到 Wikitext-103 自回归语言建模和两个语言的同步语音到文本翻译语言对，取得有竞争力的结果。</li>
</ul>

<h3>Title: Title:
          MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Xu, Junyu Lai, Yunpeng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The \textit{pretrain+fine-tune} paradigm is foundational in deploying large language models (LLMs) across a diverse range of downstream applications. Among these, Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning (PEFT), producing numerous off-the-shelf task-specific LoRA adapters. However, this approach requires explicit task intention selection, posing challenges for automatic task sensing and switching during inference with multiple existing LoRA adapters embedded in a single LLM. In this work, we introduce \textbf{\method} (\textbf{M}ultiple-\textbf{T}asks embedded \textbf{LoRA}), a scalable multi-knowledge LoRA fusion framework designed for LLMs. \method\ integrates various LoRA adapters in a Mixture-of-Experts (MoE) style into the base LLM, enabling the model to automatically select the most pertinent adapter based on the task input. This advancement significantly enhances the LLM's capability to handle composite tasks that require different adapters to solve various components of the problem. Our evaluations, featuring the LlaMA2-13B and LlaMA3-8B base models equipped with off-the-shelf 28 LoRA adapters through \method, demonstrate equivalent performance with the individual adapters. Furthermore, both base models equipped with \method\ achieve superior performance in sequentially solving composite tasks with ten problems in only a single inference process, highlighting the ability of timely intention switching in \method\ embedded LLMs.</li>
<li><strong>摘要：</strong>\textit{pretrain+fine-tune} 范例是在各种下游应用程序中部署大型语言模型 (LLM) 的基础。其中，低秩适应 (LoRA) 因其参数高效微调 (PEFT) 而脱颖而出，产生了大量现成的特定于任务的 LoRA 适配器。然而，这种方法需要明确的任务意图选择，这给嵌入单个 LLM 中的多个现有 LoRA 适配器进行推理期间的自动任务感知和切换带来了挑战。在这项工作中，我们引入了\textbf{\method}（\textbf{M}ultiple-\textbf{T}要求嵌入\textbf{LoRA}），这是一个专为法学硕士设计的可扩展的多知识LoRA融合框架。 \method\ 以专家混合 (MoE) 风格将各种 LoRA 适配器集成到基础 LLM 中，使模型能够根据任务输入自动选择最相关的适配器。这一进步显着增强了法学硕士处理需要不同适配器来解决问题的各个组成部分的复合任务的能力。我们的评估以 LlaMA2-13B 和 LlaMA3-8B 基本模型为特色，通过 \ 方法配备了现成的 28 个 LoRA 适配器，展示了各个适配器的同等性能。此外，两个配备“method”的基础模型在仅在单个推理过程中顺序解决包含十个问题的复合任务方面均取得了优异的性能，凸显了“method”嵌入式LLM及时意图切换的能力。</li>
</ul>

<h3>Title: Title:
          Large Language Models for Medicine: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi, Qian Liang, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models for Medicine: A Survey(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>To address challenges in the digital economy's landscape of digital intelligence, large language models (LLMs) have been developed. Improvements in computational power and available resources have significantly advanced LLMs, allowing their integration into diverse domains for human life. Medical LLMs are essential application tools with potential across various medical scenarios. In this paper, we review LLM developments, focusing on the requirements and applications of medical LLMs. We provide a concise overview of existing models, aiming to explore advanced research directions and benefit researchers for future medical applications. We emphasize the advantages of medical LLMs in applications, as well as the challenges encountered during their development. Finally, we suggest directions for technical integration to mitigate challenges and potential research directions for the future of medical LLMs, aiming to meet the demands of the medical field better.</li>
<li><strong>摘要：</strong>为了应对数字经济数字智能领域的挑战，人们开发了大语言模型（LLM）。计算能力和可用资源的提高显着提高了法学硕士的水平，使其能够融入人类生活的不同领域。医学法学硕士是重要的应用工具，在各种医疗场景中具有潜力。在本文中，我们回顾了法学硕士的发展，重点关注医学法学硕士的要求和应用。我们对现有模型进行了简要概述，旨在探索先进的研究方向，并使研究人员受益于未来的医学应用。我们强调医学法学硕士在申请中的优势，以及其发展过程中遇到的挑战。最后，我们提出了缓解挑战的技术整合方向以及医学法学硕士未来潜在的研究方向，旨在更好地满足医学领域的需求。</li>
</ul>

<h3>Title: Title:
          Large language models for sentiment analysis of newspaper articles during COVID-19: The Guardian</h3>
<ul>
<li><strong>Authors: </strong>Rohitash Chandra, Baicheng Zhu, Qingying Fang, Eka Shinjikashvili</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large language models for sentiment analysis of newspaper articles during COVID-19: The Guardian(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion</li>
<li><strong>摘要：</strong>在 COVID-19 大流行期间，新闻媒体报道涵盖了广泛的主题，包括病毒传播、医疗资源分配和政府应对措施。人们对 COVID-19 期间社交媒体平台的情绪进行了研究，以了解公众在病例增加的情况下的反应以及政府为控制病毒传播而采取的策略。情绪分析可以帮助我们更好地了解疫情期间社会舆论和情绪趋势的变化。除了社交媒体之外，报纸在信息传播方面也发挥着至关重要的作用，包括政府、专家以及公众关于各种话题的信息。对选定国家的 COVID-19 期间报纸来源进行情绪分析的研究可以概述媒体如何报道这一流行病。在这项研究中，我们选择了《卫报》，并提供了 COVID-19 各个阶段的情绪分析，包括初始传播、封锁和疫苗接种。我们采用新颖的大型语言模型 (LLM)，并使用专家标记的情感分析数据对其进行改进。我们还对大流行前的情绪进行了分析以进行比较。结果表明，在大流行早期阶段，公众情绪优先考虑紧急危机应对，随后将重点转向解决对健康和经济的影响。与社交媒体情绪分析的相关研究相比，我们发现《卫报》的负面情绪（悲伤、恼怒、焦虑和否认）占主导地位，这表明社交媒体提供了更加多元化的情绪反映。我们在《卫报》上发现了一种严峻的叙述，在 COVID-19 之前和期间，包括澳大利亚、英国、世界新闻和观点在内的新闻版块总体上负面情绪占主导地位</li>
</ul>

<h3>Title: Title:
          The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented Generation (FutureDial-RAG)</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Cai, Si Chen, Yi Huang, Junlan Feng, Zhijian Ou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented Generation (FutureDial-RAG)(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented Generation (FutureDial-RAG), Co-located with SLT 2024</li>
<li><strong>摘要：</strong>第二届 FutureDial 挑战赛：具有检索增强生成功能的对话系统 (FutureDial-RAG)，与 SLT 2024 同期举办</li>
</ul>

<h3>Title: Title:
          Multi-domain Knowledge Graph Collaborative Pre-training and Prompt Tuning for Diverse Downstream Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Binbin Hu, Zhuo Chen, Lingbing Guo, Ziqi Liu, Zhiqiang Zhang, Lei Liang, Huajun Chen, Wen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multi-domain Knowledge Graph Collaborative Pre-training and Prompt Tuning for Diverse Downstream Tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) provide reliable external knowledge for a wide variety of AI tasks in the form of structured triples. Knowledge graph pre-training (KGP) aims to pre-train neural networks on large-scale KGs and provide unified interfaces to enhance different downstream tasks, which is a key direction for KG management, maintenance, and applications. Existing works often focus on purely research questions in open domains, or they are not open source due to data security and privacy in real scenarios. Meanwhile, existing studies have not explored the training efficiency and transferability of KGP models in depth. To address these problems, We propose a framework MuDoK to achieve multi-domain collaborative pre-training and efficient prefix prompt tuning to serve diverse downstream tasks like recommendation and text understanding. Our design is a plug-and-play prompt learning approach that can be flexibly adapted to different downstream task backbones. In response to the lack of open-source benchmarks, we constructed a new multi-domain KGP benchmark called KPI with two large-scale KGs and six different sub-domain tasks to evaluate our method and open-sourced it for subsequent research. We evaluated our approach based on constructed KPI benchmarks using diverse backbone models in heterogeneous downstream tasks. The experimental results show that our framework brings significant performance gains, along with its generality, efficiency, and transferability.</li>
<li><strong>摘要：</strong>知识图（KG）以结构化三元组的形式为各种人工智能任务提供可靠的外部知识。知识图预训练（KGP）旨在在大规模知识图谱上预训练神经网络，并提供统一的接口来增强不同的下游任务，这是知识图谱管理、维护和应用的一个关键方向。现有的工作通常专注于开放领域的纯粹研究问题，或者由于实际场景中的数据安全和隐私问题而不是开源的。同时，现有研究尚未深入探讨KGP模型的训练效率和可迁移性。为了解决这些问题，我们提出了一个框架MuDoK来实现多领域协作预训练和高效的前缀提示调整，以服务推荐和文本理解等多种下游任务。我们的设计是一种即插即用的即时学习方法，可以灵活地适应不同的下游任务主干。针对开源基准的缺乏，我们构建了一个名为 KPI 的新的多领域 KGP 基准，其中包含两个大规模 KG 和六个不同的子域任务来评估我们的方法，并将其开源以供后续研究。我们根据在异构下游任务中使用不同骨干模型构建的 KPI 基准评估了我们的方法。实验结果表明，我们的框架带来了显着的性能提升，以及其通用性、效率和可移植性。</li>
</ul>

<h3>Title: Title:
          Presentations are not always linear! GNN meets LLM for Document-to-Presentation Transformation with Attribution</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Maheshwari, Sambaran Bandyopadhyay, Aparna Garimella, Anandhavelu Natarajan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Presentations are not always linear! GNN meets LLM for Document-to-Presentation Transformation with Attribution(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>Automatically generating a presentation from the text of a long document is a challenging and useful problem. In contrast to a flat summary, a presentation needs to have a better and non-linear narrative, i.e., the content of a slide can come from different and non-contiguous parts of the given document. However, it is difficult to incorporate such non-linear mapping of content to slides and ensure that the content is faithful to the document. LLMs are prone to hallucination and their performance degrades with the length of the input document. Towards this, we propose a novel graph based solution where we learn a graph from the input document and use a combination of graph neural network and LLM to generate a presentation with attribution of content for each slide. We conduct thorough experiments to show the merit of our approach compared to directly using LLMs for this task.</li>
<li><strong>摘要：</strong>从长文档的文本自动生成演示文稿是一个具有挑战性且有用的问题。与平面摘要相比，演示文稿需要有更好的非线性叙述，即幻灯片的内容可以来自给定文档的不同且不连续的部分。然而，将内容的这种非线性映射合并到幻灯片并确保内容忠实于文档是很困难的。法学硕士很容易产生幻觉，并且他们的表现会随着输入文档的长度而下降。为此，我们提出了一种新颖的基于图的解决方案，我们从输入文档中学习图，并使用图神经网络和法学硕士的组合来生成每张幻灯片内容属性的演示文稿。我们进行了彻底的实验，以展示我们的方法与直接使用法学硕士来完成此任务相比的优点。</li>
</ul>

<h3>Title: Title:
          Atomic Self-Consistency for Better Long Form Generations</h3>
<ul>
<li><strong>Authors: </strong>Raghuveer Thirukovalluru, Yukun Huang, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Atomic Self-Consistency for Better Long Form Generations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Recent work has aimed to improve LLM generations by filtering out hallucinations, thereby improving the precision of the information in responses. Correctness of a long-form response, however, also depends on the recall of multiple pieces of information relevant to the question. In this paper, we introduce Atomic Self-Consistency (ASC), a technique for improving the recall of relevant information in an LLM response. ASC follows recent work, Universal Self-Consistency (USC) in using multiple stochastic samples from an LLM to improve the long-form response. Unlike USC which only focuses on selecting the best single generation, ASC picks authentic subparts from the samples and merges them into a superior composite answer. Through extensive experiments and ablations, we show that merging relevant subparts of multiple samples performs significantly better than picking a single sample. ASC demonstrates significant gains over USC on multiple factoids and open-ended QA datasets - ASQA, QAMPARI, QUEST, ELI5 with ChatGPT and Llama2. Our analysis also reveals untapped potential for enhancing long-form generations using approach of merging multiple samples.</li>
<li><strong>摘要：</strong>最近的工作旨在通过过滤幻觉来提高法学硕士的生成，从而提高反应中信息的准确性。然而，长篇回答的正确性还取决于对与问题相关的多条信息的回忆。在本文中，我们介绍了原子自一致性（ASC），这是一种提高法学硕士回答中相关信息回忆的技术。 ASC 遵循最近的工作，通用自我一致性（USC），使用法学硕士的多个随机样本来改进长格式响应。与 USC 只专注于选择最好的单代产品不同，ASC 从样本中挑选真实的子部分，并将它们合并成一个优秀的复合答案。通过大量的实验和消融，我们表明合并多个样本的相关子部分的性能明显优于选择单个样本。 ASC 在多个事实和开放式 QA 数据集（ASQA、QAMPARI、QUEST、带有 ChatGPT 的 ELI5 和 Llama2）上表现出比 USC 显着的进步。我们的分析还揭示了使用合并多个样本的方法增强长格式生成的未开发潜力。</li>
</ul>

<h3>Title: Title:
          RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented Generation and Readability Control for Layman Summarization of Biomedical Texts</h3>
<ul>
<li><strong>Authors: </strong>Yuelyu Ji, Zhuochun Li, Rui Meng, Sonish Sivarajkumar, Yanshan Wang, Zeshui Yu, Hui Ji, Yushui Han, Hanyu Zeng, Daqing He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented Generation and Readability Control for Layman Summarization of Biomedical Texts(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This paper introduces the RAG-RLRC-LaySum framework, designed to make complex biomedical research understandable to laymen through advanced Natural Language Processing (NLP) techniques. Our Retrieval Augmented Generation (RAG) solution, enhanced by a reranking method, utilizes multiple knowledge sources to ensure the precision and pertinence of lay summaries. Additionally, our Reinforcement Learning for Readability Control (RLRC) strategy improves readability, making scientific content comprehensible to non-specialists. Evaluations using the publicly accessible PLOS and eLife datasets show that our methods surpass Plain Gemini model, demonstrating a 20% increase in readability scores, a 15% improvement in ROUGE-2 relevance scores, and a 10% enhancement in factual accuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific knowledge, enhancing public engagement with biomedical discoveries.</li>
<li><strong>摘要：</strong>本文介绍了 RAG-RLRC-LaySum 框架，旨在通过先进的自然语言处理 (NLP) 技术让外行人能够理解复杂的生物医学研究。我们的检索增强生成 (RAG) 解决方案通过重新排序方法得到增强，利用多个知识源来确保外行摘要的准确性和相关性。此外，我们的强化学习可读性控制 (RLRC) 策略提高了可读性，使非专业人士也能理解科学内容。使用可公开访问的 PLOS 和 eLife 数据集进行的评估表明，我们的方法超越了 Plain Gemini 模型，可读性分数提高了 20%，ROUGE-2 相关性分数提高了 15%，事实准确性提高了 10%。 RAG-RLRC-LaySum 框架有效地实现了科学知识的民主化，增强了公众对生物医学发现的参与。</li>
</ul>

<h3>Title: Title:
          Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting</h3>
<ul>
<li><strong>Authors: </strong>Krishna Prasad Varadarajan Srinivasan, Prasanth Gumpena, Madhusudhana Yattapu, Vishal H. Brahmbhatt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In the domain of large language models (LLMs), arXiv:2305.16938 showed that few-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and Pattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize similarly on Out-Of-Domain (OOD) datasets, but vary in terms of task adaptation. However, they both pose challenges, especially in term of memory requirements. In this paper, we further try to push the understanding of different fine-tuning strategies for LLM and aim to bring a myriad of these on the same pedestal for an elaborate comparison with full-model fine-tuning on two diverse datasets. To that end, we conducted a series of experiments, beginning with state-of-the-art methods like vanilla fine-tuning and Pattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets, COLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of LoRA adapters in a few-shot setting. Finally, we also compare an alternative approach that has gained recent popularity -- context distillation -- with the vanilla FT and PBFT with and without few-shot setup. Our findings suggest that these alternative strategies that we explored can exhibit out-of-domain generalization comparable to that of vanilla FT and PBFT. PBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the need for effective prompts. Further, our adaptive-fine tuning and LoRA experiments perform comparable or slightly worse than the standard fine-tunings as anticipated, since standard fine-tunings involve tuning the entire model. Finally, our context distillation experiments out-perform the standard fine-tuning methods. These findings underscore that eventually the choice of an appropriate fine-tuning method depends on the available resources (memory, compute, data) and task adaptability.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 领域，arXiv:2305.16938 展示了少样本全模型微调——即 Vanilla Fine Tuning (FT) 和 Pattern-Based Fine Tuning (PBFT)——以及 In-Context学习（ICL）在域外（OOD）数据集上的概括类似，但在任务适应方面有所不同。然而，它们都带来了挑战，特别是在内存要求方面。在本文中，我们进一步尝试推动对 LLM 不同微调策略的理解，并旨在将无数这些策略放在同一个基础上，以便与两个不同数据集上的全模型微调进行详细比较。为此，我们进行了一系列实验，首先是对跨两个数据集 COLA 和 MNLI 的预训练模型使用最先进的方法，例如普通微调和基于模式的微调 (PBFT)。然后，我们在几次设置中研究自适应微调和 LoRA 适配器的效率。最后，我们还将最近流行的另一种方法（上下文蒸馏）与带有或不带有少样本设置的普通 FT 和 PBFT 进行比较。我们的研究结果表明，我们探索的这些替代策略可以表现出与普通 FT 和 PBFT 相当的域外泛化能力。 PBFT 在域外 (OOD) 数据上的表现不如 Vanilla FT，这强调了有效提示的必要性。此外，我们的自适应微调和 LoRA 实验的表现与预期的标准微调相当或稍差，因为标准微调涉及调整整个模型。最后，我们的上下文蒸馏实验优于标准微调方法。这些发现强调，最终合适的微调方法的选择取决于可用资源（内存、计算、数据）和任务适应性。</li>
</ul>

<h3>Title: Title:
          Investigating Symbolic Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Neisarg Dave, Daniel Kifer, C. Lee Giles, Ankur Mali</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Investigating Symbolic Capabilities of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompting techniques have significantly enhanced the capabilities of Large Language Models (LLMs) across various complex tasks, including reasoning, planning, and solving math word problems. However, most research has predominantly focused on language-based reasoning and word problems, often overlooking the potential of LLMs in handling symbol-based calculations and reasoning. This study aims to bridge this gap by rigorously evaluating LLMs on a series of symbolic tasks, such as addition, multiplication, modulus arithmetic, numerical precision, and symbolic counting. Our analysis encompasses eight LLMs, including four enterprise-grade and four open-source models, of which three have been pre-trained on mathematical tasks. The assessment framework is anchored in Chomsky's Hierarchy, providing a robust measure of the computational abilities of these models. The evaluation employs minimally explained prompts alongside the zero-shot Chain of Thoughts technique, allowing models to navigate the solution process autonomously. The findings reveal a significant decline in LLMs' performance on context-free and context-sensitive symbolic tasks as the complexity, represented by the number of symbols, increases. Notably, even the fine-tuned GPT3.5 exhibits only marginal improvements, mirroring the performance trends observed in other models. Across the board, all models demonstrated a limited generalization ability on these symbol-intensive tasks. This research underscores LLMs' challenges with increasing symbolic complexity and highlights the need for specialized training, memory and architectural adjustments to enhance their proficiency in symbol-based reasoning tasks.</li>
<li><strong>摘要：</strong>提示技术显着增强了大型语言模型 (LLM) 处理各种复杂任务的能力，包括推理、规划和解决数学应用题。然而，大多数研究主要集中在基于语言的推理和文字问题上，往往忽视了法学硕士在处理基于符号的计算和推理方面的潜力。本研究旨在通过严格评估法学硕士在一系列符号任务（例如加法、乘法、模运算、数值精度和符号计数）上的表现来弥补这一差距。我们的分析涵盖八个法学硕士，其中包括四个企业级模型和四个开源模型，其中三个模型已经过数学任务的预训练。该评估框架以乔姆斯基层次结构为基础，为这些模型的计算能力提供了可靠的衡量标准。该评估采用最少解释的提示以及零样本思维链技术，允许模型自主导航解决方案过程。研究结果表明，随着以符号数量表示的复杂性的增加，法学硕士在上下文无关和上下文敏感的符号任务上的表现显着下降。值得注意的是，即使经过微调的 GPT3.5 也仅表现出微小的改进，反映了在其他模型中观察到的性能趋势。总体而言，所有模型在这些符号密集型任务上都表现出有限的泛化能力。这项研究强调了法学硕士因符号复杂性增加而面临的挑战，并强调需要专门的培训、记忆和架构调整，以提高他们在基于符号的推理任务中的熟练程度。</li>
</ul>

<h3>Title: Title:
          Equipping Transformer with Random-Access Reading for Long-Context Understanding</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Yang, Zi Yang, Nan Hua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Equipping Transformer with Random-Access Reading for Long-Context Understanding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Long-context modeling presents a significant challenge for transformer-based large language models (LLMs) due to the quadratic complexity of the self-attention mechanism and issues with length extrapolation caused by pretraining exclusively on short inputs. Existing methods address computational complexity through techniques such as text chunking, the kernel approach, and structured attention, and tackle length extrapolation problems through positional encoding, continued pretraining, and data engineering. These approaches typically require $\textbf{sequential access}$ to the document, necessitating reading from the first to the last token. We contend that for goal-oriented reading of long documents, such sequential access is not necessary, and a proficiently trained model can learn to omit hundreds of less pertinent tokens. Inspired by human reading behaviors and existing empirical observations, we propose $\textbf{random access}$, a novel reading strategy that enables transformers to efficiently process long documents without examining every token. Experimental results from pretraining, fine-tuning, and inference phases validate the efficacy of our method.</li>
<li><strong>摘要：</strong>由于自注意力机制的二次复杂性以及仅针对短输入进行预训练导致的长度外推问题，长上下文建模对基于 Transformer 的大型语言模型 (LLM) 提出了重大挑战。现有方法通过文本分块、内核方法和结构化注意力等技术解决计算复杂性，并通过位置编码、持续预训练和数据工程解决长度外推问题。这些方法通常需要对文档进行$\textbf{顺序访问}$，从而需要从第一个标记读取到最后一个标记。我们认为，对于以目标为导向的长文档阅读，这种顺序访问是不必要的，经过熟练训练的模型可以学会省略数百个不太相关的标记。受人类阅读行为和现有经验观察的启发，我们提出了 $\textbf{random access}$，一种新颖的阅读策略，使 Transformer 能够有效地处理长文档，而无需检查每个标记。预训练、微调和推理阶段的实验结果验证了我们方法的有效性。</li>
</ul>

<h3>Title: Title:
          Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum</h3>
<ul>
<li><strong>Authors: </strong>Hadi Pouransari, Chun-Liang Li, Jen-Hao Rick Chang, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, Oncel Tuzel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length. However, this method of concatenation can lead to cross-document attention within a sequence, which is neither a desirable learning signal nor computationally efficient. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a penalty proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy 3x faster compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常在由固定长度标记序列组成的数据集上进行训练。这些数据集是通过随机连接不同长度的文档然后将它们分成预定目标长度的序列来创建的。然而，这种连接方法可能会导致序列内的跨文档注意力，这既不是理想的学习信号，也不是计算效率高的。此外，由于注意力的二次成本，长序列的训练在计算上变得令人望而却步。在本研究中，我们引入了数据集分解（一种新颖的可变序列长度训练技术）来应对这些挑战。我们将数据集分解为桶的并集，每个桶都包含从唯一文档中提取的相同大小的序列。在训练过程中，我们使用可变的序列长度和批量大小，从课程的所有桶中同时采样。与在训练的每一步都会产生固定注意力成本的 concat-and-chunk 基线相比，我们提出的方法会在每一步产生与实际文档长度成比例的惩罚，从而显着节省训练时间。我们以与使用基线方法训练的 2k 上下文长度模型相同的成本训练 8k 上下文长度 1B 模型。在网络规模的语料库上进行的实验表明，我们的方法显着提高了标准语言评估和长上下文基准的性能，与基线相比，达到目标准确度的速度提高了 3 倍。我们的方法不仅能够对长序列进行有效的预训练，而且还可以根据数据集大小进行有效扩展。最后，我们阐明了训练大型语言模型的一个关键但研究较少的方面：序列长度的分布和课程，这会导致性能上不可忽视的差异。</li>
</ul>

<h3>Title: Title:
          Mosaic IT: Enhancing Instruction Tuning with Data Mosaics</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mosaic IT: Enhancing Instruction Tuning with Data Mosaics(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions. Current instruction tuning primarily relies on teacher models or human intervention to generate and refine the instructions and responses, which are costly, non-sustainable, and may lack diversity. In this paper, we introduce Mosaic Instruction Tuning (Mosaic-IT), a human/model-free method that can efficiently create rich and diverse augmentations from existing instruction tuning data to enhance the finetuned LLM.Mosaic-IT randomly concatenates multiple instruction data into one and trains the model to produce the corresponding responses with predefined higher-level meta-instructions to strengthen its multi-step instruction-following and format-following skills. Our extensive evaluations demonstrate a superior performance and training efficiency of Mosaic-IT, which achieves consistent performance improvements over various benchmarks and an 80% reduction in training costs compared with original instruction tuning. Our codes and data are available at this https URL.</li>
<li><strong>摘要：</strong>使用各种指令-响应对对大型语言模型进行微调，增强了它们理解和遵循指令的能力。当前的指令调整主要依靠教师模型或人为干预来生成和完善指令和响应，这是昂贵的、不可持续的，并且可能缺乏多样性。在本文中，我们介绍了马赛克指令调优（Mosaic-IT），这是一种无人类/模型的方法，可以有效地从现有指令调优数据创建丰富多样的增强，以增强微调的 LLM。Mosaic-IT 将多个指令数据随机连接成并训练模型使用预定义的高级元指令产生相应的响应，以加强其多步骤指令跟踪和格式跟踪技能。我们的广泛评估表明，Mosaic-IT 具有卓越的性能和训练效率，在各种基准测试中实现了一致的性能提升，并且与原始指令调优相比，训练成本降低了 80%。我们的代码和数据可通过此 https URL 获取。</li>
</ul>

<h3>Title: Title:
          High Performance P300 Spellers Using GPT2 Word Prediction With Cross-Subject Training</h3>
<ul>
<li><strong>Authors: </strong>Nithin Parthasarathy, James Soetedjo, Saarang Panchavati, Nitya Parthasarathy, Corey Arnold, Nader Pouratian, William Speier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, eess.SP, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          High Performance P300 Spellers Using GPT2 Word Prediction With Cross-Subject Training(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Amyotrophic lateral sclerosis (ALS) severely impairs patients' ability to communicate, often leading to a decline in their quality of life within a few years of diagnosis. The P300 speller brain-computer interface (BCI) offers an alternative communication method by interpreting a subject's EEG response to characters presented on a grid interface. This paper addresses the common speed limitations encountered in training efficient P300-based multi-subject classifiers by introducing innovative "across-subject" classifiers. We leverage a combination of the second-generation Generative Pre-Trained Transformer (GPT2) and Dijkstra's algorithm to optimize stimuli and suggest word completion choices based on typing history. Additionally, we employ a multi-layered smoothing technique to accommodate out-of-vocabulary (OOV) words. Through extensive simulations involving random sampling of EEG data from subjects, we demonstrate significant speed enhancements in typing passages containing rare and OOV words. These optimizations result in approximately 10% improvement in character-level typing speed and up to 40% improvement in multi-word prediction. We demonstrate that augmenting standard row/column highlighting techniques with layered word prediction yields close-to-optimal performance. Furthermore, we explore both "within-subject" and "across-subject" training techniques, showing that speed improvements are consistent across both approaches.</li>
<li><strong>摘要：</strong>肌萎缩侧索硬化症 (ALS) 严重损害患者的沟通能力，通常会导致患者在诊断后几年内生活质量下降。 P300 拼写器脑机接口 (BCI) 通过解释受试者对网格界面上呈现的字符的脑电图反应，提供了另一种通信方法。本文通过引入创新的“跨主题”分类器，解决了训练基于 P300 的高效多主题分类器时遇到的常见速度限制。我们利用第二代生成式预训练 Transformer (GPT2) 和 Dijkstra 算法的组合来优化刺激并根据打字历史建议单词完成选择。此外，我们采用多层平滑技术来适应词汇外（OOV）单词。通过对受试者脑电图数据进行随机采样的广泛模拟，我们证明了打字包含稀有词和 OOV 词的段落的速度显着提高。这些优化使字符级打字速度提高了约 10%，多字预测提高了 40%。我们证明，通过分层单词预测增强标准行/列突出显示技术可以产生接近最佳的性能。此外，我们探索了“科目内”和“跨科目”训练技术，表明两种方法的速度改进是一致的。</li>
</ul>

<h3>Title: Title:
          AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Alireza Ghaffari, Sharareh Younesian, Vahid Partovi Nia, Boxing Chen, Masoud Asgharian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The ever-growing computational complexity of Large Language Models (LLMs) necessitates efficient deployment strategies. The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy. This paper presents AdpQ, a novel zero-shot adaptive PTQ method for LLMs that achieves the state-of-the-art performance in low-precision quantization (e.g. 3-bit) without requiring any calibration data. Inspired by Adaptive LASSO regression model, our proposed approach tackles the challenge of outlier activations by separating salient weights using an adaptive soft-thresholding method. Guided by Adaptive LASSO, this method ensures that the quantized weights distribution closely follows the originally trained weights and eliminates the need for calibration data entirely, setting our method apart from popular approaches such as SpQR and AWQ. Furthermore, our method offers an additional benefit in terms of privacy preservation by eliminating any calibration or training data. We also delve deeper into the information-theoretic underpinnings of the proposed method. We demonstrate that it leverages the Adaptive LASSO to minimize the Kullback-Leibler divergence between the quantized weights and the originally trained weights. This minimization ensures the quantized model retains the Shannon information content of the original model to a great extent, guaranteeing efficient deployment without sacrificing accuracy or information. Our results achieve the same accuracy as the existing methods on various LLM benchmarks while the quantization time is reduced by at least 10x, solidifying our contribution to efficient and privacy-preserving LLM deployment.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 不断增长的计算复杂性需要高效的部署策略。当前最先进的训练后量化 (PTQ) 方法通常需要校准才能达到所需的精度。本文提出了 AdpQ，一种用于 LLM 的新型零样本自适应 PTQ 方法，无需任何校准数据即可在低精度量化（例如 3 位）中实现最先进的性能。受自适应 LASSO 回归模型的启发，我们提出的方法通过使用自适应软阈值方法分离显着权重来解决离群值激活的挑战。在自适应 LASSO 的指导下，该方法确保量化权重分布紧密遵循最初训练的权重，并完全消除对校准数据的需求，使我们的方法与 SpQR 和 AWQ 等流行方法区分开来。此外，我们的方法通过消除任何校准或训练数据在隐私保护方面提供了额外的好处。我们还深入研究了所提出方法的信息论基础。我们证明它利用自适应 LASSO 来最小化量化权重和原始训练权重之间的 Kullback-Leibler 散度。这种最小化保证了量化模型在很大程度上保留了原始模型的香农信息内容，保证了高效部署而不牺牲准确性或信息。我们的结果在各种 LLM 基准上达到了与现有方法相同的精度，同时量化时间减少了至少 10 倍，巩固了我们对高效和保护隐私的 LLM 部署的贡献。</li>
</ul>

<h3>Title: Title:
          Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction</h3>
<ul>
<li><strong>Authors: </strong>Tingchen Fu, Deng Cai, Lemao Liu, Shuming Shi, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) on instruction-following corpus is a crucial approach toward the alignment of large language models (LLMs). However, the performance of LLMs on standard knowledge and reasoning benchmarks tends to suffer from deterioration at the latter stage of the SFT process, echoing the phenomenon of alignment tax. Through our pilot study, we put a hypothesis that the data biases are probably one cause behind the phenomenon. To address the issue, we introduce a simple disperse-then-merge framework. To be concrete, we disperse the instruction-following data into portions and train multiple sub-models using different data portions. Then we merge multiple models into a single one via model merging techniques. Despite its simplicity, our framework outperforms various sophisticated methods such as data curation and training regularization on a series of standard knowledge and reasoning benchmarks.</li>
<li><strong>摘要：</strong>对指令跟踪语料库的监督微调 (SFT) 是对齐大型语言模型 (LLM) 的重要方法。然而，LLM 在标准知识和推理基准上的表现往往会在 SFT 过程的后期恶化，这与对齐税现象相呼应。通过我们的试点研究，我们提出了一个假设：数据偏差可能是这一现象背后的原因之一。为了解决这个问题，我们引入了一个简单的分散然后合并框架。具体来说，我们将指令跟踪数据分散成多个部分，并使用不同的数据部分训练多个子模型。然后我们通过模型合并技术将多个模型合并为一个模型。尽管很简单，但我们的框架优于各种复杂的方法，例如在一系列标准知识和推理基准上的数据管理和训练正则化。</li>
</ul>

<h3>Title: Title:
          Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning</h3>
<ul>
<li><strong>Authors: </strong>Yuanhao Yue, Chengyu Wang, Jun Huang, Peng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The process of instruction tuning aligns pre-trained large language models (LLMs) with open-domain instructions and human-preferred responses. While several studies have explored autonomous approaches to distilling and annotating instructions from more powerful proprietary LLMs, such as ChatGPT, they often neglect the impact of task distributions and the varying difficulty of instructions of the training sets. This oversight can lead to imbalanced knowledge capabilities and poor generalization powers of small student LLMs. To address this challenge, we introduce Task-Aware Curriculum Planning for Instruction Refinement (TAPIR), a multi-round distillation framework with balanced task distributions and dynamic difficulty adjustment. This approach utilizes an oracle LLM to select instructions that are difficult for a student LLM to follow and distill instructions with balanced task distributions. By incorporating curriculum planning, our approach systematically escalates the difficulty levels, progressively enhancing the student LLM's capabilities. We rigorously evaluate TAPIR using two widely recognized benchmarks, including AlpacaEval 2.0 and MT-Bench. The empirical results demonstrate that the student LLMs, trained with our method and less training data, outperform larger instruction-tuned models and strong distillation baselines. The improvement is particularly notable in complex tasks, such as logical reasoning and code generation.</li>
<li><strong>摘要：</strong>指令调整过程将预先训练的大型语言模型 (LLM) 与开放域指令和人类偏好的响应结合起来。虽然一些研究探索了从更强大的专有法学硕士（例如 ChatGPT）中提取和注释指令的自主方法，但它们经常忽略任务分布的影响和训练集指令的不同难度。这种监督可能会导致小型法学硕士学生的知识能力不平衡和泛化能力较差。为了应对这一挑战，我们引入了用于指令细化的任务感知课程规划（TAPIR），这是一种具有平衡任务分配和动态难度调整的多轮蒸馏框架。这种方法利用预言机法学硕士来选择学生法学硕士难以遵循的指令，并通过平衡的任务分配提炼指令。通过纳入课程规划，我们的方法系统地提升难度级别，逐步增强学生法学硕士的能力。我们使用两个广泛认可的基准测试（包括 AlpacaEval 2.0 和 MT-Bench）严格评估 TAPIR。实证结果表明，使用我们的方法和较少训练数据进行训练的法学硕士学生的表现优于较大的指令调整模型和强大的蒸馏基线。这种改进在复杂任务中尤其显着，例如逻辑推理和代码生成。</li>
</ul>

<h3>Title: Title:
          LIRE: listwise reward enhancement for preference alignment</h3>
<ul>
<li><strong>Authors: </strong>Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LIRE: listwise reward enhancement for preference alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators.</li>
<li><strong>摘要：</strong>最近，人们在将大型语言模型 (LLM) 的生成与人类价值观相结合以减轻有害或无益内容方面取得了巨大进步。利用人类反馈强化学习 (RLHF) 被证明是有效的，并被研究人员广泛采用。然而，实施 RLHF 很复杂，而且它对超参数的敏感性使得实现稳定的性能和可扩展性具有挑战性。此外，偏好对齐的现行方法主要集中在成对比较上，对多响应场景的探索有限，从而忽略了候选池中的潜在丰富性。出于上述原因，我们提出了一种新方法：列表式奖励增强偏好对齐 (LIRE)，这是一种基于梯度的奖励优化方法，将多个响应的离线奖励纳入精简的列表式框架，从而无需在训练期间进行在线采样。LIRE 易于实施，只需进行最少的参数调整，并与成对范式无缝对齐，同时自然扩展到多响应场景。此外，我们引入了一种自我增强算法，旨在在训练过程中迭代优化奖励。我们的实验表明，LIRE 在对话和摘要任务的多个基准测试中始终优于现有方法，并且对分布外数据具有良好的可迁移性，使用代理奖励模型和人工注释器进行评估。</li>
</ul>

<h3>Title: Title:
          Annotation-Efficient Preference Optimization for Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuu Jinnai, Ukyo Honda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Annotation-Efficient Preference Optimization for Language Model Alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Preference optimization is a standard approach to fine-tuning large language models to align with human preferences. The quality, diversity, and quantity of the preference dataset are critical to the effectiveness of preference optimization. However, obtaining a large amount of high-quality and diverse preference annotations is difficult in many applications. This raises the question of how to use the limited annotation budget to create an effective preference dataset. To this end, we propose Annotation-Efficient Preference Optimization (AEPO). Instead of exhaustively annotating preference over all available response texts, AEPO selects a subset of responses that maximizes quality and diversity from the available responses, and then annotates preference over the selected ones. In this way, AEPO focuses the annotation budget on labeling preference over a smaller subset of responses with diversity and of high quality. We evaluate the performance of Direct Preference Optimization (DPO) using AEPO and show that it outperforms models trained using a standard DPO with the same annotation budget. Our code is available at this https URL</li>
<li><strong>摘要：</strong>偏好优化是微调大型语言模型以符合人类偏好的标准方法。偏好数据集的质量、多样性和数量对于偏好优化的有效性至关重要。然而，在许多应用中获得大量高质量且多样化的偏好注释是很困难的。这就提出了如何使用有限的注释预算来创建有效的偏好数据集的问题。为此，我们提出注释高效偏好优化（AEPO）。 AEPO 不是详尽地注释对所有可用响应文本的偏好，而是选择一个响应子集，以最大限度地提高可用响应的质量和多样性，然后注释对所选响应文本的偏好。通过这种方式，AEPO 将注释预算集中在对具有多样性和高质量的较小响应子集的标记偏好上。我们使用 AEPO 评估直接偏好优化 (DPO) 的性能，并表明它优于使用具有相同注释预算的标准 DPO 训练的模型。我们的代码可在此 https URL 获取</li>
</ul>

<h3>Title: Title:
          FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research</h3>
<ul>
<li><strong>Authors: </strong>Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>With the advent of Large Language Models (LLMs), the potential of Retrieval Augmented Generation (RAG) techniques have garnered considerable research attention. Numerous novel algorithms and models have been introduced to enhance various aspects of RAG systems. However, the absence of a standardized framework for implementation, coupled with the inherently intricate RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. Existing RAG toolkits like LangChain and LlamaIndex, while available, are often heavy and unwieldy, failing to meet the personalized needs of researchers. In response to this challenge, we propose FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing existing RAG methods and in developing their own RAG algorithms within a unified framework. Our toolkit implements 12 advanced RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit has various features, including customizable modular framework, rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的出现，检索增强生成 (RAG) 技术的潜力引起了广泛的研究关注。人们引入了许多新颖的算法和模型来增强 RAG 系统的各个方面。然而，由于缺乏标准化的实施框架，再加上固有复杂的 RAG 流程，使得研究人员在一致的环境中比较和评估这些方法变得充满挑战且耗时。现有的 RAG 工具包（如 LangChain 和 LlamaIndex）虽然可用，但往往笨重且难以使用，无法满足研究人员的个性化需求。为了应对这一挑战，我们提出了 FlashRAG，这是一个高效、模块化的开源工具包，旨在帮助研究人员重现现有的 RAG 方法并在统一的框架内开发自己的 RAG 算法。我们的工具包实现了 12 种先进的 RAG 方法，并收集和组织了 32 个基准数据集。我们的工具包具有多种功能，包括可定制的模块化框架、丰富的预实现 RAG 作品集合、全面的数据集、高效的辅助预处理脚本以及广泛且标准的评估指标。我们的工具包和资源可通过此 https URL 获取。</li>
</ul>

<h3>Title: Title:
          ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation</h3>
<ul>
<li><strong>Authors: </strong>Weilong Dong, Xinwei Wu, Renren Jin, Shaoyang Xu, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Ensuring large language models (LLM) behave consistently with human goals, values, and intentions is crucial for their safety but yet computationally expensive. To reduce the computational cost of alignment training of LLMs, especially for those with a huge number of parameters, and to reutilize learned value alignment, we propose ConTrans, a novel framework that enables weak-to-strong alignment transfer via concept transplantation. From the perspective of representation engineering, ConTrans refines concept vectors in value alignment from a source LLM (usually a weak yet aligned LLM). The refined concept vectors are then reformulated to adapt to the target LLM (usually a strong yet unaligned base LLM) via affine transformation. In the third step, ConTrans transplants the reformulated concept vectors into the residual stream of the target LLM. Experiments demonstrate the successful transplantation of a wide range of aligned concepts from 7B models to 13B and 70B models across multiple LLMs and LLM families. Remarkably, ConTrans even surpasses instruction-tuned models in terms of truthfulness. Experiment results validate the effectiveness of both inter-LLM-family and intra-LLM-family concept transplantation. Our work successfully demonstrates an alternative way to achieve weak-to-strong alignment generalization and control.</li>
<li><strong>摘要：</strong>确保大型语言模型 (LLM) 的行为与人类目标、价值观和意图一致对于其安全至关重要，但计算成本高昂。为了减少法学硕士对齐训练的计算成本，特别是对于那些具有大量参数的模型，并重新利用学习到的值对齐，我们提出了 ConTrans，这是一种新颖的框架，可以通过概念移植实现弱到强的对齐迁移。从表示工程的角度来看，ConTrans从源LLM（通常是弱但对齐的LLM）中细化了值对齐的概念向量。然后，通过仿射变换重新表述细化的概念向量，以适应​​目标 LLM（通常是强大但未对齐的基础 LLM）。第三步，ConTrans将重新表述的概念向量移植到目标LLM的剩余流中。实验证明，从 7B 模型到 13B 和 70B 模型的一系列一致概念已成功移植到多个法学硕士和法学硕士系列。值得注意的是，ConTrans 在真实性方面甚至超越了指令调整模型。实验结果验证了LLM家族间和LLM家族内概念移植的有效性。我们的工作成功地展示了一种实现弱到强对齐泛化和控制的替代方法。</li>
</ul>

<h3>Title: Title:
          Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation</h3>
<ul>
<li><strong>Authors: </strong>Gauthier Guinet, Behrooz Omidvar-Tehrani, Anoop Deoras, Laurent Callot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.</li>
<li><strong>摘要：</strong>我们提出了一种新方法来测量检索增强大型语言模型（RAG）的特定于任务的准确性。评估是通过对自动生成的综合考试进行 RAG 评分来进行的，该综合考试由基于与任务相关的文档语料库的多项选择题组成。我们的方法是一种自动化、经济高效、可解释且稳健的策略，用于为 RAG 系统选择最佳组件。我们利用项目反应理论 (IRT) 来评估考试的质量及其在特定任务准确性方面的信息量。 IRT 还提供了一种自然的方式来迭代改进考试，消除那些不能充分体现模型能力的考试问题。我们基于 Arxiv 摘要、StackExchange 问题、AWS DevOps 故障排除指南和 SEC 文件，在四个新的开放式问答任务中展示了我们的方法。此外，我们的实验揭示了对影响 RAG 性能的因素（如大小、检索机制、提示和微调）的更普遍的见解。最值得注意的是，我们的研究结果表明，选择正确的检索算法通常比简单地使用更大的语言模型带来更大的性能提升。</li>
</ul>

<h3>Title: Title:
          Knowledge Graph Reasoning with Self-supervised Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ying Ma, Owen Burns, Mingqiu Wang, Gang Li, Nan Du, Laurent El Shafey, Liqiang Wang, Izhak Shafran, Hagen Soltau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Knowledge Graph Reasoning with Self-supervised Reinforcement Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) is an effective method of finding reasoning pathways in incomplete knowledge graphs (KGs). To overcome the challenges of a large action space, a self-supervised pre-training method is proposed to warm up the policy network before the RL training stage. To alleviate the distributional mismatch issue in general self-supervised RL (SSRL), in our supervised learning (SL) stage, the agent selects actions based on the policy network and learns from generated labels; this self-generation of labels is the intuition behind the name self-supervised. With this training framework, the information density of our SL objective is increased and the agent is prevented from getting stuck with the early rewarded paths. Our self-supervised RL (SSRL) method improves the performance of RL by pairing it with the wide coverage achieved by SL during pretraining, since the breadth of the SL objective makes it infeasible to train an agent with that alone. We show that our SSRL model meets or exceeds current state-of-the-art results on all Hits@k and mean reciprocal rank (MRR) metrics on four large benchmark KG datasets. This SSRL method can be used as a plug-in for any RL architecture for a KGR task. We adopt two RL architectures, i.e., MINERVA and MultiHopKG as our baseline RL models and experimentally show that our SSRL model consistently outperforms both baselines on all of these four KG reasoning tasks. Full code for the paper available at this https URL.</li>
<li><strong>摘要：</strong>强化学习（RL）是在不完整知识图（KG）中寻找推理路径的有效方法。为了克服大动作空间的挑战，提出了一种自监督预训练方法，在 RL 训练阶段之前预热策略网络。为了缓解一般自监督强化学习（SSRL）中的分布不匹配问题，在我们的监督学习（SL）阶段，代理根据策略网络选择动作并从生成的标签中学习；这种自我生成的标签就是自我监督这个名字背后的直觉。通过这个训练框架，我们的 SL 目标的信息密度增加了，并且可以防止代理陷入早期奖励路径。我们的自监督强化学习 (SSRL) 方法通过将其与预训练期间 SL 实现的广泛覆盖范围相结合来提高 RL 的性能，因为 SL 目标的广度使得仅用它来训练代理是不可行的。我们表明，我们的 SSRL 模型在所有 Hits@k 和四个大型基准 KG 数据集上的平均倒数排名 (MRR) 指标上达到或超过了当前最先进的结果。此 SSRL 方法可用作 KGR 任务的任何 RL 架构的插件。我们采用两种 RL 架构，即 MINERVA 和 MultiHopKG 作为我们的基线 RL 模型，并通过实验表明，我们的 SSRL 模型在所有这四个 KG 推理任务上始终优于这两个基线。论文的完整代码可在此 https URL 获取。</li>
</ul>

<h3>Title: Title:
          CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Guangzhi Sun, Potsawee Manakul, Adian Liusie, Kunat Pipatanakul, Chao Zhang, Phil Woodland, Mark Gales</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination</a></li>
<li><strong>Abstract: </strong>Multimodal foundation models are prone to hallucination, generating outputs that either contradict the input or are not grounded by factual information. Given the diversity in architectures, training data and instruction tuning techniques, there can be large variations in systems' susceptibility to hallucinations. To assess system hallucination robustness, hallucination ranking approaches have been developed for specific tasks such as image captioning, question answering, summarization, or biography generation. However, these approaches typically compare model outputs to gold-standard references or labels, limiting hallucination benchmarking for new domains. This work proposes "CrossCheckGPT", a reference-free universal hallucination ranking for multimodal foundation models. The core idea of CrossCheckGPT is that the same hallucinated content is unlikely to be generated by different independent systems, hence cross-system consistency can provide meaningful and accurate hallucination assessment scores. CrossCheckGPT can be applied to any model or task, provided that the information consistency between outputs can be measured through an appropriate distance metric. Focusing on multimodal large language models that generate text, we explore two information consistency measures: CrossCheck-explicit and CrossCheck-implicit. We showcase the applicability of our method for hallucination ranking across various modalities, namely the text, image, and audio-visual domains. Further, we propose the first audio-visual hallucination benchmark, "AVHalluBench", and illustrate the effectiveness of CrossCheckGPT, achieving correlations of 98% and 89% with human judgements on MHaluBench and AVHalluBench, respectively.</li>
<li><strong>摘要：</strong>多模态基础模型容易产生幻觉，产生的输出要么与输入相矛盾，要么没有事实信息作为依据。鉴于架构、训练数据和指令调整技术的多样性，系统对幻觉的敏感性可能会有很大差异。为了评估系统幻觉的稳健性，幻觉排名方法已经为特定任务开发出来，例如图像字幕、问答、摘要或传记生成。然而，这些方法通常将模型输出与黄金标准参考或标签进行比较，限制了新领域的幻觉基准测试。这项工作提出了“CrossCheckGPT”，一种针对多模态基础模型的无参考通用幻觉排名。CrossCheckGPT 的核心思想是，不同的独立系统不太可能生成相同的幻觉内容，因此跨系统一致性可以提供有意义且准确的幻觉评估分数。CrossCheckGPT 可以应用于任何模型或任务，只要可以通过适当的距离度量来测量输出之间的信息一致性。我们专注于生成文本的多模态大型语言模型，探索两种信息一致性度量：CrossCheck-explicit 和 CrossCheck-implicit。我们展示了我们的方法在各种模态（即文本、图像和视听领域）中对幻觉进行排名的适用性。此外，我们提出了第一个视听幻觉基准“AVHalluBench”，并说明了 CrossCheckGPT 的有效性，在 MHaluBench 和 AVHalluBench 上与人类判断的相关性分别达到 98% 和 89%。</li>
</ul>

<h3>Title: Title:
          Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Cyril Chhun, Fabian M. Suchanek, Chloé Clavel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning and deep understanding. Meanwhile, Large Language Models (LLM) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers.</li>
<li><strong>摘要：</strong>讲故事是人类经验中不可或缺的一部分，在社交互动中起着至关重要的作用。因此，自动故事评估 (ASE) 和生成 (ASG) 可以以多种方式造福社会，但它们是具有挑战性的任务，需要人类的高级能力，例如创造力、推理和深度理解。同时，大型语言模型 (LLM) 现在在许多 NLP 任务上都取得了最先进的性能。在本文中，我们研究了 LLM 是否可以用作 ASE 的人工注释者的替代品。我们对 LLM 评级、其他自动测量和人工注释之间的相关性进行了广泛的分析，并探讨了提示对结果和 LLM 行为可解释性的影响。最值得注意的是，我们发现 LLM 在系统级评估方面的表现优于当前的自动测量，但仍难以为其答案提供令人满意的解释。</li>
</ul>

<h3>Title: Title:
          xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token</h3>
<ul>
<li><strong>Authors: </strong>Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, Dongyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems</li>
<li><strong>摘要：</strong>本文介绍了 xRAG，一种专为检索增强生成而定制的创新上下文压缩方法。 xRAG 将密集检索中的文档嵌入（传统上仅用于检索）重新解释为检索模态的特征。通过采用模态融合方法，xRAG 将这些嵌入无缝集成到语言模型表示空间中，有效地消除了对其文本对应项的需求，并实现了极高的压缩率。在 xRAG 中，唯一可训练的组件是模态桥，而检索器和语言模型都保持冻结状态。这种设计选择允许重用离线构建的文档嵌入，并保留检索增强的即插即用性质。实验结果表明，xRAG 在六种知识密集型任务中平均提高了 10% 以上，适用于各种语言模型主干，从密集的 7B 模型到 8x7B 专家混合配置。 xRAG 不仅显着优于以前的上下文压缩方法，而且在多个数据集上与未压缩模型的性能相匹配，同时将总体 FLOP 降低了 3.53 倍。我们的工作从多模态融合的角度开创了检索增强生成的新方向，我们希望它为未来高效且可扩展的检索增强系统奠定基础</li>
</ul>

<h3>Title: Title:
          Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Raghu Mudumbai, Tyler Bell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We propose a new asymptotic equipartition property for the perplexity of a large piece of text generated by a language model and present theoretical arguments for this property. Perplexity, defined as a inverse likelihood function, is widely used as a performance metric for training language models. Our main result states that the logarithmic perplexity of any large text produced by a language model must asymptotically converge to the average entropy of its token distributions. This means that language models are constrained to only produce outputs from a ``typical set", which we show, is a vanishingly small subset of all possible grammatically correct outputs. We present preliminary experimental results from an open-source language model to support our theoretical claims. This work has possible practical applications for understanding and improving ``AI detection" tools and theoretical implications for the uniqueness, predictability and creative potential of generative models.</li>
<li><strong>摘要：</strong>我们针对语言模型生成的大段文本的复杂性提出了一种新的渐近均分性质，并提出了该性质的理论论证。困惑度定义为逆似然函数，被广泛用作训练语言模型的性能指标。我们的主要结果表明，语言模型生成的任何大型文本的对数困惑度必须渐近收敛于其标记分布的平均熵。这意味着语言模型被限制为只能从“典型集合”产生输出，我们表明，这是所有可能的语法正确输出的一个非常小的子集。我们提出了来自开源语言模型的初步实验结果来支持我们的研究这项工作对于理解和改进“人工智能检测”工具以及对生成模型的独特性、可预测性和创造潜力的理论意义具有可能的实际应用。</li>
</ul>

<h3>Title: Title:
          Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary?</h3>
<ul>
<li><strong>Authors: </strong>Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have shown impressive language capabilities. However, most of the existing LLMs are all English-centric, which have very unstable and unbalanced performance across different languages. Multilingual alignment is an effective method to enhance the LLMs' multilingual capabilities. In this work, we explore the multilingual alignment paradigm which utilizes translation data and comprehensively investigate the spontaneous multilingual improvement of LLMs. We find that LLMs only instruction-tuned on question translation data without annotated answers are able to get significant multilingual performance enhancement even across a wide range of languages unseen during instruction-tuning. Additionally, we utilize different settings and mechanistic interpretability methods to comprehensively analyze the LLM's performance in the multilingual scenario.</li>
<li><strong>摘要：</strong>最近，大型语言模型 (LLM) 显示了令人印象深刻的语言能力。然而，现有的LLM大多都是以英语为中心的，跨语言的表现非常不稳定和不平衡。多语言对齐是提升法学硕士多语言能力的有效方法。在这项工作中，我们探索利用翻译数据的多语言对齐范式，并全面研究法学硕士的自发多语言改进。我们发现，仅对没有注释答案的问题翻译数据进行指令调整的法学硕士能够获得显着的多语言性能增强，即使是在指令调整过程中看不见的各种语言中。此外，我们利用不同的设置和机械解释方法来全面分析法学硕士在多语言场景中的表现。</li>
</ul>

<h3>Title: Title:
          Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching</h3>
<ul>
<li><strong>Authors: </strong>Weixiang Zhao, Yulin Hu, Zhuojun Li, Yang Deng, Yanyan Zhao, Bing Qin, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Safety alignment of large language models (LLMs) has been gaining increasing attention. However, current safety-aligned LLMs suffer from the fragile and imbalanced safety mechanisms, which can still be induced to generate unsafe responses, exhibit over-safety by rejecting safe user inputs, and fail to preserve general utility after safety alignment. To this end, we propose a novel post safety alignment (PSA) method to address these inherent and emerging safety challenges, including safety enhancement, over-safety mitigation, and utility preservation. In specific, we introduce \textsc{SafePatching}, a novel framework for comprehensive and efficient PSA, where two distinct safety patches are developed on the harmful data to enhance safety and mitigate over-safety concerns, and then seamlessly integrated into the target LLM backbone without compromising its utility. Extensive experiments show that \textsc{SafePatching} achieves a more comprehensive and efficient PSA than baseline methods. It even enhances the utility of the backbone, further optimizing the balance between being helpful and harmless in current aligned LLMs. Also, \textsc{SafePatching} demonstrates its superiority in continual PSA scenarios.</li>
<li><strong>摘要：</strong>大语言模型（LLM）的安全对齐已经受到越来越多的关注。然而，当前的安全对齐法学硕士受到脆弱和不平衡的安全机制的影响，这些机制仍然可能被诱导产生不安全的响应，通过拒绝安全用户输入而表现出过度安全，并且在安全对齐后无法保留通用实用性。为此，我们提出了一种新颖的后安全调整（PSA）方法来解决这些固有的和新出现的安全挑战，包括安全增强、过度安全缓解和效用保护。具体来说，我们引入了 \textsc{SafePatching}，这是一种全面、高效的 PSA 新颖框架，其中针对有害数据开发了两个不同的安全补丁，以增强安全性并减轻过度安全的担忧，然后无缝集成到目标 LLM 主干中而不影响其实用性。大量实验表明，\textsc{SafePatching} 比基线方法实现了更全面、更高效的 PSA。它甚至增强了主干的实用性，进一步优化了当前一致的法学硕士中有益与无害之间的平衡。此外，\textsc{SafePatching} 在连续 PSA 场景中展示了其优越性。</li>
</ul>

<h3>Title: Title:
          Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Ma, Zekun Wang, Joyce Chai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we aim to examine how corrective feedback from interactions influences neural language acquisition from the ground up through systematically controlled experiments, assessing whether it contributes to learning efficiency in language models. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, and we highlight the significance of both trials and demonstrations. We further show that the teacher's choices of words influence students' word-specific learning efficiency, and a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves. Our findings suggest that interactive language learning, with teacher demonstrations and student trials, can facilitate efficient word learning in language models.</li>
<li><strong>摘要：</strong>人类是高效的语言学习者，也是天生的社会生物。我们的语言发展很大程度上取决于我们的社交互动，例如看护者的示范和反馈。与人类语言学习相反，大型语言模型的最新进展主要采用非交互式训练范式，并通过事后反馈完善预训练模型。在这项工作中，我们的目标是通过系统控制的实验，研究交互的纠正反馈如何从头开始影响神经语言习得，评估它是否有助于语言模型的学习效率。我们引入了一个试用和演示（TnD）学习框架，该框架包含三个组成部分：学生试验、教师演示和根据不同发展阶段的语言能力进行的奖励。我们的实验表明，TnD 方法可以加速参数数量相等和数量较少的学生模型的单词习得速度，并且我们强调了试验和演示的重要性。我们进一步表明，教师对单词的选择会影响学生特定单词的学习效率，并且通过单词在尝试中的频率与其各自的学习曲线之间的强相关性，熟能生巧的效果是显而易见的。我们的研究结果表明，通过教师演示和学生试验的交互式语言学习可以促进语言模型中高效的单词学习。</li>
</ul>

<h3>Title: Title:
          Semantic Density: Uncertainty Quantification in Semantic Space for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Qiu, Risto Miikkulainen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Semantic Density: Uncertainty Quantification in Semantic Space for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation. Existing LLMs do not have an inherent functionality to provide the users with an uncertainty metric for each response it generates, making it difficult to evaluate trustworthiness. Although a number of works aim to develop uncertainty quantification methods for LLMs, they have fundamental limitations, such as being restricted to classification tasks, requiring additional training and data, considering only lexical instead of semantic information, and being prompt-wise but not response-wise. A new framework is proposed in this paper to address these issues. Semantic density extracts uncertainty information for each response from a probability distribution perspective in semantic space. It has no restriction on task types and is "off-the-shelf" for new models and tasks. Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and Mixtral-8x22B models, on four free-form question-answering benchmarks demonstrate the superior performance and robustness of semantic density compared to prior approaches.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）在各个领域的广泛应用，人们对 LLM 在安全关键场景中的可信度产生了担忧，因为它们容易产生幻觉和产生错误信息，无法预测。现有的法学硕士不具备为用户提供其生成的每个响应的不确定性度量的固有功能，因此难以评估可信度。尽管许多工作旨在开发法学硕士的不确定性量化方法，但它们具有根本的局限性，例如仅限于分类任务，需要额外的训练和数据，仅考虑词汇而不是语义信息，以及提示明智但不响应 -明智的。本文提出了一个新的框架来解决这些问题。语义密度从语义空间中的概率分布角度提取每个响应的不确定性信息。它对任务类型没有限制，并且对于新模型和任务来说是“现成的”。在四个自由形式问答基准上对七个最先进的 LLM（包括最新的 Llama 3 和 Mixtral-8x22B 模型）进行的实验证明了与之前的方法相比，语义密度的卓越性能和鲁棒性。</li>
</ul>

<h3>Title: Title:
          Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries</h3>
<ul>
<li><strong>Authors: </strong>Adam Yang, Chen Chen, Konstantinos Pitas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>State-of-the-art large language models are sometimes distributed as open-source software but are also increasingly provided as a closed-source service. These closed-source large-language models typically see the widest usage by the public, however, they often do not provide an estimate of their uncertainty when responding to queries. As even the best models are prone to ``hallucinating" false information with high confidence, a lack of a reliable estimate of uncertainty limits the applicability of these models in critical settings. We explore estimating the uncertainty of closed-source LLMs via multiple rephrasings of an original base query. Specifically, we ask the model, multiple rephrased questions, and use the similarity of the answers as an estimate of uncertainty. We diverge from previous work in i) providing rules for rephrasing that are simple to memorize and use in practice ii) proposing a theoretical framework for why multiple rephrased queries obtain calibrated uncertainty estimates. Our method demonstrates significant improvements in the calibration of uncertainty estimates compared to the baseline and provides intuition as to how query strategies should be designed for optimal test calibration.</li>
<li><strong>摘要：</strong>最先进的大型语言模型有时作为开源软件分发，但也越来越多地作为闭源服务提供。这些闭源大语言模型通常受到公众最广泛的使用，但是，它们在响应查询时通常不提供对其不确定性的估计。由于即使是最好的模型也容易产生高置信度的“幻觉”虚假信息，缺乏对不确定性的可靠估计限制了这些模型在关键环境中的适用性。我们探索通过多种改写来估计闭源法学硕士的不确定性具体来说，我们询问模型、多个重新表述的问题，并使用答案的相似性作为不确定性的估计。我们与 i) 中提供的易于记忆和在实践中使用的重新表述规则不同。 ii）提出一个理论框架，解释为什么多个改写的查询获得校准的不确定性估计。我们的方法证明了与基线相比，不确定性估计的校准有了显着的改进，并提供了如何设计查询策略以实现最佳测试校准的直觉。</li>
</ul>

<h3>Title: Title:
          Why Not Transform Chat Large Language Models to Non-English?</h3>
<ul>
<li><strong>Authors: </strong>Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, Chang Su, Yanqing Zhao, Min Zhang, Hao Yang, Xinglin Lyu, Jiajun Chen, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Why Not Transform Chat Large Language Models to Non-English?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.</li>
<li><strong>摘要：</strong>非英语数据的稀缺限制了非英语大语言模型（LLM）的发展。将以英语为中心的法学硕士转变为非英语的法学硕士已被认为是一种有效且资源高效的方法。以前的工作从基础法学硕士开始，并使用更强的法学硕士生成的数据进行知识蒸馏（KD），例如GPT-4。与基础法学硕士相比，聊天法学硕士针对高级能力进行了进一步优化，例如多轮对话和人类偏好一致，因此在帮助性和安全性方面都更加强大。然而，改造聊天LLM涉及两个关键问题：（1）如何在没有监督数据的情况下有效地转移高级能力？ （2）如何防止原有知识在转型过程中发生灾难性遗忘？我们通过引入一个名为 TransLLM 的简单框架来解决这些问题。对于第一期，TransLLM通过翻译思路将迁移问题分解为一些常见的子任务，逐步将翻译作为英语和非英语之间的桥梁。我们利用公开数据进一步提高子任务的性能。对于第二个问题，我们提出了一种包含两个协同组件的方法：用于训练的低秩适应以维持原始LLM参数，以及恢复KD，它利用聊天LLM本身生成的数据从冻结的参数中恢复原始知识。在实验中，我们将 LLaMA-2-chat-7B 转换为泰语。我们的方法仅使用单轮数据，在多轮基准 MT 平台上的性能优于强基线和 ChatGPT。此外，我们的方法在没有安全数据的情况下，比 ChatGPT 和 GPT-4 拒绝更多有害的安全基准 AdvBench 查询。</li>
</ul>

<h3>Title: Title:
          Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian</h3>
<ul>
<li><strong>Authors: </strong>Aleksandr Nikolich, Konstantin Korolev, Artem Shelmanov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>There has been a surge in the development of various Large Language Models (LLMs). However, text generation for languages other than English often faces significant challenges, including poor generation quality and the reduced computational performance due to the disproportionate representation of tokens in model's vocabulary. In this work, we address these issues and introduce Vikhr, a new state-of-the-art open-source instruction-tuned LLM designed specifically for the Russian language. Unlike previous efforts for Russian that utilize computationally inexpensive LoRA adapters on top of English-oriented models, Vikhr features an adapted tokenizer vocabulary and undergoes the continued pre-training and instruction tuning of all weights. This approach not only enhances the model's performance but also significantly improves its computational and contextual efficiency. The remarkable performance of Vikhr across various Russian-language benchmarks can also be attributed to our efforts in expanding instruction datasets and corpora for continued pre-training. Vikhr not only sets the new state of the art among open-source LLMs for Russian, but even outperforms some proprietary closed-source models on certain benchmarks. The model weights, instruction sets, and code are publicly available</li>
<li><strong>摘要：</strong>各种大型语言模型（LLM）的发展激增。然而，英语以外语言的文本生成通常面临重大挑战，包括生成质量差以及由于模型词汇表中标记的表示不成比例而导致计算性能降低。在这项工作中，我们解决了这些问题并引入了 Vikhr，这是一种专为俄语设计的新型、最先进的开源指令调整法学硕士。与之前针对俄语的工作不同，Vikhr 在面向英语的模型上使用计算成本低廉的 LoRA 适配器，它具有适应的分词器词汇，并对所有权重进行持续的预训练和指令调整。这种方法不仅增强了模型的性能，而且还显着提高了其计算和上下文效率。 Vikhr 在各种俄语基准测试中的出色表现也可以归功于我们在扩展指令数据集和语料库以进行持续预训练方面所做的努力。 Vikhr 不仅在俄语开源法学硕士中树立了新的技术水平，甚至在某些基准上优于一些专有的闭源模型。模型权重、指令集和代码都是公开的</li>
</ul>

<h3>Title: Title:
          DeTox: Toxic Subspace Projection for Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Rheeya Uppaal, Apratim De, Yiting He, Yiquao Zhong, Junjie Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DeTox: Toxic Subspace Projection for Model Editing(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, making them prone to jailbreaking and inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative (DeTox) and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, DeTox is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic sub-space is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that DeTox is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we establish both theoretical and empirical connections between DeTox and DPO, showing that DeTox can be interpreted as a denoised version of a single DPO step.</li>
<li><strong>摘要：</strong>最近开发的对齐算法，例如直接偏好优化（DPO），通过训练这些模型来匹配偏好数据所例证的人类行为，以提高大型语言模型（LLM）的安全性。然而，这些方法计算量大，缺乏可控性和透明度，容易被越狱，限制了其广泛使用。此外，这些基于调整的方法需要大规模的偏好数据进行训练，并且容易受到噪声偏好数据的影响。在本文中，我们介绍了一种免调整对齐替代方案（DeTox），并证明了其在毒性降低用例下的有效性。 DeTox 基于因子分析理论，是一种样本高效的模型编辑方法，可识别模型参数空间中的有毒子空间，并通过投影检测到的子空间来降低模型毒性。通过从语言模型中提取偏好数据嵌入并从这些嵌入中删除无毒信息来识别有毒子空间。我们证明 DeTox 比 DPO 的样本效率更高，进一步展示了对噪声数据的更强鲁棒性。最后，我们在 DeTox 和 DPO 之间建立了理论和经验联系，表明 DeTox 可以解释为单个 DPO 步骤的去噪版本。</li>
</ul>

<h3>Title: Title:
          CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Giada Pistilli, Alina Leidinger, Yacine Jernite, Atoosa Kasirzadeh, Alexandra Sasha Luccioni, Margaret Mitchell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper introduces the "CIVICS: Culturally-Informed & Values-Inclusive Corpus for Societal impacts" dataset, designed to evaluate the social and cultural variation of Large Language Models (LLMs) across multiple languages and value-sensitive topics. We create a hand-crafted, multilingual dataset of value-laden prompts which address specific socially sensitive topics, including LGBTQI rights, social welfare, immigration, disability rights, and surrogacy. CIVICS is designed to generate responses showing LLMs' encoded and implicit values. Through our dynamic annotation processes, tailored prompt design, and experiments, we investigate how open-weight LLMs respond to value-sensitive issues, exploring their behavior across diverse linguistic and cultural contexts. Using two experimental set-ups based on log-probabilities and long-form responses, we show social and cultural variability across different LLMs. Specifically, experiments involving long-form responses demonstrate that refusals are triggered disparately across models, but consistently and more frequently in English or translated statements. Moreover, specific topics and sources lead to more pronounced differences across model answers, particularly on immigration, LGBTQI rights, and social welfare. As shown by our experiments, the CIVICS dataset aims to serve as a tool for future research, promoting reproducibility and transparency across broader linguistic settings, and furthering the development of AI technologies that respect and reflect global cultural diversities and value pluralism. The CIVICS dataset and tools will be made available upon publication under open licenses; an anonymized version is currently available at this https URL.</li>
<li><strong>摘要：</strong>本文介绍了“CIVICS：文化知情和价值观包容的社会影响语料库”数据集，旨在评估跨多种语言和价值敏感主题的大型语言模型（LLM）的社会和文化差异。我们创建了一个手工制作的多语言数据集，包含有价值的提示，涉及特定的社会敏感主题，包括 LGBTQI 权利、社会福利、移民、残疾人权利和代孕。 CIVICS 旨在生成显示法学硕士的编码值和隐含值的响应。通过我们的动态注释流程、量身定制的提示设计和实验，我们研究了开放权重法学硕士如何应对价值敏感问题，探索他们在不同语言和文化背景下的行为。使用基于对数概率和长格式响应的两个实验设置，我们展示了不同法学硕士之间的社会和文化差异。具体来说，涉及长格式响应的实验表明，拒绝在不同模型中的触发方式不同，但在英语或翻译语句中一致且更频繁地触发。此外，特定的主题和来源会导致模型答案之间存在更明显的差异，特别是在移民、LGBTQI 权利和社会福利方面。正如我们的实验所示，CIVICS 数据集旨在作为未来研究的工具，促进更广泛语言环境的可重复性和透明度，并进一步发展尊重和反映全球文化多样性和价值观多元化的人工智能技术。 CIVICS 数据集和工具将在开放许可下发布后提供；目前，此 https URL 提供匿名版本。</li>
</ul>

<h3>Title: Title:
          Feedback-aligned Mixed LLMs for Machine Language-Molecule Translation</h3>
<ul>
<li><strong>Authors: </strong>Dimitris Gkoumas, Maria Liakata</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Feedback-aligned Mixed LLMs for Machine Language-Molecule Translation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The intersection of chemistry and Artificial Intelligence (AI) is an active area of research focused on accelerating scientific discovery. While using large language models (LLMs) with scientific modalities has shown potential, there are significant challenges to address, such as improving training efficiency and dealing with the out-of-distribution problem. Focussing on the task of automated language-molecule translation, we are the first to use state-of-the art (SOTA) human-centric optimisation algorithms in the cross-modal setting, successfully aligning cross-language-molecule modals. We empirically show that we can augment the capabilities of scientific LLMs without the need for extensive data or large models. We conduct experiments using only 10% of the available data to mitigate memorisation effects associated with training large models on extensive datasets. We achieve significant performance gains, surpassing the best benchmark model trained on extensive in-distribution data by a large margin and reach new SOTA levels. Additionally we are the first to propose employing non-linear fusion for mixing cross-modal LLMs which further boosts performance gains without increasing training costs or data needs. Finally, we introduce a fine-grained, domain-agnostic evaluation method to assess hallucination in LLMs and promote responsible use.</li>
<li><strong>摘要：</strong>化学与人工智能 (AI) 的交叉点是一个活跃的研究领域，专注于加速科学发现。虽然使用具有科学模式的大型语言模型 (LLM) 已显示出潜力，但仍存在重大挑战需要解决，例如提高培训效率和处理分布外问题。专注于自动化语言分子翻译任务，我们是第一个在跨模态设置中使用最先进（SOTA）以人为中心的优化算法，成功对齐跨语言分子模态。我们的经验表明，我们可以增强科学法学硕士的能力，而无需大量数据或大型模型。我们仅使用 10% 的可用数据进行实验，以减轻与在广泛数据集上训练大型模型相关的记忆效应。我们取得了显着的性能提升，大幅超越了在广泛的分布数据上训练的最佳基准模型，并达到了新的 SOTA 水平。此外，我们是第一个提出采用非线性融合来混合跨模式 LLM 的人，这可以在不增加培训成本或数据需求的情况下进一步提高性能收益。最后，我们引入了一种细粒度、与领域无关的评估方法来评估法学硕士的幻觉并促进负责任的使用。</li>
</ul>

<h3>Title: Title:
          Evaluating Large Language Models with Human Feedback: Establishing a Swedish Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Birger Moell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating Large Language Models with Human Feedback: Establishing a Swedish Benchmark(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving field of artificial intelligence, large language models (LLMs) have demonstrated significant capabilities across numerous applications. However, the performance of these models in languages with fewer resources, such as Swedish, remains under-explored. This study introduces a comprehensive human benchmark to assess the efficacy of prominent LLMs in understanding and generating Swedish language texts using forced choice ranking. We employ a modified version of the ChatbotArena benchmark, incorporating human feedback to evaluate eleven different models, including GPT-4, GPT-3.5, various Claude and Llama models, and bespoke models like Dolphin-2.9-llama3b-8b-flashback and BeagleCatMunin. These models were chosen based on their performance on LMSYS chatbot arena and the Scandeval benchmarks. We release the this http URL benchmark as a tool to improve our understanding of language model performance in Swedish with the hopes that it will be widely used. We aim to create a leaderboard once sufficient data has been collected and analysed.</li>
<li><strong>摘要：</strong>在快速发展的人工智能领域，大型语言模型 (LLM) 已在众多应用程序中展示了重要的功能。然而，这些模型在资源较少的语言（例如瑞典语）中的性能仍有待探索。这项研究引入了一个全面的人类基准，以评估著名法学硕士在使用强制选择排名理解和生成瑞典语文本方面的功效。我们采用 ChatbotArena 基准的修改版本，结合人类反馈来评估 11 种不同的模型，包括 GPT-4、GPT-3.5、各种 Claude 和 Llama 模型，以及 Dolphin-2.9-llama3b-8b-flashback 和 BeagleCatMunin 等定制模型。这些模型是根据它们在 LMSYS 聊天机器人竞技场和 Scandeval 基准测试中的性能而选择的。我们发布此 http URL 基准测试作为工具，以提高我们对瑞典语语言模型性能的理解，并希望它能够得到广泛使用。我们的目标是在收集和分析足够的数据后创建一个排行榜。</li>
</ul>

<h3>Title: Title:
          Trajectory Volatility for Out-of-Distribution Detection in Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Zhuosheng Zhang, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Trajectory Volatility for Out-of-Distribution Detection in Mathematical Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Real-world data deviating from the independent and identically distributed (i.i.d.) assumption of in-distribution training data poses security threats to deep networks, thus advancing out-of-distribution (OOD) detection algorithms. Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation. However, another complex generative scenario mathematical reasoning poses significant challenges to embedding-based methods due to its high-density feature of output spaces, but this feature causes larger discrepancies in the embedding shift trajectory between different samples in latent spaces. Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning. Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions.</li>
<li><strong>摘要：</strong>现实世界的数据偏离分布内训练数据的独立同分布（i.i.d.）假设，对深度网络构成安全威胁，从而推进了分布外（OOD）检测算法。生成语言模型（GLM）中的检测方法主要集中在不确定性估计和嵌入距离测量上，后者被证明在摘要和翻译等传统语言任务中最有效。然而，另一种复杂的生成场景数学推理由于其输出空间的高密度特征而对基于嵌入的方法提出了重大挑战，但该特征导致潜在空间中不同样本之间的嵌入移动轨迹存在较大差异。因此，我们提出了一种基于轨迹的方法 TV Score，它利用轨迹波动性进行数学推理中的 OOD 检测。实验表明，我们的方法在数学推理场景下优于 GLM 上的所有传统算法，并且可以扩展到输出空间中具有高密度特征的更多应用，例如多项选择题。</li>
</ul>

<h3>Title: Title:
          Your Large Language Models Are Leaving Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Hope McGovern, Rickard Stureborg, Yoshi Suhara, Dimitris Alikaniotis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Your Large Language Models Are Leaving Fingerprints(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>It has been shown that finetuned transformers and other supervised detectors effectively distinguish between human and machine-generated text in some situations arXiv:2305.13242, but we find that even simple classifiers on top of n-gram and part-of-speech features can achieve very robust performance on both in- and out-of-domain data. To understand how this is possible, we analyze machine-generated output text in five datasets, finding that LLMs possess unique fingerprints that manifest as slight differences in the frequency of certain lexical and morphosyntactic features. We show how to visualize such fingerprints, describe how they can be used to detect machine-generated text and find that they are even robust across textual domains. We find that fingerprints are often persistent across models in the same model family (e.g. llama-13b vs. llama-65b) and that models fine-tuned for chat are easier to detect than standard language models, indicating that LLM fingerprints may be directly induced by the training data.</li>
<li><strong>摘要：</strong>事实证明，在某些情况下，经过微调的 Transformer 和其他监督检测器可以有效区分人类和机器生成的文本 arXiv:2305.13242，但我们发现，即使是基于 n-gram 和词性特征的简单分类器也可以在域内和域外数据上实现非常稳健的性能。为了了解这是如何实现的，我们分析了五个数据集中机器生成的输出文本，发现 LLM 拥有独特的指纹，表现为某些词汇和形态句法特征的频率略有不同。我们展示了如何可视化这些指纹，描述了如何使用它们来检测机器生成的文本，并发现它们甚至在文本域中都很稳健。我们发现指纹通常在同一模型系列中的模型之间是持久的（例如 llama-13b 与 llama-65b），并且针对聊天进行微调的模型比标准语言模型更容易检测，这表明 LLM 指纹可能是由训练数据直接诱导的。</li>
</ul>

<h3>Title: Title:
          $T^2$ of Thoughts: Temperature Tree Elicits Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chengkun Cai, Xu Zhao, Yucheng Du, Haoliang Liu, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          $T^2$ of Thoughts: Temperature Tree Elicits Reasoning in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, especially in complex decision-making scenarios, but their static problem-solving strategies often limit their adaptability to dynamic environments. We explore the enhancement of reasoning capabilities in LLMs through Temperature Tree ($T^2$) prompting via Particle Swarm Optimization, termed as $T^2$ of Thoughts ($T^2oT$). The primary focus is on enhancing decision-making processes by dynamically adjusting search parameters, especially temperature, to improve accuracy without increasing computational demands. We empirically validate that our hybrid $T^2oT$ approach yields enhancements in, single-solution accuracy, multi-solution generation and text generation quality. Our findings suggest that while dynamic search depth adjustments based on temperature can yield mixed results, a fixed search depth, when coupled with adaptive capabilities of $T^2oT$, provides a more reliable and versatile problem-solving strategy. This work highlights the potential for future explorations in optimizing algorithmic interactions with foundational language models, particularly illustrated by our development for the Game of 24 and Creative Writing tasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已成为人工智能领域的强大工具，尤其是在复杂的决策场景中，但其静态问题解决策略往往限制了其对动态环境的适应性。我们通过粒子群优化的温度树 ($T^2$) 提示探索法学硕士推理能力的增强，称为思想的 $T^2$ ($T^2oT$)。主要重点是通过动态调整搜索参数（尤其是温度）来增强决策过程，以在不增加计算需求的情况下提高准确性。我们凭经验验证我们的混合 $T^2oT$ 方法可以提高单解决方案准确性、多解决方案生成和文本生成质量。我们的研究结果表明，虽然基于温度的动态搜索深度调整可能会产生混合结果，但固定搜索深度与 $T^2oT$ 的自适应功能相结合，可以提供更可靠、更通用的问题解决策略。这项工作凸显了未来探索优化算法与基础语言模型交互的潜力，特别是我们对 24 人游戏和创意写作任务的开发就体现了这一点。</li>
</ul>

<h3>Title: Title:
          Large Language Models Can Self-Correct with Minimal Effort</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models Can Self-Correct with Minimal Effort(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple yet effective verification method can unleash inherent capabilities of the LLMs. That is to mask a key condition in the question, add the current response to construct a verification question, and predict the condition to verify the response. The condition can be an entity in an open-domain question or a numeric value in a math question, which requires minimal effort (via prompting) to identify. We propose an iterative verify-then-correct framework to progressively identify and correct (probably) false responses, named ProCo. We conduct experiments on three reasoning tasks. On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact match on four open-domain question answering datasets, $+14.1$ accuracy on three arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense reasoning dataset, compared to Self-Correct.</li>
<li><strong>摘要：</strong>内在自我纠正是一种指示大型语言模型（LLM）在没有外部反馈的情况下验证和纠正其响应的方法。不幸的是，该研究得出的结论是，法学硕士还无法自我纠正推理。我们发现一种简单而有效的验证方法可以释放法学硕士的固有能力。即屏蔽问题中的一个关键条件，添加当前响应来构建验证问题，并预测条件来验证响应。条件可以是开放域问题中的实体或数学问题中的数值，只需最少的努力（通过提示）即可识别。我们提出了一个迭代验证然后纠正的框架来逐步识别和纠正（可能的）错误响应，名为 ProCo。我们对三个推理任务进行了实验。平均而言，以 GPT-3.5-Turbo 作为后端 LLM 的 ProCo 在四个开放域问答数据集上产生 $+6.8$ 精确匹配，在三个算术推理数据集上产生 $+14.1$ 准确度，在三个算术推理数据集上产生 $+9.6$ 准确度与自我纠正相比，常识推理数据集。</li>
</ul>

<h3>Title: Title:
          Knowledge Localization: Mission Not Accomplished? Enter Query Localization!</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Knowledge Localization: Mission Not Accomplished? Enter Query Localization!(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear. The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the knowledge localization (KL) assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons. However, this assumption may be overly strong regarding knowledge storage and neglects knowledge expression mechanisms. Thus, we re-examine the KL assumption and confirm the existence of facts that do not adhere to it from both statistical and knowledge modification perspectives. Furthermore, we propose the Query Localization (QL) assumption. (1) Query-KN Mapping: The localization results are associated with the query rather than the fact. (2) Dynamic KN Selection: The attention module contributes to the selection of KNs for answering a query. Based on this, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification. We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously validate our conclusions.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）存储了大量的事实知识，但其存储和表达这些知识的机制尚不清楚。知识神经元（KN）论题是解释这些机制的一个著名理论。该理论基于知识本地化（KL）假设，该假设认为事实可以定位到少数知识存储单元，即知识神经元。然而，这个假设对于知识存储可能过于强硬，而忽略了知识表达机制。因此，我们重新审视了KL假设，并从统计和知识修改的角度证实了不符合该假设的事实的存在。此外，我们提出了查询本地化（QL）假设。（1）查询-KN映射：定位结果与查询相关，而不是与事实相关。（2）动态KN选择：注意模块有助于选择用于回答查询的KN。在此基础上，我们进一步提出了一致性感知的KN修改方法，提高了知识修改的性能。我们进行了 39 组实验以及额外的可视化实验，以严格验证我们的结论。</li>
</ul>

<h3>Title: Title:
          AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability</h3>
<ul>
<li><strong>Authors: </strong>Fei Zhao, Taotian Pang, Chunhui Li, Zhen Wu, Junjie Guo, Shangyu Xing, Xinyu Dai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) are widely regarded as crucial in the exploration of Artificial General Intelligence (AGI). The core of MLLMs lies in their capability to achieve cross-modal alignment. To attain this goal, current MLLMs typically follow a two-phase training paradigm: the pre-training phase and the instruction-tuning phase. Despite their success, there are shortcomings in the modeling of alignment capabilities within these models. Firstly, during the pre-training phase, the model usually assumes that all image-text pairs are uniformly aligned, but in fact the degree of alignment between different image-text pairs is inconsistent. Secondly, the instructions currently used for finetuning incorporate a variety of tasks, different tasks's instructions usually require different levels of alignment capabilities, but previous MLLMs overlook these differentiated alignment needs. To tackle these issues, we propose a new multimodal large language model AlignGPT. In the pre-training stage, instead of treating all image-text pairs equally, we assign different levels of alignment capabilities to different image-text pairs. Then, in the instruction-tuning phase, we adaptively combine these different levels of alignment capabilities to meet the dynamic alignment needs of different instructions. Extensive experimental results show that our model achieves competitive performance on 12 benchmarks.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）被广泛认为在通用人工智能（AGI）的探索中至关重要。 MLLM 的核心在于其实现跨模式对齐的能力。为了实现这一目标，当前的 MLLM 通常遵循两阶段训练范例：预训练阶段和指令调整阶段。尽管取得了成功，但这些模型中的对齐能力建模仍存在缺陷。首先，在预训练阶段，模型通常假设所有图文对都是均匀对齐的，但实际上不同图文对之间的对齐程度不一致。其次，目前用于微调的指令包含多种任务，不同任务的指令通常需要不同级别的对齐能力，但以前的MLLM忽略了这些差异化的对齐需求。为了解决这些问题，我们提出了一种新的多模态大语言模型 AlignGPT。在预训练阶段，我们没有同等对待所有图像文本对，而是为不同的图像文本对分配不同级别的对齐能力。然后，在指令调优阶段，我们自适应地组合这些不同级别的对齐能力，以满足不同指令的动态对齐需求。大量实验结果表明，我们的模型在 12 个基准测试中实现了具有竞争力的性能。</li>
</ul>

<h3>Title: Title:
          ViHateT5: Enhancing Hate Speech Detection in Vietnamese With A Unified Text-to-Text Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Luan Thanh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ViHateT5: Enhancing Hate Speech Detection in Vietnamese With A Unified Text-to-Text Transformer Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in hate speech detection (HSD) in Vietnamese have made significant progress, primarily attributed to the emergence of transformer-based pre-trained language models, particularly those built on the BERT architecture. However, the necessity for specialized fine-tuned models has resulted in the complexity and fragmentation of developing a multitasking HSD system. Moreover, most current methodologies focus on fine-tuning general pre-trained models, primarily trained on formal textual datasets like Wikipedia, which may not accurately capture human behavior on online platforms. In this research, we introduce ViHateT5, a T5-based model pre-trained on our proposed large-scale domain-specific dataset named VOZ-HSD. By harnessing the power of a text-to-text architecture, ViHateT5 can tackle multiple tasks using a unified model and achieve state-of-the-art performance across all standard HSD benchmarks in Vietnamese. Our experiments also underscore the significance of label distribution in pre-training data on model efficacy. We provide our experimental materials for research purposes, including the VOZ-HSD dataset, pre-trained checkpoint, the unified HSD-multitask ViHateT5 model, and related source code on GitHub publicly.</li>
<li><strong>摘要：</strong>越南语仇恨语音检测 (HSD) 的最新进展取得了重大进展，这主要归功于基于 Transformer 的预训练语言模型的出现，特别是基于 BERT 架构的模型。然而，专业微调模型的必要性导致了开发多任务 HSD 系统的复杂性和碎片化。此外，当前大多数方法都侧重于微调一般预训练模型，主要在维基百科等正式文本数据集上进行训练，这些数据集可能无法准确捕获在线平台上的人类行为。在这项研究中，我们引入了 ViHateT5，这是一种基于 T5 的模型，在我们提出的名为 VOZ-HSD 的大规模特定领域数据集上进行了预训练。通过利用文本到文本架构的强大功能，ViHateT5 可以使用统一模型处理多项任务，并在越南语的所有标准 HSD 基准测试中实现最先进的性能。我们的实验还强调了预训练数据中标签分布对模型功效的重要性。我们在 GitHub 上公开提供用于研究目的的实验材料，包括 VOZ-HSD 数据集、预训练检查点、统一的 HSD-多任务 ViHateT5 模型以及相关源代码。</li>
</ul>

<h3>Title: Title:
          Super Tiny Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dylan Hillier, Leon Guertler, Cheston Tan, Palaash Agrawal, Chen Ruirui, Bobby Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Super Tiny Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research efforts focused on Super Tiny Language Models (STLMs), which aim to deliver high performance with significantly reduced parameter counts. We explore innovative techniques such as byte-level tokenization with a pooling mechanism, weight tying, and efficient training strategies. These methods collectively reduce the parameter count by $90\%$ to $95\%$ compared to traditional models while maintaining competitive performance. This series of papers will explore into various subproblems, including tokenizer-free models, self-play based training, and alternative training objectives, targeting models with 10M, 50M, and 100M parameters. Our ultimate goal is to make high-performance language models more accessible and practical for a wide range of applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展显著提高了自然语言处理的性能，但也因其高计算和能源需求而带来了挑战。本文介绍了一系列专注于超小型语言模型 (STLM) 的研究成果，旨在通过显著减少参数数量来实现高性能。我们探索了创新技术，例如具有池化机制的字节级标记化、权重绑定和高效的训练策略。与传统模型相比，这些方法共同将参数数量减少了 $90\%$ 到 $95\%$，同时保持了竞争力。本系列论文将探讨各种子问题，包括无标记器模型、基于自我对弈的训练和替代训练目标，针对具有 10M、50M 和 100M 个参数的模型。我们的最终目标是让高性能语言模型更易于访问和适用于广泛的应用。</li>
</ul>

<h3>Title: Title:
          Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Geguang Pu, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the rising popularity of Large Language Models (LLMs), assessing their trustworthiness through security tasks has gained critical importance. Regarding the new task of universal goal hijacking, previous efforts have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt. To fill this gap, we propose a universal goal hijacking method called POUGH that incorporates semantic-guided prompt processing strategies. Specifically, the method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes the prompts. Once the prompts are organized sequentially, the method employs an iterative optimization algorithm to generate the universal fixed suffix for the prompts. Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness of our method.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的日益普及，通过安全任务评估其可信度变得至关重要。对于通用目标劫持这个新任务，之前的努力仅仅集中在优化算法上，而忽略了提示的关键作用。为了填补这一空白，我们提出了一种名为 POUGH 的通用目标劫持方法，该方法结合了语义引导的提示处理策略。具体来说，该方法从采样策略开始，从候选池中选择代表性提示，然后是对提示进行优先级排序的排名策略。一旦按顺序组织提示，该方法就会采用迭代优化算法来生成提示的通用固定后缀。对四种流行的法学硕士和十种目标反应进行的实验验证了我们方法的有效性。</li>
</ul>

<h3>Title: Title:
          Agent Planning with World Knowledge Model</h3>
<ul>
<li><strong>Authors: </strong>Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Agent Planning with World Knowledge Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ''real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. Code will be available at this https URL.</li>
<li><strong>摘要：</strong>最近直接使用大型语言模型（LLM）作为代理模型来执行交互式规划任务的努力已经显示出值得称赞的结果。然而，尽管他们取得了成就，但由于对“真实”物理世界的了解不足，他们仍然在全球规划中进行无脑的试错，并在局部规划中产生幻觉。模仿人类的心理世界知识模型，在任务之前提供全局先验知识并在任务期间维护局部动态知识，在本文中，我们引入参数化世界知识模型（WKM）来促进智能体规划。具体来说，我们引导代理模型从专家轨迹和采样轨迹中自我合成知识。然后我们开发WKM，提供先验任务知识来指导全局规划和动态状态知识来协助局部规划。使用三个最先进的开源 LLM（Mistral-7B、Gemma-7B 和 Llama-3-8B）在三个复杂的现实世界模拟数据集上进行的实验结果表明，与各种方法相比，我们的方法可以实现卓越的性能。强大的基线。此外，我们还分析说明，我们的WKM可以有效缓解盲目试错和幻觉行动问题，为智能体对世界的理解提供强有力的支持。其他有趣的发现包括：1）我们的实例级任务知识可以更好地泛化到未见过的任务，2）弱 WKM 可以指导强代理模型规划，3）统一的 WKM 训练具有进一步发展的良好潜力。代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: Title:
          ChronosLex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks</h3>
<ul>
<li><strong>Authors: </strong>T.Y.S.S Santosh, Tuan-Quang Vuong, Matthias Grabmair</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ChronosLex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>This study investigates the challenges posed by the dynamic nature of legal multi-label text classification tasks, where legal concepts evolve over time. Existing models often overlook the temporal dimension in their training process, leading to suboptimal performance of those models over time, as they treat training data as a single homogeneous block. To address this, we introduce ChronosLex, an incremental training paradigm that trains models on chronological splits, preserving the temporal order of the data. However, this incremental approach raises concerns about overfitting to recent data, prompting an assessment of mitigation strategies using continual learning and temporal invariant methods. Our experimental results over six legal multi-label text classification datasets reveal that continual learning methods prove effective in preventing overfitting thereby enhancing temporal generalizability, while temporal invariant methods struggle to capture these dynamics of temporal shifts.</li>
<li><strong>摘要：</strong>本研究调查了法律多标签文本分类任务的动态特性所带来的挑战，其中法律概念会随着时间的推移而演变。现有模型在训练过程中经常忽略时间维度，导致这些模型随着时间的推移性能不佳，因为它们将训练数据视为单个同质块。为了解决这个问题，我们引入了 ChronosLex，这是一种增量训练范式，它按时间顺序分割训练模型，保留数据的时间顺序。然而，这种增量方法引发了对最近数据的过度拟合的担忧，促使人们使用持续学习和时间不变方法来评估缓解策略。我们在六个法律多标签文本分类数据集上的实验结果表明，持续学习方法被证明能有效防止过度拟合，从而提高时间的普遍性，而时间不变方法则难以捕捉这些时间变化的动态。</li>
</ul>

<h3>Title: Title:
          From Role-Play to Drama-Interaction: An LLM Solution</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Wu, Hongqiu Wu, Lai Jiang, Xingyuan Liu, Jiale Hong, Hai Zhao, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          From Role-Play to Drama-Interaction: An LLM Solution(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Drama is a form of storytelling inspired by human creativity, proceeding with a predefined storyline, carrying emotions and thoughts. This paper introduces \emph{LLM-based interactive drama}, which endows traditional drama with an unprecedented immersion, where a person is allowed to walk into it and interact with the characters and scenes. We define this new artistic genre by 6 essential elements-plot, character, thought, diction, spectacle and interaction-and study the entire pipeline to forge a backbone \emph{drama LLM} to drive the playing process, which is challenged by limited drama resources, uncontrollable narrative development, and complicated instruction following. We propose \emph{Narrative Chain} to offer finer control over the narrative progression during interaction with players; \emph{Auto-Drama} to synthesize drama scripts given arbitrary stories; \emph{Sparse Instruction Tuning} to allow the model to follow sophisticated instructions. We manually craft 3 scripts, \emph{Detective Conan}, \emph{Harry Potter}, \emph{Romeo and Juliet}, and design a 5-dimension principle to evaluate the drama LLM comprehensively.</li>
<li><strong>摘要：</strong>戏剧是一种受人类创造力启发的讲故事形式，按照预定的故事情节进行，承载情感和思想。本文介绍了\emph{基于LLM的互动戏剧}，它赋予传统戏剧前所未有的沉浸感，让人走进其中，与人物和场景进行互动。我们通过6个基本要素来定义这种新的艺术流派——情节、人物、思想、措辞、景观和互动——并研究整个流程，以打造骨干\emph{戏剧法学硕士}来驱动游戏过程，这受到有限戏剧的挑战资源、无法控制的叙事发展以及复杂的指令遵循。我们提出 \emph{Narrative Chain} 来在与玩家互动期间提供对叙事进程的更好控制； \emph{Auto-Drama} 合成给定任意故事的戏剧脚本； \emph{稀疏指令调整}允许模型遵循复杂的指令。我们手工制作了\emph{名侦探柯南}、\emph{哈利·波特}、\emph{罗密欧与朱丽叶}三个剧本，并设计了一个5维原则来综合评价戏剧LLM。</li>
</ul>

<h3>Title: Title:
          Language processing in humans and computers</h3>
<ul>
<li><strong>Authors: </strong>Dusko Pavlovic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Language processing in humans and computers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Machine-learned language models have transformed everyday life: they steer us when we study, drive, manage money. They have the potential to transform our civilization. But they hallucinate. Their realities are virtual. This note provides a high-level overview of language models and outlines a low-level model of learning machines. It turns out that, after they become capable of recognizing hallucinations and dreaming safely, as humans tend to be, the language-learning machines proceed to generate broader systems of false beliefs and self-confirming theories, as humans tend to do.</li>
<li><strong>摘要：</strong>机器学习的语言模型已经改变了日常生活：它们在我们学习、驾驶、管理金钱时引导我们。他们有潜力改变我们的文明。但他们产生了幻觉。他们的现实是虚拟的。本说明提供了语言模型的高级概述，并概述了学习机的低级模型。事实证明，在它们能够像人类一样安全地识别幻觉和做梦之后，语言学习机器就会像人类一样，继续生成更广泛的错误信念和自我证实理论系统。</li>
</ul>

<h3>Title: Title:
          Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text Recognition</h3>
<ul>
<li><strong>Authors: </strong>Chan-Jan Hsu, Yi-Chang Chen, Feng-Ting Liao, Pei-Chen Ho, Yu-Hsiang Wang, Po-Chun Hsu, Da-shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text Recognition(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce ``Generative Fusion Decoding'' (GFD), a novel shallow fusion framework, utilized to integrate Large Language Models (LLMs) into multi-modal text recognition systems such as automatic speech recognition (ASR) and optical character recognition (OCR). We derive the formulas necessary to enable GFD to operate across mismatched token spaces of different models by mapping text token space to byte token space, enabling seamless fusion during the decoding process. The framework is plug-and-play, compatible with various auto-regressive models, and does not require re-training for feature alignment, thus overcoming limitations of previous fusion techniques. We highlight three main advantages of GFD: First, by simplifying the complexity of aligning different model sample spaces, GFD allows LLMs to correct errors in tandem with the recognition model, reducing computation latencies. Second, the in-context learning ability of LLMs is fully capitalized by GFD, increasing robustness in long-form speech recognition and instruction aware speech recognition. Third, GFD enables fusing recognition models deficient in Chinese text recognition with LLMs extensively trained on Chinese. Our evaluation demonstrates that GFD significantly improves performance in ASR and OCR tasks, with ASR reaching state-of-the-art in the NTUML2021 benchmark. GFD provides a significant step forward in model integration, offering a unified solution that could be widely applicable to leveraging existing pre-trained models through step by step fusion.</li>
<li><strong>摘要：</strong>我们引入“生成融合解码”（GFD），这是一种新颖的浅层融合框架，用于将大型语言模型（LLM）集成到多模式文本识别系统中，例如自动语音识别（ASR）和光学字符识别（OCR） 。我们通过将文本标记空间映射到字节标记空间，推导出使 GFD 能够跨不同模型的不匹配标记空间进行操作所需的公式，从而在解码过程中实现无缝融合。该框架即插即用，兼容各种自回归模型，并且不需要重新训练特征对齐，从而克服了先前融合技术的局限性。我们强调了 GFD 的三个主要优点：首先，通过简化对齐不同模型样本空间的复杂性，GFD 允许 LLM 与识别模型一起纠正错误，从而减少计算延迟。其次，法学硕士的情境学习能力被 GFD 充分利用，提高了长格式语音识别和指令感知语音识别的鲁棒性。第三，GFD 能够将缺乏中文文本识别能力的识别模型与经过大量中文训练的法学硕士融合起来。我们的评估表明，GFD 显着提高了 ASR 和 OCR 任务的性能，ASR 在 NTUML2021 基准测试中达到了最先进的水平。 GFD 在模型集成方面向前迈出了重要一步，提供了一个统一的解决方案，可广泛应用于通过逐步融合来利用现有的预训练模型。</li>
</ul>

<h3>Title: Title:
          Improving Language Models Trained with Translated Data via Continual Pre-Training and Dictionary Learning Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sabri Boughorbel, MD Rizwan Parvez, Majd Hawasly</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Improving Language Models Trained with Translated Data via Continual Pre-Training and Dictionary Learning Analysis(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Training LLMs in low resources languages usually utilizes data augmentation with machine translation (MT) from English language. However, translation brings a number of challenges: there are large costs attached to translating and curating huge amounts of content with high-end machine translation solutions, the translated content carries over cultural biases, and if the translation is not faithful and accurate, the quality of the data degrades causing issues in the trained model. In this work we investigate the role of translation and synthetic data in training language models. We translate TinyStories, a dataset of 2.2M short stories for 3-4 year old children, from English to Arabic using the free NLLB-3B MT model. We train a number of story generation models of sizes 1M-33M parameters using this data. We identify a number of quality and task-specific issues in the resulting models. To rectify these issues, we further pre-train the models with a small dataset of synthesized high-quality stories, representing 1\% of the original training data, using a capable LLM in Arabic. We show using GPT-4 as a judge and dictionary learning analysis from mechanistic interpretability that the suggested approach is a practical means to resolve some of the translation pitfalls. We illustrate the improvement through case studies of linguistic issues and cultural bias.</li>
<li><strong>摘要：</strong>以低资源语言培训法学硕士通常利用英语机器翻译 (MT) 的数据增强。然而，翻译带来了许多挑战：使用高端机器翻译解决方案翻译和整理大量内容需要高昂的成本，翻译的内容带有文化偏见，如果翻译不忠实和准确，质量就会下降。数据降级导致训练模型出现问题。在这项工作中，我们研究了翻译和合成数据在训练语言模型中的作用。我们使用免费的 NLLB-3B MT 模型将 TinyStories（包含 220 万个 3-4 岁儿童短篇故事的数据集）从英语翻译成阿拉伯语。我们使用这些数据训练了许多大小为 1M-33M 参数的故事生成模型。我们在生成的模型中发现了许多质量和特定于任务的问题。为了纠正这些问题，我们使用有能力的阿拉伯语法学硕士，使用合成的高质量故事的小数据集（占原始训练数据的 1%）进一步对模型进行预训练。我们使用 GPT-4 作为判断，并从机械可解释性进行字典学习分析，表明所提出的方法是解决一些翻译陷阱的实用方法。我们通过语言问题和文化偏见的案例研究来说明这种改进。</li>
</ul>

<h3>Title: Title:
          JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</h3>
<ul>
<li><strong>Authors: </strong>Kun Zhou, Beichen Zhang, Jiapeng Wang, Zhipeng Chen, Wayne Xin Zhao, Jing Sha, Zhichao Sheng, Shijin Wang, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \url{this https URL}.</li>
<li><strong>摘要：</strong>数学推理是大型语言模型（LLM）在实际应用中的一项重要功能。为了增强这种能力，现有的工作要么收集大规模数学相关文本进行预训练，要么依赖更强大的 LLM（例如 GPT-4）来综合大量数学问题。两种类型的工作通常都会导致大量的培训或综合成本。为了降低成本，基于开源可用文本，我们提出了一种有效的方法来训练小型法学硕士进行数学问题综合，以高效生成足够的高质量预训练数据。为了实现这一目标，我们使用 GPT-4 创建一个数据集，将其数据合成能力提炼到小型 LLM 中。具体来说，我们根据人类教育阶段制定了一套提示来指导 GPT-4，综合涵盖不同数学知识和难度级别的问题。此外，我们采用基于梯度的影响力估计方法来选择最有价值的数学相关文本。两者都被输入到 GPT-4 中，用于创建知识蒸馏数据集来训练小型 LLM。我们利用它综合了 600 万个数学问题来预训练我们的 JiuZhang3.0 模型，该模型只需要调用 GPT-4 API 9.3k 次并在 4.6B 数据上进行预训练。实验结果表明，JiuZhang3.0 在自然语言推理和工具操作设置下，在多个数学推理数据集上均实现了最先进的性能。我们的代码和数据将在\url{这个https URL}中公开发布。</li>
</ul>

<h3>Title: Title:
          MiniCache: KV Cache Compression in Depth Dimension for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MiniCache: KV Cache Compression in Depth Dimension for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context</a></li>
<li><strong>Abstract: </strong>A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance.</li>
<li><strong>摘要：</strong>高效部署计算要求高的大型语言模型 (LLM) 的关键方法是键值 (KV) 缓存。 KV 缓存存储先前生成的令牌的键值状态，显着减少重复计算的需要，从而降低自回归生成中的延迟。然而，KV 缓存的大小随着序列长度线性增长，这给需要长上下文输入和大量序列生成的应用程序带来了挑战。在本文中，我们提出了一种简单而有效的方法，称为 MiniCache，从新颖的深度角度跨层压缩 KV 缓存，显着减少 LLM 推理的内存占用。我们的方法基于这样的观察：KV 缓存状态在 LLM 中深层部分的相邻层之间表现出高度相似性。为了便于合并，我们建议将状态分解为幅度和方向分量，对状态向量的方向进行插值，同时保持其长度不变。此外，我们引入了令牌保留策略来保持高度不同的状态对不合并，从而以最小的额外存储开销保存信息。我们的 MiniCache 是免训练且通用的，补充了现有的 KV 缓存压缩策略，例如量化和稀疏性。我们利用 LLaMA-2、LLaMA-3、Phi-3、Mistral 和 Mixtral 等各种模型在多个基准测试中对 MiniCache 进行了全面评估，展示了其在实现卓越压缩比和高吞吐量方面的卓越性能。在 ShareGPT 数据集上，采用 4 位 MiniCache 的 LLaMA-2-7B 实现了高达 5.02 倍的显着压缩比，与 FP16 全缓存基准相比，推理吞吐量提高了约 5 倍，内存占用减少了 41%，所有同时保持近乎无损的性能。</li>
</ul>

<h3>Title: Title:
          Can Large Language Models Create New Knowledge for Spatial Reasoning Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Thomas Greatrix, Roger Whitaker, Liam Turner, Walter Colombo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can Large Language Models Create New Knowledge for Spatial Reasoning Tasks?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The potential for Large Language Models (LLMs) to generate new information offers a potential step change for research and innovation. This is challenging to assert as it can be difficult to determine what an LLM has previously seen during training, making "newness" difficult to substantiate. In this paper we observe that LLMs are able to perform sophisticated reasoning on problems with a spatial dimension, that they are unlikely to have previously directly encountered. While not perfect, this points to a significant level of understanding that state-of-the-art LLMs can now achieve, supporting the proposition that LLMs are able to yield significant emergent properties. In particular, Claude 3 is found to perform well in this regard.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 生成新信息的潜力为研究和创新提供了潜在的阶跃变化。很难断言这一点，因为很难确定法学硕士之前在培训期间看到过什么，使得“新颖性”难以证实。在本文中，我们观察到法学硕士能够对空间维度的问题进行复杂的推理，这是他们以前不太可能直接遇到的。虽然并不完美，但这表明最先进的法学硕士现在可以达到很高的理解水平，支持法学硕士能够产生重要的新兴特性的主张。特别是，Claude 3 在这方面表现良好。</li>
</ul>

<h3>Title: Title:
          Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zhihua Wen, Zhiliang Tian, Zexin Jian, Zhen Huang, Pei Ke, Yifu Gao, Minlie Huang, Dongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are widely used for knowledge-seeking yet suffer from hallucinations. The knowledge boundary (KB) of an LLM limits its factual understanding, beyond which it may begin to hallucinate. Investigating the perception of LLMs' KB is crucial for detecting hallucinations and LLMs' reliable generation. Current studies perceive LLMs' KB on questions with a concrete answer (close-ended questions) while paying limited attention to semi-open-ended questions (SoeQ) that correspond to many potential answers. Some researchers achieve it by judging whether the question is answerable or not. However, this paradigm is unsuitable for SoeQ, which are usually partially answerable, containing both answerable and ambiguous (unanswerable) answers. Ambiguous answers are essential for knowledge-seeking, but they may go beyond the KB of LLMs. In this paper, we perceive the LLMs' KB with SoeQ by discovering more ambiguous answers. First, we apply an LLM-based approach to construct SoeQ and obtain answers from a target LLM. Unfortunately, the output probabilities of mainstream black-box LLMs are inaccessible to sample for low-probability ambiguous answers. Therefore, we apply an open-sourced auxiliary model to explore ambiguous answers for the target LLM. We calculate the nearest semantic representation for existing answers to estimate their probabilities, with which we reduce the generation probability of high-probability answers to achieve a more effective generation. Finally, we compare the results from the RAG-based evaluation and LLM self-evaluation to categorize four types of ambiguous answers that are beyond the KB of the target LLM. Following our method, we construct a dataset to perceive the KB for GPT-4. We find that GPT-4 performs poorly on SoeQ and is often unaware of its KB. Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering more ambiguous answers.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）被广泛用于知识寻求，但却遭受了幻觉的困扰。法学硕士的知识边界（KB）限制了其对事实的理解，超出这个范围，它可能会开始产生幻觉。研究法学硕士知识库的感知对于检测幻觉和法学硕士的可靠生成至关重要。目前的研究将法学硕士的知识库视为具有具体答案的问题（封闭式问题），而对与许多潜在答案相对应的半开放式问题（SoeQ）的关注有限。一些研究人员通过判断问题是否可以回答来实现这一点。然而，这种范式不适合 SoeQ，SoeQ 通常是部分可回答的，既包含可回答的答案，又包含模糊（不可回答）的答案。模棱两可的答案对于寻求知识至关重要，但它们可能超出了法学硕士的知识库范围。在本文中，我们通过发现更多模糊答案来感知法学硕士与 SoeQ 的知识库。首先，我们应用基于 LLM 的方法来构建 SoeQ 并从目标 LLM 获得答案。不幸的是，主流黑盒法学硕士的输出概率无法对低概率的模糊答案进行采样。因此，我们应用开源辅助模型来探索目标法学硕士的模糊答案。我们计算现有答案的最接近的语义表示来估计其概率，从而降低高概率答案的生成概率，以实现更有效的生成。最后，我们比较了基于RAG的评估和LLM自我评估的结果，将超出目标LLM知识库范围的模糊答案分类为四类。按照我们的方法，我们构建了一个数据集来感知 GPT-4 的 KB。我们发现 GPT-4 在 SoeQ 上表现不佳，并且通常不知道其 KB。此外，我们的辅助模型 LLaMA-2-13B 可以有效地发现更多模糊答案。</li>
</ul>

<h3>Title: Title:
          Emotion Identification for French in Written Texts: Considering their Modes of Expression as a Step Towards Text Complexity Analysis</h3>
<ul>
<li><strong>Authors: </strong>Aline Étienne, Delphine Battistelli, Gwénolé Lecorvé</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Emotion Identification for French in Written Texts: Considering their Modes of Expression as a Step Towards Text Complexity Analysis(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The objective of this paper is to predict (A) whether a sentence in a written text expresses an emotion, (B) the mode(s) in which it is expressed, (C) whether it is basic or complex, and (D) its emotional category. One of our major contributions, through a dataset and a model, is to integrate the fact that an emotion can be expressed in different modes: from a direct mode, essentially lexicalized, to a more indirect mode, where emotions will only be suggested, a mode that NLP approaches generally don't take into account. Another originality is that the scope is on written texts, as opposed usual work focusing on conversational (often multi-modal) data. In this context, modes of expression are seen as a factor towards the automatic analysis of complexity in texts. Experiments on French texts show acceptable results compared to the human annotators' agreement, and outperforming results compared to using a large language model with in-context learning (i.e. no fine-tuning).</li>
<li><strong>摘要：</strong>本文的目标是预测 (A) 书面文本中的句子是否表达情感，(B) 表达情感的模式，(C) 是基本的还是复杂的，以及 (D)它的情感类别。我们的主要贡献之一是通过数据集和模型来整合情感可以以不同模式表达的事实：从本质上词汇化的直接模式到仅暗示情感的更间接模式。 NLP 方法通常不考虑这种模式。另一个独创性是，范围是书面文本，而不是通常的工作重点关注会话（通常是多模式）数据。在这种情况下，表达模式被视为自动分析文本复杂性的一个因素。对法语文本的实验显示，与人类注释者的协议相比，结果可以接受，并且与使用具有上下文学习功能的大型语言模型（即无需微调）相比，结果优于结果。</li>
</ul>

<h3>Title: Title:
          Instruction Tuning With Loss Over Instructions</h3>
<ul>
<li><strong>Authors: </strong>Zhengyan Shi, Adam X. Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, Aldo Lipani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Instruction Tuning With Loss Over Instructions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Instruction tuning plays a crucial role in shaping the outputs of language models (LMs) to desired styles. In this work, we propose a simple yet effective method, Instruction Modelling (IM), which trains LMs by applying a loss function to the instruction and prompt part rather than solely to the output part. Through experiments across 21 diverse benchmarks, we show that, in many scenarios, IM can effectively improve the LM performance on both NLP tasks (e.g., MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (e.g., MT-Bench and AlpacaEval). Remarkably, in the most advantageous case, IM boosts model performance on AlpacaEval 1.0 by over 100%. We identify two key factors influencing the effectiveness of IM: (1) The ratio between instruction length and output length in the training data; and (2) The number of training examples. We observe that IM is especially beneficial when trained on datasets with lengthy instructions paired with brief outputs, or under the Superficial Alignment Hypothesis (SAH) where a small amount of training examples are used for instruction tuning. Further analysis substantiates our hypothesis that the improvement can be attributed to reduced overfitting to instruction tuning datasets. Our work provides practical guidance for instruction tuning LMs, especially in low-resource scenarios.</li>
<li><strong>摘要：</strong>指令调优在将语言模型 (LM) 的输出塑造为所需风格方面发挥着至关重要的作用。在这项工作中，我们提出了一种简单而有效的方法，即指令建模（IM），它通过将损失函数应用于指令和提示部分而不是仅仅应用于输出部分来训练 LM。通过 21 个不同基准的实验，我们表明，在许多场景中，IM 可以有效提高 NLP 任务（例如 MMLU、TruthfulQA 和 HumanEval）和开放式生成基准（例如 MT-Bench 和 AlpacaEval）上的 LM 性能）。值得注意的是，在最有利的情况下，IM 将 AlpacaEval 1.0 上的模型性能提高了 100% 以上。我们确定了影响 IM 有效性的两个关键因素：（1）训练数据中指令长度与输出长度之间的比率； (2) 训练样本的数量。我们观察到，当在具有冗长指令和简短输出的数据集上进行训练时，或者在使用少量训练示例进行指令调整的表面对齐假设 (SAH) 下进行训练时，IM 特别有用。进一步的分析证实了我们的假设，即改进可归因于减少了对指令调整数据集的过度拟合。我们的工作为指令调整 LM 提供了实用指导，尤其是在资源匮乏的情况下。</li>
</ul>

<h3>Title: Title:
          Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jaewoo Yang, Hayun Kim, Younghoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) have established state-of-the-art performance through architectural improvements, but still require significant computational cost for inference. In an effort to reduce the inference cost, post-training quantization (PTQ) has become a popular approach, quantizing weights and activations to lower precision, such as INT8. In this paper, we reveal the challenges of activation quantization in GLU variants, which are widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family. The problem is that severe local quantization errors, caused by excessive magnitudes of activation in GLU variants, significantly degrade the performance of the quantized LLM. We denote these activations as activation spikes. Our further observations provide a systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of specific layers, particularly in the early and late layers, 2) The activation spikes are dedicated to a couple of tokens, rather than being shared across a sequence. Based on our observations, we propose two empirical methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during quantization. Our extensive experiments validate the effectiveness of the proposed methods for the activation quantization, especially with coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, and Gemma. In particular, our methods enhance the current alleviation techniques (e.g., SmoothQuant) that fail to control the activation spikes. Code is available at this https URL.</li>
<li><strong>摘要：</strong>现代大型语言模型 (LLM) 通过架构改进建立了最先进的性能，但仍然需要大量的计算成本来进行推理。为了降低推理成本，训练后量化 (PTQ) 已成为一种流行的方法，它将权重和激活量化到较低的精度，例如 INT8。在本文中，我们揭示了 GLU 变体中激活量化的挑战，这些变体广泛应用于现代 LLM 的前馈网络（FFN），例如 LLaMA 家族。问题在于，由 GLU 变体中的激活幅度过大引起的严重局部量化误差会显着降低量化 LLM 的性能。我们将这些激活表示为激活尖峰。我们的进一步观察提供了激活峰值的系统模式：1）激活峰值发生在特定层的 FFN 中，特别是在早期和晚期层，2）激活峰值专用于几个令牌，而不是跨多个令牌共享一个序列。根据我们的观察，我们提出了两种经验方法，即无量化模块（QFeM）和无量化前缀（QFeP），以隔离量化期间的激活尖峰。我们的大量实验验证了所提出的激活量化方法的有效性，特别是使用粗粒度方案，具有 GLU 变体的最新 LLM，包括 LLaMA-2/3、Mistral、Mixtral、SOLAR 和 Gemma。特别是，我们的方法增强了当前无法控制激活峰值的缓解技术（例如 SmoothQuant）。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Title:
          RaFe: Ranking Feedback Improves Query Rewriting for RAG</h3>
<ul>
<li><strong>Authors: </strong>Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RaFe: Ranking Feedback Improves Query Rewriting for RAG(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG) techniques have evolved, query rewriting has been widely incorporated into the RAG system for downstream tasks like open-domain QA. Many works have attempted to utilize small models with reinforcement learning rather than costly LLMs to improve query rewriting. However, current methods require annotations (e.g., labeled relevant documents or downstream answers) or predesigned rewards for feedback, which lack generalization, and fail to utilize signals tailored for query rewriting. In this paper, we propose ours, a framework for training query rewriting models free of annotations. By leveraging a publicly available reranker, ours~provides feedback aligned well with the rewriting objectives. Experimental results demonstrate that ours~can obtain better performance than baselines.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 和检索增强生成 (RAG) 技术的发展，查询重写已被广泛纳入 RAG 系统中，用于开放域 QA 等下游任务。许多作品尝试利用带有强化学习的小型模型而不是昂贵的法学硕士来改进查询重写。然而，当前的方法需要注释（例如，标记相关文档或下游答案）或预先设计的反馈奖励，这缺乏泛化性，并且无法利用为查询重写量身定制的信号。在本文中，我们提出了一个框架，用于训练无注释的查询重写模型。通过利用公开可用的重新排序器，我们提供了与重写目标非常一致的反馈。实验结果表明我们的~可以获得比基线更好的性能。</li>
</ul>

<h3>Title: Title:
          Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Alejo Lopez-Avila, Víctor Suárez-Paniagua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Recently, using large pretrained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters or combinations with unsupervised approaches, among many others. This work proposes a 3 Phase technique to adjust a base model for a classification task. First, we adapt the model's signal to the data distribution by performing further training with a Denoising Autoencoder (DAE). Second, we adjust the representation space of the output to the corresponding classes by clustering through a Contrastive Learning (CL) method. In addition, we introduce a new data augmentation approach for Supervised Contrastive Learning to correct the unbalanced datasets. Third, we apply fine-tuning to delimit the predefined categories. These different phases provide relevant and complementary knowledge to the model to learn the final task. We supply extensive experimental results on several datasets to demonstrate these claims. Moreover, we include an ablation study and compare the proposed method against other ways of combining these techniques.</li>
<li><strong>摘要：</strong>最近，使用大型预训练 Transformer 模型进行迁移学习任务已经发展到成为自然语言处理 (NLP) 社区的旗舰趋势之一，引发了各种前景，例如基于提示、适配器或与无监督的方法等等。这项工作提出了一种三阶段技术来调整分类任务的基本模型。首先，我们通过使用去噪自动编码器 (DAE) 进行进一步训练，使模型的信号适应数据分布。其次，我们通过对比学习（CL）方法进行聚类，将输出的表示空间调整为相应的类。此外，我们为监督对比学习引入了一种新的数据增强方法，以纠正不平衡的数据集。第三，我们应用微调来界定预定义的类别。这些不同的阶段为模型提供相关和补充的知识来学习最终任务。我们在多个数据集上提供了广泛的实验结果来证明这些主张。此外，我们还进行了一项消融研究，并将所提出的方法与结合这些技术的其他方法进行了比较。</li>
</ul>

<h3>Title: Title:
          Exploring the use of a Large Language Model for data extraction in systematic reviews: a rapid feasibility study</h3>
<ul>
<li><strong>Authors: </strong>Lena Schmidt, Kaitlyn Hair, Sergio Graziozi, Fiona Campbell, Claudia Kapp, Alireza Khanteymoori, Dawn Craig, Mark Engelbert, James Thomas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Exploring the use of a Large Language Model for data extraction in systematic reviews: a rapid feasibility study(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper describes a rapid feasibility study of using GPT-4, a large language model (LLM), to (semi)automate data extraction in systematic reviews. Despite the recent surge of interest in LLMs there is still a lack of understanding of how to design LLM-based automation tools and how to robustly evaluate their performance. During the 2023 Evidence Synthesis Hackathon we conducted two feasibility studies. Firstly, to automatically extract study characteristics from human clinical, animal, and social science domain studies. We used two studies from each category for prompt-development; and ten for evaluation. Secondly, we used the LLM to predict Participants, Interventions, Controls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP dataset. Overall, results indicated an accuracy of around 80%, with some variability between domains (82% for human clinical, 80% for animal, and 72% for studies of human social sciences). Causal inference methods and study design were the data extraction items with the most errors. In the PICO study, participants and intervention/control showed high accuracy (>80%), outcomes were more challenging. Evaluation was done manually; scoring methods such as BLEU and ROUGE showed limited value. We observed variability in the LLMs predictions and changes in response quality. This paper presents a template for future evaluations of LLMs in the context of data extraction for systematic review automation. Our results show that there might be value in using LLMs, for example as second or third reviewers. However, caution is advised when integrating models such as GPT-4 into tools. Further research on stability and reliability in practical settings is warranted for each type of data that is processed by the LLM.</li>
<li><strong>摘要：</strong>本文描述了使用大型语言模型 (LLM) GPT-4 在系统评价中（半）自动化数据提取的快速可行性研究。尽管最近人们对法学硕士的兴趣激增，但对于如何设计基于法学硕士的自动化工具以及如何稳健地评估其性能仍然缺乏了解。在 2023 年证据合成黑客马拉松期间，我们进行了两项可行性研究。首先，从人类临床、动物和社会科学领域研究中自动提取研究特征。我们使用每个类别的两项研究来进行快速开发；和十个用于评估。其次，我们使用 LLM 来预测 EBM-NLP 数据集中 100 个摘要内标记的参与者、干预、控制和结果 (PICO)。总体而言，结果表明准确度约为 80%，不同领域之间存在一定差异（人类临床为 82%，动物为 80%，人类社会科学研究为 72%）。因果推理方法和研究设计是错误最多的数据提取项目。在 PICO 研究中，参与者和干预/对照显示出较高的准确性（>80%），结果更具挑战性。评估是手动完成的； BLEU 和 ROUGE 等评分方法的价值有限。我们观察到法学硕士预测的可变性和响应质量的变化。本文提出了一个模板，用于在系统审查自动化的数据提取背景下对法学硕士进行未来评估。我们的结果表明，使用法学硕士可能有价值，例如作为第二或第三审稿人。但是，在将 GPT-4 等模型集成到工具中时，建议谨慎。对于法学硕士处理的每种类型的数据，有必要对实际环境中的稳定性和可靠性进行进一步研究。</li>
</ul>

<h3>Title: Title:
          RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at this https URL</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出令人印象深刻的能力，但也存在令人担忧的产生幻觉的倾向。本文介绍了 RefChecker，这是一个引入声明三元组来表示 LLM 响应中的声明的框架，旨在检测细粒度的幻觉。在 RefChecker 中，提取器根据响应生成声明三元组，然后检查器根据引用对其进行评估。我们描述了三种任务设置：零、嘈杂和准确的上下文，以反映各种现实世界的用例。我们策划了一个涵盖各种 NLP 任务的基准，并从 7 位法学硕士的 2100 个回复中注释了 11000 个声明三元组。 RefChecker 支持专有和开源模型作为提取器和检查器。实验表明，与其他粒度（例如响应、句子和子句子级别的声明）相比，声明三元组能够实现出色的幻觉检测。在我们的基准测试中，RefChecker 的性能比之前的方法高出 6.8 到 26.1 分，并且 RefChecker 的检查结果与人类的判断非常一致。这项工作在此 https URL 上开源</li>
</ul>

<h3>Title: Title:
          MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability</h3>
<ul>
<li><strong>Authors: </strong>Yanrui Du, Sendong Zhao, Danyang Zhao, Ming Ma, Yuhan Chen, Liangyu Huo, Qing Yang, Dongliang Xu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed in various applications. As their usage grows, concerns regarding their safety are rising, especially in maintaining harmless responses when faced with malicious instructions. Many defense strategies have been developed to enhance the safety of LLMs. However, our research finds that existing defense strategies lead LLMs to predominantly adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. To solve this problem, we introduce the MoGU framework, designed to enhance LLMs' safety while preserving their usability. Our MoGU framework transforms the base LLM into two variants: the usable LLM and the safe LLM, and further employs dynamic routing to balance their contribution. When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless. Conversely, for benign instructions, the router prioritizes the usable LLM, facilitating usable and helpful responses. On various open-sourced LLMs, we compare multiple defense strategies to verify the superiority of our MoGU framework. Besides, our analysis provides key insights into the effectiveness of MoGU and verifies that our designed routing mechanism can effectively balance the contribution of each variant by assigning weights. Our work released the safer Llama2, Vicuna, Falcon, Dolphin, and Baichuan2.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地部署在各种应用程序中。随着它们的使用量不断增加，人们对其安全性的担忧也在不断增加，特别是在面对恶意指令时保持无害的响应。人们已经制定了许多防御策略来增强法学硕士的安全性。然而，我们的研究发现，现有的防御策略导致法学硕士主要采取拒绝导向的立场，从而降低了他们对良性指示做出反应的可用性。为了解决这个问题，我们引入了 MoGU 框架，旨在增强法学硕士的安全性，同时保留其可用性。我们的MoGU框架将基础LLM转换为两个变体：可用LLM和安全LLM，并进一步采用动态路由来平衡它们的贡献。当遇到恶意指令时，路由器会为安全LLM分配更高的权重，以确保响应是无害的。相反，对于良性指令，路由器会优先考虑可用的 LLM，从而促进可用且有帮助的响应。在各种开源法学硕士上，我们比较了多种防御策略，以验证我们的 MoGU 框架的优越性。此外，我们的分析提供了关于 MoGU 有效性的关键见解，并验证了我们设计的路由机制可以通过分配权重来有效平衡每个变体的贡献。我们的工作发布了更安全的 Llama2、Vicuna、Falcon、Dolphin 和 Baichuan2。</li>
</ul>

<h3>Title: Title:
          Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Johan S Daniel, Anand Pal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>The advancement of large language models has significantly improved natural language processing. However, challenges such as jailbreaks (prompt injections that cause an LLM to follow instructions contrary to its intended use), hallucinations (generating incorrect or misleading information), and comprehension errors remain prevalent. In this report, we present a comparative analysis of the performance of fifteen distinct models, with each model undergoing a standardized test comprising 38 queries across three key metrics: jailbreaks, hallucinations, and comprehension errors. The models are assessed based on the total occurrences of jailbreaks, hallucinations, and comprehension errors. Our work exposes these models' inherent vulnerabilities and challenges the notion of human-level language comprehension of these models. We have empirically analysed the impact of non-standard Unicode characters on LLMs and their safeguarding mechanisms on the best-performing LLMs, including GPT-4, Gemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus. By incorporating alphanumeric symbols from Unicode outside the standard Latin block and variants of characters in other languages, we observed a reduction in the efficacy of guardrails implemented through Reinforcement Learning Human Feedback (RLHF). Consequently, these models exhibit heightened vulnerability to content policy breaches and prompt leakage. Our study also suggests a need to incorporate non-standard Unicode text in LLM training data to enhance the capabilities of these models.</li>
<li><strong>摘要：</strong>大语言模型的进步显着改善了自然语言处理。然而，诸如越狱（导致法学硕士遵循与其预期用途相反的指令的提示注射）、幻觉（生成不正确或误导性信息）和理解错误等挑战仍然普遍存在。在本报告中，我们对 15 个不同模型的性能进行了比较分析，每个模型都经过标准化测试，包括针对三个关键指标的 38 个查询：越狱、幻觉和理解错误。这些模型是根据越狱、幻觉和理解错误的总发生次数进行评估的。我们的工作暴露了这些模型固有的漏洞，并对这些模型的人类语言理解的概念提出了挑战。我们实证分析了非标准 Unicode 字符对 LLM 的影响及其对性能最佳 LLM 的保护机制，包括 GPT-4、Gemini 1.5 Pro、LlaMA-3-70B 和 Claude 3 Opus。通过将 Unicode 中的字母数字符号合并到标准拉丁语块之外以及其他语言中的字符变体，我们观察到通过强化学习人类反馈 (RLHF) 实现的护栏效率有所下降。因此，这些模型在内容策略违规和迅速泄露方面表现出更高的脆弱性。我们的研究还表明需要将非标准 Unicode 文本纳入 LLM 训练数据中，以增强这些模型的能力。</li>
</ul>

<h3>Title: Title:
          Exploring Alignment in Shared Cross-lingual Spaces</h3>
<ul>
<li><strong>Authors: </strong>Basel Mousi, Nadir Durrani, Fahim Dalvi, Majd Hawasly, Ahmed Abdelali</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Exploring Alignment in Shared Cross-lingual Spaces(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite their remarkable ability to capture linguistic nuances across diverse languages, questions persist regarding the degree of alignment between languages in multilingual embeddings. Drawing inspiration from research on high-dimensional representations in neural language models, we employ clustering to uncover latent concepts within multilingual models. Our analysis focuses on quantifying the \textit{alignment} and \textit{overlap} of these concepts across various languages within the latent space. To this end, we introduce two metrics \CA{} and \CO{} aimed at quantifying these aspects, enabling a deeper exploration of multilingual embeddings. Our study encompasses three multilingual models (\texttt{mT5}, \texttt{mBERT}, and \texttt{XLM-R}) and three downstream tasks (Machine Translation, Named Entity Recognition, and Sentiment Analysis). Key findings from our analysis include: i) deeper layers in the network demonstrate increased cross-lingual \textit{alignment} due to the presence of language-agnostic concepts, ii) fine-tuning of the models enhances \textit{alignment} within the latent space, and iii) such task-specific calibration helps in explaining the emergence of zero-shot capabilities in the models.\footnote{The code is available at \url{this https URL}}</li>
<li><strong>摘要：</strong>尽管它们具有捕捉不同语言之间的语言细微差别的非凡能力，但多语言嵌入中语言之间的对齐程度仍然存在问题。受到神经语言模型高维表示研究的启发，我们采用聚类来揭示多语言模型中的潜在概念。我们的分析重点是量化潜在空间内各种语言中这些概念的 \textit{alignment} 和 \textit{overlap}。为此，我们引入了两个指标 \CA{} 和 \CO{} 旨在量化这些方面，从而能够更深入地探索多语言嵌入。我们的研究涵盖三个多语言模型（\texttt{mT5}、\texttt{mBERT} 和 \texttt{XLM-R}）和三个下游任务（机器翻译、命名实体识别和情感分析）。我们分析的主要发现包括：i) 网络中的更深层次表明，由于与语言无关的概念的存在，跨语言 \textit{alignment} 有所增加，ii) 模型的微调增强了 \textit{alignment} 内的潜在空间，以及 iii) 这种特定于任务的校准有助于解释模型中零样本能力的出现。\footnote{代码可在 \url{this https URL}}</li>
</ul>

<h3>Title: Title:
          Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Kumar, Sarfaroz Yunusov, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: \textit{representative bias}, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and \textit{affinity bias}, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.</li>
<li><strong>摘要：</strong>对大型语言模型（LLM）的研究经常忽视微妙的偏见，这些偏见虽然不太明显，但可以显着影响模型对特定社会叙事的输出。本研究解决了法学硕士中的两个此类偏差：\textit{代表性偏差}，表示法学硕士生成反映某些身份群体经历的输出的倾向；和 \textit{亲和性偏差}，反映模型对特定群体的评估偏好叙述或观点。我们引入了两个新颖的指标来衡量这些偏见：代表性偏见得分（RBS）和亲和力偏见得分（ABS），并提出了面向创造力的生成套件（CoGS），这是一系列开放式任务，例如短篇小说写作和诗歌创作，采用定制的标准设计来检测这些微妙的偏见。我们的分析揭示了著名法学硕士的明显代表性偏见，他们偏爱与白人、异性恋和男性相关的身份。此外，我们对亲和偏差的调查揭示了每个模型中独特的评估模式，类似于“偏差指纹”。这种趋势也出现在人类评估者身上，凸显了人类和机器偏见感知之间复杂的相互作用。</li>
</ul>

<h3>Title: Title:
          Representation noising effectively prevents harmful fine-tuning on LLMs</h3>
<ul>
<li><strong>Authors: </strong>Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Representation noising effectively prevents harmful fine-tuning on LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (RepNoise), a defence mechanism that is effective even when attackers have access to the weights and the defender no longer has any control. RepNoise works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the effectiveness of our defence lies in its "depth": the degree to which information about harmful representations is removed across all layers of the LLM.</li>
<li><strong>摘要：</strong>发布开源大型语言模型 (LLM) 存在双重用途风险，因为不良行为者可以轻松地对这些模型进行微调以达到有害目的。即使没有公开发布权重，权重窃取和微调 API 也会使封闭模型容易受到有害微调攻击 (HFA)。虽然防止越狱和改进安全护栏等安全措施很重要，但这些措施很容易通过微调被逆转。在这项工作中，我们提出了表示噪声 (RepNoise)，这是一种即使攻击者可以访问权重并且防御者不再有任何控制权时仍然有效的防御机制。RepNoise 的工作原理是删除有关有害表示的信息，使其在微调过程中难以恢复。重要的是，我们的防御还能够概括防御过程中未见过的不同危害子集。我们的方法不会降低 LLM 的一般能力，并保留在无害任务上训练模型的能力。我们提供经验证据表明，我们辩护的有效性在于其“深度”：在 LLM 的所有层面上删除有关有害陈述的信息的程度。</li>
</ul>

<h3>Title: Title:
          Base of RoPE Bounds Context Length</h3>
<ul>
<li><strong>Authors: </strong>Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, Weipeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Base of RoPE Bounds Context Length(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Position embedding is a core component of current Large Language Models (LLMs). Rotary position embedding (RoPE), a technique that encodes the position information with a rotation matrix, has been the de facto choice for position embedding in many LLMs, such as the Llama series. RoPE has been further utilized to extend long context capability, which is roughly based on adjusting the \textit{base} parameter of RoPE to mitigate out-of-distribution (OOD) problems in position embedding. However, in this paper, we find that LLMs may obtain a superficial long-context ability based on the OOD theory. We revisit the role of RoPE in LLMs and propose a novel property of long-term decay, we derive that the \textit{base of RoPE bounds context length}: there is an absolute lower bound for the base value to obtain certain context length capability. Our work reveals the relationship between context length and RoPE base both theoretically and empirically, which may shed light on future long context training.</li>
<li><strong>摘要：</strong>位置嵌入是当前大型语言模型（LLM）的核心组件。旋转位置嵌入（RoPE）是一种用旋转矩阵对位置信息进行编码的技术，已经成为许多 LLM 中位置嵌入的事实上的选择，例如 Llama 系列。 RoPE 被进一步用来扩展长上下文能力，这大致是基于调整 RoPE 的 \textit{base} 参数来减轻位置嵌入中的分布外（OOD）问题。然而，在本文中，我们发现法学硕士可以基于OOD理论获得肤浅的长语境能力。我们重新审视 RoPE 在 LLM 中的作用，并提出了一种新的长期衰减特性，我们推导出 \textit{RoPE 的基数限制上下文长度}：基值有一个绝对下界以获得特定的上下文长度能力。我们的工作从理论上和实证上揭示了上下文长度和 RoPE 基础之间的关系，这可能为未来的长上下文训练带来启发。</li>
</ul>

<h3>Title: Title:
          A FAIR and Free Prompt-based Research Assistant</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Shamsabadi, Jennifer D'Souza</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A FAIR and Free Prompt-based Research Assistant(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>This demo will present the Research Assistant (RA) tool developed to assist with six main types of research tasks defined as standardized instruction templates, instantiated with user input, applied finally as prompts to well-known--for their sophisticated natural language processing abilities--AI tools, such as ChatGPT (this https URL) and Gemini (this https URL). The six research tasks addressed by RA are: creating FAIR research comparisons, ideating research topics, drafting grant applications, writing scientific blogs, aiding preliminary peer reviews, and formulating enhanced literature search queries. RA's reliance on generative AI tools like ChatGPT or Gemini means the same research task assistance can be offered in any scientific discipline. We demonstrate its versatility by sharing RA outputs in Computer Science, Virology, and Climate Science, where the output with the RA tool assistance mirrored that from a domain expert who performed the same research task.</li>
<li><strong>摘要：</strong>该演示将展示为协助六种主要类型的研究任务而开发的研究助理 (RA) 工具，该工具被定义为标准化指令模板，通过用户输入进行实例化，最终作为提示应用于众所周知的（因其复杂的自然语言处理能力） -AI工具，例如ChatGPT（此https URL）和Gemini（此https URL）。 RA 解决的六项研究任务是：创建公平的研究比较、构思研究主题、起草资助申请、撰写科学博客、协助初步同行评审以及制定增强的文献搜索查询。 RA 对 ChatGPT 或 Gemini 等生成式人工智能工具的依赖意味着可以在任何科学学科中提供相同的研究任务帮助。我们通过共享计算机科学、病毒学和气候科学领域的 RA 输出来展示其多功能性，其中 RA 工具辅助的输出反映了执行相同研究任务的领域专家的输出。</li>
</ul>

<h3>Title: Title:
          A Watermark for Low-entropy and Unbiased Generation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minjia Mao, Dongjun Wei, Zeyu Chen, Xiao Fang, Michael Chau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Watermark for Low-entropy and Unbiased Generation in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have highlighted the risk of misuse, raising concerns about accurately detecting LLM-generated content. A viable solution for the detection problem is to inject imperceptible identifiers into LLMs, known as watermarks. Previous work demonstrates that unbiased watermarks ensure unforgeability and preserve text quality by maintaining the expectation of the LLM output probability distribution. However, previous unbiased watermarking methods are impractical for local deployment because they rely on accesses to white-box LLMs and input prompts during detection. Moreover, these methods fail to provide statistical guarantees for the type II error of watermark detection. This study proposes the Sampling One Then Accepting (STA-1) method, an unbiased watermark that does not require access to LLMs nor prompts during detection and has statistical guarantees for the type II error. Moreover, we propose a novel tradeoff between watermark strength and text quality in unbiased watermarks. We show that in low-entropy scenarios, unbiased watermarks face a tradeoff between watermark strength and the risk of unsatisfactory outputs. Experimental results on low-entropy and high-entropy datasets demonstrate that STA-1 achieves text quality and watermark strength comparable to existing unbiased watermarks, with a low risk of unsatisfactory outputs. Implementation codes for this study are available online.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展凸显了滥用的风险，引发了人们对准确检测 LLM 生成的内容的担忧。检测问题的一个可行解决方案是将难以察觉的标识符注入 LLM，称为水印。之前的工作表明，无偏水印通过维持 LLM 输出概率分布的期望来确保不可伪造性并保持文本质量。然而，以前的无偏水印方法对于本地部署来说是不切实际的，因为它们依赖于对白盒LLM的访问和检测期间的输入提示。而且，这些方法无法为水印检测的II类错误提供统计保证。本研究提出了先采样后接受（STA-1）方法，这是一种无偏水印，不需要访问LLM，也不需要在检测过程中进行提示，并且对II类错误有统计保证。此外，我们提出了无偏水印中水印强度和文本质量之间的新颖权衡。我们表明，在低熵场景中，无偏水印面临水印强度和输出不令人满意的风险之间的权衡。低熵和高熵数据集上的实验结果表明，STA-1 实现了与现有无偏水印相当的文本质量和水印强度，并且输出不令人满意的风险较低。本研究的实施代码可在线获取。</li>
</ul>

<h3>Title: Title:
          Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiming Chen, Chen Zhang, Danqing Luo, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The automatic evaluation of natural language generation (NLG) systems presents a long-lasting challenge. Recent studies have highlighted various neural metrics that align well with human evaluations. Yet, the robustness of these evaluators against adversarial perturbations remains largely under-explored due to the unique challenges in obtaining adversarial data for different NLG evaluation tasks. To address the problem, we introduce AdvEval, a novel black-box adversarial framework against NLG evaluators. AdvEval is specially tailored to generate data that yield strong disagreements between human and victim evaluators. Specifically, inspired by the recent success of large language models (LLMs) in text generation and evaluation, we adopt strong LLMs as both the data generator and gold evaluator. Adversarial data are automatically optimized with feedback from the gold and victim evaluator. We conduct experiments on 12 victim evaluators and 11 NLG datasets, spanning tasks including dialogue, summarization, and question evaluation. The results show that AdvEval can lead to significant performance degradation of various victim metrics, thereby validating its efficacy.</li>
<li><strong>摘要：</strong>自然语言生成（NLG）系统的自动评估提出了一个长期的挑战。最近的研究强调了与人类评估非常一致的各种神经指标。然而，由于获取不同 NLG 评估任务的对抗性数据所面临的独特挑战，这些评估者对抗对抗性扰动的稳健性在很大程度上仍未得到充分探索。为了解决这个问题，我们引入了 AdvEval，这是一种针对 NLG 评估者的新型黑盒对抗框架。 AdvEval 专门用于生成在人类评估者和受害者评估者之间产生强烈分歧的数据。具体来说，受到大型语言模型 (LLM) 最近在文本生成和评估方面取得的成功的启发，我们采用强大的 LLM 作为数据生成器和黄金评估器。根据黄金和受害者评估者的反馈自动优化对抗性数据。我们在 12 个受害者评估者和 11 个 NLG 数据集上进行了实验，涵盖对话、总结和问题评估等任务。结果表明，AdvEval 可以导致各种受害者指标的性能显着下降，从而验证了其功效。</li>
</ul>

<h3>Title: Title:
          Efficient Medical Question Answering with Knowledge-Augmented Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Julien Khlaut, Corentin Dancette, Elodie Ferreres, Alaedine Bennani, Paul Hérent, Pierre Manceron</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Efficient Medical Question Answering with Knowledge-Augmented Question Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>In the expanding field of language model applications, medical knowledge representation remains a significant challenge due to the specialized nature of the domain. Large language models, such as GPT-4, obtain reasonable scores on medical question answering tasks, but smaller models are far behind. In this work, we introduce a method to improve the proficiency of a small language model in the medical domain by employing a two-fold approach. We first fine-tune the model on a corpus of medical textbooks. Then, we use GPT-4 to generate questions similar to the downstream task, prompted with textbook knowledge, and use them to fine-tune the model. Additionally, we introduce ECN-QA, a novel medical question answering dataset containing ``progressive questions'' composed of related sequential questions. We show the benefits of our training strategy on this dataset. The study's findings highlight the potential of small language models in the medical domain when appropriately fine-tuned. The code and weights are available at this https URL.</li>
<li><strong>摘要：</strong>在不断扩大的语言模型应用领域，由于该领域的专业性，医学知识表示仍然是一个重大挑战。大型语言模型（例如 GPT-4）在医学问答任务上获得了合理的分数，但较小的模型则远远落后。在这项工作中，我们介绍了一种通过采用两重方法来提高医学领域小语言模型熟练程度的方法。我们首先在医学教科书语料库上微调模型。然后，我们使用 GPT-4 生成与下游任务类似的问题，并用课本知识进行提示，并使用它们来微调模型。此外，我们还介绍了 ECN-QA，这是一种新颖的医学问答数据集，其中包含由相关连续问题组成的“渐进式问题”。我们在此数据集上展示了我们的训练策略的好处。该研究的结果凸显了经过适当微调后，小语言模型在医学领域的潜力。代码和权重可从此 https URL 获取。</li>
</ul>

<h3>Title: Title:
          A Declarative System for Optimizing AI Workloads</h3>
<ul>
<li><strong>Authors: </strong>Chunwei Liu, Matthew Russo, Michael Cafarella, Lei Cao, Peter Baille Chen, Zui Chen, Michael Franklin, Tim Kraska, Samuel Madden, Gerardo Vitagliano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Declarative System for Optimizing AI Workloads(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>Modern AI models provide the key to a long-standing dream: processing analytical queries about almost any kind of data. Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or insights from image and video corpora. Today's models can accomplish these tasks with high accuracy. However, a programmer who wants to answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations. For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the ideal prompt design, and so on. The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape shifts. In this paper we present Palimpzest, a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language. The system uses its cost optimization framework -- which explores the search space of AI models, prompting techniques, and related foundation model optimizations -- to implement the query with the best trade-offs between runtime, financial cost, and output data quality. We describe the workload of AI-powered analytics tasks, the optimization methods that Palimpzest uses, and the prototype system itself. We evaluate Palimpzest on tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching. We show that even our simple prototype offers a range of appealing plans, including one that is 3.3x faster, 2.9x cheaper, and offers better data quality than the baseline method. With parallelism enabled, Palimpzest can produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline. These require no additional work by the user.</li>
<li><strong>摘要：</strong>现代人工智能模型提供了实现长期梦想的关键：处理几乎任何类型数据的分析查询。直到最近，从公司文档中提取事实、从科学论文中提取数据或从图像和视频语料库中提取见解都是困难且昂贵的。今天的模型可以高精度地完成这些任务。然而，想要回答实质性的人工智能查询的程序员必须编排大量的模型、提示和数据操作。即使对于单个查询，程序员也必须做出大量决策，例如模型的选择、正确的推理方法、最具成本效益的推理硬件、理想的提示设计等等。最佳决策集可能会随着查询的变化以及快速发展的技术环境的变化而变化。在本文中，我们介绍了 Palimpzest，这是一个系统，任何人都可以通过用声明性语言定义它们来处理人工智能驱动的分析查询。该系统使用其成本优化框架（探索人工智能模型的搜索空间、提示技术和相关基础模型优化）来实现查询，并在运行时间、财务成本和输出数据质量之间实现最佳权衡。我们描述了人工智能驱动的分析任务的工作负载、Palimpzest 使用的优化方法以及原型系统本身。我们在法律发现、房地产搜索和医学模式匹配方面的任务上评估 Palimpzest。我们表明，即使是我们简单的原型也提供了一系列有吸引力的计划，其中包括比基准方法快 3.3 倍、便宜 2.9 倍并且提供更好数据质量的计划。启用并行性后，相对于单线程 GPT-4 基准，Palimpzest 可以以降低 9.1 倍的成本生成高达 90.3 倍加速的计划，同时获得在基准的 83.5% 以内的 F1 分数。这些不需要用户进行额外的工作。</li>
</ul>

<h3>Title: Title:
          Evaluating Large Language Models for Public Health Classification and Extraction Tasks</h3>
<ul>
<li><strong>Authors: </strong>Joshua Harris, Timothy Laurence, Leo Loman, Fan Grayson, Toby Nonnenmacher, Harry Long, Loes WalsGriffith, Amy Douglas, Holly Fountain, Stelios Georgiou, Jo Hardstaff, Kathryn Hopkins, Y-Ling Chi, Galena Kuyumdzhieva, Lesley Larkin, Samuel Collins, Hamish Mohammed, Thomas Finnie, Luke Hounsome, Steven Riley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating Large Language Models for Public Health Classification and Extraction Tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Advances in Large Language Models (LLMs) have led to significant interest in their potential to support human experts across a range of domains, including public health. In this work we present automated evaluations of LLMs for public health tasks involving the classification and extraction of free text. We combine six externally annotated datasets with seven new internally annotated datasets to evaluate LLMs for processing text related to: health burden, epidemiological risk factors, and public health interventions. We initially evaluate five open-weight LLMs (7-70 billion parameters) across all tasks using zero-shot in-context learning. We find that Llama-3-70B-Instruct is the highest performing model, achieving the best results on 15/17 tasks (using micro-F1 scores). We see significant variation across tasks with all open-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as Contact Classification, while all LLMs achieve greater than 80% micro-F1 on others, such as GI Illness Classification. For a subset of 12 tasks, we also evaluate GPT-4 and find comparable results to Llama-3-70B-Instruct, which scores equally or outperforms GPT-4 on 6 of the 12 tasks. Overall, based on these initial results we find promising signs that LLMs may be useful tools for public health experts to extract information from a wide variety of free text sources, and support public health surveillance, research, and interventions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的进步引起了人们对其支持包括公共卫生在内的一系列领域的人类专家的潜力的极大兴趣。在这项工作中，我们提出了对涉及自由文本分类和提取的公共卫生任务的法学硕士的自动评估。我们将六个外部注释数据集与七个新的内部注释数据集相结合，以评估法学硕士处理与健康负担、流行病学风险因素和公共卫生干预措施相关的文本的能力。我们最初使用零样本上下文学习来评估所有任务中的 5 个开放权重 LLM（7-700 亿个参数）。我们发现 Llama-3-70B-Instruct 是性能最高的模型，在 15/17 任务上取得了最佳结果（使用 micro-F1 分数）。我们发现任务之间存在显着差异，所有开放权重法学硕士在一些具有挑战性的任务（例如接触分类）上得分低于 60% 的 micro-F1，而所有法学硕士在其他任务（例如胃肠疾病分类）上的得分都高于 80% 的 micro-F1。对于 12 项任务的子集，我们还评估了 GPT-4 并发现了与 Llama-3-70B-Instruct 相当的结果，后者在 12 项任务中的 6 项上得分与 GPT-4 相同或优于 GPT-4。总体而言，基于这些初步结果，我们发现了有希望的迹象，表明法学硕士可能是公共卫生专家从各种自由文本来源中提取信息并支持公共卫生监测、研究和干预的有用工具。</li>
</ul>

<h3>Title: Title:
          WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code will be released at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）需要知识更新来满足不断增长的世界事实并纠正幻觉反应，从而促进终身模型编辑的方法。更新的知识位于记忆中的何处是模型编辑的基本问题。在本文中，我们发现编辑长期记忆（直接模型参数）或工作记忆（通过检索获得的神经网络激活/表示的非参数知识）将导致不可能的三角形——可靠性、泛化性和局部性可以在终身编辑设置中无法一起实现。对于长期记忆，直接编辑参数会导致与不相关的预训练知识或先前编辑的冲突（可靠性和局部性较差）。对于工作记忆，基于检索的激活很难使模型理解编辑并进行泛化（泛化能力差）。因此，我们提出WISE来弥合记忆之间的差距。在WISE中，我们设计了一种双参数存储方案，该方案由用于预训练知识的主存储器和用于编辑知识的辅助存储器组成。我们只编辑辅助存储器中的知识，并训练路由器来决定在给定查询时要通过哪个存储器。为了持续编辑，我们设计了一种知识分片机制，其中不同的编辑集驻留在不同的参数子空间中，并随后合并到共享内存中而不会发生冲突。大量实验表明，WISE 可以超越以前的模型编辑方法，并克服跨趋势 LLM 架构（例如 GPT、LLaMA 和 Mistral）的问答、幻觉和分布外设置的终身模型编辑下的不可能三角。代码将在此 https URL 发布。</li>
</ul>

<h3>Title: Title:
          Lessons from the Trenches on Reproducible Evaluation of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, François Yvon, Andy Zou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Lessons from the Trenches on Reproducible Evaluation of Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Effective evaluation of language models remains an open challenge in NLP. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.</li>
<li><strong>摘要：</strong>语言模型的有效评估仍然是 NLP 领域的一个开放挑战。研究人员和工程师面临方法论问题，例如模型对评估设置的敏感性、方法之间正确比较的困难以及缺乏可重复性和透明度。在本文中，我们借鉴了三年评估大型语言模型的经验，为研究人员提供指导和教训。首先，我们概述了语言模型评估中面临的常见挑战。其次，我们描述了解决或减轻这些挑战对研究影响的最佳实践。第三，我们提出了语言模型评估工具（lm-eval）：一个开源库，用于对语言模型进行独立、可复制和可扩展的评估，旨在解决这些问题。我们描述了该库的功能以及使用该库来缓解这些方法论问题的案例研究。</li>
</ul>

<h3>Title: Title:
          Can LLMs Solve longer Math Word Problems Better?</h3>
<ul>
<li><strong>Authors: </strong>Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can LLMs Solve longer Math Word Problems Better?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Math Word Problems (MWPs) are crucial for evaluating the capability of Large Language Models (LLMs), with current research primarily focusing on questions with concise contexts. However, as real-world math problems often involve complex circumstances, LLMs' ability to solve long MWPs is vital for their applications in these scenarios, yet remains under-explored. This study pioneers the exploration of Context Length Generalizability (CoLeG), the ability of LLMs to solve long MWPs. We introduce Extended Grade-School Math (E-GSM), a collection of MWPs with lengthy narratives. Two novel metrics are proposed to assess the efficacy and resilience of LLMs in solving these problems. Our examination of existing zero-shot prompting techniques and both proprietary and open-source LLMs reveals a general deficiency in CoLeG. To alleviate these challenges, we propose distinct approaches for different categories of LLMs. For proprietary LLMs, a new instructional prompt is proposed to mitigate the influence of long context. For open-source LLMs, a new data augmentation task is developed to improve CoLeG. Our comprehensive results demonstrate the effectiveness of our proposed methods, showing not only improved performance on E-GSM but also generalizability across several other MWP benchmarks. Our findings pave the way for future research in employing LLMs for complex, real-world applications, offering practical solutions to current limitations and opening avenues for further exploration of model generalizability and training methodologies.</li>
<li><strong>摘要：</strong>数学应用题（MWP）对于评估大型语言模型（LLM）的能力至关重要，当前的研究主要集中在具有简洁上下文的问题上。然而，由于现实世界的数学问题通常涉及复杂的情况，法学硕士解决长 MWP 的能力对于他们在这些场景中的应用至关重要，但仍未得到充分探索。这项研究开创了对上下文长度泛化性（CoLeG）的探索，即法学硕士解决长 MWP 的能力。我们推出了扩展小学数学 (E-GSM)，这是一个带有冗长叙述的 MWP 集合。提出了两个新颖的指标来评估法学硕士解决这些问题的有效性和弹性。我们对现有零样本提示技术以及专有和开源法学硕士的检查揭示了 CoLeG 的普遍缺陷。为了缓解这些挑战，我们针对不同类别的法学硕士提出了不同的方法。对于专有的法学硕士，提出了一种新的教学提示，以减轻长上下文的影响。对于开源法学硕士，开发了一项新的数据增强任务来改进 CoLeG。我们的综合结果证明了我们提出的方法的有效性，不仅显示了 E-GSM 上性能的改进，而且还显示了其他几个 MWP 基准的通用性。我们的研究结果为未来研究利用法学硕士进行复杂的现实世界应用铺平了道路，为当前的局限性提供了实用的解决方案，并为进一步探索模型的通用性和训练方法开辟了途径。</li>
</ul>

<h3>Title: Title:
          Implicit Personalization in Language Models: A Systematic Study</h3>
<ul>
<li><strong>Authors: </strong>Zhijing Jin, Nils Heil, Jiarui Liu, Shehzaad Dhuliawala, Yahang Qi, Bernhard Schölkopf, Rada Mihalcea, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Implicit Personalization in Language Models: A Systematic Study(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Implicit Personalization (IP) is a phenomenon of language models inferring a user's background from the implicit cues in the input prompts and tailoring the response based on this inference. While previous work has touched upon various instances of this problem, there lacks a unified framework to study this behavior. This work systematically studies IP through a rigorous mathematical formulation, a multi-perspective moral reasoning framework, and a set of case studies. Our theoretical foundation for IP relies on a structural causal model and introduces a novel method, indirect intervention, to estimate the causal effect of a mediator variable that cannot be directly intervened upon. Beyond the technical approach, we also introduce a set of moral reasoning principles based on three schools of moral philosophy to study when IP may or may not be ethically appropriate. Equipped with both mathematical and ethical insights, we present three diverse case studies illustrating the varied nature of the IP problem and offer recommendations for future research. Our code and data are at this https URL.</li>
<li><strong>摘要：</strong>隐式个性化 (IP) 是一种语言模型从输入提示中的隐式提示推断用户背景并根据此推断定制响应的现象。虽然以前的工作已经涉及到这个问题的各种实例，但缺乏一个统一的框架来研究这种行为。这项工作通过严格的数学公式、多视角的道德推理框架和一组案例研究系统地研究了知识产权。我们的知识产权理论基础依赖于结构性因果模型，并引入了一种新颖的方法，即间接干预，来估计无法直接干预的中介变量的因果效应。除了技术方法之外，我们还引入了一套基于三个道德哲学流派的道德推理原则，以研究知识产权在道德上是否适当。凭借数学和伦理方面的见解，我们提出了三个不同的案例研究，说明了知识产权问题的不同性质，并为未来的研究提供了建议。我们的代码和数据位于此 https URL 中。</li>
</ul>

<h3>Title: Title:
          HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at this https URL.</li>
<li><strong>摘要：</strong>为了在充满敌意和不断变化的自然环境中茁壮成长，哺乳动物的大脑不断进化，能够存储大量关于世界的知识，并不断整合新信息，同时避免灾难性的遗忘。尽管取得了令人印象深刻的成就，大型语言模型（LLM），即使具有检索增强生成（RAG），在预训练后仍然难以高效且有效地整合大量新经验。在这项工作中，我们介绍了 HippoRAG，这是一种新颖的检索框架，其灵感来自人类长期记忆的海马索引理论，能够对新体验进行更深入、更有效的知识整合。 HippoRAG 协同协调法学硕士、知识图谱和个性化 PageRank 算法，以模仿新皮质和海马体在人类记忆中的不同作用。我们将 HippoRAG 与现有的 RAG 方法在多跳问答上进行比较，结果表明我们的方法显着优于最先进的方法，最高可达 20%。使用 HippoRAG 的单步检索可实现与 IRCoT 等迭代检索相当或更好的性能，同时价格便宜 10-30 倍，速度提高 6-13 倍，并且将 HippoRAG 集成到 IRCoT 中可带来进一步的实质性收益。最后，我们证明我们的方法可以解决现有方法无法解决的新型场景。代码和数据可从此 https URL 获取。</li>
</ul>

<h3>Title: Title:
          From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</h3>
<ul>
<li><strong>Authors: </strong>Yuntian Deng, Yejin Choi, Stuart Shieber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.</li>
<li><strong>摘要：</strong>当利用语言模型进行推理任务时，生成明确的思维链 (CoT) 步骤通常对于实现最终输出的高精度至关重要。在本文中，我们研究是否可以训练模型来内化这些 CoT 步骤。为此，我们提出了一种简单而有效的内化 CoT 步骤的方法：从经过显式 CoT 推理训练的模型开始，我们逐渐删除中间步骤并对模型进行微调。这个过程使得模型能够内化中间推理步骤，从而在保持高性能的同时简化推理过程。我们的方法使 GPT-2 Small 模型能够以高达 99% 的准确度解决 9×9 乘法，而标准训练无法解决超过 4×4 乘法的问题。此外，我们的方法在较大的语言模型（例如 Mistral 7B）上被证明是有效的，在 GSM8K 上实现了超过 50% 的准确率，而无需产生任何中间步骤。</li>
</ul>

<h3>Title: Title:
          Bitune: Bidirectional Instruction-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Bitune: Bidirectional Instruction-Tuning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>We introduce Bitune, a method that improves instruction-tuning of pretrained decoder-only large language models, leading to consistent gains on downstream tasks. Bitune applies both causal and bidirectional attention to the prompt, to obtain a better representation of the query or instruction. We realize this by introducing two sets of parameters, for which we apply parameter-efficient finetuning techniques. These causal and bidirectional features are then combined into a weighted average with trainable coefficients, which is subsequently used to generate new tokens. We demonstrate significant improvements in zero-shot performance on commonsense reasoning, arithmetic, and language understanding tasks, while extensive ablation studies validate the role of each component and demonstrate the method's agnosticism to different PEFT techniques.</li>
<li><strong>摘要：</strong>我们引入了 Bitune，这是一种改进预训练解码器大型语言模型指令调整的方法，从而在下游任务上获得一致的收益。 Bitune 对提示应用因果和双向注意，以获得查询或指令的更好表示。我们通过引入两组参数来实现这一点，为此我们应用参数高效的微调技术。然后将这些因果和双向特征组合成具有可训练系数的加权平均值，随后用于生成新的令牌。我们展示了常识推理、算术和语言理解任务上零样本性能的显着改进，同时广泛的消融研究验证了每个组件的作用，并证明了该方法对不同 PEFT 技术的不可知论。</li>
</ul>

<h3>Title: Title:
          A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns</h3>
<ul>
<li><strong>Authors: </strong>Asaf Yehudai, Taelin Karidi, Gabriel Stanovsky, Ariel Goldstein, Omri Abend</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Cross-domain alignment refers to the task of mapping a concept from one domain to another. For example, ``If a \textit{doctor} were a \textit{color}, what color would it be?''. This seemingly peculiar task is designed to investigate how people represent concrete and abstract concepts through their mappings between categories and their reasoning processes over those mappings. In this paper, we adapt this task from cognitive science to evaluate the conceptualization and reasoning abilities of large language models (LLMs) through a behavioral study. We examine several LLMs by prompting them with a cross-domain mapping task and analyzing their responses at both the population and individual levels. Additionally, we assess the models' ability to reason about their predictions by analyzing and categorizing their explanations for these mappings. The results reveal several similarities between humans' and models' mappings and explanations, suggesting that models represent concepts similarly to humans. This similarity is evident not only in the model representation but also in their behavior. Furthermore, the models mostly provide valid explanations and deploy reasoning paths that are similar to those of humans.</li>
<li><strong>摘要：</strong>跨域对齐是指将概念从一个域映射到另一个域的任务。例如，“如果 \textit{doctor} 是 \textit{color}，它会是什么颜色？”。这项看似特殊的任务旨在研究人们如何通过类别之间的映射以及对这些映射的推理过程来表示具体和抽象的概念。在本文中，我们将认知科学中的这项任务改编为通过行为研究来评估大语言模型（LLM）的概念化和推理能力。我们通过提示法学硕士进行跨领域映射任务并分析他们在群体和个人层面的反应来对他们进行检查。此外，我们通过分析和分类模型对这些映射的解释来评估模型推理其预测的能力。结果揭示了人类和模型的映射和解释之间的一些相似之处，表明模型代表的概念与人类相似。这种相似性不仅在模型表示中很明显，而且在它们的行为中也很明显。此外，这些模型大多提供有效的解释并部署与人类相似的推理路径。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
