<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-06-04</h1>
<h3>Title: Large Language Models' Detection of Political Orientation in Newspapers</h3>
<ul>
<li><strong>Authors: </strong>Alessio Buscemi, Daniele Proverbio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00018">https://arxiv.org/abs/2406.00018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00018">https://arxiv.org/pdf/2406.00018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00018]] Large Language Models' Detection of Political Orientation in Newspapers(https://arxiv.org/abs/2406.00018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Democratic opinion-forming may be manipulated if newspapers' alignment to political or economical orientation is ambiguous. Various methods have been developed to better understand newspapers' positioning. Recently, the advent of Large Language Models (LLM), and particularly the pre-trained LLM chatbots like ChatGPT or Gemini, hold disruptive potential to assist researchers and citizens alike. However, little is know on whether LLM assessment is trustworthy: do single LLM agrees with experts' assessment, and do different LLMs answer consistently with one another? In this paper, we address specifically the second challenge. We compare how four widely employed LLMs rate the positioning of newspapers, and compare if their answers align with one another. We observe that this is not the case. Over a woldwide dataset, articles in newspapers are positioned strikingly differently by single LLMs, hinting to inconsistent training or excessive randomness in the algorithms. We thus raise a warning when deciding which tools to use, and we call for better training and algorithm development, to cover such significant gap in a highly sensitive matter for democracy and societies worldwide. We also call for community engagement in benchmark evaluation, through our open initiative this http URL.</li>
<li><strong>摘要：</strong>如果报纸的政治或经济取向不明确，民主舆论形成可能会受到操纵。人们已经开发出各种方法来更好地理解报纸的定位。最近，大型语言模型 (LLM) 的出现，尤其是像 ChatGPT 或 Gemini 这样的预先训练过的 LLM 聊天机器人，具有颠覆性的潜力，可以帮助研究人员和公民。然而，人们对 LLM 评估是否值得信赖知之甚少：单个 LLM 是否同意专家的评估，不同的 LLM 是否给出一致的答案？在本文中，我们专门解决第二个挑战。我们比较了四个广泛使用的 LLM 如何评价报纸的定位，并比较它们的答案是否彼此一致。我们观察到事实并非如此。在全球范围内的数据集中，单个 LLM 对报纸文章的定位截然不同，暗示算法中的训练不一致或过度随机。因此，我们在决定使用哪些工具时发出警告，并呼吁进行更好的培训和算法开发，以弥补全球民主和社会高度敏感问题上的如此重大差距。我们还通过我们的开放倡议这个 http URL 呼吁社区参与基准评估。</li>
</ul>

<h3>Title: Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias</h3>
<ul>
<li><strong>Authors: </strong>Rebecca Dorn, Lee Kezar, Fred Morstatter, Kristina Lerman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00020">https://arxiv.org/abs/2406.00020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00020">https://arxiv.org/pdf/2406.00020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00020]] Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias(https://arxiv.org/abs/2406.00020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Content moderation on social media platforms shapes the dynamics of online discourse, influencing whose voices are amplified and whose are suppressed. Recent studies have raised concerns about the fairness of content moderation practices, particularly for aggressively flagging posts from transgender and non-binary individuals as toxic. In this study, we investigate the presence of bias in harmful speech classification of gender-queer dialect online, focusing specifically on the treatment of reclaimed slurs. We introduce a novel dataset, QueerReclaimLex, based on 109 curated templates exemplifying non-derogatory uses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators for potential harm depending on additional context about speaker identity. We systematically evaluate the performance of five off-the-shelf language models in assessing the harm of these texts and explore the effectiveness of chain-of-thought prompting to teach large language models (LLMs) to leverage author identity context. We reveal a tendency for these models to inaccurately flag texts authored by gender-queer individuals as harmful. Strikingly, across all LLMs the performance is poorest for texts that show signs of being written by individuals targeted by the featured slur (F1 <= 0.24). We highlight an urgent need for fairness and inclusivity in content moderation systems. By uncovering these biases, this work aims to inform the development of more equitable content moderation practices and contribute to the creation of inclusive online spaces for all users.</li>
<li><strong>摘要：</strong>社交媒体平台上的内容审核塑造了在线话语的动态，影响着谁的声音被放大，谁的声音被压制。最近的研究引发了人们对内容审核实践公平性的担忧，尤其是对于积极标记跨性别者和非二元性别者的帖子为有害内容。在这项研究中，我们调查了在线性别酷儿方言有害言论分类中是否存在偏见，特别关注对回收诽谤的处理。我们引入了一个新数据集 QueerReclaimLex，它基于 109 个精选模板，这些模板举例说明了 LGBTQ+ 诽谤的非贬义用法。性别酷儿注释者根据有关说话者身份的其他背景对数据集实例的潜在危害进行评分。我们系统地评估了五种现成的语言模型在评估这些文本危害方面的表现，并探索了思路链提示在教导大型语言模型 (LLM) 利用作者身份背景的有效性。我们发现，这些模型倾向于错误地将性别酷儿个人撰写的文本标记为有害。令人惊讶的是，在所有 LLM 中，对于有迹象表明由被针对的个人撰写的文本，其性能最差（F1 <= 0.24）。我们强调内容审核系统迫切需要公平和包容。通过揭示这些偏见，这项工作旨在为更公平的内容审核实践的发展提供信息，并为所有用户创建包容性的在线空间做出贡献。</li>
</ul>

<h3>Title: LocMoE+: Enhanced Router with Token Feature Awareness for Efficient LLM Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Jing Li, Zhijie Sun, Dachao Lin, Xuan He, Yi Lin, Binfan Zheng, Li Zeng, Rongqian Zhao, Xin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00023">https://arxiv.org/abs/2406.00023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00023">https://arxiv.org/pdf/2406.00023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00023]] LocMoE+: Enhanced Router with Token Feature Awareness for Efficient LLM Pre-Training(https://arxiv.org/abs/2406.00023)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) architectures have recently gained increasing popularity within the domain of large language models (LLMs) due to their ability to significantly reduce training and inference overhead. However, MoE architectures face challenges, such as significant disparities in the number of tokens assigned to each expert and a tendency toward homogenization among experts, which adversely affects the model's semantic generation capabilities. In this paper, we introduce LocMoE+, a refined version of the low-overhead LocMoE, incorporating the following enhancements: (1) Quantification and definition of the affinity between experts and tokens. (2) Implementation of a global-level adaptive routing strategy to rearrange tokens based on their affinity scores. (3) Reestimation of the lower bound for expert capacity, which has been shown to progressively decrease as the token feature distribution evolves. Experimental results demonstrate that, without compromising model convergence or efficacy, the number of tokens each expert processes can be reduced by over 60%. Combined with communication optimizations, this leads to an average improvement in training efficiency ranging from 5.4% to 46.6%. After fine-tuning, LocMoE+ exhibits a performance improvement of 9.7% to 14.1% across the GDAD, C-Eval, and TeleQnA datasets.</li>
<li><strong>摘要：</strong>混合专家 (MoE) 架构最近在大型语言模型 (LLM) 领域越来越受欢迎，因为它们能够显著降低训练和推理开销。然而，MoE 架构面临着挑战，例如分配给每个专家的 token 数量存在显著差异，以及专家之间趋于同质化，这对模型的语义生成能力产生了不利影响。在本文中，我们介绍了 LocMoE+，这是低开销 LocMoE 的改进版本，包含以下增强功能：(1) 量化和定义专家与 token 之间的亲和力。(2) 实施全局级自适应路由策略，根据 token 的亲和力分数重新排列 token。(3) 重新估计专家容量的下限，事实证明，随着 token 特征分布的发展，下限会逐渐减小。实验结果表明，在不影响模型收敛性或有效性的情况下，每个专家处理的 token 数量可以减少 60% 以上。结合通信优化，训练效率平均提升 5.4% 至 46.6%。经过微调后，LocMoE+ 在 GDAD、C-Eval 和 TeleQnA 数据集上的性能提升了 9.7% 至 14.1%。</li>
</ul>

<h3>Title: Embedding-Aligned Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Lior Shani, Ethan Liang, Craig Boutilier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Embedding-Aligned Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our embedding-aligned guided language (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M dataset to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations.</li>
<li><strong>摘要：</strong>我们提出了一种训练大型语言模型 (LLM) 的新方法，以遵循潜在嵌入空间中定义的目标。我们的方法利用强化学习 (RL)，将预先训练的 LLM 视为环境。我们的嵌入对齐引导语言 (EAGLE) 代理经过训练，可以根据某些预定义标准迭代地引导 LLM 的生成朝向潜在嵌入空间的最佳区域。我们使用 MovieLens 25M 数据集展示了 EAGLE 代理的有效性，可以发现满足潜在用户需求的内容差距。我们还展示了使用状态相关动作集的最佳设计来提高 EAGLE 效率的好处。我们的工作为使用 LLM 进行受控且扎实的文本生成铺平了道路，确保与特定领域的知识和数据表示保持一致。</li>
</ul>

<h3>Title: SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Li, Chi Xu, Feng Wang, Isaac M von Riedemann, Cong Zhang, Jiangchuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00025">https://arxiv.org/abs/2406.00025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00025">https://arxiv.org/pdf/2406.00025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00025]] SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models(https://arxiv.org/abs/2406.00025)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become increasingly popular, transforming a wide range of applications across various domains. However, the real-world effectiveness of their query cache systems has not been thoroughly investigated. In this work, we for the first time conducted an analysis on real-world human-to-LLM interaction data, identifying key challenges in existing caching solutions for LLM-based chat services. Our findings reveal that current caching methods fail to leverage semantic connections, leading to inefficient cache performance and extra token costs. To address these issues, we propose SCALM, a new cache architecture that emphasizes semantic analysis and identifies significant cache entries and patterns. We also detail the implementations of the corresponding cache storage and eviction strategies. Our evaluations show that SCALM increases cache hit ratios and reduces operational costs for LLMChat services. Compared with other state-of-the-art solutions in GPTCache, SCALM shows, on average, a relative increase of 63% in cache hit ratio and a relative improvement of 77% in tokens savings.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已变得越来越流行，并改变了各个领域的各种应用。然而，它们的查询缓存系统的实际效果尚未得到彻底研究。在这项工作中，我们首次对现实世界的人与 LLM 交互数据进行了分析，确定了基于 LLM 的聊天服务现有缓存解决方案中的关键挑战。我们的研究结果表明，当前的缓存方法未能利用语义连接，导致缓存性能低下和额外的令牌成本。为了解决这些问题，我们提出了 SCALM，这是一种新的缓存架构，它强调语义分析并识别重要的缓存条目和模式。我们还详细介绍了相应的缓存存储和驱逐策略的实现。我们的评估表明，SCALM 提高了缓存命中率并降低了 LLMChat 服务的运营成本。与 GPTCache 中其他最先进的解决方案相比，SCALM 平均缓存命中率相对提高了 63%，令牌节省相对提高了 77%。</li>
</ul>

<h3>Title: Adapting PromptORE for Modern History: Information Extraction from Hispanic Monarchy Documents of the XVIth Century</h3>
<ul>
<li><strong>Authors: </strong>Hèctor Loopez Hidalgo, Michel Boeglin, David Kahn, Josiane Mothe, Diego Ortiz, David Panzoli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00027">https://arxiv.org/abs/2406.00027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00027">https://arxiv.org/pdf/2406.00027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00027]] Adapting PromptORE for Modern History: Information Extraction from Hispanic Monarchy Documents of the XVIth Century(https://arxiv.org/abs/2406.00027)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Semantic relations among entities are a widely accepted method for relation extraction. PromptORE (Prompt-based Open Relation Extraction) was designed to improve relation extraction with Large Language Models on generalistic documents. However, it is less effective when applied to historical documents, in languages other than English. In this study, we introduce an adaptation of PromptORE to extract relations from specialized documents, namely digital transcripts of trials from the Spanish Inquisition. Our approach involves fine-tuning transformer models with their pretraining objective on the data they will perform inference. We refer to this process as "biasing". Our Biased PromptORE addresses complex entity placements and genderism that occur in Spanish texts. We solve these issues by prompt engineering. We evaluate our method using Encoder-like models, corroborating our findings with experts' assessments. Additionally, we evaluate the performance using a binomial classification benchmark. Our results show a substantial improvement in accuracy -up to a 50% improvement with our Biased PromptORE models in comparison to the baseline models using standard PromptORE.</li>
<li><strong>摘要：</strong>实体之间的语义关系是一种广泛接受的关系提取方法。PromptORE（基于提示的开放关系提取）旨在改进使用大型语言模型对通用文档进行关系提取。但是，当应用于除英语以外的语言的历史文档时，效果较差。在本研究中，我们引入了 PromptORE 的改编版，以从专门的文档（即西班牙宗教裁判所审判的数字记录）中提取关系。我们的方法涉及使用其预训练目标对将执行推理的数据进行微调。我们将此过程称为“偏差”。我们的 Biased PromptORE 解决了西班牙语文本中出现的复杂实体位置和性别歧视。我们通过快速工程解决了这些问题。我们使用类似编码器的模型评估我们的方法，并通过专家的评估证实我们的发现。此外，我们使用二项式分类基准来评估性能。我们的结果表明，与使用标准 PromptORE 的基线模型相比，我们的 Biased PromptORE 模型的准确率显着提高 - 提高了 50%。</li>
</ul>

<h3>Title: Clustered Retrieved Augmented Generation (CRAG)</h3>
<ul>
<li><strong>Authors: </strong>Simon Akesson, Frances A. Santos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00029">https://arxiv.org/abs/2406.00029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00029">https://arxiv.org/pdf/2406.00029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00029]] Clustered Retrieved Augmented Generation (CRAG)(https://arxiv.org/abs/2406.00029)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Providing external knowledge to Large Language Models (LLMs) is a key point for using these models in real-world applications for several reasons, such as incorporating up-to-date content in a real-time manner, providing access to domain-specific knowledge, and contributing to hallucination prevention. The vector database-based Retrieval Augmented Generation (RAG) approach has been widely adopted to this end. Thus, any part of external knowledge can be retrieved and provided to some LLM as the input context. Despite RAG approach's success, it still might be unfeasible for some applications, because the context retrieved can demand a longer context window than the size supported by LLM. Even when the context retrieved fits into the context window size, the number of tokens might be expressive and, consequently, impact costs and processing time, becoming impractical for most applications. To address these, we propose CRAG, a novel approach able to effectively reduce the number of prompting tokens without degrading the quality of the response generated compared to a solution using RAG. Through our experiments, we show that CRAG can reduce the number of tokens by at least 46\%, achieving more than 90\% in some cases, compared to RAG. Moreover, the number of tokens with CRAG does not increase considerably when the number of reviews analyzed is higher, unlike RAG, where the number of tokens is almost 9x higher when there are 75 reviews compared to 4 reviews.</li>
<li><strong>摘要：</strong>向大型语言模型 (LLM) 提供外部知识是将这些模型用于实际应用的关键点，原因有多种，例如实时整合最新内容、提供对特定领域知识的访问以及有助于预防幻觉。基于向量数据库的检索增强生成 (RAG) 方法已被广泛采用。因此，可以检索外部知识的任何部分并将其作为输入上下文提供给某些 LLM。尽管 RAG 方法取得了成功，但它对于某些应用程序来说仍然可能不可行，因为检索到的上下文可能需要比 LLM 支持的大小更长的上下文窗口。即使检索到的上下文适合上下文窗口大小，标记的数量也可能具有表现力，因此会影响成本和处理时间，这对于大多数应用程序来说变得不切实际。为了解决这些问题，我们提出了 CRAG，这是一种新颖的方法，与使用 RAG 的解决方案相比，它能够有效地减少提示标记的数量，而不会降低生成的响应的质量。通过我们的实验，我们发现与 RAG 相比，CRAG 可以将标记数量减少至少 46%，在某些情况下可达到 90% 以上。此外，当分析的评论数量较多时，CRAG 的标记数量不会显着增加，而 RAG 则不同，当有 75 条评论时，标记数量几乎是 4 条评论的 9 倍。</li>
</ul>

<h3>Title: Large Language Model Pruning</h3>
<ul>
<li><strong>Authors: </strong>Hanjuan Huang (1) (2), Hao-Jia Song (1), Hsing-Kuo Pao (1) ((1) Dept. of Computer Science and Information Engineering National Taiwan University of Science and Technology, Taipei, Taiwan, (2) College of Mechanical and Electrical Engineering, WUYI University, Wuyishan, China)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00030">https://arxiv.org/abs/2406.00030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00030">https://arxiv.org/pdf/2406.00030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00030]] Large Language Model Pruning(https://arxiv.org/abs/2406.00030)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>We surely enjoy the larger the better models for their superior performance in the last couple of years when both the hardware and software support the birth of such extremely huge models. The applied fields include text mining and others. In particular, the success of LLMs on text understanding and text generation draws attention from researchers who have worked on NLP and related areas for years or even decades. On the side, LLMs may suffer from problems like model overfitting, hallucination, and device limitation to name a few. In this work, we suggest a model pruning technique specifically focused on LLMs. The proposed methodology emphasizes the explainability of deep learning models. By having the theoretical foundation, we obtain a trustworthy deep model so that huge models with a massive number of model parameters become not quite necessary. A mutual information-based estimation is adopted to find neurons with redundancy to eliminate. Moreover, an estimator with well-tuned parameters helps to find precise estimation to guide the pruning procedure. At the same time, we also explore the difference between pruning on large-scale models vs. pruning on small-scale models. The choice of pruning criteria is sensitive in small models but not for large-scale models. It is a novel finding through this work. Overall, we demonstrate the superiority of the proposed model to the state-of-the-art models.</li>
<li><strong>摘要：</strong>过去几年，当硬件和软件都支持这种超大型模型诞生时，我们当然会喜欢越大越好的模型，因为它们具有出色的性能。应用领域包括文本挖掘等。特别是，LLM 在文本理解和文本生成方面的成功引起了在 NLP 和相关领域工作多年甚至几十年的研究人员的关注。另一方面，LLM 可能会遇到模型过度拟合、幻觉和设备限制等问题。在这项工作中，我们提出了一种专门针对 LLM 的模型修剪技术。所提出的方法强调了深度学习模型的可解释性。通过理论基础，我们获得了一个值得信赖的深度模型，因此具有大量模型参数的大型模型就变得不那么必要了。采用基于互信息的估计来查找具有冗余的神经元以消除冗余。此外，具有经过良好调整的参数的估计器有助于找到精确的估计来指导修剪过程。同时，我们还探讨了大规模模型修剪与小规模模型修剪之间的区别。修剪标准的选择在小型模型中很敏感，但对于大型模型则不然。这是通过这项工作得出的一项新发现。总的来说，我们证明了所提出的模型优于最先进的模型。</li>
</ul>

<h3>Title: AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Achuth Chandrasekhar, Jonathan Chan, Francis Ogoke, Olabode Ajenifujah, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00031">https://arxiv.org/abs/2406.00031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00031">https://arxiv.org/pdf/2406.00031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00031]] AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing(https://arxiv.org/abs/2406.00031)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Generalized large language models (LLMs) such as GPT-4 may not provide specific answers to queries formulated by materials science researchers. These models may produce a high-level outline but lack the capacity to return detailed instructions on manufacturing and material properties of novel alloys. Enhancing a smaller model with specialized domain knowledge may provide an advantage over large language models which cannot be retrained quickly enough to keep up with the rapid pace of research in metal additive manufacturing (AM). We introduce "AMGPT," a specialized LLM text generator designed for metal AM queries. The goal of AMGPT is to assist researchers and users in navigating the extensive corpus of literature in AM. Instead of training from scratch, we employ a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented Generation (RAG) setup, utilizing it to dynamically incorporate information from $\sim$50 AM papers and textbooks in PDF format. Mathpix is used to convert these PDF documents into TeX format, facilitating their integration into the RAG pipeline managed by LlamaIndex. Expert evaluations of this project highlight that specific embeddings from the RAG setup accelerate response times and maintain coherence in the generated text.</li>
<li><strong>摘要：</strong>诸如 GPT-4 之类的广义大型语言模型 (LLM) 可能无法为材料科学研究人员提出的查询提供具体答案。这些模型可能会产生高级大纲，但缺乏返回有关新型合金的制造和材料特性的详细说明的能力。使用专业领域知识增强较小的模型可能比大型语言模型更具优势，因为大型语言模型无法快速重新训练以跟上金属增材制造 (AM) 研究的快速发展。我们引入了“AMGPT”，这是一种专为金属 AM 查询设计的专用 LLM 文本生成器。AMGPT 的目标是帮助研究人员和用户浏览 AM 的大量文献资料。我们不是从头开始训练，而是在检索增强生成 (RAG) 设置中使用 Hugging Face 的预训练 Llama2-7B 模型，利用它动态整合来自 PDF 格式的 50 篇 AM 论文和教科书的信息。Mathpix 用于将这些 PDF 文档转换为 TeX 格式，从而便于将它们集成到由 LlamaIndex 管理的 RAG 管道中。该项目的专家评估强调，RAG 设置中的特定嵌入可加快响应时间并保持生成文本的连贯性。</li>
</ul>

<h3>Title: Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking</h3>
<ul>
<li><strong>Authors: </strong>Sara Kemper, Justin Cui, Kai Dicarlantonio, Kathy Lin, Danjie Tang, Anton Korikov, Scott Sanner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00033">https://arxiv.org/abs/2406.00033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00033">https://arxiv.org/pdf/2406.00033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00033]] Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking(https://arxiv.org/abs/2406.00033)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Conversational recommendation (ConvRec) systems must understand rich and diverse natural language (NL) expressions of user preferences and intents, often communicated in an indirect manner (e.g., "I'm watching my weight"). Such complex utterances make retrieving relevant items challenging, especially if only using often incomplete or out-of-date metadata. Fortunately, many domains feature rich item reviews that cover standard metadata categories and offer complex opinions that might match a user's interests (e.g., "classy joint for a date"). However, only recently have large language models (LLMs) let us unlock the commonsense connections between user preference utterances and complex language in user-generated reviews. Further, LLMs enable novel paradigms for semi-structured dialogue state tracking, complex intent and preference understanding, and generating recommendations, explanations, and question answers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state tracking system for ConvRec, showcased with a video, open source GitHub repository, and interactive Google Colab notebook.</li>
<li><strong>摘要：</strong>对话推荐 (ConvRec) 系统必须理解丰富多样的用户偏好和意图的自然语言 (NL) 表达，这些表达通常以间接方式传达（例如，“我正在注意自己的体重”）。这种复杂的话语使得检索相关项目变得具有挑战性，尤其是在仅使用通常不完整或过时的元数据的情况下。幸运的是，许多领域都具有丰富的项目评论，这些评论涵盖了标准元数据类别，并提供可能符合用户兴趣的复杂意见（例如，“约会的好去处”）。然而，直到最近，大型语言模型 (LLM) 才让我们能够解开用户偏好话语与用户生成的评论中的复杂语言之间的常识性联系。此外，LLM 为半结构化对话状态跟踪、复杂意图和偏好理解以及生成建议、解释和问题答案提供了新范式。因此，我们引入了一种新技术 RA-Rec，这是一种用于 ConvRec 的检索增强、LLM 驱动的对话状态跟踪系统，并通过视频、开源 GitHub 存储库和交互式 Google Colab 笔记本展示。</li>
</ul>

<h3>Title: Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories</h3>
<ul>
<li><strong>Authors: </strong>Tianlong Wang, Xianfeng Jiao, Yifan He, Zhongzhi Chen, Yinghao Zhu, Xu Chu, Junyi Gao, Yasha Wang, Liantao Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00034">https://arxiv.org/abs/2406.00034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00034">https://arxiv.org/pdf/2406.00034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00034]] Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories(https://arxiv.org/abs/2406.00034)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Recent studies have indicated that Large Language Models (LLMs) harbor an inherent understanding of truthfulness, yet often fail to express fully and generate false statements. This gap between "knowing" and "telling" poses a challenge for ensuring the truthfulness of generated content. To address this, we introduce Adaptive Activation Steering (ACT), a tuning-free method that adaptively shift LLM's activations in "truthful" direction during inference. ACT addresses diverse categories of hallucinations by utilizing diverse steering vectors and adjusting the steering intensity adaptively. Applied as an add-on across various models, ACT significantly improves truthfulness in LLaMA ($\uparrow$ 142\%), LLaMA2 ($\uparrow$ 24\%), Alpaca ($\uparrow$ 36\%), Vicuna ($\uparrow$ 28\%), and LLaMA2-Chat ($\uparrow$ 19\%). Furthermore, we verify ACT's scalability across larger models (13B, 33B, 65B), underscoring the adaptability of ACT to large-scale language models.</li>
<li><strong>摘要：</strong>最近的研究表明，大型语言模型 (LLM) 本身就具有对真实性的理解，但往往无法充分表达并产生虚假陈述。“知道”和“讲述”之间的这种差距对确保生成内容的真实性构成了挑战。为了解决这个问题，我们引入了自适应激活转向 (ACT)，这是一种无需调整的方法，可以在推理过程中自适应地将 LLM 的激活转移到“真实”方向。ACT 通过利用不同的转向矢量并自适应地调整转向强度来解决不同类别的幻觉。ACT 作为各种模型的附加组件应用，可显著提高 LLaMA ($\uparrow$ 142\%)、LLaMA2 ($\uparrow$ 24\%)、Alpaca ($\uparrow$ 36\%)、Vicuna ($\uparrow$ 28\%) 和 LLaMA2-Chat ($\uparrow$ 19\%) 的真实性。此外，我们验证了 ACT 在更大模型（13B、33B、65B）中的可扩展性，强调了 ACT 对大规模语言模型的适应性。</li>
</ul>

<h3>Title: EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan Feng, Xi Zhu, Zhoujun Li, Liantao Ma, Chengwei Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00036">https://arxiv.org/abs/2406.00036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00036">https://arxiv.org/pdf/2406.00036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00036]] EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling(https://arxiv.org/abs/2406.00036)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The integration of multimodal Electronic Health Records (EHR) data has notably advanced clinical predictive capabilities. However, current models that utilize clinical notes and multivariate time-series EHR data often lack the necessary medical context for precise clinical tasks. Previous methods using knowledge graphs (KGs) primarily focus on structured knowledge extraction. To address this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven framework aimed at enhancing multimodal EHR predictive modeling. Our approach extracts entities from both time-series data and clinical notes by prompting Large Language Models (LLMs) and aligns them with professional PrimeKG to ensure consistency. Beyond triplet relationships, we include entities' definitions and descriptions to provide richer semantics. The extracted knowledge is then used to generate task-relevant summaries of patients' health statuses. These summaries are fused with other modalities utilizing an adaptive multimodal fusion network with cross-attention. Extensive experiments on the MIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day readmission tasks demonstrate the superior performance of the EMERGE framework compared to baseline models. Comprehensive ablation studies and analyses underscore the efficacy of each designed module and the framework's robustness to data sparsity. EMERGE significantly enhances the use of multimodal EHR data in healthcare, bridging the gap with nuanced medical contexts crucial for informed clinical predictions.</li>
<li><strong>摘要：</strong>多模态电子健康记录 (EHR) 数据的整合显著提高了临床预测能力。然而，当前利用临床笔记和多变量时间序列 EHR 数据的模型通常缺乏精确临床任务所需的医疗背景。以前使用知识图谱 (KG) 的方法主要侧重于结构化知识提取。为了解决这个问题，我们提出了 EMERGE，这是一个检索增强生成 (RAG) 驱动的框架，旨在增强多模态 EHR 预测建模。我们的方法通过提示大型语言模型 (LLM) 从时间序列数据和临床笔记中提取实体，并将它们与专业的 PrimeKG 对齐以确保一致性。除了三元组关系之外，我们还包括实体的定义和描述以提供更丰富的语义。然后使用提取的知识生成与任务相关的患者健康状况摘要。这些摘要利用具有交叉注意的自适应多模态融合网络与其他模态融合。在 MIMIC-III 和 MIMIC-IV 数据集上进行的大量住院死亡率和 30 天再入院任务实验表明，与基线模型相比，EMERGE 框架具有更出色的性能。全面的消融研究和分析强调了每个设计模块的有效性以及框架对数据稀疏性的鲁棒性。EMERGE 显著增强了多模式 EHR 数据在医疗保健领域的使用，弥补了与对明智的临床预测至关重要的细微医疗背景的差距。</li>
</ul>

<h3>Title: Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Yang, Liyang He, Min Hou, Shuanghong Shen, Rui Li, Jiahui Hou, Jianhui Ma, Junda Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00037">https://arxiv.org/abs/2406.00037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00037">https://arxiv.org/pdf/2406.00037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00037]] Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering(https://arxiv.org/abs/2406.00037)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Code Community Question Answering (CCQA) seeks to tackle programming-related issues, thereby boosting productivity in both software engineering and academic research. Recent advancements in Reinforcement Learning from Human Feedback (RLHF) have transformed the fine-tuning process of Large Language Models (LLMs) to produce responses that closely mimic human behavior. Leveraging LLMs with RLHF for practical CCQA applications has thus emerged as a promising area of study. Unlike standard code question-answering tasks, CCQA involves multiple possible answers, with varying user preferences for each response. Additionally, code communities often show a preference for new APIs. These challenges prevent LLMs from generating responses that cater to the diverse preferences of users in CCQA tasks. To address these issues, we propose a novel framework called Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering (ALMupQA) to create user-focused responses. Our approach starts with Multi-perspective Preference Ranking Alignment (MPRA), which synthesizes varied user preferences based on the characteristics of answers from code communities. We then introduce a Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of outdated answers by retrieving responses to similar questions from a question bank. Due to the limited availability of high-quality, multi-answer CCQA datasets, we also developed a dataset named StaCCQA from real code communities. Extensive experiments demonstrated the effectiveness of the ALMupQA framework in terms of accuracy and user preference. Compared to the base model, ALMupQA showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in BERTScore and CodeBERTScore, respectively.</li>
<li><strong>摘要：</strong>代码社区问答 (CCQA) 旨在解决与编程相关的问题，从而提高软件工程和学术研究的生产力。强化学习从人类反馈 (RLHF) 方面取得的最新进展已经改变了大型语言模型 (LLM) 的微调过程，使其能够产生与人类行为非常相似的响应。因此，利用 LLM 和 RLHF 进行实际的 CCQA 应用已成为一个有前途的研究领域。与标准代码问答任务不同，CCQA 涉及多个可能的答案，每个答案的用户偏好各不相同。此外，代码社区通常表现出对新 API 的偏好。这些挑战阻碍了 LLM 生成迎合 CCQA 任务中用户不同偏好的响应。为了解决这些问题，我们提出了一个新框架，称为通过基于多视角用户偏好排名的反馈对齐 LLM 进行编程问答 (ALMupQA)，以创建以用户为中心的响应。我们的方法始于多视角偏好排序对齐 (MPRA)，它根据代码社区答案的特征综合不同的用户偏好。然后，我们引入了一个检索增强上下文学习 (RIL) 模块，通过从问题库中检索类似问题的答案来缓解答案过时的问题。由于高质量、多答案 CCQA 数据集的可用性有限，我们还从真实的代码社区开发了一个名为 StaCCQA 的数据集。大量实验证明了 ALMupQA 框架在准确性和用户偏好方面的有效性。与基础模型相比，ALMupQA 的 BLEU 提高了近 11%，BERTScore 和 CodeBERTScore 分别提高了 20% 和 17.5%。</li>
</ul>

<h3>Title: How Ready Are Generative Pre-trained Large Language Models for Explaining Bengali Grammatical Errors?</h3>
<ul>
<li><strong>Authors: </strong>Subhankar Maity, Aniket Deroy, Sudeshna Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00039">https://arxiv.org/abs/2406.00039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00039">https://arxiv.org/pdf/2406.00039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00039]] How Ready Are Generative Pre-trained Large Language Models for Explaining Bengali Grammatical Errors?(https://arxiv.org/abs/2406.00039)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Grammatical error correction (GEC) tools, powered by advanced generative artificial intelligence (AI), competently correct linguistic inaccuracies in user input. However, they often fall short in providing essential natural language explanations, which are crucial for learning languages and gaining a deeper understanding of the grammatical rules. There is limited exploration of these tools in low-resource languages such as Bengali. In such languages, grammatical error explanation (GEE) systems should not only correct sentences but also provide explanations for errors. This comprehensive approach can help language learners in their quest for proficiency. Our work introduces a real-world, multi-domain dataset sourced from Bengali speakers of varying proficiency levels and linguistic complexities. This dataset serves as an evaluation benchmark for GEE systems, allowing them to use context information to generate meaningful explanations and high-quality corrections. Various generative pre-trained large language models (LLMs), including GPT-4 Turbo, GPT-3.5 Turbo, Text-davinci-003, Text-babbage-001, Text-curie-001, Text-ada-001, Llama-2-7b, Llama-2-13b, and Llama-2-70b, are assessed against human experts for performance comparison. Our research underscores the limitations in the automatic deployment of current state-of-the-art generative pre-trained LLMs for Bengali GEE. Advocating for human intervention, our findings propose incorporating manual checks to address grammatical errors and improve feedback quality. This approach presents a more suitable strategy to refine the GEC tools in Bengali, emphasizing the educational aspect of language learning.</li>
<li><strong>摘要：</strong>语法错误纠正 (GEC) 工具由先进的生成人工智能 (AI) 提供支持，可以有效地纠正用户输入中的语言错误。然而，它们往往无法提供必要的自然语言解释，而这些解释对于学习语言和深入了解语法规则至关重要。在孟加拉语等资源匮乏的语言中，对这些工具的探索有限。在这些语言中，语法错误解释 (GEE) 系统不仅应纠正句子，还应提供错误解释。这种综合方法可以帮助语言学习者提高语言熟练程度。我们的工作引入了一个来自不同熟练程度和语言复杂程度的孟加拉语使用者的真实世界多领域数据集。该数据集作为 GEE 系统的评估基准，使它们能够使用上下文信息生成有意义的解释和高质量的更正。各种生成式预训练大型语言模型 (LLM)，包括 GPT-4 Turbo、GPT-3.5 Turbo、Text-davinci-003、Text-babbage-001、Text-curie-001、Text-ada-001、Llama-2-7b、Llama-2-13b 和 Llama-2-70b，都与人类专家进行了性能比较。我们的研究强调了当前最先进的生成式预训练 LLM 在孟加拉语 GEE 中的自动部署的局限性。我们的研究结果提倡人工干预，建议结合人工检查来解决语法错误并提高反馈质量。这种方法提出了一种更合适的策略来改进孟加拉语的 GEC 工具，强调语言学习的教育方面。</li>
</ul>

<h3>Title: QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM</h3>
<ul>
<li><strong>Authors: </strong>Rui Guo, Greg Farnan, Niall McLaughlin, Barry Devereux</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by open-source LLM(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to reduce the administrative burden on clinicians by automating the creation of critical sections of patient discharge letters. This paper presents our approach using the Llama3 8B quantized model to generate the "Brief Hospital Course" and "Discharge Instructions" sections. We employ a zero-shot method combined with Retrieval-Augmented Generation (RAG) to produce concise, contextually accurate summaries. Our contributions include the development of a curated template-based approach to ensure reliability and consistency, as well as the integration of RAG for word count prediction. We also describe several unsuccessful experiments to provide insights into our pathway for the competition. Our results demonstrate the effectiveness and efficiency of our approach, achieving high scores across multiple evaluation metrics.</li>
<li><strong>摘要：</strong>BioNLP ACL'24 简化出院文档共享任务旨在通过自动创建患者出院信的关键部分来减轻临床医生的行政负担。本文介绍了我们使用 Llama3 8B 量化模型生成“简要医院课程”和“出院说明”部分的方法。我们采用零样本方法结合检索增强生成 (RAG) 来生成简洁、上下文准确的摘要。我们的贡献包括开发一种基于模板的精选方法以确保可靠性和一致性，以及集成 RAG 进行字数预测。我们还描述了几个不成功的实验，以深入了解我们的比赛之路。我们的结果证明了我们方法的有效性和效率，在多个评估指标中取得了高分。</li>
</ul>

<h3>Title: Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, Jinghui Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00045">https://arxiv.org/abs/2406.00045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00045">https://arxiv.org/pdf/2406.00045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00045]] Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization(https://arxiv.org/abs/2406.00045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM. Recent endeavors have introduced more lightweight strategies, focusing on extracting "steering vectors" to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios. This work proposes an innovative approach that could produce more effective steering vectors through bi-directional preference optimization. Our method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities. Extensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach. Moreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously.</li>
<li><strong>摘要：</strong>研究人员一直在研究控制大型语言模型 (LLM) 行为的方法，并构建针对各种应用量身定制的个性化 LLM。虽然微调似乎是一种直接的解决方案，但它需要大量的计算资源，并且可能会显著影响原始 LLM 的效用。最近的努力引入了更轻量级的策略，重点是提取“引导向量”，通过调整 LLM 转换器架构特定层内的激活来引导模型的输出朝着期望的行为发展。然而，这种引导向量是直接从人类偏好数据的激活中提取的，因此经常会导致次优结果和偶尔的失败，尤其是在与对齐相关的场景中。这项工作提出了一种创新方法，可以通过双向偏好优化产生更有效的引导向量。我们的方法旨在让引导向量直接影响对比人类偏好数据对的生成概率，从而提供更精确的目标行为表示。通过仔细调整引导向量的方向和幅度，我们能够在一系列强度范围内对期望的行为进行个性化控制。在各种开放式生成任务中开展的大量实验（尤其是针对引导 AI 角色的实验）验证了我们方法的有效性。此外，我们还全面研究了与对齐有关的关键场景，例如管理真实性、减轻幻觉和应对越狱攻击。值得注意的是，我们的方法在这些场景中仍能表现出出色的引导效果。此外，我们展示了我们的引导向量在不同模型/LoRA 之间的可迁移性，并强调了同时应用多个向量的协同优势。</li>
</ul>

<h3>Title: Towards a theory of how the structure of language is acquired by deep neural networks</h3>
<ul>
<li><strong>Authors: </strong>Francesco Cagnetta, Matthieu Wyart</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.dis-nn, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00048">https://arxiv.org/abs/2406.00048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00048">https://arxiv.org/pdf/2406.00048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00048]] Towards a theory of how the structure of language is acquired by deep neural networks(https://arxiv.org/abs/2406.00048)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>How much data is required to learn the structure of a language via next-token prediction? We study this question for synthetic datasets generated via a Probabilistic Context-Free Grammar (PCFG) -- a hierarchical generative model that captures the tree-like structure of natural languages. We determine token-token correlations analytically in our model and show that they can be used to build a representation of the grammar's hidden variables, the longer the range the deeper the variable. In addition, a finite training set limits the resolution of correlations to an effective range, whose size grows with that of the training set. As a result, a Language Model trained with increasingly many examples can build a deeper representation of the grammar's structure, thus reaching good performance despite the high dimensionality of the problem. We conjecture that the relationship between training set size and effective range of correlations holds beyond our synthetic datasets. In particular, our conjecture predicts how the scaling law for the test loss behaviour with training set size depends on the length of the context window, which we confirm empirically for a collection of lines from Shakespeare's plays.</li>
<li><strong>摘要：</strong>通过下一个标记预测来学习语言结构需要多少数据？我们针对通过概率上下文无关语法 (PCFG) 生成的合成数据集研究了这个问题——PCFG 是一种捕捉自然语言树状结构的分层生成模型。我们在模型中分析确定标记与标记之间的相关性，并表明它们可用于构建语法隐藏变量的表示，范围越长，变量越深。此外，有限的训练集将相关性的分辨率限制在有效范围内，其大小随训练集的大小而增长。因此，使用越来越多示例训练的语言模型可以构建语法结构的更深层次表示，从而尽管问题的维度很高，但仍能达到良好的性能。我们推测训练集大小与相关有效范围之间的关系超出了我们的合成数据集。具体来说，我们的猜想预测了测试损失行为随训练集大小的缩放规律如何取决于上下文窗口的长度，我们通过莎士比亚戏剧中的一系列台词进行了实证证实。</li>
</ul>

<h3>Title: QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Gonçalo R. A. Faria, Sweta Agrawal, António Farinhas, Ricardo Rei, José G. C. de Souza, André F. T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00049">https://arxiv.org/abs/2406.00049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00049">https://arxiv.org/pdf/2406.00049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00049]] QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation(https://arxiv.org/abs/2406.00049)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>An important challenge in machine translation (MT) is to generate high-quality and diverse translations. Prior work has shown that the estimated likelihood from the MT model correlates poorly with translation quality. In contrast, quality evaluation metrics (such as COMET or BLEURT) exhibit high correlations with human judgments, which has motivated their use as rerankers (such as quality-aware and minimum Bayes risk decoding). However, relying on a single translation with high estimated quality increases the chances of "gaming the metric''. In this paper, we address the problem of sampling a set of high-quality and diverse translations. We provide a simple and effective way to avoid over-reliance on noisy quality estimates by using them as the energy function of a Gibbs distribution. Instead of looking for a mode in the distribution, we generate multiple samples from high-density areas through the Metropolis-Hastings algorithm, a simple Markov chain Monte Carlo approach. The results show that our proposed method leads to high-quality and diverse outputs across multiple language pairs (English$\leftrightarrow${German, Russian}) with two strong decoder-only LLMs (Alma-7b, Tower-7b).</li>
<li><strong>摘要：</strong>机器翻译 (MT) 面临的一个重大挑战是生成高质量且多样化的翻译。先前的研究表明，机器翻译模型估计的似然值与翻译质量相关性较差。相比之下，质量评估指标（如 COMET 或 BLEURT）与人类判断具有较高的相关性，这促使它们被用作重新排序器（如质量感知和最小贝叶斯风险解码）。然而，依赖单个质量评估较高的翻译会增加“操纵指标”的可能性。在本文中，我们解决了对一组高质量和多样化翻译进行抽样的问题。我们提供了一种简单有效的方法来避免过度依赖嘈杂的质量估计，即将它们用作吉布斯分布的能量函数。我们不是在分布中寻找模式，而是通过 Metropolis-Hastings 算法（一种简单的马尔可夫链蒙特卡罗方法）从高密度区域生成多个样本。结果表明，我们提出的方法可以在多个语言对（英语$\leftrightarrow${德语、俄语}）中使用两个强大的仅解码器 LLM（Alma-7b、Tower-7b）产生高质量和多样化的输出。</li>
</ul>

<h3>Title: An Empirical Analysis on Large Language Models in Debate Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Liu, Pinxin Liu, Hangfeng He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00050">https://arxiv.org/abs/2406.00050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00050">https://arxiv.org/pdf/2406.00050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00050]] An Empirical Analysis on Large Language Models in Debate Evaluation(https://arxiv.org/abs/2406.00050)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this study, we investigate the capabilities and inherent biases of advanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the context of debate evaluation. We discover that LLM's performance exceeds humans and surpasses the performance of state-of-the-art methods fine-tuned on extensive datasets in debate evaluation. We additionally explore and analyze biases present in LLMs, including positional bias, lexical bias, order bias, which may affect their evaluative judgments. Our findings reveal a consistent bias in both GPT-3.5 and GPT-4 towards the second candidate response presented, attributed to prompt design. We also uncover lexical biases in both GPT-3.5 and GPT-4, especially when label sets carry connotations such as numerical or sequential, highlighting the critical need for careful label verbalizer selection in prompt design. Additionally, our analysis indicates a tendency of both models to favor the debate's concluding side as the winner, suggesting an end-of-discussion bias.</li>
<li><strong>摘要：</strong>在本研究中，我们研究了 GPT-3.5 和 GPT-4 等高级大型语言模型 (LLM) 在辩论评估方面的功能和固有偏见。我们发现，LLM 的表现超越了人类，也超越了在辩论评估中对大量数据集进行微调的最先进方法的表现。我们还探索和分析了 LLM 中存在的偏见，包括位置偏见、词汇偏见、顺序偏见，这些偏见可能会影响它们的评估判断。我们的研究结果表明，GPT-3.5 和 GPT-4 对呈现的第二个候选答案都存在一致的偏见，这归因于提示设计。我们还发现 GPT-3.5 和 GPT-4 都存在词汇偏见，尤其是当标签集带有数字或顺序等内涵时，这凸显了在提示设计中谨慎选择标签语言的迫切需要。此外，我们的分析表明，两种模型都倾向于支持辩论的结论方作为获胜者，这表明存在讨论结束时的偏见。</li>
</ul>

<h3>Title: Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Suraj Anand, Michael A. Lepori, Jack Merullo, Ellie Pavlick</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00053">https://arxiv.org/abs/2406.00053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00053">https://arxiv.org/pdf/2406.00053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00053]] Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting(https://arxiv.org/abs/2406.00053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models have the ability to perform in-context learning (ICL), allowing them to flexibly adapt their behavior based on context. This contrasts with in-weights learning, where information is statically encoded in model parameters from iterated observations of the data. Despite this apparent ability to learn in-context, language models are known to struggle when faced with unseen or rarely seen tokens. Hence, we study $\textbf{structural in-context learning}$, which we define as the ability of a model to execute in-context learning on arbitrary tokens -- so called because the model must generalize on the basis of e.g. sentence structure or task structure, rather than semantic content encoded in token embeddings. An ideal model would be able to do both: flexibly deploy in-weights operations (in order to robustly accommodate ambiguous or unknown contexts using encoded semantic information) and structural in-context operations (in order to accommodate novel tokens). We study structural in-context algorithms in a simple part-of-speech setting using both practical and toy models. We find that active forgetting, a technique that was recently introduced to help models generalize to new languages, forces models to adopt structural in-context learning solutions. Finally, we introduce $\textbf{temporary forgetting}$, a straightforward extension of active forgetting that enables one to control how much a model relies on in-weights vs. in-context solutions. Importantly, temporary forgetting allows us to induce a $\textit{dual process strategy}$ where in-context and in-weights solutions coexist within a single model.</li>
<li><strong>摘要：</strong>语言模型能够执行上下文学习 (ICL)，从而可以根据上下文灵活地调整其行为。这与权重学习形成对比，权重学习中的信息是从对数据的迭代观察中静态编码到模型参数中的。尽管语言模型具有这种明显的上下文学习能力，但众所周知，在面对看不见或很少见到的标记时，语言模型会遇到困难。因此，我们研究了结构化上下文学习，我们将其定义为模型对任意标记执行上下文学习的能力——之所以这样称呼，是因为模型必须基于句子结构或任务结构等进行概括，而不是基于标记嵌入中编码的语义内容。理想的模型应该能够同时做到这两点：灵活地部署权重操作（以便使用编码的语义信息稳健地适应模糊或未知的上下文）和结构化上下文操作（以便适应新的标记）。我们使用实用模型和玩具模型在简单的词性设置中研究结构化上下文算法。我们发现，主动遗忘（一种最近引入的技术，用于帮助模型推广到新语言）迫使模型采用结构化的上下文学习解决方案。最后，我们引入了 $\textbf{临时遗忘}$，这是主动遗忘的直接扩展，可以控制模型对权重和上下文解决方案的依赖程度。重要的是，临时遗忘使我们能够引入 $\textit{双重过程策略}$，其中上下文和权重解决方案共存于单个模型中。</li>
</ul>

<h3>Title: Toward Conversational Agents with Context and Time Sensitive Long-term Memory</h3>
<ul>
<li><strong>Authors: </strong>Nick Alonso, Tomás Figliolia, Anthony Ndirango, Beren Millidge</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00057">https://arxiv.org/abs/2406.00057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00057">https://arxiv.org/pdf/2406.00057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00057]] Toward Conversational Agents with Context and Time Sensitive Long-term Memory(https://arxiv.org/abs/2406.00057)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>There has recently been growing interest in conversational agents with long-term memory which has led to the rapid development of language models that use retrieval-augmented generation (RAG). Until recently, most work on RAG has focused on information retrieval from large databases of texts, like Wikipedia, rather than information from long-form conversations. In this paper, we argue that effective retrieval from long-form conversational data faces two unique problems compared to static database retrieval: 1) time/event-based queries, which requires the model to retrieve information about previous conversations based on time or the order of a conversational event (e.g., the third conversation on Tuesday), and 2) ambiguous queries that require surrounding conversational context to understand. To better develop RAG-based agents that can deal with these challenges, we generate a new dataset of ambiguous and time-based questions that build upon a recent dataset of long-form, simulated conversations, and demonstrate that standard RAG based approaches handle such questions poorly. We then develop a novel retrieval model which combines chained-of-table search methods, standard vector-database retrieval, and a prompting method to disambiguate queries, and demonstrate that this approach substantially improves over current methods at solving these tasks. We believe that this new dataset and more advanced RAG agent can act as a key benchmark and stepping stone towards effective memory augmented conversational agents that can be used in a wide variety of AI applications.</li>
<li><strong>摘要：</strong>最近，人们对具有长期记忆的对话代理的兴趣日益浓厚，这导致了使用检索增强生成 (RAG) 的语言模型的快速发展。直到最近，RAG 的大多数工作都集中在从大型文本数据库（如 Wikipedia）中检索信息，而不是从长篇对话中检索信息。在本文中，我们认为，与静态数据库检索相比，从长篇对话数据中进行有效检索面临两个独特的问题：1) 基于时间/事件的查询，这需要模型根据时间或对话事件的顺序检索有关先前对话的信息（例如，周二的第三次对话），以及 2) 需要周围对话上下文才能理解的模糊查询。为了更好地开发能够应对这些挑战的基于 RAG 的代理，我们生成了一个新的模糊和基于时间的问题数据集，该数据集基于最近的长篇模拟对话数据集，并证明基于标准 RAG 的方法无法很好地处理此类问题。然后，我们开发了一种新颖的检索模型，该模型结合了链式表搜索方法、标准矢量数据库检索和用于消除查询歧义的提示方法，并证明这种方法在解决这些任务方面比当前方法有了显著改进。我们相信，这个新的数据集和更先进的 RAG 代理可以作为有效的记忆增强对话代理的关键基准和垫脚石，这些代理可用于各种 AI 应用程序。</li>
</ul>

<h3>Title: Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution</h3>
<ul>
<li><strong>Authors: </strong>Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00059">https://arxiv.org/abs/2406.00059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00059">https://arxiv.org/pdf/2406.00059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00059]] Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution(https://arxiv.org/abs/2406.00059)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can improve request completion latency by up to 38.8%.</li>
<li><strong>摘要：</strong>由于与外部工具调用（例如 ChatGPT 插件）的集成，大型语言模型 (LLM) 服务工作负载的复杂性大幅增加。在本文中，我们发现了为触发工具的请求提供高效 LLM 服务的新机会：在 LLM 解码的同时执行工具部分执行。为此，我们设计了 Conveyor，这是一种高效的 LLM 服务系统，针对处理涉及外部工具的请求进行了优化。我们为工具开发人员引入了一个新颖的接口，以向 LLM 服务系统公开部分执行机会，并引入了一个促进部分工具执行的请求调度程序。我们的结果表明，工具部分执行可以将请求完成延迟提高多达 38.8%。</li>
</ul>

<h3>Title: Cascade-Aware Training of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Congchao Wang, Sean Augenstein, Keith Rush, Wittawat Jitkrittum, Harikrishna Narasimhan, Ankit Singh Rawat, Aditya Krishna Menon, Alec Go</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00060">https://arxiv.org/abs/2406.00060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00060">https://arxiv.org/pdf/2406.00060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00060]] Cascade-Aware Training of Language Models(https://arxiv.org/abs/2406.00060)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reducing serving cost and latency is a fundamental concern for the deployment of language models (LMs) in business applications. To address this, cascades of LMs offer an effective solution that conditionally employ smaller models for simpler queries. Cascaded systems are typically built with independently trained models, neglecting the advantages of considering inference-time interactions of the cascaded LMs during training. In this paper, we present cascade-aware training(CAT), an approach to optimizing the overall quality-cost performance tradeoff of a cascade of LMs. We achieve inference-time benefits by training the small LM with awareness of its place in a cascade and downstream capabilities. We demonstrate the value of the proposed method with over 60 LM tasks of the SuperGLUE, WMT22, and FLAN2021 datasets.</li>
<li><strong>摘要：</strong>降低服务成本和延迟是部署语言模型 (LM) 到业务应用中的基本考虑因素。为了解决这个问题，LM 级联提供了一种有效的解决方案，即有条件地使用较小的模型来执行更简单的查询。级联系统通常使用独立训练的模型构建，忽略了在训练期间考虑级联 LM 的推理时间交互的优势。在本文中，我们提出了级联感知训练 (CAT)，这是一种优化 LM 级联总体质量成本性能权衡的方法。我们通过训练小型 LM 并意识到其在级联中的位置和下游功能来实现推理时间优势。我们通过 SuperGLUE、WMT22 和 FLAN2021 数据集的 60 多个 LM 任务证明了所提方法的价值。</li>
</ul>

<h3>Title: Unlocking the Potential of Large Language Models for Clinical Text Anonymization: A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>David Pissarra, Isabel Curioso, João Alveira, Duarte Pereira, Bruno Ribeiro, Tomás Souper, Vasco Gomes, André V. Carreiro, Vitor Rolla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00062">https://arxiv.org/abs/2406.00062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00062">https://arxiv.org/pdf/2406.00062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00062]] Unlocking the Potential of Large Language Models for Clinical Text Anonymization: A Comparative Study(https://arxiv.org/abs/2406.00062)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Automated clinical text anonymization has the potential to unlock the widespread sharing of textual health data for secondary usage while assuring patient privacy and safety. Despite the proposal of many complex and theoretically successful anonymization solutions in literature, these techniques remain flawed. As such, clinical institutions are still reluctant to apply them for open access to their data. Recent advances in developing Large Language Models (LLMs) pose a promising opportunity to further the field, given their capability to perform various tasks. This paper proposes six new evaluation metrics tailored to the challenges of generative anonymization with LLMs. Moreover, we present a comparative study of LLM-based methods, testing them against two baseline techniques. Our results establish LLM-based models as a reliable alternative to common approaches, paving the way toward trustworthy anonymization of clinical text.</li>
<li><strong>摘要：</strong>临床文本自动匿名化有可能解锁文本健康数据的广泛共享以供二次使用，同时确保患者的隐私和安全。尽管文献中提出了许多复杂且理论上成功的匿名化解决方案，但这些技术仍然存在缺陷。因此，临床机构仍然不愿意将它们应用于开放访问其数据。大型语言模型 (LLM) 开发方面的最新进展为进一步发展该领域提供了有希望的机会，因为它们能够执行各种任务。本文提出了六种新的评估指标，专门针对使用 LLM 进行生成匿名化的挑战。此外，我们对基于 LLM 的方法进行了比较研究，并将它们与两种基线技术进行了测试。我们的研究结果确立了基于 LLM 的模型作为常用方法的可靠替代方案的地位，为实现值得信赖的临床文本匿名化铺平了道路。</li>
</ul>

<h3>Title: Confidence-Aware Sub-Structure Beam Search (CABS): Mitigating Hallucination in Structured Data Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chengwei Wei, Kee Kiat Koo, Amir Tavanaei, Karim Bouyarmane</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Confidence-Aware Sub-Structure Beam Search (CABS): Mitigating Hallucination in Structured Data Generation with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have facilitated structured data generation, with applications in domains like tabular data, document databases, product catalogs, etc. However, concerns persist about generation veracity due to incorrect references or hallucinations, necessitating the incorporation of some form of model confidence for mitigation. Existing confidence estimation methods on LLM generations primarily focus on the confidence at the individual token level or the entire output sequence level, limiting their applicability to structured data generation, which consists of an intricate mix of both independent and correlated entries at the sub-structure level. In this paper, we first investigate confidence estimation methods for generated sub-structure-level data. We introduce the concept of Confidence Network that applies on the hidden state of the LLM transformer, as a more targeted estimate than the traditional token conditional probability. We further propose Confidence-Aware sub-structure Beam Search (CABS), a novel decoding method operating at the sub-structure level in structured data generation. CABS enhances the faithfulness of structured data generation by considering confidence scores from the Confidence Network for each sub-structure-level data and iteratively refining the prompts. Results show that CABS outperforms traditional token-level beam search for structured data generation by 16.7% Recall at 90% precision averagely on the problem of product attribute generation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 促进了结构化数据的生成，可应用于表格数据、文档数据库、产品目录等领域。然而，由于错误的引用或幻觉，人们仍然担心生成的真实性，因此需要加入某种形式的模型置信度来缓解这一问题。现有的 LLM 生成置信度估计方法主要关注单个 token 级别或整个输出序列级别的置信度，这限制了它们在结构化数据生成中的适用性，因为结构化数据生成由子结构级别的独立和相关条目的复杂组合组成。在本文中，我们首先研究生成的子结构级数据的置信度估计方法。我们引入了置信网络的概念，它应用于 LLM 转换器的隐藏状态，作为比传统 token 条件概率更有针对性的估计。我们进一步提出了置信感知子结构束搜索 (CABS)，这是一种在结构化数据生成子结构级别运行的新型解码方法。 CABS 通过考虑每个子结构级数据的置信度网络置信度得分并迭代细化提示来提高结构化数据生成的忠实度。结果表明，在产品属性生成问题上，CABS 的平均召回率为 16.7%，准确率为 90%，优于传统的结构化数据生成标记级束搜索。</li>
</ul>

<h3>Title: On the referential capacity of language models: An internalist rejoinder to Mandelkern & Linzen</h3>
<ul>
<li><strong>Authors: </strong>Giosue Baggio, Elliot Murphy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00159">https://arxiv.org/abs/2406.00159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00159">https://arxiv.org/pdf/2406.00159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00159]] On the referential capacity of language models: An internalist rejoinder to Mandelkern & Linzen(https://arxiv.org/abs/2406.00159)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In a recent paper, Mandelkern & Linzen (2024) - henceforth M&L - address the question of whether language models' (LMs) words refer. Their argument draws from the externalist tradition in philosophical semantics, which views reference as the capacity of words to "achieve 'word-to-world' connections". In the externalist framework, causally uninterrupted chains of usage, tracing every occurrence of a name back to its bearer, guarantee that, for example, 'Peano' refers to the individual Peano (Kripke 1980). This account is externalist both because words pick out referents 'out there' in the world, and because what determines reference are coordinated linguistic actions by members of a community, and not individual mental states. The "central question to ask", for M&L, is whether LMs too belong to human linguistic communities, such that words by LMs may also trace back causally to their bearers. Their answer is a cautious "yes": inputs to LMs are linguistic "forms with particular histories of referential use"; "those histories ground the referents of those forms"; any occurrence of 'Peano' in LM outputs is as causally connected to the individual Peano as any other occurrence of the same proper name in human speech or text; therefore, occurrences of 'Peano' in LM outputs refer to Peano. In this commentary, we first qualify M&L's claim as applying to a narrow class of natural language expressions. Thus qualified, their claim is valid, and we emphasise an additional motivation for that in Section 2. Next, we discuss the actual scope of their claim, and we suggest that the way they formulate it may lead to unwarranted generalisations about reference in LMs. Our critique is likewise applicable to other externalist accounts of LMs (e.g., Lederman & Mahowald 2024; Mollo & Milliere 2023). Lastly, we conclude with a comment on the status of LMs as members of human linguistic communities.</li>
<li><strong>摘要：</strong>在最近的一篇论文中，Mandelkern & Linzen (2024)（以下简称 M&L）探讨了语言模型 (LM) 的词语是否指称的问题。他们的论点源自哲学语义学中的外部主义传统，该传统将指称视为词语“实现‘词语到世界’联系”的能力。在外部主义框架中，因果不间断的使用链将名称的每次出现追溯到其承载者，从而保证例如“Peano”指代个体 Peano (Kripke 1980)。这种解释是外部主义的，因为词语会挑选出世界上“外面”的指称物，而且决定指称的是社区成员协调的语言行为，而不是个人的心理状态。对于 M&L 来说，“核心问题是 LM 是否也属于人类语言社区，因此 LM 的词语也可以因果追溯到其承载者。他们的回答是谨慎的“是”：语言模型的输入是语言的“具有特定指称使用历史的形式”；“这些历史为这些形式的指称奠定了基础”；语言模型输出中出现的任何“皮亚诺”都与单个皮亚诺有因果关系，就像人类语音或文本中出现任何其他相同专有名词一样；因此，语言模型输出中出现的“皮亚诺”指的是皮亚诺。在本评论中，我们首先将 M&L 的主张限定为适用于一类狭义的自然语言表达。经过限定，他们的主张是有效的，我们在第 2 节中强调了这一主张的另一个动机。接下来，我们讨论他们主张的实际范围，并指出他们提出这一主张的方式可能会导致对语言模型中指称的不合理概括。我们的批评同样适用于其他外在主义的语言模型解释（例如，Lederman & Mahowald 2024；Mollo & Milliere 2023）。最后，我们对 LM 作为人类语言社区成员的地位进行了评论。</li>
</ul>

<h3>Title: Long-Span Question-Answering: Automatic Question Generation and QA-System Ranking via Side-by-Side Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Bernd Bohnet, Kevin Swersky, Rosanne Liu, Pranjal Awasthi, Azade Nova, Javier Snaider, Hanie Sedghi, Aaron T Parisi, Michael Collins, Angeliki Lazaridou, Orhan Firat, Noah Fiedel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00179">https://arxiv.org/abs/2406.00179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00179">https://arxiv.org/pdf/2406.00179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00179]] Long-Span Question-Answering: Automatic Question Generation and QA-System Ranking via Side-by-Side Evaluation(https://arxiv.org/abs/2406.00179)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We explore the use of long-context capabilities in large language models to create synthetic reading comprehension data from entire books. Previous efforts to construct such datasets relied on crowd-sourcing, but the emergence of transformers with a context size of 1 million or more tokens now enables entirely automatic approaches. Our objective is to test the capabilities of LLMs to analyze, understand, and reason over problems that require a detailed comprehension of long spans of text, such as questions involving character arcs, broader themes, or the consequences of early actions later in the story. We propose a holistic pipeline for automatic data generation including question generation, answering, and model scoring using an ``Evaluator''. We find that a relative approach, comparing answers between models in a pairwise fashion and ranking with a Bradley-Terry model, provides a more consistent and differentiating scoring mechanism than an absolute scorer that rates answers individually. We also show that LLMs from different model families produce moderate agreement in their ratings. We ground our approach using the manually curated NarrativeQA dataset, where our evaluator shows excellent agreement with human judgement and even finds errors in the dataset. Using our automatic evaluation approach, we show that using an entire book as context produces superior reading comprehension performance compared to baseline no-context (parametric knowledge only) and retrieval-based approaches.</li>
<li><strong>摘要：</strong>我们探索在大型语言模型中使用长上下文功能来从整本书中创建合成阅读理解数据。以前构建此类数据集的努力依赖于众包，但上下文大小为 100 万或更多标记的转换器的出现现在可以实现完全自动化的方法。我们的目标是测试 LLM 分析、理解和推理需要详细理解长篇文本的问题的能力，例如涉及角色弧、更广泛的主题或故事后期早期行动的后果的问题。我们提出了一个用于自动数据生成的整体流程，包括问题生成、回答和使用“评估器”进行模型评分。我们发现，与对答案进行单独评分的绝对评分器相比，相对方法（以成对的方式比较模型之间的答案并使用 Bradley-Terry 模型进行排名）提供了更一致和差异化的评分机制。我们还表明，来自不同模型系列的 LLM 在评级中产生了中等程度的一致性。我们利用手动整理的 NarrativeQA 数据集来验证我们的方法，我们的评估器与人类判断非常吻合，甚至能发现数据集中的错误。使用我们的自动评估方法，我们表明，与基线无上下文（仅参数知识）和基于检索的方法相比，使用整本书作为上下文可以产生更出色的阅读理解性能。</li>
</ul>

<h3>Title: Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision</h3>
<ul>
<li><strong>Authors: </strong>Qian Ruan, Ilia Kuznetsov, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00197">https://arxiv.org/abs/2406.00197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00197">https://arxiv.org/pdf/2406.00197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00197]] Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision(https://arxiv.org/abs/2406.00197)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.</li>
<li><strong>摘要：</strong>文本文档的协作审阅和修订是知识工作的核心，也是实证分析和 NLP 辅助的有希望的目标。然而，缺乏一个能够对文档修订、审阅和作者回复之间的复杂关系进行建模的整体框架。为了解决这一差距，我们引入了 Re3，这是一个用于联合分析协作文档修订的框架。我们在学术领域实例化了这个框架，并提出了 Re3-Sci，这是一个大型的对齐科学论文修订语料库，根据其行动和意图手动标记，并补充了相应的同行评审和人工编写的编辑摘要。我们使用新数据为学术领域的协作文档修订提供初步的实证见解，并评估最先进的 LLM 在自动化编辑分析和促进基于文本的协作方面的能力。我们公开了我们的注释环境和协议、结果数据和实验代码。</li>
</ul>

<h3>Title: Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training</h3>
<ul>
<li><strong>Authors: </strong>Maximillian Chen, Ruoxi Sun, Sercan Ö. Arık, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00222">https://arxiv.org/abs/2406.00222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00222">https://arxiv.org/pdf/2406.00222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00222]] Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training(https://arxiv.org/abs/2406.00222)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) aligned through reinforcement learning from human feedback (RLHF) have quickly become one of the dominant paradigms for building intelligent conversational assistant agents. However, despite their strong performance across many benchmarks, LLM-based agents still lack conversational skills such as disambiguation: when generalized assistants are faced with ambiguity, they often overhedge or implicitly guess users' ground-truth intents rather than asking clarification questions, and under task-specific settings, high-quality conversation samples are often limited, affecting models' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (henceforth ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO) which allows for sample-efficient dialogue policy learning in multi-turn conversation. We demonstrate ACT's efficacy under sample-efficient conditions in three difficult conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for text-to-SQL generation. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard approaches to supervised fine-tuning and DPO.</li>
<li><strong>摘要：</strong>通过从人类反馈中强化学习 (RLHF) 对齐的大型语言模型 (LLM) 已迅速成为构建智能对话助手代理的主要范例之一。然而，尽管基于 LLM 的代理在许多基准测试中表现强劲，但它们仍然缺乏诸如消歧之类的对话技能：当通用助手面临歧义时，它们通常会过度含糊或隐含地猜测用户的真实意图，而不是提出澄清问题，并且在特定于任务的设置下，高质量对话样本通常有限，影响模型学习最佳对话行动策略的能力。我们提出了基于动作的对比自训练 (ACT)，这是一种基于直接偏好优化 (DPO) 的准在线偏好优化算法，可在多轮对话中进行样本高效的对话策略学习。我们在三个困难的对话任务中展示了 ACT 在样本高效条件下的有效性：基于表格的问答、机器阅读理解和 AmbigSQL，AmbigSQL 是一种用于消除文本到 SQL 生成的信息搜索请求歧义的新任务。此外，我们建议通过检查 LLM 是否能够隐式识别和推理对话中的歧义来评估其作为对话代理的能力。与监督微调和 DPO 的标准方法相比，ACT 展示了对话建模的显著改进。</li>
</ul>

<h3>Title: Controlling Large Language Model Agents with Entropic Activation Steering</h3>
<ul>
<li><strong>Authors: </strong>Nate Rahn, Pierluca D'Oro, Marc G. Bellemare</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00244">https://arxiv.org/abs/2406.00244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00244">https://arxiv.org/pdf/2406.00244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00244]] Controlling Large Language Model Agents with Entropic Activation Steering(https://arxiv.org/abs/2406.00244)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>The generality of pretrained large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. To be successful, such agents must form beliefs about how to achieve their goals based on limited interaction with their environment, resulting in uncertainty about the best action to take at each step. In this paper, we study how LLM agents form and act on these beliefs by conducting experiments in controlled sequential decision-making tasks. To begin, we find that LLM agents are overconfident: They draw strong conclusions about what to do based on insufficient evidence, resulting in inadequately explorative behavior. We dig deeper into this phenomenon and show how it emerges from a collapse in the entropy of the action distribution implied by sampling from the LLM. We then demonstrate that existing token-level sampling techniques are by themselves insufficient to make the agent explore more. Motivated by this fact, we introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. EAST computes a steering vector as an entropy-weighted combination of representations, and uses it to manipulate an LLM agent's uncertainty over actions by intervening on its activations during the forward pass. We show that EAST can reliably increase the entropy in an LLM agent's actions, causing more explorative behavior to emerge. Finally, EAST modifies the subjective uncertainty an LLM agent expresses, paving the way to interpreting and controlling how LLM agents represent uncertainty about their decisions.</li>
<li><strong>摘要：</strong>预训练大型语言模型 (LLM) 的通用性引起了人们对将其用作上下文学习代理的兴趣。为了取得成功，此类代理必须基于与环境的有限交互形成关于如何实现目标的信念，从而导致对每一步采取的最佳行动产生不确定性。在本文中，我们通过在受控的顺序决策任务中进行实验来研究 LLM 代理如何形成并根据这些信念采取行动。首先，我们发现 LLM 代理过于自信：他们根据不充分的证据得出关于做什么的有力结论，导致探索行为不足。我们深入研究了这一现象，并展示了它是如何从 LLM 采样所暗示的动作分布熵崩溃中出现的。然后，我们证明现有的 token 级采样技术本身不足以使代理进行更多探索。受此事实的启发，我们引入了熵激活转向 (EAST)，这是一种用于上下文 LLM 代理的激活转向方法。 EAST 计算一个控制向量作为表示的熵加权组合，并通过在前向传递过程中干预其激活来操纵 LLM 代理对动作的不确定性。我们表明 EAST 可以可靠地增加 LLM 代理动作的熵，从而导致更多的探索性行为出现。最后，EAST 修改了 LLM 代理表达的主观不确定性，为解释和控制 LLM 代理如何表示其决策的不确定性铺平了道路。</li>
</ul>

<h3>Title: Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Saidul Islam, Raian Rahman, Ahmed Masry, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, Enamul Hoque</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00257">https://arxiv.org/abs/2406.00257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00257">https://arxiv.org/pdf/2406.00257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00257]] Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of LVLMs(https://arxiv.org/abs/2406.00257)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Natural language is a powerful complementary modality of communication for data visualizations, such as bar and line charts. To facilitate chart-based reasoning using natural language, various downstream tasks have been introduced recently such as chart question answering, chart summarization, and fact-checking with charts. These tasks pose a unique challenge, demanding both vision-language reasoning and a nuanced understanding of chart data tables, visual encodings, and natural language prompts. Despite the recent success of Large Language Models (LLMs) across diverse NLP tasks, their abilities and limitations in the realm of data visualization remain under-explored, possibly due to their lack of multi-modal capabilities. To bridge the gap, this paper presents the first comprehensive evaluation of the recently developed large vision language models (LVLMs) for chart understanding and reasoning tasks. Our evaluation includes a comprehensive assessment of LVLMs, including GPT-4V and Gemini, across four major chart reasoning tasks. Furthermore, we perform a qualitative evaluation of LVLMs' performance on a diverse range of charts, aiming to provide a thorough analysis of their strengths and weaknesses. Our findings reveal that LVLMs demonstrate impressive abilities in generating fluent texts covering high-level data insights while also encountering common problems like hallucinations, factual errors, and data bias. We highlight the key strengths and limitations of chart comprehension tasks, offering insights for future research.</li>
<li><strong>摘要：</strong>自然语言是数据可视化（例如条形图和折线图）的一种强大的补充交流方式。为了促进使用自然语言进行基于图表的推理，最近引入了各种下游任务，例如图表问答、图表摘要和使用图表进行事实核查。这些任务带来了独特的挑战，既需要视觉语言推理，又需要对图表数据表、视觉编码和自然语言提示有细致的理解。尽管大型语言模型 (LLM) 最近在各种 NLP 任务中取得了成功，但它们在数据可视化领域的能力和局限性仍未得到充分探索，这可能是因为它们缺乏多模态能力。为了弥补这一差距，本文首次对最近开发的用于图表理解和推理任务的大型视觉语言模型 (LVLM) 进行了全面评估。我们的评估包括对四项主要图表推理任务中的 LVLM（包括 GPT-4V 和 Gemini）的全面评估。此外，我们对 LVLM 在各种图表上的表现进行了定性评估，旨在对其优缺点进行全面分析。我们的研究结果表明，LVLM 在生成流畅的文本方面表现出色，涵盖了高级数据洞察，但同时也遇到了幻觉、事实错误和数据偏差等常见问题。我们强调了图表理解任务的主要优势和局限性，为未来的研究提供了见解。</li>
</ul>

<h3>Title: A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters</h3>
<ul>
<li><strong>Authors: </strong>Long Hei Matthew Lam, Ehsan Shareghi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00284">https://arxiv.org/abs/2406.00284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00284">https://arxiv.org/pdf/2406.00284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00284]] A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters(https://arxiv.org/abs/2406.00284)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Logical reasoning serves as a cornerstone for human cognition. Recently, the emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively. To improve this capability, recent studies have delved into integrating LLMs with various symbolic solvers using diverse techniques and methodologies. While some combinations excel on specific datasets, others fall short. However, it remains unclear whether the variance in performance stems from the methodologies employed or the specific symbolic solvers utilized. Therefore, there is a lack of consistent comparison between symbolic solvers and how they influence LLM's logical reasoning ability. We perform experiments on LLMs integrated with 3 symbolic solvers: Z3, Pyke, and Prover9, and compare their performance on 3 logical reasoning datasets: ProofWriter, PrOntoQA, and FOLIO. Our findings indicate that when combined with LLMs Pyke's performance is significantly inferior to that of Prover9 and Z3. Z3's overall accuracy performance slightly surpasses Prover9, but Prover9 could execute more questions.</li>
<li><strong>摘要：</strong>逻辑推理是人类认知的基石。最近，大型语言模型 (LLM) 的出现表明在有效解决逻辑推理任务方面取得了令人鼓舞的进展。为了提高这种能力，最近的研究深入研究了使用各种技术和方法将 LLM 与各种符号求解器集成。虽然有些组合在特定数据集上表现出色，但其他组合则表现不佳。然而，目前尚不清楚性能差异是源于所采用的方法还是所使用的特定符号求解器。因此，符号求解器之间缺乏一致的比较，以及它们如何影响 LLM 的逻辑推理能力。我们对集成了 3 个符号求解器的 LLM 进行了实验：Z3、Pyke 和 Prover9，并在 3 个逻辑推理数据集上比较了它们的性能：ProofWriter、PrOntoQA 和 FOLIO。我们的研究结果表明，与 LLM 结合使用时，Pyke 的性能明显低于 Prover9 和 Z3。Z3 的整体准确度性能略优于 Prover9，但 Prover9 可以执行更多问题。</li>
</ul>

<h3>Title: Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Millicent Ochieng, Varun Gumma, Sunayana Sitaram, Jindong Wang, Keshet Ronen, Kalika Bali, Jacki O'Neill</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00343">https://arxiv.org/abs/2406.00343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00343">https://arxiv.org/pdf/2406.00343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00343]] Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced, Low-Resource Real-World Scenarios(https://arxiv.org/abs/2406.00343)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The deployment of Large Language Models (LLMs) in real-world applications presents both opportunities and challenges, particularly in multilingual and code-mixed communication settings. This research evaluates the performance of seven leading LLMs in sentiment analysis on a dataset derived from multilingual and code-mixed WhatsApp chats, including Swahili, English and Sheng. Our evaluation includes both quantitative analysis using metrics like F1 score and qualitative assessment of LLMs' explanations for their predictions. We find that, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and other LLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled with understanding linguistic and contextual nuances, as well as lack of transparency in their decision-making process as observed from their explanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverse linguistic inputs and managing various contextual information, demonstrating high consistency with human alignment and transparency in their decision-making process. The LLMs however, encountered difficulties in incorporating cultural nuance especially in non-English settings with GPT-4s doing so inconsistently. The findings emphasize the necessity of continuous improvement of LLMs to effectively tackle the challenges of culturally nuanced, low-resource real-world settings.</li>
<li><strong>摘要：</strong>在实际应用中部署大型语言模型 (LLM) 既带来了机遇，也带来了挑战，尤其是在多语言和混合代码的通信环境中。这项研究评估了七种领先的 LLM 在情绪分析方面的表现，这些 LLM 的数据来自多语言和混合代码的 WhatsApp 聊天，包括斯瓦希里语、英语和 Sheng。我们的评估包括使用 F1 分数等指标的定量分析和对 LLM 对其预测的解释的定性评估。我们发现，虽然 Mistral-7b 和 Mixtral-8x7b 获得了较高的 F1 分数，但它们和其他 LLM（如 GPT-3.5-Turbo、Llama-2-70b 和 Gemma-7b）在理解语言和语境细微差别方面存在困难，而且从它们的解释中可以看出，它们的决策过程缺乏透明度。相比之下，GPT-4 和 GPT-4-Turbo 在掌握各种语言输入和管理各种上下文信息方面表现出色，在决策过程中表现出与人类高度一致和透明度。然而，法学硕士在融入文化细微差别方面遇到了困难，尤其是在非英语环境中，而 GPT-4 在这方面表现不一致。研究结果强调，法学硕士必须不断改进，以有效应对文化细微差别、资源匮乏的现实环境的挑战。</li>
</ul>

<h3>Title: The Best of Both Worlds: Toward an Honest and Helpful Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Chujie Gao, Qihui Zhang, Dongping Chen, Yue Huang, Siyuan Wu, Zhengyan Fu, Yao Wan, Xiangliang Zhang, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00380">https://arxiv.org/abs/2406.00380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00380">https://arxiv.org/pdf/2406.00380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00380]] The Best of Both Worlds: Toward an Honest and Helpful Large Language Model(https://arxiv.org/abs/2406.00380)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success across various industries due to their exceptional generative capabilities. However, for safe and effective real-world deployments, ensuring honesty and helpfulness is critical. This paper addresses the question: Can we prioritize the helpfulness of LLMs while preserving their honesty? To begin with, we establish exhaustive principles aimed at guaranteeing the honesty of LLM. Additionally, we introduce a novel dataset, referred to as HoneSet, comprising 930 queries spanning six categories meticulously crafted to assess an LLM's capacity for maintaining honesty. Subsequently, we present two approaches to augmenting honesty and helpfulness in LLMs: a training-free enhancement and a fine-tuning-based improvement. The training-free approach, which is based on curiosity-driven prompting, empowers LLMs to articulate internal confusion and uncertainty regarding queries, thereby optimizing their responses. Conversely, the fine-tuning-based method employs a two-stage process inspired by curriculum learning: initially instructing LLMs to discern between honest and dishonest responses, then refining their training to enhance helpfulness. Experiments conducted on nine prominent LLMs demonstrate a significant improvement in alignment with honesty across all models through the implementation of our proposed enhancements. Particularly noteworthy is the 65.3% enhancement observed in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as measured by the H$^{2}$ (honest and helpful) assessment. We believe that our work can pave the way for developing more trustworthy LLMs for real-world applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 凭借其卓越的生成能力在各个行业取得了显著的成功。然而，为了安全有效地在现实世界中部署，确保诚实和乐于助人至关重要。本文探讨了这样一个问题：我们能否在保持 LLM 诚实的同时优先考虑其乐于助人？首先，我们建立了旨在保证 LLM 诚实的详尽原则。此外，我们引入了一个新数据集，称为 HoneSet，其中包含 930 个查询，涵盖六个类别，经过精心设计，可评估 LLM 保持诚实的能力。随后，我们提出了两种增强 LLM 诚实和乐于助人的方法：无需训练的增强和基于微调的改进。无需训练的方法基于好奇心驱动的提示，使 LLM 能够表达有关查询的内部困惑和不确定性，从而优化其响应。相反，基于微调的方法采用了受课程学习启发的两阶段过程：首先指导 LLM 辨别诚实和不诚实的回答，然后改进训练以提高帮助性。对九个著名 LLM 进行的实验表明，通过实施我们提出的增强措施，所有模型的诚实一致性都有显著改善。特别值得注意的是，根据 H$^{2}$（诚实和乐于助人）评估，Llama3-8b 中观察到 65.3% 的增强，Mistral-7b 中观察到 124.7% 的显著改进。我们相信，我们的工作可以为开发更值得信赖的 LLM 以用于实际应用铺平道路。</li>
</ul>

<h3>Title: Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Shichao Sun, Ruifeng Yuan, Ziqiang Cao, Wenjie Li, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00507">https://arxiv.org/abs/2406.00507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00507">https://arxiv.org/pdf/2406.00507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00507]] Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization(https://arxiv.org/abs/2406.00507)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已证明能够通过从初稿开始模仿人类的迭代批评和改进过程来提高摘要质量。设计了两种策略来执行此迭代过程：提示链和逐步提示。提示链通过一系列三个离散提示来协调起草、批评和改进阶段，而逐步提示将这些阶段集成在单个提示中。然而，这两种方法的相对有效性尚未得到广泛研究。本文致力于在文本摘要的背景下检查和比较这两种方法，以确定哪种方法最有效。实验结果表明，提示链方法可以产生更有利的结果。这可能是因为根据我们的各种实验，逐步提示可能会产生模拟的改进过程。由于改进适用于各种任务，我们的结论有可能推广到其他应用程序，从而提供可能有助于 LLM 更广泛发展的见解。</li>
</ul>

<h3>Title: A Survey on Large Language Models for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00515">https://arxiv.org/abs/2406.00515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00515">https://arxiv.org/pdf/2406.00515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00515]] A Survey on Large Language Models for Code Generation(https://arxiv.org/abs/2406.00515)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource website (this https URL) to continuously document and disseminate the most recent advances in the field.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种与代码相关的任务（称为代码 LLM）中取得了显著的进步，特别是在使用 LLM 从自然语言描述生成源代码的代码生成方面。这个新兴领域因其在软件开发中的实际意义而引起了学术研究人员和行业专业人士的极大兴趣，例如 GitHub Copilot。尽管人们积极探索将 LLM 用于各种代码任务，无论是从自然语言处理 (NLP) 还是软件工程 (SE) 的角度，还是两者兼而有之，但明显缺乏专门针对代码生成 LLM 的全面和最新的文献综述。在这篇综述中，我们旨在通过提供系统的文献综述来弥补这一差距，该综述为研究代码生成 LLM 前沿进展的研究人员提供了有价值的参考。我们引入了一个分类法来对代码生成 LLM 的最新发展进行分类和讨论，涵盖数据管理、最新进展、性能评估和实际应用等方面。此外，我们概述了 LLM 代码生成的发展历史，并使用广受认可的 HumanEval 和 MBPP 基准进行了实证比较，以突出 LLM 代码生成功能的逐步增强。我们确定了学术界和实践发展之间差距的关键挑战和有希望的机会。此外，我们还建立了一个专门的资源网站（此 https URL），以持续记录和传播该领域的最新进展。</li>
</ul>

<h3>Title: Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming</h3>
<ul>
<li><strong>Authors: </strong>Phoebe J. Wang, Max Kreminski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00554">https://arxiv.org/abs/2406.00554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00554">https://arxiv.org/pdf/2406.00554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00554]] Guiding and Diversifying LLM-Based Story Generation via Answer Set Programming(https://arxiv.org/abs/2406.00554)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction-tuned large language models (LLMs) are capable of generating stories in response to open-ended user requests, but the resulting stories tend to be limited in their diversity. Older, symbolic approaches to story generation (such as planning) can generate substantially more diverse plot outlines, but are limited to producing stories that recombine a fixed set of hand-engineered character action templates. Can we combine the strengths of these approaches while mitigating their weaknesses? We propose to do so by using a higher-level and more abstract symbolic specification of high-level story structure -- implemented via answer set programming (ASP) -- to guide and diversify LLM-based story generation. Via semantic similarity analysis, we demonstrate that our approach produces more diverse stories than an unguided LLM, and via code excerpts, we demonstrate the improved compactness and flexibility of ASP-based outline generation over full-fledged narrative planning.</li>
<li><strong>摘要：</strong>指令调整的大型语言模型 (LLM) 能够根据开放式用户请求生成故事，但生成的故事往往在多样性方面受到限制。较旧的符号故事生成方法（例如规划）可以生成更加多样化的情节大纲，但仅限于生成重新组合一组固定的手工设计角色动作模板的故事。我们能否结合这些方法的优势，同时减轻它们的弱点？我们建议通过使用更高级、更抽象的高级故事结构符号规范（通过答案集编程 (ASP) 实现）来指导和多样化基于 LLM 的故事生成。通过语义相似性分析，我们证明我们的方法比无指导的 LLM 生成了更加多样化的故事，并且通过代码摘录，我们证明了基于 ASP 的大纲生成比成熟的叙事规划具有更好的紧凑性和灵活性。</li>
</ul>

<h3>Title: SPAGHETTI: Open-Domain Question Answering from Heterogeneous Data Sources with Retrieval and Semantic Parsing</h3>
<ul>
<li><strong>Authors: </strong>Heidi C. Zhang, Sina J. Semnani, Farhad Ghassemi, Jialiang Xu, Shicheng Liu, Monica S. Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00562">https://arxiv.org/abs/2406.00562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00562">https://arxiv.org/pdf/2406.00562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00562]] SPAGHETTI: Open-Domain Question Answering from Heterogeneous Data Sources with Retrieval and Semantic Parsing(https://arxiv.org/abs/2406.00562)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We introduce SPAGHETTI: Semantic Parsing Augmented Generation for Hybrid English information from Text Tables and Infoboxes, a hybrid question-answering (QA) pipeline that utilizes information from heterogeneous knowledge sources, including knowledge base, text, tables, and infoboxes. Our LLM-augmented approach achieves state-of-the-art performance on the Compmix dataset, the most comprehensive heterogeneous open-domain QA dataset, with 56.5% exact match (EM) rate. More importantly, manual analysis on a sample of the dataset suggests that SPAGHETTI is more than 90% accurate, indicating that EM is no longer suitable for assessing the capabilities of QA systems today.</li>
<li><strong>摘要：</strong>我们推出了 SPAGHETTI：基于文本表和信息框的混合英语信息的语义解析增强生成，这是一种混合问答 (QA) 流程，利用来自异构知识源（包括知识库、文本、表格和信息框）的信息。我们的 LLM 增强方法在最全面的异构开放域 QA 数据集 Compmix 数据集上实现了最先进的性能，精确匹配 (EM) 率为 56.5%。更重要的是，对数据集样本的手动分析表明 SPAGHETTI 的准确率超过 90%，这表明 EM 不再适合评估当今 QA 系统的能力。</li>
</ul>

<h3>Title: LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo Zhu, Yimeng Gan, Rui Hu, Shuicheng Yan, Han Fang, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00605">https://arxiv.org/abs/2406.00605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00605">https://arxiv.org/pdf/2406.00605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00605]] LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models(https://arxiv.org/abs/2406.00605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce LongSkywork, a long-context Large Language Model (LLM) capable of processing up to 200,000 tokens. We provide a training recipe for efficiently extending context length of LLMs. We identify that the critical element in enhancing long-context processing capability is to incorporate a long-context SFT stage following the standard SFT stage. A mere 200 iterations can convert the standard SFT model into a long-context model. To reduce the effort in collecting and annotating data for long-context language modeling, we develop two novel methods for creating synthetic data. These methods are applied during the continual pretraining phase as well as the Supervised Fine-Tuning (SFT) phase, greatly enhancing the training efficiency of our long-context LLMs. Our findings suggest that synthetic long-context SFT data can surpass the performance of data curated by humans to some extent. LongSkywork achieves outstanding performance on a variety of long-context benchmarks. In the Needle test, a benchmark for long-context information retrieval, our models achieved perfect accuracy across multiple context spans. Moreover, in realistic application scenarios, LongSkywork-13B demonstrates performance on par with Claude2.1, the leading long-context model, underscoring the effectiveness of our proposed methods.</li>
<li><strong>摘要：</strong>我们推出了 LongSkywork，这是一种能够处理多达 200,000 个标记的长上下文大型语言模型 (LLM)。我们提供了一种有效扩展 LLM 上下文长度的训练方法。我们发现，增强长上下文处理能力的关键要素是在标准 SFT 阶段之后加入长上下文 SFT 阶段。仅需 200 次迭代即可将标准 SFT 模型转换为长上下文模型。为了减少为长上下文语言建模收集和注释数据的工作量，我们开发了两种用于创建合成数据的新方法。这些方法应用于持续预训练阶段以及监督微调 (SFT) 阶段，大大提高了我们的长上下文 LLM 的训练效率。我们的研究结果表明，合成的长上下文 SFT 数据在某种程度上可以超越人类策划的数据的性能。LongSkywork 在各种长上下文基准上都取得了出色的表现。在长上下文信息检索基准 Needle 测试中，我们的模型在多个上下文跨度上都取得了完美的准确率。此外，在实际应用场景中，LongSkywork-13B 的表现与领先的长上下文模型 Claude2.1 相当，证明了我们提出的方法的有效性。</li>
</ul>

<h3>Title: LLMs Could Autonomously Learn Without External Supervision</h3>
<ul>
<li><strong>Authors: </strong>Ke Ji, Junying Chen, Anningzhe Gao, Wenya Xie, Xiang Wan, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00606">https://arxiv.org/abs/2406.00606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00606">https://arxiv.org/pdf/2406.00606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00606]] LLMs Could Autonomously Learn Without External Supervision(https://arxiv.org/abs/2406.00606)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the quest for super-human performance, Large Language Models (LLMs) have traditionally been tethered to human-annotated datasets and predefined training objectives-a process that is both labor-intensive and inherently limited. This paper presents a transformative approach: Autonomous Learning for LLMs, a self-sufficient learning paradigm that frees models from the constraints of human supervision. This method endows LLMs with the ability to self-educate through direct interaction with text, akin to a human reading and comprehending literature. Our approach eliminates the reliance on annotated data, fostering an Autonomous Learning environment where the model independently identifies and reinforces its knowledge gaps. Empirical results from our comprehensive experiments, which utilized a diverse array of learning materials and were evaluated against standard public quizzes, reveal that Autonomous Learning outstrips the performance of both Pre-training and Supervised Fine-Tuning (SFT), as well as retrieval-augmented methods. These findings underscore the potential of Autonomous Learning to not only enhance the efficiency and effectiveness of LLM training but also to pave the way for the development of more advanced, self-reliant AI systems.</li>
<li><strong>摘要：</strong>为了追求超越人类的表现，大型语言模型 (LLM) 传统上一直与人工注释的数据集和预定义的训练目标相联系，这一过程既耗费人力，又具有内在局限性。本文提出了一种变革性方法：LLM 的自主学习，这是一种自给自足的学习范式，可将模型从人工监督的束缚中解放出来。这种方法使 LLM 能够通过与文本的直接交互进行自我教育，类似于人类阅读和理解文学。我们的方法消除了对注释数据的依赖，促进了自主学习环境，模型可以独立识别和强化其知识差距。我们进行了全面的实验，利用了各种各样的学习材料，并根据标准公开测验进行了评估，从中得出的实证结果表明，自主学习的表现优于预训练和监督微调 (SFT) 以及检索增强方法。这些发现强调了自主学习的潜力，不仅可以提高法学硕士培训的效率和有效性，而且还可以为开发更先进、自力更生的人工智能系统铺平道路。</li>
</ul>

<h3>Title: Prompt Framework for Role-playing: Generation and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xun Liu, Zhengwei Ni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00627">https://arxiv.org/abs/2406.00627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00627">https://arxiv.org/pdf/2406.00627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00627]] Prompt Framework for Role-playing: Generation and Evaluation(https://arxiv.org/abs/2406.00627)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) have demonstrated remarkable abilities in generating natural language, understanding user instruction, and mimicking human language use. These capabilities have garnered considerable interest in applications such as role-playing. However, the process of collecting individual role scripts (or profiles) data and manually evaluating the performance can be costly. We introduce a framework that uses prompts to leverage the state-of-the-art (SOTA) LLMs to construct role-playing dialogue datasets and evaluate the role-playing performance. Additionally, we employ recall-oriented evaluation Rouge-L metric to support the result of the LLM evaluator.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在生成自然语言、理解用户指令和模仿人类语言使用方面表现出了卓越的能力。这些能力在角色扮演等应用中引起了极大的兴趣。然而，收集单个角色脚本（或配置文件）数据并手动评估性能的过程可能成本高昂。我们引入了一个框架，该框架使用提示来利用最先进的 (SOTA) LLM 来构建角色扮演对话数据集并评估角色扮演性能。此外，我们采用面向回忆的评估 Rouge-L 指标来支持 LLM 评估器的结果。</li>
</ul>

<h3>Title: Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Garrett Crumrine, Izzat Alsmadi, Jesus Guerrero, Yuvaraj Munian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00628">https://arxiv.org/abs/2406.00628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00628">https://arxiv.org/pdf/2406.00628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00628]] Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models(https://arxiv.org/abs/2406.00628)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized how we interact with machines. However, this technological advancement has been paralleled by the emergence of "Mallas," malicious services operating underground that exploit LLMs for nefarious purposes. Such services create malware, phishing attacks, and deceptive websites, escalating the cyber security threats landscape. This paper delves into the proliferation of Mallas by examining the use of various pre-trained language models and their efficiency and vulnerabilities when misused. Building on a dataset from the Common Vulnerabilities and Exposures (CVE) program, it explores fine-tuning methodologies to generate code and explanatory text related to identified vulnerabilities. This research aims to shed light on the operational strategies and exploitation techniques of Mallas, leading to the development of more secure and trustworthy AI applications. The paper concludes by emphasizing the need for further research, enhanced safeguards, and ethical guidelines to mitigate the risks associated with the malicious application of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了我们与机器的交互方式。然而，这项技术进步与“Mallas”的出现相伴而生，Mallas 是一种地下运行的恶意服务，利用 LLM 进行邪恶目的。此类服务会产生恶意软件、网络钓鱼攻击和欺骗性网站，加剧网络安全威胁形势。本文通过研究各种预训练语言模型的使用及其效率和滥用时的漏洞，深入研究了 Mallas 的扩散。基于通用漏洞和暴露 (CVE) 计划的数据集，本文探索了微调方法来生成与已识别漏洞相关的代码和解释性文本。这项研究旨在揭示 Mallas 的运营策略和利用技术，从而开发更安全、更可靠的 AI 应用程序。本文最后强调需要进一步研究、加强保障措施和道德准则，以减轻与 LLM 的恶意应用相关的风险。</li>
</ul>

<h3>Title: Presence or Absence: Are Unknown Word Usages in Dictionaries?</h3>
<ul>
<li><strong>Authors: </strong>Xianghe Ma, Dominik Schlechtweg, Wei Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00656">https://arxiv.org/abs/2406.00656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00656">https://arxiv.org/pdf/2406.00656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00656]] Presence or Absence: Are Unknown Word Usages in Dictionaries?(https://arxiv.org/abs/2406.00656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In this work, we outline the components and results of our system submitted to the AXOLOTL-24 shared task for Finnish, Russian and German languages. Our system is fully unsupervised. It leverages a graph-based clustering approach to predict mappings between unknown word usages and dictionary entries for Subtask 1, and generates dictionary-like definitions for those novel word usages through the state-of-the-art Large Language Models such as GPT-4 and LLaMA-3 for Subtask 2. In Subtask 1, our system outperforms the baseline system by a large margin, and it offers interpretability for the mapping results by distinguishing between matched and unmatched (novel) word usages through our graph-based clustering approach. Our system ranks first in Finnish and German, and ranks second in Russian on the Subtask 2 test-phase leaderboard. These results show the potential of our system in managing dictionary entries, particularly for updating dictionaries to include novel sense entries. Our code and data are made publicly available\footnote{\url{this https URL}}.</li>
<li><strong>摘要：</strong>在这项工作中，我们概述了我们提交给芬兰语、俄语和德语的 AXOLOTL-24 共享任务的系统的组件和结果。我们的系统完全是无人监督的。它利用基于图的聚类方法来预测子任务 1 中未知单词用法和词典条目之间的映射，并通过最先进的大型语言模型（如 GPT-4 和 LLaMA-3）为子任务 2 中的这些新单词用法生成类似词典的定义。在子任务 1 中，我们的系统的表现远远优于基线系统，并且它通过基于图的聚类方法区分匹配和不匹配（新）单词用法，为映射结果提供了可解释性。我们的系统在子任务 2 测试阶段排行榜上芬兰语和德语排名第一，俄语排名第二。这些结果显示了我们的系统在管理词典条目方面的潜力，特别是在更新词典以包含新意义条目方面。我们的代码和数据已公开提供\footnote{\url{此 https URL}}。</li>
</ul>

<h3>Title: Topic Modeling for Short Texts with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tomoki Doi, Masaru Isonuma, Hitomi Yanaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Topic Modeling for Short Texts with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As conventional topic models rely on word co-occurrence to infer latent topics, topic modeling for short texts has been a long-standing challenge. Large Language Models (LLMs) can potentially overcome this challenge by contextually learning the semantics of words via pretraining. This paper studies two approaches, parallel prompting and sequential prompting, to use LLMs for topic modeling. Due to the input length limitations, LLMs cannot process many texts at once. By splitting the texts into smaller subsets and processing them parallelly or sequentially, an arbitrary number of texts can be handled by LLMs. Experimental results demonstrated that our methods can identify more coherent topics than existing ones while maintaining the diversity of the induced topics. Furthermore, we found that the inferred topics adequately covered the input texts, while hallucinated topics were hardly generated.</li>
<li><strong>摘要：</strong>由于传统主题模型依靠词语共现来推断潜在主题，因此短文本的主题建模一直是一个长期存在的挑战。大型语言模型 (LLM) 可以通过预训练在上下文中学习单词的语义，从而潜在地克服这一挑战。本文研究了使用 LLM 进行主题建模的两种方法，即并行提示和顺序提示。由于输入长度限制，LLM 无法一次处理大量文本。通过将文本拆分为更小的子集并并行或顺序处理它们，LLM 可以处理任意数量的文本。实验结果表明，我们的方法可以识别出比现有主题更连贯的主题，同时保持诱导主题的多样性。此外，我们发现推断的主题充分覆盖了输入文本，而几乎不会生成幻觉主题。</li>
</ul>

<h3>Title: How well do distributed representations convey contextual lexical semantics: a Thesis Proposal</h3>
<ul>
<li><strong>Authors: </strong>Zhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00751">https://arxiv.org/abs/2406.00751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00751">https://arxiv.org/pdf/2406.00751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00751]] How well do distributed representations convey contextual lexical semantics: a Thesis Proposal(https://arxiv.org/abs/2406.00751)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Modern neural networks (NNs), trained on extensive raw sentence data, construct distributed representations by compressing individual words into dense, continuous, high-dimensional vectors. These representations are specifically designed to capture the varied meanings, including ambiguity, of word occurrences within context. In this thesis, our objective is to examine the efficacy of distributed representations from NNs in encoding lexical meaning. Initially, we identify four sources of ambiguity - homonymy, polysemy, semantic roles, and multifunctionality - based on the relatedness and similarity of meanings influenced by context. Subsequently, we aim to evaluate these sources by collecting or constructing multilingual datasets, leveraging various language models, and employing linguistic analysis tools.</li>
<li><strong>摘要：</strong>现代神经网络 (NN) 经过大量原始句子数据训练，通过将单个单词压缩为密集、连续、高维向量来构建分布式表示。这些表示专门用于捕捉上下文中单词出现的各种含义（包括歧义）。在本文中，我们的目标是检验 NN 的分布式表示在编码词汇含义方面的有效性。首先，我们根据受上下文影响的含义的相关性和相似性确定了四个歧义来源 - 同音异义、多义性、语义角色和多功能性。随后，我们旨在通过收集或构建多语言数据集、利用各种语言模型和使用语言分析工具来评估这些来源。</li>
</ul>

<h3>Title: Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, Fuli Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00755">https://arxiv.org/abs/2406.00755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00755">https://arxiv.org/pdf/2406.00755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00755]] Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction(https://arxiv.org/abs/2406.00755)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eleven representative LLMs. Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9\%. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs. Our code and dataset is available on this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在数学推理领域的快速发展需要全面的评估来衡量进展并启发未来的方向。现有的评估主要侧重于从考生的角度解决问题，忽略了考官在错误识别和纠正方面的双重视角。从考官的角度来看，我们定义了四个错误识别和纠正评估任务，以及一个带有注释错误类型和步骤的新数据集。我们还设计了不同的提示来彻底评估 11 个代表性的 LLM。我们的主要发现表明，GPT-4 优于所有模型，而开源模型 LLaMA-2-7B 表现出与闭源模型 GPT-3.5 和 Gemini Pro 相当的能力。值得注意的是，计算错误是最具挑战性的错误类型。此外，用错误类型提示 LLM 可以将平均纠正准确率提高 47.9%。这些结果揭示了发展 LLM 数学推理能力的潜在方向。我们的代码和数据集可在此 https URL 上找到。</li>
</ul>

<h3>Title: Automatic Instruction Evolving for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, Weizhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00770">https://arxiv.org/abs/2406.00770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00770">https://arxiv.org/pdf/2406.00770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00770]] Automatic Instruction Evolving for Large Language Models(https://arxiv.org/abs/2406.00770)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval.</li>
<li><strong>摘要：</strong>使用 Evol-Instruct 对大型预训练语言模型进行微调已在各种任务中取得了令人鼓舞的成果。然而，设计有效的指令演化方法需要大量的人类专业知识。本文提出了 Auto Evol-Instruct，这是一个端到端框架，它使用大型语言模型演化指令数据集，无需任何人工干预。该框架会自动分析和总结适合给定指令数据的演化策略，并根据指令演化过程中暴露的问题迭代改进演化方法。我们进行了大量的实验，结果表明，Auto Evol-Instruct 优化的最佳方法在各种基准测试（包括 MT-Bench、AlpacaEval、GSM8K 和 HumanEval）上的表现均优于人工设计的方法。</li>
</ul>

<h3>Title: BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling</h3>
<ul>
<li><strong>Authors: </strong>Lin Gui, Cristina Gârbacea, Victor Veitch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00832">https://arxiv.org/abs/2406.00832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00832">https://arxiv.org/pdf/2406.00832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00832]] BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling(https://arxiv.org/abs/2406.00832)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper concerns the problem of aligning samples from large language models to human preferences using best-of-$n$ sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and approaches to alignment that train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)? To answer this, we embed both the best-of-$n$ distribution and the sampling distributions learned by alignment procedures in a common class of tiltings of the base LLM distribution. We then show that, within this class, best-of-$n$ is essentially optimal in terms of the trade-off between win-rate against the base model vs KL distance from the base model. That is, best-of-$n$ is the best choice of alignment distribution if the goal is to maximize win rate. However, best-of-$n$ requires drawing $n$ samples for each inference, a substantial cost. To avoid this, the second problem we consider is how to fine-tune a LLM to mimic the best-of-$n$ sampling distribution. We derive BoNBoN Alignment to achieve this by exploiting the special structure of the best-of-$n$ distribution. Experiments show that BoNBoN alignment yields substantial improvements in producing a model that is preferred to the base policy while minimally affecting off-target aspects.</li>
<li><strong>摘要：</strong>本文涉及使用 best-of-n 采样将大型语言模型中的样本与人类偏好对齐的问题，其中我们抽取 $n$ 个样本，对它们进行排序，然后返回最佳样本。我们考虑两个基本问题。首先：best-of-n 与训练 LLM 以输出具有高预期奖励的样本的对齐方法（例如 RLHF 或 DPO）之间的关系是什么？为了回答这个问题，我们将 best-of-n 分布和通过对齐程序学习的采样分布嵌入到基本 LLM 分布的共同倾斜类中。然后我们表明，在这个类中，best-of-n 在与基本模型的胜率与与基本模型的 KL 距离之间的权衡方面本质上是最佳的。也就是说，如果目标是最大化胜率，best-of-n 是对齐分布的最佳选择。但是，best-of-n 需要为每次推理抽取 $n$ 个样本，这需要相当大的成本。为了避免这种情况，我们考虑的第二个问题是如何微调 LLM 以模仿最佳 $n$ 采样分布。我们通过利用最佳 $n$ 分布的特殊结构来推导 BoNBoN 对齐以实现此目的。实验表明，BoNBoN 对齐在生成优于基础策略的模型方面取得了显着的改进，同时将对偏离目标方面的影响降至最低。</li>
</ul>

<h3>Title: FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaixin Lan, Tao Fang, Derek F. Wong, Yabo Xu, Lidia S. Chao, Cecilia G. Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00839">https://arxiv.org/abs/2406.00839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00839">https://arxiv.org/pdf/2406.00839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00839]] FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models(https://arxiv.org/abs/2406.00839)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Pre-trained Language Models (PLMs) have shown impressive results in various Natural Language Generation (NLG) tasks, such as powering chatbots and generating stories. However, an ethical concern arises due to their potential to produce verbatim copies of paragraphs from their training data. This is problematic as PLMs are trained on corpora constructed by human authors. As such, there is a pressing need for research to promote the generation of original content by these models. In this study, we introduce a unique "self-plagiarism" contrastive decoding strategy, aimed at boosting the originality of text produced by PLMs. Our method entails modifying prompts in LLMs to develop an amateur model and a professional model. Specifically, the amateur model is urged to plagiarize using three plagiarism templates we have designed, while the professional model maintains its standard language model status. This strategy employs prompts to stimulate the model's capacity to identify non-original candidate token combinations and subsequently impose penalties. The application of this strategy is integrated prior to the model's final layer, ensuring smooth integration with most existing PLMs (T5, GPT, LLaMA) without necessitating further adjustments. Implementing our strategy, we observe a significant decline in non-original sequences comprised of more than three words in the academic AASC dataset and the story-based ROCStories dataset.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 在各种自然语言生成 (NLG) 任务中表现出色，例如为聊天机器人提供动力和生成故事。然而，由于它们有可能从训练数据中生成段落的逐字副本，因此出现了道德问题。这是有问题的，因为 PLM 是在人类作者构建的语料库上进行训练的。因此，迫切需要进行研究以促进这些模型生成原创内容。在本研究中，我们引入了一种独特的“自我剽窃”对比解码策略，旨在提高 PLM 生成的文本的原创性。我们的方法需要修改 LLM 中的提示以开发业余模型和专业模型。具体来说，业余模型被鼓励使用我们设计的三个剽窃模板进行剽窃，而专业模型则保持其标准语言模型状态。该策略使用提示来刺激模型识别非原创候选标记组合并随后施加惩罚的能力。此策略的应用在模型的最后一层之前集成，确保与大多数现有 PLM（T5、GPT、LLaMA）顺利集成，而无需进一步调整。实施我们的策略后，我们观察到学术 AASC 数据集和基于故事的 ROCStories 数据集中由三个以上单词组成的非原创序列显著减少。</li>
</ul>

<h3>Title: Show, Don't Tell: Aligning Language Models with Demonstrated Feedback</h3>
<ul>
<li><strong>Authors: </strong>Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Michael Bernstein, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00888">https://arxiv.org/abs/2406.00888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00888">https://arxiv.org/pdf/2406.00888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00888]] Show, Don't Tell: Aligning Language Models with Demonstrated Feedback(https://arxiv.org/abs/2406.00888)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number ($<10$) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants ($N=16$). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.</li>
<li><strong>摘要：</strong>语言模型经过调整以模仿许多人的集体声音，从而产生与任何特定对象都不一致的输出。通过监督微调或 RLHF 可以使 LLM 远离通用输出，但对于新的临时任务，需要非常大的数据集。我们认为，相反，可以通过利用非常少量（<10 美元）的演示作为反馈，将 LLM 与特定设置对齐。我们的方法，演示迭代任务优化 (DITTO)，直接将语言模型输出与用户的演示行为对齐。DITTO 源自在线模仿学习的思想，通过将用户的演示视为优于 LLM 及其中间检查点的输出，以低成本生成在线比较数据。我们评估了 DITTO 在新闻文章、电子邮件和博客文章等领域学习细粒度风格和任务对齐的能力。此外，我们还进行了一项用户研究，征求参与者的一系列演示（N=16 美元）。在我们的基准测试和用户研究中，我们发现 DITTO 的胜率比小样本提示、监督微调和其他自玩方法平均高出 19%。通过直接使用演示作为反馈，DITTO 提供了一种有效定制 LLM 的新方法。</li>
</ul>

<h3>Title: MEDIQ: Question-Asking LLMs for Adaptive and Reliable Medical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] MEDIQ: Question-Asking LLMs for Adaptive and Reliable Medical Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In high-stakes domains like medical reasoning, AI assistants powered by large language models (LLMs) are yet to be reliable and safe. We identify a key obstacle towards reliability: existing LLMs are trained to answer any question, even with incomplete context in the prompt or insufficient parametric knowledge. We propose to change this paradigm to develop more careful LLMs that ask follow-up questions to gather necessary and sufficient information and respond reliably. We introduce MEDIQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System. The Patient may provide incomplete information in the beginning; the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details from the Patient via follow-up questions. To evaluate MEDIQ, we convert MEDQA and CRAFT-MD -- medical benchmarks for diagnostic question answering -- into an interactive setup. We develop a reliable Patient system and prototype several Expert systems, first showing that directly prompting state-of-the-art LLMs to ask questions degrades the quality of clinical reasoning, indicating that adapting LLMs to interactive information-seeking settings is nontrivial. We then augment the Expert with a novel abstention module to better estimate model confidence and decide whether to ask more questions, thereby improving diagnostic accuracy by 20.3%; however, performance still lags compared to an (unrealistic in practice) upper bound when full information is given upfront. Further analyses reveal that interactive performance can be improved by filtering irrelevant contexts and reformatting conversations. Overall, our paper introduces a novel problem towards LLM reliability, a novel MEDIQ framework, and highlights important future directions to extend the information-seeking abilities of LLM assistants in critical domains.</li>
<li><strong>摘要：</strong>在医学推理等高风险领域，由大型语言模型 (LLM) 驱动的 AI 助手尚不可靠且安全。我们发现了可靠性的一个关键障碍：现有的 LLM 经过训练可以回答任何问题，即使提示中的上下文不完整或参数知识不足。我们建议改变这种模式，开发更谨慎的 LLM，这些 LLM 会提出后续问题来收集必要和充分的信息并做出可靠的反应。我们引入了 MEDIQ，这是一个模拟真实临床互动的框架，它结合了患者系统和自适应专家系统。患者可能一开始提供的信息不完整；专家在没有信心时会避免做出诊断决定，而是通过后续问题从患者那里引出缺失的细节。为了评估 MEDIQ，我们将 MEDQA 和 CRAFT-MD（诊断问答的医学基准）转换为交互式设置。我们开发了一个可靠的患者系统并制作了几个专家系统的原型，首先表明直接提示最先进的 LLM 提出问题会降低临床推理的质量，这表明将 LLM 适应交互式信息搜索设置并非易事。然后，我们用一个新的弃权模块增强了专家，以更好地估计模型置信度并决定是否提出更多问题，从而将诊断准确率提高 20.3%；然而，当预先提供完整信息时，性能仍然落后于（在实践中不切实际的）上限。进一步的分析表明，可以通过过滤不相关的上下文和重新格式化对话来提高交互式性能。总的来说，我们的论文介绍了一个针对 LLM 可靠性的新问题、一个新颖的 MEDIQ 框架，并强调了未来在关键领域扩展 LLM 助手信息搜索能力的重要方向。</li>
</ul>

<h3>Title: A Survey of Useful LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00936">https://arxiv.org/abs/2406.00936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00936">https://arxiv.org/pdf/2406.00936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00936]] A Survey of Useful LLM Evaluation(https://arxiv.org/abs/2406.00936)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>LLMs have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks. Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake. Our study mainly discussed how LLMs, as useful tools, should be effectively assessed. We proposed the two-stage framework: from ``core ability'' to ``agent'', clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts. After confirming LLMs possess core ability, they can solve real-world and complex tasks as agent. In the "core ability" stage, we discussed the reasoning ability, societal impact, and domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied action, planning, and tool learning of LLMs agent applications. Finally, we examined the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.</li>
<li><strong>摘要：</strong>LLM 因其在各种复杂任务上的出色表现而受到各个研究领域的关注。因此，需要完善方法来评估 LLM 的能力，以确定其应承担的任务和责任。我们的研究主要讨论了如何有效地评估 LLM 作为有用的工具。我们提出了从“核心能力”到“代理”的两阶段框架，清楚地解释了如何根据 LLM 的具体能力应用 LLM，以及每个阶段的评估方法。核心能力是指 LLM 生成高质量自然语言文本所需的能力。在确认 LLM 具备核心能力后，它们可以作为代理解决现实世界和复杂的任务。在“核心能力”阶段，我们讨论了 LLM 的推理能力、社会影响和领域知识。在“代理”阶段，我们展示了 LLM 代理应用的具体行动、规划和工具学习。最后，我们研究了 LLM 评估方法目前面临的挑战以及未来的发展方向。</li>
</ul>

<h3>Title: Unveil the Duality of Retrieval-Augmented Generation: Theoretical Analysis and Practical Solution</h3>
<ul>
<li><strong>Authors: </strong>Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00944">https://arxiv.org/abs/2406.00944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00944">https://arxiv.org/pdf/2406.00944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00944]] Unveil the Duality of Retrieval-Augmented Generation: Theoretical Analysis and Practical Solution(https://arxiv.org/abs/2406.00944)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large language models (LLMs). However, studies show that RAG is not consistently effective and can even mislead LLMs due to noisy or incorrect retrieved texts. This suggests that RAG possesses a duality including both benefit and detriment. Although many existing methods attempt to address this issue, they lack a theoretical explanation for the duality in RAG. The benefit and detriment within this duality remain a black box that cannot be quantified or compared in an explainable manner. This paper takes the first step in theoretically giving the essential explanation of benefit and detriment in RAG by: (1) decoupling and formalizing them from RAG prediction, (2) approximating the gap between their values by representation similarity and (3) establishing the trade-off mechanism between them, to make them explainable, quantifiable, and comparable. We demonstrate that the distribution difference between retrieved texts and LLMs' knowledge acts as double-edged sword, bringing both benefit and detriment. We also prove that the actual effect of RAG can be predicted at token level. Based on our theory, we propose a practical novel method, X-RAG, which achieves collaborative generation between pure LLM and RAG at token level to preserve benefit and avoid detriment. Experiments in real-world tasks based on LLMs including OPT, LLaMA-2, and Mistral show the effectiveness of our method and support our theoretical results.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 利用检索到的文本来增强大型语言模型 (LLM)。然而，研究表明，RAG 并非始终有效，甚至会由于检索到的文本嘈杂或不正确而误导 LLM。这表明 RAG 具有二元性，既有好处也有坏处。尽管许多现有方法试图解决这个问题，但它们缺乏对 RAG 二元性的理论解释。这种二元性中的好处和坏处仍然是一个黑匣子，无法量化或以可解释的方式进行比较。本文通过以下方法，在理论上对 RAG 中的好处和坏处进行了初步解释：(1) 将它们与 RAG 预测分离并形式化，(2) 通过表示相似性来近似它们值之间的差距，(3) 建立它们之间的权衡机制，使它们可解释、可量化和可比较。我们证明检索到的文本和 LLM 知识之间的分布差异是一把双刃剑，既带来好处，也带来坏处。我们还证明了 RAG 的实际效果可以在 token 级别进行预测。基于我们的理论，我们提出了一种实用的新方法 X-RAG，它在 token 级别实现了纯 LLM 和 RAG 之间的协同生成，以保留利益并避免损害。在基于 LLM 的现实任务（包括 OPT、LLaMA-2 和 Mistral）中的实验证明了我们方法的有效性并支持了我们的理论结果。</li>
</ul>

<h3>Title: Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Liu, Sannyuya Liu, Lele Sha, Zijie Zeng, Dragan Gasevic, Zhi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00954">https://arxiv.org/abs/2406.00954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00954">https://arxiv.org/pdf/2406.00954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00954]] Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification(https://arxiv.org/abs/2406.00954)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Various machine learning approaches have gained significant popularity for the automated classification of educational text to identify indicators of learning engagement -- i.e. learning engagement classification (LEC). LEC can offer comprehensive insights into human learning processes, attracting significant interest from diverse research communities, including Natural Language Processing (NLP), Learning Analytics, and Educational Data Mining. Recently, Large Language Models (LLMs), such as ChatGPT, have demonstrated remarkable performance in various NLP tasks. However, their comprehensive evaluation and improvement approaches in LEC tasks have not been thoroughly investigated. In this study, we propose the Annotation Guidelines-based Knowledge Augmentation (AGKA) approach to improve LLMs. AGKA employs GPT 4.0 to retrieve label definition knowledge from annotation guidelines, and then applies the random under-sampler to select a few typical examples. Subsequently, we conduct a systematic evaluation benchmark of LEC, which includes six LEC datasets covering behavior classification (question and urgency level), emotion classification (binary and epistemic emotion), and cognition classification (opinion and cognitive presence). The study results demonstrate that AGKA can enhance non-fine-tuned LLMs, particularly GPT 4.0 and Llama 3 70B. GPT 4.0 with AGKA few-shot outperforms full-shot fine-tuned models such as BERT and RoBERTa on simple binary classification datasets. However, GPT 4.0 lags in multi-class tasks that require a deep understanding of complex semantic information. Notably, Llama 3 70B with AGKA is a promising combination based on open-source LLM, because its performance is on par with closed-source GPT 4.0 with AGKA. In addition, LLMs struggle to distinguish between labels with similar names in multi-class classification.</li>
<li><strong>摘要：</strong>各种机器学习方法已在自动分类教育文本以识别学习参与度指标方面获得了广泛的欢迎——即学习参与度分类 (LEC)。LEC 可以提供对人类学习过程的全面洞察，吸引了包括自然语言处理 (NLP)、学习分析和教育数据挖掘在内的不同研究社区的极大兴趣。最近，大型语言模型 (LLM)，例如 ChatGPT，在各种 NLP 任务中表现出色。然而，它们在 LEC 任务中的综合评估和改进方法尚未得到彻底研究。在本研究中，我们提出了基于注释指南的知识增强 (AGKA) 方法来改进 LLM。AGKA 使用 GPT 4.0 从注释指南中检索标签定义知识，然后应用随机欠采样器选择一些典型示例。随后，我们对 LEC 进行了系统的评估基准，其中包括六个 LEC 数据集，涵盖行为分类（问题和紧迫性级别）、情绪分类（二元和认知情绪）和认知分类（意见和认知存在）。研究结果表明，AGKA 可以增强非微调 LLM，尤其是 GPT 4.0 和 Llama 3 70B。在简单的二分类数据集上，GPT 4.0 搭配 AGKA 的小样本表现优于 BERT 和 RoBERTa 等全样本微调模型。然而，GPT 4.0 在需要深入理解复杂语义信息的多类任务中表现不佳。值得注意的是，Llama 3 70B 搭配 AGKA 是基于开源 LLM 的有前途的组合，因为它的性能与闭源 GPT 4.0 搭配 AGKA 相当。此外，LLM 很难在多类分类中区分名称相似的标签。</li>
</ul>

<h3>Title: Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Mehta, Dan Goldwasser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00969">https://arxiv.org/abs/2406.00969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00969">https://arxiv.org/pdf/2406.00969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00969]] Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media(https://arxiv.org/abs/2406.00969)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>The large scale usage of social media, combined with its significant impact, has made it increasingly important to understand it. In particular, identifying user communities, can be helpful for many downstream tasks. However, particularly when models are trained on past data and tested on future, doing this is difficult. In this paper, we hypothesize to take advantage of Large Language Models (LLMs), to better identify user communities. Due to the fact that many LLMs, such as ChatGPT, are fixed and must be treated as black-boxes, we propose an approach to better prompt them, by training a smaller LLM to do this. We devise strategies to train this smaller model, showing how it can improve the larger LLMs ability to detect communities. Experimental results show improvements on Reddit and Twitter data, on the tasks of community detection, bot detection, and news media profiling.</li>
<li><strong>摘要：</strong>社交媒体的大规模使用及其重大影响使得了解它变得越来越重要。特别是，识别用户社区对许多下游任务很有帮助。但是，特别是当模型基于过去的数据进行训练并在未来进行测试时，做到这一点很困难。在本文中，我们假设利用大型语言模型 (LLM) 来更好地识别用户社区。由于许多 LLM（例如 ChatGPT）是固定的并且必须被视为黑匣子，我们提出了一种更好地提示它们的方法，即通过训练较小的 LLM 来执行此操作。我们设计了训练这个较小模型的策略，展示了它如何提高较大的 LLM 检测社区的能力。实验结果显示，在 Reddit 和 Twitter 数据上，在社区检测、机器人检测和新闻媒体分析任务上都有所改进。</li>
</ul>

<h3>Title: Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost</h3>
<ul>
<li><strong>Authors: </strong>Masha Belyi, Robert Friel, Shuai Shao, Atindriyo Sanyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00975">https://arxiv.org/abs/2406.00975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00975">https://arxiv.org/pdf/2406.00975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00975]] Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost(https://arxiv.org/abs/2406.00975)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Retriever Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations: instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in diverse industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously. We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97% and 96% reduction in cost and latency, respectively. Luna is lightweight and generalizes across multiple industry verticals and out-of-domain data, making it an ideal candidate for industry LLM applications.</li>
<li><strong>摘要：</strong>通过整合外部知识检索机制，检索器增强生成 (RAG) 系统已成为增强语言模型功能的关键。然而，在行业应用中部署这些系统的一个重大挑战是检测和缓解幻觉：模型生成的信息并非基于检索到的上下文的情况。解决这个问题对于确保大型语言模型 (LLM) 在不同行业环境中生成的响应的可靠性和准确性至关重要。当前的幻觉检测技术无法同时实现准确性、低延迟和低成本。我们推出了 Luna：一个 DeBERTA-large (440M) 编码器，针对 RAG 设置中的幻觉检测进行了微调。我们证明 Luna 在幻觉检测任务上的表现优于 GPT-3.5 和商业评估框架，成本和延迟分别降低了 97% 和 96%。Luna 是轻量级的，可推广到多个行业垂直领域和域外数据，使其成为行业 LLM 应用程序的理想候选者。</li>
</ul>

<h3>Title: Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yongxin Zhu, Dan Su, Liqiang He, Linli Xu, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.00976">https://arxiv.org/abs/2406.00976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.00976">https://arxiv.org/pdf/2406.00976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.00976]] Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer(https://arxiv.org/abs/2406.00976)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \textbf{G}enerative \textbf{P}re-trained \textbf{S}peech \textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. See \url{this https URL} for demo samples.</li>
<li><strong>摘要：</strong>虽然语音语言模型最近取得了重大进展，但它们在对神经音频编解码器的长声学序列进行建模方面面临着巨大的挑战。在本文中，我们介绍了 \textbf{G}enerative \textbf{P}re-trained \textbf{S}peech \textbf{T}transformer (GPST)，这是一种专为高效语音语言建模而设计的分层转换器。GPST 将音频波形量化为两种不同类型的离散语音表示，并将它们集成到分层转换器架构中，从而实现统一的单阶段生成过程并增强高分辨率音频生成功能。通过以端到端无监督的方式对大量语音进行训练，GPST 可以生成具有不同说话者身份的语法一致的语音。在短短的 3 秒提示下，GPST 可以产生自然连贯的个性化语音，展示出上下文学习能力。此外，通过结合多语言语义标记和通用声学标记，我们的方法可以轻松扩展到口语跨语言语音生成。实验结果表明，GPST 在误词率、语音质量、说话人相似度等方面的表现明显优于现有的语音语言模型。演示示例请参见 \url{此 https URL}。</li>
</ul>

<h3>Title: SemCoder: Training Code Language Models with Comprehensive Semantics</h3>
<ul>
<li><strong>Authors: </strong>Yangruibo Ding, Jinjun Peng, Marcus J. Min, Gail Kaiser, Junfeng Yang, Baishakhi Ray</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01006">https://arxiv.org/abs/2406.01006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01006">https://arxiv.org/pdf/2406.01006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01006]] SemCoder: Training Code Language Models with Comprehensive Semantics(https://arxiv.org/abs/2406.01006)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for thorough semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy to train Code LLMs with comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states. We begin by collecting PyX, a clean code corpus of fully executable samples with functional descriptions and execution tracing. We propose training Code LLMs to write code and represent and reason about execution behaviors using natural language, mimicking human verbal debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 81.1% on HumanEval (GPT-3.5-turbo: 76.8%) and 54.5% on CRUXEval-I (GPT-3.5-turbo: 50.3%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities.</li>
<li><strong>摘要：</strong>代码大型语言模型 (Code LLM) 在代码完成等任务上表现出色，但往往缺少更深层次的语义，例如执行效果和动态状态。本文旨在弥合 Code LLM 对静态文本数据的依赖与调试和程序修复等复杂任务对彻底语义理解的需求之间的差距。我们引入了一种新颖的策略来训练具有全面语义的代码 LLM，涵盖高级功能描述、单个语句的局部执行效果以及整体输入/输出行为，从而将静态代码文本与动态执行状态联系起来。我们首先收集 PyX，这是一个干净的完全可执行样本代码语料库，具有功能描述和执行跟踪。我们建议训练 Code LLM 编写代码并使用自然语言表示和推理执行行为，模仿人类的口头调试。这种方法导致了 SemCoder 的开发，这是一个只有 6.7B 参数的代码 LLM，它在代码生成和执行推理任务上表现出与 GPT-3.5-turbo 相媲美的性能。 SemCoder 在 HumanEval 上达到 81.1%（GPT-3.5-turbo：76.8%），在 CRUXEval-I 上达到 54.5%（GPT-3.5-turbo：50.3%）。我们还研究了 SemCoder 独白式执行推理与具体暂存器推理的有效性，表明我们的方法可以更顺畅地整合来自多个维度的语义。最后，我们展示了应用学习到的语义来提高代码 LLM 的调试和自我改进能力的潜力。</li>
</ul>

<h3>Title: Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01014">https://arxiv.org/abs/2406.01014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01014">https://arxiv.org/pdf/2406.01014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01014]] Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration(https://arxiv.org/abs/2406.01014)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks, task progress navigation and focus content navigation, are significantly complicated under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent generates task progress, making the navigation of history operations more efficient. To retain focus content, we design a memory unit that updates with task progress. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistakes accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>移动设备操作任务正日益成为一种流行的多模态AI应用场景。目前的多模态大型语言模型（MLLM）受限于训练数据，无法有效充当操作助手。相反，基于MLLM的代理通过工具调用来增强功能，并逐渐应用于此场景。然而，在现有工作的单代理架构下，移动设备操作任务中的两大导航挑战——任务进度导航和焦点内容导航——变得非常复杂。这是由于过长的token序列和交错的文本图像数据格式限制了性能。为了有效解决这些导航挑战，我们提出了Mobile-Agent-v2，一种用于移动设备操作辅助的多代理架构。该架构包括三个代理：规划代理、决策代理和反射代理。规划代理生成任务进度，使历史操作的导航更加高效。为了保留焦点内容，我们设计了一个随任务进度更新的记忆单元。此外，为了纠正错误操作，反射代理会观察每个操作的结果并相应地处理任何错误。实验结果表明，与 Mobile-Agent 的单代理架构相比，Mobile-Agent-v2 在任务完成方面实现了 30% 以上的提升。代码已在此 https URL 上开源。</li>
</ul>

<h3>Title: Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors</h3>
<ul>
<li><strong>Authors: </strong>Mengge Xue, Zhenyu Hu, Meng Zhao, Liqun Liu, Kuo Liao, Shuang Li, Honglin Han, Chengguo Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01026">https://arxiv.org/abs/2406.01026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01026">https://arxiv.org/pdf/2406.01026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01026]] Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors(https://arxiv.org/abs/2406.01026)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models (LLMs). Previous works have investigated the selection bias problem in MCQs within few-shot scenarios, in which the LLM's performance may be influenced by the presentation of answer choices, leaving the selection bias during Supervised Fine-Tuning (SFT) unexplored. In this paper, we reveal that selection bias persists in the SFT phase , primarily due to the LLM's inadequate Multiple Choice Symbol Binding (MCSB) ability. This limitation implies that the model struggles to associate the answer options with their corresponding symbols (e.g., A/B/C/D) effectively. To enhance the model's MCSB capability, we first incorporate option contents into the loss function and subsequently adjust the weights of the option symbols and contents, guiding the model to understand the option content of the current symbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed Point-wise Intelligent Feedback (PIF). PIF constructs negative instances by randomly combining the incorrect option contents with all candidate symbols, and proposes a point-wise loss to provide feedback on these negative samples into LLMs. Our experimental results demonstrate that PIF significantly reduces the model's selection bias by improving its MCSB capability. Remarkably, PIF exhibits a substantial enhancement in the accuracy for MCQs.</li>
<li><strong>摘要：</strong>多项选择题 (MCQ) 是大型语言模型 (LLM) 研究中的一个重要研究领域。先前的研究已经研究了小样本场景中 MCQ 的选择偏差问题，其中 LLM 的性能可能受到答案选项的呈现方式的影响，而监督微调 (SFT) 期间的选择偏差尚未得到探究。在本文中，我们发现选择偏差在 SFT 阶段仍然存在，主要是由于 LLM 的多项选择符号绑定 (MCSB) 能力不足。这一限制意味着该模型难以有效地将答案选项与其相应的符号（例如 A/B/C/D）相关联。为了增强模型的 MCSB 能力，我们首先将选项内容纳入损失函数，然后调整选项符号和内容的权重，引导模型理解当前符号的选项内容。基于此，我们引入了一种用于 MCQ 的有效 SFT 算法，称为逐点智能反馈 (PIF)。 PIF 通过将错误选项内容与所有候选符号随机组合来构建负实例，并提出逐点损失以将这些负样本反馈到 LLM 中。我们的实验结果表明，PIF 通过提高其 MCSB 能力显著降低了模型的选择偏差。值得注意的是，PIF 在 MCQ 的准确率方面表现出显著的提升。</li>
</ul>

<h3>Title: Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Shiri, Van Nguyen, Farhad Moghimifar, John Yoo, Gholamreza Haffari, Yuan-Fang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01045">https://arxiv.org/abs/2406.01045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01045">https://arxiv.org/pdf/2406.01045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01045]] Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs(https://arxiv.org/abs/2406.01045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method's superior performance compared to baseline approaches.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在处理自然语言数据方面表现出了强大的能力，有望从各种文本来源中高效地提取知识，以增强态势感知并支持决策。然而，由于它们容易产生幻觉，导致上下文内容不准确，因此人们对此产生了担忧。这项工作专注于利用 LLM 进行自动事件提取，通过将任务分解为事件检测和事件参数提取来引入一种解决幻觉的新方法。此外，所提出的方法将动态模式感知增强检索示例集成到针对每个特定查询量身定制的提示中，从而扩展和调整高级提示技术，例如检索增强生成。对突出事件提取基准的评估结果和综合基准的结果表明，该方法与基线方法相比具有更优越的性能。</li>
</ul>

<h3>Title: MACT: Model-Agnostic Cross-Lingual Training for Discourse Representation Structure Parsing</h3>
<ul>
<li><strong>Authors: </strong>Jiangming Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01052">https://arxiv.org/abs/2406.01052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01052">https://arxiv.org/pdf/2406.01052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01052]] MACT: Model-Agnostic Cross-Lingual Training for Discourse Representation Structure Parsing(https://arxiv.org/abs/2406.01052)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Discourse Representation Structure (DRS) is an innovative semantic representation designed to capture the meaning of texts with arbitrary lengths across languages. The semantic representation parsing is essential for achieving natural language understanding through logical forms. Nevertheless, the performance of DRS parsing models remains constrained when trained exclusively on monolingual data. To tackle this issue, we introduce a cross-lingual training strategy. The proposed method is model-agnostic yet highly effective. It leverages cross-lingual training data and fully exploits the alignments between languages encoded in pre-trained language models. The experiments conducted on the standard benchmarks demonstrate that models trained using the cross-lingual training method exhibit significant improvements in DRS clause and graph parsing in English, German, Italian and Dutch. Comparing our final models to previous works, we achieve state-of-the-art results in the standard benchmarks. Furthermore, the detailed analysis provides deep insights into the performance of the parsers, offering inspiration for future research in DRS parsing. We keep updating new results on benchmarks to the appendix.</li>
<li><strong>摘要：</strong>语篇表征结构 (DRS) 是一种创新的语义表征，旨在捕捉跨语言任意长度文本的含义。语义表征解析对于通过逻辑形式实现自然语言理解至关重要。然而，当仅对单语数据进行训练时，DRS 解析模型的性能仍然受到限制。为了解决这个问题，我们引入了一种跨语言训练策略。所提出的方法与模型无关，但非常有效。它利用跨语言训练数据并充分利用预训练语言模型中编码的语言之间的对齐。在标准基准上进行的实验表明，使用跨语言训练方法训练的模型在英语、德语、意大利语和荷兰语的 DRS 子句和图形解析方面表现出显着的改进。将我们的最终模型与之前的作品进行比较，我们在标准基准中取得了最先进的结果。此外，详细的分析提供了对解析器性能的深刻见解，为 DRS 解析的未来研究提供了灵感。我们不断将基准的新结果更新到附录中。</li>
</ul>

<h3>Title: Guiding ChatGPT to Generate Salient Domain Summaries</h3>
<ul>
<li><strong>Authors: </strong>Jun Gao, Ziqiang Cao, Shaoyao Huang, Luozheng Qin, Chunhui Ai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01070">https://arxiv.org/abs/2406.01070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01070">https://arxiv.org/pdf/2406.01070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01070]] Guiding ChatGPT to Generate Salient Domain Summaries(https://arxiv.org/abs/2406.01070)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>ChatGPT is instruct-tuned to generate general and human-expected content to align with human preference through Reinforcement Learning from Human Feedback (RLHF), meanwhile resulting in generated responses not salient enough. Therefore, in this case, ChatGPT may fail to satisfy domain requirements in zero-shot settings, leading to poor ROUGE scores. Inspired by the In-Context Learning (ICL) and retelling ability of ChatGPT, this paper proposes PADS, a \textbf{P}ipeline for \textbf{A}ssisting ChatGPT in \textbf{D}omain \textbf{S}ummarization. PADS consists of a retriever to retrieve similar examples from corpora and a rank model to rerank the multiple candidate summaries generated by ChatGPT. Specifically, given an inference document, we first retrieve an in-context demonstration via the retriever. Then, we require ChatGPT to generate $k$ candidate summaries for the inference document at a time under the guidance of the retrieved demonstration. Finally, the rank model independently scores the $k$ candidate summaries according to their quality and selects the optimal one. We extensively explore dense and sparse retrieval methods to select effective demonstrations for reference and efficiently train the rank model to reflect the quality of candidate summaries for each given summarized document. Additionally, PADS contains merely 400M trainable parameters originating from the rank model and we merely collect 2.5k data to train it. We evaluate PADS on five datasets from different domains, and the result indicates that each module in PADS is committed to effectively guiding ChatGPT to generate salient summaries fitting different domain requirements. Specifically, in the popular summarization dataset Gigaword, PADS achieves over +8 gain on ROUGE-L, compared with the naive ChatGPT in the zero-shot setting. \footnote{Our code are available at \url{this https URL}}</li>
<li><strong>摘要：</strong>ChatGPT 通过从人类反馈中强化学习 (RLHF) 来指令调整，以生成符合人类偏好的一般和人类预期的内容，同时导致生成的响应不够突出。因此，在这种情况下，ChatGPT 可能无法满足零样本设置下的领域要求，从而导致 ROUGE 分数不佳。受 ChatGPT 的上下文学习 (ICL) 和复述能力的启发，本文提出了 PADS，一种用于协助 ChatGPT 进行领域摘要的 \textbf{Pipeline}。PADS 由一个从语料库中检索类似示例的检索器和一个用于对 ChatGPT 生成的多个候选摘要进行重新排序的排序模型组成。具体而言，给定一个推理文档，我们首先通过检索器检索上下文演示。然后，我们要求 ChatGPT 在检索到的演示的指导下一次为推理文档生成 $k$ 个候选摘要。最后，排序模型根据质量独立对 $k$ 个候选摘要进行评分，并选择最佳摘要。我们广泛探索密集和稀疏检索方法，以选择有效的演示作为参考，并有效地训练排序模型以反映每个给定摘要文档的候选摘要的质量。此外，PADS 仅包含来自排序模型的 400M 个可训练参数，我们仅收集了 2.5k 个数据来训练它。我们在来自不同领域的五个数据集上评估了 PADS，结果表明 PADS 中的每个模块都致力于有效指导 ChatGPT 生成符合不同领域要求的突出摘要。具体来说，在流行的摘要数据集 Gigaword 中，与零样本设置中的幼稚 ChatGPT 相比，PADS 在 ROUGE-L 上实现了超过 +8 的增益。\footnote{我们的代码可在 \url{此 https URL}} 上找到</li>
</ul>

<h3>Title: Synergizing Unsupervised and Supervised Learning: A Hybrid Approach for Accurate Natural Language Task Modeling</h3>
<ul>
<li><strong>Authors: </strong>Wrick Talukdar, Anjanava Biswas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01096">https://arxiv.org/abs/2406.01096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01096">https://arxiv.org/pdf/2406.01096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01096]] Synergizing Unsupervised and Supervised Learning: A Hybrid Approach for Accurate Natural Language Task Modeling(https://arxiv.org/abs/2406.01096)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While supervised learning models have shown remarkable performance in various natural language processing (NLP) tasks, their success heavily relies on the availability of large-scale labeled datasets, which can be costly and time-consuming to obtain. Conversely, unsupervised learning techniques can leverage abundant unlabeled text data to learn rich representations, but they do not directly optimize for specific NLP tasks. This paper presents a novel hybrid approach that synergizes unsupervised and supervised learning to improve the accuracy of NLP task modeling. While supervised models excel at specific tasks, they rely on large labeled datasets. Unsupervised techniques can learn rich representations from abundant unlabeled text but don't directly optimize for tasks. Our methodology integrates an unsupervised module that learns representations from unlabeled corpora (e.g., language models, word embeddings) and a supervised module that leverages these representations to enhance task-specific models. We evaluate our approach on text classification and named entity recognition (NER), demonstrating consistent performance gains over supervised baselines. For text classification, contextual word embeddings from a language model pretrain a recurrent or transformer-based classifier. For NER, word embeddings initialize a BiLSTM sequence labeler. By synergizing techniques, our hybrid approach achieves SOTA results on benchmark datasets, paving the way for more data-efficient and robust NLP systems.</li>
<li><strong>摘要：</strong>虽然监督学习模型在各种自然语言处理 (NLP) 任务中表现出色，但它们的成功在很大程度上依赖于大规模标记数据集的可用性，而获取这些数据集的成本可能很高且耗时。相反，无监督学习技术可以利用丰富的未标记文本数据来学习丰富的表示，但它们不会直接针对特定的 NLP 任务进行优化。本文提出了一种新颖的混合方法，该方法协同无监督和监督学习来提高 NLP 任务建模的准确性。虽然监督模型在特定任务上表现出色，但它们依赖于大型标记数据集。无监督技术可以从丰富的未标记文本中学习丰富的表示，但不会直接针对任务进行优化。我们的方法集成了一个无监督模块，该模块从未标记语料库（例如语言模型、词嵌入）中学习表示，以及一个监督模块，该模块利用这些表示来增强特定于任务的模型。我们在文本分类和命名实体识别 (NER) 上评估了我们的方法，结果显示与监督基线相比，性能持续提升。对于文本分类，语言模型中的上下文词嵌入会预训练循环或基于变换器的分类器。对于 NER，词嵌入会初始化 BiLSTM 序列标记器。通过协同技术，我们的混合方法在基准数据集上取得了 SOTA 结果，为更高效、更强大的 NLP 系统铺平了道路。</li>
</ul>

<h3>Title: TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models in Traditional Chinese Medicine</h3>
<ul>
<li><strong>Authors: </strong>Wenjing Yue, Xiaoling Wang, Wei Zhu, Ming Guan, Huanran Zheng, Pengfei Wang, Changzhi Sun, Xin Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01126">https://arxiv.org/abs/2406.01126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01126">https://arxiv.org/pdf/2406.01126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01126]] TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models in Traditional Chinese Medicine(https://arxiv.org/abs/2406.01126)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have performed remarkably well in various natural language processing tasks by benchmarking, including in the Western medical domain. However, the professional evaluation benchmarks for LLMs have yet to be covered in the traditional Chinese medicine(TCM) domain, which has a profound history and vast influence. To address this research gap, we introduce TCM-Bench, an comprehensive benchmark for evaluating LLM performance in TCM. It comprises the TCM-ED dataset, consisting of 5,473 questions sourced from the TCM Licensing Exam (TCMLE), including 1,300 questions with authoritative analysis. It covers the core components of TCMLE, including TCM basis and clinical practice. To evaluate LLMs beyond accuracy of question answering, we propose TCMScore, a metric tailored for evaluating the quality of answers generated by LLMs for TCM related questions. It comprehensively considers the consistency of TCM semantics and knowledge. After conducting comprehensive experimental analyses from diverse perspectives, we can obtain the following findings: (1) The unsatisfactory performance of LLMs on this benchmark underscores their significant room for improvement in TCM. (2) Introducing domain knowledge can enhance LLMs' performance. However, for in-domain models like ZhongJing-TCM, the quality of generated analysis text has decreased, and we hypothesize that their fine-tuning process affects the basic LLM capabilities. (3) Traditional metrics for text generation quality like Rouge and BertScore are susceptible to text length and surface semantic ambiguity, while domain-specific metrics such as TCMScore can further supplement and explain their evaluation results. These findings highlight the capabilities and limitations of LLMs in the TCM and aim to provide a more profound assistance to medical research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种自然语言处理任务中表现出色，包括在西医领域。然而，历史悠久、影响广泛的中医领域尚未涵盖 LLM 的专业评估基准。为了弥补这一研究空白，我们推出了 TCM-Bench，这是一个用于评估中医药领域 LLM 性能的综合基准。它包含 TCM-ED 数据集，包含来自中医师执照考试 (TCMLE) 的 5,473 道题，其中包括 1,300 道具有权威分析的题。它涵盖了 TCMLE 的核心部分，包括中医基础和临床实践。为了评估 LLM 不仅仅是看问答准确性，我们提出了 TCMScore，这是一个专门用于评估 LLM 针对中医相关问题生成的答案质量的指标。它综合考虑了中医语义和知识的一致性。我们从多个角度进行全面的实验分析后，得出以下结论：（1）LLM 在这个基准测试中的表现不尽如人意，表明它们在中医领域还有很大的改进空间。（2）引入领域知识可以提升 LLM 的性能，但对于像 ZhongJing-TCM 这样的领域内模型，生成的分析文本的质量有所下降，我们假设它们的微调过程影响了基本的 LLM 能力。（3）传统的文本生成质量指标（如 Rouge 和 BertScore）容易受到文本长度和表面语义模糊性的影响，而领域特定指标（如 TCMScore）可以进一步补充和解释它们的评估结果。这些发现凸显了 LLM 在中医领域的能力和局限性，旨在为医学研究提供更深层次的帮助。</li>
</ul>

<h3>Title: Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Guangyi Liu, Yongqi Zhang, Yong Li, Quanming Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01145">https://arxiv.org/abs/2406.01145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01145">https://arxiv.org/pdf/2406.01145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01145]] Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph(https://arxiv.org/abs/2406.01145)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The task of reasoning over Knowledge Graphs (KGs) poses a significant challenge for Large Language Models (LLMs) due to the complex structure and large amounts of irrelevant information. Existing LLM reasoning methods overlook the importance of compositional learning on KG to supply with precise knowledge. Besides, the fine-tuning and frequent interaction with LLMs incur substantial time and resource costs. This paper focuses on the Question Answering over Knowledge Graph (KGQA) task and proposes an Explore-then-Determine (EtD) framework that synergizes LLMs with graph neural networks (GNNs) for reasoning over KGs. The Explore stage employs a lightweight GNN to explore promising candidates and relevant fine-grained knowledge to the questions, while the Determine stage utilizes the explored information to construct a knowledge-enhanced multiple-choice prompt, guiding a frozen LLM to determine the final answer. Extensive experiments on three benchmark KGQA datasets demonstrate that EtD achieves state-of-the-art performance and generates faithful reasoning results.</li>
<li><strong>摘要：</strong>由于知识图谱结构复杂、存在大量无关信息，对知识图谱进行推理对大型语言模型 (LLM) 提出了重大挑战。现有的 LLM 推理方法忽视了对 KG 进行组合学习以提供精确知识的重要性。此外，微调和与 LLM 的频繁交互会产生大量的时间和资源成本。本文重点关注知识图谱问答 (KGQA) 任务，并提出了一个探索然后确定 (EtD) 框架，该框架将 LLM 与图神经网络 (GNN) 协同用于 KG 推理。探索阶段使用轻量级 GNN 来探索有希望的候选者和与问题相关的细粒度知识，而确定阶段利用探索的信息构建知识增强的多项选择提示，引导冻结的 LLM 确定最终答案。在三个基准 KGQA 数据集上进行的大量实验表明，EtD 实现了最佳性能并生成忠实的推理结果。</li>
</ul>

<h3>Title: Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-Ching Hsu, Jia-Yin Foo, Chao-Wei Huang, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01171">https://arxiv.org/abs/2406.01171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01171">https://arxiv.org/pdf/2406.01171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01171]] Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization(https://arxiv.org/abs/2406.01171)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, methods investigating how to adapt large language models (LLMs) for specific scenarios have gained great attention. Particularly, the concept of \textit{persona}, originally adopted in dialogue literature, has re-surged as a promising avenue. However, the growing research on persona is relatively disorganized, lacking a systematic overview. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. To the best of our knowledge, we present the first survey tailored for LLM role-playing and LLM personalization under the unified view of persona, including taxonomy, current challenges, and potential directions. To foster future endeavors, we actively maintain a paper collection available to the community: this https URL</li>
<li><strong>摘要：</strong>最近，研究如何将大型语言模型 (LLM) 适应特定场景的方法引起了极大关注。特别是，最初在对话文献中采用的 \textit{persona} 概念重新成为一种有前途的途径。然而，对角色的研究日益增多，相对混乱，缺乏系统的概述。为了弥补这一差距，我们进行了一项全面的调查，以对该领域的现状进行分类。我们确定了两条研究路线，即 (1) LLM 角色扮演，其中角色被分配给 LLM，以及 (2) LLM 个性化，其中 LLM 负责用户角色。据我们所知，我们在统一的角色视图下提出了第一份针对 LLM 角色扮演和 LLM 个性化的调查，包括分类法、当前挑战和潜在方向。为了促进未来的努力，我们积极维护一个可供社区使用的论文集：此 https URL</li>
</ul>

<h3>Title: Are AI-Generated Text Detectors Robust to Adversarial Perturbations?</h3>
<ul>
<li><strong>Authors: </strong>Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, Zhouwang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01179">https://arxiv.org/abs/2406.01179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01179">https://arxiv.org/pdf/2406.01179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01179]] Are AI-Generated Text Detectors Robust to Adversarial Perturbations?(https://arxiv.org/abs/2406.01179)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confidence predictions under different noise, which improves the model's robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5\%-18.25\% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的广泛使用引发了人们对人工智能生成文本可能被滥用的担忧，因为这些模型可以生成与人类生成的文本非常相似的内容。当前的人工智能生成文本 (AIGT) 检测器缺乏对抗性干扰的鲁棒性，即使字符或单词的微小变化也会导致无法区分人类创建的文本和人工智能生成的文本。本文研究了现有 AIGT 检测方法的鲁棒性，并介绍了一种新型检测器——孪生校准重建网络 (SCRN)。SCRN 采用重建网络在文本中添加和消除噪声，提取对局部扰动具有鲁棒性的语义表示。我们还提出了一种孪生校准技术来训练模型在不同噪声下做出同等置信度预测，从而提高了模型对对抗性干扰的鲁棒性。在四个公开数据集上进行的实验表明，SCRN 的表现优于所有基线方法，在对抗性攻击下，其绝对准确率比最佳基线方法提高了 6.5\%-18.25\%。此外，它在跨域、跨类型和混合源场景中表现出卓越的通用性。代码可在 \url{此 https URL} 上找到。</li>
</ul>

<h3>Title: Demonstration Augmentation for Zero-shot In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Yi Su, Yunpeng Tai, Yixin Ji, Juntao Li, Bowen Yan, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01224">https://arxiv.org/abs/2406.01224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01224">https://arxiv.org/pdf/2406.01224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01224]] Demonstration Augmentation for Zero-shot In-context Learning(https://arxiv.org/abs/2406.01224)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies have highlighted that the model's performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries. Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs. In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model's reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming. To address these issues, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model's previously predicted historical samples as demonstrations for subsequent ones. DAIL brings no additional inference cost and does not rely on the model's generative capabilities. Our experiments reveal that DAIL can significantly improve the model's performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经展示了一种令人印象深刻的功能，即上下文学习 (ICL)，这使它们能够从文本演示中获取知识而无需更新参数。然而，许多研究都强调，模型的性能对演示的选择很敏感，这对我们缺乏用户查询先验知识的实际应用提出了重大挑战。因此，我们需要构建一个广泛的演示池并结合外部数据库来协助模型，这将花费大量的时间和财务成本。鉴于此，最近的一些研究将重点转向零样本 ICL，旨在通过利用其固有的生成能力来减少模型对外部信息的依赖。尽管这些方法很有效，但模型生成的内容可能不可靠，并且生成过程很耗时。为了解决这些问题，我们提出了用于上下文学习的演示增强 (DAIL)，它使用模型先前预测的历史样本作为后续样本的演示。DAIL 不会带来额外的推理成本，也不依赖于模型的生成能力。我们的实验表明，DAIL 可以显著提高模型在直接零样本推理方面的性能，甚至可以胜过没有任何外部信息的小样本 ICL。</li>
</ul>

<h3>Title: EffiQA: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Dong, Baoyun Peng, Yufei Wang, Jia Fu, Xiaodong Wang, Yongxue Shan, Xin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01238">https://arxiv.org/abs/2406.01238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01238">https://arxiv.org/pdf/2406.01238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01238]] EffiQA: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs(https://arxiv.org/abs/2406.01238)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm. EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve the global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在自然语言处理方面表现出色，但它们在涉及知识图谱 (KG) 的复杂、多步骤推理任务方面却举步维艰。现有的集成 LLM 和 KG 的方法要么未充分利用 LLM 的推理能力，要么由于紧密耦合而导致计算成本过高。为了解决这些限制，我们提出了一个名为 EffiQA 的新型协作框架，该框架可以通过迭代范式在性能和效率之间取得平衡。EffiQA 包括三个阶段：全局规划、高效 KG 探索和自我反思。具体来说，EffiQA 利用 LLM 的常识能力通过全局规划探索潜在的推理路径。然后，它将语义修剪卸载到一个小的插件模型上，以进行高效的 KG 探索。最后，将探索结果输入到 LLM 进行自我反思，以进一步改进全局规划和高效的 KG 探索。多个 KBQA 基准测试的经验证据表明 EffiQA 的有效性，在推理准确性和计算成本之间实现了最佳平衡。我们希望提出的新框架能够通过重新定义 LLM 和 KG 的集成来为高效、知识密集型查询铺平道路，促进未来基于知识的问答研究。</li>
</ul>

<h3>Title: Towards Scalable Automated Alignment of LLMs: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, Bowen Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01252">https://arxiv.org/abs/2406.01252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01252">https://arxiv.org/pdf/2406.01252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01252]] Towards Scalable Automated Alignment of LLMs: A Survey(https://arxiv.org/abs/2406.01252)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Alignment is the most critical step in building large language models (LLMs) that meet human needs. With the rapid development of LLMs gradually surpassing human capabilities, traditional alignment methods based on human-annotation are increasingly unable to meet the scalability demands. Therefore, there is an urgent need to explore new sources of automated alignment signals and technical approaches. In this paper, we systematically review the recently emerging methods of automated alignment, attempting to explore how to achieve effective, scalable, automated alignment once the capabilities of LLMs exceed those of humans. Specifically, we categorize existing automated alignment methods into 4 major categories based on the sources of alignment signals and discuss the current status and potential development of each category. Additionally, we explore the underlying mechanisms that enable automated alignment and discuss the essential factors that make automated alignment technologies feasible and effective from the fundamental role of alignment.</li>
<li><strong>摘要：</strong>对齐是构建符合人类需求的大型语言模型（LLM）最关键的一步。随着LLM的快速发展逐渐超越人类的能力，传统的基于人工标注的对齐方法越来越不能满足可扩展性的需求，因此迫切需要探索新的自动对齐信号来源和技术途径。本文系统地回顾了近年来出现的自动对齐方法，试图探索当LLM的能力超越人类时，如何实现有效、可扩展的自动对齐。具体来说，我们根据对齐信号的来源将现有的自动对齐方法分为4大类，并讨论了每类方法的现状和潜在发展。此外，我们从对齐的根本作用出发，探索实现自动对齐的底层机制，并讨论使自动对齐技术可行且有效的本质因素。</li>
</ul>

<h3>Title: Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses</h3>
<ul>
<li><strong>Authors: </strong>Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01288">https://arxiv.org/abs/2406.01288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01288">https://arxiv.org/pdf/2406.01288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01288]] Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses(https://arxiv.org/abs/2406.01288)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recently, Anil et al. (2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>最近，Anil 等人 (2024) 表明，通过利用其长上下文功能，多次（多达数百次）演示可以越狱最先进的 LLM。然而，是否可以使用少量演示在有限的上下文大小内有效地越狱 LLM？虽然普通的少量越狱可能效率低下，但我们提出了改进的技术，例如注入特殊系统令牌（如 [/INST]）并从收集的演示池中进行演示级随机搜索。这些简单的技术可以对对齐的 LLM 进行令人惊讶的有效越狱（即使具有高级防御）。例如，我们的方法在 Llama-2-7B 和 Llama-3-8B 上实现了 >80%（大多数 >95%）的 ASR，而无需多次重启，即使模型通过强大的防御（例如困惑检测和/或 SmoothLLM）得到增强，这对于基于后缀的越狱来说具有挑战性。此外，我们针对其他同类 LLM 和高级答辩进行了全面而细致的评估（例如，确保使用正确的系统提示），我们的方法始终能够实现近 100% 的 ASR。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01297">https://arxiv.org/abs/2406.01297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01297">https://arxiv.org/pdf/2406.01297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01297]] When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs(https://arxiv.org/abs/2406.01297)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Self-correction is an approach to improving responses from large language models (LLMs) by refining the responses using LLMs during inference. Prior work has proposed various self-correction frameworks using different sources of feedback, including self-evaluation and external feedback. However, there is still no consensus on the question of when LLMs can correct their own mistakes, as recent studies also report negative results. In this work, we critically survey broad papers and discuss the conditions required for successful self-correction. We first find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over-evaluate self-correction. To tackle these issues, we categorize research questions in self-correction research and provide a checklist for designing appropriate experiments. Our critical survey based on the newly categorized research questions shows that (1) no prior work demonstrates successful self-correction with feedback from prompted LLMs in general tasks, (2) self-correction works well in tasks that can use reliable external feedback, and (3) large-scale fine-tuning enables self-correction.</li>
<li><strong>摘要：</strong>自我纠正是一种通过在推理过程中使用 LLM 细化响应来改进大型语言模型 (LLM) 响应的方法。先前的研究提出了使用不同反馈源（包括自我评估和外部反馈）的各种自我纠正框架。然而，对于 LLM 何时可以纠正自己的错误的问题仍然没有达成共识，因为最近的研究也报告了负面结果。在这项工作中，我们批判性地调查了广泛的论文，并讨论了成功自我纠正所需的条件。我们首先发现，先前的研究通常没有详细定义他们的研究问题，并且涉及不切实际的框架或不公平的评估，从而过度评价自我纠正。为了解决这些问题，我们对自我纠正研究中的研究问题进行了分类，并提供了设计适当实验的清单。我们根据新分类的研究问题进行的批判性调查表明：（1）先前的研究表明，在一般任务中，通过 LLM 提示的反馈，自我纠正可以取得成功；（2）自我纠正在可以使用可靠的外部反馈的任务中效果很好；（3）大规模微调可以实现自我纠正。</li>
</ul>

<h3>Title: CodeR: Issue Resolving with Multi-Agent and Task Graphs</h3>
<ul>
<li><strong>Authors: </strong>Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, Jie Wang, Xiao Cheng, Guangtai Liang, Yuchi Ma, Pan Bian, Tao Xie, Qianxiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01304">https://arxiv.org/abs/2406.01304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01304">https://arxiv.org/pdf/2406.01304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01304]] CodeR: Issue Resolving with Multi-Agent and Task Graphs(https://arxiv.org/abs/2406.01304)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>GitHub issue resolving recently has attracted significant attention from academia and industry. SWE-bench is proposed to measure the performance in resolving issues. In this paper, we propose CodeR, which adopts a multi-agent framework and pre-defined task graphs to Repair & Resolve reported bugs and add new features within code Repository. On SWE-bench lite, CodeR is able to solve 28.00% of issues, in the case of submitting only once for each issue. We examine the performance impact of each design of CodeR and offer insights to advance this research direction.</li>
<li><strong>摘要：</strong>GitHub 问题解决最近引起了学术界和工业界的极大关注。SWE-bench 被提出来衡量解决问题的性能。在本文中，我们提出了 CodeR，它采用多智能体框架和预定义的任务图来修复和解决报告的错误并在代码存储库中添加新功能。在 SWE-bench lite 上，CodeR 能够解决 28.00% 的问题，而每个问题只需提交一次。我们研究了 CodeR 每种设计对性能的影响，并提供了推进这一研究方向的见解。</li>
</ul>

<h3>Title: Unsupervised Distractor Generation via Large Language Model Distilling and Counterfactual Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Fanyi Qu, Hao Sun, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01306">https://arxiv.org/abs/2406.01306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01306">https://arxiv.org/pdf/2406.01306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01306]] Unsupervised Distractor Generation via Large Language Model Distilling and Counterfactual Contrastive Decoding(https://arxiv.org/abs/2406.01306)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Within the context of reading comprehension, the task of Distractor Generation (DG) aims to generate several incorrect options to confuse readers. Traditional supervised methods for DG rely heavily on expensive human-annotated distractor labels. In this paper, we propose an unsupervised DG framework, leveraging Large Language Models (LLMs) as cost-effective annotators to enhance the DG capability of smaller student models. Specially, to perform knowledge distilling, we propose a dual task training strategy that integrates pseudo distractors from LLMs and the original answer in-formation as the objective targets with a two-stage training process. Moreover, we devise a counterfactual contrastive decoding mechanism for increasing the distracting capability of the DG model. Experiments show that our unsupervised generation method with Bart-base greatly surpasses GPT-3.5-turbo performance with only 200 times fewer model parameters. Our proposed unsupervised DG method offers a cost-effective framework for practical reading comprehension applications, without the need of laborious distractor annotation and costly large-size models</li>
<li><strong>摘要：</strong>在阅读理解的背景下，干扰项生成 (DG) 的任务旨在生成几个错误的选项来迷惑读者。传统的 DG 监督方法严重依赖昂贵的人工注释干扰项标签。在本文中，我们提出了一个无监督的 DG 框架，利用大型语言模型 (LLM) 作为经济高效的注释器来增强小型学生模型的 DG 能力。具体来说，为了进行知识提炼，我们提出了一种双任务训练策略，将来自 LLM 的伪干扰项和原始答案信息作为目标目标，并采用两阶段训练过程。此外，我们设计了一种反事实对比解码机制来提高 DG 模型的干扰能力。实验表明，我们基于 Bart-base 的无监督生成方法大大超越了 GPT-3.5-turbo 的性能，而模型参数仅减少了 200 倍。我们提出的无监督 DG 方法为实际的阅读理解应用提供了一个经济高效的框架，无需费力的干扰项注释和昂贵的大型模型</li>
</ul>

<h3>Title: FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sushant Gautam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01311">https://arxiv.org/abs/2406.01311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01311">https://arxiv.org/pdf/2406.01311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01311]] FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs(https://arxiv.org/abs/2406.01311)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Fact-checking is a crucial natural language processing (NLP) task that verifies the truthfulness of claims by considering reliable evidence. Traditional methods are often limited by labour-intensive data curation and rule-based approaches. In this paper, we present FactGenius, a novel method that enhances fact-checking by combining zero-shot prompting of large language models (LLMs) with fuzzy text matching on knowledge graphs (KGs). Leveraging DBpedia, a structured linked data dataset derived from Wikipedia, FactGenius refines LLM-generated connections using similarity measures to ensure accuracy. The evaluation of FactGenius on the FactKG, a benchmark dataset for fact verification, demonstrates that it significantly outperforms existing baselines, particularly when fine-tuning RoBERTa as a classifier. The two-stage approach of filtering and validating connections proves crucial, achieving superior performance across various reasoning types and establishing FactGenius as a promising tool for robust fact-checking. The code and materials are available at this https URL.</li>
<li><strong>摘要：</strong>事实核查是一项重要的自然语言处理 (NLP) 任务，它通过考虑可靠的证据来验证声明的真实性。传统方法通常受到劳动密集型数据管理和基于规则的方法的限制。在本文中，我们介绍了 FactGenius，这是一种新方法，它通过将大型语言模型 (LLM) 的零样本提示与知识图谱 (KG) 上的模糊文本匹配相结合来增强事实核查。利用 DBpedia（源自维基百科的结构化链接数据数据集），FactGenius 使用相似性度量来改进 LLM 生成的连接以确保准确性。在事实验证的基准数据集 FactKG 上对 FactGenius 的评估表明，它的表现明显优于现有基线，尤其是在微调 RoBERTa 作为分类器时。过滤和验证连接的两阶段方法被证明是至关重要的，它在各种推理类型中实现了卓越的性能，并确立了 FactGenius 作为一种强大的事实核查工具的前景。代码和材料可在此 https URL 上获得。</li>
</ul>

<h3>Title: Probing Language Models for Pre-training Data Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Haonan Lu, Bing Liu, Wenliang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01333">https://arxiv.org/abs/2406.01333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01333">https://arxiv.org/pdf/2406.01333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01333]] Probing Language Models for Pre-training Data Detection(https://arxiv.org/abs/2406.01333)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy (Our code and dataset are available at this https URL).</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经展示了其令人印象深刻的功能，同时也引发了人们对数据污染问题的担忧，这是由于隐私问题和预训练阶段基准数据集的泄露造成的。因此，通过检查 LLM 是否已在目标文本上进行预训练来检测污染至关重要。最近的研究集中于生成的文本并计算困惑度，这些困惑度是表面特征，并不可靠。在本研究中，我们建议通过检查模型的内部激活来利用探测技术进行预训练数据检测。我们的方法简单有效，可以实现更可靠的预训练数据检测。此外，我们提出了 ArxivMIA，这是一个新的具有挑战性的基准，包括来自计算机科学和数学类别的 arxiv 摘要。我们的实验表明，我们的方法优于所有基线，并在 WikiMIA 和 ArxivMIA 上都实现了最先进的性能，其他实验证实了它的有效性（我们的代码和数据集可在此 https URL 上找到）。</li>
</ul>

<h3>Title: R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, Yuanxing Zhang, Wenbo Su, Bangyu Xiang, Tiezheng Ge, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01359">https://arxiv.org/abs/2406.01359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01359">https://arxiv.org/pdf/2406.01359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01359]] R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models(https://arxiv.org/abs/2406.01359)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Code completion models have made significant progress in recent years. Recently, repository-level code completion has drawn more attention in modern software development, and several baseline methods and benchmarks have been proposed. However, existing repository-level code completion methods often fall short of fully using the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies. Besides, the existing benchmarks usually focus on limited code completion scenarios, which cannot reflect the repository-level code completion abilities well of existing methods. To address these limitations, we propose the R2C2-Coder to enhance and benchmark the real-world repository-level code completion abilities of code Large Language Models, where the R2C2-Coder includes a code prompt construction method R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically, first, in R2C2-Enhance, we first construct the candidate retrieval pool and then assemble the completion prompt by retrieving from the retrieval pool for each completion cursor position. Second, based on R2C2 -Enhance, we can construct a more challenging and diverse R2C2-Bench with training, validation and test splits, where a context perturbation strategy is proposed to simulate the real-world repository-level code completion well. Extensive results on multiple benchmarks demonstrate the effectiveness of our R2C2-Coder.</li>
<li><strong>摘要：</strong>近年来，代码补全模型取得了长足的进步。近年来，存储库级代码补全在现代软件开发中引起了更多的关注，并且已经提出了一些基线方法和基准测试。然而，现有的存储库级代码补全方法往往没有充分利用项目存储库的广泛上下文，例如相关文件和类层次结构的复杂性。此外，现有的基准测试通常侧重于有限的代码补全场景，不能很好地反映现有方法的存储库级代码补全能力。针对这些限制，我们提出了 R2C2-Coder 来增强和对标现实世界中的代码大型语言模型的存储库级代码补全能力，其中 R2C2-Coder 包括代码提示构造方法 R2C2-Enhance 和精心设计的基准测试 R2C2-Bench。具体而言，首先，在 R2C2-Enhance 中，我们首先构造候选检索池，然后通过从检索池中检索每个补全光标位置来组装补全提示。其次，基于 R2C2 -Enhance，我们可以构建一个更具挑战性和多样性的 R2C2-Bench，其中包含训练、验证和测试拆分，其中提出了一种上下文扰动策略，可以很好地模拟现实世界存储库级代码完成。多个基准测试的大量结果证明了我们的 R2C2-Coder 的有效性。</li>
</ul>

<h3>Title: Privacy in LLM-based Recommendation: Recent Advances and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Sichun Luo, Wei Shao, Yuxuan Yao, Jian Xu, Mingyang Liu, Qintong Li, Bowei He, Maolin Wang, Guanzhi Deng, Hanxu Hou, Xinyi Zhang, Linqi Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01363">https://arxiv.org/abs/2406.01363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01363">https://arxiv.org/pdf/2406.01363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01363]] Privacy in LLM-based Recommendation: Recent Advances and Future Directions(https://arxiv.org/abs/2406.01363)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Nowadays, large language models (LLMs) have been integrated with conventional recommendation models to improve recommendation performance. However, while most of the existing works have focused on improving the model performance, the privacy issue has only received comparatively less attention. In this paper, we review recent advancements in privacy within LLM-based recommendation, categorizing them into privacy attacks and protection mechanisms. Additionally, we highlight several challenges and propose future directions for the community to address these critical problems.</li>
<li><strong>摘要：</strong>如今，大型语言模型 (LLM) 已与传统推荐模型集成以提高推荐性能。然而，虽然大多数现有工作都集中在提高模型性能上，但隐私问题却受到的关注相对较少。在本文中，我们回顾了基于 LLM 的推荐中隐私方面的最新进展，将其分为隐私攻击和保护机制。此外，我们强调了几个挑战，并提出了社区解决这些关键问题的未来方向。</li>
</ul>

<h3>Title: D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, Xu Tan, Jie Fu, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01375">https://arxiv.org/abs/2406.01375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01375">https://arxiv.org/pdf/2406.01375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01375]] D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models(https://arxiv.org/abs/2406.01375)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model's fundamental understanding of specific downstream domains (e.g., math and code). For the CPT on domain-specific LLMs, one important question is how to choose the optimal mixture ratio between the general-corpus (e.g., Dolma, Slim-pajama) and the downstream domain-corpus. Existing methods usually adopt laborious human efforts by grid-searching on a set of mixture ratios, which require high GPU training consumption costs. Besides, we cannot guarantee the selected ratio is optimal for the specific domain. To address the limitations of existing methods, inspired by the Scaling Law for performance prediction, we propose to investigate the Scaling Law of the Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal mixture ratio with acceptable training costs for LLMs of different sizes. Specifically, by fitting the D-CPT Law, we can easily predict the general and downstream performance of arbitrary mixture ratios, model sizes, and dataset sizes using small-scale training costs on limited experiments. Moreover, we also extend our standard D-CPT Law on cross-domain settings and propose the Cross-Domain D-CPT Law to predict the D-CPT law of target domains, where very small training costs (about 1% of the normal training costs) are needed for the target domains. Comprehensive experimental results on six downstream domains demonstrate the effectiveness and generalizability of our proposed D-CPT Law and Cross-Domain D-CPT Law.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 上的持续预训练 (CPT) 已被广泛用于扩展模型对特定下游领域 (例如数学和代码) 的基本理解。对于领域特定 LLM 上的 CPT，一个重要的问题是如何选择通用语料库 (例如 Dolma、Slim-pajama) 和下游领域语料库之间的最佳混合比。现有方法通常通过在一组混合比上进行网格搜索来采取费力的人力，这需要很高的 GPU 训练消耗成本。此外，我们无法保证所选比例对于特定领域是最佳的。为了解决现有方法的局限性，受性能预测的缩放定律的启发，我们建议研究领域特定持续预训练的缩放定律 (D-CPT 定律)，以确定不同规模的 LLM 具有可接受的训练成本的最佳混合比。具体来说，通过拟合 D-CPT 定律，我们可以轻松地在有限的实验中使用小规模训练成本预测任意混合比、模型大小和数据集大小的一般和下游性能。此外，我们还在跨域设置上扩展了标准 D-CPT 定律，并提出了跨域 D-CPT 定律来预测目标域的 D-CPT 定律，其中目标域需要的训练成本非常小（约为正常训练成本的 1%）。在六个下游域上的综合实验结果证明了我们提出的 D-CPT 定律和跨域 D-CPT 定律的有效性和通用性。</li>
</ul>

<h3>Title: Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function</h3>
<ul>
<li><strong>Authors: </strong>Keyon Vafa, Ashesh Rambachan, Sendhil Mullainathan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01382">https://arxiv.org/abs/2406.01382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01382">https://arxiv.org/pdf/2406.01382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01382]] Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function(https://arxiv.org/abs/2406.01382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, we must understand the purposes they will be used for. We consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. We model such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. We collect a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. We show that the human generalization function can be predicted using NLP methods: people have consistent structured ways to generalize. We then evaluate LLM alignment with the human generalization function. Our results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 令人印象深刻的原因也是难以评估的原因：它们的用途多种多样。要评估这些模型，我们必须了解它们的用途。我们考虑这样一种情况，其中这些部署决策是由人做出的，特别是人们对 LLM 表现良好的信念。我们将这种信念建模为人类泛化功能的结果：在看到 LLM 的正确或错误之后，人们会将其推广到其他可能成功的地方。我们从 MMLU 和 BIG-Bench 基准测试中收集了一个包含 19K 个示例的数据集，这些示例展示了人类如何在 79 个任务中进行泛化。我们表明，可以使用 NLP 方法预测人类泛化功能：人们有一致的结构化泛化方式。然后，我们评估 LLM 与人类泛化功能的一致性。我们的结果表明——尤其是在错误成本高的情况下——更强大的模型（例如 GPT-4）在人们选择使用它们的实例上表现更差，正是因为它们与人类泛化功能不一致。</li>
</ul>

<h3>Title: Sparsity-Accelerated Training for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Da Ma, Lu Chen, Pengyu Wang, Hongshen Xu, Hanqi Li, Liangtai Sun, Su Zhu, Shuai Fan, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01392">https://arxiv.org/abs/2406.01392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01392">https://arxiv.org/pdf/2406.01392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01392]] Sparsity-Accelerated Training for Large Language Models(https://arxiv.org/abs/2406.01392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging \emph{sparsity} in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process. Specifically, SAT achieves a $45\%$ throughput improvement in continual pre-training and saves $38\%$ training time in supervised fine-tuning in practice. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已证明其在各种自然语言处理 (NLP) 任务中表现出色，但通常需要额外的训练，例如持续的预训练和监督微调。然而，与此相关的成本仍然很高，主要是因为它们的参数数量很大。本文建议利用预训练 LLM 中的 \emph{sparsity} 来加快这一训练过程。通过在前向迭代期间观察激活神经元中的稀疏性，我们确定了通过排除非活动神经元来提高计算速度的潜力。我们通过扩展现有的神经元重要性评估指标并引入阶梯遗漏率调度程序来解决相关挑战。我们在 Llama-2 上的实验表明，稀疏加速训练 (SAT) 实现了与标准训练相当或更优异的性能，同时显著加快了该过程。具体而言，SAT 在持续预训练中实现了 45% 的吞吐量提升，并在实践中节省了 38% 的监督微调训练时间。它为额外的 LLM 培训提供了一个简单、与硬件无关且易于部署的框架。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Superhuman performance in urology board questions by an explainable large language model enabled for context integration of the European Association of Urology guidelines: the UroBot study</h3>
<ul>
<li><strong>Authors: </strong>Martin J. Hetz, Nicolas Carl, Sarah Haggenmüller, Christoph Wies, Maurice Stephan Michel, Frederik Wessels, Titus J. Brinker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01428">https://arxiv.org/abs/2406.01428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01428">https://arxiv.org/pdf/2406.01428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01428]] Superhuman performance in urology board questions by an explainable large language model enabled for context integration of the European Association of Urology guidelines: the UroBot study(https://arxiv.org/abs/2406.01428)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are revolutionizing medical Question-Answering (medQA) through extensive use of medical literature. However, their performance is often hampered by outdated training data and a lack of explainability, which limits clinical applicability. This study aimed to create and assess UroBot, a urology-specialized chatbot, by comparing it with state-of-the-art models and the performance of urologists on urological board questions, ensuring full clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4, and GPT-4o models, employing retrieval-augmented generation (RAG) and the latest 2023 guidelines from the European Association of Urology (EAU). The evaluation included ten runs of 200 European Board of Urology (EBU) In-Service Assessment (ISA) questions, with performance assessed by the mean Rate of Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979). By comparison, the average performance of urologists on board questions, as reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and superior accuracy compared to both existing models and urologists on board questions highlight its potential for clinical integration. The study also provides the necessary code and instructions for further development of UroBot.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 正在通过广泛使用医学文献彻底改变医学问答 (medQA)。然而，它们的性能往往受到过时的训练数据和缺乏可解释性的阻碍，从而限制了其临床适用性。这项研究旨在创建和评估泌尿科专业聊天机器人 UroBot，将其与最先进的模型和泌尿科医生在泌尿科委员会问题上的表现进行比较，确保完全的临床可验证性。UroBot 是使用 OpenAI 的 GPT-3.5、GPT-4 和 GPT-4o 模型开发的，采用了检索增强生成 (RAG) 和欧洲泌尿外科协会 (EAU) 2023 年的最新指南。评估包括 10 次 200 个欧洲泌尿外科委员会 (EBU) 在职评估 (ISA) 问题，并通过平均正确答案率 (RoCA) 来评估表现。 UroBot-4o 的平均 RoCA 为 88.4%，比 GPT-4o 高出 10.8%，得分为 77.6%。它还可通过临床医生验证，并表现出最高的运行一致性，如 Fleiss' Kappa（k = 0.979）所示。相比之下，文献中报道的泌尿科医生在船上问题的平均表现为 68.7%。与现有模型和泌尿科医生在船上问题相比，UroBot 的临床医生可验证性和卓越的准确性凸显了其临床整合的潜力。该研究还为 UroBot 的进一步开发提供了必要的代码和说明。</li>
</ul>

<h3>Title: Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Hsun Hsueh, Paul Kuo-Ming Huang, Tzu-Han Lin, Che-Wei Liao, Hung-Chieh Fang, Chao-Wei Huang, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01436">https://arxiv.org/abs/2406.01436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01436">https://arxiv.org/pdf/2406.01436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01436]] Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models(https://arxiv.org/abs/2406.01436)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge editing is a rising technique for efficiently updating factual knowledge in Large Language Models (LLMs) with minimal alteration of parameters. However, recent studies have identified concerning side effects, such as knowledge distortion and the deterioration of general abilities, that have emerged after editing. This survey presents a comprehensive study of these side effects, providing a unified view of the challenges associated with knowledge editing in LLMs. We discuss related works and summarize potential research directions to overcome these limitations. Our work highlights the limitations of current knowledge editing methods, emphasizing the need for deeper understanding of inner knowledge structures of LLMs and improved knowledge editing methods. To foster future research, we have released the complementary materials such as paper collection publicly at this https URL</li>
<li><strong>摘要：</strong>知识编辑是一种新兴技术，用于在参数更改最少的情况下有效更新大型语言模型 (LLM) 中的事实知识。然而，最近的研究发现了编辑后出现的令人担忧的副作用，例如知识扭曲和一般能力下降。本调查对这些副作用进行了全面的研究，提供了与 LLM 中的知识编辑相关的挑战的统一观点。我们讨论了相关工作并总结了克服这些限制的潜在研究方向。我们的工作突出了当前知识编辑方法的局限性，强调需要更深入地了解 LLM 的内部知识结构并改进知识编辑方法。为了促进未来的研究，我们已在此 https URL 上公开发布了补充材料，例如论文集</li>
</ul>

<h3>Title: LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01441">https://arxiv.org/abs/2406.01441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01441">https://arxiv.org/pdf/2406.01441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01441]] LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation(https://arxiv.org/abs/2406.01441)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The fine-tuning of open-source large language models (LLMs) for machine translation has recently received considerable attention, marking a shift towards data-centric research from traditional neural machine translation. However, the area of data collection for instruction fine-tuning in machine translation remains relatively underexplored. In this paper, we present LexMatcher, a simple yet effective method for data collection that leverages bilingual dictionaries to generate a dataset, the design of which is driven by the coverage of senses found in these dictionaries. The dataset comprises a subset retrieved from an existing corpus and a smaller synthesized subset which supplements the infrequent senses of polysemous words. Utilizing LLaMA2 as our base model, our approach outperforms the established baselines on the WMT2022 test sets and also exhibits significant performance improvements in tasks related to word sense disambiguation and specialized terminology translation. These results underscore the effectiveness of LexMatcher in enhancing LLM-based machine translation.</li>
<li><strong>摘要：</strong>开源大型语言模型 (LLM) 的机器翻译微调最近受到了广泛关注，标志着从传统的神经机器翻译转向以数据为中心的研究。然而，机器翻译中指令微调的数据收集领域仍然相对未被充分探索。在本文中，我们介绍了 LexMatcher，这是一种简单而有效的数据收集方法，它利用双语词典来生成数据集，其设计由这些词典中发现的意义的覆盖范围驱动。数据集包括从现有语料库中检索到的子集和较小的合成子集，后者补充了多义词的不常见意义。利用 LLaMA2 作为我们的基础模型，我们的方法在 WMT2022 测试集上的表现优于既定的基线，并且在与词义消歧和专业术语翻译相关的任务中也表现出显着的性能改进。这些结果强调了 LexMatcher 在增强基于 LLM 的机器翻译方面的有效性。</li>
</ul>

<h3>Title: Understanding Token Probability Encoding in Output Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Hakaze Cho, Yoshihiro Sakai, Kenshiro Tanaka, Mariko Kato, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01468">https://arxiv.org/abs/2406.01468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01468">https://arxiv.org/pdf/2406.01468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01468]] Understanding Token Probability Encoding in Output Embeddings(https://arxiv.org/abs/2406.01468)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the output token probability information in the output embedding of language models. We provide an approximate common log-linear encoding of output token probabilities within the output embedding vectors and demonstrate that it is accurate and sparse when the output space is large and output logits are concentrated. Based on such findings, we edit the encoding in output embedding to modify the output probability distribution accurately. Moreover, the sparsity we find in output probability encoding suggests that a large number of dimensions in the output embedding do not contribute to causal language modeling. Therefore, we attempt to delete the output-unrelated dimensions and find more than 30% of the dimensions can be deleted without significant movement in output distribution and degeneration on sequence generation. Additionally, in training dynamics, we use such encoding as a probe and find that the output embeddings capture token frequency information in early steps, even before an obvious convergence starts.</li>
<li><strong>摘要：</strong>在本文中，我们研究了语言模型输出嵌入中的输出标记概率信息。我们提供了输出嵌入向量中输出标记概率的近似公共对数线性编码，并证明当输出空间很大且输出对数集中时，它是准确且稀疏的。基于这些发现，我们编辑了输出嵌入中的编码以准确地修改输出概率分布。此外，我们在输出概率编码中发现的稀疏性表明输出嵌入中的大量维度对因果语言建模没有贡献。因此，我们尝试删除与输出无关的维度，发现可以删除超过 30% 的维度，而不会导致输出分布发生显著变化和序列生成退化。此外，在训练动态中，我们使用这种编码作为探测，发现输出嵌入在早期步骤中捕获了标记频率信息，甚至在明显的收敛开始之前。</li>
</ul>

<h3>Title: Reflection-Reinforced Self-Training for Language Agents</h3>
<ul>
<li><strong>Authors: </strong>Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Reflection-Reinforced Self-Training for Language Agents(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Self-training can potentially improve the performance of language agents without relying on demonstrations from humans or stronger models. The general process involves generating samples from a model, evaluating their quality, and updating the model by training on high-quality samples. However, self-training can face limitations because achieving good performance requires a good amount of high-quality samples, yet relying solely on model sampling for obtaining such samples can be inefficient. In addition, these methods often disregard low-quality samples, failing to leverage them effectively. To address these limitations, we present Reflection-Reinforced Self-Training (Re-ReST), which leverages a reflection model to refine low-quality samples and subsequently uses these improved samples to augment self-training. The reflection model takes both the model output and feedback from an external environment (e.g., unit test results in code generation) as inputs and produces improved samples as outputs. By employing this technique, we effectively enhance the quality of inferior samples, and enrich the self-training dataset with higher-quality samples efficiently. We perform extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. Results demonstrate improvements over self-training baselines across settings. Moreover, ablation studies confirm the reflection model's efficiency in generating quality self-training samples and its compatibility with self-consistency decoding.</li>
<li><strong>摘要：</strong>自训练可以潜在地提高语言代理的性能，而无需依赖人类或更强大的模型的演示。一般过程包括从模型生成样本、评估其质量以及通过对高质量样本进行训练来更新模型。然而，自训练可能会面临局限性，因为要获得良好的性能需要大量高质量样本，而仅仅依靠模型采样来获取此类样本效率低下。此外，这些方法通常会忽略低质量样本，无法有效利用它们。为了解决这些限制，我们提出了反射强化自训练 (Re-ReST)，它利用反射模型来改进低质量样本，然后使用这些改进的样本来增强自训练。反射模型将模型输出和来自外部环境的反馈（例如，代码生成中的单元测试结果）作为输入，并产生改进的样本作为输出。通过采用这种技术，我们有效地提高了劣质样本的质量，并有效地用更高质量的样本丰富了自训练数据集。我们对开源语言代理进行了广泛的实验，涉及多个任务，包括多跳问答、顺序决策、代码生成、视觉问答和文本到图像生成。结果表明，在各种设置下，自训练基线均有所改进。此外，消融研究证实了反射模型在生成高质量自训练样本方面的效率及其与自洽解码的兼容性。</li>
</ul>

<h3>Title: The Geometry of Categorical and Hierarchical Concepts in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01506">https://arxiv.org/abs/2406.01506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01506">https://arxiv.org/pdf/2406.01506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01506]] The Geometry of Categorical and Hierarchical Concepts in Large Language Models(https://arxiv.org/abs/2406.01506)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as {'mammal', 'bird', 'reptile', 'fish'}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.</li>
<li><strong>摘要：</strong>理解语义意义如何在大型语言模型的表示空间中编码是可解释性的一个基本问题。在本文中，我们研究了该领域的两个基本问题。首先，如何表示分类概念，例如{“哺乳动物”、“鸟类”、“爬行动物”、“鱼类”}？其次，如何编码概念之间的层次关系？例如，“狗”是一种“哺乳动物”这一事实是如何编码的？我们展示了如何扩展线性表示假设来回答这些问题。我们发现了一个非常简单的结构：简单的分类概念表示为单纯形，层次相关概念在我们精确的意义上是正交的，并且（因此）复杂概念表示为由单纯形的直接和构成的多面体，反映了层次结构。我们在 Gemma 大型语言模型上验证了这些理论结果，使用来自 WordNet 的数据估计了 957 个层次相关概念的表示。</li>
</ul>

<h3>Title: Decoupled Alignment for Robust Plug-and-Play Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Haozheng Luo, Jiahao Yu, Wenxin Zhang, Jialong Li, Jerry Yao-Chieh Hu, Xingyu Xin, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01514">https://arxiv.org/abs/2406.01514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01514">https://arxiv.org/pdf/2406.01514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01514]] Decoupled Alignment for Robust Plug-and-Play Adaptation(https://arxiv.org/abs/2406.01514)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce a low-resource safety enhancement method for aligning large language models (LLMs) without the need for supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF). Our main idea is to exploit knowledge distillation to extract the alignment information from existing well-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play fashion. Methodology, we employ delta debugging to identify the critical components of knowledge necessary for effective distillation. On the harmful question dataset, our method significantly enhances the average defense success rate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned pre-trained LLMs, without compromising performance.</li>
<li><strong>摘要：</strong>我们引入了一种低资源安全增强方法，用于对齐大型语言模型 (LLM)，而无需监督微调 (SFT) 或从人工反馈中进行强化学习 (RLHF)。我们的主要思想是利用知识蒸馏从现有的对齐良好的 LLM 中提取对齐信息，并以即插即用的方式将其集成到未对齐的 LLM 中。方法论中，我们采用增量调试来识别有效蒸馏所需的关键知识组件。在有害问题数据集上，我们的方法在 17 个未对齐的预训练 LLM 中显著提高了平均防御成功率约 14.41%，高达 51.39%，而不会影响性能。</li>
</ul>

<h3>Title: What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores</h3>
<ul>
<li><strong>Authors: </strong>Ebrahim Feghhi, Nima Hadidi, Bryan Song, Idan A. Blank, Jonathan C. Kao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01538">https://arxiv.org/abs/2406.01538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01538">https://arxiv.org/pdf/2406.01538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01538]] What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores(https://arxiv.org/abs/2406.01538)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called "brain score". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore use contiguous splits moving forward. Second, we explain the surprisingly high brain scores of untrained LLMs by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.</li>
<li><strong>摘要：</strong>鉴于大型语言模型 (LLM) 的卓越能力，人们越来越有兴趣评估它们与人脑的相似性。量化这种相似性的一种方法是测量模型预测神经信号的能力，也称为“大脑得分”。LLM 的内部表征实现了最先进的大脑得分，这导致人们推测它们与人类语言处理共享计算原理。只有当 LLM 预测的神经活动子集反映语言处理的核心元素时，这种推论才有效。在这里，我们通过分析一项影响深远的 LLM 到大脑映射研究中使用的三个神经数据集来质疑这一假设，特别关注参与者阅读短文的 fMRI 数据集。我们首先发现，当使用打乱的训练测试分割时，就像之前对这些数据集的研究一样，编码时间自相关的一个微不足道的特征不仅优于 LLM，而且还解释了 LLM 解释的大部分神经方差。因此，我们今后将使用连续的分割。其次，我们通过表明未经训练的 LLM 无法解释除两个简单特征（句子长度和句子位置）之外的其他神经方差来解释其出人意料的高大脑得分。这削弱了声称 Transformer 架构使计算更像大脑的证据。第三，我们发现，该数据集上经过训练的 LLM 的大脑得分主要可以通过句子长度、位置和代词去引用的静态词嵌入来解释；一小部分额外的得分可以通过特定意义的嵌入和句子结构的上下文表示来解释。我们得出结论，过度依赖大脑得分会导致对 LLM 和大脑之间相似性的过度解读，并强调了解构 LLM 在神经信号中映射到什么的重要性。</li>
</ul>

<h3>Title: An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01549">https://arxiv.org/abs/2406.01549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01549">https://arxiv.org/pdf/2406.01549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01549]] An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation(https://arxiv.org/abs/2406.01549)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\%$ compression rate.</li>
<li><strong>摘要：</strong>检索增强生成将大型语言模型的功能与从大量语料库中检索到的相关信息相结合，但在面对现实世界的噪声数据时仍会遇到挑战。最近的一种解决方案是训练一个过滤模块来查找相关内容，但只能实现次优的噪声压缩。在本文中，我们建议将信息瓶颈理论引入检索增强生成。我们的方法通过同时最大化压缩和基础输出之间的互信息，同时最小化压缩和检索到的段落之间的互信息来过滤噪声。此外，我们推导了信息瓶颈的公式，以促进其在新型综合评估、监督微调数据的选择和强化学习奖励的构建中的应用。实验结果表明，我们的方法在各种问答数据集上都取得了显着的改进，不仅在答案生成的正确性方面，而且在压缩率为 $2.5\%$ 的简洁性方面也是如此。</li>
</ul>

<h3>Title: LoFiT: Localized Fine-tuning on LLM Representations</h3>
<ul>
<li><strong>Authors: </strong>Fangcong Yin, Xi Ye, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01563">https://arxiv.org/abs/2406.01563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01563">https://arxiv.org/pdf/2406.01563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01563]] LoFiT: Localized Fine-tuning on LLM Representations(https://arxiv.org/abs/2406.01563)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, for the tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods.</li>
<li><strong>摘要：</strong>可解释性方面的最新研究表明，大型语言模型 (LLM) 可以以无需学习的方式适应新任务：可以对 LLM 表示进行干预以引发所需的对齐行为。例如，据报道，将某些偏差向量添加到某些注意力头的输出中可以提高模型的真实性。在这项工作中，我们表明局部微调是此类表示干预方法的有效替代方案。我们引入了一个称为 LLM 表示局部微调 (LoFiT) 的框架，该框架可识别对学习特定任务最重要的注意力头子集，然后训练偏移向量以添加到这些选定头的模型隐藏表示中。LoFiT 定位到一组稀疏的头部 (3%) 并从有限的训练数据中学习偏移向量，与用于表示干预的设置相当。对于真实性和推理任务，我们发现 LoFiT 的干预向量对于 LLM 适应比来自表示干预方法（例如推理时间干预）的向量更有效。我们还发现定位步骤很重要：选择一组特定于任务的注意力头可以带来比干预为其他任务选择的注意力头更高的性能。最后，对于我们研究的任务，LoFiT 实现了与其他参数高效的微调方法（如 LoRA）相当的性能，尽管修改的参数比这些方法少 20 到 200 倍。</li>
</ul>

<h3>Title: MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2406.01574">https://arxiv.org/abs/2406.01574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2406.01574">https://arxiv.org/pdf/2406.01574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2406.01574]] MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark(https://arxiv.org/abs/2406.01574)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.</li>
<li><strong>摘要：</strong>在大规模语言模型时代，大规模多任务语言理解 (MMLU) 等基准测试对于推动 AI 在不同领域的语言理解和推理能力的极限至关重要。然而，随着模型的不断改进，它们在这些基准测试中的表现已开始趋于稳定，使得辨别模型能力的差异变得越来越困难。本文介绍了 MMLU-Pro，这是一个增强型数据集，旨在通过整合更具挑战性、以推理为重点的问题并将选择集从四个选项扩展到十个选项来扩展主要由知识驱动的 MMLU 基准测试。此外，MMLU-Pro 消除了 MMLU 中琐碎和嘈杂的问题。我们的实验结果表明，MMLU-Pro 不仅增加了挑战性，导致准确率与 MMLU 相比显著下降 16% 至 33%，而且在不同提示下表现出更高的稳定性。通过测试 24 种不同的提示风格，模型分数对提示变化的敏感度从 MMLU 中的 4-5% 下降到 MMLU-Pro 中的仅 2%。此外，我们发现，与直接回答相比，使用思维链 (CoT) 推理的模型在 MMLU-Pro 上的表现更好，这与原始 MMLU 的结果形成鲜明对比，表明 MMLU-Pro 包含更复杂的推理问题。我们的评估证实，MMLU-Pro 是一个更具辨别力的基准，可以更好地跟踪该领域的进展。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
