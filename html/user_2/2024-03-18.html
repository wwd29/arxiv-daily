<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-18</h1>
<h3>Title: Navigating the Peril of Generated Alternative Facts: A ChatGPT-4  Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Malik Sallam, Jan Egger, Rainer Roehrig, Behrus Puladi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09674">https://arxiv.org/abs/2403.09674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09674">https://arxiv.org/pdf/2403.09674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09674]] Navigating the Peril of Generated Alternative Facts: A ChatGPT-4  Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation(https://arxiv.org/abs/2403.09674)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>In an era where artificial intelligence (AI) intertwines with medical research, the delineation of truth becomes increasingly complex. This study ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega variant, showcasing 31 unique mutations in the S gene region. However, the real undercurrent of this narrative is a demonstration of the ease with which AI, specifically ChatGPT-4, can fabricate convincing yet entirely fictional scientific data. The so-called Omega variant was identified in a fully vaccinated, previously infected 35-year-old male presenting with severe COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and contact tracing, this study mirrors the rigorous methodology of genuine case reports, thereby setting the stage for a compelling but entirely constructed narrative. The entire case study was generated by ChatGPT-4, a large language model by OpenAI. The fabricated Omega variant features an ensemble of mutations, including N501Y and E484K, known for enhancing ACE2 receptor affinity, alongside L452R and P681H, ostensibly indicative of immune evasion. This variant's contrived interaction dynamics - severe symptoms in a vaccinated individual versus mild ones in unvaccinated contacts - were designed to mimic real-world complexities, including suggestions of antibody-dependent enhancement (ADE). While the Omega variant is a product of AI-generated fiction, the implications of this exercise are real and profound. The ease with which AI can generate believable but false scientific information, as illustrated in this case, raises significant concerns about the potential for misinformation in medicine. This study, therefore, serves as a cautionary tale, emphasizing the necessity for critical evaluation of sources, especially in an age where AI tools like ChatGPT are becoming increasingly sophisticated and widespread in their use.</li>
<li><strong>摘要：</strong>在人工智能（AI）与医学研究交织在一起的时代，真相的界定变得越来越复杂。这项研究表面上检查了一种所谓的新型 SARS-CoV-2 变体，称为 Omega 变体，展示了 S 基因区域的 31 个独特突变。然而，这种叙述的真正潜流是人工智能，特别是 ChatGPT-4，可以轻松地制造令人信服但完全虚构的科学数据。所谓的 Omega 变种是在一名完全接种疫苗、之前感染过的 35 岁男性中发现的，该男性出现了严重的 COVID-19 症状。通过详细的、尽管是人为的基因组分析和接触者追踪，这项研究反映了真实病例报告的严格方法，从而为令人信服但完全构建的叙述奠定了基础。整个案例研究由 OpenAI 的大型语言模型 ChatGPT-4 生成。制造的 Omega 变体具有一系列突变，包括以增强 ACE2 受体亲和力而闻名的 N501Y 和 E484K，以及表面上表明免疫逃避的 L452R 和 P681H。这种变体的人为交互动态——接种疫苗的个体出现严重症状，而未接种疫苗的接触者出现轻微症状——旨在模拟现实世界的复杂性，包括抗体依赖性增强（ADE）的建议。虽然欧米茄变体是人工智能生成的小说的产物，但这一练习的含义是真实而深刻的。正如本例所示，人工智能可以轻松生成可信但虚假的科学信息，这引起了人们对医学中可能出现错误信息的严重担忧。因此，这项研究起到了警示作用，强调了对来源进行严格评估的必要性，特别是在像 ChatGPT 这样的人工智能工具变得越来越复杂和广泛使用的时代。</li>
</ul>

<h3>Title: Unmasking the Shadows of AI: Investigating Deceptive Capabilities in  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linge Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09676">https://arxiv.org/abs/2403.09676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09676">https://arxiv.org/pdf/2403.09676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09676]] Unmasking the Shadows of AI: Investigating Deceptive Capabilities in  Large Language Models(https://arxiv.org/abs/2403.09676)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, and specific elements of digital education.</li>
<li><strong>摘要：</strong>这项研究批判性地探讨了人工智能欺骗的复杂情况，重点关注大型语言模型（LLM）的欺骗行为。我的目标是阐明这个问题，研究围绕它的讨论，然后深入研究它的分类和后果。本文首先对 2023 年人工智能安全峰会 (ASS) 进行了评估，并介绍了法学硕士，强调了其欺骗行为背后的多维偏见。文献综述涵盖了四种类型的欺骗：战略欺骗、模仿、阿谀奉承和不忠实推理，以及它们带来的社会影响和风险。最后，我对与应对欺骗性人工智能持续挑战相关的各个方面采取了评估立场。这包括对国际协作治理的考虑、个人与人工智能的重新配置、实际调整的建议以及数字教育的具体要素。</li>
</ul>

<h3>Title: Generator-Guided Crowd Reaction Assessment</h3>
<ul>
<li><strong>Authors: </strong>Sohom Ghosh, Chung-Chi Chen, Sudip Kumar Naskar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09702">https://arxiv.org/abs/2403.09702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09702">https://arxiv.org/pdf/2403.09702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09702]] Generator-Guided Crowd Reaction Assessment(https://arxiv.org/abs/2403.09702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate GGEA's ability to predict which post will elicit the most reactions. We believe this novel application of LLMs provides a significant advancement in predicting social media post reach.</li>
<li><strong>摘要：</strong>在社交媒体领域，理解和预测帖子覆盖率是一项重大挑战。本文提出了一项群体反应评估 (CReAM) 任务，旨在估计特定社交媒体帖子是否会比其他帖子收到更多反应，这对于数字营销人员和内容作者来说是一项特别重要的任务。我们引入了人群反应估计数据集（CRED），其中包含来自白宫的推文对以及转发计数的比较度量。所提出的生成器引导估计方法 (GGEA) 利用生成式大型语言模型 (LLM)（例如 ChatGPT、FLAN-UL2 和 Claude）来指导分类模型做出更好的预测。我们的结果表明，经过微调的 FLANG-RoBERTa 模型利用交叉编码器架构以及 Claude 生成的推文内容和响应，表现最佳。我们进一步使用基于 T5 的释义器来生成给定帖子的释义，并证明 GGEA 能够预测哪个帖子会引起最多的反应。我们相信法学硕士的这种新颖应用在预测社交媒体帖子覆盖率方面取得了重大进展。</li>
</ul>

<h3>Title: Concept-aware Data Construction Improves In-context Learning of Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Michal Štefánik, Marek Kadlčík, Petr Sojka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09703">https://arxiv.org/abs/2403.09703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09703">https://arxiv.org/pdf/2403.09703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09703]] Concept-aware Data Construction Improves In-context Learning of Language  Models(https://arxiv.org/abs/2403.09703)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings. In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functional deficiencies of the previous models. Finally, we show that concept-aware in-context learning is more effective for a majority of new tasks when compared to traditional instruction tuning, resulting in a performance comparable to the previous in-context learners using magnitudes of more training data.</li>
<li><strong>摘要：</strong>许多最近的语言模型 (LM) 都能够进行上下文学习 (ICL)，这体现在 LM 能够仅通过自然语言指令执行新任务。之前策划情境学习者的工作假设 ICL 源于大量的过度参数化或多任务训练的规模。然而，最近的理论工作将 ICL 能力归因于概念相关的训练数据，即使在小规模的综合环境中也能创建功能性的上下文学习器。在这项工作中，我们实际探索了这个新确定的 ICL 质量轴。我们提出了概念感知训练（CoAT），这是一个用于构建训练场景的框架，使 LM 有利于学习利用演示中的类比推理概念。我们发现，通过使用 CoAT，预训练的 Transformer 可以学习更好地利用演示中的新潜在概念，并且这种能力使 ICL 对于先前模型的功能缺陷更加稳健。最后，我们表明，与传统的指令调整相比，概念感知的上下文学习对于大多数新任务来说更有效，其性能与之前使用大量训练数据的上下文学习器相当。</li>
</ul>

<h3>Title: Alignment Studio: Aligning Large Language Models to Particular  Contextual Regulations</h3>
<ul>
<li><strong>Authors: </strong>Swapnaja Achintalwar, Ioana Baldini, Djallel Bouneffouf, Joan Byamugisha, Maria Chang, Pierre Dognin, Eitan Farchi, Ndivhuwo Makondo, Aleksandra Mojsilovic, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Inkit Padhi, Orna Raz, Jesus Rios, Prasanna Sattigeri, Moninder Singh, Siphiwe Thwala, Rosario A. Uceda-Sosa, Kush R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09704">https://arxiv.org/abs/2403.09704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09704">https://arxiv.org/pdf/2403.09704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09704]] Alignment Studio: Aligning Large Language Models to Particular  Contextual Regulations(https://arxiv.org/abs/2403.09704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.</li>
<li><strong>摘要：</strong>大型语言模型的对齐通常由模型提供者完成，以添加或控制跨用例和上下文常见或普遍理解的行为。相比之下，在本文中，我们提出了一种方法和架构，使应用程序开发人员能够根据其特定价值观、社会规范、法律和其他法规调整模型，并在上下文中协调潜在冲突的需求。我们列出了这种 Alignment Studio 架构的三个主要组件：Framers、Instructors 和 Auditors，它们协同工作来控制语言模型的行为。我们通过一个运行示例来说明这种方法，该示例将公司面向内部的企业聊天机器人与其业务行为准则保持一致。</li>
</ul>

<h3>Title: A Novel Nuanced Conversation Evaluation Framework for Large Language  Models in Mental Health</h3>
<ul>
<li><strong>Authors: </strong>Alexander Marrapese, Basem Suleiman, Imdad Ullah, Juno Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09705">https://arxiv.org/abs/2403.09705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09705">https://arxiv.org/pdf/2403.09705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09705]] A Novel Nuanced Conversation Evaluation Framework for Large Language  Models in Mental Health(https://arxiv.org/abs/2403.09705)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question. In this paper, we propose a novel framework for evaluating the nuanced conversation abilities of LLMs. Within it, we develop a series of quantitative metrics developed from literature on using psychotherapy conversation analysis literature. While we ensure that our framework and metrics are transferable by researchers to relevant adjacent domains, we apply them to the mental health field. We use our framework to evaluate several popular frontier LLMs, including some GPT and Llama models, through a verified mental health dataset. Our results show that GPT4 Turbo can perform significantly more similarly to verified therapists than other selected LLMs. We conduct additional analysis to examine how LLM conversation performance varies across specific mental health topics. Our results indicate that GPT4 Turbo performs well in achieving high correlation with verified therapists in particular topics such as Parenting and Relationships. We believe our contributions will help researchers develop better LLMs that, in turn, will more positively support people's lives.</li>
<li><strong>摘要：</strong>了解大型语言模型 (LLM) 的对话能力有助于使其更加谨慎和适当的部署。这对于心理健康等安全关键领域尤其重要，在这些领域，某人的生命可能取决于对紧急问题的准确措辞。在本文中，我们提出了一个新的框架来评估法学硕士的细致入微的对话能力。在其中，我们开发了一系列根据使用心理治疗对话分析文献的文献开发的定量指标。虽然我们确保我们的框架和指标可以由研究人员转移到相关的相邻领域，但我们将它们应用到心理健康领域。我们使用我们的框架通过经过验证的心理健康数据集来评估几种流行的前沿法学硕士，包括一些 GPT 和 Llama 模型。我们的结果表明，与其他选定的法学硕士相比，GPT4 Turbo 的表现与经过验证的治疗师更加相似。我们进行了额外的分析，以检查法学硕士对话表现在特定心理健康主题上的差异。我们的结果表明，GPT4 Turbo 在与经过验证的治疗师在育儿和关系等特定主题上实现高度相关性方面表现良好。我们相信我们的贡献将帮助研究人员开发更好的法学硕士，进而更积极地支持人们的生活。</li>
</ul>

<h3>Title: A Knowledge-Injected Curriculum Pretraining Framework for Question  Answering</h3>
<ul>
<li><strong>Authors: </strong>Xin Lin, Tianhuang Su, Zhenya Huang, Shangzi Xue, Haifeng Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09712">https://arxiv.org/abs/2403.09712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09712">https://arxiv.org/pdf/2403.09712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09712]] A Knowledge-Injected Curriculum Pretraining Framework for Question  Answering(https://arxiv.org/abs/2403.09712)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge-based question answering (KBQA) is a key task in NLP research, and also an approach to access the web data and knowledge, which requires exploiting knowledge graphs (KGs) for reasoning. In the literature, one promising solution for KBQA is to incorporate the pretrained language model (LM) with KGs by generating KG-centered pretraining corpus, which has shown its superiority. However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with KGs, while neglect the more important human-like complex reasoning. To this end, in this paper, we propose a general Knowledge-Injected Curriculum Pretraining framework (KICP) to achieve comprehensive KG learning and exploitation for KBQA tasks, which is composed of knowledge injection (KI), knowledge adaptation (KA) and curriculum reasoning (CR). Specifically, the KI module first injects knowledge into the LM by generating KG-centered pretraining corpus, and generalizes the process into three key steps that could work with different implementations for flexible application. Next, the KA module learns knowledge from the generated corpus with LM equipped with an adapter as well as keeps its original natural language understanding ability to reduce the negative impacts of the difference between the generated and natural corpus. Last, to enable the LM with complex reasoning, the CR module follows human reasoning patterns to construct three corpora with increasing difficulties of reasoning, and further trains the LM from easy to hard in a curriculum manner. We provide an implementation of the general framework, and evaluate the proposed KICP on four real-word datasets. The results demonstrate that our framework can achieve higher performances.</li>
<li><strong>摘要：</strong>基于知识的问答（KBQA）是NLP研究的关键任务，也是一种访问网络数据和知识的方法，需要利用知识图（KG）进行推理。文献中，一种有前景的 KBQA 解决方案是将预训练语言模型（LM）与 KG 结合起来，生成以 KG 为中心的预训练语料库，这已经显示了其优越性。然而，这些方法通常依赖于特定的技术和资源来工作，而这些技术和资源可能并不总是可用并限制了其应用。此外，现有方法更多地关注利用 KG 来提高语言理解，而忽略了更重要的类人复杂推理。为此，在本文中，我们提出了一种通用的知识注入课程预训练框架（KICP），以实现针对 KBQA 任务的全面知识图谱学习和开发，该框架由知识注入（KI）、知识适应（KA）和课程推理组成（CR）。具体来说，KI模块首先通过生成以KG为中心的预训练语料库将知识注入到LM中，并将该过程概括为三个关键步骤，这些步骤可以与不同的实现一起使用以实现灵活的应用。接下来，KA模块通过配备适配器的LM从生成的语料库中学习知识，并保持其原有的自然语言理解能力，以减少生成语料库和自然语料库之间差异的负面影响。最后，为了使LM能够进行复杂的推理，CR模块遵循人类推理模式构建了三个推理难度逐渐增加的语料库，并以课程的方式进一步从易到难地训练LM。我们提供了总体框架的实现，并在四个真实数据集上评估了所提出的 KICP。结果表明我们的框架可以实现更高的性能。</li>
</ul>

<h3>Title: Linguistic Structure Induction from Language Models</h3>
<ul>
<li><strong>Authors: </strong>Omar Momen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09714">https://arxiv.org/abs/2403.09714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09714">https://arxiv.org/pdf/2403.09714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09714]] Linguistic Structure Induction from Language Models(https://arxiv.org/abs/2403.09714)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Linear sequences of words are implicitly represented in our brains by hierarchical structures that organize the composition of words in sentences. Linguists formalize different frameworks to model this hierarchy; two of the most common syntactic frameworks are Constituency and Dependency. Constituency represents sentences as nested groups of phrases, while dependency represents a sentence by assigning relations between its words. Recently, the pursuit of intelligent machines has produced Language Models (LMs) capable of solving many language tasks with a human-level performance. Many studies now question whether LMs implicitly represent syntactic hierarchies. This thesis focuses on producing constituency and dependency structures from LMs in an unsupervised setting. I review the critical methods in this field and highlight a line of work that utilizes a numerical representation for binary constituency trees (Syntactic Distance). I present a detailed study on StructFormer (SF) (Shen et al., 2021), which retrofits a transformer encoder architecture with a parser network to produce constituency and dependency structures. I present six experiments to analyze and address this field's challenges; experiments include investigating the effect of repositioning the parser network within the SF architecture, evaluating subword-based induced trees, and benchmarking the models developed in the thesis experiments on linguistic tasks. Models benchmarking is performed by participating in the BabyLM challenge, published at CoNLL 2023 (Momen et al., 2023). The results of this thesis encourage further development in the direction of retrofitting transformer-based models to induce syntactic structures, supported by the acceptable performance of SF in different experimental settings and the observed limitations that require innovative solutions to advance the state of syntactic structure induction.</li>
<li><strong>摘要：</strong>单词的线性序列在我们的大脑中通过组织句子中单词组成的层次结构隐式地表示。语言学家形式化了不同的框架来模拟这种层次结构；两个最常见的句法框架是选区和依赖。成分将句子表示为嵌套的短语组，而依赖关系则通过分配单词之间的关系来表示句子。最近，对智能机器的追求已经产生了能够以人类水平的性能解决许多语言任务的语言模型（LM）。现在许多研究质疑语言模型是否隐含地代表了句法层次结构。本论文的重点是在无监督的环境中从 LM 生成选区和依赖结构。我回顾了该领域的关键方法，并重点介绍了利用二元选区树（句法距离）的数字表示的一系列工作。我提出了对 StructFormer (SF) (Shen et al., 2021) 的详细研究，该研究使用解析器网络改进了 Transformer 编码器架构，以生成选区和依赖结构。我提出了六个实验来分析和解决该领域的挑战；实验包括研究在 SF 架构中重新定位解析器网络的效果、评估基于子词的归纳树以及对论文实验中开发的模型在语言任务上进行基准测试。模型基准测试是通过参加 CoNLL 2023 上发布的 BabyLM 挑战来进行的（Momen 等人，2023）。本论文的结果鼓励在改进基于 Transformer 的模型以诱导句法结构的方向上进一步发展，并得到 SF 在不同实验设置中可接受的性能以及观察到的限制的支持，这些限制需要创新的解决方案来推进句法结构归纳的状态。</li>
</ul>

<h3>Title: Fine-tuning vs Prompting, Can Language Models Understand Human Values?</h3>
<ul>
<li><strong>Authors: </strong>Pingwei Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09720">https://arxiv.org/abs/2403.09720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09720">https://arxiv.org/pdf/2403.09720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09720]] Fine-tuning vs Prompting, Can Language Models Understand Human Values?(https://arxiv.org/abs/2403.09720)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Accurately handling the underlying support values in sentences is crucial for understanding the speaker's tendencies, yet it poses a challenging task in natural language understanding (NLU). In this article, we explore the potential of fine-tuning and prompt tuning in this downstream task, using the Human Value Detection 2023. Additionally, we attempt to validate whether models can effectively solve the problem based on the knowledge acquired during the pre-training stage. Simultaneously, our interest lies in the capabilities of large language models (LLMs) aligned with RLHF in this task, and some preliminary attempts are presented.</li>
<li><strong>摘要：</strong>准确处理句子中的潜在支持值对于理解说话者的倾向至关重要，但这对自然语言理解（NLU）提出了一项具有挑战性的任务。在本文中，我们使用人类价值检测 2023 探索了此下游任务中微调和即时调整的潜力。此外，我们尝试根据预训练期间获得的知识来验证模型是否可以有效解决问题阶段。同时，我们的兴趣在于在此任务中与 RLHF 相一致的大型语言模型 (LLM) 的能力，并提出了一些初步尝试。</li>
</ul>

<h3>Title: A Semantic Mention Graph Augmented Model for Document-Level Event  Argument Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jian Zhang, Changlin Yang, Haiping Zhu, Qika Lin, Fangzhi Xu, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09721">https://arxiv.org/abs/2403.09721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09721">https://arxiv.org/pdf/2403.09721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09721]] A Semantic Mention Graph Augmented Model for Document-Level Event  Argument Extraction(https://arxiv.org/abs/2403.09721)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Document-level Event Argument Extraction (DEAE) aims to identify arguments and their specific roles from an unstructured document. The advanced approaches on DEAE utilize prompt-based methods to guide pre-trained language models (PLMs) in extracting arguments from input documents. They mainly concentrate on establishing relations between triggers and entity mentions within documents, leaving two unresolved problems: a) independent modeling of entity mentions; b) document-prompt isolation. To this end, we propose a semantic mention Graph Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM constructs a semantic mention graph that captures relations within and between documents and prompts, encompassing co-existence, co-reference and co-type relations. Furthermore, we introduce an ensembled graph transformer module to address mentions and their three semantic relations effectively. Later, the graph-augmented encoder-decoder module incorporates the relation-specific graph into the input embedding of PLMs and optimizes the encoder section with topology information, enhancing the relations comprehensively. Extensive experiments on the RAMS and WikiEvents datasets demonstrate the effectiveness of our approach, surpassing baseline methods and achieving a new state-of-the-art performance.</li>
<li><strong>摘要：</strong>文档级事件参数提取 (DEAE) 旨在从非结构化文档中识别参数及其特定角色。 DEAE 的高级方法利用基于提示的方法来指导预训练语言模型 (PLM) 从输入文档中提取参数。他们主要集中于在文档中建立触发器和实体提及之间的关系，留下两个未解决的问题：a）实体提及的独立建模； b) 文件提示隔离。为此，我们在本文中提出了语义提及图增强模型（GAM）来解决这两个问题。首先，GAM 构建一个语义提及图，捕获文档和提示内部以及之间的关系，包括共存、共同引用和共同类型关系。此外，我们引入了一个集成图转换器模块来有效地解决提及及其三种语义关系。随后，图增强编码器-解码器模块将关系特定图合并到PLM的输入嵌入中，并利用拓扑信息优化编码器部分，全面增强关系。 RAMS 和 WikiEvents 数据集上的大量实验证明了我们方法的有效性，超越了基线方法并实现了新的最先进的性能。</li>
</ul>

<h3>Title: RAD-PHI2: Instruction Tuning PHI-2 for Radiology</h3>
<ul>
<li><strong>Authors: </strong>Mercy Ranjit, Gopinath Ganapathy, Shaury Srivastav, Tanuja Ganu, Srujana Oruganti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09725">https://arxiv.org/abs/2403.09725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09725">https://arxiv.org/pdf/2403.09725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09725]] RAD-PHI2: Instruction Tuning PHI-2 for Radiology(https://arxiv.org/abs/2403.09725)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Small Language Models (SLMs) have shown remarkable performance in general domain language understanding, reasoning and coding tasks, but their capabilities in the medical domain, particularly concerning radiology text, is less explored. In this study, we investigate the application of SLMs for general radiology knowledge specifically question answering related to understanding of symptoms, radiological appearances of findings, differential diagnosis, assessing prognosis, and suggesting treatments w.r.t diseases pertaining to different organ systems. Additionally, we explore the utility of SLMs in handling text-related tasks with respect to radiology reports within AI-driven radiology workflows. We fine-tune Phi-2, a SLM with 2.7 billion parameters using high-quality educational content from Radiopaedia, a collaborative online radiology resource. The resulting language model, RadPhi-2-Base, exhibits the ability to address general radiology queries across various systems (e.g., chest, cardiac). Furthermore, we investigate Phi-2 for instruction tuning, enabling it to perform specific tasks. By fine-tuning Phi-2 on both general domain tasks and radiology-specific tasks related to chest X-ray reports, we create Rad-Phi2. Our empirical results reveal that Rad-Phi2 Base and Rad-Phi2 perform comparably or even outperform larger models such as Mistral-7B-Instruct-v0.2 and GPT-4 providing concise and precise answers. In summary, our work demonstrates the feasibility and effectiveness of utilizing SLMs in radiology workflows both for knowledge related queries as well as for performing specific tasks related to radiology reports thereby opening up new avenues for enhancing the quality and efficiency of radiology practice.</li>
<li><strong>摘要：</strong>小语言模型（SLM）在一般领域语言理解、推理和编码任务中表现出了卓越的性能，但它们在医学领域，特别是放射学文本方面的能力却很少被探索。在这项研究中，我们研究了 SLM 在一般放射学知识中的应用，特别是与理解症状、影像学表现、鉴别诊断、评估预后以及针对不同器官系统疾病的治疗建议相关的问题解答。此外，我们还探讨了 SLM 在处理人工智能驱动的放射学工作流程中放射学报告的文本相关任务方面的实用性。我们使用来自协作在线放射学资源 Radiopaedia 的高质量教育内容对 Phi-2 进行微调，这是一个具有 27 亿个参数的 SLM。由此产生的语言模型 RadPhi-2-Base 表现出解决跨各种系统（例如胸部、心脏）的一般放射学查询的能力。此外，我们研究了 Phi-2 的指令调整，使其能够执行特定任务。通过在一般领域任务和与胸部 X 射线报告相关的放射学特定任务上微调 Phi-2，我们创建了 Rad-Phi2。我们的实证结果表明，Rad-Phi2 Base 和 Rad-Phi2 的性能与 Mistral-7B-Instruct-v0.2 和 GPT-4 等更大的模型相当甚至优于 Mistral-7B-Instruct-v0.2 和 GPT-4，提供了简洁而精确的答案。总之，我们的工作证明了在放射学工作流程中利用 SLM 进行知识相关查询以及执行与放射学报告相关的特定任务的可行性和有效性，从而为提高放射学实践的质量和效率开辟了新途径。</li>
</ul>

<h3>Title: Investigating the performance of Retrieval-Augmented Generation and  fine-tuning for the development of AI-driven knowledge-based systems</h3>
<ul>
<li><strong>Authors: </strong>Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09727">https://arxiv.org/abs/2403.09727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09727">https://arxiv.org/pdf/2403.09727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09727]] Investigating the performance of Retrieval-Augmented Generation and  fine-tuning for the development of AI-driven knowledge-based systems(https://arxiv.org/abs/2403.09727)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case of the BLEU score, and 53% based on the cosine similarity. This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8% better METEOR score of FN models indicates greater creativity compared to RAG.</li>
<li><strong>摘要：</strong>生成式大语言模型（G-LLM）的发展为开发类似于 ChatGPT、Bing 或 Gemini 的新型知识系统提供了新的机会。微调（FN）和检索增强生成（RAG）是可用于实现领域适应以开发基于 G-LLM 的知识系统的技术。在我们的研究中，我们使用 ROUGE、BLEU、METEOR 分数和余弦相似度，比较和检查 GPT-J-6B、OPT-6.7B、LlaMA、LlaMA-2 语言模型的 RAG 和 FN 的性能。根据不同数据集上显示的测量结果，我们证明基于 RAG 的构造比使用 FN 生成的模型更有效。我们指出，连接 RAG 和 FN 并不简单，因为将 FN 模型与 RAG 连接可能会导致性能下降。此外，我们概述了一个简单的基于 RAG 的架构，该架构在 ROGUE 分数方面平均优于 FN 模型 16%，在 BLEU 分数方面优于 FN 模型 15%，在余弦相似度方面优于 FN 模型 53%。这表明 RAG 在幻觉方面比 FN 具有显着优势，但 FN 模型的 METEOR 分数平均高出 8%，这表明与 RAG 相比具有更大的创造力，这一事实并不能抵消这一优势。</li>
</ul>

<h3>Title: PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with  Cross-consistency</h3>
<ul>
<li><strong>Authors: </strong>Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09732">https://arxiv.org/abs/2403.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09732">https://arxiv.org/pdf/2403.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09732]] PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with  Cross-consistency(https://arxiv.org/abs/2403.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the prompt's schema information and instruct the LLM to produce the final SQL. Finally, as the post-refinement module, we propose using cross-consistency across different LLMs rather than self-consistency within a particular LLM. Our methods achieve new SOTA results on the Spider benchmark, with an execution accuracy of 87.6%.</li>
<li><strong>摘要：</strong>文本到 SQL (Text2SQL) 的最新进展强调在上下文学习中刺激大型语言模型 (LLM)，取得了显着的成果。然而，他们在处理详细的数据库信息和复杂的用户意图时面临挑战。本文提出了一个两阶段框架来增强当前基于 LLM 的自然语言到 SQL 系统的性能。我们首先引入一种新颖的提示表示，称为引用增强表示，其中包括模式信息和从表中随机采样的单元格值，以指示 LLM 生成 SQL 查询。然后，在第一阶段，问题-SQL 对作为小样本演示进行检索，促使 LLM 生成初步 SQL (PreSQL)。之后，解析PreSQL中提到的实体以进行模式链接，这可以显着压缩有用信息。在第二阶段，通过链接模式，我们简化提示的模式信息并指示 LLM 生成最终的 SQL。最后，作为后细化模块，我们建议在不同的法学硕士之间使用交叉一致性，而不是在特定的法学硕士内使用自我一致性。我们的方法在 Spider 基准测试中取得了新的 SOTA 结果，执行准确度为 87.6%。</li>
</ul>

<h3>Title: OverleafCopilot: Empowering Academic Writing in Overleaf with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, Huaiyu Wan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09733">https://arxiv.org/abs/2403.09733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09733">https://arxiv.org/pdf/2403.09733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09733]] OverleafCopilot: Empowering Academic Writing in Overleaf with Large  Language Models(https://arxiv.org/abs/2403.09733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>The rapid development of Large Language Models (LLMs) has facilitated a variety of applications from different domains. In this technical report, we explore the integration of LLMs and the popular academic writing tool, Overleaf, to enhance the efficiency and quality of academic writing. To achieve the above goal, there are three challenges: i) including seamless interaction between Overleaf and LLMs, ii) establishing reliable communication with the LLM provider, and iii) ensuring user privacy. To address these challenges, we present OverleafCopilot, the first-ever tool (i.e., a browser extension) that seamlessly integrates LLMs and Overleaf, enabling researchers to leverage the power of LLMs while writing papers. Specifically, we first propose an effective framework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a website for researchers to easily find and share high-quality up-to-date prompts. Thirdly, we propose an agent command system to help researchers quickly build their customizable agents. OverleafCopilot (https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb ) has been on the Chrome Extension Store, which now serves thousands of researchers. Additionally, the code of PromptGenius is released at https://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the potential to revolutionize academic writing practices, empowering researchers to produce higher-quality papers in less time.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展促进了不同领域的各种应用。在这份技术报告中，我们探讨了法学硕士与流行的学术写作工具Overleaf的整合，以提高学术写作的效率和质量。为了实现上述目标，存在三个挑战：i）包括 Overleaf 和 LLM 之间的无缝交互，ii）与 LLM 提供商建立可靠的通信，以及 iii）确保用户隐私。为了应对这些挑战，我们推出了 OverleafCopilot，这是第一个无缝集成法学硕士和 Overleaf 的工具（即浏览器扩展），使研究人员能够在撰写论文时利用法学硕士的强大功能。具体来说，我们首先提出一个有效的框架来连接法学硕士和 Overleaf。然后，我们开发了 PromptGenius，这是一个供研究人员轻松查找和共享高质量的最新提示的网站。第三，我们提出了一个代理命令系统来帮助研究人员快速构建可定制的代理。 OverleafCopilot (https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb ) 已上线 Chrome 扩展商店，目前为数千名研究人员提供服务。另外，PromptGenius的代码发布在https://github.com/wenhaomin/ChatGPT-PromptGenius。我们相信我们的工作有可能彻底改变学术写作实践，使研究人员能够在更短的时间内写出更高质量的论文。</li>
</ul>

<h3>Title: Do Large Language Models Solve ARC Visual Analogies Like People Do?</h3>
<ul>
<li><strong>Authors: </strong>Gustaw Opiełka, Hannes Rosenbusch, Veerle Vijverberg, Claire E. Stevenson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09734">https://arxiv.org/abs/2403.09734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09734">https://arxiv.org/pdf/2403.09734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09734]] Do Large Language Models Solve ARC Visual Analogies Like People Do?(https://arxiv.org/abs/2403.09734)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar "fallback" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, "concept" errors were more common in humans, and "matrix" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.</li>
<li><strong>摘要：</strong>抽象推理语料库（ARC）是一种专为人类和机器设计的视觉类比推理测试（Chollet，2019）。我们在一组新的儿童友好型 ARC 项目上比较了人类和大语言模型 (LLM) 的性能。结果表明，儿童和成人在这些任务上的表现都优于大多数法学硕士。错误分析揭示了法学硕士和幼儿中类似的“后备”解决方案策略，其中部分类比只是简单地复制。此外，我们还发现了另外两种错误类型，一种基于看似掌握的关键概念（例如，内部-外部），另一种基于类比输入矩阵的简单组合。总体而言，“概念”错误在人类中更常见，“矩阵”错误在法学硕士中更常见。这项研究为法学硕士的推理能力以及我们可以在多大程度上使用错误分析和与人类发展的比较来了解法学硕士如何解决视觉类比问题提供了新的视角。</li>
</ul>

<h3>Title: Evaluating Large Language Models as Generative User Simulators for  Conversational Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09738">https://arxiv.org/abs/2403.09738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09738">https://arxiv.org/pdf/2403.09738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09738]] Evaluating Large Language Models as Generative User Simulators for  Conversational Recommendation(https://arxiv.org/abs/2403.09738)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.</li>
<li><strong>摘要：</strong>在对话推荐系统的评估中，合成用户是真实用户的具有成本效益的代理。大型语言模型在模拟类人行为方面显示出希望，这引发了它们代表不同用户群体的能力的问题。我们引入了一种新协议来衡量语言模型在会话推荐中准确模拟人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应表现出的关键属性：选择要谈论的项目、表达二元偏好、表达开放式偏好、请求建议和提供反馈。通过对基线模拟器的评估，我们证明这些任务有效地揭示了语言模型与人类行为的偏差，并提供了如何通过模型选择和提示策略来减少偏差的见解。</li>
</ul>

<h3>Title: The Human Factor in Detecting Errors of Large Language Models: A  Systematic Literature Review and Future Research Directions</h3>
<ul>
<li><strong>Authors: </strong>Christian A. Schiller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09743">https://arxiv.org/abs/2403.09743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09743">https://arxiv.org/pdf/2403.09743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09743]] The Human Factor in Detecting Errors of Large Language Models: A  Systematic Literature Review and Future Research Directions(https://arxiv.org/abs/2403.09743)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - "hallucinations" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks. There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essential for organizations aiming to leverage LLM technology efficiently, guiding targeted training and deployment strategies to enhance error detection by users. This approach not only aims to optimize the use of LLMs but also to prevent potential downstream issues stemming from reliance on inaccurate model responses. The research emphasizes the balance between technological advancement and human insight in maximizing the benefits of LLMs while minimizing the risks, particularly in areas where precision is paramount. This paper performs a systematic literature research on this research topic, analyses and synthesizes the findings, and outlines future research directions. Literature selection cut-off date is January 11th 2024.</li>
<li><strong>摘要：</strong>OpenAI 于 2022 年 11 月推出的 ChatGPT 标志着人工智能的关键时刻，将大型语言模型 (LLM) 引入主流，并创下了用户采用率的新记录。法学硕士，尤其是 ChatGPT，经过大量互联网数据的培训，在各个领域表现出了卓越的对话能力，这表明对劳动力产生了重大影响。然而，这些模型很容易出现错误——“幻觉”和遗漏，产生不正确或不完整的信息。这会带来风险，尤其是在准确性至关重要的情况下，例如法律合规性、医学或细粒度的流程框架。有技术和人力解决方案来应对这个问题。本文探讨了使用户能够检测 LLM 输出中的错误的人为因素，这是减轻与专业环境中使用相关的风险的关键组成部分。了解这些因素对于旨在有效利用 LLM 技术、指导有针对性的培训和部署策略以增强用户错误检测的组织至关重要。这种方法不仅旨在优化法学硕士的使用，而且还旨在防止因依赖不准确的模型响应而产生潜在的下游问题。该研究强调技术进步和人类洞察力之间的平衡，以最大限度地提高法学硕士的效益，同时最大限度地降低风险，特别是在精度至关重要的领域。本文针对本研究课题进行了系统的文献研究，分析和综合研究结果，并概述了未来的研究方向。文献选择截止日期为2024年1月11日。</li>
</ul>

<h3>Title: Evaluating the Application of Large Language Models to Generate Feedback  in Programming Education</h3>
<ul>
<li><strong>Authors: </strong>Sven Jacobs, Steffen Jaschke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09744">https://arxiv.org/abs/2403.09744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09744">https://arxiv.org/pdf/2403.09744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09744]] Evaluating the Application of Large Language Models to Generate Feedback  in Programming Education(https://arxiv.org/abs/2403.09744)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This study investigates the application of large language models, specifically GPT-4, to enhance programming education. The research outlines the design of a web application that uses GPT-4 to provide feedback on programming tasks, without giving away the solution. A web application for working on programming tasks was developed for the study and evaluated with 51 students over the course of one semester. The results show that most of the feedback generated by GPT-4 effectively addressed code errors. However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.</li>
<li><strong>摘要：</strong>本研究调查了大型语言模型（特别是 GPT-4）在增强编程教育方面的应用。该研究概述了一个 Web 应用程序的设计，该应用程序使用 GPT-4 来提供编程任务的反馈，而不泄露解决方案。为这项研究开发了一个用于执行编程任务的 Web 应用程序，并在一个学期的过程中对 51 名学生进行了评估。结果表明，GPT-4 生成的大部分反馈有效解决了代码错误。然而，错误建议和幻觉问题带来的挑战表明需要进一步改进。</li>
</ul>

<h3>Title: Re-Search for The Truth: Multi-round Retrieval-augmented Large Language  Models are Strong Fake News Detectors</h3>
<ul>
<li><strong>Authors: </strong>Guanghua Li, Wensheng Lu, Wei Zhang, Defu Lian, Kezhong Lu, Rui Mao, Kai Shu, Hao Liao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09747">https://arxiv.org/abs/2403.09747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09747">https://arxiv.org/pdf/2403.09747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09747]] Re-Search for The Truth: Multi-round Retrieval-augmented Large Language  Models are Strong Fake News Detectors(https://arxiv.org/abs/2403.09747)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these challenges, we introduce a novel, retrieval-augmented LLMs framework--the first of its kind to automatically and strategically extract key evidence from web sources for claim verification. Employing a multi-round retrieval strategy, our framework ensures the acquisition of sufficient, relevant evidence, thereby enhancing performance. Comprehensive experiments across three real-world datasets validate the framework's superiority over existing methods. Importantly, our model not only delivers accurate verdicts but also offers human-readable explanations to improve result interpretability.</li>
<li><strong>摘要：</strong>假新闻的泛滥对政治、经济和整个社会产生了深远的影响。虽然假新闻检测方法已被用来缓解这个问题，但它们主要取决于两个基本要素：证据的质量和相关性，以及判决预测机制的有效性。传统方法通常从维基百科等静态存储库获取信息，但受到过时或不完整数据的限制，特别是对于新兴或罕见的主张。大型语言模型 (LLM) 以其卓越的推理和生成能力而闻名，为假新闻检测开辟了新领域。然而，与传统方法一样，基于法学硕士的解决方案也克服了陈旧和长尾知识的局限性。此外，检索增强型法学硕士经常面临低质量证据检索和上下文长度限制等问题。为了应对这些挑战，我们引入了一种新颖的、检索增强的法学硕士框架——这是第一个能够自动、战略性地从网络资源中提取关键证据以进行索赔验证的框架。我们的框架采用多轮检索策略，确保获取足够的相关证据，从而提高性能。三个现实世界数据集的综合实验验证了该框架相对于现有方法的优越性。重要的是，我们的模型不仅提供准确的判决，还提供人类可读的解释以提高结果的可解释性。</li>
</ul>

<h3>Title: Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge  in Datasets and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuoqun Li, Hongyu Lin, Yaojie Lu, Hao Xiang, Xianpei Han, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09750">https://arxiv.org/abs/2403.09750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09750">https://arxiv.org/pdf/2403.09750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09750]] Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge  in Datasets and Large Language Models(https://arxiv.org/abs/2403.09750)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis for the findings and this can provide primary guidance for evaluation and enhancement of large language models.</li>
<li><strong>摘要：</strong>陈述性知识和程序性知识是元认知理论中的两个关键部分，这两者对于法学硕士的预训练和推理具有重要意义。然而，缺乏对这两类知识进行全面分析比较，这主要是由于定义、探究和定量评估方面的挑战。在本文中，我们从一个新的角度进行探索，为法学硕士提供真实知识并评估有效分数。通过对广泛使用的数据集和模型进行大量实验，我们得出结论：（1）在大多数任务中，声明性知识的好处大于过程性知识的好处。 (2)只有在逻辑简单的推理任务中，程序性知识的收益才大于陈述性知识。 （3）随着预训练的进行和规模的增加，模型利用两种知识的能力显着提高，但速度不同。我们对研究结果进行了详细的分析，这可以为大型语言模型的评估和增强提供初步指导。</li>
</ul>

<h3>Title: Scaling Behavior of Machine Translation with Large Language Models under  Prompt Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Zhifan Sun, Antonio Valerio Miceli-Barone</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09832">https://arxiv.org/abs/2403.09832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09832">https://arxiv.org/pdf/2403.09832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09832]] Scaling Behavior of Machine Translation with Large Language Models under  Prompt Injection Attacks(https://arxiv.org/abs/2403.09832)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023). To our knowledge, this is the first work to study non-trivial LLM scaling behaviour in a multi-lingual setting.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 正日益成为许多自然语言处理任务（例如机器翻译）的首选基础平台，因为它们的质量通常可与特定于任务的模型相媲美或更好，并且通过自然语言指令指定任务的简单性或上下文中的示例。然而，它们的普遍性使它们容易受到最终用户的破坏，最终用户可能会在请求中嵌入导致模型以未经授权且可能不安全的方式运行的指令。在这项工作中，我们研究了机器翻译任务中多个 LLM 系列的即时注入攻击 (PIA)，重点关注模型大小对攻击成功率的影响。我们引入了一个新的基准数据集，我们发现在多个语言对和用英语编写的注入提示上，在某些条件下较大的模型可能更容易受到成功的攻击，这是逆缩放现象的一个实例（McKenzie et al., 2023） 。据我们所知，这是第一个在多语言环境下研究非平凡 LLM 扩展行为的工作。</li>
</ul>

<h3>Title: Self-Consistency Boosts Calibration for Math Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09849">https://arxiv.org/abs/2403.09849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09849">https://arxiv.org/pdf/2403.09849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09849]] Self-Consistency Boosts Calibration for Math Reasoning(https://arxiv.org/abs/2403.09849)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).</li>
<li><strong>摘要：</strong>校准建立了准确性和模型置信度之间的相关性，对于法学硕士的发展非常重要。我们为数学推理任务设计了三种基于自洽性的现成校准方法（Wang et al., 2022）。使用强大的开源 LLM（Mistral 和 LLaMA2）对两个流行的基准（GSM8K 和 MathQA）进行评估，我们的方法比基于 p(True)（Kadavath 等人，2022）或 logit 的现有方法更好地桥接模型置信度和准确性（卡达瓦斯等人，2022）。</li>
</ul>

<h3>Title: FakeWatch: A Framework for Detecting Fake News to Ensure Credible  Elections</h3>
<ul>
<li><strong>Authors: </strong>Shaina Raza, Tahniat Khan, Drai Paulen-Patterson, Veronica Chatrath, Mizanur Rahman, Oluwanifemi Bamgbose</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09858">https://arxiv.org/abs/2403.09858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09858">https://arxiv.org/pdf/2403.09858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09858]] FakeWatch: A Framework for Detecting Fake News to Ensure Credible  Elections(https://arxiv.org/abs/2403.09858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In today's technologically driven world, the rapid spread of fake news, particularly during critical events like elections, poses a growing threat to the integrity of information. To tackle this challenge head-on, we introduce FakeWatch, a comprehensive framework carefully designed to detect fake news. Leveraging a newly curated dataset of North American election-related news articles, we construct robust classification models. Our framework integrates a model hub comprising of both traditional machine learning (ML) techniques and cutting-edge Language Models (LMs) to discern fake news effectively. Our overarching objective is to provide the research community with adaptable and precise classification models adept at identifying the ever-evolving landscape of misinformation. Quantitative evaluations of fake news classifiers on our dataset reveal that, while state-of-the-art LMs exhibit a slight edge over traditional ML models, classical models remain competitive due to their balance of accuracy and computational efficiency. Additionally, qualitative analyses shed light on patterns within fake news articles. This research lays the groundwork for future endeavors aimed at combating misinformation, particularly concerning electoral processes. We provide our labeled data and model publicly for use and reproducibility.</li>
<li><strong>摘要：</strong>在当今技术驱动的世界中，假新闻的迅速传播，特别是在选举等关键事件期间，对信息的完整性构成了越来越大的威胁。为了正面应对这一挑战，我们推出了 FakeWatch，这是一个精心设计的综合框架，用于检测假新闻。利用新整理的北美选举相关新闻文章数据集，我们构建了强大的分类模型。我们的框架集成了一个由传统机器学习（ML）技术和尖端语言模型（LM）组成的模型中心，以有效识别假新闻。我们的首要目标是为研究界提供适应性强且精确的分类模型，能够识别不断变化的错误信息。对我们数据集上的假新闻分类器进行的定量评估表明，虽然最先进的 LM 比传统的 ML 模型表现出轻微的优势，但经典模型由于其准确性和计算效率的平衡而仍然具有竞争力。此外，定性分析揭示了假新闻文章中的模式。这项研究为未来打击错误信息，特别是有关选举进程的错误信息的努力奠定了基础。我们公开提供标记数据和模型以供使用和再现。</li>
</ul>

<h3>Title: Sabiá-2: A New Generation of Portuguese Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09887">https://arxiv.org/abs/2403.09887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09887">https://arxiv.org/pdf/2403.09887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09887]] Sabiá-2: A New Generation of Portuguese Large Language Models(https://arxiv.org/abs/2403.09887)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.</li>
<li><strong>摘要：</strong>我们引入了 Sabi\'a-2，这是一个在葡萄牙语文本上训练的大型语言模型家族。这些模型在各种考试中进行评估，包括巴西大学的入门级考试、专业认证考试以及会计、经济、工程、法律和医学等各个学科的研究生水平考试。我们的结果显示，我们迄今为止最好的模型 Sabi\'a-2 Medium 在 64 项考试中的 23 项中与 GPT-4 的表现相匹配或超过了 GPT-4，在 64 项考试中的 58 项中优于 GPT-3.5。值得注意的是，专业化对模型的性能有重大影响，而无需增加其规模，这使我们能够以比 GPT-4 便宜 10 倍的每个代币价格提供 Sabi\'a-2 Medium。最后，我们发现数学和编码是需要改进的关键能力。</li>
</ul>

<h3>Title: Fisher Mask Nodes for Language Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Thennal D K, Ganesh Nathan, Suchithra M S</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09891">https://arxiv.org/abs/2403.09891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09891">https://arxiv.org/pdf/2403.09891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09891]] Fisher Mask Nodes for Language Model Merging(https://arxiv.org/abs/2403.09891)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance increase across various models in the BERT family, outperforming full-scale Fisher-weighted averaging in a fraction of the computational cost, with baseline performance improvements of up to +6.5 and a speedup of 57.4x. Our results prove the potential of our method in current multi-task learning environments and suggest its scalability and adaptability to new model architectures and learning scenarios.</li>
<li><strong>摘要：</strong>微调预训练模型可在下游性能方面提供显着优势。 BERT 等预训练模型及其在自然语言处理中的衍生物的普遍存在也导致了针对特定任务的微调模型的激增。由于这些模型通常只能很好地执行一项任务，因此在多任务场景中需要额外的训练或集成。不断发展的模型合并领域提供了一种解决方案，可以应对将多个特定于任务的模型组合成单个多任务模型的挑战。在这项研究中，我们引入了一种新颖的 Transformer 模型合并方法，结合了之前费希尔加权平均工作的见解以及费希尔信息在模型剪枝中的使用。利用 Transformer 架构中掩码节点的 Fisher 信息，我们设计了一种计算高效的加权平均方案。我们的方法在 BERT 系列的各种模型中表现出定期且显着的性能提升，在计算成本的一小部分内优于全面的 Fisher 加权平均，基线性能提升高达 +6.5 倍，加速高达 57.4 倍。我们的结果证明了我们的方法在当前多任务学习环境中的潜力，并表明其可扩展性和对新模型架构和学习场景的适应性。</li>
</ul>

<h3>Title: Recurrent Drafter for Fast Speculative Decoding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09919">https://arxiv.org/abs/2403.09919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09919">https://arxiv.org/pdf/2403.09919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09919]] Recurrent Drafter for Fast Speculative Decoding in Large Language Models(https://arxiv.org/abs/2403.09919)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attention structure only for inference in Medusa. We empirically demonstrate the effectiveness of the proposed method on several popular open source language models, along with a comprehensive analysis of the trade-offs involved in adopting this approach.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了一种改进的推测解码方法，旨在提高服务大型语言模型的效率。我们的方法利用了两种既定技术的优势：经典的双模型推测解码方法和更新的单模型方法 Medusa。受到美杜莎的启发，我们的方法采用单模型策略进行推测解码。然而，我们的方法的独特之处在于采用了具有循环依赖设计的单个轻量级草稿头，本质上类似于经典推测解码中使用的小型草稿模型，但没有完整变压器架构的复杂性。并且由于反复出现的依赖关系，我们可以使用集束搜索（beam search）来通过草稿头快速过滤掉不需要的候选人。结果是一种结合了单模型设计的简单性的方法，并且避免了创建仅用于 Medusa 推理的数据依赖树注意结构的需要。我们凭经验证明了所提出的方法在几种流行的开源语言模型上的有效性，并对采用这种方法所涉及的权衡进行了全面分析。</li>
</ul>

<h3>Title: Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias  in Factual Knowledge Extraction</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09963">https://arxiv.org/abs/2403.09963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09963">https://arxiv.org/pdf/2403.09963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09963]] Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias  in Factual Knowledge Extraction(https://arxiv.org/abs/2403.09963)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the model's internal representations to generate the debiased representations, which are used to produce the final debiased outputs. Experiments across various prompts, PLMs, and benchmarks show that our approach can not only correct the overfitted performance caused by prompt bias, but also significantly improve the prompt retrieval capability (up to 10% absolute performance gain). Our findings shed new light on the underlying predicting mechanisms of prompt-based queries in PLMs. Hopefully, our plug-and-play approach can be a golden standard to strengthen PLMs toward reliable knowledge bases. Code and data are released in https://github.com/FelliYang/PromptBias.</li>
<li><strong>摘要：</strong>最近的研究表明，预训练语言模型（PLM）在事实知识提取中存在“提示偏差”，即提示往往会引入对特定标签的偏差。然而，模型中即时偏差的程度和影响仍未得到充分探索。作为回应，本文量化了各种类型提示的偏差，并评估了它们对不同基准的影响。我们表明：1）实验中的所有提示都表现出不可忽略的偏差，其中基于梯度的提示（例如 AutoPrompt 和 OptiPrompt）显示出明显更高水平的偏差； 2) 即时偏差可以通过过度拟合测试数据集来不合理地放大基准准确性，特别是在像 LAMA 这样的不平衡数据集上。基于这些发现，我们提出了一种基于表示的方法来减轻推理期间的即时偏差。具体来说，我们首先使用仅提示查询来估计有偏差的表示，然后将其从模型的内部表示中删除以生成无偏差的表示，这些表示用于产生最终的无偏差的输出。跨各种提示、PLM 和基准的实验表明，我们的方法不仅可以纠正提示偏差导致的过度拟合性能，而且可以显着提高提示检索能力（绝对性能增益高达 10%）。我们的研究结果为 PLM 中基于提示的查询的基本预测机制提供了新的线索。希望我们的即插即用方法能够成为加强 PLM 以获得可靠知识库的黄金标准。代码和数据发布在https://github.com/FelliYang/PromptBias。</li>
</ul>

<h3>Title: Think Twice Before Assure: Confidence Estimation for Large Language  Models through Reflection on Multiple Answers</h3>
<ul>
<li><strong>Authors: </strong>Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09972">https://arxiv.org/abs/2403.09972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09972">https://arxiv.org/pdf/2403.09972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09972]] Think Twice Before Assure: Confidence Estimation for Large Language  Models through Reflection on Multiple Answers(https://arxiv.org/abs/2403.09972)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.</li>
<li><strong>摘要：</strong>旨在评估输出可信度的置信度估计对于大型语言模型（LLM）的应用至关重要，尤其是黑盒模型。由于 LLM 对其生成的错误答案过于自信，因此现有的 LLM 置信度估计通常未经过校准。解决过度自信问题的现有方法受到一个重大限制的阻碍，即它们仅考虑法学硕士生成的一个答案的置信度。为了解决这个限制，我们提出了一种新颖的范例，可以彻底评估多个候选答案的可信度，以减轻对错误答案的过度自信。在此范式的基础上，我们引入了一个两步框架，首先指示法学硕士反思并为每个答案提供理由，然后汇总综合置信度估计的理由。该框架可以与现有的置信估计方法集成，以实现卓越的校准。三个任务的六个数据集的实验结果证明了所提出框架的合理性和有效性。</li>
</ul>

<h3>Title: Identifying Health Risks from Family History: A Survey of Natural  Language Processing Techniques</h3>
<ul>
<li><strong>Authors: </strong>Xiang Dai, Sarvnaz Karimi, Nathan O'Callaghan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09997">https://arxiv.org/abs/2403.09997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09997">https://arxiv.org/pdf/2403.09997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09997]] Identifying Health Risks from Family History: A Survey of Natural  Language Processing Techniques(https://arxiv.org/abs/2403.09997)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Electronic health records include information on patients' status and medical history, which could cover the history of diseases and disorders that could be hereditary. One important use of family history information is in precision health, where the goal is to keep the population healthy with preventative measures. Natural Language Processing (NLP) and machine learning techniques can assist with identifying information that could assist health professionals in identifying health risks before a condition is developed in their later years, saving lives and reducing healthcare costs. We survey the literature on the techniques from the NLP field that have been developed to utilise digital health records to identify risks of familial diseases. We highlight that rule-based methods are heavily investigated and are still actively used for family history extraction. Still, more recent efforts have been put into building neural models based on large-scale pre-trained language models. In addition to the areas where NLP has successfully been utilised, we also identify the areas where more research is needed to unlock the value of patients' records regarding data collection, task formulation and downstream applications.</li>
<li><strong>摘要：</strong>电子健康记录包括有关患者状况和病史的信息，其中可能涵盖可能遗传的疾病和病症的历史。家族史信息的一项重要用途是精准健康，其目标是通过预防措施保持人口健康。自然语言处理 (NLP) 和机器学习技术可以帮助识别信息，从而帮助健康专业人员在晚年出现病情之前识别健康风险，从而挽救生命并降低医疗成本。我们调查了有关 NLP 领域技术的文献，这些技术是为了利用数字健康记录来识别家族疾病的风险而开发的。我们强调，基于规则的方法经过了深入研究，并且仍然积极用于家族史提取。尽管如此，最近人们还是致力于构建基于大规模预训练语言模型的神经模型。除了 NLP 已成功应用的领域外，我们还确定了需要进行更多研究的领域，以释放患者记录在数据收集、任务制定和下游应用方面的价值。</li>
</ul>

<h3>Title: Lost in Overlap: Exploring Watermark Collision in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Luo, Ke Lin, Chao Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10020">https://arxiv.org/abs/2403.10020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10020">https://arxiv.org/pdf/2403.10020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10020]] Lost in Overlap: Exploring Watermark Collision in LLMs(https://arxiv.org/abs/2403.10020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在内容生成中的激增引起了人们对文本版权的担忧。水印方法，特别是基于逻辑的方法，将难以察觉的标识符嵌入文本中以应对这些挑战。然而，水印在不同的法学硕士中的广泛使用导致了在问答和释义等常见任务中不可避免的问题，称为水印冲突。本研究重点关注双水印碰撞，即同一文本中同时存在两个水印。研究表明，水印冲突对上游和下游水印算法的检测器的检测性能构成威胁。</li>
</ul>

<h3>Title: Don't Half-listen: Capturing Key-part Information in Continual  Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yongquan He, Xuancheng Huang, Minghao Tang, Lingxun Meng, Xiang Li, Wei Lin, Wenyuan Zhang, Yifu Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10056">https://arxiv.org/abs/2403.10056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10056">https://arxiv.org/pdf/2403.10056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10056]] Don't Half-listen: Capturing Key-part Information in Continual  Instruction Tuning(https://arxiv.org/abs/2403.10056)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的指令调整可以驱动它们在特定下游任务中产生与人类目标一致的结果。然而，法学硕士的持续指令调整（CIT）过程可能会带来灾难性遗忘（CF）问题，导致先前学习的能力下降。最近的方法试图通过修改模型或重放数据来缓解 CF 问题，这可能只记住指令的表面模式，并对保留的任务感到困惑。在本文中，我们提出了一种基于关键部分信息增益（KPIG）的新颖的连续指令调整方法。我们的方法计算屏蔽部分的信息增益，以动态重放数据并细化训练目标，这使法学硕士能够捕获与正确响应相关的任务感知信息，并减轻对指令中一般描述的过度拟合。此外，我们提出了两个指标：P-score 和 V-score，来衡量法学硕士的泛化能力和指令跟踪能力。实验证明我们的方法在可见任务和保留任务上都取得了优异的性能。</li>
</ul>

<h3>Title: DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time  Information Needs of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10081">https://arxiv.org/abs/2403.10081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10081">https://arxiv.org/pdf/2403.10081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10081]] DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time  Information Needs of Large Language Models(https://arxiv.org/abs/2403.10081)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main</li>
<li><strong>摘要：</strong>动态检索增强生成（RAG）范式主动决定在大型语言模型（LLM）的文本生成过程中检索何时以及检索什么。该范例有两个关键要素：确定激活检索模块的最佳时刻（决定何时检索）以及在触发检索后制定适当的查询（确定检索什么）。然而，当前的动态 RAG 方法在这两方面都存在不足。首先，决定何时检索的策略通常依赖于静态规则。此外，决定检索内容的策略通常仅限于 LLM 最近的句子或最后几个标记，而 LLM 的实时信息需求可能跨越整个上下文。为了克服这些限制，我们引入了一个新的框架，DRAGIN，即基于法学硕士实时信息需求的动态检索增强生成。我们的框架专门设计用于根据法学硕士在文本生成过程中的实时信息需求来决定检索的时间和内容。我们在 4 个知识密集型生成数据集上全面评估 DRAGIN 以及现有方法。实验结果表明，DRAGIN 在所有任务上都取得了优异的性能，证明了我们方法的有效性。我们已在 GitHub 中开源了所有代码、数据和模型：https://github.com/oneal2000/DRAGIN/tree/main</li>
</ul>

<h3>Title: Intent-conditioned and Non-toxic Counterspeech Generation using  Multi-Task Instruction Tuning with RLAIF</h3>
<ul>
<li><strong>Authors: </strong>Amey Hengle, Aswini Kumar, Sahajpreet Singh, Anil Bandhakavi, Md Shad Akhtar, Tanmoy Chakroborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10088">https://arxiv.org/abs/2403.10088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10088">https://arxiv.org/pdf/2403.10088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10088]] Intent-conditioned and Non-toxic Counterspeech Generation using  Multi-Task Instruction Tuning with RLAIF(https://arxiv.org/abs/2403.10088)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL's first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms existing benchmarks in intent-conditioned counterspeech generation, showing an average improvement of 3 points in intent-conformity and 4 points in argument-quality metrics. Extensive human evaluation supports CoARL's efficacy in generating superior and more context-appropriate responses compared to existing systems, including prominent LLMs like ChatGPT.</li>
<li><strong>摘要：</strong>反言论被定义为减轻在线仇恨言论的回应，越来越多地被用作非审查解决方案。有效地解决仇恨言论涉及消除通常在简短的单句陈述或辱骂中巧妙暗示的刻板印象、偏见和偏见。这些隐式表达式对语言模型提出了挑战，尤其是在 seq2seq 任务中，因为模型性能通常在较长的上下文中表现出色。我们的研究引入了 CoARL，这是一种新颖的框架，通过对仇恨言论中社会偏见背后的实用含义进行建模，增强反言论的生成。 CoARL 的前两个阶段涉及顺序多指令调整，教导模型理解攻击性语句的意图、反应和危害，然后学习特定于任务的低级适配器权重以生成意图条件的反言语。最后阶段使用强化学习来微调输出的有效性和无毒性。 CoARL 在意图条件反言语生成方面优于现有基准，在意图一致性方面平均提高了 3 点，在论证质量指标方面平均提高了 4 点。与现有系统（包括 ChatGPT 等著名的法学硕士）相比，广泛的人类评估支持 CoARL 能够生成更出色且更适合上下文的响应。</li>
</ul>

<h3>Title: RAFT: Adapting Language Model to Domain Specific RAG</h3>
<ul>
<li><strong>Authors: </strong>Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10131">https://arxiv.org/abs/2403.10131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10131">https://arxiv.org/pdf/2403.10131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10131]] RAFT: Adapting Language Model to Domain Specific RAG(https://arxiv.org/abs/2403.10131)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.</li>
<li><strong>摘要：</strong>在大型文本数据语料库上预训练大型语言模型 (LLM) 现在已成为一种标准范例。当将这些 LLM 用于许多下游应用程序时，通常会通过基于 RAG 的提示或微调将新知识（例如，时间关键的新闻或私有领域知识）额外融入到预训练模型中。然而，模型获得此类新知识的最佳方法仍然是一个悬而未决的问题。在本文中，我们提出了检索增强微调（RAFT），这是一种训练方法，可以提高模型在“开放书籍”域内设置中回答问题的能力。在 RAFT 中，给定一个问题和一组检索到的文档，我们训练模型忽略那些无助于回答问题的文档，我们称之为干扰文档。 RAFT 通过逐字引用相关文档中有助于回答问题的正确序列来实现这一点。这与 RAFT 的思维链式响应相结合，有助于提高模型的推理能力。在特定领域的 RAG 中，RAFT 持续改进了 PubMed、HotpotQA 和 Gorilla 数据集中的模型性能，提供了一种训练后配方，可将预训练的 LLM 改进为域内 RAG。 RAFT 的代码和演示在 github.com/ShishirPatil/gorilla 上开源。</li>
</ul>

<h3>Title: Read between the lines -- Functionality Extraction From READMEs</h3>
<ul>
<li><strong>Authors: </strong>Prince Kumar, Srikanth Tamilselvam, Dinesh Garg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10205">https://arxiv.org/abs/2403.10205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10205">https://arxiv.org/pdf/2403.10205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10205]] Read between the lines -- Functionality Extraction From READMEs(https://arxiv.org/abs/2403.10205)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT and Bard respectively.</li>
<li><strong>摘要：</strong>虽然文本摘要是一项众所周知的 NLP 任务，但在本文中，我们介绍了它的一种新颖且有用的变体，称为从 Git README 文件中提取功能。尽管此任务是抽象级别的文本到文本生成，但它涉及其自身的特殊性和挑战，使得现有的文本到文本生成系统不是很有用。这项任务背后的动机源于最近围绕使用大型语言模型进行代码相关任务（例如代码重构、代码摘要等）的研发活动激增。我们还发布了一个名为 FuncRead 的人工注释数据集，并且为该任务开发一组模型。我们详尽的实验表明，小尺寸微调模型击败了任何可以使用流行的黑盒或白盒大型语言模型 (LLM)（例如 ChatGPT 和 Bard）设计的基线模型。我们最好的微调 70 亿 CodeLlama 模型在 F1 分数上相对于 ChatGPT 和 Bard 分别提高了 70% 和 20%。</li>
</ul>

<h3>Title: Is Translation All You Need? A Study on Solving Multilingual Tasks with  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10258">https://arxiv.org/abs/2403.10258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10258">https://arxiv.org/pdf/2403.10258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10258]] Is Translation All You Need? A Study on Solving Multilingual Tasks with  Large Language Models(https://arxiv.org/abs/2403.10258)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）展现了强大的多语言能力；然而，由于训练语料库不平衡，它们大多以英语为中心。现有的作品利用这种现象来提高他们在 NLP 任务上的多语言表现。在这项工作中，我们将评估从 NLP 任务扩展到真实的用户查询。我们发现，尽管翻译成英语有助于提高以英语为中心的法学硕士的多语言 NLP 任务的性能，但它可能并非适合所有场景。对于需要深入语言理解的文化相关任务，用母语提示被证明更有前途，因为它可以捕捉与文化和语言相关的细微差别。因此，我们主张加大力度发展强大的多语言法学硕士，而不仅仅是以英语为中心的法学硕士。</li>
</ul>

<h3>Title: A Question on the Explainability of Large Language Models and the  Word-Level Univariate First-Order Plausibility Assumption</h3>
<ul>
<li><strong>Authors: </strong>Jeremie Bogaert, Francois-Xavier Standaert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10275">https://arxiv.org/abs/2403.10275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10275">https://arxiv.org/pdf/2403.10275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10275]] A Question on the Explainability of Large Language Models and the  Word-Level Univariate First-Order Plausibility Assumption(https://arxiv.org/abs/2403.10275)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity. In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models. To this end, we give statistical definitions for the explanations' signal, noise and signal-to-noise ratio. We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of transformer ones. We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers.</li>
<li><strong>摘要：</strong>最近，大型语言模型的解释被证明对其训练中使用的随机性很敏感，因此需要描述这种敏感性。在本文中，我们提出了一种表征，质疑为此类模型提供简单且信息丰富的解释的可能性。为此，我们给出了解释信号、噪声和信噪比的统计定义。我们强调，在使用一阶统计工具分析字级单变量解释的典型案例研究中，基于简单特征的模型的解释比变压器模型的解释携带更多的信号和更少的噪声。然后，我们讨论通过信号和噪声的替代定义来改进这些结果的可能性，这些定义将捕获更复杂的解释和分析方法，同时也质疑其对读者的合理性的权衡。</li>
</ul>

<h3>Title: Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification  with Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Shang-Hsuan Chiang, Ming-Chih Lo, Lin-Wei Chao, Wen-Chih Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10281">https://arxiv.org/abs/2403.10281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10281">https://arxiv.org/pdf/2403.10281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10281]] Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification  with Fine-Tuning(https://arxiv.org/abs/2403.10281)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.</li>
<li><strong>摘要：</strong>在本文中，我们提出了 Pre-CoFactv3，这是一个由问答和文本分类组件组成的综合框架，用于事实验证。利用上下文学习、微调大型语言模型 (LLM) 和 FakeNet 模型，我们解决了事实验证的挑战。我们的实验探索了多种方法，比较了不同的预训练法学硕士，引入了 FakeNet，并实现了各种集成方法。值得注意的是，我们的团队 Trifecta 在 AAAI-24 Factify 3.0 Workshop 中获得第一名，超出基线准确率 103%，并保持比第二名竞争对手 70% 的领先优势。这一成功强调了我们方法的有效性及其对推进事实验证研究的潜在贡献。</li>
</ul>

<h3>Title: Uni-SMART: Universal Science Multimodal Analysis and Research  Transformer</h3>
<ul>
<li><strong>Authors: </strong>Hengxing Cai, Xiaochen Cai, Shuwen Yang, Jiankun Wang, Lin Yao, Zhifeng Gao, Junhan Chang, Sihang Li, Mingjun Xu, Changxin Wang, Hongshuai Wang, Yongge Li, Mujie Lin, Yaqi Li, Yuqi Yin, Linfeng Zhang, Guolin Ke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10301">https://arxiv.org/abs/2403.10301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10301">https://arxiv.org/pdf/2403.10301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10301]] Uni-SMART: Universal Science Multimodal Analysis and Research  Transformer(https://arxiv.org/abs/2403.10301)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of Large Language Models (LLMs) has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature. However, existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature. To answer this demand, we present Uni-SMART (Universal Science Multimodal Analysis and Research Transformer), an innovative model designed for in-depth understanding of multimodal scientific literature. Through rigorous quantitative evaluation across several domains, Uni-SMART demonstrates superior performance over leading text-focused LLMs. Furthermore, our exploration extends to practical applications, including patent infringement detection and nuanced analysis of charts. These applications not only highlight Uni-SMART's adaptability but also its potential to revolutionize how we interact with scientific literature.</li>
<li><strong>摘要：</strong>在科学研究及其应用中，科学文献分析至关重要，因为它使研究人员能够以他人的工作为基础。然而，科学知识的快速增长导致学术文章大量增加，使得深入的文献分析变得越来越具有挑战性和耗时。大型语言模型（LLM）的出现为应对这一挑战提供了一种新方法。法学硕士以其强大的文本总结能力而闻名，被视为改进科学文献分析的潜在工具。然而，现有的法学硕士有其自身的局限性。科学文献通常包含广泛的多模式元素，例如分子结构、表格和图表，这些对于以文本为中心的法学硕士来说很难理解和分析。这个问题表明迫切需要能够充分理解和分析科学文献中多模态内容的新解决方案。为了满足这一需求，我们推出了 Uni-SMART（Universal Science Multimodal Analysis and Research Transformer），这是一种专为深入理解多模态科学文献而设计的创新模型。通过跨多个领域的严格定量评估，Uni-SMART 表现出了优于领先的以文本为中心的法学硕士的性能。此外，我们的探索延伸到实际应用，包括专利侵权检测和图表的细致分析。这些应用不仅凸显了 Uni-SMART 的适应性，而且还凸显了它彻底改变我们与科学文献互动方式的潜力。</li>
</ul>

<h3>Title: CDGP: Automatic Cloze Distractor Generation based on Pre-trained  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Shang-Hsuan Chiang, Ssu-Cheng Wang, Yao-Chung Fan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10326">https://arxiv.org/abs/2403.10326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10326">https://arxiv.org/pdf/2403.10326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10326]] CDGP: Automatic Cloze Distractor Generation based on Pre-trained  Language Model(https://arxiv.org/abs/2403.10326)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.</li>
<li><strong>摘要：</strong>手动设计完形填空测试消耗大量的时间和精力。主要挑战在于错误的选项（干扰因素）选择。精心设计的干扰因素可以提高学习者能力评估的有效性。由此，萌生了自动生成完形填空干扰项的想法。在本文中，我们通过探索使用预先训练的语言模型（PLM）作为候选干扰项生成的替代方案来研究完形填空干扰项的生成。实验表明，PLM增强模型带来了显着的性能提升。我们表现​​最佳的模型将最先进的结果从 14.94 提高到 34.17（NDCG@10 分数）。我们的代码和数据集可在 https://github.com/AndyChiangSH/CDGP 获取。</li>
</ul>

<h3>Title: Investigating grammatical abstraction in language models using few-shot  learning of novel noun gender</h3>
<ul>
<li><strong>Authors: </strong>Priyanka Sukumaran, Conor Houghton, Nina Kazanina</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10338">https://arxiv.org/abs/2403.10338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10338">https://arxiv.org/pdf/2403.10338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10338]] Investigating grammatical abstraction in language models using few-shot  learning of novel noun gender(https://arxiv.org/abs/2403.10338)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Humans can learn a new word and infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the few-shot updates were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the word embedding space. While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented. For a comparative perspective with human behaviour, we conducted an analogous one-shot novel noun gender learning experiment, which revealed that native French speakers, like language models, also exhibited a masculine gender bias and are not excellent one-shot learners either.</li>
<li><strong>摘要：</strong>人类可以学习一个新单词并从很少的例子中推断出它的语法属性。它们具有语言属性的抽象概念，例如语法性别和一致性规则，可以应用于新的句法上下文和单词。受到心理语言学的启发，我们进行了一项名词学习实验，以评估 LSTM 和纯解码器变压器是否可以实现法语中语法性别的类人抽象。语言模型的任务是从一个语法一致上下文中的几个示例中学习嵌入的新名词的性别，并预测另一个不可见上下文中的一致性。我们发现，两种语言模型都有效地从一到两个学习示例中概括出新的名词性别，并将学到的性别应用于协议上下文中，尽管对男性性别类别存在偏见。重要的是，几次更新仅应用于嵌入层，这表明模型在词嵌入空间内编码了足够的性别信息。虽然模型的泛化行为表明它们将语法性别表示为一个抽象类别，就像人类一样，但仍需要进一步的工作来探索其具体实现方式的细节。为了与人类行为进行比较，我们进行了一项类似的一次性小说名词性别学习实验，结果表明，法语母语人士和语言模型一样，也表现出男性性别偏见，也不是优秀的一次性学习者。</li>
</ul>

<h3>Title: TriSum: Learning Summarization Ability from Large Language Models with  Structured Rationale</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10351">https://arxiv.org/abs/2403.10351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10351">https://arxiv.org/pdf/2403.10351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10351]] TriSum: Learning Summarization Ability from Large Language Models with  Structured Rationale(https://arxiv.org/abs/2403.10351)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summarization rationale.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现极大地推进了文本摘要等自然语言处理任务。然而，它们的大尺寸和计算需求，加上数据传输中的隐私问题，限制了它们在资源受限和以隐私为中心的环境中的使用。为了克服这个问题，我们引入了 TriSum，一个将法学硕士的文本摘要能力提炼成紧凑的本地模型的框架。最初，法学硕士提取一组三重方面的基本原理和摘要，并使用双评分方法对质量进行细化。接下来，使用从简单任务演变到复杂任务的课程学习策略来训练较小的本地模型来完成这些任务。我们的方法增强了各种基准（CNN/DailyMail、XSum 和 ClinicalTrial）上的本地模型性能，分别比基准高出 4.5%、8.5% 和 7.4%。它还通过提供对摘要基本原理的见解来提高可解释性。</li>
</ul>

<h3>Title: EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for  Evaluating Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10378">https://arxiv.org/abs/2403.10378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10378">https://arxiv.org/pdf/2403.10378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10378]] EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for  Evaluating Vision Language Models(https://arxiv.org/abs/2403.10378)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision-text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.</li>
<li><strong>摘要：</strong>我们推出 EXAMS-V，这是一种新的具有挑战性的多学科多模式多语言考试基准，用于评估视觉语言模型。它由 20 个学校学科的 20,932 道多项选择题组成，涵盖自然科学、社会科学和其他杂项研究，例如宗教、美术、商业等。EXAMS-V 包括多种多模态特征，例如文本、图像、表格、图形、图表、地图、科学符号和方程。问题以来自 7 个语系的 11 种语言提供。与现有基准不同，EXAMS-V 是通过收集来自不同国家、不同教育体系的学校考试问题而独特策划的。这种独特的方法需要跨不同语言进行复杂的推理，并依赖于特定区域的知识。解决数据集中的问题需要对文本和图像的视觉内容进行高级感知和联合推理。我们的评估结果表明，这是一个具有挑战性的数据集，即使对于 GPT-4V 和 Gemini 等高级视觉文本模型来说也很困难；这强调了该数据集固有的复杂性及其作为未来基准的重要性。</li>
</ul>

<h3>Title: Monotonic Representation of Numeric Properties in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Heinzerling, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10381">https://arxiv.org/abs/2403.10381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10381">https://arxiv.org/pdf/2403.10381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10381]] Monotonic Representation of Numeric Properties in Language Models(https://arxiv.org/abs/2403.10381)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we introduce a simple method for finding and editing representations of numeric properties such as an entity's birth year. Empirically, we find low-dimensional subspaces that encode numeric properties monotonically, in an interpretable and editable fashion. When editing representations along directions in these subspaces, LM output changes accordingly. For example, by patching activations along a "birthyear" direction we can make the LM express an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was born in 1957, Karl Popper was born in 1968. Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic representation of numeric properties consistently emerges during LM pretraining. Code: https://github.com/bheinzerling/numeric-property-repr</li>
<li><strong>摘要：</strong>语言模型 (LM) 可以表达涉及数字属性的事实知识，例如 Karl Popper 出生于 1902 年。然而，这些信息如何编码在模型的内部表示中尚不清楚。在这里，我们介绍一种用于查找和编辑数字属性（例如实体的出生年份）表示形式的简单方法。根据经验，我们发现低维子空间以可解释和可编辑的方式单调编码数字属性。当沿这些子空间中的方向编辑表示时，LM 输出会相应变化。例如，通过沿着“出生年份”方向修补激活，我们可以使 LM 表达越来越晚的出生年份：Karl Popper 出生于 1929 年，Karl Popper 出生于 1957 年，Karl Popper 出生于 1968 年。属性编码方向存在于跨领域所考虑的所有模型中都有几个数值属性，这表明在 LM 预训练期间数值属性的单调表示可能始终出现。代码：https://github.com/bheinzerling/numeric-property-repr</li>
</ul>

<h3>Title: Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A  Case Study on Domain-Specific Queries in Private Knowledge-Bases</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Li, Ye Yuan, Zehua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10446">https://arxiv.org/abs/2403.10446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10446">https://arxiv.org/pdf/2403.10446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10446]] Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A  Case Study on Domain-Specific Queries in Private Knowledge-Bases(https://arxiv.org/abs/2403.10446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.</li>
<li><strong>摘要：</strong>我们提出了一种端到端系统设计，旨在利用检索增强生成（RAG）来提高大型语言模型（LLM）针对与私有知识库相关的特定领域和时间敏感查询的事实准确性。我们的系统将 RAG 管道与上游数据集处理和下游性能评估集成在一起。为了解决 LLM 幻觉的挑战，我们使用源自 CMU 广泛资源并用教师模型注释的精选数据集对模型进行微调。我们的实验证明了该系统可以有效地为特定领域和时间敏感的查询生成更准确的答案。结果还揭示了使用小规模和倾斜数据集微调法学硕士的局限性。这项研究强调了 RAG 系统在利用外部数据集增强法学硕士方面的潜力，以提高知识密集型任务的性能。我们的代码和模型可以在 Github 上找到。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
