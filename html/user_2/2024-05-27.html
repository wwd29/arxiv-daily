<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-27</h1>
<h3>Title: DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Zijian Zhou, Xiaoqiang Lin, Xinyi Xu, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) allows transformer-based language models that are pre-trained on general text to quickly learn a specific task with a few "task demonstrations" without updating their parameters, significantly boosting their flexibility and generality. ICL possesses many distinct characteristics from conventional machine learning, thereby requiring new approaches to interpret this learning paradigm. Taking the viewpoint of recent works showing that transformers learn in context by formulating an internal optimizer, we propose an influence function-based attribution technique, DETAIL, that addresses the specific characteristics of ICL. We empirically verify the effectiveness of our approach for demonstration attribution while being computationally efficient. Leveraging the results, we then show how DETAIL can help improve model performance in real-world scenarios through demonstration reordering and curation. Finally, we experimentally prove the wide applicability of DETAIL by showing our attribution scores obtained on white-box models are transferable to black-box models in improving model performance.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 允许在一般文本上预先训练的基于 Transformer 的语言模型通过一些“任务演示”快速学习特定任务而无需更新其参数，从而显著提高其灵活性和通用性。ICL 具有许多与传统机器学习不同的特征，因此需要新的方法来解释这种学习范式。从最近的研究表明 Transformer 通过制定内部优化器在上下文中学习的观点来看，我们提出了一种基于影响函数的归因技术 DETAIL，该技术解决了 ICL 的特定特征。我们通过经验验证了我们的方法在计算效率高的同时，对于演示归因的​​有效性。利用这些结果，我们展示了 DETAIL 如何通过演示重新排序和管理来帮助提高模型在现实场景中的表现。最后，我们通过实验证明了 DETAIL 的广泛适用性，证明了我们在白盒模型上获得的归因分数可以转移到黑盒模型以提高模型性能。</li>
</ul>

<h3>Title: Linking In-context Learning in Transformers to Human Episodic Memory</h3>
<ul>
<li><strong>Authors: </strong>Li Ji-An, Corey Y. Zhou, Marcus K. Benna, Marcelo G. Mattar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Linking In-context Learning in Transformers to Human Episodic Memory(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Understanding the connections between artificial and biological intelligent systems can reveal fundamental principles underlying general intelligence. While many artificial intelligence (AI) models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between attention heads and human episodic memory. We focus on the induction heads, which contribute to the in-context learning capabilities of Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate model layers and that their behavior qualitatively mirrors the memory biases seen in humans. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.</li>
<li><strong>摘要：</strong>了解人工智能和生物智能系统之间的联系可以揭示通用智能的基本原理。虽然许多人工智能 (AI) 模型都有神经科学对应物，但 Transformer 模型和自注意力机制中很大程度上缺少这种联系。在这里，我们研究了注意力头和人类情景记忆之间的关系。我们重点关注归纳头，它有助于基于 Transformer 的大型语言模型 (LLM) 的上下文学习能力。我们证明感应头在行为、功能和机制上与人类情景记忆的情境维护和检索（CMR）模型相似。我们对基于大量文本数据进行预训练的法学硕士的分析表明，类似 CMR 的头部经常出现在中间模型层中，并且它们的行为定性地反映了人类的记忆偏差。我们的研究结果揭示了法学硕士和人类记忆的计算机制之间的相似之处，为这两个研究领域提供了宝贵的见解。</li>
</ul>

<h3>Title: RE-Adapt: Reverse Engineered Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>William Fleshman, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] RE-Adapt: Reverse Engineered Adaptation of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>We introduce RE-Adapt, an approach to fine-tuning large language models on new domains without degrading any pre-existing instruction-tuning. We reverse engineer an adapter which isolates what an instruction-tuned model has learned beyond its corresponding pretrained base model. Importantly, this requires no additional data or training. We can then fine-tune the base model on a new domain and readapt it to instruction following with the reverse engineered adapter. RE-Adapt and our low-rank variant LoRE-Adapt both outperform other methods of fine-tuning, across multiple popular LLMs and datasets, even when the models are used in conjunction with retrieval-augmented generation.</li>
<li><strong>摘要：</strong>我们引入了 RE-Adapt，这是一种在新领域微调大型语言模型的方法，而不会降低任何预先存在的指令调整。我们对一个适配器进行逆向工程，该适配器将指令调整模型所学到的知识隔离到其相应的预训练基础模型之外。重要的是，这不需要额外的数据或培训。然后，我们可以在新域上微调基本模型，并使用逆向工程适配器将其重新适应指令。 RE-Adapt 和我们的低排名变体 LoRE-Adapt 在多个流行的 LLM 和数据集中都优于其他微调方法，即使模型与检索增强生成结合使用也是如此。</li>
</ul>

<h3>Title: Extracting Prompts by Inverting LLM Outputs</h3>
<ul>
<li><strong>Authors: </strong>Collin Zhang, John X. Morris, Vitaly Shmatikov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Extracting Prompts by Inverting LLM Outputs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We consider the problem of language model inversion: given outputs of a language model, we seek to extract the prompt that generated these outputs. We develop a new black-box method, output2prompt, that learns to extract prompts without access to the model's logits and without adversarial or jailbreaking queries. In contrast to previous work, output2prompt only needs outputs of normal user queries. To improve memory efficiency, output2prompt employs a new sparse encoding techique. We measure the efficacy of output2prompt on a variety of user and system prompts and demonstrate zero-shot transferability across different LLMs.</li>
<li><strong>摘要：</strong>我们考虑语言模型反转的问题：给定语言模型的输出，我们寻求提取生成这些输出的提示。我们开发了一种新的黑盒方法，output2prompt，它可以学习提取提示，而无需访问模型的 logits，也无需进行对抗性或越狱查询。与之前的工作相比，output2prompt 只需要普通用户查询的输出。为了提高内存效率，output2prompt 采用了新的稀疏编码技术。我们测量了output2prompt在各种用户和系统提示上的功效，并演示了不同法学硕士之间的零样本可转移性。</li>
</ul>

<h3>Title: AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Revanth Gangi Reddy, Omar Attia, Yunyao Li, Heng Ji, Saloni Potdar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Ranking is a fundamental and popular problem in search. However, existing ranking algorithms usually restrict the granularity of ranking to full passages or require a specific dense index for each desired level of granularity. Such lack of flexibility in granularity negatively affects many applications that can benefit from more granular ranking, such as sentence-level ranking for open-domain question-answering, or proposition-level ranking for attribution. In this work, we introduce the idea of any-granularity ranking, which leverages multi-vector embeddings to rank at varying levels of granularity while maintaining encoding at a single (coarser) level of granularity. We propose a multi-granular contrastive loss for training multi-vector approaches, and validate its utility with both sentences and propositions as ranking units. Finally, we demonstrate the application of proposition-level ranking to post-hoc citation addition in retrieval-augmented generation, surpassing the performance of prompt-driven citation generation.</li>
<li><strong>摘要：</strong>排名是搜索中的一个基本且流行的问题。然而，现有的排名算法通常将排名的粒度限制为完整段落，或者需要针对每个所需粒度级别的特定密集索引。这种粒度缺乏灵活性会对许多可以从更细粒度的排名中受益的应用程序产生负面影响，例如开放域问答的句子级排名或归因的命题级排名。在这项工作中，我们引入了任意粒度排序的想法，它利用多向量嵌入在不同的粒度级别上进行排序，同时将编码保持在单个（较粗）粒度级别。我们提出了一种用于训练多向量方法的多粒度对比损失，并以句子和命题作为排名单位验证其实用性。最后，我们演示了命题级排序在检索增强生成中的事后引文添加中的应用，超越了提示驱动引文生成的性能。</li>
</ul>

<h3>Title: Aya 23: Open Weight Releases to Further Multilingual Progress</h3>
<ul>
<li><strong>Authors: </strong>Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Aya 23: Open Weight Releases to Further Multilingual Progress(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model (Üstün et al., 2024), focusing on pairing a highly performant pre-trained model with the recently released Aya collection (Singh et al., 2024). The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world's population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. We release the open weights for both the 8B and 35B models as part of our continued commitment for expanding access to multilingual progress.</li>
<li><strong>摘要：</strong>本技术报告介绍了 Aya 23，一个多语言语言模型家族。 Aya 23 基于最近发布的 Aya 模型（Üstün 等人，2024）构建，专注于将高性能的预训练模型与最近发布的 Aya 集合（Singh 等人，2024）配对。结果是一个强大的多语言大型语言模型，可服务 23 种语言，将最先进的语言建模能力扩展到世界上大约一半的人口。 Aya 模型涵盖 101 种语言，而 Aya 23 是一个深度与广度的实验，探索将更多容量分配给预训练期间包含的更少语言的影响。 Aya 23 在涵盖的语言方面优于 Aya 101 等之前的大规模多语言模型，在广泛的判别和生成任务中也优于 Gemma、Mistral 和 Mixtral 等广泛使用的模型。作为我们不断致力于扩大多语言进步的一部分，我们发布了 8B 和 35B 模型的开放权重。</li>
</ul>

<h3>Title: CEEBERT: Cross-Domain Inference in Early Exit BERT</h3>
<ul>
<li><strong>Authors: </strong>Divya Jyoti Bajpai, Manjesh Kumar Hanawal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] CEEBERT: Cross-Domain Inference in Early Exit BERT(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-trained Language Models (PLMs), like BERT, with self-supervision objectives exhibit remarkable performance and generalization across various tasks. However, they suffer in inference latency due to their large size. To address this issue, side branches are attached at intermediate layers, enabling early inference of samples without requiring them to pass through all layers. However, the challenge is to decide which layer to infer and exit each sample so that the accuracy and latency are balanced. Moreover, the distribution of the samples to be inferred may differ from that used for training necessitating cross-domain adaptation. We propose an online learning algorithm named Cross-Domain Inference in Early Exit BERT (CeeBERT) that dynamically determines early exits of samples based on the level of confidence at each exit point. CeeBERT learns optimal thresholds from domain-specific confidence observed at intermediate layers on the fly, eliminating the need for labeled data. Experimental results on five distinct datasets with BERT and ALBERT models demonstrate CeeBERT's ability to improve latency by reducing unnecessary computations with minimal drop in performance. By adapting to the threshold values, CeeBERT can speed up the BERT/ALBERT models by $2\times$ - $3.5\times$ with minimal drop in accuracy.</li>
<li><strong>摘要：</strong>具有自我监督目标的预训练语言模型 (PLM)，例如 BERT，在各种任务中表现出卓越的性能和泛化能力。然而，由于其尺寸较大，它们会受到推理延迟的影响。为了解决这个问题，在中间层附加了侧分支，从而可以对样本进行早期推理，而无需它们穿过所有层。然而，挑战在于决定在哪一层推断并退出每个样本，以便平衡准确性和延迟。此外，要推断的样本分布可能与用于需要跨域适应的训练的分布不同。我们在早期退出 BERT (CeeBERT) 中提出了一种名为跨域推理的在线学习算法，该算法根据每个退出点的置信水平动态确定样本的早期退出。 CeeBERT 从中间层动态观察到的特定领域置信度中学习最佳阈值，从而无需标记数据。使用 BERT 和 ALBERT 模型在五个不同数据集上进行的实验结果表明，CeeBERT 能够通过减少不必要的计算来改善延迟，同时将性能下降降至最低。通过适应阈值，CeeBERT 可以将 BERT/ALBERT 模型的速度提高 $2\times$ - $3.5\times$，同时精度下降最小。</li>
</ul>

<h3>Title: Reframing Spatial Reasoning Evaluation in Language Models: A Real-World Simulation Benchmark for Qualitative Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Fangjun Li, David C. Hogg, Anthony G. Cohn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Reframing Spatial Reasoning Evaluation in Language Models: A Real-World Simulation Benchmark for Qualitative Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Spatial reasoning plays a vital role in both human cognition and machine intelligence, prompting new research into language models' (LMs) capabilities in this regard. However, existing benchmarks reveal shortcomings in evaluating qualitative spatial reasoning (QSR). These benchmarks typically present oversimplified scenarios or unclear natural language descriptions, hindering effective evaluation. We present a novel benchmark for assessing QSR in LMs, which is grounded in realistic 3D simulation data, offering a series of diverse room layouts with various objects and their spatial relationships. This approach provides a more detailed and context-rich narrative for spatial reasoning evaluation, diverging from traditional, toy-task-oriented scenarios. Our benchmark encompasses a broad spectrum of qualitative spatial relationships, including topological, directional, and distance relations. These are presented with different viewing points, varied granularities, and density of relation constraints to mimic real-world complexities. A key contribution is our logic-based consistency-checking tool, which enables the assessment of multiple plausible solutions, aligning with real-world scenarios where spatial relationships are often open to interpretation. Our benchmark evaluation of advanced LMs reveals their strengths and limitations in spatial reasoning. They face difficulties with multi-hop spatial reasoning and interpreting a mix of different view descriptions, pointing to areas for future improvement.</li>
<li><strong>摘要：</strong>空间推理在人类认知和机器智能中都发挥着至关重要的作用，促进了对语言模型（LM）在这方面能力的新研究。然而，现有的基准揭示了评估定性空间推理（QSR）的缺陷。这些基准通常呈现过于简单的场景或不清楚的自然语言描述，阻碍了有效的评估。我们提出了一种评估 LM 中 QSR 的新颖基准，该基准基于真实的 3D 模拟数据，提供了一系列具有各种对象及其空间关系的不同房间布局。这种方法为空间推理评估提供了更详细、上下文更丰富的叙述，与传统的、面向玩具任务的场景不同。我们的基准涵盖广泛的定性空间关系，包括拓扑、方向和距离关系。这些以不同的视角、不同的粒度和关系约束密度来呈现，以模拟现实世界的复杂性。一个关键的贡献是我们基于逻辑的一致性检查工具，它能够评估多个看似合理的解决方案，与空间关系通常可以解释的现实世界场景保持一致。我们对高级语言模型的基准评估揭示了它们在空间推理方面的优势和局限性。他们在多跳空间推理和解释不同视图描述的混合方面面临困难，指出了未来需要改进的领域。</li>
</ul>

<h3>Title: Promoting Constructive Deliberation: Reframing for Receptiveness</h3>
<ul>
<li><strong>Authors: </strong>Gauri Kambhatla, Matthew Lease, Ashwin Rajadesingan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Promoting Constructive Deliberation: Reframing for Receptiveness(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>To promote constructive discussion of controversial topics online, we propose automatic reframing of disagreeing responses to signal receptiveness while preserving meaning. Drawing on research from psychology, communications, and linguistics, we identify six strategies for reframing. We automatically reframe replies according to each strategy, using a dataset of Reddit comments and replies. Through human-centered experiments, we find that the replies generated with our framework are perceived to be significantly more receptive than the original replies, as well as a generic receptiveness baseline. We analyze and discuss the implications of our results and highlight applications to content moderation. Overall, we illustrate how transforming receptiveness, a particular social science construct, into a computational framework, can make LLM generations more aligned with human perceptions.</li>
<li><strong>摘要：</strong>为了促进在线争议话题的建设性讨论，我们建议自动重新构建对信号接受性的不同反应，同时保留意义。根据心理学、传播学和语言学的研究，我们确定了六种重构策略。我们使用 Reddit 评论和回复数据集，根据每种策略自动重新构建回复。通过以人为中心的实验，我们发现使用我们的框架生成的回复被认为比原始回复以及通用的接受度基线更具接受性。我们分析和讨论我们的结果的含义，并重点介绍内容审核的应用。总的来说，我们说明了如何将接受性（一种特定的社会科学结构）转变为计算框架，可以使法学硕士一代更符合人类的看法。</li>
</ul>

<h3>Title: Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</h3>
<ul>
<li><strong>Authors: </strong>Boshi Wang, Xiang Yue, Yu Su, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.</li>
<li><strong>摘要：</strong>我们研究 Transformer 是否可以学习对参数知识进行隐式推理，即使是最有能力的语言模型也难以掌握这项技能。着眼于两种代表性的推理类型，组合和比较，我们一致发现 Transformer 可以学习隐式推理，但只能通过摸索，即远远超出过度拟合的扩展训练。不同推理类型的泛化水平也有所不同：当面对分布外的例子时，变压器无法系统地泛化组合，但可以成功进行比较。我们在整个训练过程中深入研究模型的内部结构，进行分析实验，揭示：1）摸索背后的机制，例如泛化回路的形成及其与泛化和记忆回路相对效率的关系，2）系统性之间的联系以及泛化电路的配置。我们的研究结果指导数据和训练设置，以更好地诱导隐式推理，并提出对变压器架构的潜在改进，例如鼓励跨层知识共享。此外，我们证明，对于具有大搜索空间的具有挑战性的推理任务，基于非参数记忆的 GPT-4-Turbo 和 Gemini-1.5-Pro 会严重失败，无论提示样式或检索增强如何，而完全 grokked 的变压器可以实现近乎完美的准确性，展示了参数记忆在复杂推理中的强大功能。</li>
</ul>

<h3>Title: Eliciting Informative Text Evaluations with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Lu, Shengwei Xu, Yichi Zhang, Yuqing Kong, Grant Schoenebeck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Eliciting Informative Text Evaluations with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Peer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models. This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media. We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset. We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels -- human-written reviews, GPT-4-generated reviews, and GPT-3.5-generated reviews in terms of expected scores. Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.</li>
<li><strong>摘要：</strong>同行预测机制通过可证明的保证激发高质量的反馈。然而，当前的方法仅适用于相当简单的报告，例如多项选择或标量数字。我们的目标是利用大型语言模型的最新发展，将这些技术扩展到更大的基于文本的报告领域。这极大地增加了同行预测机制的适用性，因为文本反馈是多种反馈渠道的常态：同行评审、电子商务客户评论和社交媒体评论。我们引入两种机制，生成同行预测机制（GPPM）和生成概要同行预测机制（GSPPM）。这些机制利用 LLM 作为预测器，将一个代理的报告映射到对其同行报告的预测。从理论上讲，我们表明，当 LLM 预测足够准确时，我们的机制可以作为（近似）贝叶斯纳什均衡来激励高度努力和说真话。根据经验，我们通过在两个真实数据集（Yelp 评论数据集和 ICLR OpenReview 数据集）上进行的实验来确认我们机制的有效性。我们强调在 ICLR 数据集上的结果，我们的机制可以根据预期分数区分三个质量级别——人工撰写的评论、GPT-4 生成的评论和 GPT-3.5 生成的评论。此外，GSPPM 比 GPPM 更有效地惩罚 LLM 生成的评审。</li>
</ul>

<h3>Title: Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Suyoung Kim, Jiyeon Hwang, Ho-Young Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken Language Understanding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, deep end-to-end learning has been studied for intent classification in Spoken Language Understanding (SLU). However, end-to-end models require a large amount of speech data with intent labels, and highly optimized models are generally sensitive to the inconsistency between the training and evaluation conditions. Therefore, a natural language understanding approach based on Automatic Speech Recognition (ASR) remains attractive because it can utilize a pre-trained general language model and adapt to the mismatch of the speech input environment. Using this module-based approach, we improve a noisy-channel model to handle transcription inconsistencies caused by ASR errors. We propose a two-stage method, Contrastive and Consistency Learning (CCL), that correlates error patterns between clean and noisy ASR transcripts and emphasizes the consistency of the latent features of the two transcripts. Experiments on four benchmark datasets show that CCL outperforms existing methods and improves the ASR robustness in various noisy environments. Code is available at this https URL.</li>
<li><strong>摘要：</strong>最近，深度端到端学习已被研究用于口语理解（SLU）中的意图分类。然而，端到端模型需要大量带有意图标签的语音数据，并且高度优化的模型通常对训练和评估条件之间的不一致敏感。因此，基于自动语音识别（ASR）的自然语言理解方法仍然具有吸引力，因为它可以利用预先训练的通用语言模型并适应语音输入环境的不匹配。使用这种基于模块的方法，我们改进了噪声通道模型，以处理由 ASR 错误引起的转录不一致。我们提出了一种两阶段方法，即对比和一致性学习（CCL），它将干净和嘈杂的 ASR 转录本之间的错误模式关联起来，并强调两个转录本的潜在特征的一致性。在四个基准数据集上的实验表明，CCL 优于现有方法，并提高了 ASR 在各种噪声环境中的鲁棒性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: CHARP: Conversation History AwaReness Probing for Knowledge-grounded Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Abbas Ghaddar, David Alfonso-Hermelo, Philippe Langlais, Mehdi Rezagholizadeh, Boxing Chen, Prasanna Parthasarathi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] CHARP: Conversation History AwaReness Probing for Knowledge-grounded Dialogue Systems(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>In this work, we dive deep into one of the popular knowledge-grounded dialogue benchmarks that focus on faithfulness, FaithDial. We show that a significant portion of the FaithDial data contains annotation artifacts, which may bias models towards completely ignoring the conversation history. We therefore introduce CHARP, a diagnostic test set, designed for an improved evaluation of hallucinations in conversational model. CHARP not only measures hallucination but also the compliance of the models to the conversation task. Our extensive analysis reveals that models primarily exhibit poor performance on CHARP due to their inability to effectively attend to and reason over the conversation history. Furthermore, the evaluation methods of FaithDial fail to capture these shortcomings, neglecting the conversational history. Our findings indicate that there is substantial room for contribution in both dataset creation and hallucination evaluation for knowledge-grounded dialogue, and that CHARP can serve as a tool for monitoring the progress in this particular research area. CHARP is publicly available at this https URL</li>
<li><strong>摘要：</strong>在这项工作中，我们深入研究了一种流行的以知识为基础的对话基准，它专注于忠诚度，FaithDial。我们发现 FaithDial 数据的很大一部分包含注释工件，这可能会使模型偏向于完全忽略对话历史记录。因此，我们引入了 CHARP，一种诊断测试集，旨在改进对话模型中幻觉的评估。 CHARP 不仅测量幻觉，还测量模型对对话任务的符合性。我们的广泛分析表明，模型在 CHARP 上表现不佳主要是因为它们无法有效地关注和推理对话历史记录。此外，FaithDial 的评估方法未能捕捉到这些缺点，忽略了对话历史。我们的研究结果表明，在基于知识的对话的数据集创建和幻觉评估方面都有很大的贡献空间，并且 CHARP 可以作为监测这一特定研究领域进展的工具。 CHARP 可通过此 https URL 公开获取</li>
</ul>

<h3>Title: Generalizable and Scalable Multistage Biomedical Concept Normalization Leveraging Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nicholas J Dobbins</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Generalizable and Scalable Multistage Biomedical Concept Normalization Leveraging Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Background: Biomedical entity normalization is critical to biomedical research because the richness of free-text clinical data, such as progress notes, can often be fully leveraged only after translating words and phrases into structured and coded representations suitable for analysis. Large Language Models (LLMs), in turn, have shown great potential and high performance in a variety of natural language processing (NLP) tasks, but their application for normalization remains understudied. Methods: We applied both proprietary and open-source LLMs in combination with several rule-based normalization systems commonly used in biomedical research. We used a two-step LLM integration approach, (1) using an LLM to generate alternative phrasings of a source utterance, and (2) to prune candidate UMLS concepts, using a variety of prompting methods. We measure results by $F_{\beta}$, where we favor recall over precision, and F1. Results: We evaluated a total of 5,523 concept terms and text contexts from a publicly available dataset of human-annotated biomedical abstracts. Incorporating GPT-3.5-turbo increased overall $F_{\beta}$ and F1 in normalization systems +9.5 and +7.3 (MetaMapLite), +13.9 and +10.9 (QuickUMLS), and +10.5 and +10.3 (BM25), while the open-source Vicuna model achieved +10.8 and +12.2 (MetaMapLite), +14.7 and +15 (QuickUMLS), and +15.6 and +18.7 (BM25). Conclusions: Existing general-purpose LLMs, both propriety and open-source, can be leveraged at scale to greatly improve normalization performance using existing tools, with no fine-tuning.</li>
<li><strong>摘要：</strong>背景：生物医学实体规范化对于生物医学研究至关重要，因为只有将单词和短语翻译成适合分析的结构化和编码表示后，通常才能充分利用自由文本临床数据（例如进度注释）的丰富性。反过来，大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中显示出巨大的潜力和高性能，但其在规范化方面的应用仍未得到充分研究。方法：我们将专有和开源法学硕士与生物医学研究中常用的几种基于规则的标准化系统相结合。我们使用了两步 LLM 集成方法，(1) 使用 LLM 生成源话语的替代短语，以及 (2) 使用各种提示方法来修剪候选 UMLS 概念。我们通过 $F_{\beta}$ 和 F1 来衡量结果，其中我们更倾向于召回率而不是精度。结果：我们从公开的人类注释生物医学摘要数据集中评估了总共 5,523 个概念术语和文本上下文。纳入 GPT-3.5-turbo 提高了标准化系统中的整体 $F_{\beta}$ 和 F1 +9.5 和 +7.3 (MetaMapLite)、+13.9 和 +10.9 (QuickUMLS) 以及 +10.5 和 +10.3 (BM25)，而开源 Vicuna 模型取得了 +10.8 和 +12.2 (MetaMapLite)、+14.7 和 +15 (QuickUMLS) 以及 +15.6 和 +18.7 (BM25)。结论：现有的通用 LLM，无论是专有的还是开源的，都可以大规模利用现有工具来极大地提高规范化性能，而无需进行微调。</li>
</ul>

<h3>Title: Efficient Biomedical Entity Linking: Clinical Text Standardization with Low-Resource Techniques</h3>
<ul>
<li><strong>Authors: </strong>Akshit Achara, Sanand Sasidharan, Gagan N</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Efficient Biomedical Entity Linking: Clinical Text Standardization with Low-Resource Techniques(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Clinical text is rich in information, with mentions of treatment, medication and anatomy among many other clinical terms. Multiple terms can refer to the same core concepts which can be referred as a clinical entity. Ontologies like the Unified Medical Language System (UMLS) are developed and maintained to store millions of clinical entities including the definitions, relations and other corresponding information. These ontologies are used for standardization of clinical text by normalizing varying surface forms of a clinical term through Biomedical entity linking. With the introduction of transformer-based language models, there has been significant progress in Biomedical entity linking. In this work, we focus on learning through synonym pairs associated with the entities. As compared to the existing approaches, our approach significantly reduces the training data and resource consumption. Moreover, we propose a suite of context-based and context-less reranking techniques for performing the entity disambiguation. Overall, we achieve similar performance to the state-of-the-art zero-shot and distant supervised entity linking techniques on the Medmentions dataset, the largest annotated dataset on UMLS, without any domain-based training. Finally, we show that retrieval performance alone might not be sufficient as an evaluation metric and introduce an article level quantitative and qualitative analysis to reveal further insights on the performance of entity linking methods.</li>
<li><strong>摘要：</strong>临床文本信息丰富，提到了治疗、药物和解剖学以及许多其他临床术语。多个术语可以指代相同的核心概念，这些概念可以被称为临床实体。开发和维护诸如统一医学语言系统（UMLS）之类的本体来存储数百万个临床实体，包括定义、关系和其他相应信息。这些本体用于通过生物医学实体链接标准化临床术语的不同表面形式，从而实现临床文本的标准化。随着基于变压器的语言模型的引入，生物医学实体链接取得了重大进展。在这项工作中，我们专注于通过与实体相关的同义词对进行学习。与现有方法相比，我们的方法显着减少了训练数据和资源消耗。此外，我们提出了一套基于上下文和无上下文的重新排序技术来执行实体消歧。总体而言，我们在 Medmentions 数据集（UMLS 上最大的注释数据集）上实现了与最先进的零样本和远程监督实体链接技术类似的性能，而无需任何基于领域的训练。最后，我们表明，单独的检索性能可能不足以作为评估指标，并引入文章级别的定量和定性分析，以揭示对实体链接方法性能的进一步见解。</li>
</ul>

<h3>Title: Machine Unlearning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Saaketh Koundinya Gundavarapu, Shreya Agarwal, Arushi Arora, Chandana Thimmalapura Jagadeeshaiah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Machine Unlearning in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Machine unlearning, a novel area within artificial intelligence, focuses on addressing the challenge of selectively forgetting or reducing undesirable knowledge or behaviors in machine learning models, particularly in the context of large language models (LLMs). This paper introduces a methodology to align LLMs, such as Open Pre-trained Transformer Language Models, with ethical, privacy, and safety standards by leveraging the gradient ascent algorithm for knowledge unlearning. Our approach aims to selectively erase or modify learned information in LLMs, targeting harmful responses and copyrighted content. This paper presents a dual-pronged approach to enhance the ethical and safe behavior of large language models (LLMs) by addressing the issues of harmful responses and copyrighted content. To mitigate harmful responses, we applied gradient ascent on the PKU dataset, achieving a 75\% reduction in harmful responses for Open Pre-trained Transformer Language Models (OPT1.3b and OPT2.7b) \citet{zhang2022opt} while retaining previous knowledge using the TruthfulQA dataset \citet{DBLP:journals/corr/abs-2109-07958}. For handling copyrighted content, we constructed a custom dataset based on the Lord of the Rings corpus and aligned LLMs (OPT1.3b and OPT2.7b) \citet{zhang2022opt} through LoRA: Low-Rank Adaptation of Large Language Models \citet{DBLP:journals/corr/abs-2106-09685} finetuning. Subsequently, we employed gradient ascent to unlearn the Lord of the Rings content, resulting in a remarkable reduction in the presence of copyrighted material. To maintain a diverse knowledge base, we utilized the Book Corpus dataset. Additionally, we propose a new evaluation technique for assessing the effectiveness of harmful unlearning.</li>
<li><strong>摘要：</strong>机器遗忘是人工智能中的一个新领域，专注于解决机器学习模型中选择性遗忘或减少不良知识或行为的挑战，特别是在大型语言模型（LLM）的背景下。本文介绍了一种方法，通过利用梯度上升算法进行知识遗忘，使法学硕士（例如开放式预训练 Transformer 语言模型）与道德、隐私和安全标准保持一致。我们的方法旨在有选择地删除或修改法学硕士中学到的信息，针对有害反应和受版权保护的内容。本文提出了一种双管齐下的方法，通过解决有害反应和受版权保护的内容问题来增强大型语言模型（LLM）的道德和安全行为。为了减轻有害反应，我们在 PKU 数据集上应用了梯度上升，使开放预训练 Transformer 语言模型（OPT1.3b 和 OPT2.7b）的有害反应减少了 75%\citet{zhang2022opt}，同时保留了以前的知识TruthfulQA 数据集 \citet{DBLP:journals/corr/abs-2109-07958}。为了处理受版权保护的内容，我们基于《指环王》语料库构建了一个自定义数据集，并通过 LoRA 对齐的 LLM（OPT1.3b 和 OPT2.7b）\citet{zhang2022opt}：大型语言模型的低阶适应 \citet{DBLP ：journals/corr/abs-2106-09685} 微调。随后，我们采用梯度上升来忘记《指环王》的内容，从而显着减少受版权保护的材料。为了维护多样化的知识库，我们利用了图书语料库数据集。此外，我们提出了一种新的评估技术来评估有害遗忘的有效性。</li>
</ul>

<h3>Title: A Solution-based LLM API-using Methodology for Academic Information Seeking</h3>
<ul>
<li><strong>Authors: </strong>Yuanchun Wang, Jifan Yu, Zijun Yao, Jing Zhang, Yuyang Xie, Shangqing Tu, Yiyang Fu, Youhe Feng, Jinkai Zhang, Jingyao Zhang, Bowen Huang, Yuanyao Li, Huihui Yuan, Lei Hou, Juanzi Li, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] A Solution-based LLM API-using Methodology for Academic Information Seeking(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Applying large language models (LLMs) for academic API usage shows promise in reducing researchers' academic information seeking efforts. However, current LLM API-using methods struggle with complex API coupling commonly encountered in academic queries. To address this, we introduce SoAy, a solution-based LLM API-using methodology for academic information seeking. It uses code with a solution as the reasoning method, where a solution is a pre-constructed API calling sequence. The addition of the solution reduces the difficulty for the model to understand the complex relationships between APIs. Code improves the efficiency of reasoning. To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied by SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental results demonstrate a 34.58-75.99\% performance improvement compared to state-of-the-art LLM API-based baselines. All datasets, codes, tuned models, and deployed online services are publicly accessible at this https URL.</li>
<li><strong>摘要：</strong>将大型语言模型 (LLM) 应用于学术 API 使用表明有望减少研究人员的学术信息搜索工作。然而，当前的 LLM API 使用方法很难应对学术查询中常见的复杂 API 耦合问题。为了解决这个问题，我们引入了 SoAy，一种基于解决方案的 LLM API 使用方法来查找学术信息。它使用带有解决方案的代码作为推理方法，其中解决方案是预先构建的 API 调用序列。该解决方案的加入降低了模型理解API之间复杂关系的难度。代码提高了推理效率。为了评估 SoAy，我们引入了 SoAyBench，这是一个伴随 SoAyEval 的评估基准，构建在 AMiner API 的克隆环境之上。实验结果表明，与最先进的基于 LLM API 的基线相比，性能提高了 34.58-75.99%。所有数据集、代码、调整模型和部署的在线服务都可以通过此 https URL 公开访问。</li>
</ul>

<h3>Title: VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Shaobo Han, Shihao Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a "divide-and-share" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules and layers by sharing parameters globally via a \textit{vector bank}. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites \textit{all} the low-rank matrices of LoRA from a shared \textit{vector bank} with a differentiable top-$k$ admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, and instruction tuning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4\% of LoRA's stored parameters yet attaining superior results. Our source code is available at \url{this https URL}.</li>
<li><strong>摘要：</strong>随着大型语言模型的采用增加以及对每个用户或每个任务模型定制的需求的增长，参数高效微调（PEFT）方法，例如低秩适应（LoRA）及其变体，会产生大量的费用。存储和传输成本。为了进一步减少存储的参数，我们引入了“分而共享”范例，通过 \textit{向量库} 全局共享参数，打破了跨矩阵维度、模块和层的低秩分解的障碍。作为 LoRA 范例的实例，我们提出的 VB-LoRA 将来自共享 \textit{向量库} 的 LoRA 低秩矩阵 \textit{all} 与可微分的 top-$k$ 混合模块组合在一起。与最先进的 PEFT 方法相比，VB-LoRA 实现了极高的参数效率，同时保持了可比或更好的性能。大量实验证明了 VB-LoRA 在自然语言理解、自然语言生成和指令调优任务方面的有效性。在微调 Llama2-13B 模型时，VB-LoRA 仅使用 LoRA 存储参数的 0.4%，却获得了优异的结果。我们的源代码可在 \url{此 https URL} 中找到。</li>
</ul>

<h3>Title: An Evaluation of Estimative Uncertainty in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhisheng Tang, Ke Shen, Mayank Kejriwal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] An Evaluation of Estimative Uncertainty in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Words of estimative probability (WEPs), such as ''maybe'' or ''probably not'' are ubiquitous in natural language for communicating estimative uncertainty, compared with direct statements involving numerical probability. Human estimative uncertainty, and its calibration with numerical estimates, has long been an area of study -- including by intelligence agencies like the CIA. This study compares estimative uncertainty in commonly used large language models (LLMs) like GPT-4 and ERNIE-4 to that of humans, and to each other. Here we show that LLMs like GPT-3.5 and GPT-4 align with human estimates for some, but not all, WEPs presented in English. Divergence is also observed when the LLM is presented with gendered roles and Chinese contexts. Further study shows that an advanced LLM like GPT-4 can consistently map between statistical and estimative uncertainty, but a significant performance gap remains. The results contribute to a growing body of research on human-LLM alignment.</li>
<li><strong>摘要：</strong>与直接涉及数值概率的陈述相比，估计概率词 (WEP)，例如“可能”或“可能不会”在自然语言中普遍用于传达估计不确定性。人类的估计不确定性及其与数值估计的校准一直是一个研究领域——包括中央情报局 (CIA) 等情报机构。这项研究将 GPT-4 和 ERNIE-4 等常用大型语言模型 (LLM) 中的估计不确定性与人类的估计不确定性以及彼此之间的估计不确定性进行了比较。在这里，我们表明，对于一些（但不是全部）以英语呈现的 WEP，GPT-3.5 和 GPT-4 等 LLM 与人类估计一致。当 LLM 呈现性别角色和中文背景时也会观察到差异。进一步的研究表明，像 GPT-4 这样的高级 LLM 可以在统计和估计不确定性之间保持一致，但仍然存在显着的性能差距。这些结果有助于越来越多的关于人类-LLM 对齐的研究。</li>
</ul>

<h3>Title: RAEE: A Training-Free Retrieval-Augmented Early Exiting Framework for Efficient Inference</h3>
<ul>
<li><strong>Authors: </strong>Lianming Huang, Shangyu Wu, Yufei Cui, Ying Xiong, Xue Liu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] RAEE: A Training-Free Retrieval-Augmented Early Exiting Framework for Efficient Inference(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Deploying large language model inference remains challenging due to their high computational overhead. Early exiting accelerates model inference by adaptively reducing the number of inference layers. Existing methods require training internal classifiers to determine whether to exit at each intermediate layer. However, such classifier-based early exiting frameworks require significant effort to design and train the classifiers. To address these limitations, this paper proposes RAEE, a training-free Retrieval-Augmented Early Exiting framework for efficient inference. First, this paper demonstrates that the early exiting problem can be modeled as a distribution prediction problem, where the distribution is approximated using similar data's existing information. Next, the paper details the process of collecting existing information to build the retrieval database. Finally, based on the pre-built retrieval database, RAEE leverages the retrieved similar data's exiting information to guide the backbone model to exit at the layer, which is predicted by the approximated distribution. Experimental results demonstrate that the proposed RAEE can significantly accelerate inference. RAEE also achieves state-of-the-art zero-shot performance on 8 classification tasks.</li>
<li><strong>摘要：</strong>由于计算开销较高，部署大型语言模型推理仍然具有挑战性。提前退出通过自适应减少推理层数来加速模型推理。现有方法需要训练内部分类器来确定是否在每个中间层退出。然而，这种基于分类器的早期现有框架需要付出巨大的努力来设计和训练分类器。为了解决这些限制，本文提出了 RAEE，一种无需训练的检索增强早期退出框架，用于高效推理。首先，本文证明了早期存在的问题可以建模为分布预测问题，其中使用类似数据的现有信息来近似分布。接下来，本文详细介绍了收集现有信息构建检索数据库的过程。最后，基于预先建立的检索数据库，RAEE利用检索到的相似数据的存在信息来引导骨干模型在该层退出，这是通过近似分布来预测的。实验结果表明，所提出的 RAEE 可以显着加速推理速度。 RAEE 还在 8 个分类任务上实现了最先进的零样本性能。</li>
</ul>

<h3>Title: Cross-Task Defense: Instruction-Tuning LLMs for Content Safety</h3>
<ul>
<li><strong>Authors: </strong>Yu Fu, Wen Xiao, Jia Chen, Jiachen Li, Evangelos Papalexakis, Aichi Chien, Yue Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Cross-Task Defense: Instruction-Tuning LLMs for Content Safety(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent studies reveal that Large Language Models (LLMs) face challenges in balancing safety with utility, particularly when processing long texts for NLP tasks like summarization and translation. Despite defenses against malicious short questions, the ability of LLMs to safely handle dangerous long content, such as manuals teaching illicit activities, remains unclear. Our work aims to develop robust defenses for LLMs in processing malicious documents alongside benign NLP task queries. We introduce a defense dataset comprised of safety-related examples and propose single-task and mixed-task losses for instruction tuning. Our empirical results demonstrate that LLMs can significantly enhance their capacity to safely manage dangerous content with appropriate instruction tuning. Additionally, strengthening the defenses of tasks most susceptible to misuse is effective in protecting LLMs against processing harmful information. We also observe that trade-offs between utility and safety exist in defense strategies, where Llama2, utilizing our proposed approach, displays a significantly better balance compared to Llama1.</li>
<li><strong>摘要：</strong>最近的研究表明，大型语言模型 (LLM) 在平衡安全性与实用性方面面临挑战，特别是在处理摘要和翻译等 NLP 任务的长文本时。尽管有针对恶意短问题的防御措施，但法学硕士安全处理危险的长内容（例如教授非法活动的手册）的能力仍不清楚。我们的工作旨在为法学硕士开发强大的防御措施，以处理恶意文档以及良性 NLP 任务查询。我们引入了由安全相关示例组成的防御数据集，并提出了用于指令调整的单任务和混合任务损失。我们的实证结果表明，法学硕士可以通过适当的指令调整来显着增强其安全管理危险内容的能力。此外，加强对最容易被滥用的任务的防御可以有效地保护法学硕士免遭处理有害信息的侵害。我们还观察到防御策略中存在实用性和安全性之间的权衡，其中利用我们提出的方法的 Llama2 与 Llama1 相比表现出明显更好的平衡。</li>
</ul>

<h3>Title: Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Sun, Hongzhi Zhang, Zijia Lin, Jingyuan Zhang, Fuzheng Zhang, Zhongyuan Wang, Bin Chen, Chengru Song, Di Zhang, Kun Gai, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated exceptional capability in natural language understanding and generation. However, their generation speed is limited by the inherently sequential nature of their decoding process, posing challenges for real-time applications. This paper introduces Lexical Unit Decoding (LUD), a novel decoding methodology implemented in a data-driven manner, accelerating the decoding process without sacrificing output quality. The core of our approach is the observation that a pre-trained language model can confidently predict multiple contiguous tokens, forming the basis for a \textit{lexical unit}, in which these contiguous tokens could be decoded in parallel. Extensive experiments validate that our method substantially reduces decoding time while maintaining generation quality, i.e., 33\% speed up on natural language generation with no quality loss, and 30\% speed up on code generation with a negligible quality loss of 3\%. Distinctively, LUD requires no auxiliary models and does not require changes to existing architectures. It can also be integrated with other decoding acceleration methods, thus achieving an even more pronounced inference efficiency boost. We posit that the foundational principles of LUD could define a new decoding paradigm for future language models, enhancing their applicability for a broader spectrum of applications. All codes are be publicly available at this https URL. Keywords: Parallel Decoding, Lexical Unit Decoding, Large Language Model</li>
<li><strong>摘要：</strong>大型语言模型在自然语言理解和生成方面表现出了卓越的能力。然而，它们的生成速度受到解码过程固有的顺序性质的限制，这给实时应用带来了挑战。本文介绍了词汇单元解码（LUD），这是一种以数据驱动方式实现的新颖解码方法，可在不牺牲输出质量的情况下加速解码过程。我们方法的核心是观察到预训练的语言模型可以自信地预测多个连续的标记，形成 \textit{词汇单元} 的基础，其中这些连续的标记可以并行解码。大量实验验证了我们的方法在保持生成质量的同时大大减少了解码时间，即自然语言生成速度提高了 33%，且没有质量损失；代码生成速度提高了 30%，质量损失可忽略不计，质量损失为 3%。与众不同的是，LUD 不需要辅助模型，也不需要更改现有架构。它还可以与其他解码加速方法集成，从而实现更显着的推理效率提升。我们认为 LUD 的基本原理可以为未来的语言模型定义新的解码范式，从而增强其在更广泛应用中的适用性。所有代码均可通过此 https URL 公开获取。关键词：并行解码、词汇单元解码、大语言模型</li>
</ul>

<h3>Title: DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ</h3>
<ul>
<li><strong>Authors: </strong>Jonas Belouadi, Steffen Eger, Simone Paolo Ponzetto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTikZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and SciCap++, a collection of diverse scientific figures and associated metadata. We train DeTikZify on SciCap++ and DaTikZv2, along with synthetically generated sketches learned from SketchFig. We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available.</li>
<li><strong>摘要：</strong>尽管在纸上绘制想法相对容易，但创建高质量的科学图形可能既耗时又具有挑战性。此外，重新创建未以保留语义信息的格式存储的现有图形同样复杂。为了解决这个问题，我们引入了 DeTikZify，这是一种新颖的多模态语言模型，它可以根据草图和现有图形自动合成科学图形作为保留语义的 TikZ 图形程序。为了实现这一目标，我们创建了三个新数据集：DaTikZv2，迄今为止最大的 TikZ 数据集，包含超过 36 万个人工创建的 TikZ 图形； SketchFig，一个将手绘草图与其相应的科学图形配对的数据集； SciCap++，不同科学数据和相关元数据的集合。我们在 SciCap++ 和 DaTikZv2 上训练 DeTikZify，以及从 SketchFig 学习的综合生成的草图。我们还引入了基于 MCTS 的推理算法，使 DeTikZify 能够迭代地细化其输出，而无需额外的训练。通过自动和人工评估，我们证明 DeTikZify 在合成 TikZ 程序方面优于商业 Claude 3 和 GPT-4V，MCTS 算法有效提升了其性能。我们公开我们的代码、模型和数据集。</li>
</ul>

<h3>Title: Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation</h3>
<ul>
<li><strong>Authors: </strong>Ge Qu, Jinyang Li, Bowen Li, Bowen Qin, Nan Huo, Chenhao Ma, Reynold Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) driven by In-Context Learning (ICL) have significantly improved the performance of text-to-SQL. Previous methods generally employ a two-stage reasoning framework, namely 1) schema linking and 2) logical synthesis, making the framework not only effective but also interpretable. Despite these advancements, the inherent bad nature of the generalization of LLMs often results in hallucinations, which limits the full potential of LLMs. In this work, we first identify and categorize the common types of hallucinations at each stage in text-to-SQL. We then introduce a novel strategy, Task Alignment (TA), designed to mitigate hallucinations at each stage. TA encourages LLMs to take advantage of experiences from similar tasks rather than starting the tasks from scratch. This can help LLMs reduce the burden of generalization, thereby mitigating hallucinations effectively. We further propose TA-SQL, a text-to-SQL framework based on this strategy. The experimental results and comprehensive analysis demonstrate the effectiveness and robustness of our framework. Specifically, it enhances the performance of the GPT-4 baseline by 21.23% relatively on BIRD dev and it yields significant improvements across six models and four mainstream, complex text-to-SQL benchmarks.</li>
<li><strong>摘要：</strong>由上下文学习 (ICL) 驱动的大型语言模型 (LLM) 显着提高了文本到 SQL 的性能。以前的方法通常采用两阶段推理框架，即1）模式链接和2）逻辑综合，使得该框架不仅有效而且可解释。尽管取得了这些进步，法学硕士泛化固有的不良本质常常会导致幻觉，从而限制了法学硕士的全部潜力。在这项工作中，我们首先对文本到 SQL 的每个阶段的常见幻觉类型进行识别和分类。然后，我们引入一种新颖的策略，即任务调整（TA），旨在减轻每个阶段的幻觉。助教鼓励法学硕士利用类似任务的经验，而不是从头开始执行任务。这可以帮助法学硕士减轻泛化负担，从而有效减轻幻觉。我们进一步提出了 TA-SQL，一个基于此策略的文本到 SQL 框架。实验结果和综合分析证明了我们框架的有效性和鲁棒性。具体来说，它相对于 BIRD dev 将 GPT-4 基线的性能提高了 21.23%，并且在六个模型和四个主流复杂文本到 SQL 基准测试中产生了显着改进。</li>
</ul>

<h3>Title: Are Long-LLMs A Necessity For Long-Context Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia Zhou, Xu Chen, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Are Long-LLMs A Necessity For Long-Context Tasks?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>The learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we argue that the long-LLMs are not a necessity to solve long-context tasks, as common long-context tasks are short-context solvable, i.e. they can be solved by purely working with oracle short-contexts within the long-context tasks' inputs. On top of this argument, we propose a framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where LC-Boost is able to achieve a substantially improved performance with a much smaller consumption of resource.</li>
<li><strong>摘要：</strong>尽管最近取得了进展，长期法学硕士的学习和部署仍然是一个具有挑战性的问题。在这项工作中，我们认为长LLM并不是解决长上下文任务的必要条件，因为常见的长上下文任务是短上下文可解决的，即它们可以通过在长上下文中纯粹使用oracle短上下文来解决。 - 上下文任务的输入。在此基础上，我们提出了一个名为 LC-Boost（长上下文引导程序）的框架，它使短 LLM 能够以引导方式解决长上下文任务。在我们的框架中，短期法学硕士会提示自己做出两个关键决策：1）如何访问输入中上下文的适当部分，2）如何有效地利用所访问的上下文。通过根据所呈现的任务自适应地访问和利用上下文，LC-Boost 可以作为处理多样化长上下文处理问题的通用框架。我们从流行的长上下文基准测试中全面评估了不同类型的任务，其中 LC-Boost 能够以更少的资源消耗实现显着提高的性能。</li>
</ul>

<h3>Title: Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>LLMs are computationally expensive to pre-train due to their large scale. Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored. This work identifies three critical $\underline{\textit{O}}$bstacles: ($\textit{O}$1) lack of comprehensive evaluation, ($\textit{O}$2) untested viability for scaling, and ($\textit{O}$3) lack of empirical guidelines. To tackle $\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting. Our findings reveal that a depthwise stacking operator, called $G_{\text{stack}}$, exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\text{stack}}$ to address $\textit{O}$2 and $\textit{O}$3. For $\textit{O}$2 (untested scalability), our study shows that $G_{\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens. For example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\text{stack}}$ model converges to the same loss with 194B tokens, resulting in a 54.6\% speedup. We further address $\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\text{stack}}$, making it practical in general LLM pre-training. We also provide in-depth discussions and comprehensive ablation studies of $G_{\text{stack}}$. Our code and pre-trained model are available at $\href{this https URL}{this https URL}$.</li>
<li><strong>摘要：</strong>由于法学硕士规模庞大，预训练的计算成本很高。通过利用较小的模型来加速较大模型的训练，模型增长成为一种有前途的方法。然而，这些模型增长方法在高效法学硕士预训练中的可行性仍有待探索。这项工作确定了三个关键的 $\underline{\textit{O}}$bstacles：($\textit{O}$1) 缺乏全面评估，($\textit{O}$2) 未经测试的扩展可行性，以及 ($\ textit{O}$3) 缺乏经验指导。为了解决$\textit{O}$1问题，我们将现有方法总结为四个原子增长算子，并在标准化的LLM预训练环境中系统地评估它们。我们的研究结果表明，称为 $G_{\text{stack}}$ 的深度堆叠运算符在训练中表现出显着的加速效果，与强基线相比，在八个标准 NLP 基准上减少了损失并提高了整体性能。受到这些有希望的结果的激励，我们进行了广泛的实验来更深入地研究 $G_{\text{stack}}$ 以解决 $\textit{O}$2 和 $\textit{O}$3 的问题。对于 $\textit{O}$2 （未经测试的可扩展性），我们的研究表明 $G_{\text{stack}}$ 具有可扩展性并且始终表现良好，在增长和使用 750B 代币预训练 LLM 后进行了高达 7B LLM 的实验。例如，与使用 300B 令牌的传统训练的 7B 模型相比，我们的 $G_{\text{stack}}$ 模型使用 194B 令牌收敛到相同的损失，从而实现 54.6\% 的加速。我们进一步解决了$\textit{O}$3（缺乏经验指南）问题，通过形式化指南来确定$G_{\text{stack}}$的增长时间和增长因子，使其在一般LLM预训练中实用。我们还提供对$G_{\text{stack}}$的深入讨论和全面的消融研究。我们的代码和预训练模型可在 $\href{this https URL}{this https URL}$ 获取。</li>
</ul>

<h3>Title: Organic Data-Driven Approach for Turkish Grammatical Error Correction and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Asım Ersoy, Olcay Taner Yıldız</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Organic Data-Driven Approach for Turkish Grammatical Error Correction and LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Grammatical Error Correction has seen significant progress with the recent advancements in deep learning. As those methods require huge amounts of data, synthetic datasets are being built to fill this gap. Unfortunately, synthetic datasets are not organic enough in some cases and even require clean data to start with. Furthermore, most of the work that has been done is focused mostly on English. In this work, we introduce a new organic data-driven approach, clean insertions, to build parallel Turkish Grammatical Error Correction datasets from any organic data, and to clean the data used for training Large Language Models. We achieve state-of-the-art results on two Turkish Grammatical Error Correction test sets out of the three publicly available ones. We also show the effectiveness of our method on the training losses of training language models.</li>
<li><strong>摘要：</strong>随着深度学习的最新进展，语法错误纠正取得了重大进展。由于这些方法需要大量数据，因此正在构建合成数据集来填补这一空白。不幸的是，在某些情况下，合成数据集不够有机，甚至需要干净的数据来开始。此外，已经完成的大部分工作主要集中在英语方面。在这项工作中，我们引入了一种新的有机数据驱动方法，即干净插入，从任何有机数据构建并行土耳其语语法纠错数据集，并清理用于训练大型语言模型的数据。我们在三个公开可用的测试集中的两个土耳其语语法错误纠正测试集上取得了最先进的结果。我们还展示了我们的方法在训练语言模型的训练损失方面的有效性。</li>
</ul>

<h3>Title: Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Minzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy F. Chen, Min-Yen Kan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The acceleration of Large Language Models (LLMs) research has opened up new possibilities for evaluating generated texts. They serve as scalable and economical evaluators, but the question of how reliable these evaluators are has emerged as a crucial research question. Prior research efforts in the meta-evaluation of LLMs as judges limit the prompting of an LLM to a single use to obtain a final evaluation decision. They then compute the agreement between LLMs' outputs and human labels. This lacks interpretability in understanding the evaluation capability of LLMs. In light of this challenge, we propose Decompose and Aggregate, which breaks down the evaluation process into different stages based on pedagogical practices. Our experiments illustrate that it not only provides a more interpretable window for how well LLMs evaluate, but also leads to improvements up to 39.6% for different LLMs on a variety of meta-evaluation benchmarks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 研究的加速为评估生成的文本开辟了新的可能性。它们是可扩展且经济的评估器，但这些评估器的可靠性如何已成为一个关键的研究问题。先前对法学硕士作为法官进行元评估的研究工作将法学硕士的提示限制为单次使用以获得最终评估决定。然后，他们计算法学硕士的输出与人类标签之间的一致性。这对于理解法学硕士的评估能力缺乏可解释性。鉴于这一挑战，我们提出了分解和聚合，根据教学实践将评估过程分解为不同的阶段。我们的实验表明，它不仅为法学硕士的评估提供了一个更容易解释的窗口，而且还使不同法学硕士在各种元评估基准上的改进高达 39.6%。</li>
</ul>

<h3>Title: Detection and Positive Reconstruction of Cognitive Distortion sentences: Mandarin Dataset and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Shuya Lin, Yuxiong Wang, Jonathan Dong, Shiguang Ni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Detection and Positive Reconstruction of Cognitive Distortion sentences: Mandarin Dataset and Evaluation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>This research introduces a Positive Reconstruction Framework based on positive psychology theory. Overcoming negative thoughts can be challenging, our objective is to address and reframe them through a positive reinterpretation. To tackle this challenge, a two-fold approach is necessary: identifying cognitive distortions and suggesting a positively reframed alternative while preserving the original thought's meaning. Recent studies have investigated the application of Natural Language Processing (NLP) models in English for each stage of this process. In this study, we emphasize the theoretical foundation for the Positive Reconstruction Framework, grounded in broaden-and-build theory. We provide a shared corpus containing 4001 instances for detecting cognitive distortions and 1900 instances for positive reconstruction in Mandarin. Leveraging recent NLP techniques, including transfer learning, fine-tuning pretrained networks, and prompt engineering, we demonstrate the effectiveness of automated tools for both tasks. In summary, our study contributes to multilingual positive reconstruction, highlighting the effectiveness of NLP in cognitive distortion detection and positive reconstruction.</li>
<li><strong>摘要：</strong>本研究介绍了基于积极心理学理论的积极重建框架。克服消极想法可能具有挑战性，我们的目标是通过积极的重新解释来解决和重新构建它们。为了应对这一挑战，需要采取双管齐下的方法：识别认知扭曲并提出积极重构的替代方案，同时保留原始思想的含义。最近的研究调查了自然语言处理（NLP）模型在英语中在此过程的每个阶段的应用。在这项研究中，我们强调积极重建框架的理论基础，以拓宽和构建理论为基础。我们提供了一个共享语料库，其中包含 4001 个用于检测认知扭曲的实例和 1900 个用于普通话正向重建的实例。利用最新的 NLP 技术，包括迁移学习、微调预训练网络和即时工程，我们展示了自动化工具对这两项任务的有效性。总之，我们的研究有助于多语言正向重建，凸显了 NLP 在认知扭曲检测和正向重建方面的有效性。</li>
</ul>

<h3>Title: BiSup: Bidirectional Quantization Error Suppression for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minghui Zou, Ronghui Guo, Sai Zhang, Xiaowang Zhang, Zhiyong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] BiSup: Bidirectional Quantization Error Suppression for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As the size and context length of Large Language Models (LLMs) grow, weight-activation quantization has emerged as a crucial technique for efficient deployment of LLMs. Compared to weight-only quantization, weight-activation quantization presents greater challenges due to the presence of outliers in activations. Existing methods have made significant progress by exploring mixed-precision quantization and outlier suppression. However, these methods primarily focus on optimizing the results of single matrix multiplication, neglecting the bidirectional propagation of quantization errors in LLMs. Specifically, errors accumulate vertically within the same token through layers, and diffuse horizontally across different tokens due to self-attention mechanisms. To address this issue, we introduce BiSup, a Bidirectional quantization error Suppression method. By constructing appropriate optimizable parameter spaces, BiSup utilizes a small amount of data for quantization-aware parameter-efficient fine-tuning to suppress the error vertical accumulation. Besides, BiSup employs prompt mixed-precision quantization strategy, which preserves high precision for the key-value cache of system prompts, to mitigate the error horizontal diffusion. Extensive experiments on Llama and Qwen families demonstrate that BiSup can improve performance over two state-of-the-art methods (the average WikiText2 perplexity decreases from 13.26 to 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128 configuration), further facilitating the practical applications of low-bit weight-activation quantization.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的大小和上下文长度的增长，权重激活量化已成为高效部署 LLM 的关键技术。与仅权重量化相比，由于激活中存在异常值，权重激活量化提出了更大的挑战。现有方法通过探索混合精度量化和异常值抑制取得了重大进展。然而，这些方法主要侧重于优化单矩阵乘法的结果，忽略了LLM中量化误差的双向传播。具体来说，错误在同一令牌内通过层垂直累积，并由于自注意力机制而在不同令牌之间水平扩散。为了解决这个问题，我们引入了 BiSup，一种双向量化误差抑制方法。通过构建适当的可优化参数空间，BiSup 利用少量数据进行量化感知参数高效微调，以抑制误差垂直累积。此外，BiSup采用提示混合精度量化策略，为系统提示的键值缓存保留高精度，以减轻误差水平扩散。对 Llama 和 Qwen 系列的大量实验表明，BiSup 可以比两种最先进的方法提高性能（在 W3A3-g128 配置下，Atom 的平均 WikiText2 困惑度从 13.26 降低到 9.41，QuaRot 的平均困惑度从 14.33 降低到 7.85），进一步促进低位权重激活量化的实际应用。</li>
</ul>

<h3>Title: UnKE: Unstructured Knowledge Editing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] UnKE: Unstructured Knowledge Editing in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent knowledge editing methods have primarily focused on modifying structured knowledge in large language models, heavily relying on the assumption that structured knowledge is stored as key-value pairs locally in MLP layers or specific neurons. However, this task setting overlooks the fact that a significant portion of real-world knowledge is stored in an unstructured format, characterized by long-form content, noise, and a complex yet comprehensive nature. The "knowledge locating" and "term-driven optimization" techniques conducted from the assumption used in previous methods (e.g., MEMIT) are ill-suited for unstructured knowledge. To address these challenges, we propose a novel unstructured knowledge editing method, namely UnKE, which extends previous assumptions in the layer dimension and token dimension. Firstly, in the layer dimension, we discard the "knowledge locating" step and treat first few layers as the key, which expand knowledge storage through layers to break the "knowledge stored locally" assumption. Next, we replace "term-driven optimization" with "cause-driven optimization" across all inputted tokens in the token dimension, directly optimizing the last layer of the key generator to perform editing to generate the required key vectors. By utilizing key-value pairs at the layer level, UnKE effectively represents and edits complex and comprehensive unstructured knowledge, leveraging the potential of both the MLP and attention layers. Results on newly proposed unstructure knowledge editing dataset (UnKEBench) and traditional structured datasets demonstrate that UnKE achieves remarkable performance, surpassing strong baselines.</li>
<li><strong>摘要：</strong>最近的知识编辑方法主要侧重于修改大型语言模型中的结构化知识，严重依赖于结构化知识以键值对形式本地存储在 MLP 层或特定神经元中的假设。然而，这种任务设置忽略了现实世界知识的很大一部分以非结构化格式存储的事实，其特点是内容长、噪声大、复杂而全面。以前方法（例如 MEMIT）中使用的假设进行的“知识定位”和“术语驱动优化”技术不适合非结构化知识。为了应对这些挑战，我们提出了一种新颖的非结构化知识编辑方法，即 UnKE，它在层维度和 token 维度上扩展了以前的假设。首先，在层维度上，我们丢弃了“知识定位”步骤，将前几层视为关键，通过层扩展知识存储，打破了“知识本地存储”的假设。接下来，我们在 token 维度上用“原因驱动优化”取代所有输入 token 的“词驱动优化”，直接优化关键生成器的最后一层进行编辑以生成所需的关键向量。通过在层级利用键值对，UnKE 有效地表示和编辑复杂而全面的非结构化知识，充分利用 MLP 和注意力层的潜力。在新提出的非结构化知识编辑数据集 (UnKEBench) 和传统结构化数据集上的结果表明，UnKE 取得了卓越的性能，超越了强大的基线。</li>
</ul>

<h3>Title: Large Language Models can Deliver Accurate and Interpretable Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jun Liu, Chaoyun Zhang, Jiaxu Qian, Minghua Ma, Si Qin, Chetan Bansal, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Large Language Models can Deliver Accurate and Interpretable Time Series Anomaly Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) plays a crucial role in various industries by identifying atypical patterns that deviate from standard trends, thereby maintaining system integrity and enabling prompt response measures. Traditional TSAD models, which often rely on deep learning, require extensive training data and operate as black boxes, lacking interpretability for detected anomalies. To address these challenges, we propose LLMAD, a novel TSAD method that employs Large Language Models (LLMs) to deliver accurate and interpretable TSAD results. LLMAD innovatively applies LLMs for in-context anomaly detection by retrieving both positive and negative similar time series segments, significantly enhancing LLMs' effectiveness. Furthermore, LLMAD employs the Anomaly Detection Chain-of-Thought (AnoCoT) approach to mimic expert logic for its decision-making process. This method further enhances its performance and enables LLMAD to provide explanations for their detections through versatile perspectives, which are particularly important for user decision-making. Experiments on three datasets indicate that our LLMAD achieves detection performance comparable to state-of-the-art deep learning methods while offering remarkable interpretability for detections. To the best of our knowledge, this is the first work that directly employs LLMs for TSAD.</li>
<li><strong>摘要：</strong>时间序列异常检测 (TSAD) 通过识别偏离标准趋势的非典型模式，从而维护系统完整性并实现及时响应措施，在各个行业中发挥着至关重要的作用。传统的 TSAD 模型通常依赖于深度学习，需要大量的训练数据，并且像黑匣子一样运行，缺乏对检测到的异常的可解释性。为了应对这些挑战，我们提出了 LLMAD，这是一种新颖的 TSAD 方法，它采用大型语言模型 (LLM) 来提供准确且可解释的 TSAD 结果。 LLMAD 通过检索正面和负面的相似时间序列片段，创新地将法学硕士应用于上下文异常检测，显着提高了法学硕士的有效性。此外，LLMAD 采用异常检测思想链 (AnoCoT) 方法来模仿专家逻辑的决策过程。该方法进一步增强了其性能，并使 LLMAD 能够通过多种视角为其检测提供解释，这对于用户决策尤为重要。对三个数据集的实验表明，我们的 LLMAD 实现了与最先进的深度学习方法相当的检测性能，同时为检测提供了卓越的可解释性。据我们所知，这是第一个直接聘请法学硕士从事 TSAD 的工作。</li>
</ul>

<h3>Title: Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top</h3>
<ul>
<li><strong>Authors: </strong>Keyuan Cheng, Muhammad Asif Ali, Shu Yang, Gang Ling, Yuxuan Zhai, Haoyang Fei, Ke Xu, Lu Yu, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multi-hop Question Answering (MQA) under knowledge editing (KE) is a key challenge in Large Language Models (LLMs). While best-performing solutions in this domain use a plan and solve paradigm to split a question into sub-questions followed by response generation, we claim that this approach is sub-optimal as it fails for hard to decompose questions, and it does not explicitly cater to correlated knowledge updates resulting as a consequence of knowledge edits. This has a detrimental impact on the overall consistency of the updated knowledge. To address these issues, in this paper, we propose a novel framework named RULE-KE, i.e., RULE based Knowledge Editing, which is a cherry on the top for augmenting the performance of all existing MQA methods under KE. Specifically, RULE-KE leverages rule discovery to discover a set of logical rules. Then, it uses these discovered rules to update knowledge about facts highly correlated with the edit. Experimental evaluation using existing and newly curated datasets (i.e., RKE-EVAL) shows that RULE-KE helps augment both performances of parameter-based and memory-based solutions up to 92% and 112.9%, respectively.</li>
<li><strong>摘要：</strong>知识编辑（KE）下的多跳问答（MQA）是大型语言模型（LLM）中的一个关键挑战。虽然该领域中表现最佳的解决方案使用计划和解决范式将问题拆分为子问题，然后生成响应，但我们声称这种方法不是最佳的，因为它难以分解问题，并且它没有明确迎合因知识编辑而产生的相关知识更新。这对更新知识的整体一致性产生不利影响。为了解决这些问题，在本文中，我们提出了一种名为 RULE-KE 的新颖框架，即基于 RULE 的知识编辑，它是增强 KE 下所有现有 MQA 方法性能的锦上添花。具体来说，RULE-KE 利用规则发现来发现一组逻辑规则。然后，它使用这些发现的规则来更新与编辑高度相关的事实的知识。使用现有和新整理的数据集（即 RKE-EVAL）进行的实验评估表明，RULE-KE 有助于将基于参数和基于内存的解决方案的性能分别提高高达 92% 和 112.9%。</li>
</ul>

<h3>Title: Benchmarking Pre-trained Large Language Models' Potential Across Urdu NLP tasks</h3>
<ul>
<li><strong>Authors: </strong>Munief Hassan Tahir, Sana Shams, Layba Fiaz, Farah Adeeba, Sarmad Hussain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Benchmarking Pre-trained Large Language Models' Potential Across Urdu NLP tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) pre-trained on multilingual data have revolutionized natural language processing research, by transitioning from languages and task specific model pipelines to a single model adapted on a variety of tasks. However majority of existing multilingual NLP benchmarks for LLMs provide evaluation data in only few languages with little linguistic diversity. In addition these benchmarks lack quality assessment against the respective state-of the art models. This study presents an in-depth examination of prominent LLMs; GPT-3.5-turbo, Llama2-7B-Chat, Bloomz 7B1 and Bloomz 3B, across 14 tasks using 15 Urdu datasets, in a zero-shot setting, and their performance against state-of-the-art (SOTA) models, has been compared and analysed. Our experiments show that SOTA models surpass all the encoder-decoder pre-trained language models in all Urdu NLP tasks with zero-shot learning. Our results further show that LLMs with fewer parameters, but more language specific data in the base model perform better than larger computational models, but low language data.</li>
<li><strong>摘要：</strong>在多语言数据上预训练的大型语言模型 (LLM) 彻底改变了自然语言处理研究，从语言和任务特定的模型管道转变为适应各种任务的单一模型。然而，大多数现有的法学硕士多语言 NLP 基准仅提供几种语言的评估数据，语言多样性很少。此外，这些基准缺乏针对各自最先进模型的质量评估。这项研究对著名的法学硕士进行了深入研究； GPT-3.5-turbo、Llama2-7B-Chat、Bloomz 7B1 和 Bloomz 3B 在零样本设置下使用 15 个乌尔都语数据集完成 14 个任务，并且它们相对于最先进 (SOTA) 模型的性能进行了比较和分析。我们的实验表明，SOTA 模型在所有乌尔都语 NLP 任务中通过零样本学习超越了所有编码器-解码器预训练语言模型。我们的结果进一步表明，参数较少但基础模型中语言特定数据较多的法学硕士比较大的计算模型但语言数据较少的表现更好。</li>
</ul>

<h3>Title: Linearly Controlled Language Generation with Performative Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Emily Cheng, Marco Baroni, Carmen Amo Alonso</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Linearly Controlled Language Generation with Performative Guarantees(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The increasing prevalence of Large Language Models (LMs) in critical applications highlights the need for controlled language generation strategies that are not only computationally efficient but that also enjoy performance guarantees. To achieve this, we use a common model of concept semantics as linearly represented in an LM's latent space. In particular, we take the view that natural language generation traces a trajectory in this continuous semantic space, realized by the language model's hidden activations. This view permits a control-theoretic treatment of text generation in latent space, in which we propose a lightweight, gradient-free intervention that dynamically steers trajectories away from regions corresponding to undesired meanings. Crucially, we show that this intervention, which we compute in closed form, is guaranteed (in probability) to steer the output into the allowed region. Finally, we demonstrate on a toxicity avoidance objective that the intervention steers language away from undesired content while maintaining text quality.</li>
<li><strong>摘要：</strong>大型语言模型 (LM) 在关键应用中的日益普及凸显了对受控语言生成策略的需求，这些策略不仅计算效率高，而且还享有性能保证。为了实现这一目标，我们使用概念语义的通用模型，在 LM 的潜在空间中线性表示。特别是，我们认为自然语言生成在这个连续的语义空间中追踪轨迹，这是通过语言模型的隐藏激活来实现的。这种观点允许对潜在空间中的文本生成进行控制理论处理，其中我们提出了一种轻量级、无梯度的干预，可以动态地将轨迹引导远离与不需要的含义相对应的区域。至关重要的是，我们表明，这种以封闭形式计算的干预措施保证（以概率）将输出引导到允许的区域。最后，我们证明了避免毒性的目标，即干预措施可以引导语言远离不需要的内容，同时保持文本质量。</li>
</ul>

<h3>Title: Emergence of a High-Dimensional Abstraction Phase in Language Transformers</h3>
<ul>
<li><strong>Authors: </strong>Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Jade Yu, Alessandro Laio, Marco Baroni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Emergence of a High-Dimensional Abstraction Phase in Language Transformers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.</li>
<li><strong>摘要：</strong>语言模型 (LM) 是从语言上下文到输出标记的映射。然而，关于这种映射还有很多事情有待了解，包括其几何特性与其功能的关系。我们采用高级几何方法来分析和观察五个基于变压器的预训练 LM 和三个输入数据集，这是一个以高内在维度为特征的独特阶段。在此阶段，表示 (1) 对应于输入的第一个完整语言抽象； （2）率先可行地转移到下游任务； (3) 在不同的 LM 之间相互预测。此外，我们发现该阶段的较早开始强烈预示着更好的语言建模性能。简而言之，我们的结果表明，中心高维阶段是许多常见 LM 架构中核心语言处理的基础。</li>
</ul>

<h3>Title: Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The immense datasets used to develop Large Language Models (LLMs) often include copyright-protected content, typically without the content creator's consent. Copyright traps have been proposed to be injected into the original content, improving content detectability in newly released LLMs. Traps, however, rely on the exact duplication of a unique text sequence, leaving them vulnerable to commonly deployed data deduplication techniques. We here propose the generation of fuzzy copyright traps, featuring slight modifications across duplication. When injected in the fine-tuning data of a 1.3B LLM, we show fuzzy trap sequences to be memorized nearly as well as exact duplicates. Specifically, the Membership Inference Attack (MIA) ROC AUC only drops from 0.90 to 0.87 when 4 tokens are replaced across the fuzzy duplicates. We also find that selecting replacement positions to minimize the exact overlap between fuzzy duplicates leads to similar memorization, while making fuzzy duplicates highly unlikely to be removed by any deduplication process. Lastly, we argue that the fact that LLMs memorize across fuzzy duplicates challenges the study of LLM memorization relying on naturally occurring duplicates. Indeed, we find that the commonly used training dataset, The Pile, contains significant amounts of fuzzy duplicates. This introduces a previously unexplored confounding factor in post-hoc studies of LLM memorization, and questions the effectiveness of (exact) data deduplication as a privacy protection technique.</li>
<li><strong>摘要：</strong>用于开发大型语言模型 (LLM) 的庞大数据集通常包含受版权保护的内容，通常未经内容创建者同意。建议将版权陷阱注入到原始内容中，以提高新发布的法学硕士内容的可检测性。然而，陷阱依赖于唯一文本序列的精确重复，这使得它们容易受到普遍部署的重复数据删除技术的影响。我们在这里提出了模糊版权陷阱的生成，其特点是对重复进行轻微修改。当注入 1.3B LLM 的微调数据时，我们显示模糊陷阱序列几乎可以和精确重复一样被记忆。具体来说，当在模糊重复中替换 4 个令牌时，成员推理攻击 (MIA) ROC AUC 仅从 0.90 下降到 0.87。我们还发现，选择替换位置以最小化模糊重复项之间的精确重叠会导致类似的记忆，同时使模糊重复项极不可能被任何重复数据删除过程删除。最后，我们认为，法学硕士跨越模糊重复进行记忆的事实挑战了依赖于自然发生的重复的法学硕士记忆研究。事实上，我们发现常用的训练数据集 The Pile 包含大量模糊重复项。这在法学硕士记忆的事后研究中引入了先前未探索的混杂因素，并对（精确）重复数据删除作为隐私保护技术的有效性提出了质疑。</li>
</ul>

<h3>Title: Sparse Matrix in Large Language Model Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Haoze He, Juncheng Billy Li, Xuan Jiang, Heather Miller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Sparse Matrix in Large Language Model Fine-tuning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>LoRA and its variants have become popular parameter-efficient fine-tuning (PEFT) methods due to their ability to avoid excessive computational costs. However, an accuracy gap often exists between PEFT methods and full fine-tuning (FT), and this gap has yet to be systematically studied. In this work, we introduce a method for selecting sparse sub-matrices that aim to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method begins by identifying the most significant sub-matrices in the gradient update, updating only these blocks during the fine-tuning process. In our experiments, we demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issue.</li>
<li><strong>摘要：</strong>LoRA 及其变体由于能够避免过多的计算成本而成为流行的参数高效微调（PEFT）方法。然而，PEFT 方法和完全微调（FT）之间通常存在精度差距，并且这种差距尚未得到系统研究。在这项工作中，我们引入了一种选择稀疏子矩阵的方法，旨在最小化 PEFT 与完全微调 (FT) 之间的性能差距，同时减少微调计算成本和内存成本。我们的稀疏矩阵调整（SMT）方法首先识别梯度更新中最重要的子矩阵，在微调过程中仅更新这些块。在我们的实验中，我们证明 SMT 在跨广泛任务范围内微调流行的大型语言模型（例如 LLaMA）方面始终超越其他 PEFT 基线（例如 LoRA 和 DoRA），同时与 FT 相比将 GPU 内存占用减少了 67%。我们还研究了随着可训练参数数量的增加，LoRA 和 DoRA 的性能如何趋于稳定和下降，相比之下，我们的 SMT 方法不会遇到这样的问题。</li>
</ul>

<h3>Title: Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems</h3>
<ul>
<li><strong>Authors: </strong>Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu, Mausam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) based end-to-end task-oriented dialog (TOD) systems built using few-shot (in-context) learning perform better than supervised models only when the train data is limited. This is due to the inherent ability of LLMs to learn any task with just a few demonstrations. As the number of train dialogs increases, supervised SoTA models surpass in-context learning LLMs as they learn to better align with the style of the system responses in the training data, which LLMs struggle to mimic. In response, we propose SyncTOD, which synergizes LLMs with useful hints about the task for improved alignment. At a high level, SyncTOD trains auxiliary models to provide these hints and select exemplars for the in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings</li>
<li><strong>摘要：</strong>仅当训练数据有限时，使用少样本（上下文中）学习构建的基于大型语言模型 (LLM) 的端到端面向任务的对话 (TOD) 系统才比监督模型表现更好。这是由于法学硕士具有只需几次演示即可学习任何任务的固有能力。随着训练对话数量的增加，有监督的 SoTA 模型超越了上下文学习法学硕士，因为它们学会了更好地与训练数据中的系统响应风格保持一致，而法学硕士很难模仿。作为回应，我们提出了 SyncTOD，它将 LLM 与有关改进对齐的任务的有用提示相结合。在较高层面上，SyncTOD 训练辅助模型来提供这些提示并为上下文提示选择示例。借助 ChatGPT，SyncTOD 在低数据设置中比基于 LLM 的基线和 SoTA 模型实现了卓越的性能，同时在全数据设置中保持了具有竞争力的性能</li>
</ul>

<h3>Title: Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Jonas Becker, Jan Philip Wahle, Bela Gipp, Terry Ruas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Text generation has become more accessible than ever, and the increasing interest in these systems, especially those using large language models, has spurred an increasing number of related publications. We provide a systematic literature review comprising 244 selected papers between 2017 and 2024. This review categorizes works in text generation into five main tasks: open-ended text generation, summarization, translation, paraphrasing, and question answering. For each task, we review their relevant characteristics, sub-tasks, and specific challenges (e.g., missing datasets for multi-document summarization, coherence in story generation, and complex reasoning for question answering). Additionally, we assess current approaches for evaluating text generation systems and ascertain problems with current metrics. Our investigation shows nine prominent challenges common to all tasks and sub-tasks in recent text generation publications: bias, reasoning, hallucinations, misuse, privacy, interpretability, transparency, datasets, and computing. We provide a detailed analysis of these challenges, their potential solutions, and which gaps still require further engagement from the community. This systematic literature review targets two main audiences: early career researchers in natural language processing looking for an overview of the field and promising research directions, as well as experienced researchers seeking a detailed view of tasks, evaluation methodologies, open challenges, and recent mitigation strategies.</li>
<li><strong>摘要：</strong>文本生成变得比以往任何时候都更容易获得，人们对这些系统（尤其是使用大型语言模型的系统）的兴趣日益增加，这刺激了相关出版物的数量不断增加。我们提供了一份系统的文献综述，其中包括 2017 年至 2024 年期间的 244 篇精选论文。本综述将文本生成工作分为五个主要任务：开放式文本生成、摘要、翻译、释义和问答。对于每项任务，我们回顾了它们的相关特征、子任务和特定挑战（例如，多文档摘要的缺失数据集、故事生成的连贯性和问答的复杂推理）。此外，我们评估了当前评估文本生成系统的方法，并确定了当前指标的问题。我们的调查显示，最近的文本生成出版物中所有任务和子任务都面临九个突出的挑战：偏见、推理、幻觉、误用、隐私、可解释性、透明度、数据集和计算。我们对这些挑战、它们的潜在解决方案以及哪些差距仍需要社区的进一步参与进行了详细分析。本系统的文献综述主要针对两类受众：寻求该领域概况和有前景的研究方向的自然语言处理早期研究人员，以及寻求详细了解任务、评估方法、未解决的挑战和最近的缓解策略的经验丰富的研究人员。</li>
</ul>

<h3>Title: GECKO: Generative Language Model for English, Code and Korean</h3>
<ul>
<li><strong>Authors: </strong>Sungwoo Oh, Donggyu Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] GECKO: Generative Language Model for English, Code and Korean(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture. In this report, we share the experiences of several efforts to build a better data pipeline for the corpus and to train our model. GECKO shows great efficiency in token generations for both Korean and English, despite its small size of vocabulary. We measure the performance on the representative benchmarks in terms of Korean, English and Code, and it exhibits great performance on KMMLU (Korean MMLU) and modest performance in English and Code, even with its smaller number of trained tokens compared to English-focused LLMs. GECKO is available to the open-source community under a permissive license. We hope our work offers a research baseline and practical insights for Korean LLM research. The model can be found at: this https URL</li>
<li><strong>摘要：</strong>我们引入了 GECKO，这是一种针对韩语和英语以及编程语言进行了优化的双语大型语言模型 (LLM)。GECKO 在采用 LLaMA 架构的平衡、高质量的韩语和英语语料库上进行了预训练。在本报告中，我们分享了为语料库构建更好的数据管道和训练模型的几项努力的经验。尽管词汇量很小，但 GECKO 在韩语和英语的标记生成方面都表现出色。我们在韩语、英语和代码方面的代表性基准上衡量了性能，它在 KMMLU（韩语 MMLU）上表现出色，在英语和代码上表现出色，即使与以英语为重点的 LLM 相比，它的训练标记数量较少。GECKO 在宽松的许可下可供开源社区使用。我们希望我们的工作为韩国 LLM 研究提供研究基础和实用见解。该模型可以在以下位置找到：此 https URL</li>
</ul>

<h3>Title: EmpathicStories++: A Multimodal Dataset for Empathy towards Personal Experiences</h3>
<ul>
<li><strong>Authors: </strong>Jocelyn Shen, Yubin Kim, Mohit Hulse, Wazeer Zulfikar, Sharifa Alghowinem, Cynthia Breazeal, Hae Won Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] EmpathicStories++: A Multimodal Dataset for Empathy towards Personal Experiences(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Modeling empathy is a complex endeavor that is rooted in interpersonal and experiential dimensions of human interaction, and remains an open problem within AI. Existing empathy datasets fall short in capturing the richness of empathy responses, often being confined to in-lab or acted scenarios, lacking longitudinal data, and missing self-reported labels. We introduce a new multimodal dataset for empathy during personal experience sharing: the EmpathicStories++ dataset (this https URL) containing 53 hours of video, audio, and text data of 41 participants sharing vulnerable experiences and reading empathically resonant stories with an AI agent. EmpathicStories++ is the first longitudinal dataset on empathy, collected over a month-long deployment of social robots in participants' homes, as participants engage in natural, empathic storytelling interactions with AI agents. We then introduce a novel task of predicting individuals' empathy toward others' stories based on their personal experiences, evaluated in two contexts: participants' own personal shared story context and their reflections on stories they read. We benchmark this task using state-of-the-art models to pave the way for future improvements in contextualized and longitudinal empathy modeling. Our work provides a valuable resource for further research in developing empathetic AI systems and understanding the intricacies of human empathy within genuine, real-world settings.</li>
<li><strong>摘要：</strong>建立同理心模型是一项复杂的工作，植根于人类互动的人际和体验维度，并且仍然是人工智能中的一个悬而未决的问题。现有的同理心数据集无法捕捉丰富的同理心反应，通常仅限于实验室或表演场景，缺乏纵向数据，并且缺少自我报告的标签。我们引入了一个新的多模态数据集，用于在个人经验分享过程中实现同理心：EmpathicStories++ 数据集（此 https URL）包含 41 名参与者的 53 小时视频、音频和文本数据，这些参与者分享了脆弱的经历，并与 AI 代理一起阅读了产生同理心共鸣的故事。 EmpathicStories++ 是第一个关于同理心的纵向数据集，是在参与者与人工智能代理进行自然、同理心的讲故事互动时，在参与者家中部署社交机器人长达一个月的时间里收集的。然后，我们引入了一项新颖的任务，即根据个人经历预测个人对其他人故事的同理心，并在两种背景下进行评估：参与者自己的个人共享故事背景以及他们对所读故事的反思。我们使用最先进的模型对这项任务进行基准测试，为未来情境化和纵向同理心建模的改进铺平道路。我们的工作为进一步研究开发同理心人工智能系统和理解真实世界环境中人类同理心的复杂性提供了宝贵的资源。</li>
</ul>

<h3>Title: Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Patil, Jaap Jumelet, Yu Ying Chiu, Andy Lapastora, Peter Shen, Lexie Wang, Clevis Willrich, Shane Steinert-Threlkeld</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.</li>
<li><strong>摘要：</strong>本文介绍了过滤语料库训练（Filtered Corpus Training），这是一种在从训练数据中过滤出某些语言结构的语料库上训练语言模型（LM）的方法，并用它来衡量 LM 在间接证据的基础上进行语言泛化的能力。我们将该方法应用于 LSTM 和 Transformer LM（大小大致相当），开发针对各种语言现象的过滤语料库。我们的结果表明，虽然 Transformer 是更好的 LM（通过困惑度来衡量），但这两个模型在语言泛化测量上表现相同且令人惊讶的好，这表明它们能够从间接证据中进行泛化。</li>
</ul>

<h3>Title: GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction</h3>
<ul>
<li><strong>Authors: </strong>Virginia K. Felkner, Jennifer A. Thompson, Jonathan May</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.</li>
<li><strong>摘要：</strong>法学硕士的社会偏见通常通过偏见基准数据集来衡量。当前的基准在范围、基础、质量和所需的人力方面存在局限性。以前的工作已经表明，采用社区来源而非众包的基准开发方法取得了成功。然而，这项工作仍然需要具有相关生活经验的注释者付出巨大的努力。本文探讨了法学硕士（具体来说，GPT-3.5-Turbo）是否可以协助完成根据开放式社区调查的回复开发偏差基准数据集的任务。我们还将之前的工作扩展到一个新的社区和一系列偏见：犹太社区和反犹太主义。我们的分析表明，GPT-3.5-Turbo 在此注释任务上表现不佳，并在其输出中产生不可接受的质量问题。因此，我们得出的结论是，在与社会偏见相关的敏感任务中，GPT-3.5-Turbo 并不是人类注释的适当替代品，而且它的使用实际上抵消了社区采购偏见基准的许多好处。</li>
</ul>

<h3>Title: Scaling Laws for Discriminative Classification in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dean Wyatte, Fatemeh Tahmasbi, Ming Li, Thomas Markovich</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Scaling Laws for Discriminative Classification in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) represent a paradigm shift in what can plausibly be expected of machine learning models. The fact that LLMs can effectively generate sensible answers to a diverse range of queries suggests that they would be useful in customer support applications. While powerful, LLMs have been observed to be prone to hallucination which unfortunately makes their near term use in customer support applications challenging. To address this issue we present a system that allows us to use an LLM to augment our customer support advocates by re-framing the language modeling task as a discriminative classification task. In this framing, we seek to present the top-K best template responses for a customer support advocate to use when responding to a customer. We present the result of both offline and online experiments where we observed offline gains and statistically significant online lifts for our experimental system. Along the way, we present observed scaling curves for validation loss and top-K accuracy, resulted from model parameter ablation studies. We close by discussing the space of trade-offs with respect to model size, latency, and accuracy as well as and suggesting future applications to explore.</li>
<li><strong>摘要：</strong>现代大语言模型 (LLM) 代表了机器学习模型的范式转变。事实上，法学硕士可以有效地为各种查询生成合理的答案，这表明它们在客户支持应用程序中非常有用。虽然法学硕士很强大，但人们发现它们很容易产生幻觉，不幸的是，这使得它们在客户支持应用程序中的短期使用具有挑战性。为了解决这个问题，我们提出了一个系统，允许我们使用法学硕士通过将语言建模任务重新定义为判别性分类任务来增强我们的客户支持倡导者。在此框架中，我们寻求提供前 K 个最佳模板响应，供客户支持倡导者在响应客户时使用。我们展示了离线和在线实验的结果，其中我们观察到我们的实验系统的离线收益和统计上显着的在线提升。在此过程中，我们展示了模型参数消融研究中观察到的验证损失和 top-K 准确度的缩放曲线。最后，我们讨论了模型大小、延迟和准确性方面的权衡空间，并建议了未来要探索的应用程序。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
