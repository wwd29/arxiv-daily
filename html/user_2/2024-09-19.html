<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-19</h1>
<h3>Title: Optimizing Performance: How Compact Models Match or Exceed GPT's Classification Capabilities through Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Baptiste Lefort, Eric Benhamou, Jean-Jacques Ohana, David Saltiel, Beatrice Guez</a></li>
<li><strong>Subjects: </strong>cs.CL, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11408">https://arxiv.org/abs/2409.11408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11408">https://arxiv.org/pdf/2409.11408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11408]] Optimizing Performance: How Compact Models Match or Exceed GPT's Classification Capabilities through Fine-Tuning(https://arxiv.org/abs/2409.11408)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>In this paper, we demonstrate that non-generative, small-sized models such as FinBERT and FinDRoBERTa, when fine-tuned, can outperform GPT-3.5 and GPT-4 models in zero-shot learning settings in sentiment analysis for financial news. These fine-tuned models show comparable results to GPT-3.5 when it is fine-tuned on the task of determining market sentiment from daily financial news summaries sourced from Bloomberg. To fine-tune and compare these models, we created a novel database, which assigns a market score to each piece of news without human interpretation bias, systematically identifying the mentioned companies and analyzing whether their stocks have gone up, down, or remained neutral. Furthermore, the paper shows that the assumptions of Condorcet's Jury Theorem do not hold suggesting that fine-tuned small models are not independent of the fine-tuned GPT models, indicating behavioural similarities. Lastly, the resulted fine-tuned models are made publicly available on HuggingFace, providing a resource for further research in financial sentiment analysis and text classification.</li>
<li><strong>摘要：</strong>在本文中，我们证明，在金融新闻情绪分析中，经过微调的非生成式小型模型（例如 FinBERT 和 FinDRoBERTa）在零样本学习环境中的表现可以优于 GPT-3.5 和 GPT-4 模型。在根据彭博社提供的每日金融新闻摘要确定市场情绪的任务上，这些经过微调的模型显示出与 GPT-3.5 相当的结果。为了微调和比较这些模型，我们创建了一个新颖的数据库，该数据库为每条新闻分配一个市场分数（没有人工解释偏差），系统地识别所提及的公司并分析它们的股价是上涨、下跌还是保持中性。此外，本文表明孔多塞陪审团定理的假设不成立，这表明经过微调的小型模型并不独立于经过微调的 GPT 模型，表明行为相似。最后，经过微调的模型在 HuggingFace 上公开发布，为进一步研究金融情绪分析和文本分类提供资源。</li>
</ul>

<h3>Title: Enriching Datasets with Demographics through Large Language Models: What's in a Name?</h3>
<ul>
<li><strong>Authors: </strong>Khaled AlNuaimi, Gautier Marti, Mathieu Ravaut, Abdulla AlKetbi, Andreas Henschel, Raed Jaradat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11491">https://arxiv.org/abs/2409.11491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11491">https://arxiv.org/pdf/2409.11491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11491]] Enriching Datasets with Demographics through Large Language Models: What's in a Name?(https://arxiv.org/abs/2409.11491)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Enriching datasets with demographic information, such as gender, race, and age from names, is a critical task in fields like healthcare, public policy, and social sciences. Such demographic insights allow for more precise and effective engagement with target populations. Despite previous efforts employing hidden Markov models and recurrent neural networks to predict demographics from names, significant limitations persist: the lack of large-scale, well-curated, unbiased, publicly available datasets, and the lack of an approach robust across datasets. This scarcity has hindered the development of traditional supervised learning approaches. In this paper, we demonstrate that the zero-shot capabilities of Large Language Models (LLMs) can perform as well as, if not better than, bespoke models trained on specialized data. We apply these LLMs to a variety of datasets, including a real-life, unlabelled dataset of licensed financial professionals in Hong Kong, and critically assess the inherent demographic biases in these models. Our work not only advances the state-of-the-art in demographic enrichment but also opens avenues for future research in mitigating biases in LLMs.</li>
<li><strong>摘要：</strong>利用姓名中的性别、种族和年龄等人口统计信息丰富数据集是医疗保健、公共政策和社会科学等领域的一项关键任务。此类人口统计洞察有助于更精准、更有效地与目标人群互动。尽管之前曾尝试采用隐马尔可夫模型和循环神经网络根据姓名预测人口统计数据，但仍然存在重大限制：缺乏大规模、精心策划、无偏见、公开可用的数据集，以及缺乏适用于所有数据集的方法。这种稀缺性阻碍了传统监督学习方法的发展。在本文中，我们证明了大型语言模型 (LLM) 的零样本能力可以与在专门数据上训练的定制模型一样好，甚至更好。我们将这些 LLM 应用于各种数据集，包括香港持牌金融专业人士的真实、未标记数据集，并严格评估这些模型中固有的人口统计偏差。我们的工作不仅推动了人口统计学丰富化的最新进展，而且为未来减轻法学硕士偏见的研究开辟了道路。</li>
</ul>

<h3>Title: Multi-Document Grounded Multi-Turn Synthetic Dialog Generation</h3>
<ul>
<li><strong>Authors: </strong>Young-Suk Lee, Chulaka Gunasekara, Danish Contractor, Ramón Fernandez Astudillo, Radu Florian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11500">https://arxiv.org/abs/2409.11500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11500">https://arxiv.org/pdf/2409.11500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11500]] Multi-Document Grounded Multi-Turn Synthetic Dialog Generation(https://arxiv.org/abs/2409.11500)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We introduce a technique for multi-document grounded multi-turn synthetic dialog generation that incorporates three main ideas. First, we control the overall dialog flow using taxonomy-driven user queries that are generated with Chain-of-Thought (CoT) prompting. Second, we support the generation of multi-document grounded dialogs by mimicking real-world use of retrievers to update the grounding documents after every user-turn in the dialog. Third, we apply LLM-as-a-Judge to filter out queries with incorrect answers. Human evaluation of the synthetic dialog data suggests that the data is diverse, coherent, and includes mostly correct answers. Both human and automatic evaluations of answerable queries indicate that models fine-tuned on synthetic dialogs consistently out-perform those fine-tuned on existing human generated training data across four publicly available multi-turn document grounded benchmark test sets.</li>
<li><strong>摘要：</strong>我们介绍了一种基于多文档的多轮合成对话生成技术，该技术结合了三个主要思想。首先，我们使用分类驱动的用户查询来控制整体对话流，这些查询是通过思维链 (CoT) 提示生成的。其次，我们通过模仿现实世界中检索器的使用来支持基于多文档的对话的生成，在对话中的每个用户回合后更新基础文档。第三，我们应用 LLM-as-a-Judge 来过滤掉带有错误答案的查询。人工对合成对话数据的评估表明，数据是多样的、连贯的，并且包含大多数正确答案。对可回答查询的人工和自动评估都表明，在四个公开可用的基于多轮文档的基准测试集上，对合成对话进行微调的模型始终优于对现有人工生成的训练数据进行微调的模型。</li>
</ul>

<h3>Title: Egalitarian Language Representation in Language Models: It All Begins with Tokenizers</h3>
<ul>
<li><strong>Authors: </strong>Menan Velayuthan, Kengatharaiyer Sarveswaran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11501">https://arxiv.org/abs/2409.11501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11501">https://arxiv.org/pdf/2409.11501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11501]] Egalitarian Language Representation in Language Models: It All Begins with Tokenizers(https://arxiv.org/abs/2409.11501)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Tokenizers act as a bridge between human language and the latent space of language models, influencing how language is represented in these models. Due to the immense popularity of English-Centric Large Language Models (LLMs), efforts are being made to adapt them for other languages. However, we demonstrate that, from a tokenization standpoint, not all tokenizers offer fair representation for complex script languages such as Tamil, Sinhala, and Hindi, primarily due to the choice of pre-tokenization methods. We go further to show that pre-tokenization plays a more critical role than the tokenization algorithm itself in achieving an egalitarian representation of these complex script languages. To address this, we introduce an improvement to the Byte Pair Encoding (BPE) algorithm by incorporating graphemes, which we term Grapheme Pair Encoding (GPE). Our experiments show that grapheme-based character extraction outperforms byte-level tokenizers for complex scripts. We validate this approach through experiments on Tamil, Sinhala, and Hindi.</li>
<li><strong>摘要：</strong>标记器充当人类语言与语言模型潜在空间之间的桥梁，影响语言在这些模型中的表示方式。由于以英语为中心的大型语言模型 (LLM) 非常流行，人们正在努力将其应用于其他语言。然而，我们证明，从标记化的角度来看，并非所有标记器都能公平地表示复杂的脚本语言，例如泰米尔语、僧伽罗语和印地语，这主要是由于预标记化方法的选择。我们进一步表明，在实现这些复杂脚本语言的平等表示方面，预标记化比标记化算法本身发挥着更为关键的作用。为了解决这个问题，我们通过结合字素对字节对编码 (BPE) 算法进行了改进，我们称之为字素对编码 (GPE)。我们的实验表明，基于字素的字符提取优于复杂脚本的字节级标记器。我们通过对泰米尔语、僧伽罗语和印地语的实验验证了这种方法。</li>
</ul>

<h3>Title: Chain-of-Thought Prompting for Speech Translation</h3>
<ul>
<li><strong>Authors: </strong>Ke Hu, Zhehuai Chen, Chao-Han Huck Yang, Piotr Żelasko, Oleksii Hrinchuk, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11538">https://arxiv.org/abs/2409.11538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11538">https://arxiv.org/pdf/2409.11538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11538]] Chain-of-Thought Prompting for Speech Translation(https://arxiv.org/abs/2409.11538)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable advancements in language understanding and generation. Building on the success of text-based LLMs, recent research has adapted these models to use speech embeddings for prompting, resulting in Speech-LLM models that exhibit strong performance in automatic speech recognition (ASR) and automatic speech translation (AST). In this work, we propose a novel approach to leverage ASR transcripts as prompts for AST in a Speech-LLM built on an encoder-decoder text LLM. The Speech-LLM model consists of a speech encoder and an encoder-decoder structure Megatron-T5. By first decoding speech to generate ASR transcripts and subsequently using these transcripts along with encoded speech for prompting, we guide the speech translation in a two-step process like chain-of-thought (CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model adaptation and shows superior performance to full model fine-tuning. Experimental results show that the proposed CoT prompting significantly improves AST performance, achieving an average increase of 2.4 BLEU points across 6 En->X or X->En AST tasks compared to speech prompting alone. Additionally, compared to a related CoT prediction method that predicts a concatenated sequence of ASR and AST transcripts, our method performs better by an average of 2 BLEU points.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在语言理解和生成方面取得了显著的进步。在基于文本的 LLM 取得成功的基础上，最近的研究已将这些模型调整为使用语音嵌入进行提示，从而产生了在自动语音识别 (ASR) 和自动语音翻译 (AST) 中表现出色性能的 Speech-LLM 模型。在这项工作中，我们提出了一种新方法，利用 ASR 转录作为基于编码器-解码器文本 LLM 构建的 Speech-LLM 中的 AST 提示。Speech-LLM 模型由语音编码器和编码器-解码器结构 Megatron-T5 组成。通过首先解码语音以生成 ASR 转录，然后将这些转录与编码语音一起用于提示，我们以类似思路链 (CoT) 提示的两步过程指导语音翻译。低秩自适应 (LoRA) 用于 T5 LLM 进行模型自适应，其性能优于完整模型微调。实验结果表明，提出的 CoT 提示显著提高了 AST 性能，与单独的语音提示相比，在 6 个 En->X 或 X->En AST 任务中平均提高了 2.4 个 BLEU 点。此外，与预测 ASR 和 AST 转录连接序列的相关 CoT 预测方法相比，我们的方法平均提高了 2 个 BLEU 点。</li>
</ul>

<h3>Title: Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Guillermo Marco, Luz Rello, Julio Gonzalo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11547">https://arxiv.org/abs/2409.11547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11547">https://arxiv.org/pdf/2409.11547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11547]] Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs(https://arxiv.org/abs/2409.11547)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we evaluate the creative fiction writing abilities of a fine-tuned small language model (SLM), BART Large, and compare its performance to humans and two large language models (LLMs): GPT-3.5 and GPT-4o. Our evaluation consists of two experiments: (i) a human evaluation where readers assess the stories generated by the SLM compared to human-written stories, and (ii) a qualitative linguistic analysis comparing the textual characteristics of the stories generated by the different models. In the first experiment, we asked 68 participants to rate short stories generated by the models and humans along dimensions such as grammaticality, relevance, creativity, and attractiveness. BART Large outperformed human writers in most aspects, except creativity, with an overall score of 2.11 compared to 1.85 for human-written texts -- a 14% improvement. In the second experiment, the qualitative analysis revealed that, while GPT-4o exhibited near-perfect internal and external coherence, it tended to produce more predictable narratives, with only 3% of its stories seen as novel. In contrast, 15% of BART's stories were considered novel, indicating a higher degree of creativity despite its smaller model size. This study provides both quantitative and qualitative insights into how model size and fine-tuning influence the balance between creativity, fluency, and coherence in creative writing tasks.</li>
<li><strong>摘要：</strong>在本文中，我们评估了经过微调的小型语言模型 (SLM) BART Large 的创意小说写作能力，并将其表现与人类和两个大型语言模型 (LLM)：GPT-3.5 和 GPT-4o 进行了比较。我们的评估包括两个实验：(i) 人工评估，读者评估 SLM 生成的故事与人工编写的故事；(ii) 定性语言学分析，比较不同模型生成的故事的文本特征。在第一个实验中，我们要求 68 名参与者从语法性、相关性、创造力和吸引力等维度对模型和人类生成的短篇小说进行评分。BART Large 在创造力以外的大多数方面都胜过人类作家，总分为 2.11，而人工编写的文本为 1.85，提高了 14%。在第二个实验中，定性分析显示，虽然 GPT-4o 表现出近乎完美的内部和外部连贯性，但它倾向于产生更可预测的叙述，只有 3% 的故事被视为新颖。相比之下，BART 的故事中有 15% 被认为是新颖的，这表明尽管模型规模较小，但创造力程度更高。这项研究提供了定量和定性的见解，说明模型大小和微调如何影响创意写作任务中的创造力、流畅性和连贯性之间的平衡。</li>
</ul>

<h3>Title: HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection</h3>
<ul>
<li><strong>Authors: </strong>Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, Philip Treleaven</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11579">https://arxiv.org/abs/2409.11579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11579">https://arxiv.org/pdf/2409.11579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11579]] HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection(https://arxiv.org/abs/2409.11579)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Stereotypes are generalised assumptions about societal groups, and even state-of-the-art LLMs using in-context learning struggle to identify them accurately. Due to the subjective nature of stereotypes, where what constitutes a stereotype can vary widely depending on cultural, social, and individual perspectives, robust explainability is crucial. Explainable models ensure that these nuanced judgments can be understood and validated by human users, promoting trust and accountability. We address these challenges by introducing HEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text Stereotype Detection), a framework that enhances model performance, minimises carbon footprint, and provides transparent, interpretable explanations. We establish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising 57,201 labeled texts across six groups, including under-represented demographics like LGBTQ+ and regional stereotypes. Ablation studies confirm that BERT models fine-tuned on EMGSD outperform those trained on individual components. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model using SHAP to generate token-level importance values, ensuring alignment with human understanding, and calculate explainability confidence scores by comparing SHAP and LIME outputs. Finally, HEARTS is applied to assess stereotypical bias in 12 LLM outputs, revealing a gradual reduction in bias over time within model families.</li>
<li><strong>摘要：</strong>刻板印象是对社会群体的普遍假设，即使是使用情境学习的最先进的法学硕士也难以准确识别它们。由于刻板印象的主观性，即构成刻板印象的因素会因文化、社会和个人观点的不同而有很大差异，因此强大的可解释性至关重要。可解释的模型确保这些细微的判断能够被人类用户理解和验证，从而促进信任和责任感。我们通过引入 HEARTS（可解释、可持续和强大的文本刻板印象检测整体框架）来应对这些挑战，该框架可提高模型性能、最大限度地减少碳足迹并提供透明、可解释的解释。我们建立了扩展多粒度刻板印象数据集 (EMGSD)，包含六个群体的 57,201 个带标签的文本，包括代表性不足的人口统计数据，如 LGBTQ+ 和区域刻板印象。消融研究证实，在 EMGSD 上微调的 BERT 模型优于在单个组件上训练的模型。然后，我们使用 SHAP 分析经过微调的碳效率 ALBERT-V2 模型，以生成 token 级重要性值，确保与人类理解保持一致，并通过比较 SHAP 和 LIME 输出来计算可解释性置信度分数。最后，应用 HEARTS 评估 12 个 LLM 输出中的刻板偏见，结果显示模型系列中的偏见随着时间的推移逐渐减少。</li>
</ul>

<h3>Title: ProSLM : A Prolog Synergized Language Model for explainable Domain Specific Knowledge Based Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Priyesh Vakharia, Abigail Kufeldt, Max Meyers, Ian Lane, Leilani Gilpin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11589">https://arxiv.org/abs/2409.11589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11589">https://arxiv.org/pdf/2409.11589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11589]] ProSLM : A Prolog Synergized Language Model for explainable Domain Specific Knowledge Based Question Answering(https://arxiv.org/abs/2409.11589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Neurosymbolic approaches can add robustness to opaque neural systems by incorporating explainable symbolic representations. However, previous approaches have not used formal logic to contextualize queries to and validate outputs of large language models (LLMs). We propose \systemname{}, a novel neurosymbolic framework, to improve the robustness and reliability of LLMs in question-answering tasks. We provide \systemname{} with a domain-specific knowledge base, a logical reasoning system, and an integration to an existing LLM. This framework has two capabilities (1) context gathering: generating explainable and relevant context for a given query, and (2) validation: confirming and validating the factual accuracy of a statement in accordance with a knowledge base (KB). Our work opens a new area of neurosymbolic generative AI text validation and user personalization.</li>
<li><strong>摘要：</strong>神经符号方法可以通过结合可解释的符号表示来增加不透明神经系统的鲁棒性。然而，以前的方法没有使用形式逻辑将查询情境化并验证大型语言模型 (LLM) 的输出。我们提出了一种新颖的神经符号框架 \systemname{}，以提高 LLM 在问答任务中的鲁棒性和可靠性。我们为 \systemname{} 提供了一个领域特定的知识库、一个逻辑推理系统以及与现有 LLM 的集成。该框架有两种功能 (1) 上下文收集：为给定查询生成可解释和相关的上下文，以及 (2) 验证：根据知识库 (KB) 确认和验证陈述的事实准确性。我们的工作开辟了神经符号生成 AI 文本验证和用户个性化的新领域。</li>
</ul>

<h3>Title: "A Woman is More Culturally Knowledgeable than A Man?": The Effect of Personas on Cultural Norm Interpretation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mahammed Kamruzzaman, Hieu Nguyen, Nazmul Hassan, Gene Louis Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11636">https://arxiv.org/abs/2409.11636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11636">https://arxiv.org/pdf/2409.11636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11636]] "A Woman is More Culturally Knowledgeable than A Man?": The Effect of Personas on Cultural Norm Interpretation in LLMs(https://arxiv.org/abs/2409.11636)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As the deployment of large language models (LLMs) expands, there is an increasing demand for personalized LLMs. One method to personalize and guide the outputs of these models is by assigning a persona -- a role that describes the expected behavior of the LLM (e.g., a man, a woman, an engineer). This study investigates whether an LLM's understanding of social norms varies across assigned personas. Ideally, the perception of a social norm should remain consistent regardless of the persona, since acceptability of a social norm should be determined by the region the norm originates from, rather than by individual characteristics such as gender, body size, or race. A norm is universal within its cultural context. In our research, we tested 36 distinct personas from 12 sociodemographic categories (e.g., age, gender, beauty) across four different LLMs. We find that LLMs' cultural norm interpretation varies based on the persona used and the norm interpretation also varies within a sociodemographic category (e.g., a fat person and a thin person as in physical appearance group) where an LLM with the more socially desirable persona (e.g., a thin person) interprets social norms more accurately than with the less socially desirable persona (e.g., a fat person). We also discuss how different types of social biases may contribute to the results that we observe.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的部署不断扩大，对个性化 LLM 的需求也日益增加。个性化和指导这些模型输出的一种方法是分配一个角色——一个描述 LLM 预期行为的角色（例如，男性、女性、工程师）。这项研究调查了 LLM 对社会规范的理解是否因分配的角色而异。理想情况下，对社会规范的看法应该与角色无关，因为社会规范的可接受性应该由规范起源的地区决定，而不是由性别、体型或种族等个人特征决定。规范在其文化背景下是普遍的。在我们的研究中，我们在四个不同的 LLM 中测试了来自 12 个社会人口类别（例如，年龄、性别、美貌）的 36 个不同角色。我们发现，LLM 对文化规范的解读会因所使用的角色而异，并且规范解读也会因社会人口类别而异（例如，外表组中的胖人和瘦人），其中具有更受社会欢迎的角色（例如，瘦人）的 LLM 比具有不太受社会欢迎的角色（例如，胖人）的 LLM 更准确地解读社会规范。我们还讨论了不同类型的社会偏见如何影响我们观察到的结果。</li>
</ul>

<h3>Title: BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla</h3>
<ul>
<li><strong>Authors: </strong>Mahammed Kamruzzaman, Abdullah Al Monsur, Shrabon Das, Enamul Hassan, Gene Louis Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11638">https://arxiv.org/abs/2409.11638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11638">https://arxiv.org/pdf/2409.11638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11638]] BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla(https://arxiv.org/abs/2409.11638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study presents BanStereoSet, a dataset designed to evaluate stereotypical social biases in multilingual LLMs for the Bangla language. In an effort to extend the focus of bias research beyond English-centric datasets, we have localized the content from the StereoSet, IndiBias, and Kamruzzaman et. al.'s datasets, producing a resource tailored to capture biases prevalent within the Bangla-speaking community. Our BanStereoSet dataset consists of 1,194 sentences spanning 9 categories of bias: race, profession, gender, ageism, beauty, beauty in profession, region, caste, and religion. This dataset not only serves as a crucial tool for measuring bias in multilingual LLMs but also facilitates the exploration of stereotypical bias across different social categories, potentially guiding the development of more equitable language technologies in Bangladeshi contexts. Our analysis of several language models using this dataset indicates significant biases, reinforcing the necessity for culturally and linguistically adapted datasets to develop more equitable language technologies.</li>
<li><strong>摘要：</strong>本研究介绍了 BanStereoSet，这是一个旨在评估孟加拉语多语言法学硕士中的刻板社会偏见的数据集。为了将偏见研究的重点扩展到以英语为中心的数据集之外，我们对 StereoSet、IndiBias 和 Kamruzzaman 等人的数据集的内容进行了本地化，制作了一种专门用于捕捉孟加拉语社区中普遍存在的偏见的资源。我们的 BanStereoSet 数据集包含 1,194 个句子，涵盖 9 个偏见类别：种族、职业、性别、年龄歧视、美貌、职业美貌、地区、种姓和宗教。该数据集不仅是衡量多语言法学硕士偏见的重要工具，而且还有助于探索不同社会类别中的刻板偏见，从而有可能指导在孟加拉语环境中开发更公平的语言技术。我们使用该数据集对几种语言模型的分析表明存在显著的偏见，这进一步强调了文化和语言适应数据集以开发更公平的语言技术的必要性。</li>
</ul>

<h3>Title: RUIE: Retrieval-based Unified Information Extraction using Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xincheng Liao, Junwen Duan, Yixi Huang, Jianxin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11673">https://arxiv.org/abs/2409.11673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11673">https://arxiv.org/pdf/2409.11673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11673]] RUIE: Retrieval-based Unified Information Extraction using Large Language Model(https://arxiv.org/abs/2409.11673)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Unified information extraction (UIE) aims to complete all information extraction tasks using a single model or framework. While previous work has primarily focused on instruction-tuning large language models (LLMs) with constructed datasets, these methods require significant computational resources and struggle to generalize to unseen tasks. To address these limitations, we propose RUIE (Retrieval-based Unified Information Extraction), a framework that leverages in-context learning to enable rapid generalization while reducing computational costs. The key challenge in RUIE is selecting the most beneficial demonstrations for LLMs to effectively handle diverse IE tasks. To achieve this, we integrate LLM preferences for ranking candidate demonstrations and design a keyword-enhanced reward model to capture fine-grained relationships between queries and demonstrations. We then train a bi-encoder retriever for UIE through contrastive learning and knowledge distillation. To the best of our knowledge, RUIE is the first trainable retrieval framework for UIE. Experimental results on 8 held-out datasets demonstrate RUIE's effectiveness in generalizing to unseen tasks, with average F1-score improvements of 19.22 and 3.13 compared to instruction-tuning methods and other retrievers, respectively. Further analysis confirms RUIE's adaptability to LLMs of varying sizes and the importance of its key components.</li>
<li><strong>摘要：</strong>统一信息提取 (UIE) 旨在使用单一模型或框架完成所有信息提取任务。虽然以前的工作主要集中在使用构建的数据集对大型语言模型 (LLM) 进行指令调整，但这些方法需要大量计算资源，并且难以推广到未见过的任务。为了解决这些限制，我们提出了 RUIE（基于检索的统一信息提取），这是一个利用上下文学习实现快速泛化同时降低计算成本的框架。RUIE 的关键挑战是选择对 LLM 最有益的演示，以有效处理各种 IE 任务。为了实现这一点，我们整合了 LLM 对排名候选演示的偏好，并设计了一个关键字增强奖励模型来捕捉查询和演示之间的细粒度关系。然后，我们通过对比学习和知识提炼为 UIE 训练双编码器检索器。据我们所知，RUIE 是第一个可训练的 UIE 检索框架。在 8 个保留数据集上的实验结果证明了 RUIE 在推广到未见任务方面的有效性，与指令调整方法和其他检索器相比，平均 F1 分数分别提高了 19.22 和 3.13。进一步的分析证实了 RUIE 对不同大小的 LLM 的适应性及其关键组件的重要性。</li>
</ul>

<h3>Title: Harnessing LLMs for API Interactions: A Framework for Classification and Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Chunliang Tao, Xiaojing Fan, Yahe Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11703">https://arxiv.org/abs/2409.11703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11703">https://arxiv.org/pdf/2409.11703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11703]] Harnessing LLMs for API Interactions: A Framework for Classification and Synthetic Data Generation(https://arxiv.org/abs/2409.11703)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) advance in natural language processing, there is growing interest in leveraging their capabilities to simplify software interactions. In this paper, we propose a novel system that integrates LLMs for both classifying natural language inputs into corresponding API calls and automating the creation of sample datasets tailored to specific API functions. By classifying natural language commands, our system allows users to invoke complex software functionalities through simple inputs, improving interaction efficiency and lowering the barrier to software utilization. Our dataset generation approach also enables the efficient and systematic evaluation of different LLMs in classifying API calls, offering a practical tool for developers or business owners to assess the suitability of LLMs for customized API management. We conduct experiments on several prominent LLMs using generated sample datasets for various API functions. The results show that GPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B performs much worse at 0.759. These findings highlight the potential of LLMs to transform API management and validate the effectiveness of our system in guiding model testing and selection across diverse applications.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在自然语言处理领域的进步，人们越来越有兴趣利用它们的功能来简化软件交互。在本文中，我们提出了一种集成 LLM 的新系统，用于将自然语言输入分类为相应的 API 调用，并自动创建针对特定 API 函数的样本数据集。通过对自然语言命令进行分类，我们的系统允许用户通过简单的输入调用复杂的软件功能，从而提高交互效率并降低软件使用的门槛。我们的数据集生成方法还可以高效、系统地评估不同的 LLM 在分类 API 调用方面的表现，为开发人员或企业主提供了一个实用的工具来评估 LLM 是否适合定制 API 管理。我们使用为各种 API 函数生成的样本数据集对几个著名的 LLM 进行了实验。结果表明，GPT-4 实现了 0.996 的高分类准确率，而 LLaMA-3-8B 的表现要差得多，为 0.759。这些发现凸显了 LLM 改变 API 管理的潜力，并验证了我们的系统在指导跨不同应用程序的模型测试和选择方面的有效性。</li>
</ul>

<h3>Title: From Lists to Emojis: How Format Bias Affects Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11704">https://arxiv.org/abs/2409.11704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11704">https://arxiv.org/pdf/2409.11704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11704]] From Lists to Emojis: How Format Bias Affects Model Alignment(https://arxiv.org/abs/2409.11704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In this paper, we study format biases in reinforcement learning from human feedback (RLHF). We observe that many widely-used preference models, including human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark, exhibit strong biases towards specific format patterns, such as lists, links, bold text, and emojis. Furthermore, large language models (LLMs) can exploit these biases to achieve higher rankings on popular benchmarks like AlpacaEval and LMSYS Chatbot Arena. One notable example of this is verbosity bias, where current preference models favor longer responses that appear more comprehensive, even when their quality is equal to or lower than shorter, competing responses. However, format biases beyond verbosity remain largely underexplored in the literature. In this work, we extend the study of biases in preference learning beyond the commonly recognized length bias, offering a comprehensive analysis of a wider range of format biases. Additionally, we show that with a small amount of biased data (less than 1%), we can inject significant bias into the reward model. Moreover, these format biases can also be easily exploited by downstream alignment algorithms, such as best-of-n sampling and online iterative DPO, as it is usually easier to manipulate the format than to improve the quality of responses. Our findings emphasize the need to disentangle format and content both for designing alignment algorithms and evaluating models.</li>
<li><strong>摘要：</strong>在本文中，我们研究了从人类反馈 (RLHF) 中强化学习的格式偏差。我们观察到，许多广泛使用的偏好模型，包括人类评估者、GPT-4 和 RewardBench 基准上的顶级模型，都对特定格式模式表现出强烈的偏见，例如列表、链接、粗体文本和表情符号。此外，大型语言模型 (LLM) 可以利用这些偏见在 AlpacaEval 和 LMSYS Chatbot Arena 等流行基准上获得更高的排名。一个值得注意的例子是冗长偏见，当前的偏好模型倾向于看起来更全面的较长响应，即使它们的质量等于或低于较短的竞争响应。然而，除了冗长之外的格式偏见在文献中仍然很少得到充分探索。在这项工作中，我们将偏好学习中的偏见研究扩展到普遍认可的长度偏见之外，对更广泛的格式偏见进行了全面分析。此外，我们表明，使用少量有偏见的数据（少于 1%），我们可以将显着的偏见注入奖励模型。此外，这些格式偏差也很容易被下游对齐算法（如最佳 n 抽样和在线迭代 DPO）利用，因为操纵格式通常比提高响应质量更容易。我们的研究结果强调，无论是在设计对齐算法还是在评估模型时，都需要解开格式和内容之间的联系。</li>
</ul>

<h3>Title: TART: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Lu, Liangming Pan, Yubo Ma, Preslav Nakov, Min-Yen Kan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11724">https://arxiv.org/abs/2409.11724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11724">https://arxiv.org/pdf/2409.11724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11724]] TART: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning(https://arxiv.org/abs/2409.11724)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) exhibit limited ability to understand table structures and to apply precise numerical reasoning, which is crucial for tasks such as table question answering (TQA) and table-based fact verification (TFV). To address these challenges, we introduce our Tool-Augmented Reasoning framework for Tables (TART), which integrates LLMs with specialized tools. TART contains three key components: a table formatter to ensure accurate data representation, a tool maker to develop specific computational tools, and an explanation generator to maintain explainability. We also present the TOOLTAB dataset, a new benchmark designed specifically for training LLMs in table-tool integration. Our experiments indicate that TART achieves substantial improvements over existing methods (e.g., Chain-of-Thought) by improving both the precision of data processing and the clarity of the reasoning process. Notably, TART paired with CodeLlama achieves 90.0% of the accuracy of the closed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse real-world scenarios. All the code and data are available at this https URL.</li>
<li><strong>摘要：</strong>当前的大型语言模型 (LLM) 在理解表格结构和应用精确数字推理方面的能力有限，而这对于表格问答 (TQA) 和基于表格的事实验证 (TFV) 等任务至关重要。为了应对这些挑战，我们引入了用于表格的工具增强推理框架 (TART)，它将 LLM 与专用工具集成在一起。TART 包含三个关键组件：用于确保准确数据表示的表格格式化程序、用于开发特定计算工具的工具制作器以及用于保持可解释性的解释生成器。我们还介绍了 TOOLTAB 数据集，这是一个专门为训练 LLM 进行表格工具集成而设计的新基准。我们的实验表明，TART 通过提高数据处理的精度和推理过程的清晰度，实现了对现有方法（例如，思维链）的显着改进。值得注意的是，TART 与 CodeLlama 配对实现了闭源 LLM GPT-3.5-turbo 的 90.0% 的准确率，凸显了其在各种现实场景中的稳健性。所有代码和数据均可在此 https URL 上获取。</li>
</ul>

<h3>Title: Revealing the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Zhang, Jiawei Sheng, Shuaiyi Nie, Zefeng Zhang, Xinghua Zhang, Yongquan He, Tingwen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11726">https://arxiv.org/abs/2409.11726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11726">https://arxiv.org/pdf/2409.11726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11726]] Revealing the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing(https://arxiv.org/abs/2409.11726)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) role-playing has gained widespread attention, where the authentic character knowledge is crucial for constructing realistic LLM role-playing agents. However, existing works usually overlook the exploration of LLMs' ability to detect characters' known knowledge errors (KKE) and unknown knowledge errors (UKE) while playing roles, which would lead to low-quality automatic construction of character trainable corpus. In this paper, we propose a probing dataset to evaluate LLMs' ability to detect errors in KKE and UKE. The results indicate that even the latest LLMs struggle to effectively detect these two types of errors, especially when it comes to familiar knowledge. We experimented with various reasoning strategies and propose an agent-based reasoning method, Self-Recollection and Self-Doubt (S2RD), to further explore the potential for improving error detection capabilities. Experiments show that our method effectively improves the LLMs' ability to detect error character knowledge, but it remains an issue that requires ongoing attention.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 角色扮演受到广泛关注，其中真实的角色知识对于构建逼真的 LLM 角色扮演代理至关重要。然而，现有的研究通常忽略了对 LLM 在扮演角色时检测角色已知知识错误 (KKE) 和未知知识错误 (UKE) 的能力的探索，这将导致角色可训练语料库的自动构建质量低下。在本文中，我们提出了一个探测数据集来评估 LLM 检测 KKE 和 UKE 错误的能力。结果表明，即使是最新的 LLM 也难以有效检测这两类错误，尤其是在熟悉知识方面。我们尝试了各种推理策略，并提出了一种基于代理的推理方法，即自我回忆和自我怀疑 (S2RD)，以进一步探索提高错误检测能力的潜力。实验表明，我们的方法有效地提高了 LLM 检测错误角色知识的能力，但这仍然是一个需要持续关注的问题。</li>
</ul>

<h3>Title: Enabling Real-Time Conversations with Minimal Training Costs</h3>
<ul>
<li><strong>Authors: </strong>Wang Xu, Shuo Wang, Weilin Zhao, Xu Han, Yukun Yan, Yudi Zhang, Zhe Tao, Zhiyuan Liu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11727">https://arxiv.org/abs/2409.11727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11727">https://arxiv.org/pdf/2409.11727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11727]] Enabling Real-Time Conversations with Minimal Training Costs(https://arxiv.org/abs/2409.11727)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated the ability to improve human efficiency through conversational interactions. Conventional LLM-powered dialogue systems, operating on a turn-based paradigm, preclude real-time interaction during response generation. To address this limitation, researchers have proposed duplex models. These models can dynamically adapt to user input, facilitating real-time interactive feedback. However, these methods typically require substantial computational resources to acquire the ability. To reduce overhead, this paper presents a new duplex decoding approach that enhances LLMs with duplex ability, requiring minimal additional training. Specifically, our method employs parallel decoding of queries and responses in conversations, effectively implementing a channel-division-multiplexing decoding strategy. Experimental results indicate that our proposed method significantly enhances the naturalness and human-likeness of user-AI interactions with minimal training costs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已证明能够通过对话交互提高人类效率。传统的 LLM 驱动的对话系统采用回合制模式，在响应生成期间无法进行实时交互。为了解决这一限制，研究人员提出了双工模型。这些模型可以动态适应用户输入，促进实时交互反馈。然而，这些方法通常需要大量的计算资源才能获得这种能力。为了减少开销，本文提出了一种新的双工解码方法，该方法增强了具有双工能力的 LLM，只需要很少的额外训练。具体来说，我们的方法采用对话中查询和响应的并行解码，有效地实现了通道分复用解码策略。实验结果表明，我们提出的方法以最小的训练成本显著提高了用户与人工智能交互的自然性和人性化。</li>
</ul>

<h3>Title: Human-like Affective Cognition in Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Kanishk Gandhi, Zoe Lynch, Jan-Philipp Fränken, Kayla Patterson, Sharon Wambu, Tobias Gerstenberg, Desmond C. Ong, Noah D. Goodman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11733">https://arxiv.org/abs/2409.11733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11733">https://arxiv.org/pdf/2409.11733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11733]] Human-like Affective Cognition in Foundation Models(https://arxiv.org/abs/2409.11733)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Understanding emotions is fundamental to human interaction and experience. Humans easily infer emotions from situations or facial expressions, situations from emotions, and do a variety of other \emph{affective cognition}. How adept is modern AI at these inferences? We introduce an evaluation framework for testing affective cognition in foundation models. Starting from psychological theory, we generate 1,280 diverse scenarios exploring relationships between appraisals, emotions, expressions, and outcomes. We evaluate the abilities of foundation models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully selected conditions. Our results show foundation models tend to agree with human intuitions, matching or exceeding interparticipant agreement. In some conditions, models are ``superhuman'' -- they better predict modal human judgements than the average human. All models benefit from chain-of-thought reasoning. This suggests foundation models have acquired a human-like understanding of emotions and their influence on beliefs and behavior.</li>
<li><strong>摘要：</strong>理解情绪是人类互动和体验的基础。人类很容易从情境或面部表情推断情绪，从情绪推断情境，并进行各种其他情感认知。现代人工智能在这些推理方面有多熟练？我们引入了一个评估框架来测试基础模型中的情感认知。从心理学理论出发，我们生成了 1,280 个不同的场景，探索评估、情绪、表情和结果之间的关系。我们在精心选择的条件下评估基础模型 (GPT-4、Claude-3、Gemini-1.5-Pro) 和人类 (N = 567) 的能力。我们的结果表明，基础模型倾向于与人类的直觉一致，达到或超过参与者之间的一致性。在某些情况下，模型是“超人”——它们比普通人更好地预测模态人类判断。所有模型都受益于思路链推理。这表明基础模型已经获得了类似人类的对情绪及其对信仰和行为的影响的理解。</li>
</ul>

<h3>Title: Development and bilingual evaluation of Japanese medical large language model within reasonably low computational resources</h3>
<ul>
<li><strong>Authors: </strong>Issey Sukeda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11783">https://arxiv.org/abs/2409.11783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11783">https://arxiv.org/pdf/2409.11783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11783]] Development and bilingual evaluation of Japanese medical large language model within reasonably low computational resources(https://arxiv.org/abs/2409.11783)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The recent success of large language models (LLMs) and the scaling law has led to a widespread adoption of larger models. Particularly in the healthcare industry, there is an increasing demand for locally operated LLMs due to security concerns. However, the majority of high quality open-source LLMs have a size of 70B parameters, imposing significant financial burdens on users for GPU preparation and operation. To overcome these issues, we present a medical adaptation based on the recent 7B models, which enables the operation in low computational resources. We compare the performance on medical question-answering benchmarks in two languages (Japanese and English), demonstrating that its scores reach parity with or surpass those of currently existing medical LLMs that are ten times larger. We find that fine-tuning an English-centric base model on Japanese medical dataset improves the score in both language, supporting the effect of cross-lingual knowledge transfer. We hope that this study will alleviate financial challenges, serving as a stepping stone for clinical institutions to practically utilize LLMs locally. Our evaluation code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 和缩放定律最近取得的成功导致了更大模型的广泛采用。特别是在医疗保健行业，出于安全考虑，对本地运营的 LLM 的需求日益增加。然而，大多数高质量的开源 LLM 都有 70B 个参数，这给用户在 GPU 准备和操作方面带来了巨大的财务负担。为了克服这些问题，我们提出了一种基于最近 7B 模型的医学改编版，该改编版可以在低计算资源下运行。我们比较了两种语言（日语和英语）医学问答基准上的表现，表明其得分与目前大十倍的现有医学 LLM 相当或超过其得分。我们发现，在日语医学数据集上微调以英语为中心的基础模型可以提高两种语言的得分，支持跨语言知识转移的效果。我们希望这项研究能够缓解财务挑战，成为临床机构在本地实际使用 LLM 的垫脚石。我们的评估代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: The Factuality of Large Language Models in the Legal Domain</h3>
<ul>
<li><strong>Authors: </strong>Rajaa El Hamdani, Thomas Bonald, Fragkiskos Malliaros, Nils Holzenberger, Fabian Suchanek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11798">https://arxiv.org/abs/2409.11798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11798">https://arxiv.org/pdf/2409.11798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11798]] The Factuality of Large Language Models in the Legal Domain(https://arxiv.org/abs/2409.11798)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper investigates the factuality of large language models (LLMs) as knowledge bases in the legal domain, in a realistic usage scenario: we allow for acceptable variations in the answer, and let the model abstain from answering when uncertain. First, we design a dataset of diverse factual questions about case law and legislation. We then use the dataset to evaluate several LLMs under different evaluation methods, including exact, alias, and fuzzy matching. Our results show that the performance improves significantly under the alias and fuzzy matching methods. Further, we explore the impact of abstaining and in-context examples, finding that both strategies enhance precision. Finally, we demonstrate that additional pre-training on legal documents, as seen with SaulLM, further improves factual precision from 63% to 81%.</li>
<li><strong>摘要：</strong>本文在现实使用场景中研究了大型语言模型 (LLM) 作为法律领域知识库的事实性：我们允许答案有可接受的变化，并让模型在不确定时放弃回答。首先，我们设计了一个关于判例法和立法的各种事实问题的数据集。然后，我们使用该数据集在不同评估方法下评估几个 LLM，包括精确匹配、别名匹配和模糊匹配。我们的结果表明，在别名匹配和模糊匹配方法下，性能显著提高。此外，我们探讨了放弃和上下文示例的影响，发现这两种策略都能提高精度。最后，我们证明，对法律文件进行额外的预训练，如 SaulLM 所示，可将事实精度从 63% 进一步提高到 81%。</li>
</ul>

<h3>Title: MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts</h3>
<ul>
<li><strong>Authors: </strong>Tianle Gu, Kexin Huang, Ruilin Luo, Yuanqi Yao, Yujiu Yang, Yan Teng, Yingchun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11844">https://arxiv.org/abs/2409.11844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11844">https://arxiv.org/pdf/2409.11844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11844]] MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts(https://arxiv.org/abs/2409.11844)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can memorize sensitive information, raising concerns about potential misuse. LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them. We evaluate MEOW on the commonly used unlearn benchmark, ToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks. Results demonstrate significant improvement of MEOW in forget quality without substantial loss in model utility. Meanwhile, MEOW does not exhibit significant degradation in NLU or NLG capabilities, and there is even a slight improvement in NLU performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可以记忆敏感信息，这引发了对潜在滥用的担忧。LLM 反学习是一种事后方法，用于从经过训练的 LLM 中删除这些信息，它提供了一种有希望的解决方案来减轻这些风险。然而，以前的做法面临三个关键挑战：1. 实用性：成功的反学习往往会导致不相关任务的灾难性崩溃。2. 效率：许多方法要么涉及添加类似大小的模型，这会减慢反学习或推理的速度，要么需要保留难以获得的数据。3. 稳健性：即使是有效的方法也可能通过提取技术泄露数据。为了应对这些挑战，我们提出了 MEOW，一种简单而有效的基于梯度下降的反学习方法。具体来说，我们使用离线 LLM 来生成一组反转事实。然后，我们设计了一个新的指标 MEMO，以量化 LLM 中的记忆。最后，根据 MEMO 提供的信号，我们选择最合适的一组反转事实并根据它们对模型进行微调。我们在常用的遗忘基准 ToFU 上对 MEOW 进行了评估，使用 Llama2-7B-Chat 和 Phi-1.5B，并在 NLU 和 NLG 任务上对其进行了测试。结果表明，MEOW 在遗忘质量方面有显著改善，而模型效用没有显著损失。同时，MEOW 在 NLU 或 NLG 能力方面没有表现出显著的下降，甚至 NLU 性能还略有提升。</li>
</ul>

<h3>Title: LLMs + Persona-Plug = Personalized LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiongnan Liu, Yutao Zhu, Shuting Wang, Xiaochi Wei, Erxue Min, Yu Lu, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11901">https://arxiv.org/abs/2409.11901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11901">https://arxiv.org/pdf/2409.11901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11901]] LLMs + Persona-Plug = Personalized LLMs(https://arxiv.org/abs/2409.11901)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Personalization plays a critical role in numerous language tasks and applications, since users with the same requirements may prefer diverse outputs based on their individual interests. This has led to the development of various personalized approaches aimed at adapting large language models (LLMs) to generate customized outputs aligned with user preferences. Some of them involve fine-tuning a unique personalized LLM for each user, which is too expensive for widespread application. Alternative approaches introduce personalization information in a plug-and-play manner by retrieving the user's relevant historical texts as demonstrations. However, this retrieval-based strategy may break the continuity of the user history and fail to capture the user's overall styles and patterns, hence leading to sub-optimal performance. To address these challenges, we propose a novel personalized LLM model, \ours{}. It constructs a user-specific embedding for each individual by modeling all her historical contexts through a lightweight plug-in user embedder module. By attaching this embedding to the task input, LLMs can better understand and capture user habits and preferences, thereby producing more personalized outputs without tuning their own parameters. Extensive experiments on various tasks in the language model personalization (LaMP) benchmark demonstrate that the proposed model significantly outperforms existing personalized LLM approaches.</li>
<li><strong>摘要：</strong>个性化在许多语言任务和应用中起着至关重要的作用，因为具有相同需求的用户可能会根据其个人兴趣偏好不同的输出。这导致了各种个性化方法的发展，旨在调整大型语言模型 (LLM) 以生成符合用户偏好的定制输出。其中一些方法涉及为每个用户微调独特的个性化 LLM，这对于广泛应用来说太昂贵了。替代方法通过检索用户的相关历史文本作为演示，以即插即用的方式引入个性化信息。然而，这种基于检索的策略可能会破坏用户历史的连续性，无法捕捉用户的整体风格和模式，从而导致性能不佳。为了应对这些挑战，我们提出了一种新颖的个性化 LLM 模型 \ours{}。它通过轻量级插件用户嵌入器模块对每个人的所有历史背景进行建模，为每个个人构建一个用户特定的嵌入。通过将此嵌入附加到任务输入，LLM 可以更好地理解和捕捉用户的习惯和偏好，从而在不调整自身参数的情况下产生更个性化的输出。在语言模型个性化（LaMP）基准上对各种任务进行的大量实验表明，所提出的模型明显优于现有的个性化 LLM 方法。</li>
</ul>

<h3>Title: LLMs in Education: Novel Perspectives, Challenges, and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Bashar Alhafni, Sowmya Vajjala, Stefano Bannò, Kaushal Kumar Maurya, Ekaterina Kochmar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11917">https://arxiv.org/abs/2409.11917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11917">https://arxiv.org/pdf/2409.11917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11917]] LLMs in Education: Novel Perspectives, Challenges, and Opportunities(https://arxiv.org/abs/2409.11917)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The role of large language models (LLMs) in education is an increasing area of interest today, considering the new opportunities they offer for teaching, learning, and assessment. This cutting-edge tutorial provides an overview of the educational applications of NLP and the impact that the recent advances in LLMs have had on this field. We will discuss the key challenges and opportunities presented by LLMs, grounding them in the context of four major educational applications: reading, writing, and speaking skills, and intelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for researchers and practitioners interested in the educational applications of NLP and the role LLMs have to play in this area. It is the first of its kind to address this timely topic.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在教育中的作用如今越来越受到关注，因为它们为教学、学习和评估提供了新的机会。本前沿教程概述了 NLP 的教育应用以及 LLM 的最新进展对该领域的影响。我们将讨论 LLM 带来的主要挑战和机遇，并将其置于四个主要教育应用的背景下：阅读、写作和口语技能以及智能辅导系统 (ITS)。本 COLING 2025 教程专为对 NLP 的教育应用以及 LLM 在该领域的作用感兴趣的研究人员和从业者而设计。这是第一个解决这个及时主题的教程。</li>
</ul>

<h3>Title: Efficacy of Synthetic Data as a Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Maheshwari, Dmitry Ivanov, Kevin El Haddad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11968">https://arxiv.org/abs/2409.11968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11968">https://arxiv.org/pdf/2409.11968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11968]] Efficacy of Synthetic Data as a Benchmark(https://arxiv.org/abs/2409.11968)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have enabled a range of applications in zero-shot and few-shot learning settings, including the generation of synthetic datasets for training and testing. However, to reliably use these synthetic datasets, it is essential to understand how representative they are of real-world data. We investigate this by assessing the effectiveness of generating synthetic data through LLM and using it as a benchmark for various NLP tasks. Our experiments across six datasets, and three different tasks, show that while synthetic data can effectively capture performance of various methods for simpler tasks, such as intent classification, it falls short for more complex tasks like named entity recognition. Additionally, we propose a new metric called the bias factor, which evaluates the biases introduced when the same LLM is used to both generate benchmarking data and to perform the tasks. We find that smaller LLMs exhibit biases towards their own generated data, whereas larger models do not. Overall, our findings suggest that the effectiveness of synthetic data as a benchmark varies depending on the task, and that practitioners should rely on data generated from multiple larger models whenever possible.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已在零样本和少样本学习环境中实现了一系列应用，包括生成用于训练和测试的合成数据集。但是，要可靠地使用这些合成数据集，必须了解它们对现实世界数据的代表性。我们通过评估通过 LLM 生成合成数据的有效性并将其用作各种 NLP 任务的基准来研究这一点。我们在六个数据集和三个不同任务上进行的实验表明，虽然合成数据可以有效地捕捉各种方法在意图分类等简单任务中的表现，但它在命名实体识别等更复杂的任务中却有所欠缺。此外，我们提出了一个称为偏差因子的新指标，它评估当使用同一个 LLM 生成基准数据和执行任务时引入的偏差。我们发现较小的 LLM 对自己生成的数据表现出偏差，而较大的模型则没有。总体而言，我们的研究结果表明，合成数据作为基准的有效性因任务而异，并且从业者应尽可能依赖从多个较大模型生成的数据。</li>
</ul>

<h3>Title: Sampling Latent Material-Property Information From LLM-Derived Embedding Representations</h3>
<ul>
<li><strong>Authors: </strong>Luke P. J. Gilligan, Matteo Cobelli, Hasan M. Sayeed, Taylor D. Sparks, Stefano Sanvito</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.11971">https://arxiv.org/abs/2409.11971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.11971">https://arxiv.org/pdf/2409.11971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.11971]] Sampling Latent Material-Property Information From LLM-Derived Embedding Representations(https://arxiv.org/abs/2409.11971)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Vector embeddings derived from large language models (LLMs) show promise in capturing latent information from the literature. Interestingly, these can be integrated into material embeddings, potentially useful for data-driven predictions of materials properties. We investigate the extent to which LLM-derived vectors capture the desired information and their potential to provide insights into material properties without additional training. Our findings indicate that, although LLMs can be used to generate representations reflecting certain property information, extracting the embeddings requires identifying the optimal contextual clues and appropriate comparators. Despite this restriction, it appears that LLMs still have the potential to be useful in generating meaningful materials-science representations.</li>
<li><strong>摘要：</strong>从大型语言模型 (LLM) 派生的向量嵌入有望从文献中捕获潜在信息。有趣的是，这些可以集成到材料嵌入中，可能有助于数据驱动的材料特性预测。我们研究了 LLM 派生向量捕获所需信息的程度，以及它们在无需额外训练的情况下提供材料特性洞察的潜力。我们的研究结果表明，虽然 LLM 可用于生成反映某些属性信息的表示，但提取嵌入需要确定最佳上下文线索和适当的比较器。尽管存在这种限制，但 LLM 似乎仍有潜力在生成有意义的材料科学表示方面发挥作用。</li>
</ul>

<h3>Title: Using Large Language Models to Generate Clinical Trial Tables and Figures</h3>
<ul>
<li><strong>Authors: </strong>Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12046">https://arxiv.org/abs/2409.12046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12046">https://arxiv.org/pdf/2409.12046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12046]] Using Large Language Models to Generate Clinical Trial Tables and Figures(https://arxiv.org/abs/2409.12046)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Tables, figures, and listings (TFLs) are essential tools for summarizing clinical trial data. Creation of TFLs for reporting activities is often a time-consuming task encountered routinely during the execution of clinical trials. This study explored the use of large language models (LLMs) to automate the generation of TFLs through prompt engineering and few-shot transfer learning. Using public clinical trial data in ADaM format, our results demonstrated that LLMs can efficiently generate TFLs with prompt instructions, showcasing their potential in this domain. Furthermore, we developed a conservational agent named Clinical Trial TFL Generation Agent: An app that matches user queries to predefined prompts that produce customized programs to generate specific predefined TFLs.</li>
<li><strong>摘要：</strong>表格、图表和列表 (TFL) 是总结临床试验数据的重要工具。创建用于报告活动的 TFL 通常是临床试验执行过程中经常遇到的一项耗时任务。本研究探索了使用大型语言模型 (LLM) 通过快速工程和少量迁移学习自动生成 TFL。使用 ADaM 格式的公共临床试验数据，我们的结果表明 LLM 可以有效地生成带有快速指令的 TFL，展示了它们在该领域的潜力。此外，我们开发了一个名为临床试验 TFL 生成代理的保守代理：一款将用户查询与预定义提示相匹配的应用程序，这些提示会生成定制程序以生成特定的预定义 TFL。</li>
</ul>

<h3>Title: Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking</h3>
<ul>
<li><strong>Authors: </strong>Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12059">https://arxiv.org/abs/2409.12059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12059">https://arxiv.org/pdf/2409.12059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12059]] Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking(https://arxiv.org/abs/2409.12059)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE.</li>
<li><strong>摘要：</strong>大型语言模型可以合理地理解和生成人类的表达，但可能缺乏彻底的思考和推理机制。最近有几项研究增强了语言模型的思考能力，但其中大多数不是数据驱动或基于训练的。在本文中，我们受到自然界认知机制的启发，设计了一种名为 TaS 的新型模型架构，使其首先考虑想法，然后根据查询表达响应。我们设计了几个管道来注释或从提示响应样本中生成思想内容，然后在充当思考层的中间层添加语言头。我们通过思想增强的数据训练语言模型，并成功地让思考层自动生成合理的想法并最终输出更合理的响应。定性示例和定量结果都验证了 TaS 的有效性和性能。我们的代码可在 https://anonymous.4open.science/r/TadE 获得。</li>
</ul>

<h3>Title: Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval</h3>
<ul>
<li><strong>Authors: </strong>Warren Jouanneau, Marc Palyart, Emma Jouffroy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12097">https://arxiv.org/abs/2409.12097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12097">https://arxiv.org/pdf/2409.12097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12097]] Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval(https://arxiv.org/abs/2409.12097)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Finding the perfect match between a job proposal and a set of freelancers is not an easy task to perform at scale, especially in multiple languages. In this paper, we propose a novel neural retriever architecture that tackles this problem in a multilingual setting. Our method encodes project descriptions and freelancer profiles by leveraging pre-trained multilingual language models. The latter are used as backbone for a custom transformer architecture that aims to keep the structure of the profiles and project. This model is trained with a contrastive loss on historical data. Thanks to several experiments, we show that this approach effectively captures skill matching similarity and facilitates efficient matching, outperforming traditional methods.</li>
<li><strong>摘要：</strong>找到工作提案和一组自由职业者之间的完美匹配并非一项容易大规模完成的任务，尤其是在多种语言的情况下。在本文中，我们提出了一种新颖的神经检索器架构，可在多语言环境中解决此问题。我们的方法利用预先训练的多语言语言模型对项目描述和自由职业者资料进行编码。后者用作自定义转换器架构的主干，旨在保留资料和项目的结构。该模型使用历史数据的对比损失进行训练。通过多次实验，我们表明这种方法可以有效捕捉技能匹配相似性并促进高效匹配，优于传统方法。</li>
</ul>

<h3>Title: Measuring Human and AI Values based on Generative Psychometrics with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoran Ye, Yuhang Xie, Yuanyi Ren, Hanjun Fang, Xin Zhang, Guojie Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12106">https://arxiv.org/abs/2409.12106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12106">https://arxiv.org/pdf/2409.12106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12106]] Measuring Human and AI Values based on Generative Psychometrics with Large Language Models(https://arxiv.org/abs/2409.12106)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Human values and their measurement are long-standing interdisciplinary inquiry. Recent advances in AI have sparked renewed interest in this area, with large language models (LLMs) emerging as both tools and subjects of value measurement. This work introduces Generative Psychometrics for Values (GPV), an LLM-based, data-driven value measurement paradigm, theoretically grounded in text-revealed selective perceptions. We begin by fine-tuning an LLM for accurate perception-level value measurement and verifying the capability of LLMs to parse texts into perceptions, forming the core of the GPV pipeline. Applying GPV to human-authored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools. Then, extending GPV to LLM value measurement, we advance the current art with 1) a psychometric methodology that measures LLM values based on their scalable and free-form outputs, enabling context-specific measurement; 2) a comparative analysis of measurement paradigms, indicating response biases of prior methods; and 3) an attempt to bridge LLM values and their safety, revealing the predictive power of different value systems and the impacts of various values on LLM safety. Through interdisciplinary efforts, we aim to leverage AI for next-generation psychometrics and psychometrics for value-aligned AI.</li>
<li><strong>摘要：</strong>人类价值观及其衡量是长期存在的跨学科研究。人工智能的最新进展引发了人们对这一领域的新兴趣，大型语言模型 (LLM) 既是价值衡量的工具，也是价值衡量的主题。这项工作介绍了价值观生成心理测量 (GPV)，这是一种基于 LLM 的数据驱动价值衡量范式，理论上以文本揭示的选择性感知为基础。我们首先对 LLM 进行微调以进行准确的感知级价值衡量，并验证 LLM 将文本解析为感知的能力，形成 GPV 管道的核心。通过将 GPV 应用于人类撰写的博客，我们证明了它的稳定性、有效性以及相对于之前心理工具的优越性。然后，将 GPV 扩展到 LLM 价值衡量，我们通过 1) 一种心理测量方法推进当前技术，该方法基于可扩展和自由形式的输出来衡量 LLM 价值，从而实现特定于上下文的测量；2) 对测量范式的比较分析，表明先前方法的反应偏差； 3）尝试将 LLM 价值观与安全性联系起来，揭示不同价值体系的预测能力以及各种价值观对 LLM 安全性的影响。通过跨学科的努力，我们的目标是利用人工智能实现下一代心理测量，并利用心理测量实现与价值观一致的人工智能。</li>
</ul>

<h3>Title: Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12122">https://arxiv.org/abs/2409.12122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12122">https://arxiv.org/pdf/2409.12122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12122]] Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement(https://arxiv.org/abs/2409.12122)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance. Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems.</li>
<li><strong>摘要：</strong>在这份报告中，我们提出了一系列数学专用的大型语言模型：Qwen2.5-Math 和 Qwen2.5-Math-Instruct-1.5B/7B/72B。Qwen2.5 系列的核心创新在于将自我改进的理念融入到从预训练、后训练到推理的整个流程中：（1）在预训练阶段，利用 Qwen2-Math-Instruct 生成大规模高质量的数学数据。（2）在后训练阶段，我们通过从 Qwen2-Math-Instruct 进行大量采样来开发奖励模型 (RM)。然后将此 RM 应用于监督微调 (SFT) 中的数据迭代演变。有了更强大的 SFT 模型，就可以迭代地训练和更新 RM，进而指导下一轮 SFT 数据迭代。在最终的 SFT 模型上，我们引入终极 RM 进行强化学习，从而形成了 Qwen2.5-Math-Instruct。（3）此外，在推理阶段，RM 用于指导采样，优化模型性能。Qwen2.5-Math-Instruct 支持中英文，并具有高级数学推理能力，包括思维链 (CoT) 和工具集成推理 (TIR)。我们在 10 个中英文数学数据集上对我们的模型进行了评估，例如 GSM8K、MATH、GaoKao、AMC23 和 AIME24，涵盖了从小学水平到数学竞赛问题的一系列难度。</li>
</ul>

<h3>Title: Linguini: A benchmark for language-agnostic linguistic reasoning</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Sánchez, Belen Alastruey, Christophe Ropers, Pontus Stenetorp, Mikel Artetxe, Marta R. Costa-jussà</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12126">https://arxiv.org/abs/2409.12126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12126">https://arxiv.org/pdf/2409.12126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12126]] Linguini: A benchmark for language-agnostic linguistic reasoning(https://arxiv.org/abs/2409.12126)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We propose a new benchmark to measure a language model's linguistic reasoning skills without relying on pre-existing language-specific knowledge. The test covers 894 questions grouped in 160 problems across 75 (mostly) extremely low-resource languages, extracted from the International Linguistic Olympiad corpus. To attain high accuracy on this benchmark, models don't need previous knowledge of the tested language, as all the information needed to solve the linguistic puzzle is presented in the context. We find that, while all analyzed models rank below 25% accuracy, there is a significant gap between open and closed models, with the best-performing proprietary model at 24.05% and the best-performing open model at 8.84%.</li>
<li><strong>摘要：</strong>我们提出了一个新的基准来衡量语言模型的语言推理能力，而无需依赖预先存在的语言特定知识。该测试涵盖了 894 个问题，分为 160 个问题，涉及 75 种（大多数）资源极低的语言，这些问题摘自国际语言奥林匹克语料库。为了在这个基准上获得高精度，模型不需要预先了解测试语言，因为解决语言难题所需的所有信息都呈现在上下文中。我们发现，虽然所有分析的模型的准确率都低于 25%，但开放模型和封闭模型之间存在显著差距，表现最好的专有模型为 24.05%，而表现最好的开放模型为 8.84%。</li>
</ul>

<h3>Title: GRIN: GRadient-INformed MoE</h3>
<ul>
<li><strong>Authors: </strong>Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12136">https://arxiv.org/abs/2409.12136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12136">https://arxiv.org/pdf/2409.12136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12136]] GRIN: GRadient-INformed MoE(https://arxiv.org/abs/2409.12136)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16$\times$3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.</li>
<li><strong>摘要：</strong>由于通过专家路由进行稀疏计算，选择性地仅激活一小部分专家模块，因此混合专家 (MoE) 模型比密集模型扩展得更有效。然而，稀疏计算挑战了传统的训练实践，因为离散专家路由会阻碍标准反向传播，从而阻碍基于梯度的优化，而这两者都是深度学习的基石。为了更好地追求 MoE 的扩展能力，我们引入了 GRIN（梯度信息 MoE 训练），它结合了稀疏梯度估计用于专家路由，并配置模型并行性以避免令牌丢失。将 GRIN 应用于自回归语言建模，我们开发了一个前 2 名的 16$\times$3.8B MoE 模型。我们的模型只有 6.6B 个激活参数，但性能优于 7B 密集模型，并且与在相同数据上训练的 14B 密集模型的性能相当。在不同任务上进行的广泛评估表明，GRIN 具有显著提高 MoE 效能的潜力，在 MMLU 上达到 79.4，在 HellaSwag 上达到 83.7，在 HumanEval 上达到 74.4，在 MATH 上达到 58.9。</li>
</ul>

<h3>Title: MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12147">https://arxiv.org/abs/2409.12147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12147">https://arxiv.org/pdf/2409.12147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12147]] MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning(https://arxiv.org/abs/2409.12147)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models' (LLM) reasoning can be improved using test-time aggregation strategies, i.e., generating multiple samples and voting among generated samples. While these improve performance, they often reach a saturation point. Refinement offers an alternative by using LLM-generated feedback to improve solution quality. However, refinement introduces 3 key challenges: (1) Excessive refinement: Uniformly refining all instances can over-correct and reduce the overall performance. (2) Inability to localize and address errors: LLMs have a limited ability to self-correct and struggle to identify and correct their own mistakes. (3) Insufficient refinement: Deciding how many iterations of refinement are needed is non-trivial, and stopping too soon could leave errors unaddressed. To tackle these issues, we propose MAgICoRe, which avoids excessive refinement by categorizing problem difficulty as easy or hard, solving easy problems with coarse-grained aggregation and hard ones with fine-grained and iterative multi-agent refinement. To improve error localization, we incorporate external step-wise reward model (RM) scores. Moreover, to ensure effective refinement, we employ a multi-agent loop with three agents: Solver, Reviewer (which generates targeted feedback based on step-wise RM scores), and the Refiner (which incorporates feedback). To ensure sufficient refinement, we re-evaluate updated solutions, iteratively initiating further rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5 and show its effectiveness across 5 math datasets. Even one iteration of MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by 4.0% while using less than half the samples. Unlike iterative refinement with baselines, MAgICoRe continues to improve with more iterations. Finally, our ablations highlight the importance of MAgICoRe's RMs and multi-agent communication.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的推理可以使用测试时聚合策略来改进，即生成多个样本并在生成的样本中投票。虽然这些方法可以提高性能，但它们通常会达到饱和点。细化提供了一种替代方案，即使用 LLM 生成的反馈来提高解决方案的质量。然而，细化带来了 3 个关键挑战：(1) 过度细化：统一细化所有实例可能会过度纠正并降低整体性能。(2) 无法定位和解决错误：LLM 的自我纠正能力有限，难以识别和纠正自己的错误。(3) 细化不足：决定需要多少次细化迭代并非易事，过早停止可能会导致错误得不到解决。为了解决这些问题，我们提出了 MAgICoRe，它通过将问题难度分为易或难来避免过度细化，使用粗粒度聚合解决简单问题，使用细粒度和迭代多智能体细化解决难题。为了提高错误定位准确率，我们加入了外部的逐步奖励模型 (RM) 分数。此外，为了确保有效的改进，我们采用了一个包含三个代理的多智能体循环：求解器、审阅者（根据逐步 RM 分数生成有针对性的反馈）和精炼器（包含反馈）。为了确保充分改进，我们重新评估更新的解决方案，迭代地启动进一步的改进。我们在 Llama-3-8B 和 GPT-3.5 上评估了 MAgICoRe，并在 5 个数学数据集上展示了其有效性。MAgICoRe 的一次迭代就能在使用不到一半样本的情况下，将自一致性提高 3.4%，将 Best-of-k 提高 3.2%，将自优化提高 4.0%。与使用基线的迭代改进不同，MAgICoRe 会随着更多迭代而不断改进。最后，我们的消融强调了 MAgICoRe 的 RM 和多智能体通信的重要性。</li>
</ul>

<h3>Title: Finetuning Language Models to Emit Linguistic Expressions of Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Arslan Chaudhry, Sridhar Thiagarajan, Dilan Gorur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12180">https://arxiv.org/abs/2409.12180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12180">https://arxiv.org/pdf/2409.12180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12180]] Finetuning Language Models to Emit Linguistic Expressions of Uncertainty(https://arxiv.org/abs/2409.12180)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly employed in information-seeking and decision-making tasks. Despite their broad utility, LLMs tend to generate information that conflicts with real-world facts, and their persuasive style can make these inaccuracies appear confident and convincing. As a result, end-users struggle to consistently align the confidence expressed by LLMs with the accuracy of their predictions, often leading to either blind trust in all outputs or a complete disregard for their reliability. In this work, we explore supervised finetuning on uncertainty-augmented predictions as a method to develop models that produce linguistic expressions of uncertainty. Specifically, we measure the calibration of pre-trained models and then fine-tune language models to generate calibrated linguistic expressions of uncertainty. Through experiments on various question-answering datasets, we demonstrate that LLMs are well-calibrated in assessing their predictions, and supervised finetuning based on the model's own confidence leads to well-calibrated expressions of uncertainty, particularly for single-claim answers.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在信息搜索和决策任务中的应用越来越广泛。尽管 LLM 具有广泛的用途，但它们往往会生成与现实世界事实相冲突的信息，而且它们的说服风格可以使这些不准确之处显得自信和令人信服。因此，最终用户很难始终将 LLM 表达的信心与其预测的准确性保持一致，这通常会导致盲目信任所有输出或完全无视其可靠性。在这项工作中，我们探索了对不确定性增强预测的监督微调，以此作为一种开发产生不确定性语言表达的模型的方法。具体来说，我们测量预训练模型的校准，然后微调语言模型以生成校准的不确定性语言表达。通过对各种问答数据集进行实验，我们证明了 LLM 在评估其预测时具有良好的校准性，并且基于模型自身信心的监督微调可以产生良好校准的不确定性表达，特别是对于单一声明答案。</li>
</ul>

<h3>Title: A Controlled Study on Long Context Extension and Generalization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yi Lu, Jing Nathan Yan, Songlin Yang, Justin T. Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, Alexander M. Rush</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12181">https://arxiv.org/abs/2409.12181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12181">https://arxiv.org/pdf/2409.12181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12181]] A Controlled Study on Long Context Extension and Generalization in LLMs(https://arxiv.org/abs/2409.12181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Broad textual understanding and in-context learning require language models that utilize full document contexts. Due to the implementation challenges associated with directly training long-context models, many methods have been proposed for extending models to handle long contexts. However, owing to differences in data and model classes, it has been challenging to compare these approaches, leading to uncertainty as to how to evaluate long-context performance and whether it differs from standard evaluation. We implement a controlled protocol for extension methods with a standardized evaluation, utilizing consistent base models and extension data. Our study yields several insights into long-context behavior. First, we reaffirm the critical role of perplexity as a general-purpose performance indicator even in longer-context tasks. Second, we find that current approximate attention methods systematically underperform across long-context tasks. Finally, we confirm that exact fine-tuning based methods are generally effective within the range of their extension, whereas extrapolation remains challenging. All codebases, models, and checkpoints will be made available open-source, promoting transparency and facilitating further research in this critical area of AI development.</li>
<li><strong>摘要：</strong>广泛的文本理解和上下文学习需要利用完整文档上下文的语言模型。由于直接训练长上下文模型存在实施挑战，因此提出了许多方法来扩展模型以处理长上下文。然而，由于数据和模型类别的差异，比较这些方法具有挑战性，导致不确定如何评估长上下文性能以及它是否与标准评估不同。我们实施了一种受控的扩展方法协议，使用标准化评估，利用一致的基础模型和扩展数据。我们的研究对长上下文行为产生了一些见解。首先，我们重申了困惑度作为通用性能指标的关键作用，即使在较长的上下文任务中也是如此。其次，我们发现当前的近似注意方法在长上下文任务中系统性地表现不佳。最后，我们确认基于精确微调的方法在其扩展范围内通常是有效的，而外推仍然具有挑战性。所有代码库、模型和检查点都将开源，以促进透明度并促进人工智能开发这一关键领域的进一步研究。</li>
</ul>

<h3>Title: To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12183">https://arxiv.org/abs/2409.12183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12183">https://arxiv.org/pdf/2409.12183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12183]] To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning(https://arxiv.org/abs/2409.12183)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.</li>
<li><strong>摘要：</strong>通过提示的思路链 (CoT) 是从大型语言模型 (LLM) 中引出推理能力的事实上的方法。但是对于哪些类型的任务来说，这种额外的“思考”真的有用呢？为了分析这一点，我们对 100 多篇使用 CoT 的论文进行了定量荟萃分析，并对 14 个模型中的 20 个数据集进行了我们自己的评估。我们的结果表明，CoT 主要在涉及数学或逻辑的任务上提供了强大的性能优势，而在其他类型的任务上的优势要小得多。在 MMLU 上，直接生成不使用 CoT 的答案会导致与 CoT 几乎相同的准确度，除非问题或模型的响应包含等号，表示符号运算和推理。根据这一发现，我们通过分离规划和执行并与工具增强的 LLM 进行比较来分析 CoT 在这些问题上的行为。CoT 的大部分收益来自改进符号执行，但与使用符号求解器相比，它的表现不佳。我们的结果表明，可以有选择地应用 CoT，在保持性能的同时节省推理成本。此外，他们建议需要超越基于提示的 CoT 转向新的范式，以便更好地利用整个 LLM 应用中中间计算。</li>
</ul>

<h3>Title: Gender Representation and Bias in Indian Civil Service Mock Interviews</h3>
<ul>
<li><strong>Authors: </strong>Somonnoy Banerjee, Sujan Dutta, Soumyajit Datta, Ashiqur R. KhudaBukhsh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12194">https://arxiv.org/abs/2409.12194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12194">https://arxiv.org/pdf/2409.12194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12194]] Gender Representation and Bias in Indian Civil Service Mock Interviews(https://arxiv.org/abs/2409.12194)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper makes three key contributions. First, via a substantial corpus of 51,278 interview questions sourced from 888 YouTube videos of mock interviews of Indian civil service candidates, we demonstrate stark gender bias in the broad nature of questions asked to male and female candidates. Second, our experiments with large language models show a strong presence of gender bias in explanations provided by the LLMs on the gender inference task. Finally, we present a novel dataset of 51,278 interview questions that can inform future social science studies.</li>
<li><strong>摘要：</strong>本文做出了三项重要贡献。首先，通过从 888 个印度公务员候选人模拟面试 YouTube 视频中收集的 51,278 个面试问题，我们证明了在针对男性和女性候选人的问题中存在明显的性别偏见。其次，我们对大型语言模型的实验表明，法学硕士在性别推断任务中提供的解释存在强烈的性别偏见。最后，我们提出了一个包含 51,278 个面试问题的新数据集，可以为未来的社会科学研究提供参考。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
