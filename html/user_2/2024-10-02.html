<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-02</h1>
<h3>Title: Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach</h3>
<ul>
<li><strong>Authors: </strong>Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00025">https://arxiv.org/abs/2410.00025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00025">https://arxiv.org/pdf/2410.00025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00025]] Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach(https://arxiv.org/abs/2410.00025)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent progress in Spoken Language Modeling has demonstrated the feasibility of learning language directly from speech. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems tend to trail behind text-based language models in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, which in turn improve downstream language modeling performance.</li>
<li><strong>摘要：</strong>口语语言建模的最新进展证明了直接从语音中学习语言的可行性。通过在文本级别运行的管道生成语音通常会丢失细微差别、语调和非语言发声。直接从语音建模开辟了通往更自然、更富有表现力的系统的道路。另一方面，纯语音系统在语义能力方面往往落后于基于文本的语言模型。我们表明，在音素分类上微调语音表示模型会产生更多上下文不变的表示，从而提高下游语言建模性能。</li>
</ul>

<h3>Title: Semantic-Driven Topic Modeling Using Transformer-Based Embeddings and Clustering Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Melkamu Abay Mersha, Mesay Gemeda yigezu, Jugal Kalita</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00134">https://arxiv.org/abs/2410.00134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00134">https://arxiv.org/pdf/2410.00134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00134]] Semantic-Driven Topic Modeling Using Transformer-Based Embeddings and Clustering Algorithms(https://arxiv.org/abs/2410.00134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Topic modeling is a powerful technique to discover hidden topics and patterns within a collection of documents without prior knowledge. Traditional topic modeling and clustering-based techniques encounter challenges in capturing contextual semantic information. This study introduces an innovative end-to-end semantic-driven topic modeling technique for the topic extraction process, utilizing advanced word and document embeddings combined with a powerful clustering algorithm. This semantic-driven approach represents a significant advancement in topic modeling methodologies. It leverages contextual semantic information to extract coherent and meaningful topics. Specifically, our model generates document embeddings using pre-trained transformer-based language models, reduces the dimensions of the embeddings, clusters the embeddings based on semantic similarity, and generates coherent topics for each cluster. Compared to ChatGPT and traditional topic modeling algorithms, our model provides more coherent and meaningful topics.</li>
<li><strong>摘要：</strong>主题建模是一种强大的技术，可以在没有先验知识的情况下发现文档集合中的隐藏主题和模式。传统的主题建模和基于聚类的技术在捕获上下文语义信息方面遇到了挑战。这项研究为主题提取过程引入了一种创新的端到端语义驱动主题建模技术，利用高级单词和文档嵌入与强大的聚类算法相结合。这种语义驱动的方法代表了主题建模方法的重大进步。它利用上下文语义信息来提取连贯且有意义的主题。具体来说，我们的模型使用预先训练的基于 Transformer 的语言模型生成文档嵌入，降低嵌入的维度，根据语义相似性对嵌入进行聚类，并为每个聚类生成连贯的主题。与 ChatGPT 和传统主题建模算法相比，我们的模型提供了更连贯、更有意义的主题。</li>
</ul>

<h3>Title: Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!</h3>
<ul>
<li><strong>Authors: </strong>Divya Patel, Pathik Patel, Ankush Chander, Sourish Dasgupta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00149">https://arxiv.org/abs/2410.00149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00149">https://arxiv.org/pdf/2410.00149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00149]] Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!(https://arxiv.org/abs/2410.00149)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have succeeded considerably in In-Context-Learning (ICL) based summarization. However, saliency is subject to the users' specific preference histories. Hence, we need reliable In-Context Personalization Learning (ICPL) capabilities within such LLMs. For any arbitrary LLM to exhibit ICPL, it needs to have the ability to discern contrast in user profiles. A recent study proposed a measure for degree-of-personalization called EGISES for the first time. EGISES measures a model's responsiveness to user profile differences. However, it cannot test if a model utilizes all three types of cues provided in ICPL prompts: (i) example summaries, (ii) user's reading histories, and (iii) contrast in user profiles. To address this, we propose the iCOPERNICUS framework, a novel In-COntext PERsonalization learNIng sCrUtiny of Summarization capability in LLMs that uses EGISES as a comparative measure. As a case-study, we evaluate 17 state-of-the-art LLMs based on their reported ICL performances and observe that 15 models' ICPL degrades (min: 1.6%; max: 3.6%) when probed with richer prompts, thereby showing lack of true ICPL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在基于上下文学习 (ICL) 的摘要方面取得了巨大成功。然而，显著性取决于用户的特定偏好历史。因此，我们需要在此类 LLM 中实现可靠的上下文个性化学习 (ICPL) 功能。任何 LLM 要表现出 ICPL，都需要能够辨别用户个人资料中的对比。最近的一项研究首次提出了一种称为 EGISES 的个性化程度测量方法。EGISES 测量模型对用户个人资料差异的响应能力。但是，它无法测试模型是否利用了 ICPL 提示中提供的所有三种类型的提示：(i) 示例摘要、(ii) 用户的阅读历史和 (iii) 用户个人资料中的对比。为了解决这个问题，我们提出了 iCOPERNICUS 框架，这是一种新颖的上下文个性化学习 LLM 中的摘要能力审查方法，使用 EGISES 作为比较衡量标准。作为案例研究，我们根据报告的 ICL 性能对 17 个最先进的 LLM 进行了评估，并观察到 ​​15 个模型的 ICPL 在用更丰富的提示探测时会下降（最小值：1.6%；最大值：3.6%），从而表明缺乏真正的 ICPL。</li>
</ul>

<h3>Title: Scheherazade: Evaluating Chain-of-Thought Math Reasoning in LLMs with Chain-of-Problems</h3>
<ul>
<li><strong>Authors: </strong>Stephen Miner, Yoshiki Takashima, Simeng Han, Ferhat Erata, Timos Antonopoulos, Ruzica Piskac, Scott J Shapiro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00151">https://arxiv.org/abs/2410.00151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00151">https://arxiv.org/pdf/2410.00151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00151]] Scheherazade: Evaluating Chain-of-Thought Math Reasoning in LLMs with Chain-of-Problems(https://arxiv.org/abs/2410.00151)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Benchmarks are critical for measuring progress of math reasoning abilities of Large Language Models (LLMs). However, existing widely-used benchmarks such as GSM8K have been rendered less useful as multiple cutting-edge LLMs achieve over 94% accuracy. While harder benchmarks have been proposed, their creation is often manual and expensive. We present Scheherazade, an automated approach for producing challenging mathematical reasoning benchmarks by logically chaining mathematical reasoning problems. We propose two different chaining methods, forward chaining and backward chaining, which require reasoning forward and backward through the chain respectively. We apply Scheherazade on GSM8K to create GSM8K-Scheherazade and evaluate 3 frontier LLMs and OpenAI's o1-preview on it. We show that while frontier models' performance declines precipitously at only a few questions chained, a preliminary evaluation suggests o1-preview performance persists up to 5 questions chained backwards. In addition, while all other models perform worse when problems are chained backwards, o1-preview performs better on backward-chained benchmarks. We will release the dataset and code publicly.</li>
<li><strong>摘要：</strong>基准对于衡量大型语言模型 (LLM) 的数学推理能力的进步至关重要。然而，由于多个尖端 LLM 的准确率超过 94%，现有的广泛使用的基准（如 GSM8K）已变得不那么有用。虽然已经提出了更难的基准，但它们的创建通常是手动的且成本高昂。我们提出了 Scheherazade，这是一种通过逻辑链接数学推理问题来生成具有挑战性的数学推理基准的自动化方法。我们提出了两种不同的链接方法，即前向链接和后向链接，它们分别需要通过链进行正向和反向推理。我们在 GSM8K 上应用 Scheherazade 来创建 GSM8K-Scheherazade，并在其上评估 3 个前沿 LLM 和 OpenAI 的 o1 预览。我们表明，虽然前沿模型的性能在仅链接几个问题时急剧下降，但初步评估表明 o1 预览性能可以持续到反向链接的 5 个问题。此外，尽管所有其他模型在问题反向链接时表现较差，但 o1-preview 在反向链接基准测试中表现更好。我们将公开发布数据集和代码。</li>
</ul>

<h3>Title: Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution</h3>
<ul>
<li><strong>Authors: </strong>Haiyan Zhao, Heng Zhao, Bo Shen, Ali Payani, Fan Yang, Mengnan Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00153">https://arxiv.org/abs/2410.00153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00153">https://arxiv.org/pdf/2410.00153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00153]] Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution(https://arxiv.org/abs/2410.00153)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Probing learned concepts in large language models (LLMs) is crucial for understanding how semantic knowledge is encoded internally. Training linear classifiers on probing tasks is a principle approach to denote the vector of a certain concept in the representation space. However, the single vector identified for a concept varies with both data and training, making it less robust and weakening its effectiveness in real-world applications. To address this challenge, we propose an approach to approximate the subspace representing a specific concept. Built on linear probing classifiers, we extend the concept vectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's effectiveness through measuring its faithfulness and plausibility across multiple LLMs with different sizes and architectures. Additionally, we use representation intervention tasks to showcase its efficacy in real-world applications such as emotion steering. Experimental results indicate that GCS concept vectors have the potential to balance steering performance and maintaining the fluency in natural language generation tasks.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 中探索学习到的概念对于理解语义知识的内部编码方式至关重要。在探索任务上训练线性分类器是一种表示表示空间中某个概念向量的主要方法。然而，为概念识别的单个向量会随着数据和训练而变化，使其稳定性降低并削弱其在实际应用中的有效性。为了应对这一挑战，我们提出了一种近似表示特定概念的子空间的方法。基于线性探索分类器，我们将概念向量扩展为高斯概念子空间 (GCS)。我们通过测量 GCS 在具有不同大小和架构的多个 LLM 中的忠实度和合理性来证明其有效性。此外，我们使用表示干预任务来展示其在情绪引导等实际应用中的有效性。实验结果表明，GCS 概念向量有可能在自然语言生成任务中平衡引导性能并保持流畅性。</li>
</ul>

<h3>Title: KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head</h3>
<ul>
<li><strong>Authors: </strong>Isaac Rehg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00161">https://arxiv.org/abs/2410.00161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00161">https://arxiv.org/pdf/2410.00161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00161]] KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head(https://arxiv.org/abs/2410.00161)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Context lengths of Large Language Models (LLMs) have exploded in recent years, with 128k-token context becoming a standard and million-token context becoming a reality. Efficiently supporting long-context inference remains challenging as the memory that must be allocated in key-value (KV) cache for a generation scales with its context length, limiting the number of long-context requests that can be served concurrently under a given memory budget. KV cache compression can mitigate this issue by removing under-utilized KVs from each attention head's cache and reducing its memory footprint. Higher theoretical compression rates can be achieved when the number of removed KVs varies across attention heads, but application of such a strategy within existing inference frameworks adds fragmentation and cannot realize the theoretical compression rates in physical memory. We introduce KV-Compress, a novel compression method that evicts contiguous KV blocks within a PagedAttention framework, reducing the memory footprint of the KV cache proportionally to this theoretical compression rate. Our method achieves state-of-the-art performance on LongBench for both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the total number of compressed KVs by 4x compared with prior methods. Evaluations on Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression rates up to 8x with negligible impact on performance, and up to 64x while retaining over 90% of full-cache performance for all but three of the suite's subsets. We benchmark an integration of our method with vLLM that increases total throughput by up to 5.18x by enabling larger decoding batches.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 的上下文长度呈爆炸式增长，128k 标记上下文成为标准，百万标记上下文成为现实。有效支持长上下文推理仍然具有挑战性，因为必须为一代分配的键值 (KV) 缓存中的内存会随着上下文长度而扩展，从而限制了在给定内存预算下可以同时提供的长上下文请求的数量。KV 缓存压缩可以通过从每个注意力头的缓存中删除未充分利用的 KV 并减少其内存占用来缓解此问题。当删除的 KV 数量因注意力头而异时，可以实现更高的理论压缩率，但在现有推理框架中应用这种策略会增加碎片，并且无法在物理内存中实现理论压缩率。我们引入了 KV-Compress，这是一种新颖的压缩方法，它可以在 PagedAttention 框架内逐出连续的 KV 块，从而按与理论压缩率成比例地减少 KV 缓存的内存占用。我们的方法在 LongBench 上实现了 Mistral-7B-Instruct-v0.2 和 Llama-3.1-8B-Instruct 的最佳性能，同时与之前的方法相比，压缩的 KV 总数降低了 4 倍。在 Llama-3.1-8B-Instruct 和 Llama-3.1-70B-Instruct-FP8 上的评估实现了高达 8 倍的压缩率，对性能的影响微乎其微，并且对于除套件的三个子集之外的所有子集，压缩率高达 64 倍，同时保留了 90% 以上的全缓存性能。我们对我们的方法与 vLLM 的集成进行了基准测试，通过启用更大的解码批次，将总吞吐量提高了 5.18 倍。</li>
</ul>

<h3>Title: Adapting LLMs for the Medical Domain in Portuguese: A Study on Fine-Tuning and Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Pedro Henrique Paiola, Gabriel Lino Garcia, João Renato Ribeiro Manesco, Mateus Roder, Douglas Rodrigues, João Paulo Papa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00163">https://arxiv.org/abs/2410.00163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00163">https://arxiv.org/pdf/2410.00163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00163]] Adapting LLMs for the Medical Domain in Portuguese: A Study on Fine-Tuning and Model Evaluation(https://arxiv.org/abs/2410.00163)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>This study evaluates the performance of large language models (LLMs) as medical agents in Portuguese, aiming to develop a reliable and relevant virtual assistant for healthcare professionals. The HealthCareMagic-100k-en and MedQuAD datasets, translated from English using GPT-3.5, were used to fine-tune the ChatBode-7B model using the PEFT-QLoRA method. The InternLM2 model, with initial training on medical data, presented the best overall performance, with high precision and adequacy in metrics such as accuracy, completeness and safety. However, DrBode models, derived from ChatBode, exhibited a phenomenon of catastrophic forgetting of acquired medical knowledge. Despite this, these models performed frequently or even better in aspects such as grammaticality and coherence. A significant challenge was low inter-rater agreement, highlighting the need for more robust assessment protocols. This work paves the way for future research, such as evaluating multilingual models specific to the medical field, improving the quality of training data, and developing more consistent evaluation methodologies for the medical field.</li>
<li><strong>摘要：</strong>本研究评估了大型语言模型 (LLM) 作为葡萄牙语医疗代理的性能，旨在为医疗专业人士开发可靠且相关的虚拟助手。使用 GPT-3.5 从英语翻译而来的 HealthCareMagic-100k-en 和 MedQuAD 数据集用于使用 PEFT-QLoRA 方法对 ChatBode-7B 模型进行微调。最初在医疗数据上进行训练的 InternLM2 模型表现出最佳的整体性能，在准确性、完整性和安全性等指标上具有较高的精确度和充分性。然而，源自 ChatBode 的 DrBode 模型表现出对获得的医学知识的灾难性遗忘现象。尽管如此，这些模型在语法和连贯性等方面的表现往往甚至更好。一个重大挑战是评分者间一致性低，凸显了对更强大的评估协议的需求。这项工作为未来的研究铺平了道路，例如评估特定于医学领域的多语言模型、提高训练数据的质量以及为医学领域开发更一致的评估方法。</li>
</ul>

<h3>Title: SSR: Alignment-Aware Modality Connector for Speech Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weiting Tan, Hirofumi Inaguma, Ning Dong, Paden Tomasello, Xutai Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00168">https://arxiv.org/abs/2410.00168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00168">https://arxiv.org/pdf/2410.00168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00168]] SSR: Alignment-Aware Modality Connector for Speech Language Models(https://arxiv.org/abs/2410.00168)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fusing speech into pre-trained language model (SpeechLM) usually suffers from inefficient encoding of long-form speech and catastrophic forgetting of pre-trained text modality. We propose SSR-Connector (Segmented Speech Representation Connector) for better modality fusion. Leveraging speech-text alignments, our approach segments and compresses speech features to match the granularity of text embeddings. Additionally, we introduce a two-stage training pipeline that includes the distillation and fine-tuning phases to mitigate catastrophic forgetting. SSR-Connector outperforms existing mechanism for speech-text modality fusion, consistently achieving better speech understanding (e.g., +10 accuracy on StoryCloze and +20 on Speech-MMLU) while preserving pre-trained text ability.</li>
<li><strong>摘要：</strong>将语音融合到预训练语言模型 (SpeechLM) 中通常会遭遇长格式语音编码效率低下和预训练文本模态灾难性遗忘的问题。我们提出了 SSR-Connector（分段语音表示连接器）以实现更好的模态融合。利用语音文本对齐，我们的方法可以分割和压缩语音特征以匹配文本嵌入的粒度。此外，我们引入了一个两阶段训练流程，其中包括提炼和微调阶段，以减轻灾难性遗忘。SSR-Connector 优于现有的语音文本模态融合机制，在保留预训练文本能力的同时，始终实现更好的语音理解（例如，StoryCloze 上的准确率 +10，Speech-MMLU 上的准确率 +20）。</li>
</ul>

<h3>Title: Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse</h3>
<ul>
<li><strong>Authors: </strong>Rongchen Guo, Isar Nejadgholi, Hillary Dawkins, Kathleen C. Fraser, Svetlana Kiritchenko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00175">https://arxiv.org/abs/2410.00175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00175">https://arxiv.org/pdf/2410.00175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00175]] Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse(https://arxiv.org/abs/2410.00175)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work provides an explanatory view of how LLMs can apply moral reasoning to both criticize and defend sexist language. We assessed eight large language models, all of which demonstrated the capability to provide explanations grounded in varying moral perspectives for both critiquing and endorsing views that reflect sexist assumptions. With both human and automatic evaluation, we show that all eight models produce comprehensible and contextually relevant text, which is helpful in understanding diverse views on how sexism is perceived. Also, through analysis of moral foundations cited by LLMs in their arguments, we uncover the diverse ideological perspectives in models' outputs, with some models aligning more with progressive or conservative views on gender roles and sexism. Based on our observations, we caution against the potential misuse of LLMs to justify sexist language. We also highlight that LLMs can serve as tools for understanding the roots of sexist beliefs and designing well-informed interventions. Given this dual capacity, it is crucial to monitor LLMs and design safety mechanisms for their use in applications that involve sensitive societal topics, such as sexism.</li>
<li><strong>摘要：</strong>这项研究提供了一个解释性的观点，即法学硕士如何运用道德推理来批评和捍卫性别歧视语言。我们评估了八个大型语言模型，所有这些模型都展示了基于不同道德观点的解释能力，既可以批评也可以认可反映性别歧视假设的观点。通过人工和自动评估，我们发现所有八个模型都生成了易于理解且与上下文相关的文本，这有助于理解人们对性别歧视的不同看法。此外，通过分析法学硕士在其论点中引用的道德基础，我们发现模型输出中的不同意识形态观点，其中一些模型更符合进步或保守的性别角色和性别歧视观点。根据我们的观察，我们警告不要滥用法学硕士来为性别歧视语言辩护。我们还强调，法学硕士可以作为理解性别歧视信念根源和设计明智干预措施的工具。鉴于这种双重能力，监控 LLM 并设计安全机制以用于涉及敏感社会话题（例如性别歧视）的应用至关重要。</li>
</ul>

<h3>Title: Evaluating the fairness of task-adaptive pretraining on unlabeled test data before few-shot text classification</h3>
<ul>
<li><strong>Authors: </strong>Kush Dubey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00179">https://arxiv.org/abs/2410.00179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00179">https://arxiv.org/pdf/2410.00179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00179]] Evaluating the fairness of task-adaptive pretraining on unlabeled test data before few-shot text classification(https://arxiv.org/abs/2410.00179)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Few-shot learning benchmarks are critical for evaluating modern NLP techniques. It is possible, however, that benchmarks favor methods which easily make use of unlabeled text, because researchers can use unlabeled text from the test set to pretrain their models. Given the dearth of research on this potential problem, we run experiments to quantify the bias caused by pretraining on unlabeled test set text instead of on unlabeled, independently drawn text. Controlled few-shot and zero-shot experiments on 25 classification tasks and 3 language models -- BERT, GPT-2, and Mistral 7B -- do not find evidence of overoptimism. Furthermore, we demonstrate the importance of repeated subsampling when studying few-shot text classification, and recommend that few-shot learning benchmarks include multiple training folds. Code and data are available at this https URL.</li>
<li><strong>摘要：</strong>少量学习基准对于评估现代 NLP 技术至关重要。然而，基准可能偏向于容易利用未标记文本的方法，因为研究人员可以使用测试集中的未标记文本来预训练他们的模型。鉴于对这一潜在问题的研究不足，我们进行了实验来量化对未标记测试集文本而不是未标记的独立绘制文本进行预训练所造成的偏差。对 25 个分类任务和 3 个语言模型（BERT、GPT-2 和 Mistral 7B）进行的受控少量和零样本实验没有发现过度乐观的证据。此外，我们证明了在研究少量文本分类时重复子采样的重要性，并建议少量学习基准包括多个训练折叠。代码和数据可在此 https URL 上获得。</li>
</ul>

<h3>Title: Zero-Shot Classification of Crisis Tweets Using Instruction-Finetuned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Emma McDaniel, Samuel Scheele, Jeff Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00182">https://arxiv.org/abs/2410.00182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00182">https://arxiv.org/pdf/2410.00182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00182]] Zero-Shot Classification of Crisis Tweets Using Instruction-Finetuned Large Language Models(https://arxiv.org/abs/2410.00182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Social media posts are frequently identified as a valuable source of open-source intelligence for disaster response, and pre-LLM NLP techniques have been evaluated on datasets of crisis tweets. We assess three commercial large language models (OpenAI GPT-4o, Gemini 1.5-flash-001 and Anthropic Claude-3-5 Sonnet) capabilities in zero-shot classification of short social media posts. In one prompt, the models are asked to perform two classification tasks: 1) identify if the post is informative in a humanitarian context; and 2) rank and provide probabilities for the post in relation to 16 possible humanitarian classes. The posts being classified are from the consolidated crisis tweet dataset, CrisisBench. Results are evaluated using macro, weighted, and binary F1-scores. The informative classification task, generally performed better without extra information, while for the humanitarian label classification providing the event that occurred during which the tweet was mined, resulted in better performance. Further, we found that the models have significantly varying performance by dataset, which raises questions about dataset quality.</li>
<li><strong>摘要：</strong>社交媒体帖子经常被视为灾难响应的宝贵开源情报来源，LLM 之前的 NLP 技术已在危机推文数据集上进行了评估。我们评估了三种商业大型语言模型（OpenAI GPT-4o、Gemini 1.5-flash-001 和 Anthropic Claude-3-5 Sonnet）在短社交媒体帖子零样本分类中的能力。在一个提示中，要求模型执行两个分类任务：1）确定帖子在人道主义背景下是否具有信息性；2）对帖子进行排名并提供与 16 种可能的人道主义类别相关的概率。要分类的帖子来自合并的危机推文数据集 CrisisBench。使用宏、加权和二进制 F1 分数评估结果。信息分类任务通常在没有额外信息的情况下表现更好，而对于提供挖掘推文期间发生的事件的人道主义标签分类，其性能更好。此外，我们发现模型的性能因数据集而异，这引发了对数据集质量的质疑。</li>
</ul>

<h3>Title: Do Vision-Language Models Really Understand Visual Language?</h3>
<ul>
<li><strong>Authors: </strong>Buse Giledereli, Yifan Hou, Yilei Tu, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00193">https://arxiv.org/abs/2410.00193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00193">https://arxiv.org/pdf/2410.00193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00193]] Do Vision-Language Models Really Understand Visual Language?(https://arxiv.org/abs/2410.00193)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an image. The symbolic nature of diagrams presents significant challenges for building models capable of understanding them. Yet, recent studies seem to suggest that Large Vision-Language Models (LVLMs) can even tackle complex reasoning tasks involving diagrams. In this paper, we investigate this phenomenon by developing a comprehensive test suite to evaluate the diagram comprehension capability of LVLMs. Our test suite uses a variety of questions focused on concept entities and their relationships over a set of synthetic as well as real diagrams across several domains to evaluate the recognition and reasoning abilities of models. Our evaluation of three LVLMs (GPT-4V, GPT-4o, and Gemini) shows that while these models can accurately identify and reason about entities, their ability to understand relationships is notably limited. Further testing reveals that the decent performance on diagram understanding largely stems from leveraging their background knowledge as shortcuts to identify and reason about the relational information. Thus, we conclude that LVLMs have a limited capability for genuine diagram understanding, and their impressive performance in diagram reasoning is an illusion emanating from other confounding factors, such as the background knowledge in the models.</li>
<li><strong>摘要：</strong>视觉语言是一种通过符号、形状和空间排列传达信息的交流系统。图表是视觉语言的典型示例，它以图像的形式描绘复杂的概念及其关系。图表的符号性质给构建能够理解它们的模型带来了重大挑战。然而，最近的研究似乎表明，大型视觉语言模型 (LVLM) 甚至可以处理涉及图表的复杂推理任务。在本文中，我们通过开发一个全面的测试套件来评估 LVLM 的图表理解能力，以研究这一现象。我们的测试套件使用各种问题，重点关注概念实体及其关系，这些问题涉及多个领域的一组合成和真实图表，以评估模型的识别和推理能力。我们对三个 LVLM（GPT-4V、GPT-4o 和 Gemini）的评估表明，虽然这些模型可以准确识别和推理实体，但它们理解关系的能力明显有限。进一步的测试表明，图表理解的良好表现很大程度上源于利用他们的背景知识作为识别和推理关系信息的捷径。因此，我们得出结论，LVLM 真正的图表理解能力有限，他们在图表推理方面的出色表现是一种假象，源于其他混杂因素，例如模型中的背景知识。</li>
</ul>

<h3>Title: Evaluating the performance of state-of-the-art esg domain-specific pre-trained large language models in text classification against existing models and traditional machine learning techniques</h3>
<ul>
<li><strong>Authors: </strong>Tin Yuet Chung, Majid Latifi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00207">https://arxiv.org/abs/2410.00207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00207">https://arxiv.org/pdf/2410.00207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00207]] Evaluating the performance of state-of-the-art esg domain-specific pre-trained large language models in text classification against existing models and traditional machine learning techniques(https://arxiv.org/abs/2410.00207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This research investigates the classification of Environmental, Social, and Governance (ESG) information within textual disclosures. The aim is to develop and evaluate binary classification models capable of accurately identifying and categorizing E, S and G-related content respectively. The motivation for this research stems from the growing importance of ESG considerations in investment decisions and corporate accountability. Accurate and efficient classification of ESG information is crucial for stakeholders to understand the impact of companies on sustainability and to make informed decisions. The research uses a quantitative approach involving data collection, data preprocessing, and the development of ESG-focused Large Language Models (LLMs) and traditional machine learning (Support Vector Machines, XGBoost) classifiers. Performance evaluation guides iterative refinement until satisfactory metrics are achieved. The research compares traditional machine learning techniques (Support Vector Machines, XGBoost), state-of-the-art language model (FinBERT-ESG) and fine-tuned LLMs like Llama 2, by employing standard Natural Language Processing performance metrics such as accuracy, precision, recall, F1-score. A novel fine-tuning method, Qlora, is applied to LLMs, resulting in significant performance improvements across all ESG domains. The research also develops domain-specific fine-tuned models, such as EnvLlama 2-Qlora, SocLlama 2-Qlora, and GovLlama 2-Qlora, which demonstrate impressive results in ESG text classification.</li>
<li><strong>摘要：</strong>本研究调查了文本披露中的环境、社会和治理 (ESG) 信息的分类。目的是开发和评估能够准确识别和分类 E、S 和 G 相关内容的二元分类模型。这项研究的动机源于 ESG 考虑在投资决策和企业责任中的重要性日益增加。准确有效地对 ESG 信息进行分类对于利益相关者了解公司对可持续性的影响并做出明智的决策至关重要。该研究采用定量方法，包括数据收集、数据预处理以及开发以 ESG 为中心的大型语言模型 (LLM) 和传统机器学习（支持向量机、XGBoost）分类器。绩效评估指导迭代改进，直到达到令人满意的指标。该研究通过采用标准自然语言处理性能指标（例如准确率、精确度、召回率、F1 分数）比较了传统机器学习技术（支持向量机、XGBoost）、最先进的语言模型（FinBERT-ESG）和 Llama 2 等微调 LLM。一种新颖的微调方法 Qlora 被应用于 LLM，从而显著提高了所有 ESG 领域的性能。该研究还开发了特定领域的微调模型，例如 EnvLlama 2-Qlora、SocLlama 2-Qlora 和 GovLlama 2-Qlora，这些模型在 ESG 文本分类中表现出色。</li>
</ul>

<h3>Title: T-KAER: Transparency-enhanced Knowledge-Augmented Entity Resolution Framework</h3>
<ul>
<li><strong>Authors: </strong>Lan Li, Liri Fang, Yiren Liu, Vetle I. Torvik, Bertram Ludaescher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00218">https://arxiv.org/abs/2410.00218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00218">https://arxiv.org/pdf/2410.00218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00218]] T-KAER: Transparency-enhanced Knowledge-Augmented Entity Resolution Framework(https://arxiv.org/abs/2410.00218)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Entity resolution (ER) is the process of determining whether two representations refer to the same real-world entity and plays a crucial role in data curation and data cleaning. Recent studies have introduced the KAER framework, aiming to improve pre-trained language models by augmenting external knowledge. However, identifying and documenting the external knowledge that is being augmented and understanding its contribution to the model's predictions have received little to no attention in the research community. This paper addresses this gap by introducing T-KAER, the Transparency-enhanced Knowledge-Augmented Entity Resolution framework. To enhance transparency, three Transparency-related Questions (T-Qs) have been proposed: T-Q(1): What is the experimental process for matching results based on data inputs? T-Q(2): Which semantic information does KAER augment in the raw data inputs? T-Q(3): Which semantic information of the augmented data inputs influences the predictions? To address the T-Qs, T-KAER is designed to improve transparency by documenting the entity resolution processes in log files. In experiments, a citation dataset is used to demonstrate the transparency components of T-KAER. This demonstration showcases how T-KAER facilitates error analysis from both quantitative and qualitative perspectives, providing evidence on "what" semantic information is augmented and "why" the augmented knowledge influences predictions differently.</li>
<li><strong>摘要：</strong>实体解析 (ER) 是确定两个表示是否指向同一个现实世界实体的过程，在数据管理和数据清理中起着至关重要的作用。最近的研究引入了 KAER 框架，旨在通过增强外部知识来改进预训练语言模型。然而，识别和记录正在增强的外部知识并了解其对模型预测的贡献在研究界几乎没有受到关注。本文通过介绍 T-KAER（透明度增强的知识增强实体解析框架）来解决这一差距。为了提高透明度，提出了三个与透明度相关的问题 (T-Q)：T-Q(1)：基于数据输入匹配结果的实验​​过程是什么？T-Q(2)：KAER 在原始数据输入中增强了哪些语义信息？T-Q(3)：增强数据输入的哪些语义信息会影响预测？为了解决 T-Q，T-KAER 旨在通过在日志文件中记录实体解析过程来提高透明度。在实验中，引文数据集用于演示 T-KAER 的透明度组件。此演示展示了 T-KAER 如何从定量和定性角度促进错误分析，提供有关“什么”语义信息被增强以及“为什么”增强的知识对预测产生不同影响的证据。</li>
</ul>

<h3>Title: A Methodology for Explainable Large Language Models with Integrated Gradients and Linguistic Analysis in Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Marina Ribeiro (1 and 2), Bárbara Malcorra (2), Natália B. Mota (2 and 3), Rodrigo Wilkens (4 and 5), Aline Villavicencio (5 and 6)Lilian C. Hubner (7), César Rennó-Costa (1) ((1) Bioinformatics Multidisciplinary Environment (BioME), Digital Metropolis Institute (IMD), Federal University of Rio Grande do Norte (UFRN), Natal (RN), Brazil, (2) Research Department at Mobile Brain, Mobile Brain, Rio de Janeiro (RJ), Brazil, (3) Institute of Psychiatry (IPUB), Federal University of Rio de Janeiro (UFRJ), Rio de Janeiro (RJ), Brazil, (4) Department of Computer Science, The University of Exeter, Exeter, UK, (5) Institute for Data Science and Artificial Intelligence at the University of Exeter, Exeter, UK, (6) Department of Computer Science, The University of Sheffield, Sheffield, UK, (7) School of Humanities, Pontifical Catholic University of Rio Grande do Sul (PUCRS), Porto Alegre (RS), Brazil)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00250">https://arxiv.org/abs/2410.00250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00250">https://arxiv.org/pdf/2410.00250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00250]] A Methodology for Explainable Large Language Models with Integrated Gradients and Linguistic Analysis in Text Classification(https://arxiv.org/abs/2410.00250)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Neurological disorders that affect speech production, such as Alzheimer's Disease (AD), significantly impact the lives of both patients and caregivers, whether through social, psycho-emotional effects or other aspects not yet fully understood. Recent advancements in Large Language Model (LLM) architectures have developed many tools to identify representative features of neurological disorders through spontaneous speech. However, LLMs typically lack interpretability, meaning they do not provide clear and specific reasons for their decisions. Therefore, there is a need for methods capable of identifying the representative features of neurological disorders in speech and explaining clearly why these features are relevant. This paper presents an explainable LLM method, named SLIME (Statistical and Linguistic Insights for Model Explanation), capable of identifying lexical components representative of AD and indicating which components are most important for the LLM's decision. In developing this method, we used an English-language dataset consisting of transcriptions from the Cookie Theft picture description task. The LLM Bidirectional Encoder Representations from Transformers (BERT) classified the textual descriptions as either AD or control groups. To identify representative lexical features and determine which are most relevant to the model's decision, we used a pipeline involving Integrated Gradients (IG), Linguistic Inquiry and Word Count (LIWC), and statistical analysis. Our method demonstrates that BERT leverages lexical components that reflect a reduction in social references in AD and identifies which further improve the LLM's accuracy. Thus, we provide an explainability tool that enhances confidence in applying LLMs to neurological clinical contexts, particularly in the study of neurodegeneration.</li>
<li><strong>摘要：</strong>影响言语产生的神经系统疾病，例如阿尔茨海默病 (AD)，会严重影响患者和护理人员的生活，无论是通过社会、心理情感影响还是其他尚未完全了解的方面。大型语言模型 (LLM) 架构的最新进展已经开发出许多工具，可以通过自发语音识别神经系统疾病的代表性特征。然而，LLM 通常缺乏可解释性，这意味着它们没有提供明确而具体的决策理由。因此，需要能够识别语音中神经系统疾病的代表性特征并清楚地解释这些特征为何相关的方法。本文介绍了一种可解释的 LLM 方法，名为 SLIME（用于模型解释的统计和语言学见解），能够识别代表 AD 的词汇成分并指出哪些成分对 LLM 的决策最重要。在开发此方法时，我们使用了一个由 Cookie Theft 图片描述任务的转录组成的英语数据集。LLM Transformers 的双向编码器表示 (BERT) 将文本描述分类为 AD 或对照组。为了识别具有代表性的词汇特征并确定哪些特征与模型的决策最相关，我们使用了涉及积分梯度 (IG)、语言查询和字数统计 (LIWC) 以及统计分析的流程。我们的方法表明，BERT 利用了反映 AD 中社交参考减少的词汇成分，并确定了哪些成分可以进一步提高 LLM 的准确性。因此，我们提供了一种可解释性工具，可增强将 LLM 应用于神经临床环境（特别是在神经退行性疾病研究中）的信心。</li>
</ul>

<h3>Title: DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data Mining</h3>
<ul>
<li><strong>Authors: </strong>Vinayak Arannil, Sourav Sanjukta Bhabesh, Neha Narwal, Sai Nikhil Thirandas, Darren Yow-Bang Wang, Graham Horwood, Alex Anto Chirayath, Gouri Pandeshwar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00260">https://arxiv.org/abs/2410.00260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00260">https://arxiv.org/pdf/2410.00260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00260]] DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data Mining(https://arxiv.org/abs/2410.00260)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable ability to generalize effectively across numerous industry domains while executing a range of tasks. Many of these competencies are obtained from the data utilized during the pre-training phase of the Language Models (LMs). However, these models exhibit limitations when tasked with performing in specialized or low-resource industry domains. More recent approaches use LLMs for generating domain-specific synthetic data but most often they lack in truthfulness and complexity. Alternatively, in cases where domain data is available like healthcare and finance most of the LMs are proprietary necessitating the need for a scalable method to curate real world industry specific pre-training data. In this work, we propose an automated and scalable framework - DoPAMine:Domain-specific Pre-training Adaptation from seed-guided data Mining, to mine domain specific training data from a large data corpus for domain adaptation of a LM. The framework leverages the parametric knowledge of a LLM to generate diverse and representative seed data tailored to a specific domain which is then used to mine real world data from a large data corpus like Common Crawl. We evaluated our framework's performance in the continual pre-training (CPT) setting by training two domain specific 7B parameter LMs in healthcare and finance with data mined via DoPAMine. Our experiments show that DoPAMine boosts the performance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and 5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and PubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings respectively on finance tasks from FiQA-SA, FPB and Headlines datasets when compared to the baseline.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在执行一系列任务时，已表现出在众多行业领域有效推广的卓越能力。这些能力中的许多都是从语言模型 (LM) 预训练阶段使用的数据中获得的。然而，当这些模型在专业或资源匮乏的行业领域中执行任务时，它们会表现出局限性。较新的方法使用 LLM 来生成特定领域的合成数据，但它们通常缺乏真实性和复杂性。或者，在医疗保健和金融等领域数据可用的情况下，大多数 LM 都是专有的，因此需要一种可扩展的方法来管理现实世界行业特定的预训练数据。在这项工作中，我们提出了一个自动化和可扩展的框架 - DoPAMine：从种子引导数据挖掘中进行领域特定预训练自适应，以从大型数据语料库中挖掘领域特定训练数据，以进行 LM 的领域自适应。该框架利用 LLM 的参数知识来生成针对特定领域定制的多样化和代表性种子数据，然后使用这些数据从大型数据语料库（如 Common Crawl）中挖掘真实世界数据。我们通过使用通过 DoPAMine 挖掘的数据训练医疗保健和金融领域两个特定于领域的 7B 参数 LM，评估了我们框架在持续预训练 (CPT) 设置中的表现。我们的实验表明，与基线相比，DoPAMine 在 MMLU、MedQA、MedMCQA 和 PubMedQA 数据集的医疗保健任务中分别在零样本和 5 样本设置中将预训练 LLM 的性能平均提高了 4.9% 和 5.1%，在 FiQA-SA、FPB 和 Headlines 数据集的金融任务中分别在零样本和 5 样本设置中将预训练 LLM 的性能提高了 2.9% 和 6.7%。</li>
</ul>

<h3>Title: Insight: A Multi-Modal Diagnostic Pipeline using LLMs for Ocular Surface Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Chun-Hsiao Yeh, Jiayun Wang, Andrew D. Graham, Andrea J. Liu, Bo Tan, Yubei Chen, Yi Ma, Meng C. Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00292">https://arxiv.org/abs/2410.00292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00292">https://arxiv.org/pdf/2410.00292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00292]] Insight: A Multi-Modal Diagnostic Pipeline using LLMs for Ocular Surface Disease Diagnosis(https://arxiv.org/abs/2410.00292)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Accurate diagnosis of ocular surface diseases is critical in optometry and ophthalmology, which hinge on integrating clinical data sources (e.g., meibography imaging and clinical metadata). Traditional human assessments lack precision in quantifying clinical observations, while current machine-based methods often treat diagnoses as multi-class classification problems, limiting the diagnoses to a predefined closed-set of curated answers without reasoning the clinical relevance of each variable to the diagnosis. To tackle these challenges, we introduce an innovative multi-modal diagnostic pipeline (MDPipe) by employing large language models (LLMs) for ocular surface disease diagnosis. We first employ a visual translator to interpret meibography images by converting them into quantifiable morphology data, facilitating their integration with clinical metadata and enabling the communication of nuanced medical insight to LLMs. To further advance this communication, we introduce a LLM-based summarizer to contextualize the insight from the combined morphology and clinical metadata, and generate clinical report summaries. Finally, we refine the LLMs' reasoning ability with domain-specific insight from real-life clinician diagnoses. Our evaluation across diverse ocular surface disease diagnosis benchmarks demonstrates that MDPipe outperforms existing standards, including GPT-4, and provides clinically sound rationales for diagnoses.</li>
<li><strong>摘要：</strong>在验光学和眼科学中，准确诊断眼表疾病至关重要，这依赖于整合临床数据源（例如，眼睑造影成像和临床元数据）。传统的人工评估在量化临床观察方面缺乏精确度，而当前的机器方法通常将诊断视为多类分类问题，将诊断限制为预定义的封闭式精选答案集，而不推理每个变量与诊断的临床相关性。为了应对这些挑战，我们引入了一种创新的多模态诊断管道 (MDPipe)，采用大型语言模型 (LLM) 进行眼表疾病诊断。我们首先使用视觉翻译器来解释眼睑造影图像，将它们转换为可量化的形态数据，促进它们与临床元数据的集成，并能够将细致入微的医学见解传达给 LLM。为了进一步推进这种交流，我们引入了一个基于 LLM 的摘要器，以将形态和临床元数据相结合的见解情境化，并生成临床报告摘要。最后，我们利用来自现实生活中临床医生诊断的特定领域洞察力来改进 LLM 的推理能力。我们对各种眼表疾病诊断基准的评估表明，MDPipe 的表现优于现有标准（包括 GPT-4），并为诊断提供了临床上合理的依据。</li>
</ul>

<h3>Title: Preserving Generalization of Language models in Few-shot Continual Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Quyen Tran, Nguyen Xuan Thanh, Nguyen Hoang Anh, Nam Le Hai, Trung Le, Linh Van Ngo, Thien Huu Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00334">https://arxiv.org/abs/2410.00334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00334">https://arxiv.org/pdf/2410.00334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00334]] Preserving Generalization of Language models in Few-shot Continual Relation Extraction(https://arxiv.org/abs/2410.00334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior knowledge from pre-trained backbones. In this work, we introduce a novel method that leverages often-discarded language model heads. By employing these components via a mutual information maximization strategy, our approach helps maintain prior knowledge from the pre-trained backbone and strategically aligns the primary classification head, thereby enhancing model performance. Furthermore, we explore the potential of Large Language Models (LLMs), renowned for their wealth of knowledge, in addressing FCRE challenges. Our comprehensive experimental results underscore the efficacy of the proposed method and offer valuable insights for future work.</li>
<li><strong>摘要：</strong>少量样本连续关系提取 (FCRE) 是一个新兴且充满活力的研究领域，其中模型可以顺序整合来自新关系的知识和有限的标记数据，同时避免灾难性遗忘并保留来自预训练主干的先验知识。在这项工作中，我们介绍了一种利用经常被丢弃的语言模型头的新方法。通过相互信息最大化策略采用这些组件，我们的方法有助于维护来自预训练主干的先验知识并战略性地调整主要分类头，从而提高模型性能。此外，我们探索了以知识丰富而闻名的大型语言模型 (LLM) 在解决 FCRE 挑战方面的潜力。我们全面的实验结果强调了所提方法的有效性，并为未来的工作提供了宝贵的见解。</li>
</ul>

<h3>Title: Hierarchical Organization Simulacra in the Investment Sector</h3>
<ul>
<li><strong>Authors: </strong>Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00354">https://arxiv.org/abs/2410.00354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00354">https://arxiv.org/pdf/2410.00354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00354]] Hierarchical Organization Simulacra in the Investment Sector(https://arxiv.org/abs/2410.00354)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, agent</a></li>
<li><strong>Abstract: </strong>This paper explores designing artificial organizations with professional behavior in investments using a multi-agent simulation. The method mimics hierarchical decision-making in investment firms, using news articles to inform decisions. A large-scale study analyzing over 115,000 news articles of 300 companies across 15 years compared this approach against professional traders' decisions. Results show that hierarchical simulations align closely with professional choices, both in frequency and profitability. However, the study also reveals biases in decision-making, where changes in prompt wording and perceived agent seniority significantly influence outcomes. This highlights both the potential and limitations of large language models in replicating professional financial decision-making.</li>
<li><strong>摘要：</strong>本文探讨了使用多智能体模拟设计具有专业投资行为的人工组织。该方法模仿投资公司的分层决策，使用新闻文章来指导决策。一项大规模研究分析了 15 年来 300 家公司的 115,000 多篇新闻文章，将这种方法与专业交易员的决策进行了比较。结果表明，分层模拟与专业选择密切相关，无论是频率还是盈利能力。然而，这项研究也揭示了决策中的偏见，其中提示措辞的变化和感知到的代理资历会显著影响结果。这凸显了大型语言模型在复制专业金融决策方面的潜力和局限性。</li>
</ul>

<h3>Title: Self-controller: Controlling LLMs with Multi-round Step-by-step Self-awareness</h3>
<ul>
<li><strong>Authors: </strong>Xiao Peng, Xufan Geng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00359">https://arxiv.org/abs/2410.00359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00359">https://arxiv.org/pdf/2410.00359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00359]] Self-controller: Controlling LLMs with Multi-round Step-by-step Self-awareness(https://arxiv.org/abs/2410.00359)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>The applications of large language models (LLMs) have been widely spread across all domains. However, the basic abilities such as the controllability of LLMs are still limited. To address this, we propose "Self-controller", a novel agentic framework bringing self-awareness into LLMs' reasoning logic. The core idea of this work is to maintain states based on the LLM's response, letting the LLM become self-aware of current status and think step by step in a multi-round chain-of-thought paradigm. Our experiment on the state of textual length has shown the controllability and effectiveness of the Self-controller. We further implement a binary search algorithm to accelerate the generation process based on the linearity and monotonicity of the textual length state. Another advantage of the Self-controller comes with DeepSeek's Context Caching technology, which significantly saves computational token consumption when a cluster of conversations shares the same prefix of context. Theoretically, we prove that in this scenario the extra time complexity is $O(c \log n)$. Results of the back-of-the-envelope estimation suggest that the token consumption of our method is no more than twice as much as that of the trivial single-round generation. Furthermore, our ablation study on word constraints demonstrates the Self-controller's consistent controllability across all foundation models.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的应用已经广泛普及到各个领域，但LLM的可控性等基本能力仍然有限。针对这一问题，我们提出了“自控制器”这一新型代理框架，将自我意识引入LLM的推理逻辑。该工作的核心思想是根据LLM的响应来维持状态，让LLM自我感知当前状态，并在多轮的思路链范式中逐步思考。我们在文本长度状态上的实验证明了自控制器的可控性和有效性。我们进一步实现了一个二分搜索算法，以加速基于文本长度状态的线性和单调性的生成过程。自控制器的另一个优势来自DeepSeek的上下文缓存技术，当一组对话共享相同的上下文前缀时，该技术可以显著节省计算令牌消耗。从理论上讲，我们证明在这种情况下额外的时间复杂度为 $O(c \log n)$。粗略估计的结果表明，我们的方法的 token 消耗不超过平凡单轮生成的两倍。此外，我们对字约束的消融研究表明 Self-controller 在所有基础模型中都具有一致的可控性。</li>
</ul>

<h3>Title: PclGPT: A Large Language Model for Patronizing and Condescending Language Detection</h3>
<ul>
<li><strong>Authors: </strong>Hongbo Wang, Mingda Li, Junyu Lu, Hebin Xia, Liang Yang, Bo Xu, Ruizhu Liu, Hongfei Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00361">https://arxiv.org/abs/2410.00361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00361">https://arxiv.org/pdf/2410.00361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00361]] PclGPT: A Large Language Model for Patronizing and Condescending Language Detection(https://arxiv.org/abs/2410.00361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Disclaimer: Samples in this paper may be harmful and cause discomfort! Patronizing and condescending language (PCL) is a form of speech directed at vulnerable groups. As an essential branch of toxic language, this type of language exacerbates conflicts and confrontations among Internet communities and detrimentally impacts disadvantaged groups. Traditional pre-trained language models (PLMs) perform poorly in detecting PCL due to its implicit toxicity traits like hypocrisy and false sympathy. With the rise of large language models (LLMs), we can harness their rich emotional semantics to establish a paradigm for exploring implicit toxicity. In this paper, we introduce PclGPT, a comprehensive LLM benchmark designed specifically for PCL. We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate implicit toxic detection. Group detection results and fine-grained detection from PclGPT and other models reveal significant variations in the degree of bias in PCL towards different vulnerable groups, necessitating increased societal attention to protect them.</li>
<li><strong>摘要：</strong>免责声明：本文中的样本可能有害并引起不适！居高临下的语言（PCL）是一种针对弱势群体的言论形式。作为毒性语言的一个重要分支，这种语言加剧了互联网社区之间的冲突和对抗，并对弱势群体产生了不利影响。传统的预训练语言模型（PLM）在检测PCL方面表现不佳，因为它具有虚伪和虚假同情等隐性毒性特征。随着大型语言模型（LLM）的兴起，我们可以利用其丰富的情感语义来建立探索隐性毒性的范式。在本文中，我们介绍了PclGPT，这是一个专为PCL设计的综合LLM基准。我们收集、注释和集成Pcl-PT / SFT数据集，然后通过全面的预训练和监督微调阶梯过程开发双语PclGPT-EN / CN模型组，以促进隐性毒性检测。 PclGPT 等模型的群体检测结果和细粒度检测表明，PCL 对不同弱势群体的偏见程度存在显著差异，需要社会加大关注来保护这些群体。</li>
</ul>

<h3>Title: FedPT: Federated Proxy-Tuning of Large Language Models on Resource-Constrained Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Zhidong Gao, Yu Zhang, Zhenxiao Zhang, Yanmin Gong, Yuanxiong Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00362">https://arxiv.org/abs/2410.00362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00362">https://arxiv.org/pdf/2410.00362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00362]] FedPT: Federated Proxy-Tuning of Large Language Models on Resource-Constrained Edge Devices(https://arxiv.org/abs/2410.00362)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite demonstrating superior performance across a variety of linguistic tasks, pre-trained large language models (LMs) often require fine-tuning on specific datasets to effectively address different downstream tasks. However, fine-tuning these LMs for downstream tasks necessitates collecting data from individuals, which raises significant privacy concerns. Federated learning (FL) has emerged as the de facto solution, enabling collaborative model training without sharing raw data. While promising, federated fine-tuning of large LMs faces significant challenges, including restricted access to model parameters and high computation, communication, and memory overhead. To address these challenges, this paper introduces \textbf{Fed}erated \textbf{P}roxy-\textbf{T}uning (FedPT), a novel framework for federated fine-tuning of black-box large LMs, requiring access only to their predictions over the output vocabulary instead of their parameters. Specifically, devices in FedPT first collaboratively tune a smaller LM, and then the server combines the knowledge learned by the tuned small LM with the knowledge learned by the larger pre-trained LM to construct a large proxy-tuned LM that can reach the performance of directly tuned large LMs. The experimental results demonstrate that FedPT can significantly reduce computation, communication, and memory overhead while maintaining competitive performance compared to directly federated fine-tuning of large LMs. FedPT offers a promising solution for efficient, privacy-preserving fine-tuning of large LMs on resource-constrained devices, broadening the accessibility and applicability of state-of-the-art large LMs.</li>
<li><strong>摘要：</strong>尽管在各种语言任务中表现出色，但预训练的大型语言模型 (LM) 通常需要在特定数据集上进行微调才能有效解决不同的下游任务。但是，对这些 LM 进行下游任务的微调需要收集个人数据，这引发了严重的隐私问题。联邦学习 (FL) 已成为事实上的解决方案，无需共享原始数据即可实现协作模型训练。虽然前景看好，但大型 LM 的联邦微调面临着重大挑战，包括对模型参数的访问受限以及高计算、通信和内存开销。为了应对这些挑战，本文引入了 \textbf{Fed}erated \textbf{P}roxy-\textbf{T}uning (FedPT)，这是一种用于黑盒大型 LM 联邦微调的新框架，只需要访问它们对输出词汇表的预测，而不是它们的参数。具体来说，FedPT 中的设备首先协作调整一个较小的语言模型，然后服务器将调整后的小型语言模型学到的知识与较大的预训练语言模型学到的知识相结合，构建一个大型代理调整语言模型，该语言模型可以达到直接调整的大型语言模型的性能。实验结果表明，与直接联合微调大型语言模型相比，FedPT 可以显著减少计算、通信和内存开销，同时保持有竞争力的性能。FedPT 为在资源受限的设备上高效、隐私保护地微调大型语言模型提供了一种有前途的解决方案，扩大了最先进的大型语言模型的可访问性和适用性。</li>
</ul>

<h3>Title: Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shitian Zhao, Renrui Zhang, Xu Luo, Yan Wang, Shanghang Zhang, Peng Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00363">https://arxiv.org/abs/2410.00363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00363">https://arxiv.org/pdf/2410.00363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00363]] Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models(https://arxiv.org/abs/2410.00363)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Model fusing has always been an important topic, especially in an era where large language models (LLM) and multi-modal language models (MLM) with different architectures, parameter sizes and training pipelines, are being created all the time. In this work, we propose a post-hoc framework, aiming at fusing heterogeneous models off-the-shell, which we call \textit{likelihood composition}, and the basic idea is to compose multiple models' likelihood distribution when doing a multi-choice visual-question-answering task. Here the core concept, \textit{likelihood}, is actually the log-probability of the candidate answer. In \textit{likelihood composition}, we introduce some basic operations: \textit{debias}, \textit{highlight}, \textit{majority-vote} and \textit{ensemble}. By combining (composing) these basic elements, we get the mixed composition methods: \textit{mix-composition}. Through conducting comprehensive experiments on 9 VQA datasets and 10 MLMs, we prove the effectiveness of \textit{mix-composition} compared with simple \textit{ensemble} or \textit{majority-vote} methods. In this framework, people can propose new basic composition methods and combine them to get the new mixed composition methods. We hope our proposed \textit{likelihood composition} can provide a new perspective of fusing heterogeneous models and inspire the exploration under this framework.</li>
<li><strong>摘要：</strong>模型融合一直是一个重要的课题，尤其是在大型语言模型 (LLM) 和具有不同架构、参数大小和训练管道的多模态语言模型 (MLM) 不断被创建的时代。在这项工作中，我们提出了一个事后框架，旨在现成地融合异构模型，我们称之为 \textit{似然组合}，其基本思想是在执行多项选择视觉问答任务时组合多个模型的似然分布。这里的核心概念 \textit{似然} 实际上是候选答案的对数概率。在 \textit{似然组合} 中，我们引入了一些基本操作：\textit{debias}、\textit{highlight}、\textit{majority-vote} 和 \textit{ensemble}。通过组合（组合）这些基本元素，我们得到了混合组合方法：\textit{mix-composition}。通过在 9 个 VQA 数据集和 10 个 MLM 上进行全面的实验，我们证明了 \textit{mix-composition} 与简单的 \textit{ensemble} 或 \textit{majority-vote} 方法相比的有效性。在这个框架中，人们可以提出新的基本组合方法，并将它们结合起来得到新的混合组合方法。我们希望我们提出的 \textit{likelihood composite} 可以为融合异构模型提供新的视角，并启发在这个框架下的探索。</li>
</ul>

<h3>Title: Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Shota Takashiro, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00382">https://arxiv.org/abs/2410.00382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00382">https://arxiv.org/pdf/2410.00382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00382]] Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning(https://arxiv.org/abs/2410.00382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information has become increasingly essential. For instance, LLMs are expected to provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. In response to this challenge, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the context of the query. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving other knowledge. Experiments on the TOFU and AGE datasets using Llama2-7B/13B and Mistral-7B models show our method achieves up to 95% forgetting accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation into the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and maintain them up to the final layer, they make the decision to forget at the last layer, i.e., ``LLMs pretend to forget''. Our findings offer valuable insights into enhancing the robustness of unlearning mechanisms in LLMs, setting a foundation for future research in the field.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 应用于不同的领域，选择性地忘记特定信息的能力变得越来越重要。例如，LLM 需要向授权的内部用户（例如员工或受信任的合作伙伴）提供机密信息，同时向外部用户（包括公众和未经授权的实体）隐瞒这些信息。为了应对这一挑战，我们提出了一种称为“上下文知识忘记”的新方法，该方法使模型能够根据查询的上下文在测试时选择性地忘记信息。我们的方法对预先训练的 LLM 进行了微调，以便在上下文中快速忘记目标知识，同时保留其他知识。使用 Llama2-7B/13B 和 Mistral-7B 模型在 TOFU 和 AGE 数据集上进行的实验表明，我们的方法实现了高达 95% 的遗忘准确率，同时保留了 80% 的无关知识，在域内和域外场景中的表现都远远优于基线。进一步研究该模型的内部行为后发现，虽然经过微调的 LLM 在中间层生成正确的预测并将其保持到最后一层，但它们会在最后一层做出忘记的决定，即“LLM 假装忘记”。我们的研究结果为增强 LLM 中反学习机制的稳健性提供了宝贵的见解，为该领域的未来研究奠定了基础。</li>
</ul>

<h3>Title: Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Bhargav Shandilya, Alexis Palmer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00387">https://arxiv.org/abs/2410.00387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00387">https://arxiv.org/pdf/2410.00387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00387]] Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation(https://arxiv.org/abs/2410.00387)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The data and compute requirements of current language modeling technology pose challenges for the processing and analysis of low-resource languages. Declarative linguistic knowledge has the potential to partially bridge this data scarcity gap by providing models with useful inductive bias in the form of language-specific rules. In this paper, we propose a retrieval augmented generation (RAG) framework backed by a large language model (LLM) to correct the output of a smaller model for the linguistic task of morphological glossing. We leverage linguistic information to make up for the lack of data and trainable parameters, while allowing for inputs from written descriptive grammars interpreted and distilled through an LLM. The results demonstrate that significant leaps in performance and efficiency are possible with the right combination of: a) linguistic inputs in the form of grammars, b) the interpretive power of LLMs, and c) the trainability of smaller token classification networks. We show that a compact, RAG-supported model is highly effective in data-scarce settings, achieving a new state-of-the-art for this task and our target languages. Our work also offers documentary linguists a more reliable and more usable tool for morphological glossing by providing well-reasoned explanations and confidence scores for each output.</li>
<li><strong>摘要：</strong>当前语言建模技术的数据和计算要求对低资源语言的处理和分析提出了挑战。声明性语言知识有可能通过以语言特定规则的形式为模型提供有用的归纳偏差来部分弥补这一数据稀缺差距。在本文中，我们提出了一个由大型语言模型 (LLM) 支持的检索增强生成 (RAG) 框架，以纠正较小模型的输出，以完成形态注释的语言任务。我们利用语言信息来弥补数据和可训练参数的不足，同时允许通过 LLM 解释和提炼书面描述性语法的输入。结果表明，通过正确组合以下各项，可以实现性能和效率的显著飞跃：a) 语法形式的语言输入、b) LLM 的解释能力和 c) 较小标记分类网络的可训练性。我们表明，紧凑的 RAG 支持模型在数据稀缺的环境中非常有效，为这项任务和我们的目标语言取得了新的最高水平。我们的工作还为文献语言学家提供了一种更可靠、更实用的形态注释工具，为每个输出提供了合理的解释和置信度分数。</li>
</ul>

<h3>Title: AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference</h3>
<ul>
<li><strong>Authors: </strong>Yang Han, Yiming Wang, Rui Wang, Lu Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00409">https://arxiv.org/abs/2410.00409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00409">https://arxiv.org/pdf/2410.00409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00409]] AlignSum: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference(https://arxiv.org/abs/2410.00409)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Text summarization tasks commonly employ Pre-trained Language Models (PLMs) to fit diverse standard datasets. While these PLMs excel in automatic evaluations, they frequently underperform in human evaluations, indicating a deviation between their generated summaries and human summarization preferences. This discrepancy is likely due to the low quality of fine-tuning datasets and the limited availability of high-quality human-annotated data that reflect true human preference. To address this challenge, we introduce a novel human summarization preference alignment framework AlignSum. This framework consists of three parts: Firstly, we construct a Data Pymarid with extractive, abstractive, and human-annotated summary data. Secondly, we conduct the Gaussian Resampling to remove summaries with extreme lengths. Finally, we implement the two-stage hierarchical fine-tuning with Data Pymarid after Gaussian Resampling. We apply AlignSum to PLMs on the human-annotated CNN/DailyMail and BBC XSum datasets. Experiments show that with AlignSum, PLMs like BART-Large surpass 175B GPT-3 in both automatic and human evaluations. This demonstrates that AlignSum significantly enhances the alignment of language models with human summarization preferences.</li>
<li><strong>摘要：</strong>文本摘要任务通常使用预训练语言模型 (PLM) 来拟合各种标准数据集。虽然这些 PLM 在自动评估中表现出色，但它们在人工评估中的表现往往不佳，这表明它们生成的摘要和人工摘要偏好之间存在偏差。这种差异可能是由于微调数据集质量低以及反映人类真实偏好的高质量人工注释数据有限造成的。为了应对这一挑战，我们引入了一个新颖的人工摘要偏好对齐框架 AlignSum。该框架由三部分组成：首先，我们用抽取的、抽象的和人工注释的摘要数据构建一个数据金字塔。其次，我们进行高斯重采样以删除过长的摘要。最后，我们在高斯重采样后使用数据金字塔实现两阶段分层微调。我们将 AlignSum 应用于人工注释的 CNN/DailyMail 和 BBC XSum 数据集上的 PLM。实验表明，使用 AlignSum，像 BART-Large 这样的 PLM 在自动和人工评估中都超过了 175B GPT-3。这表明 AlignSum 显著增强了语言模型与人类摘要偏好的对齐。</li>
</ul>

<h3>Title: Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Daehwan Nam, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00414">https://arxiv.org/abs/2410.00414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00414">https://arxiv.org/pdf/2410.00414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00414]] Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering(https://arxiv.org/abs/2410.00414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Semantic parsers convert natural language to logical forms, which can be evaluated on knowledge bases (KBs) to produce denotations. Recent semantic parsers have been developed with sequence-to-sequence (seq2seq) pre-trained language models (PLMs) or large language models, where the models treat logical forms as sequences of tokens. For syntactic and semantic validity, the semantic parsers use grammars that enable constrained decoding. However, the grammars lack the ability to utilize large information of KBs, although logical forms contain representations of KB elements, such as entities or relations. In this work, we propose a grammar augmented with candidate expressions for semantic parsing on a large KB with a seq2seq PLM. The grammar defines actions as production rules, and our semantic parser predicts actions during inference under the constraints by types and candidate expressions. We apply the grammar to knowledge base question answering, where the constraints by candidate expressions assist a semantic parser to generate valid KB elements. In experiments on two benchmarks, KQA Pro and Overnight, the constraints by candidate expressions increased the accuracy of our semantic parser, whether it was trained with strong supervision or weak supervision. Our semantic parser achieved state-of-the-art accuracies on KQA Pro and Overnight.</li>
<li><strong>摘要：</strong>语义解析器将自然语言转换为逻辑形式，然后可以在知识库 (KB) 上对其进行评估以产生符号。最近的语义解析器是使用序列到序列 (seq2seq) 预训练语言模型 (PLM) 或大型语言模型开发的，其中模型将逻辑形式视为标记序列。为了确保句法和语义有效性，语义解析器使用支持约束解码的语法。然而，语法缺乏利用知识库的大量信息的能力，尽管逻辑形式包含知识库元素的表示，例如实体或关系。在这项工作中，我们提出了一种增强了候选表达式的语法，用于使用 seq2seq PLM 在大型知识库上进行语义解析。语法将动作定义为生成规则，我们的语义解析器在类型和候选表达式的约束下在推理过程中预测动作。我们将语法应用于知识库问答，其中候选表达式的约束有助于语义解析器生成有效的知识库元素。在 KQA Pro 和 Overnight 两个基准测试的实验中，候选表达式的约束提高了我们的语义解析器的准确率，无论是使用强监督还是弱监督进行训练。我们的语义解析器在 KQA Pro 和 Overnight 上实现了最佳准确率。</li>
</ul>

<h3>Title: Are LLMs Aware that Some Questions are not Open-ended?</h3>
<ul>
<li><strong>Authors: </strong>Dongjie Yang, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00423">https://arxiv.org/abs/2410.00423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00423">https://arxiv.org/pdf/2410.00423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00423]] Are LLMs Aware that Some Questions are not Open-ended?(https://arxiv.org/abs/2410.00423)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown the impressive capability of answering questions in a wide range of scenarios. However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not. We refer to this as question awareness of LLMs. The lack of question awareness in LLMs leads to two phenomena that LLMs are: (1) too casual to answer non-open-ended questions or (2) too boring to answer open-ended questions. In this paper, we first evaluate the question awareness in LLMs. The experimental results show that LLMs have the issues of lacking awareness of questions in certain domains, e.g. factual knowledge, resulting in hallucinations during the generation. To mitigate these, we propose a method called Question Awareness Temperature Sampling (QuATS). This method enhances the question awareness of LLMs by adaptively adjusting the output distributions based on question features. The automatic adjustment in QuATS eliminates the need for manual temperature tuning in text generation and consistently improves model performance in various benchmarks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出在广泛场景中回答问题的出色能力。然而，当 LLM 面对不同类型的问题时，值得探索的是 LLM 是否意识到某些问题的答案有限，需要更确定地做出回答，而有些则不需要。我们将此称为 LLM 的问题意识。LLM 缺乏问题意识导致 LLM 出现两种现象：(1) 过于随意而无法回答非开放式问题或 (2) 过于无聊而无法回答开放式问题。在本文中，我们首先评估 LLM 中的问题意识。实验结果表明，LLM 在某些领域（例如事实知识）存在问题意识不足的问题，导致在生成过程中出现幻觉。为了缓解这些问题，我们提出了一种称为问题意识温度采样 (QuATS) 的方法。该方法通过根据问题特征自适应地调整输出分布来增强 LLM 的问题意识。 QuATS 中的自动调整功能消除了文本生成中手动调整温度的需要，并持续提高了各种基准测试中的模型性能。</li>
</ul>

<h3>Title: Self-Updatable Large Language Models with Parameter Integration</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Xinshuang Liu, Xiusi Chen, Sean O'Brien, Junda Wu, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00487">https://arxiv.org/abs/2410.00487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00487">https://arxiv.org/pdf/2410.00487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00487]] Self-Updatable Large Language Models with Parameter Integration(https://arxiv.org/abs/2410.00487)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in large language models (LLMs), the rapid and frequent integration of small-scale experiences, such as interactions with surrounding objects, remains a substantial challenge. Two critical factors in assimilating these experiences are (1) Efficacy: the ability to accurately remember recent events; (2) Retention: the capacity to recall long-past experiences. Current methods either embed experiences within model parameters using continual learning, model editing, or knowledge distillation techniques, which often struggle with rapid updates and complex interactions, or rely on external storage to achieve long-term retention, thereby increasing storage requirements. In this paper, we propose SELF-PARAM (Self-Updatable Large Language Models with Parameter Integration). SELF-PARAM requires no extra parameters while ensuring near-optimal efficacy and long-term retention. Our method employs a training objective that minimizes the Kullback-Leibler (KL) divergence between the predictions of an original model (with access to contextual information) and a target model (without such access). By generating diverse question-answer pairs related to the knowledge and minimizing the KL divergence across this dataset, we update the target model to internalize the knowledge seamlessly within its parameters. Evaluations on question-answering and conversational recommendation tasks demonstrate that SELF-PARAM significantly outperforms existing methods, even when accounting for non-zero storage requirements. This advancement paves the way for more efficient and scalable integration of experiences in large language models by embedding knowledge directly into model parameters.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 取得了重大进展，但快速频繁地整合小规模经验（例如与周围物体的交互）仍然是一项艰巨的挑战。吸收这些经验的两个关键因素是 (1) 功效：准确记住最近事件的能力；(2) 记忆：回忆过去很久的经历的能力。当前的方法要么使用持续学习、模型编辑或知识提炼技术将经验嵌入模型参数中，这些技术通常难以应对快速更新和复杂的交互，要么依赖外部存储来实现长期记忆，从而增加了存储要求。在本文中，我们提出了 SELF-PARAM（具有参数集成的自更新大型语言模型）。SELF-PARAM 不需要额外的参数，同时确保近乎最佳的功效和长期记忆。我们的方法采用了一个训练目标，可以最小化原始模型（可以访问上下文信息）和目标模型（不能访问上下文信息）的预测之间的 Kullback-Leibler (KL) 差异。通过生成与知识相关的各种问答对并最小化整个数据集的 KL 散度，我们更新了目标模型，以在其参数内无缝地内化知识。对问答和对话推荐任务的评估表明，即使考虑到非零存储要求，SELF-PARAM 的表现也明显优于现有方法。通过将知识直接嵌入模型参数，这一进步为更高效、更可扩展地集成大型语言模型中的经验铺平了道路。</li>
</ul>

<h3>Title: FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization</h3>
<ul>
<li><strong>Authors: </strong>Mingye Zhu, Yi Liu, Quan Wang, Junbo Guo, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00508">https://arxiv.org/abs/2410.00508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00508">https://arxiv.org/pdf/2410.00508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00508]] FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization(https://arxiv.org/abs/2410.00508)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in preference alignment have significantly improved Large Language Models' ability to generate texts that align with human preferences and values. However, current alignment metrics typically emphasize the post-hoc overall improvement, while overlooking a critical aspect: regression, which refers to the backsliding on previously correctly-handled data after updates. This potential pitfall may arise from excessive fine-tuning on already well-aligned data, which subsequently leads to over-alignment and degeneration. To address this challenge, we propose FlipGuard, a constrained optimization approach to detect and mitigate update regression with focal attention. Specifically, FlipGuard identifies performance degradation using a customized reward characterization and strategically enforces a constraint to encourage conditional congruence with the pre-aligned model during training. Comprehensive experiments demonstrate that FlipGuard effectively alleviates update regression while demonstrating excellent overall performance, with the added benefit of knowledge preservation while aligning preferences.</li>
<li><strong>摘要：</strong>偏好对齐方面的最新突破显著提高了大型语言模型生成符合人类偏好和价值观的文本的能力。然而，当前的对齐指标通常强调事后整体改进，而忽略了一个关键方面：回归，指的是更新后之前正确处理的数据的倒退。这种潜在的陷阱可能源于对已经对齐良好的数据的过度微调，这随后会导致过度对齐和退化。为了应对这一挑战，我们提出了 FlipGuard，这是一种约束优化方法，用于通过重点关注来检测和缓解更新回归。具体来说，FlipGuard 使用定制的奖励表征来识别性能下降，并策略性地强制执行约束以鼓励在训练期间与预对齐模型的条件一致性。全面的实验表明，FlipGuard 有效地缓解了更新回归，同时表现出出色的整体性能，并且在对齐偏好的同时还具有知识保存的额外好处。</li>
</ul>

<h3>Title: Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing</h3>
<ul>
<li><strong>Authors: </strong>Deokhyung Kang, Seonjeong Hwang, Yunsu Kim, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00513">https://arxiv.org/abs/2410.00513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00513">https://arxiv.org/pdf/2410.00513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00513]] Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing(https://arxiv.org/abs/2410.00513)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent efforts have aimed to utilize multilingual pretrained language models (mPLMs) to extend semantic parsing (SP) across multiple languages without requiring extensive annotations. However, achieving zero-shot cross-lingual transfer for SP remains challenging, leading to a performance gap between source and target languages. In this study, we propose Cross-Lingual Back-Parsing (CBP), a novel data augmentation methodology designed to enhance cross-lingual transfer for SP. Leveraging the representation geometry of the mPLMs, CBP synthesizes target language utterances from source meaning representations. Our methodology effectively performs cross-lingual data augmentation in challenging zero-resource settings, by utilizing only labeled data in the source language and monolingual corpora. Extensive experiments on two cross-language SP benchmarks (Mschema2QA and Xspider) demonstrate that CBP brings substantial gains in the target language. Further analysis of the synthesized utterances shows that our method successfully generates target language utterances with high slot value alignment rates while preserving semantic integrity. Our codes and data are publicly available at this https URL.</li>
<li><strong>摘要：</strong>最近的研究旨在利用多语言预训练语言模型 (mPLM) 来扩展跨多种语言的语义解析 (SP)，而无需大量注释。然而，实现 SP 的零样本跨语言迁移仍然具有挑战性，导致源语言和目标语言之间存在性能差距。在本研究中，我们提出了跨语言反向解析 (CBP)，这是一种新颖的数据增强方法，旨在增强 SP 的跨语言迁移。利用 mPLM 的表示几何，CBP 从源含义表示中合成目标语言话语。我们的方法通过仅利用源语言和单语语料库中的标记数据，在具有挑战性的零资源设置中有效地执行跨语言数据增强。在两个跨语言 SP 基准 (Mschema2QA 和 Xspider) 上进行的大量实验表明，CBP 在目标语言中带来了显着的收益。对合成话语的进一步分析表明，我们的方法成功地生成了具有高槽值对齐率的目标语言话语，同时保持了语义完整性。我们的代码和数据可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Exploring the Learning Capabilities of Language Models using LEVERWORLDS</h3>
<ul>
<li><strong>Authors: </strong>Eitan Wagner, Amir Feder, Omri Abend</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00519">https://arxiv.org/abs/2410.00519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00519">https://arxiv.org/pdf/2410.00519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00519]] Exploring the Learning Capabilities of Language Models using LEVERWORLDS(https://arxiv.org/abs/2410.00519)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Learning a model of a stochastic setting often involves learning both general structure rules and specific properties of the instance. This paper investigates the interplay between learning the general and the specific in various learning methods, with emphasis on sample efficiency. We design a framework called {\sc LeverWorlds}, which allows the generation of simple physics-inspired worlds that follow a similar generative process with different distributions, and their instances can be expressed in natural language. These worlds allow for controlled experiments to assess the sample complexity of different learning methods. We experiment with classic learning algorithms as well as Transformer language models, both with fine-tuning and In-Context Learning (ICL). Our general finding is that (1) Transformers generally succeed in the task; but (2) they are considerably less sample efficient than classic methods that make stronger assumptions about the structure, such as Maximum Likelihood Estimation and Logistic Regression. This finding is in tension with the recent tendency to use Transformers as general-purpose estimators. We propose an approach that leverages the ICL capabilities of contemporary language models to apply simple algorithms for this type of data. Our experiments show that models currently struggle with the task but show promising potential.</li>
<li><strong>摘要：</strong>学习随机环境的模型通常涉及学习实例的一般结构规则和特定属性。本文研究了各种学习方法中学习一般和特定之间的相互作用，重点是样本效率。我们设计了一个名为 {\sc LeverWorlds} 的框架，它允许生成简单的物理启发世界，这些世界遵循具有不同分布的类似生成过程，并且它们的实例可以用自然语言表达。这些世界允许进行受控实验来评估不同学习方法的样本复杂性。我们尝试了经典学习算法以及 Transformer 语言模型，两者都进行了微调和上下文学习 (ICL)。我们的总体发现是 (1) Transformer 通常能够成功完成任务；但 (2) 它们的样本效率远低于对结构做出更强假设的经典方法，例如最大似然估计和逻辑回归。这一发现与最近将 Transformer 用作通用估计器的趋势相矛盾。我们提出了一种方法，利用当代语言模型的 ICL 功能对此类数据应用简单的算法。我们的实验表明，模型目前难以完成这项任务，但显示出良好的潜力。</li>
</ul>

<h3>Title: Benchmarking Large Language Models for Conversational Question Answering in Multi-instructional Documents</h3>
<ul>
<li><strong>Authors: </strong>Shiwei Wu, Chen Zhang, Yan Gao, Qimeng Wang, Tong Xu, Yao Hu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00526">https://arxiv.org/abs/2410.00526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00526">https://arxiv.org/pdf/2410.00526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00526]] Benchmarking Large Language Models for Conversational Question Answering in Multi-instructional Documents(https://arxiv.org/abs/2410.00526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instructional documents are rich sources of knowledge for completing various tasks, yet their unique challenges in conversational question answering (CQA) have not been thoroughly explored. Existing benchmarks have primarily focused on basic factual question-answering from single narrative documents, making them inadequate for assessing a model`s ability to comprehend complex real-world instructional documents and provide accurate step-by-step guidance in daily life. To bridge this gap, we present InsCoQA, a novel benchmark tailored for evaluating large language models (LLMs) in the context of CQA with instructional documents. Sourced from extensive, encyclopedia-style instructional content, InsCoQA assesses models on their ability to retrieve, interpret, and accurately summarize procedural guidance from multiple documents, reflecting the intricate and multi-faceted nature of real-world instructional tasks. Additionally, to comprehensively assess state-of-the-art LLMs on the InsCoQA benchmark, we propose InsEval, an LLM-assisted evaluator that measures the integrity and accuracy of generated responses and procedural instructions.</li>
<li><strong>摘要：</strong>教学文档是完成各种任务的丰富知识来源，但它们在对话式问答 (CQA) 中面临的独特挑战尚未得到彻底探索。现有的基准测试主要集中于单一叙述性文档中的基本事实问答，不足以评估模型理解复杂的现实世界教学文档和在日常生活中提供准确的分步指导的能力。为了弥补这一差距，我们提出了 InsCoQA，这是一种新颖的基准测试，专门用于评估带有教学文档的 CQA 环境中的大型语言模型 (LLM)。InsCoQA 源自广泛的百科全书式教学内容，评估模型从多个文档中检索、解释和准确总结程序指导的能力，反映了现实世界教学任务的复杂性和多面性。此外，为了全面评估 InsCoQA 基准上最先进的 LLM，我们提出了 InsEval，这是一个 LLM 辅助评估器，用于衡量生成的响应和程序指令的完整性和准确性。</li>
</ul>

<h3>Title: AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00558">https://arxiv.org/abs/2410.00558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00558">https://arxiv.org/pdf/2410.00558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00558]] AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation(https://arxiv.org/abs/2410.00558)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The impressive performance of proprietary LLMs like GPT4 in code generation has led to a trend to replicate these capabilities in open-source models through knowledge distillation (e.g. Code Evol-Instruct). However, these efforts often neglect the crucial aspect of response quality, relying heavily on teacher models for direct response distillation. This paradigm, especially for complex instructions, can degrade the quality of synthesized data, compromising the knowledge distillation process. To this end, our study introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which employs a two-stage process to refine response distillation. The first stage, modular decomposition, breaks down the direct response into more manageable sub-modules. The second stage, adaptive response evolution, automatically evolves the response with the related function modules. Our experiments with three popular code benchmarks (HumanEval, MBPP, and EvalPlus) attest to the superiority of the AMR-Evol framework over baseline response distillation methods. By comparing with the open-source Code LLMs trained on a similar scale of data, we observed performance enhancements: more than +3.0 points on HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the effectiveness of our framework. Our codes are available at this https URL.</li>
<li><strong>摘要：</strong>专有 LLM（如 GPT4）在代码生成方面的出色表现，导致了一种趋势，即通过知识提炼（例如 Code Evol-Instruct）在开源模型中复制这些功能。然而，这些努力往往忽略了响应质量这一关键方面，严重依赖教师模型进行直接响应提炼。这种范式，尤其是对于复杂的指令，会降低合成数据的质量，从而损害知识提炼过程。为此，我们的研究引入了自适应模块化响应演化 (AMR-Evol) 框架，该框架采用两阶段过程来改进响应提炼。第一阶段，模块化分解，将直接响应分解为更易于管理的子模块。第二阶段，自适应响应演化，使用相关功能模块自动演化响应。我们对三个流行的代码基准（HumanEval、MBPP 和 EvalPlus）进行的实验证明了 AMR-Evol 框架优于基线响应提炼方法。通过与在类似规模的数据上训练的开源代码 LLM 进行比较，我们观察到了性能增强：在 HumanEval-Plus 上超过 +3.0 分，在 MBPP-Plus 上超过 +1.0 分，这凸显了我们框架的有效性。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Style-Specific Neurons for Steering LLMs in Text Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Wen Lai, Viktor Hangya, Alexander Fraser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00593">https://arxiv.org/abs/2410.00593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00593">https://arxiv.org/pdf/2410.00593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00593]] Style-Specific Neurons for Steering LLMs in Text Style Transfer(https://arxiv.org/abs/2410.00593)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Text style transfer (TST) aims to modify the style of a text without altering its original meaning. Large language models (LLMs) demonstrate superior performance across multiple tasks, including TST. However, in zero-shot setups, they tend to directly copy a significant portion of the input text to the output without effectively changing its style. To enhance the stylistic variety and fluency of the text, we present sNeuron-TST, a novel approach for steering LLMs using style-specific neurons in TST. Specifically, we identify neurons associated with the source and target styles and deactivate source-style-only neurons to give target-style words a higher probability, aiming to enhance the stylistic diversity of the generated text. However, we find that this deactivation negatively impacts the fluency of the generated text, which we address by proposing an improved contrastive decoding method that accounts for rapid token probability shifts across layers caused by deactivated source-style neurons. Empirical experiments demonstrate the effectiveness of the proposed method on six benchmarks, encompassing formality, toxicity, politics, politeness, authorship, and sentiment.</li>
<li><strong>摘要：</strong>文本风格迁移 (TST) 旨在修改文本的风格而不改变其原意。大型语言模型 (LLM) 在包括 TST 在内的多个任务中表现出色。然而，在零样本设置中，它们倾向于直接将输入文本的很大一部分复制到输出，而不会有效地改变其风格。为了增强文本的风格多样性和流畅性，我们提出了 sNeuron-TST，这是一种使用 TST 中的风格特定神经元来控制 LLM 的新方法。具体来说，我们识别与源风格和目标风格相关的神经元，并停用仅源风格的神经元，以赋予目标风格单词更高的概率，旨在增强生成文本的风格多样性。然而，我们发现这种停用会对生成文本的流畅性产生负面影响，我们通过提出一种改进的对比解码方法来解决这个问题，该方法考虑了由停用源风格神经元引起的跨层快速标记概率变化。实证实验证明了所提出的方法在六个基准上的有效性，包括形式性、毒性、政治性、礼貌性、作者身份和情感。</li>
</ul>

<h3>Title: Detecci\'on Autom\'atica de Patolog\'ias en Notas Cl\'inicas en Espa\~nol Combinando Modelos de Lenguaje y Ontolog\'ias M\'edicos</h3>
<ul>
<li><strong>Authors: </strong>Léon-Paul Schaub Torre, Pelayo Quirós, Helena García Mieres</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00616">https://arxiv.org/abs/2410.00616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00616">https://arxiv.org/pdf/2410.00616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00616]] Detecci\'on Autom\'atica de Patolog\'ias en Notas Cl\'inicas en Espa\~nol Combinando Modelos de Lenguaje y Ontolog\'ias M\'edicos(https://arxiv.org/abs/2410.00616)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper we present a hybrid method for the automatic detection of dermatological pathologies in medical reports. We use a large language model combined with medical ontologies to predict, given a first appointment or follow-up medical report, the pathology a person may suffer from. The results show that teaching the model to learn the type, severity and location on the body of a dermatological pathology as well as in which order it has to learn these three features significantly increases its accuracy. The article presents the demonstration of state-of-the-art results for classification of medical texts with a precision of 0.84, micro and macro F1-score of 0.82 and 0.75, and makes both the method and the dataset used available to the community. -- En este artículo presentamos un método híbrido para la detección automática de patologías dermatológicas en informes médicos. Usamos un modelo de lenguaje amplio en español combinado con ontologías médicas para predecir, dado un informe médico de primera cita o de seguimiento, la patología del paciente. Los resultados muestran que el tipo, la gravedad y el sitio en el cuerpo de una patología dermatológica, así como en qué orden tiene un modelo que aprender esas tres características, aumentan su precisión. El artículo presenta la demostración de resultados comparables al estado del arte de clasificación de textos médicos con una precisión de 0.84, micro y macro F1-score de 0.82 y 0.75, y deja a disposición de la comunidad tanto el método como el conjunto de datos utilizado.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了一种自动检测医疗报告中皮肤病的混合方法。我们使用一个大型语言模型结合医学本体来预测，根据首次预约或后续医疗报告，一个人可能患有的病理。结果表明，教模型学习皮肤病的类型、严重程度和身体位置以及学习这三个特征的顺序，可显著提高其准确性。本文展示了最先进的医学文本分类结果，精度为 0.84，微观和宏观 F1 分数分别为 0.82 和 0.75，并将该方法和所使用的数据集提供给社区。——在本文中，我们介绍了一种用于自动检测医学报告中皮肤病的混合方法。我们使用西班牙语组合了多种语言模型和医学本体论，以了解医学入门或后续知识、病理学知识。其结果包括皮肤病学、坟墓和地点，以及根据模型确定特征、精确度的方法。本文介绍了结果演示，可与医疗文本分类技术的精度 0.84、微观和宏观 F1 分数 0.82 和 0.75 进行比较，并且与使用数据的方法相结合。</li>
</ul>

<h3>Title: Efficient Technical Term Translation: A Knowledge Distillation Approach for Parenthetical Terminology Translation</h3>
<ul>
<li><strong>Authors: </strong>Jiyoon Myung, Jihyeon Park, Jungki Son, Kyungro Lee, Joohyung Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00683">https://arxiv.org/abs/2410.00683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00683">https://arxiv.org/pdf/2410.00683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00683]] Efficient Technical Term Translation: A Knowledge Distillation Approach for Parenthetical Terminology Translation(https://arxiv.org/abs/2410.00683)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of accurately translating technical terms, which are crucial for clear communication in specialized fields. We introduce the Parenthetical Terminology Translation (PTT) task, designed to mitigate potential inaccuracies by displaying the original term in parentheses alongside its translation. To implement this approach, we generated a representative PTT dataset using a collaborative approach with large language models and applied knowledge distillation to fine-tune traditional Neural Machine Translation (NMT) models and small-sized Large Language Models (sLMs). Additionally, we developed a novel evaluation metric to assess both overall translation accuracy and the correct parenthetical presentation of terms. Our findings indicate that sLMs did not consistently outperform NMT models, with fine-tuning proving more effective than few-shot prompting, particularly in models with continued pre-training in the target language. These insights contribute to the advancement of more reliable terminology translation methodologies.</li>
<li><strong>摘要：</strong>本文探讨了准确翻译技术术语的挑战，这对于专业领域的清晰沟通至关重要。我们引入了括号术语翻译 (PTT) 任务，旨在通过在翻译旁边的括号中显示原始术语来减少潜在的不准确性。为了实现这种方法，我们使用与大型语言模型的协作方法生成了一个代表性的 PTT 数据集，并应用知识提炼来微调传统的神经机器翻译 (NMT) 模型和小型大型语言模型 (sLM)。此外，我们开发了一种新颖的评估指标，以评估整体翻译准确性和术语的正确括号呈现。我们的研究结果表明，sLM 的表现并不总是优于 NMT 模型，微调被证明比少样本提示更有效，特别是在使用目标语言进行持续预训练的模型中。这些见解有助于开发更可靠的术语翻译方法。</li>
</ul>

<h3>Title: VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models</h3>
<ul>
<li><strong>Authors: </strong>Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00741">https://arxiv.org/abs/2410.00741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00741">https://arxiv.org/pdf/2410.00741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00741]] VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models(https://arxiv.org/abs/2410.00741)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has been widely studied and applied in numerous applications. However, the emphasis on brief summary texts during pre-training prevents CLIP from understanding long descriptions. This issue is particularly acute regarding videos given that videos often contain abundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra Length) model, which aims to unleash the long-description understanding capability of video CLIP models. Firstly, we establish an automatic data collection system and gather a large-scale VILD pre-training dataset with VIdeo and Long-Description pairs. Then, we propose Text-similarity-guided Primary Component Matching (TPCM) to better learn the distribution of feature space while expanding the long description capability. We also introduce two new tasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware Description Ranking (HDR) for further understanding improvement. Finally, we construct a Long Video Description Ranking (LVDR) benchmark for evaluating the long-description capability more comprehensively. Extensive experimental results on widely-used text-video retrieval benchmarks with both short and long descriptions and our LVDR benchmark can fully demonstrate the effectiveness of our method.</li>
<li><strong>摘要：</strong>对比语言-图像预训练 (CLIP) 已得到广泛研究并应用于众多应用。然而，预训练期间对简短摘要文本的强调阻碍了 CLIP 理解长描述。由于视频通常包含丰富的详细内容，这个问题对于视频来说尤其严重。在本文中，我们提出了 VideoCLIP-XL (eXtra Length) 模型，旨在释放视频 CLIP 模型的长描述理解能力。首先，我们建立一个自动数据收集系统，并收集一个包含视频和长描述对的大规模 VILD 预训练数据集。然后，我们提出了文本相似性引导的主成分匹配 (TPCM)，以更好地学习特征空间的分布，同时扩展长描述能力。我们还引入了两个新任务，即细节感知描述排名 (DDR) 和幻觉感知描述排名 (HDR)，以进一步提高理解能力。最后，我们构建了一个长视频描述排名 (LVDR) 基准，以更全面地评估长描述能力。在广泛使用的包含短描述和长描述的文本视频检索基准以及我们的 LVDR 基准上进行的大量实验结果可以充分证明我们方法的有效性。</li>
</ul>

<h3>Title: Optimizing Token Usage on Large Language Model Conversations Using the Design Structure Matrix</h3>
<ul>
<li><strong>Authors: </strong>Ramon Maria Garcia Alarcia, Alessandro Golkar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00749">https://arxiv.org/abs/2410.00749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00749">https://arxiv.org/pdf/2410.00749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00749]] Optimizing Token Usage on Large Language Model Conversations Using the Design Structure Matrix(https://arxiv.org/abs/2410.00749)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models become ubiquitous in many sectors and tasks, there is a need to reduce token usage, overcoming challenges such as short context windows, limited output sizes, and costs associated with token intake and generation, especially in API-served LLMs. This work brings the Design Structure Matrix from the engineering design discipline into LLM conversation optimization. Applied to a use case in which the LLM conversation is about the design of a spacecraft and its subsystems, the DSM, with its analysis tools such as clustering and sequencing, demonstrates being an effective tool to organize the conversation, minimizing the number of tokens sent to or retrieved from the LLM at once, as well as grouping chunks that can be allocated to different context windows. Hence, this work broadens the current set of methodologies for token usage optimization and opens new avenues for the integration of engineering design practices into LLMs.</li>
<li><strong>摘要：</strong>随着大型语言模型在许多行业和任务中变得无处不在，需要减少令牌的使用，克服诸如上下文窗口短、输出大小有限以及与令牌输入和生成相关的成本等挑战，尤其是在 API 服务的 LLM 中。这项工作将工程设计学科的设计结构矩阵引入到 LLM 对话优化中。应用于 LLM 对话涉及航天器及其子系统设计的用例，DSM 及其分析工具（如聚类和排序）被证明是一种有效的组织对话的工具，最大限度地减少了一次发送到 LLM 或从 LLM 检索的令牌数量，以及对可分配给不同上下文窗口的块进行分组。因此，这项工作拓宽了当前的令牌使用优化方法集，并为将工程设计实践集成到 LLM 中开辟了新的途径。</li>
</ul>

<h3>Title: Thinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting</h3>
<ul>
<li><strong>Authors: </strong>Stephen Meisenbacher, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00751">https://arxiv.org/abs/2410.00751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00751">https://arxiv.org/pdf/2410.00751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00751]] Thinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting(https://arxiv.org/abs/2410.00751)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The field of privacy-preserving Natural Language Processing has risen in popularity, particularly at a time when concerns about privacy grow with the proliferation of Large Language Models. One solution consistently appearing in recent literature has been the integration of Differential Privacy (DP) into NLP techniques. In this paper, we take these approaches into critical view, discussing the restrictions that DP integration imposes, as well as bring to light the challenges that such restrictions entail. To accomplish this, we focus on $\textbf{DP-Prompt}$, a recent method for text privatization leveraging language models to rewrite texts. In particular, we explore this rewriting task in multiple scenarios, both with DP and without DP. To drive the discussion on the merits of DP in NLP, we conduct empirical utility and privacy experiments. Our results demonstrate the need for more discussion on the usability of DP in NLP and its benefits over non-DP approaches.</li>
<li><strong>摘要：</strong>隐私保护的自然语言处理领域越来越受欢迎，尤其是在随着大型语言模型的激增，人们对隐私的担忧日益增加的时代。最近文献中不断出现的一种解决方案是将差异隐私 (DP) 集成到 NLP 技术中。在本文中，我们批判性地看待这些方法，讨论 DP 集成所带来的限制，并揭示这些限制带来的挑战。为此，我们专注于 $\textbf{DP-Prompt}$，这是一种利用语言模型重写文本的文本私有化方法。特别是，我们在有 DP 和没有 DP 的多种场景中探索了这个重写任务。为了推动关于 DP 在 NLP 中的优点的讨论，我们进行了实证效用和隐私实验。我们的结果表明需要更多地讨论 DP 在 NLP 中的可用性及其相对于非 DP 方法的优势。</li>
</ul>

<h3>Title: Decoding Hate: Exploring Language Models' Reactions to Hate Speech</h3>
<ul>
<li><strong>Authors: </strong>Paloma Piot, Javier Parapar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00775">https://arxiv.org/abs/2410.00775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00775">https://arxiv.org/pdf/2410.00775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00775]] Decoding Hate: Exploring Language Models' Reactions to Hate Speech(https://arxiv.org/abs/2410.00775)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Hate speech is a harmful form of online expression, often manifesting as derogatory posts. It is a significant risk in digital environments. With the rise of Large Language Models (LLMs), there is concern about their potential to replicate hate speech patterns, given their training on vast amounts of unmoderated internet data. Understanding how LLMs respond to hate speech is crucial for their responsible deployment. However, the behaviour of LLMs towards hate speech has been limited compared. This paper investigates the reactions of seven state-of-the-art LLMs (LLaMA 2, Vicuna, LLaMA 3, Mistral, GPT-3.5, GPT-4, and Gemini Pro) to hate speech. Through qualitative analysis, we aim to reveal the spectrum of responses these models produce, highlighting their capacity to handle hate speech inputs. We also discuss strategies to mitigate hate speech generation by LLMs, particularly through fine-tuning and guideline guardrailing. Finally, we explore the models' responses to hate speech framed in politically correct language.</li>
<li><strong>摘要：</strong>仇恨言论是一种有害的网络表达形式，通常表现为贬损性帖子。在数字环境中，仇恨言论是一个重大风险。随着大型语言模型 (LLM) 的兴起，人们担心它们可能会复制仇恨言论模式，因为它们是在大量未经审核的互联网数据上进行训练的。了解 LLM 如何应对仇恨言论对于负责任地部署它们至关重要。然而，相比之下，LLM 对仇恨言论的行为却有限。本文研究了七种最先进的 LLM（LLaMA 2、Vicuna、LLaMA 3、Mistral、GPT-3.5、GPT-4 和 Gemini Pro）对仇恨言论的反应。通过定性分析，我们旨在揭示这些模型产生的响应范围，突出它们处理仇恨言论输入的能力。我们还讨论了减轻 LLM 产生仇恨言论的策略，特别是通过微调和指导方针护栏。最后，我们探讨了模型对以政治正确语言表达的仇恨言论的反应。</li>
</ul>

<h3>Title: A generative framework to bridge data-driven models and scientific theories in language neuroscience</h3>
<ul>
<li><strong>Authors: </strong>Richard Antonello, Chandan Singh, Shailee Jain, Aliyah Hsu, Jianfeng Gao, Bin Yu, Alexander Huth</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00812">https://arxiv.org/abs/2410.00812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00812">https://arxiv.org/pdf/2410.00812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00812]] A generative framework to bridge data-driven models and scientific theories in language neuroscience(https://arxiv.org/abs/2410.00812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Representations from large language models are highly effective at predicting BOLD fMRI responses to language stimuli. However, these representations are largely opaque: it is unclear what features of the language stimulus drive the response in each brain area. We present generative explanation-mediated validation, a framework for generating concise explanations of language selectivity in the brain and then validating those explanations in follow-up experiments that use synthetic stimuli. This approach is successful at explaining selectivity both in individual voxels and cortical regions of interest (ROIs).We show that explanatory accuracy is closely related to the predictive power and stability of the underlying statistical models. These results demonstrate that LLMs can be used to bridge the widening gap between data-driven models and formal scientific theories.</li>
<li><strong>摘要：</strong>大型语言模型的表征在预测语言刺激的 BOLD fMRI 反应方面非常有效。然而，这些表征在很大程度上是不透明的：尚不清楚语言刺激的哪些特征会驱动每个大脑区域的反应。我们提出了生成解释介导的验证，这是一个框架，用于生成大脑语言选择性的简明解释，然后在使用合成刺激的后续实验中验证这些解释。这种方法成功地解释了单个体素和皮质感兴趣区域 (ROI) 中的选择性。我们表明，解释准确性与底层统计模型的预测能力和稳定性密切相关。这些结果表明，LLM 可用于弥合数据驱动模型与正式科学理论之间日益扩大的差距。</li>
</ul>

<h3>Title: Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis</h3>
<ul>
<li><strong>Authors: </strong>Reshmi Ghosh, Rahul Seetharaman, Hitesh Wadhwa, Somyaa Aggarwal, Samyadeep Basu, Soundararajan Srinivasan, Wenlong Zhao, Shreyas Chaudhari, Ehsan Aghazadeh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00857">https://arxiv.org/abs/2410.00857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00857">https://arxiv.org/pdf/2410.00857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00857]] Quantifying reliance on external information over parametric knowledge during Retrieval Augmented Generation (RAG) using mechanistic analysis(https://arxiv.org/abs/2410.00857)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) is a widely used approach for leveraging external context in several natural language applications such as question answering and information retrieval. Yet, the exact nature in which a Language Model (LM) leverages this non-parametric memory or retrieved context isn't clearly understood. This paper mechanistically examines the RAG pipeline to highlight that LMs demonstrate a "shortcut'' effect and have a strong bias towards utilizing the retrieved context to answer questions, while relying minimally on model priors. We propose (a) Causal Mediation Analysis; for proving that parametric memory is minimally utilized when answering a question and (b) Attention Contributions and Knockouts for showing the last token residual stream do not get enriched from the subject token in the question, but gets enriched from tokens of RAG-context. We find this pronounced "shortcut'' behaviour to be true across both LLMs (e.g.,LlaMa) and SLMs (e.g., Phi)</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 是一种广泛使用的方法，用于在问答和信息检索等多种自然语言应用中利用外部上下文。然而，语言模型 (LM) 利用这种非参数记忆或检索上下文的确切性质尚不清楚。本文从机制上研究了 RAG 管道，以强调 LM 表现出“捷径”效应，并且强烈倾向于利用检索到的上下文来回答问题，同时尽量减少对模型先验的依赖。我们提出 (a) 因果中介分析；用于证明在回答问题时参数记忆的使用最少；(b) 注意力贡献和淘汰赛，用于显示最后一个标记残差流不会从问题中的主题标记中丰富，而是从 RAG 上下文的标记中丰富。我们发现这种明显的“捷径”行为在 LLM（例如 LlaMa）和 SLM（例如 Phi）中都是正确的</li>
</ul>

<h3>Title: On the Implications of Verbose LLM Outputs: A Case Study in Translation Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Eleftheria Briakou, Zhongtao Liu, Colin Cherry, Markus Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00863">https://arxiv.org/abs/2410.00863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00863">https://arxiv.org/pdf/2410.00863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00863]] On the Implications of Verbose LLM Outputs: A Case Study in Translation Evaluation(https://arxiv.org/abs/2410.00863)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>This paper investigates the impact of verbose LLM translations on evaluation. We first demonstrate the prevalence of this behavior across several LLM outputs drawn from the WMT 2024 general shared task on machine translation. We then identify the primary triggers of verbosity, including safety, copyright concerns, and insufficient context in short input queries. Finally, we show that ignoring this behavior unfairly penalizes more verbose LLMs according to both automatic and human evaluations, highlighting the need to address this issue for more accurate future evaluations.</li>
<li><strong>摘要：</strong>本文探讨了冗长的 LLM 翻译对评估的影响。我们首先展示了这种行为在 WMT 2024 机器翻译通用共享任务的几个 LLM 输出中的普遍性。然后，我们确定了冗长的主要触发因素，包括安全、版权问题和简短输入查询中的上下文不足。最后，我们表明，忽略这种行为会不公平地惩罚更冗长的 LLM，无论是根据自动评估还是人工评估，这强调了解决这个问题的必要性，以便在未来进行更准确的评估。</li>
</ul>

<h3>Title: Addition is All You Need for Energy-efficient Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyin Luo, Wei Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00907">https://arxiv.org/abs/2410.00907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00907">https://arxiv.org/pdf/2410.00907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00907]] Addition is All You Need for Energy-efficient Language Models(https://arxiv.org/abs/2410.00907)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.</li>
<li><strong>摘要：</strong>大型神经网络将大部分计算花在浮点张量乘法上。在这项工作中，我们发现浮点乘法器可以用一个高精度整数加法器来近似。我们提出了线性复杂度乘法 L-Mul 算法，该算法用整数加法运算来近似浮点数乘法。新算法比 8 位浮点乘法消耗的计算资源少得多，但精度更高。与 8 位浮点乘法相比，所提出的方法实现了更高的精度，但消耗的位级计算量却少得多。由于与整数加法运算相比，浮点数乘法需要的能量要高得多，因此在张量处理硬件中应用 L-Mul 运算可以通过逐元素浮点张量乘法降低 95% 的能量成本，并降低 80% 的点积能量成本。我们计算了 L-Mul 的理论误差期望，并在广泛的文本、视觉和符号任务上评估了该算法，包括自然语言理解、结构推理、数学和常识性问答。我们的数值分析实验与理论误差估计一致，这表明具有 4 位尾数的 L-Mul 实现了与 float8_e4m3 乘法相当的精度，而具有 3 位尾数的 L-Mul 优于 float8_e5m2。在流行基准上的评估结果表明，直接将 L-Mul 应用于注意力机制几乎是无损的。我们进一步表明，在 Transformer 模型中用 3 位尾数 L-Mul 替换所有浮点乘法可实现与在微调和推理中使用 float8_e4m3 作为累积精度相当的精度。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
