<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-06-25</h1>
<h3>Title: Title:
          Introducing Syllable Tokenization for Low-resource Languages: A Case Study with Swahili</h3>
<ul>
<li><strong>Authors: </strong>Jesse Atuhurra, Hiroyuki Shindo, Hidetaka Kamigaito, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Introducing Syllable Tokenization for Low-resource Languages: A Case Study with Swahili(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Many attempts have been made in multilingual NLP to ensure that pre-trained language models, such as mBERT or GPT2 get better and become applicable to low-resource languages. To achieve multilingualism for pre-trained language models (PLMs), we need techniques to create word embeddings that capture the linguistic characteristics of any language. Tokenization is one such technique because it allows for the words to be split based on characters or subwords, creating word embeddings that best represent the structure of the language. Creating such word embeddings is essential to applying PLMs to other languages where the model was not trained, enabling multilingual NLP. However, most PLMs use generic tokenization methods like BPE, wordpiece, or unigram which may not suit specific languages. We hypothesize that tokenization based on syllables within the input text, which we call syllable tokenization, should facilitate the development of syllable-aware language models. The syllable-aware language models make it possible to apply PLMs to languages that are rich in syllables, for instance, Swahili. Previous works introduced subword tokenization. Our work extends such efforts. Notably, we propose a syllable tokenizer and adopt an experiment-centric approach to validate the proposed tokenizer based on the Swahili language. We conducted text-generation experiments with GPT2 to evaluate the effectiveness of the syllable tokenizer. Our results show that the proposed syllable tokenizer generates syllable embeddings that effectively represent the Swahili language.</li>
<li><strong>摘要：</strong>在多语言 NLP 中，人们进行了许多尝试，以确保预训练语言模型（例如 mBERT 或 GPT2）变得更好并适用于资源匮乏的语言。为了使预训练语言模型 (PLM) 实现多语言性，我们需要创建能够捕捉任何语言的语言特征的词向量的技术。标记化就是这样一种技术，因为它允许根据字符或子词拆分单词，从而创建最能代表语言结构的词向量。创建这样的词向量对于将 PLM 应用于模型未经过训练的其他语言至关重要，从而实现多语言 NLP。但是，大多数 PLM 使用通用的标记化方法，例如 BPE、wordpiece 或 unigram，这些方法可能不适合特定语言。我们假设基于输入文本中的音节的标记化（我们称之为音节标记化）应该有助于开发音节感知语言模型。音节感知语言模型使得将 PLM 应用于富含音节的语言（例如斯瓦希里语）成为可能。先前的研究引入了子词标记化。我们的工作扩展了此类努力。值得注意的是，我们提出了一个音节标记器，并采用以实验为中心的方法来验证基于斯瓦希里语的所提出的标记器。我们使用 GPT2 进行了文本生成实验，以评估音节标记器的有效性。我们的结果表明，所提出的音节标记器生成的音节嵌入可以有效地表示斯瓦希里语。</li>
</ul>

<h3>Title: Title:
          Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jesse Atuhurra, Iqra Ali, Tatsuya Hiraoka, Hidetaka Kamigaito, Tomoya Iwakura, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have increased interest in vision language models (VLMs), which process image-text pairs as input. Studies investigating the visual understanding ability of VLMs have been proposed, but such studies are still preliminary because existing datasets do not permit a comprehensive evaluation of the fine-grained visual linguistic abilities of VLMs across multiple languages. To further explore the strengths of VLMs, such as GPT-4V \cite{openai2023GPT4}, we developed new datasets for the systematic and qualitative analysis of VLMs. Our contribution is four-fold: 1) we introduced nine vision-and-language (VL) tasks (including object recognition, image-text matching, and more) and constructed multilingual visual-text datasets in four languages: English, Japanese, Swahili, and Urdu through utilizing templates containing \textit{questions} and prompting GPT4-V to generate the \textit{answers} and the \textit{rationales}, 2) introduced a new VL task named \textit{unrelatedness}, 3) introduced rationales to enable human understanding of the VLM reasoning process, and 4) employed human evaluation to measure the suitability of proposed datasets for VL tasks. We show that VLMs can be fine-tuned on our datasets. Our work is the first to conduct such analyses in Swahili and Urdu. Also, it introduces \textit{rationales} in VL analysis, which played a vital role in the evaluation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 引起了人们对视觉语言模型 (VLM) 的兴趣，该模型将图像-文本对作为输入进行处理。已经提出了研究 VLM 视觉理解能力的研究，但此类研究仍处于初步阶段，因为现有数据集无法全面评估 VLM 在多种语言中的细粒度视觉语言能力。为了进一步探索 GPT-4V \cite{openai2023GPT4} 等 VLM 的优势，我们开发了新的数据集，用于对 VLM 进行系统和定性分析。我们的贡献有四个方面：1）我们引入了九个视觉和语言 (VL) 任务（包括对象识别、图像文本匹配等），并通过使用包含 \textit{问题} 的模板并提示 GPT4-V 生成 \textit{答案} 和 \textit{理由}，构建了四种语言的多语言视觉文本数据集：英语、日语、斯瓦希里语和乌尔都语；2）引入了一个名为 \textit{不相关} 的新 VL 任务；3）引入了理由以使人类理解 VLM 推理过程；4）采用人工评估来衡量所提出的数据集对 VL 任务的适用性。我们表明，VLM 可以在我们的数据集上进行微调。我们的工作是首次在斯瓦希里语和乌尔都语中进行此类分析。此外，它在 VL 分析中引入了 \textit{理由}，这在评估中发挥了至关重要的作用。</li>
</ul>

<h3>Title: Title:
          Diverse Perspectives, Divergent Models: Cross-Cultural Evaluation of Depression Detection on Twitter</h3>
<ul>
<li><strong>Authors: </strong>Nuredin Ali, Charles Chuankai Zhang, Ned Mayo, Stevie Chancellor</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Diverse Perspectives, Divergent Models: Cross-Cultural Evaluation of Depression Detection on Twitter(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Social media data has been used for detecting users with mental disorders, such as depression. Despite the global significance of cross-cultural representation and its potential impact on model performance, publicly available datasets often lack crucial metadata related to this aspect. In this work, we evaluate the generalization of benchmark datasets to build AI models on cross-cultural Twitter data. We gather a custom geo-located Twitter dataset of depressed users from seven countries as a test dataset. Our results show that depression detection models do not generalize globally. The models perform worse on Global South users compared to Global North. Pre-trained language models achieve the best generalization compared to Logistic Regression, though still show significant gaps in performance on depressed and non-Western users. We quantify our findings and provide several actionable suggestions to mitigate this issue.</li>
<li><strong>摘要：</strong>社交媒体数据已用于检测患有抑郁症等精神障碍的用户。尽管跨文化表征具有全球意义，并可能影响模型性能，但公开可用的数据集通常缺少与此方面相关的关键元数据。在这项工作中，我们评估了基准数据集的泛化能力，以在跨文化 Twitter 数据上构建 AI 模型。我们收集了来自七个国家的抑郁用户的自定义地理位置 Twitter 数据集作为测试数据集。我们的结果表明，抑郁症检测模型并不能在全球范围内推广。与全球北方相比，这些模型对全球南方用户的表现更差。与逻辑回归相比，预训练语言模型实现了最佳泛化，但在抑郁和非西方用户方面仍存在显著的性能差距。我们量化了我们的发现并提供了一些可行的建议来缓解这个问题。</li>
</ul>

<h3>Title: Title:
          Exploring LLM Multi-Agents for ICD Coding</h3>
<ul>
<li><strong>Authors: </strong>Rumeng Li, Xun Wang, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Exploring LLM Multi-Agents for ICD Coding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive and diverse abilities that can benefit various domains, such as zero and few-shot information extraction from clinical text without domain-specific training. However, for the ICD coding task, they often hallucinate key details and produce high recall but low precision results due to the high-dimensional and skewed distribution of the ICD codes. Existing LLM-based methods fail to account for the complex and dynamic interactions among the human agents involved in coding, such as patients, physicians, and coders, and they lack interpretability and reliability. In this paper, we present a novel multi-agent method for ICD coding, which mimics the real-world coding process with five agents: a patient agent, a physician agent, a coder agent, a reviewer agent, and an adjuster agent. Each agent has a specific function and uses a LLM-based model to perform it. We evaluate our method on the MIMIC-III dataset and show that our proposed multi-agent coding framework substantially improves performance on both common and rare codes compared to Zero-shot Chain of Thought (CoT) prompting and self-consistency with CoT. The ablation study confirms the proposed agent roles' efficacy. Our method also matches the state-of-the-art ICD coding methods that require pre-training or fine-tuning, in terms of coding accuracy, rare code accuracy, and explainability.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展示出令人印象深刻的多样化能力，可使各个领域受益，例如无需经过特定领域的训练即可从临床文本中提取零样本和少样本信息。然而，对于 ICD 编码任务，由于 ICD 代码的高维和偏斜分布，它们通常会对关键细节产生幻觉并产生高召回率但低准确率的结果。现有的基于 LLM 的方法无法解释参与编码的人类代理（例如患者、医生和编码员）之间复杂而动态的交互，而且它们缺乏可解释性和可靠性。在本文中，我们提出了一种新颖的 ICD 编码多代理方法，它通过五个代理模拟现实世界的编码过程：患者代理、医生代理、编码员代理、审阅者代理和调整者代理。每个代理都有特定的功能，并使用基于 LLM 的模型来执行它。我们在 MIMIC-III 数据集上评估了我们的方法，并表明与零样本思维链 (CoT) 提示和 CoT 自洽相比，我们提出的多智能体编码框架显著提高了常见和罕见代码的性能。消融研究证实了所提出的智能体角色的有效性。在编码准确性、罕见代码准确性和可解释性方面，我们的方法也与需要预训练或微调的最先进的 ICD 编码方法相匹配。</li>
</ul>

<h3>Title: Title:
          Investigating the Robustness of LLMs on Math Word Problems</h3>
<ul>
<li><strong>Authors: </strong>Ujjwala Anantheswaran, Himanshu Gupta, Kevin Scaria, Shreyas Verma, Chitta Baral, Swaroop Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Investigating the Robustness of LLMs on Math Word Problems(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at various tasks, including solving math word problems (MWPs), but struggle with real-world problems containing irrelevant information. To address this, we propose a prompting framework that generates adversarial variants of MWPs by adding irrelevant variables. We introduce a dataset, ProbleMATHIC, containing both adversarial and non-adversarial MWPs. Our experiments reveal that LLMs are susceptible to distraction by numerical noise, resulting in an average relative performance drop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2, Mistral) on the adversarial samples from our dataset. Fine-tuning on adversarial training instances improves performance on adversarial MWPs by ~8%, indicating increased robustness to noise and better ability to identify relevant data for reasoning. Finally, to assess the generalizability of our prompting framework, we introduce GSM-8K-Adv, an adversarial variant of the GSM-8K benchmark. LLMs continue to struggle when faced with adversarial information, reducing performance by up to ~6%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 擅长各种任务，包括解决数学应用题 (MWP)，但在解决包含不相关信息的现实问题时却举步维艰。为了解决这个问题，我们提出了一个提示框架，通过添加不相关的变量来生成 MWP 的对抗变体。我们引入了一个数据集 ProbleMATHIC，其中包含对抗和非对抗 MWP。我们的实验表明，LLM 容易受到数值噪声的干扰，导致对抗 MWP 的平均相对性能下降约 26%。为了缓解这种情况，我们对来自我们数据集的对抗样本的 LLM（Llama-2、Mistral）进行了微调。对抗训练实例的微调可将对抗 MWP 的性能提高约 8%，表明对噪声的鲁棒性增强，并且能够更好地识别相关数据以进行推理。最后，为了评估我们的提示框架的通用性，我们引入了 GSM-8K-Adv，这是 GSM-8K 基准的对抗变体。面对对抗信息时，LLM 仍然举步维艰，性能下降高达约 6%。</li>
</ul>

<h3>Title: Title:
          RadEx: A Framework for Structured Information Extraction from Radiology Reports based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel Reichenpfader, Jonas Knupp, André Sander, Kerstin Denecke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RadEx: A Framework for Structured Information Extraction from Radiology Reports based on Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Annually and globally, over three billion radiography examinations and computer tomography scans result in mostly unstructured radiology reports containing free text. Despite the potential benefits of structured reporting, its adoption is limited by factors such as established processes, resource constraints and potential loss of information. However, structured information would be necessary for various use cases, including automatic analysis, clinical trial matching, and prediction of health outcomes. This study introduces RadEx, an end-to-end framework comprising 15 software components and ten artifacts to develop systems that perform automated information extraction from radiology reports. It covers the complete process from annotating training data to extracting information by offering a consistent generic information model and setting boundaries for model development. Specifically, RadEx allows clinicians to define relevant information for clinical domains (e.g., mammography) and to create report templates. The framework supports both generative and encoder-only models and the decoupling of information extraction from template filling enables independent model improvements. Developing information extraction systems according to the RadEx framework facilitates implementation and maintenance as components are easily exchangeable, while standardized artifacts ensure interoperability between components.</li>
<li><strong>摘要：</strong>每年，全球有超过 30 亿次放射检查和计算机断层扫描，这些检查产生的放射学报告大多是非结构化的，包含自由文本。尽管结构化报告具有潜在优势，但其采用受到既定流程、资源限制和潜在信息丢失等因素的限制。然而，结构化信息对于各种用例都是必要的，包括自动分析、临床试验匹配和健康结果预测。本研究介绍了 RadEx，这是一个端到端框架，由 15 个软件组件和 10 个工件组成，用于开发从放射学报告中自动提取信息的系统。它涵盖了从注释训练数据到提取信息的完整过程，提供了一致的通用信息模型并为模型开发设定了界限。具体而言，RadEx 允许临床医生定义临床领域（例如乳房 X 线照相术）的相关信息并创建报告模板。该框架支持生成模型和仅编码器模型，并且将信息提取与模板填充分离可以实现独立的模型改进。根据 RadEx 框架开发信息提取系统有利于实施和维护，因为组件易于交换，而标准化工件确保组件之间的互操作性。</li>
</ul>

<h3>Title: Title:
          Reasoning or Simply Next Token Prediction? A Benchmark for Stress-Testing Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wentian Wang, Paul Kantor, Jacob Feldman, Lazaros Gallos, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Reasoning or Simply Next Token Prediction? A Benchmark for Stress-Testing Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>We propose MMLU-SR, a novel dataset designed to measure the true comprehension abilities of Large Language Models (LLMs) by challenging their performance in question-answering tasks with modified terms. We reasoned that an agent that ``truly'' understands a concept can still evaluate it when key terms are replaced by suitably defined alternate terms, and sought to differentiate such comprehension from mere text replacement. In our study, we modified standardized test questions by replacing a key term with a dummy word along with its definition. The key term could be in the context of questions, answers, or both questions and answers. Notwithstanding the high scores achieved by recent popular LLMs on the MMLU leaderboard, we found a substantial reduction in model performance after such replacement, suggesting poor comprehension. This new benchmark provides a rigorous benchmark for testing true model comprehension, and poses a challenge to the broader scientific community.</li>
<li><strong>摘要：</strong>我们提出了 MMLU-SR，这是一种新颖的数据集，旨在通过用修改后的术语挑战大型语言模型 (LLM) 在问答任务中的表现来衡量大型语言模型 (LLM) 的真实理解能力。我们推断，当关键词被适当定义的替代词替换时，“真正”理解概念的代理仍然可以对其进行评估，并试图将这种理解与单纯的文本替换区分开来。在我们的研究中，我们修改了标准化测试问题，用虚拟词替换关键词及其定义。关键词可以出现在问题、答案或问题和答案的上下文中。尽管最近流行的 LLM 在 MMLU 排行榜上取得了高分，但我们发现在进行这种替换后模型性能大幅下降，表明理解能力较差。这个新的基准为测试真正的模型理解能力提供了一个严格的基准，并对更广泛的科学界提出了挑战。</li>
</ul>

<h3>Title: Title:
          Mental Disorder Classification via Temporal Representation of Text</h3>
<ul>
<li><strong>Authors: </strong>Raja Kumar, Kishan Maharaj, Ashita Saxena, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mental Disorder Classification via Temporal Representation of Text(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mental disorders pose a global challenge, aggravated by the shortage of qualified mental health professionals. Mental disorder prediction from social media posts by current LLMs is challenging due to the complexities of sequential text data and the limited context length of language models. Current language model-based approaches split a single data instance into multiple chunks to compensate for limited context size. The predictive model is then applied to each chunk individually, and the most voted output is selected as the final prediction. This results in the loss of inter-post dependencies and important time variant information, leading to poor performance. We propose a novel framework which first compresses the large sequence of chronologically ordered social media posts into a series of numbers. We then use this time variant representation for mental disorder classification. We demonstrate the generalization capabilities of our framework by outperforming the current SOTA in three different mental conditions: depression, self-harm, and anorexia, with an absolute improvement of 5% in the F1 score. We investigate the situation where current data instances fall within the context length of language models and present empirical results highlighting the importance of temporal properties of textual data. Furthermore, we utilize the proposed framework for a cross-domain study, exploring commonalities across disorders and the possibility of inter-domain data usage.</li>
<li><strong>摘要：</strong>精神障碍是全球性挑战，合格精神卫生专业人员的短缺加剧了这一挑战。由于连续文本数据的复杂性和语言模型的上下文长度有限，当前法学硕士从社交媒体帖子中预测精神障碍具有挑战性。当前基于语言模型的方法将单个数据实例拆分为多个块，以弥补有限的上下文大小。然后将预测模型单独应用于每个块，并选择得票最多的输出作为最终预测。这会导致帖子间依赖性和重要的时变信息的丢失，从而导致性能不佳。我们提出了一个新颖的框架，首先将按时间顺序排列的大量社交媒体帖子压缩为一系列数字。然后，我们使用这种时变表示进行精神障碍分类。我们通过在三种不同的精神状况（抑郁、自残和厌食症）中超越当前的 SOTA 来展示我们框架的泛化能力，F1 分数绝对提高了 5%。我们研究了当前数据实例在语言模型的上下文长度范围内的情况，并提出了经验结果，强调了文本数据的时间属性的重要性。此外，我们利用提出的框架进行跨领域研究，探索不同疾病之间的共性以及跨领域数据使用的可能性。</li>
</ul>

<h3>Title: Title:
          Improving Large Models with Small models: Lower Costs and Better Performance</h3>
<ul>
<li><strong>Authors: </strong>Dong Chen, Shuo Zhang, Yueting Zhuang, Siliang Tang, Qidong Liu, Hua Wang, Mingliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Improving Large Models with Small models: Lower Costs and Better Performance(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>Pretrained large models (PLMs), such as ChatGPT, have demonstrated remarkable performance across diverse tasks. However, the significant computational requirements of PLMs have discouraged most product teams from running or fine-tuning them. In such cases, to harness the exceptional performance of PLMs, one must rely on expensive APIs, thereby exacerbating the economic burden. Despite the overall inferior performance of small models, in specific distributions, they can achieve comparable or even superior results. Consequently, some input can be processed exclusively by small models. On the other hand, certain tasks can be broken down into multiple subtasks, some of which can be completed without powerful capabilities. Under these circumstances, small models can handle the simple subtasks, allowing large models to focus on challenging subtasks, thus improving the performance. We propose Data Shunt$^+$ (DS$^+$), a general paradigm for collaboration of small and large models. DS$^+$ not only substantially reduces the cost associated with querying large models but also effectively improves large models' performance. For instance, ChatGPT achieves an accuracy of $94.43\%$ on Amazon Product sentiment analysis, and DS$^+$ achieves an accuracy of $95.64\%$, while the cost has been reduced to only $31.18\%$. Besides, experiments also prove that the proposed collaborative-based paradigm can better inject specific task knowledge into PLMs compared to fine-tuning.</li>
<li><strong>摘要：</strong>预训练大型模型 (PLM)，例如 ChatGPT，在各种任务中都表现出色。然而，PLM 的大量计算要求使大多数产品团队不愿运行或微调它们。在这种情况下，要利用 PLM 的卓越性能，必须依赖昂贵的 API，从而加剧了经济负担。尽管小模型的整体性能较差，但在特定分布中，它们可以实现相当甚至更好的结果。因此，一些输入可以完全由小模型处理。另一方面，某些任务可以分解为多个子任务，其中一些子任务无需强大的功能即可完成。在这种情况下，小模型可以处理简单的子任务，让大模型专注于具有挑战性的子任务，从而提高性能。我们提出了数据分流 $^+$ (DS$^+$)，这是小模型和大模型协作的通用范例。DS$^+$ 不仅大大降低了查询大模型的成本，而且有效提高了大模型的性能。例如，ChatGPT 在亚马逊产品情绪分析中实现了 94.43% 的准确率，DS$^+$ 实现了 95.64% 的准确率，而成本已降至仅 31.18%。此外，实验还证明，与微调相比，所提出的基于协作的范式可以更好地将特定任务知识注入 PLM。</li>
</ul>

<h3>Title: Title:
          Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Bonlarron, Jean-Charles Régin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Intertwining CP and NLP: The Generation of Unreasonably Constrained Sentences(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Constrained text generation remains a challenging task, particularly when dealing with hard constraints. Traditional Natural Language Processing (NLP) approaches prioritize generating meaningful and coherent output. Also, the current state-of-the-art methods often lack the expressiveness and constraint satisfaction capabilities to handle such tasks effectively. This paper presents the Constraints First Framework to remedy this issue. This framework considers a constrained text generation problem as a discrete combinatorial optimization problem. It is solved by a constraint programming method that combines linguistic properties (e.g., n-grams or language level) with other more classical constraints (e.g., the number of characters, syllables, or words). Eventually, a curation phase allows for selecting the best-generated sentences according to perplexity using a large language model. The effectiveness of this approach is demonstrated by tackling a new more tediously constrained text generation problem: the iconic RADNER sentences problem. This problem aims to generate sentences respecting a set of quite strict rules defined by their use in vision and clinical research. Thanks to our CP-based approach, many new strongly constrained sentences have been successfully generated in an automatic manner. This highlights the potential of our approach to handle unreasonably constrained text generation scenarios.</li>
<li><strong>摘要：</strong>受限文本生成仍然是一项具有挑战性的任务，尤其是在处理严格约束时。传统的自然语言处理 (NLP) 方法优先生成有意义且连贯的输出。此外，当前最先进的方法通常缺乏表达能力和约束满足能力，无法有效地处理此类任务。本文提出了约束优先框架来解决这个问题。该框架将受限文本生成问题视为离散组合优化问题。它通过约束编程方法解决，该方法结合了语言属性（例如，n-gram 或语言级别）与其他更经典的约束（例如，字符、音节或单词的数量）。最终，策展阶段允许使用大型语言模型根据困惑度选择最佳生成的句子。通过解决一个新的更繁琐的受限文本生成问题：标志性的 RADNER 句子问题，证明了这种方法的有效性。这个问题旨在生成遵守一组非常严格的规则的句子，这些规则由它们在视觉和临床研究中的使用定义。得益于我们基于 CP 的方法，许多新的强约束句子已成功自动生成。这凸显了我们的方法在处理不合理约束的文本生成场景方面的潜力。</li>
</ul>

<h3>Title: Title:
          CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics</h3>
<ul>
<li><strong>Authors: </strong>Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.</li>
<li><strong>摘要：</strong>在危机/灾难信息学领域，社交媒体越来越多地被用于提高态势感知，为响应和救援工作提供信息。高效准确的文本分类工具一直是危机信息学研究的重点领域。然而，当前的方法大多依赖于单标签文本分类模型，无法捕捉到动态和多方面的灾难相关社交媒体数据中嵌入的不同见解。本研究通过针对灾难相关推文的多标签分类的指令微调增强了预训练的大型语言模型 (LLM)，引入了一种灾难文本分类的新方法。我们的方法包括从灾难相关推文中创建一个综合指令数据集，然后用它来微调开源 LLM，从而将特定于灾难的知识嵌入其中。这个经过微调的模型可以同时对灾难相关信息的多个方面进行分类，例如事件类型、信息量和人为援助的参与度，从而显著提高社交媒体数据在灾难态势感知中的效用。结果表明，这种方法可以增强社交媒体帖子中关键信息的分类，从而有助于在紧急情况下更有效地部署态势感知。这项研究为更先进、适应性更强、更强大的灾害管理工具铺平了道路，利用 LLM 的功能来改善灾害场景中的实时态势感知和响应策略。</li>
</ul>

<h3>Title: Title:
          Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $12$ datasets for both discriminative and generative tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. (Our implementation is available in this https URL.)</li>
<li><strong>摘要：</strong>在大型语言模型时代，模型合并是一种很有前途的方法，无需额外训练即可将多个特定任务模型组合成一个多任务模型。然而，仍然存在两个挑战：（a）不同模型之间的干扰和（b）测试期间的异构数据。由于这些问题，传统的模型合并方法与微调模型相比通常表现出明显的性能差距。此外，一刀切的模型缺乏对各种测试数据的灵活性，导致性能下降。我们表明，共享和独有的任务特定知识对于合并性能都至关重要，但直接合并独有知识会阻碍整体性能。鉴于此，我们提出了 Twin-Merging，这种方法包含两个主要阶段：（1）将知识模块化为共享和独有的组件，并通过压缩来减少冗余并提高效率；（2）根据输入动态合并共享和特定任务的知识。这种方法缩小了合并和微调模型之间的性能差距，并提高了对异构数据的适应性。在判别任务和生成任务的 12 个数据集上进行的大量实验证明了我们方法的有效性，判别任务的绝对归一化分数平均提高了 28.34%，甚至超过了生成任务的微调上限。（我们的实现可在此 https URL 中找到。）</li>
</ul>

<h3>Title: Title:
          On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Fan, Zhenyi Lu, Wei Wei, Jie Tian, Xiaoye Qu, Dangyang Chen, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Efficient fine-tuning of large language models for task-specific applications is imperative, yet the vast number of parameters in these models makes their training increasingly challenging. Despite numerous proposals for effective methods, a substantial memory overhead remains for gradient computations during updates. \thm{Can we fine-tune a series of task-specific small models and transfer their knowledge directly to a much larger model without additional training?} In this paper, we explore weak-to-strong specialization using logit arithmetic, facilitating a direct answer to this question. Existing weak-to-strong methods often employ a static knowledge transfer ratio and a single small model for transferring complex knowledge, which leads to suboptimal performance. % To address this, To surmount these limitations, we propose a dynamic logit fusion approach that works with a series of task-specific small models, each specialized in a different task. This method adaptively allocates weights among these models at each decoding step, learning the weights through Kullback-Leibler divergence constrained optimization problems. We conduct extensive experiments across various benchmarks in both single-task and multi-task settings, achieving leading results. By transferring expertise from the 7B model to the 13B model, our method closes the performance gap by 96.4\% in single-task scenarios and by 86.3\% in multi-task scenarios compared to full fine-tuning of the 13B model. Notably, we achieve surpassing performance on unseen tasks. Moreover, we further demonstrate that our method can effortlessly integrate in-context learning for single tasks and task arithmetic for multi-task scenarios. (Our implementation is available in this https URL.)</li>
<li><strong>摘要：</strong>针对特定任务的应用，对大型语言模型进行有效的微调势在必行，但这些模型中大量的参数使得它们的训练越来越具有挑战性。尽管提出了许多有效的方法，但在更新过程中梯度计算仍然会产生大量的内存开销。\thm{我们能否微调一系列特定于任务的小模型，并将其知识直接转移到更大的模型而无需额外的训练？}在本文中，我们使用 logit 算法探索从弱到强的专业化，从而直接回答这个问题。现有的从弱到强的方法通常采用静态知识转移率和单个小模型来转移复杂知识，这会导致性能不佳。% 为了解决这个问题，为了克服这些限制，我们提出了一种动态 logit 融合方法，该方法适用于一系列特定于任务的小模型，每个模型都专门用于不同的任务。该方法在每个解码步骤中自适应地在这些模型之间分配权重，通过 Kullback-Leibler 散度约束优化问题学习权重。我们在单任务和多任务设置的各种基准上进行了广泛的实验，取得了领先的成果。通过将专业知识从 7B 模型转移到 13B 模型，与完全微调 13B 模型相比，我们的方法在单任务场景中缩小了 96.4% 的性能差距，在多任务场景中缩小了 86.3% 的性能差距。值得注意的是，我们在看不见的任务上取得了卓越的表现。此外，我们进一步证明，我们的方法可以毫不费力地集成单任务的上下文学习和多任务场景的任务算法。（我们的实现可在此 https URL 中找到。）</li>
</ul>

<h3>Title: Title:
          Duplicate Detection with GenAI</h3>
<ul>
<li><strong>Authors: </strong>Ian Ormesher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Duplicate Detection with GenAI(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Customer data is often stored as records in Customer Relations Management systems (CRMs). Data which is manually entered into such systems by one of more users over time leads to data replication, partial duplication or fuzzy duplication. This in turn means that there no longer a single source of truth for customers, contacts, accounts, etc. Downstream business processes become increasing complex and contrived without a unique mapping between a record in a CRM and the target customer. Current methods to detect and de-duplicate records use traditional Natural Language Processing techniques known as Entity Matching. In this paper we show how using the latest advancements in Large Language Models and Generative AI can vastly improve the identification and repair of duplicated records. On common benchmark datasets we find an improvement in the accuracy of data de-duplication rates from 30 percent using NLP techniques to almost 60 percent using our proposed method.</li>
<li><strong>摘要：</strong>客户数据通常以记录形式存储在客户关系管理系统 (CRM) 中。随着时间的推移，一个或多个用户手动输入此类系统的数据会导致数据重复、部分重复或模糊重复。这反过来意味着客户、联系人、帐户等不再有单一的真实来源。如果 CRM 中的记录与目标客户之间没有唯一的映射，下游业务流程将变得越来越复杂和做作。当前检测和删除重复记录的方法使用称为实体匹配的传统自然语言处理技术。在本文中，我们展示了如何使用大型语言模型和生成式 AI 的最新进展来极大地提高重复记录的识别和修复。在常见的基准数据集上，我们发现数据删除重复的准确率从使用 NLP 技术的 30% 提高到使用我们提出的方法的近 60%。</li>
</ul>

<h3>Title: Title:
          JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ze Wang, Zekun Wu, Xin Guan, Michael Thaler, Adriano Koshiyama, Skylar Lu, Sachin Beepath, Ediz Ertekin Jr., Maria Perez-Ortiz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This paper presents a novel framework for benchmarking hierarchical gender hiring bias in Large Language Models (LLMs) for resume scoring, revealing significant issues of reverse bias and overdebiasing. Our contributions are fourfold: First, we introduce a framework using a real, anonymized resume dataset from the Healthcare, Finance, and Construction industries, meticulously used to avoid confounding factors. It evaluates gender hiring biases across hierarchical levels, including Level bias, Spread bias, Taste-based bias, and Statistical bias. This framework can be generalized to other social traits and tasks easily. Second, we propose novel statistical and computational hiring bias metrics based on a counterfactual approach, including Rank After Scoring (RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed Effects Model-based Metrics. These metrics, rooted in labor economics, NLP, and law, enable holistic evaluation of hiring biases. Third, we analyze hiring biases in ten state-of-the-art LLMs. Six out of ten LLMs show significant biases against males in healthcare and finance. An industry-effect regression reveals that the healthcare industry is the most biased against males. GPT-4o and GPT-3.5 are the most biased models, showing significant bias in all three industries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and Llama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except for Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of random expansion or reduction of resume content. Finally, we offer a user-friendly demo to facilitate adoption and practical application of the framework.</li>
<li><strong>摘要：</strong>本文提出了一个新颖的框架，用于对大型语言模型 (LLM) 中的分层性别招聘偏见进行基准测试，以进行简历评分，揭示了反向偏见和过度去偏见的重大问题。我们的贡献有四个方面：首先，我们引入了一个框架，该框架使用来自医疗保健、金融和建筑行业的真实匿名简历数据集，并精心使用以避免混杂因素。它评估了各个层级的性别招聘偏见，包括级别偏见、传播偏见、基于品味的偏见和统计偏见。该框架可以轻松推广到其他社交特征和任务。其次，我们基于反事实方法提出了新颖的统​​计和计算招聘偏见指标，包括评分后排名 (RAS)、基于排名的影响率、基于排列测试的指标和基于固定效应模型的指标。这些指标植根于劳动经济学、NLP 和法律，可以对招聘偏见进行整体评估。第三，我们分析了十个最先进的 LLM 中的招聘偏见。十分之六的 LLM 在医疗和金融领域表现出对男性的明显偏见。行业效应回归显示，医疗行业对男性的偏见最为明显。GPT-4o 和 GPT-3.5 是偏见最明显的模型，在这三个行业中都表现出明显的偏见。相反，Gemini-1.5-Pro、Llama3-8b-Instruct 和 Llama3-70b-Instruct 的偏见最小。除 Llama3-8b-Instruct 和 Claude-3-Sonnet 外，所有 LLM 的招聘偏见无论简历内容如何随机扩展或减少，都保持一致。最后，我们提供了一个用户友好的演示，以方便采用和实际应用该框架。</li>
</ul>

<h3>Title: Title:
          Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Xiao Chuanfu, Xingcheng Zhang, Dahua Lin, Chao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared with FlashAttention.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 现在支持极长的上下文窗口，但 vanilla 注意力的二次复杂度会导致显著较长的首次标记时间 (TTFT) 延迟。现有的解决这种复杂性的方法需要额外的预训练或微调，并且通常会牺牲模型准确性。在本文中，我们首先为近无损稀疏注意力提供理论和经验基础。我们发现在运行时以低开销动态捕获特定于头部的稀疏模式至关重要。为了解决这个问题，我们提出了 SampleAttention，一种自适应结构化和近无损的稀疏注意力。利用观察到的显著稀疏模式，SampleAttention 关注固定百分比的相邻标记以捕获局部窗口模式，并采用两阶段查询引导的键值过滤方法，该方法自适应地选择一组最小的键值，开销低，以捕获列条纹模式。综合评估表明，SampleAttention 可以无缝替代现成的 LLM 中的 vanilla Attention，且几乎没有准确度损失，并且与 FlashAttention 相比，TTFT 减少了高达 $2.42\times$。</li>
</ul>

<h3>Title: Title:
          Improving Text-To-Audio Models with Synthetic Captions</h3>
<ul>
<li><strong>Authors: </strong>Zhifeng Kong, Sang-gil Lee, Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Rafael Valle, Soujanya Poria, Bryan Catanzaro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Improving Text-To-Audio Models with Synthetic Captions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>It is an open challenge to obtain high quality training data, especially captions, for text-to-audio models. Although prior methods have leveraged \textit{text-only language models} to augment and improve captions, such methods have limitations related to scale and coherence between audio and captions. In this work, we propose an audio captioning pipeline that uses an \textit{audio language model} to synthesize accurate and diverse captions for audio at scale. We leverage this pipeline to produce a dataset of synthetic captions for AudioSet, named \texttt{AF-AudioSet}, and then evaluate the benefit of pre-training text-to-audio models on these synthetic captions. Through systematic evaluations on AudioCaps and MusicCaps, we find leveraging our pipeline and synthetic captions leads to significant improvements on audio generation quality, achieving a new \textit{state-of-the-art}.</li>
<li><strong>摘要：</strong>对于文本转音频模型来说，获取高质量的训练数据（尤其是字幕）是一个开放的挑战。虽然之前的方法已经利用 \textit{纯文本语言模型} 来增强和改进字幕，但这些方法在音频和字幕之间的规模和连贯性方面存在局限性。在这项工作中，我们提出了一种音频字幕管道，该管道使用 \textit{音频语言模型} 来大规模合成准确且多样化的音频字幕。我们利用这个管道为 AudioSet 生成一个名为 \texttt{AF-AudioSet} 的合成字幕数据集，然后评估在这些合成字幕上预训练文本转音频模型的好处。通过对 AudioCaps 和 MusicCaps 的系统评估，我们发现利用我们的管道和合成字幕可以显着提高音频生成质量，实现新的 \textit{最新}。</li>
</ul>

<h3>Title: Title:
          Dr.E Bridges Graphs with Large Language Models through Words</h3>
<ul>
<li><strong>Authors: </strong>Zipeng Liu, Likang Wu, Ming He, Zhong Guan, Hongke Zhao, Nan Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Dr.E Bridges Graphs with Large Language Models through Words(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Significant efforts have been directed toward integrating powerful Large Language Models (LLMs) with diverse modalities, particularly focusing on the fusion of vision, language, and audio data. However, the graph-structured data, inherently rich in structural and domain-specific knowledge, have not yet been gracefully adapted to LLMs. Existing methods either describe the graph with raw text, suffering the loss of graph structural information, or feed Graph Neural Network (GNN) embeddings directly into LLM at the cost of losing semantic representation. To bridge this gap, we introduce an innovative, end-to-end modality-aligning framework, equipped with a pretrained Dual-Residual Vector Quantized-Variational AutoEncoder (Dr.E). This framework is specifically designed to facilitate token-level alignment with LLMs, enabling an effective translation of the intrinsic `language' of graphs into comprehensible natural language. Our experimental evaluations on standard GNN node classification tasks demonstrate competitive performance against other state-of-the-art approaches. Additionally, our framework ensures interpretability, efficiency, and robustness, with its effectiveness further validated under both fine-tuning and few-shot settings. This study marks the first successful endeavor to achieve token-level alignment between GNNs and LLMs.</li>
<li><strong>摘要：</strong>人们付出了巨大的努力，致力于将强大的大型语言模型 (LLM) 与多种模态集成，尤其专注于视觉、语言和音频数据的融合。然而，图结构数据本身就富含结构和领域特定知识，尚未很好地适应 LLM。现有方法要么用原始文本描述图，从而导致图结构信息的丢失，要么将图神经网络 (GNN) 嵌入直接输入 LLM，代价是丢失语义表示。为了弥补这一差距，我们引入了一个创新的端到端模态对齐框架，配备了预训练的双残差向量量化变分自动编码器 (Dr.E)。该框架专门设计用于促进与 LLM 的标记级对齐，从而能够有效地将图的内在“语言”转换为可理解的自然语言。我们对标准 GNN 节点分类任务的实验评估表明，与其他最先进的方法相比，它具有竞争力。此外，我们的框架确保了可解释性、效率和稳健性，其有效性在微调和少样本设置下得到进一步验证。这项研究标志着首次成功实现 GNN 和 LLM 之间的 token 级对齐。</li>
</ul>

<h3>Title: Title:
          Steering Without Side Effects: Improving Post-Deployment Control of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Asa Cooper Stickland, Alexander Lyzhov, Jacob Pfau, Salsabila Mahdi, Samuel R. Bowman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Steering Without Side Effects: Improving Post-Deployment Control of Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have been shown to behave unexpectedly post-deployment. For example, new jailbreaks continually arise, allowing model misuse, despite extensive red-teaming and adversarial training from developers. Given most model queries are unproblematic and frequent retraining results in unstable user experience, methods for mitigation of worst-case behavior should be targeted. One such method is classifying inputs as potentially problematic, then selectively applying steering vectors on these problematic inputs, i.e. adding particular vectors to model hidden states. However, steering vectors can also negatively affect model performance, which will be an issue on cases where the classifier was incorrect. We present KL-then-steer (KTS), a technique that decreases the side effects of steering while retaining its benefits, by first training a model to minimize Kullback-Leibler (KL) divergence between a steered and unsteered model on benign inputs, then steering the model that has undergone this training. Our best method prevents 44% of jailbreak attacks compared to the original Llama-2-chat-7B model while maintaining helpfulness (as measured by MT-Bench) on benign requests almost on par with the original LM. To demonstrate the generality and transferability of our method beyond jailbreaks, we show that our KTS model can be steered to reduce bias towards user-suggested answers on TruthfulQA. Code is available: this https URL.</li>
<li><strong>摘要：</strong>语言模型 (LM) 在部署后表现出意想不到的行为。例如，尽管开发人员进行了广泛的红队和对抗训练，但新的越狱不断出现，导致模型被滥用。鉴于大多数模型查询都没有问题，频繁的重新训练会导致用户体验不稳定，因此应该有针对性地采取缓解最坏情况行为的方法。一种方法是将输入分类为潜在问题，然后有选择地对这些有问题的输入应用转向向量，即向模型隐藏状态添加特定向量。然而，转向向量也会对模型性能产生负面影响，这在分类器不正确的情况下会成为一个问题。我们提出了 KL-then-steer (KTS)，这是一种减少转向副作用同时保留其优点的技术，通过首先训练模型以最小化良性输入上转向和未转向模型之间的 Kullback-Leibler (KL) 差异，然后转向经过这种训练的模型。与原始的 Llama-2-chat-7B 模型相比，我们的最佳方法可防止 44% 的越狱攻击，同时保持对良性请求的帮助（以 MT-Bench 衡量）几乎与原始 LM 相当。为了证明我们的方法在越狱之外的通用性和可转移性，我们展示了我们的 KTS 模型可以减少对 TruthfulQA 上用户建议答案的偏见。代码可用：此 https URL。</li>
</ul>

<h3>Title: Title:
          Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization</h3>
<ul>
<li><strong>Authors: </strong>Sungbin Shin, Wonpyo Park, Jaeho Lee, Namhoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work suggests fundamentally rethinking the current practice of pruning large language models (LLMs). The way it is done is by divide and conquer: split the model into submodels, sequentially prune them, and reconstruct predictions of the dense counterparts on small calibration data one at a time; the final model is obtained simply by putting the resulting sparse submodels together. While this approach enables pruning under memory constraints, it generates high reconstruction errors. In this work, we first present an array of reconstruction techniques that can significantly reduce this error by more than $90\%$. Unwittingly, however, we discover that minimizing reconstruction error is not always ideal and can overfit the given calibration data, resulting in rather increased language perplexity and poor performance at downstream tasks. We find out that a strategy of self-generating calibration data can mitigate this trade-off between reconstruction and generalization, suggesting new directions in the presence of both benefits and pitfalls of reconstruction for pruning LLMs.</li>
<li><strong>摘要：</strong>这项研究表明，人们应该从根本上重新思考当前大型语言模型 (LLM) 的修剪实践。修剪方法是分而治之：将模型拆分为子模型，依次修剪它们，然后逐个重建密集对应模型在小型校准数据上的预测；只需将生成的稀疏子模型放在一起，即可获得最终模型。虽然这种方法可以在内存限制下进行修剪，但它会产生较高的重建误差。在这项研究中，我们首先介绍了一系列重建技术，这些技术可以显著减少这种误差超过 90%$。然而，我们无意中发现，最小化重建误差并不总是理想的，可能会过度拟合给定的校准数据，导致语言困惑度增加，下游任务的性能不佳。我们发现，自生成校准数据的策略可以缓解重建和泛化之间的这种权衡，这为 LLM 修剪重建的优缺点提供了新的方向。</li>
</ul>

<h3>Title: Title:
          Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods</h3>
<ul>
<li><strong>Authors: </strong>Kathleen C. Fraser, Hillary Dawkins, Svetlana Kiritchenko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize state-of-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how "detectable" AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经发展到连人类都难以辨别文本是由另一个人还是计算机生成的地步。然而，知道文本是由人类还是人工智能 (AI) 生成的对于确定其可信度非常重要，并且在许多领域都有应用，包括检测欺诈和学术不诚实，以及打击错误信息和政治宣传的传播。因此，人工智能生成文本 (AIGT) 检测任务既非常具有挑战性，又至关重要。在本次调查中，我们总结了 AIGT 检测的最新方法，包括水印、统计和风格分析以及机器学习分类。我们还提供了有关此任务的现有数据集的信息。综合研究结果，我们旨在深入了解在不同情况下决定 AIGT 文本“可检测性”的显著因素，并为未来应对这一重大技术和社会挑战的工作提出切实可行的建议。</li>
</ul>

<h3>Title: Title:
          TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris Callison-Burch, Kathleen McKeown, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>The goal of text style transfer is to transform the style of texts while preserving their original meaning, often with only a few examples of the target style. Existing style transfer methods generally rely on the few-shot capabilities of large language models or on complex controllable text generation approaches that are inefficient and underperform on fluency metrics. We introduce TinyStyler, a lightweight but effective approach, which leverages a small language model (800M params) and pre-trained authorship embeddings to perform efficient, few-shot text style transfer. We evaluate on the challenging task of authorship style transfer and find TinyStyler outperforms strong approaches such as GPT-4. We also evaluate TinyStyler's ability to perform text attribute style transfer (formal $\leftrightarrow$ informal) with automatic and human evaluations and find that the approach outperforms recent controllable text generation methods. Our model has been made publicly available at this https URL .</li>
<li><strong>摘要：</strong>文本风格转换的目标是在保留文本原意的同时转换文本风格，通常仅使用目标风格的几个示例。现有的风格转换方法通常依赖于大型语言模型的少样本能力或复杂的可控文本生成方法，这些方法效率低下且流畅度指标不佳。我们介绍了一种轻量级但有效的方法 TinyStyler，它利用小型语言模型（8 亿个参数）和预先训练的作者嵌入来执行高效的少样本文本风格转换。我们对作者风格转换这项具有挑战性的任务进行了评估，发现 TinyStyler 的表现优于 GPT-4 等强大的方法。我们还通过自动和人工评估评估了 TinyStyler 执行文本属性风格转换（正式 $\leftrightarrow$ 非正式）的能力，发现该方法优于最近的可控文本生成方法。我们的模型已在此 https URL 上公开发布。</li>
</ul>

<h3>Title: Title:
          News Deja Vu: Connecting Past and Present with Semantic Search</h3>
<ul>
<li><strong>Authors: </strong>Brevin Franklin, Emily Silcock, Abhishek Arora, Tom Bryan, Melissa Dell</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          News Deja Vu: Connecting Past and Present with Semantic Search(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Social scientists and the general public often analyze contemporary events by drawing parallels with the past, a process complicated by the vast, noisy, and unstructured nature of historical texts. For example, hundreds of millions of page scans from historical newspapers have been noisily transcribed. Traditional sparse methods for searching for relevant material in these vast corpora, e.g., with keywords, can be brittle given complex vocabularies and OCR noise. This study introduces News Deja Vu, a novel semantic search tool that leverages transformer large language models and a bi-encoder approach to identify historical news articles that are most similar to modern news queries. News Deja Vu first recognizes and masks entities, in order to focus on broader parallels rather than the specific named entities being discussed. Then, a contrastively trained, lightweight bi-encoder retrieves historical articles that are most similar semantically to a modern query, illustrating how phenomena that might seem unique to the present have varied historical precedents. Aimed at social scientists, the user-friendly News Deja Vu package is designed to be accessible for those who lack extensive familiarity with deep learning. It works with large text datasets, and we show how it can be deployed to a massive scale corpus of historical, open-source news articles. While human expertise remains important for drawing deeper insights, News Deja Vu provides a powerful tool for exploring parallels in how people have perceived past and present.</li>
<li><strong>摘要：</strong>社会科学家和公众经常通过与过去进行比较来分析当代事件，这一过程因历史文本庞大、嘈杂和非结构化的特性而变得复杂。例如，数亿页历史报纸的扫描件被嘈杂地转录。在这些庞大的语料库中搜索相关材料的传统稀疏方法（例如使用关键词）在词汇复杂和 OCR 噪声的情况下可能会变得脆弱。本研究介绍了 News Deja Vu，这是一种新颖的语义搜索工具，它利用 Transformer 大型语言模型和双编码器方法来识别与现代新闻查询最相似的历史新闻文章。News Deja Vu 首先识别和屏蔽实体，以便专注于更广泛的相似之处，而不是正在讨论的特定命名实体。然后，经过对比训练的轻量级双编码器检索在语义上与现代查询最相似的历史文章，说明看似独一无二但如今却有不同历史先例的现象。 News Deja Vu 软件包面向社会科学家，易于使用，旨在让那些对深度学习不太熟悉的人也能轻松使用。它适用于大型文本数据集，我们展示了如何将其部署到大规模历史开源新闻文章语料库中。虽然人类的专业知识对于获得更深入的见解仍然很重要，但 News Deja Vu 提供了一个强大的工具，可以探索人们对过去和现在的看法的相似之处。</li>
</ul>

<h3>Title: Title:
          Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem</h3>
<ul>
<li><strong>Authors: </strong>Sara Court, Micha Elsner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This work investigates the in-context learning abilities of pretrained large language models (LLMs) when instructed to translate text from a low-resource language into a high-resource language as part of an automated machine translation pipeline. We conduct a set of experiments translating Southern Quechua to Spanish and examine the informativity of various types of information retrieved from a constrained database of digitized pedagogical materials (dictionaries and grammar lessons) and parallel corpora. Using both automatic and human evaluation of model output, we conduct ablation studies that manipulate (1) context type (morpheme translations, grammar descriptions, and corpus examples), (2) retrieval methods (automated vs. manual), and (3) model type. Our results suggest that even relatively small LLMs are capable of utilizing prompt context for zero-shot low-resource translation when provided a minimally sufficient amount of relevant linguistic information. However, the variable effects of prompt type, retrieval method, model type, and language-specific factors highlight the limitations of using even the best LLMs as translation systems for the majority of the world's 7,000+ languages and their speakers.</li>
<li><strong>摘要：</strong>本研究调查了在自动机器翻译流程中，当预训练大型语言模型 (LLM) 被要求将文本从资源匮乏的语言翻译成资源丰富的语言时，其语境学习能力。我们进行了一系列实验，将南克丘亚语翻译成西班牙语，并检查从数字化教学材料（词典和语法课程）和平行语料库的受限数据库中检索到的各种信息的信息量。我们使用自动和人工两种方式对模型输出进行评估，进行消融研究，以操纵 (1) 语境类型（词素翻译、语法描述和语料库示例）、(2) 检索方法（自动与手动）和 (3) 模型类型。我们的结果表明，即使是相对较小的 LLM，在提供足够数量的相关语言信息时，也能够利用即时语境进行零样本低资源翻译。然而，提示类型、检索方法、模型类型和特定语言因素的不同影响凸显了即使是最好的 LLM 作为世界上 7,000 多种语言及其使用者中的大多数语言的翻译系统的局限性。</li>
</ul>

<h3>Title: Title:
          Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph</h3>
<ul>
<li><strong>Authors: </strong>Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Lyudmila Rvanova, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) is becoming increasingly recognized as a critical component of applications that rely on machine learning (ML). The rapid proliferation of large language models (LLMs) has stimulated researchers to seek efficient and effective approaches to UQ in text generation tasks, as in addition to their emerging capabilities, these models have introduced new challenges for building safe applications. As with other ML models, LLMs are prone to make incorrect predictions, ``hallucinate'' by fabricating claims, or simply generate low-quality output for a given input. UQ is a key element in dealing with these challenges. However research to date on UQ methods for LLMs has been fragmented, with disparate evaluation methods. In this work, we tackle this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines, and provides an environment for controllable and consistent evaluation of novel techniques by researchers in various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across nine tasks and shed light on the most promising approaches.</li>
<li><strong>摘要：</strong>不确定性量化 (UQ) 越来越被认为是依赖机器学习 (ML) 的应用程序的关键组件。大型语言模型 (LLM) 的快速普及促使研究人员寻求在文本生成任务中实现 UQ 的有效方法，因为除了新兴功能之外，这些模型还为构建安全的应用程序带来了新的挑战。与其他 ML 模型一样，LLM 容易做出错误的预测，通过捏造声明产生“幻觉”，或者只是为给定的输入生成低质量的输出。UQ 是应对这些挑战的关键要素。然而，迄今为止对 LLM 的 UQ 方法的研究是零散的，评估方法也各不相同。在这项工作中，我们通过引入一个新颖的基准来解决这个问题，该基准实现了一组最先进的 UQ 基线，并为研究人员在各种文本生成任务中可控且一致地评估新技术提供了一个环境。我们的基准还支持对置信度规范化方法提供可解释分数的能力的评估。利用我们的基准，我们对九项任务的 UQ 和规范化技术进行了大规模实证调查，并阐明了最有前景的方法。</li>
</ul>

<h3>Title: Title:
          Large Language Models have Intrinsic Self-Correction Ability</h3>
<ul>
<li><strong>Authors: </strong>Dancheng Liu, Amir Nassereldine, Ziming Yang, Chenhui Xu, Yuting Hu, Jiajie Li, Utkarsh Kumar, Changjae Lee, Jinjun Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models have Intrinsic Self-Correction Ability(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have attracted significant attention for their remarkable abilities in various natural language processing tasks, but they suffer from hallucinations that will cause performance degradation. One promising solution to improve the LLMs' performance is to ask LLMs to revise their answer after generation, a technique known as self-correction. Among the two types of self-correction, intrinsic self-correction is considered a promising direction because it does not utilize external knowledge. However, recent works doubt the validity of LLM's ability to conduct intrinsic self-correction. In this paper, we present a novel perspective on the intrinsic self-correction capabilities of LLMs through theoretical analyses and empirical experiments. In addition, we identify two critical factors for successful self-correction: zero temperature and fair prompts. Leveraging these factors, we demonstrate that intrinsic self-correction ability is exhibited across multiple existing LLMs. Our findings offer insights into the fundamental theories underlying the self-correction behavior of LLMs and remark on the importance of unbiased prompts and zero temperature settings in harnessing their full potential.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其在各种自然语言处理任务中的出色能力而备受关注，但它们存在会导致性能下降的幻觉。提高 LLM 性能的一个有希望的解决方案是要求 LLM 在生成后修改其答案，这种技术称为自我纠正。在两种自我纠正类型中，内在自我纠正被认为是一个有前途的方向，因为它不利用外部知识。然而，最近的研究对 LLM 进行内在自我纠正的能力的有效性提出了质疑。在本文中，我们通过理论分析和实证实验对 LLM 的内在自我纠正能力提出了新的视角。此外，我们确定了成功自我纠正的两个关键因素：零温和公平提示。利用这些因素，我们证明了内在的自我纠正能力在多个现有 LLM 中都得到了展现。我们的研究结果深入了解了 LLM 自我修正行为背后的基本理论，并强调了无偏提示和零温设置在充分发挥其潜力方面的重要性。</li>
</ul>

<h3>Title: Title:
          SS-Bench: A Benchmark for Social Story Generation and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yi Feng, Mingyang Song, Jiaqi Wang, Mao Zheng, Liping Jing, Jian Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SS-Bench: A Benchmark for Social Story Generation and Evaluation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Children with Autism Spectrum Disorder (ASD) often misunderstand social situations and struggle to participate in daily routines. Psychology experts write Social Stories under strict constraints of structural clarity, descriptive orientation, and situational safety to enhance their abilities in these regimes. However, Social Stories are costly in creation and often limited in diversity and timeliness. As Large Language Models (LLMs) become increasingly powerful, there is a growing need for more automated, affordable, and accessible methods to generate Social Stories in real-time with broad coverage. Adapting LLMs to meet the unique and strict constraints of Social Stories is a challenging issue. To this end, we propose \textbf{SS-Bench}, a \textbf{S}ocial \textbf{S}tory \textbf{Bench}mark for generating and evaluating Social Stories. Specifically, we develop a constraint-driven strategy named \textbf{\textsc{StarSow}} to hierarchically prompt LLMs to generate Social Stories and build a benchmark, which has been validated through experiments to fine-tune smaller models for generating qualified Social Stories. Additionally, we introduce \textbf{Quality Assessment Criteria}, employed in human and GPT evaluations, to verify the effectiveness of the generated stories. We hope this work benefits the autism community and catalyzes future research focusing on particular groups.</li>
<li><strong>摘要：</strong>患有自闭症谱系障碍 (ASD) 的儿童经常误解社交情况并难以参与日常生活。心理学专家在结构清晰度、描述性取向和情境安全性的严格约束下撰写社交故事，以提高他们在这些方面的能力。然而，社交故事的创作成本高昂，而且多样性和时效性往往有限。随着大型语言模型 (LLM) 变得越来越强大，人们越来越需要更自动化、更实惠、更易于访问的方法来实时生成覆盖范围广泛的社交故事。调整 LLM 以满足社交故事独特而严格的约束是一个具有挑战性的问题。为此，我们提出了 \textbf{SS-Bench}，这是一个用于生成和评估社交故事的 \textbf{S}ocial \textbf{S}tory \textbf{Bench} 基准。具体来说，我们开发了一个名为 \textbf{\textsc{StarSow}} 的约束驱动策略，以分层方式提示 LLM 生成社交故事并建立基准，该基准已通过实验验证，可以微调较小的模型以生成合格的社交故事。此外，我们引入了 \textbf{质量评估标准}，用于人工和 GPT 评估，以验证生成的故事的有效性。我们希望这项工作能造福自闭症社区，并催化未来针对特定群体的研究。</li>
</ul>

<h3>Title: Title:
          Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O. Arik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated remarkable capabilities, but their performance is heavily reliant on effective prompt engineering. Automatic prompt optimization (APO) methods are designed to automate this and can be broadly categorized into those targeting instructions (instruction optimization, IO) vs. those targeting exemplars (exemplar selection, ES). Despite their shared objective, these have evolved rather independently, with IO recently receiving more research attention. This paper seeks to bridge this gap by comprehensively comparing the performance of representative IO and ES techniques, both isolation and combination, on a diverse set of challenging tasks. Our findings reveal that intelligently reusing model-generated input-output pairs obtained from evaluating prompts on the validation set as exemplars consistently improves performance over IO methods but is currently under-investigated. We also find that despite the recent focus on IO, how we select exemplars can outweigh how we optimize instructions, with ES strategies as simple as random search outperforming state-of-the-art IO methods with seed instructions without any optimization. Moreover, we observe synergy between ES and IO, with optimal combinations surpassing individual contributions. We conclude that studying exemplar selection as a standalone method and its optimal combination with instruction optimization remains a crucial aspect of APO and deserves greater consideration in future research, even in the era of highly capable instruction-following models.</li>
<li><strong>摘要：</strong>大型语言模型已经展示了卓越的能力，但它们的性能在很大程度上依赖于有效的提示工程。自动提示优化 (APO) 方法旨在实现此自动化，可大致分为针对指令的方法（指令优化，IO）和针对样例的方法（样例选择，ES）。尽管它们有共同的目标，但它们的发展却相当独立，IO 最近受到了更多的研究关注。本文试图通过全面比较代表性 IO 和 ES 技术（包括隔离和组合）在一系列具有挑战性的任务上的性能来弥合这一差距。我们的研究结果表明，智能地重复使用从评估验证集上的提示中获得的模型生成的输入输出对作为样例可以持续提高性能，优于 IO 方法，但目前研究不足。我们还发现，尽管最近关注 IO，但我们如何选择样例可能比我们如何优化指令更重要，ES 策略就像随机搜索一样简单，其表现优于使用种子指令而无需任何优化的最先进的 IO 方法。此外，我们观察到 ES 和 IO 之间的协同作用，最佳组合超过单个贡献。我们得出结论，研究样本选择作为一种独立方法及其与指令优化的最佳组合仍然是 APO 的一个重要方面，值得在未来的研究中给予更多考虑，即使在高性能指令跟随模型时代也是如此。</li>
</ul>

<h3>Title: Title:
          Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models</h3>
<ul>
<li><strong>Authors: </strong>Xinrong Zhang, Yingfa Chen, Shengding Hu, Xu Han, Zihang Xu, Yuanwei Xu, Weilin Zhao, Maosong Sun, Zhiyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations. Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while it is generating responses. To overcome these limitations, we adapt existing LLMs to \textit{duplex models} so that these LLMs can listen for users while generating output and dynamically adjust themselves to provide users with instant feedback. % such as in response to interruptions. Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to pseudo-simultaneously process these slices. Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses as well as covering typical feedback types in instantaneous interactions. Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 日益渗透到日常生活中，对反映人类对话的实时交互的需求也日益增长。传统的由 LLM 驱动的回合制聊天系统阻止用户在系统生成响应时与系统进行口头交互。为了克服这些限制，我们将现有的 LLM 调整为 \textit{双工模型}，以便这些 LLM 可以在生成输出的同时监听用户并动态调整自身以向用户提供即时反馈。% 例如响应中断。具体来说，我们将对话的查询和响应划分为几个时间片，然后采用时分复用 (TDM) 编码解码策略对这些时间片进行伪同时处理。此外，为了使 LLM 足够熟练以处理实时对话，我们构建了一个微调数据集，该数据集由交替的查询和响应时间片组成，并涵盖即时交互中的典型反馈类型。我们的实验表明，尽管对话的查询和响应被分割成不完整的部分进行处理，但 LLM 可以通过在我们的数据集上进行一些微调步骤来保持其在标准基准上的原始性能。自动和人工评估表明，与普通 LLM 相比，双工模型使用户与 AI 的交互更加自然和人性化，并且大大提高了用户满意度。我们的双工模型和数据集即将发布。</li>
</ul>

<h3>Title: Title:
          Scaling Laws for Fact Memorization of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Lu, Xiaonan Li, Qinyuan Cheng, Kai Ding, Xuanjing Huang, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Scaling Laws for Fact Memorization of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fact knowledge memorization is crucial for Large Language Models (LLM) to generate factual and reliable responses. However, the behaviors of LLM fact memorization remain under-explored. In this paper, we analyze the scaling laws for LLM's fact knowledge and LLMs' behaviors of memorizing different types of facts. We find that LLMs' fact knowledge capacity has a linear and negative exponential law relationship with model size and training epochs, respectively. Estimated by the built scaling law, memorizing the whole Wikidata's facts requires training an LLM with 1000B non-embed parameters for 100 epochs, suggesting that using LLMs to memorize all public facts is almost implausible for a general pre-training setting. Meanwhile, we find that LLMs can generalize on unseen fact knowledge and its scaling law is similar to general pre-training. Additionally, we analyze the compatibility and preference of LLMs' fact memorization. For compatibility, we find LLMs struggle with memorizing redundant facts in a unified way. Only when correlated facts have the same direction and structure, the LLM can compatibly memorize them. This shows the inefficiency of LLM memorization for redundant facts. For preference, the LLM pays more attention to memorizing more frequent and difficult facts, and the subsequent facts can overwrite prior facts' memorization, which significantly hinders low-frequency facts memorization. Our findings reveal the capacity and characteristics of LLMs' fact knowledge learning, which provide directions for LLMs' fact knowledge augmentation.</li>
<li><strong>摘要：</strong>事实知识记忆对于大型语言模型 (LLM) 生成事实可靠的响应至关重要。然而，LLM 事实记忆的行为仍未得到充分探索。在本文中，我们分析了 LLM 事实知识的缩放规律以及 LLM 记忆不同类型事实的行为。我们发现 LLM 的事实知识容量与模型大小和训练次数分别呈线性和负指数规律关系。通过建立的缩放规律估计，记忆整个 Wikidata 的事实需要使用 1000B 非嵌入参数训练 LLM 100 个 epoch，这表明在一般的预训练环境中使用 LLM 记忆所有公开事实几乎是不可信的。同时，我们发现 LLM 可以概括看不见的事实知识，其缩放规律与一般的预训练相似。此外，我们分析了 LLM 事实记忆的兼容性和偏好。在兼容性方面，我们发现LLM在记忆冗余事实时很难统一，只有当相关事实具有相同的方向和结构时，LLM才能兼容地记忆它们。这表明LLM记忆冗余事实的效率低下。在偏好方面，LLM更注重记忆更频繁和更困难的事实，而后续事实会覆盖先前事实的记忆，这严重阻碍了低频事实的记忆。我们的研究结果揭示了LLM事实知识学习的能力和特点，为LLM事实知识的增强提供了方向。</li>
</ul>

<h3>Title: Title:
          RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Changhai Zhou, Shijie Han, Shiyang Zhang, Shichao Weng, Zekai Liu, Cheng Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The efficient compression of large language models (LLMs) is becoming increasingly popular. However, recovering the accuracy of compressed LLMs is still a major challenge. Structural pruning with standard Low-Rank Adaptation (LoRA) is a common technique in current LLM compression. In structural pruning, the model architecture is modified unevenly, resulting in suboptimal performance in various downstream tasks via standard LoRA with fixed rank. To address this problem, we introduce RankAdaptor, an efficient fine-tuning method with hierarchical dynamic rank scheduling for pruned LLMs. An end-to-end automatic optimization flow is developed that utilizes a lightweight performance model to determine the different ranks during fine-tuning. Comprehensive experiments on popular benchmarks show that RankAdaptor consistently outperforms standard LoRA with structural pruning over different pruning settings. Without increasing the trainable parameters, RankAdaptor further reduces the accuracy performance gap between the recovery of the pruned model and the original model compared to standard LoRA.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的高效压缩正变得越来越流行。然而，恢复压缩后的 LLM 的准确率仍然是一个重大挑战。使用标准低秩自适应 (LoRA) 进行结构剪枝是当前 LLM 压缩中的一种常见技术。在结构剪枝中，模型架构的修改不均衡，导致通过具有固定秩的标准 LoRA 在各种下游任务中性能不佳。为了解决这个问题，我们引入了 RankAdaptor，这是一种具有分层动态秩调度的高效微调方法，适用于剪枝后的 LLM。开发了一种端到端自动优化流程，该流程利用轻量级性能模型来确定微调过程中的不同秩。在流行基准上进行的综合实验表明，RankAdaptor 在不同剪枝设置下始终优于使用结构剪枝的标准 LoRA。在不增加可训练参数的情况下，与标准 LoRA 相比，RankAdaptor 进一步缩小了剪枝模型与原始模型恢复之间的准确率性能差距。</li>
</ul>

<h3>Title: Title:
          Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level</h3>
<ul>
<li><strong>Authors: </strong>Zhaopeng Feng, Ruizhe Chen, Yan Zhang, Zijie Meng, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>General-purpose Large Language Models (LLMs) like GPT-4 have achieved remarkable advancements in machine translation (MT) by leveraging extensive web content. On the other hand, translation-specific LLMs are built by pre-training on domain-specific monolingual corpora and fine-tuning with human-annotated translation data. Despite the superior performance, these methods either demand an unprecedented scale of computing and data or substantial human editing and annotation efforts. In this paper, we develop Ladder, a novel model-agnostic and cost-effective tool to refine the performance of general LLMs for MT. Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost. During training, we propose a hierarchical fine-tuning strategy with an easy-to-hard schema, improving Ladder's refining performance progressively. The trained Ladder can be seamlessly integrated with any general-purpose LLMs to boost their translation performance. By utilizing Gemma-2B/7B as the backbone, Ladder-2B can elevate raw translations to the level of top-tier open-source models (e.g., refining BigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and Ladder-7B can further enhance model performance to be on par with the state-of-the-art GPT-4. Extensive ablation and analysis corroborate the effectiveness of Ladder in diverse settings. Our code is available at this https URL</li>
<li><strong>摘要：</strong>诸如 GPT-4 之类的通用大型语言模型 (LLM) 通过利用大量网络内容在机器翻译 (MT) 方面取得了显著进步。另一方面，特定于翻译的 LLM 是通过在特定领域的单语语料库上进行预训练并使用人工注释的翻译数据进行微调而构建的。尽管性能卓越，但这些方法要么需要前所未有的计算和数据规模，要么需要大量的人工编辑和注释工作。在本文中，我们开发了 Ladder，这是一种与模型无关且经济高效的新工具，用于改进用于 MT 的通用 LLM 的性能。Ladder 在伪细化三元组上进行训练，这些三元组可以从现有的 LLM 中轻松获得，而无需额外的人力成本。在训练过程中，我们提出了一种具有由易到难模式的分层微调策略，逐步提高 Ladder 的细化性能。训练后的 Ladder 可以与任何通用 LLM 无缝集成，以提高其翻译性能。通过利用 Gemma-2B/7B 作为主干，Ladder-2B 可以将原始翻译提升到顶级开源模型的水平（例如，改进 BigTranslate-13B，XX-En 的 BLEU 为 +6.91，COMET 为 +3.52），而 Ladder-7B 可以进一步提高模型性能，使其与最先进的 GPT-4 相媲美。广泛的消融和分析证实了 Ladder 在不同环境中的有效性。我们的代码可在此 https URL 上找到</li>
</ul>

<h3>Title: Title:
          DABL: Detecting Semantic Anomalies in Business Processes Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Guan, Jian Cao, Jianqi Gao, Haiyan Zhao, Shiyou Qian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DABL: Detecting Semantic Anomalies in Business Processes Using Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in business processes is crucial for ensuring operational success. While many existing methods rely on statistical frequency to detect anomalies, it's important to note that infrequent behavior doesn't necessarily imply undesirability. To address this challenge, detecting anomalies from a semantic viewpoint proves to be a more effective approach. However, current semantic anomaly detection methods treat a trace (i.e., process instance) as multiple event pairs, disrupting long-distance dependencies. In this paper, we introduce DABL, a novel approach for detecting semantic anomalies in business processes using large language models (LLMs). We collect 143,137 real-world process models from various domains. By generating normal traces through the playout of these process models and simulating both ordering and exclusion anomalies, we fine-tune Llama 2 using the resulting log. Through extensive experiments, we demonstrate that DABL surpasses existing state-of-the-art semantic anomaly detection methods in terms of both generalization ability and learning of given processes. Users can directly apply DABL to detect semantic anomalies in their own datasets without the need for additional training. Furthermore, DABL offers the capability to interpret the causes of anomalies in natural language, providing valuable insights into the detected anomalies.</li>
<li><strong>摘要：</strong>检测业务流程中的异常对于确保运营成功至关重要。虽然许多现有方法依靠统计频率来检测异常，但需要注意的是，不频繁的行为并不一定意味着不良行为。为了应对这一挑战，从语义角度检测异常被证明是一种更有效的方法。然而，当前的语义异常检测方法将跟踪（即流程实例）视为多个事件对，从而破坏了长距离依赖关系。在本文中，我们介绍了 DABL，这是一种使用大型语言模型 (LLM) 检测业务流程中语义异常的新方法。我们从各个领域收集了 143,137 个真实世界流程模型。通过这些流程模型的播放生成正常跟踪并模拟排序和排除异常，我们使用生成的日志对 Llama 2 进行微调。通过大量实验，我们证明 DABL 在泛化能力和给定流程的学习方面都超越了现有的最先进的语义异常检测方法。用户可以直接应用 DABL 来检测自己数据集中的语义异常，而无需进行额外训练。此外，DABL 还能够用自然语言解释异常的原因，从而为检测到的异常提供有价值的见解。</li>
</ul>

<h3>Title: Title:
          Rethinking Entity-level Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Rethinking Entity-level Unlearning for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language model unlearning has gained increasing attention due to its potential to mitigate security and privacy concerns. Current research predominantly focuses on Instance-level unlearning, specifically aiming at forgetting predefined instances of sensitive content. However, a notable gap still exists in exploring the deletion of complete entity-related information, which is crucial in many real-world scenarios, such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, where the entity-related knowledge within the target model is supposed to be entirely erased. Given the challenge of practically accessing all entity-related knowledge within a model, we begin by simulating entity-level unlearning scenarios through fine-tuning models to introduce pseudo entities. Following this, we develop baseline methods inspired by trending unlearning techniques and conduct a detailed comparison of their effectiveness in this task. Extensive experiments reveal that current unlearning algorithms struggle to achieve effective entity-level unlearning. Additionally, our analyses further indicate that entity-related knowledge injected through fine-tuning is more susceptible than original entities from pre-training during unlearning, highlighting the necessity for more thorough pseudo-entity injection methods to make them closer to pre-trained knowledge.</li>
<li><strong>摘要：</strong>大型语言模型反学习因其缓解安全和隐私问题的潜力而受到越来越多的关注。当前的研究主要集中在实例级反学习上，具体目的是忘记敏感内容的预定义实例。然而，在探索删除完整的实体相关信息方面仍然存在明显的差距，这在许多现实场景中至关重要，例如版权保护。为此，我们提出了一个实体级反学习的新任务，其中目标模型中的实体相关知识应该被完全删除。考虑到在模型中实际访问所有实体相关知识的挑战，我们首先通过微调模型来引入伪实体，以模拟实体级反学习场景。在此之后，我们开发了受流行反学习技术启发的基线方法，并对它们在此任务中的有效性进行了详细的比较。大量实验表明，当前的反学习算法难以实现有效的实体级反学习。此外，我们的分析进一步表明，通过微调注入的实体相关知识比原始实体在去学习过程中更容易受到预训练的影响，这凸显了更彻底的伪实体注入方法的必要性，以使它们更接近预训练的知识。</li>
</ul>

<h3>Title: Title:
          LaMSUM: A Novel Framework for Extractive Summarization of User Generated Content using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Garima Chhikara, Anurag Sharma, V. Gurucharan, Kripabandhu Ghosh, Abhijnan Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LaMSUM: A Novel Framework for Extractive Summarization of User Generated Content using LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performance across a wide range of NLP tasks, including summarization. Inherently LLMs produce abstractive summaries, and the task of achieving extractive summaries through LLMs still remains largely unexplored. To bridge this gap, in this work, we propose a novel framework LaMSUM to generate extractive summaries through LLMs for large user-generated text by leveraging voting algorithms. Our evaluation on three popular open-source LLMs (Llama 3, Mixtral and Gemini) reveal that the LaMSUM outperforms state-of-the-art extractive summarization methods. We further attempt to provide the rationale behind the output summary produced by LLMs. Overall, this is one of the early attempts to achieve extractive summarization for large user-generated text by utilizing LLMs, and likely to generate further interest in the community.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在包括摘要在内的各种 NLP 任务中表现出色。LLM 本质上会生成抽象摘要，而通过 LLM 实现提取摘要的任务仍然在很大程度上尚未被探索。为了弥补这一差距，在这项工作中，我们提出了一个新颖的框架 LaMSUM，利用投票算法通过 LLM 为大型用户生成文本生成提取摘要。我们对三种流行的开源 LLM（Llama 3、Mixtral 和 Gemini）的评估表明，LaMSUM 优于最先进的提取摘要方法。我们进一步尝试提供 LLM 生成的输出摘要背后的原理。总的来说，这是利用 LLM 实现大型用户生成文本提取摘要的早期尝试之一，可能会引起社区的进一步关注。</li>
</ul>

<h3>Title: Title:
          CaT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans</h3>
<ul>
<li><strong>Authors: </strong>Yash Kumar Lal, Vanya Cohen, Nathanael Chambers, Niranjan Balasubramanian, Raymond Mooney</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CaT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps needs to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs' ability to detect dependence between steps has significant room for improvement.</li>
<li><strong>摘要：</strong>了解 LLM 推理自然语言计划（例如教学文本和食谱）的能力对于在决策系统中可靠地使用它们至关重要。计划的一个基本方面是其步骤需要执行的时间顺序，这反映了它们之间潜在的因果依赖关系。我们引入了 CaT-Bench，这是步骤顺序预测问题的基准，它测试烹饪食谱计划中一个步骤是否必须在另一个步骤之前或之后发生。我们用它来评估前沿 LLM 对因果和时间依赖关系的理解程度。我们发现 SOTA LLM 表现不佳（最佳零样本 F1 仅为 0.59），并且更倾向于预测依赖关系，可能依赖步骤的时间顺序作为启发式方法。虽然提示解释和使用少量样本示例可以提高性能，但最佳 F1 结果仅为 0.73。此外，人类对解释的评估以及答案的正确性表明，平均而言，人类并不同意模型推理。令人惊讶的是，我们还发现，回答后解释比正常的思路链提示能带来更好的表现，而且 LLM 的答案在关于相同步骤对的问题之间并不一致。总体而言，结果表明 LLM 检测步骤之间依赖关系的能力有很大的改进空间。</li>
</ul>

<h3>Title: Title:
          A multitask learning framework for leveraging subjectivity of annotators to identify misogyny</h3>
<ul>
<li><strong>Authors: </strong>Jason Angel, Segun Taofeek Aroyehun, Grigori Sidorov, Alexander Gelbukh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A multitask learning framework for leveraging subjectivity of annotators to identify misogyny(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Identifying misogyny using artificial intelligence is a form of combating online toxicity against women. However, the subjective nature of interpreting misogyny poses a significant challenge to model the phenomenon. In this paper, we propose a multitask learning approach that leverages the subjectivity of this task to enhance the performance of the misogyny identification systems. We incorporated diverse perspectives from annotators in our model design, considering gender and age across six profile groups, and conducted extensive experiments and error analysis using two language models to validate our four alternative designs of the multitask learning technique to identify misogynistic content in English tweets. The results demonstrate that incorporating various viewpoints enhances the language models' ability to interpret different forms of misogyny. This research advances content moderation and highlights the importance of embracing diverse perspectives to build effective online moderation systems.</li>
<li><strong>摘要：</strong>使用人工智能识别厌女症是打击针对女性的网络恶意攻击的一种形式。然而，解读厌女症的主观性对建模这一现象提出了重大挑战。在本文中，我们提出了一种多任务学习方法，利用这项任务的主观性来提高厌女症识别系统的性能。我们在模型设计中结合了注释者的不同观点，考虑了六个个人资料组中的性别和年龄，并使用两种语言模型进行了广泛的实验和错误分析，以验证我们四种多任务学习技术替代设计，以识别英文推文中的厌女症内容。结果表明，结合各种观点可以增强语言模型解释不同形式厌女症的能力。这项研究推动了内容审核，并强调了接受不同观点以建立有效的在线审核系统的重要性。</li>
</ul>

<h3>Title: Title:
          Uncovering Hidden Intentions: Exploring Prompt Recovery for Deeper Insights into Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>Louis Give, Timo Zaoral, Maria Antonietta Bruno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Uncovering Hidden Intentions: Exploring Prompt Recovery for Deeper Insights into Generated Texts(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Today, the detection of AI-generated content is receiving more and more attention. Our idea is to go beyond detection and try to recover the prompt used to generate a text. This paper, to the best of our knowledge, introduces the first investigation in this particular domain without a closed set of tasks. Our goal is to study if this approach is promising. We experiment with zero-shot and few-shot in-context learning but also with LoRA fine-tuning. After that, we evaluate the benefits of using a semi-synthetic dataset. For this first study, we limit ourselves to text generated by a single model. The results show that it is possible to recover the original prompt with a reasonable degree of accuracy.</li>
<li><strong>摘要：</strong>如今，对人工智能生成内容的检测越来越受到关注。我们的想法是超越检测，尝试恢复用于生成文本的提示。据我们所知，本文介绍了该特定领域的首次调查，没有封闭的任务集。我们的目标是研究这种方法是否有前景。我们尝试了零样本和少样本上下文学习，也尝试了 LoRA 微调。之后，我们评估了使用半合成数据集的好处。对于这项初步研究，我们将自己限制在由单个模型生成的文本中。结果表明，可以以合理的准确度恢复原始提示。</li>
</ul>

<h3>Title: Title:
          SimSMoE: Solving Representational Collapse via Similarity Measure</h3>
<ul>
<li><strong>Authors: </strong>Giang Do, Hung Le, Truyen Tran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SimSMoE: Solving Representational Collapse via Similarity Measure(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sparse mixture of experts (SMoE) have emerged as an effective approach for scaling large language models while keeping a constant computational cost. Regardless of several notable successes of SMoE, effective training such architecture remains elusive due to the representation collapse problem, which in turn harms model performance and causes parameter redundancy. In this work, we present Similarity-based Sparse Mixture of Experts (SimSMoE), a novel similarity of neural network algorithm, that guarantees a solution to address the representation collapse issue between experts given a fixed FLOPs budget. We conduct extensive empirical evaluations on three large language models for both Pre-training and Fine-tuning tasks to illustrate the efficacy, robustness, and scalability of our method. The results demonstrate that SimSMoE significantly enhances existing routing policy and outperforms other SMoE training methods in performance for the tasks.</li>
<li><strong>摘要：</strong>稀疏混合专家 (SMoE) 已成为一种有效的方法，可在保持计算成本不变的情况下扩展大型语言模型。尽管 SMoE 取得了一些显著的成功，但由于表示崩溃问题，这种架构的有效训练仍然难以实现，这反过来又损害了模型性能并导致参数冗余。在这项工作中，我们提出了基于相似性的稀疏混合专家 (SimSMoE)，这是一种新颖的神经网络相似性算法，它保证在固定 FLOPs 预算的情况下解决专家之间的表示崩溃问题。我们对三个大型语言模型进行了广泛的实证评估，包括预训练和微调任务，以说明我们方法的有效性、稳健性和可扩展性。结果表明，SimSMoE 显著增强了现有的路由策略，并且在任务性能方面优于其他 SMoE 训练方法。</li>
</ul>

<h3>Title: Title:
          Real-time Speech Summarization for Medical Conversations</h3>
<ul>
<li><strong>Authors: </strong>Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Real-time Speech Summarization for Medical Conversations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online: this https URL</li>
<li><strong>摘要：</strong>在医患对话中，识别医学相关信息至关重要，因此需要进行对话摘要。在这项工作中，我们提出了第一个可部署的实时语音摘要系统，用于行业中的实际应用，该系统在对话中每 N 次语音表达后生成一个局部摘要，在对话结束后生成一个全局摘要。我们的系统可以从业务角度增强用户体验，同时从技术角度降低计算成本。其次，我们提出了 VietMed-Sum，据我们所知，这是第一个用于医疗对话的语音摘要数据集。第三，我们是第一个利用 LLM 和人工注释者合作创建医疗对话摘要的黄金标准和合成摘要的人。最后，我们展示了 VietMed-Sum 上最先进模型的基线结果。所有代码、数据（英语翻译和越南语）和模型都可以在线获取：此 https URL</li>
</ul>

<h3>Title: Title:
          The Unlikely Duel: Evaluating Creative Writing in LLMs through a Unique Scenario</h3>
<ul>
<li><strong>Authors: </strong>Carlos Gómez-Rodríguez, Paul Williams</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The Unlikely Duel: Evaluating Creative Writing in LLMs through a Unique Scenario(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This is a summary of the paper "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing", which was published in Findings of EMNLP 2023. We evaluate a range of recent state-of-the-art, instruction-tuned large language models (LLMs) on an English creative writing task, and compare them to human writers. For this purpose, we use a specifically-tailored prompt (based on an epic combat between Ignatius J. Reilly, main character of John Kennedy Toole's "A Confederacy of Dunces", and a pterodactyl) to minimize the risk of training data leakage and force the models to be creative rather than reusing existing stories. The same prompt is presented to LLMs and human writers, and evaluation is performed by humans using a detailed rubric including various aspects like fluency, style, originality or humor. Results show that some state-of-the-art commercial LLMs match or slightly outperform our human writers in most of the evaluated dimensions. Open-source LLMs lag behind. Humans keep a close lead in originality, and only the top three LLMs can handle humor at human-like levels.</li>
<li><strong>摘要：</strong>这是发表在 EMNLP 2023 成果中的论文“模型联盟：创意写作法学硕士综合评估”的摘要。我们在英语创意写作任务中评估了一系列最新、指令调整的大型语言模型 (LLM)，并将它们与人类作家进行比较。为此，我们使用专门定制的提示（基于约翰·肯尼迪·图尔的《笨蛋联盟》主角伊格纳修斯·J·赖利和翼手龙之间的史诗般的战斗）来最大限度地降低训练数据泄露的风险，并迫使模型发挥创造力，而不是重复使用现有的故事。向 LLM 和人类作家呈现相同的提示，并由人类使用详细的评分标准进行评估，包括流畅度、风格、原创性或幽默等各个方面。结果表明，一些最先进的商业 LLM 在大多数评估维度上与我们的人类作家相当或略胜一筹。开源 LLM 落后了。人类在原创性方面保持领先，只有排名前三的法学硕士能够以类似人类的水平处理幽默。</li>
</ul>

<h3>Title: Title:
          Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.</li>
<li><strong>摘要：</strong>我们提出了语义熵探测 (SEP)，这是一种廉价而可靠的大型语言模型 (LLM) 不确定性量化方法。幻觉听起来似乎合理，但事实上是不正确的，并且是任意的模型生成，对 LLM 的实际应用提出了重大挑战。Farquhar 等人 (2024) 最近的一项研究提出了语义熵 (SE)，它可以通过估计一组模型生成的空间语义含义的不确定性来检测幻觉。然而，与 SE 计算相关的 5 到 10 倍的计算成本增加阻碍了实际应用。为了解决这个问题，我们提出了 SEP，它直接从单个代的隐藏状态近似 SE。SEP 易于训练，不需要在测试时对多个模型生成进行采样，从而将语义不确定性量化的开销降低到几乎为零。我们表明，与直接预测模型准确性的先前探测方法相比，SEP 在幻觉检测方面保持了高性能，并且更好地推广到分布外的数据。我们在各个模型和任务中的结果表明模型隐藏状态捕获了 SE，而我们的消融研究进一步深入了解了这种情况下的标记位置和模型层。</li>
</ul>

<h3>Title: Title:
          RuleR: Improving LLM Controllability by Rule-based Data Recycling</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Han Chen, Chenguang Wang, Dang Nguyen, Dianqi Li, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RuleR: Improving LLM Controllability by Rule-based Data Recycling(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) still lack delicate controllability over their responses, which is critical to enhancing their performance and the user experience. However, curating supervised fine-tuning (SFT) datasets to improve LLM controllability usually relies on human experts or proprietary LLMs, which requires additional costs. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a data augmentation method incorporating multiple constraints into the original data samples according to predefined rules, which creates new training tasks to consolidate the controllability of LLMs. Instead of creating new data from scratch, RuleR ``recycles'' existing data by simply applying rule-based edits to their responses and appending the rule-instructions in their original instructions. Experimental results demonstrate RuleR's effectiveness in improving LLM controllability while maintaining general instruction-following capabilities. The code will be released on this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 仍然缺乏对其响应的精细可控性，而这对于提高其性能和用户体验至关重要。然而，策划监督微调 (SFT) 数据集以提高 LLM 可控性通常依赖于人类专家或专有 LLM，这需要额外的成本。为了弥补这一差距，我们提出了基于规则的数据回收 (RuleR)，这是一种根据预定义规则将多个约束纳入原始数据样本的数据增强方法，它创建新的训练任务以巩固 LLM 的可控性。RuleR 不是从头开始创建新数据，而是通过简单地将基于规则的编辑应用于其响应并在其原始指令中附加规则指令来“回收”现有数据。实验结果证明了 RuleR 在提高 LLM 可控性的同时保持一般指令遵循能力的有效性。代码将在此 https URL 上发布。</li>
</ul>

<h3>Title: Title:
          Teaching LLMs to Abstain across Languages via Multilingual Feedback</h3>
<ul>
<li><strong>Authors: </strong>Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Orevaoghene Ahia, Shuyue Stella Li, Vidhisha Balachandran, Sunayana Sitaram, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Teaching LLMs to Abstain across Languages via Multilingual Feedback(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5% performance gaps between high and low-resource languages, potentially due to LLMs' drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2% improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling.</li>
<li><strong>摘要：</strong>多语言法学硕士在不同语言间的知识差异通常较大，资源匮乏的语言之间的差距更大。因此，教导法学硕士在面对知识差距时弃权是一种有前途的策略，可以减轻多语言环境中的幻觉。然而，之前对法学硕士弃权的研究主要集中在英语上；我们发现，直接应用英语以外的现有解决方案会导致高资源语言和低资源语言之间的绩效差距高达 20.5%，这可能是由于法学硕士在少数资源丰富的语言之外的校准和推理能力下降。为此，我们提出了通过从多语言反馈中学习来增强法学硕士弃权的策略，其中法学硕士通过生成相关语言中的多个反馈项目来自我反思一种语言中提出的答案：我们表明这有助于识别不同语言、文化和社区之间的知识差距。大量实验表明，我们的多语言反馈方法优于各种强大的基线，在三个数据集上的三个黑盒和开放模型中，针对低资源语言实现了高达 9.2% 的改进，包括开卷、闭卷和常识性问答。进一步的分析表明，多语言反馈是一种有效且更公平的弃权策略，可以为不同语言使用者提供服务，而文化因素对语言选择和 LLM 弃权行为有很大影响，这突出了多语言和多文化可靠语言建模的未来方向。</li>
</ul>

<h3>Title: Title:
          Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it "plugs into" a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.</li>
<li><strong>摘要：</strong>虽然现有的对齐范式对于开发大型语言模型 (LLM) 至关重要，但 LLM 通常会学习平均的人类偏好，并且难以对跨文化、人口统计和社区的不同偏好进行建模。我们提出了模块化多元化，这是一个基于多 LLM 协作的模块化框架，用于实现多元化对齐：它“插入”基础 LLM，其中包含一个较小但专业的社区 LM 池，其中模型以不同的模式协作，以灵活地支持三种多元化模式：Overton、可操纵和分布式。模块化多元化与黑盒 LLM 具有独特的兼容性，并为以前代表性不足的社区提供添加新社区 LM 的模块化控制。我们用六个任务和四个数据集评估了模块化多元化，其中包含具有价值和观点信息响应的问题/说明。大量实验表明，模块化多元化在六个黑盒和开源 LLM 中推进了三个多元化目标。进一步的分析表明，LLM 通常忠实于较小社区 LLM 的输入，可以通过添加新的社区 LM 进行无缝修补，以更好地覆盖以前代表性不足的社区。</li>
</ul>

<h3>Title: Title:
          Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care</h3>
<ul>
<li><strong>Authors: </strong>Hassan Alhuzali, Ashwag Alasmari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Pre-trained Language Models (PLMs) have the potential to transform mental health support by providing accessible and culturally sensitive resources. However, despite this potential, their effectiveness in mental health care and specifically for the Arabic language has not been extensively explored. To bridge this gap, this study evaluates the effectiveness of foundational models for classification of Questions and Answers (Q&A) in the domain of mental health care. We leverage the MentalQA dataset, an Arabic collection featuring Q&A interactions related to mental health. In this study, we conducted experiments using four different types of learning approaches: traditional feature extraction, PLMs as feature extractors, Fine-tuning PLMs and prompting large language models (GPT-3.5 and GPT-4) in zero-shot and few-shot learning settings. While traditional feature extractors combined with Support Vector Machines (SVM) showed promising performance, PLMs exhibited even better results due to their ability to capture semantic meaning. For example, MARBERT achieved the highest performance with a Jaccard Score of 0.80 for question classification and a Jaccard Score of 0.86 for answer classification. We further conducted an in-depth analysis including examining the effects of fine-tuning versus non-fine-tuning, the impact of varying data size, and conducting error analysis. Our analysis demonstrates that fine-tuning proved to be beneficial for enhancing the performance of PLMs, and the size of the training data played a crucial role in achieving high performance. We also explored prompting, where few-shot learning with GPT-3.5 yielded promising results. There was an improvement of 12% for question and classification and 45% for answer classification. Based on our findings, it can be concluded that PLMs and prompt-based approaches hold promise for mental health support in Arabic.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 有可能通过提供可访问且具有文化敏感性的资源来改变心理健康支持。然而，尽管具有这种潜力，但它们在心理健康护理方面的有效性，特别是对阿拉伯语的有效性尚未得到广泛探索。为了弥补这一差距，本研究评估了基础模型在心理健康护理领域对问答 (Q&A) 进行分类的有效性。我们利用 MentalQA 数据集，这是一个阿拉伯语集合，包含与心理健康相关的问答互动。在本研究中，我们使用四种不同类型的学习方法进行了实验：传统特征提取、PLM 作为特征提取器、微调 PLM 和在零样本和小样本学习环境中提示大型语言模型 (GPT-3.5 和 GPT-4)。虽然传统的特征提取器与支持向量机 (SVM) 相结合表现出令人鼓舞的性能，但 PLM 由于其捕捉语义的能力而表现出更好的结果。例如，MARBERT 在问题分类方面取得了最高的性能，Jaccard 得分为 0.80，在答案分类方面取得了 0.86 的 Jaccard 得分。我们进一步进行了深入分析，包括检查微调与非微调的效果、不同数据大小的影响以及进行错误分析。我们的分析表明，微调被证明有利于提高 PLM 的性能，而训练数据的大小在实现高性能方面起着至关重要的作用。我们还探索了提示，其中使用 GPT-3.5 进行的小样本学习产生了有希望的结果。问题和分类提高了 12%，答案分类提高了 45%。根据我们的研究结果，可以得出结论，PLM 和基于提示的方法有望为阿拉伯语的心理健康支持提供帮助。</li>
</ul>

<h3>Title: Title:
          ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods</h3>
<ul>
<li><strong>Authors: </strong>Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei, Neil Zhenqiang Gong, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the pretraining data used for training them. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs' pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs' behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速扩展引发了人们对用于训练它们的预训练数据的透明度和公平使用的担忧。由于数据规模庞大以及训练期间每个实例的曝光有限，检测此类内容具有挑战性。我们提出了 ReCaLL（相对条件对数似然），这是一种新颖的成员推理攻击 (MIA)，通过利用 LLM 的条件语言建模功能来检测其预训练数据。ReCaLL 检查在目标数据点上使用非成员上下文前缀时条件对数似然的相对变化。我们的实证结果表明，与非成员数据相比，用非成员前缀来条件成员数据会导致对数似然的大幅下降。我们进行了全面的实验，并表明 ReCaLL 在 WikiMIA 数据集上实现了最佳性能，即使使用随机和合成前缀也是如此，并且可以使用集成方法进一步改进。此外，我们对具有不同成员上下文的 LLM 行为进行了深入分析，深入了解了 LLM 如何利用成员信息在序列和标记级别进行有效推理。</li>
</ul>

<h3>Title: Title:
          Serial Position Effects of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaobo Guo, Soroush Vosoughi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Serial Position Effects of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities in zero-shot learning applications, generating responses to queries using only pre-training information without the need for additional fine-tuning. This represents a significant departure from traditional machine learning approaches. Previous research has indicated that LLMs may exhibit serial position effects, such as primacy and recency biases, which are well-documented cognitive biases in human psychology. Our extensive testing across various tasks and models confirms the widespread occurrence of these effects, although their intensity varies. We also discovered that while carefully designed prompts can somewhat mitigate these biases, their effectiveness is inconsistent. These findings underscore the significance of serial position effects during the inference process, particularly in scenarios where there are no ground truth labels, highlighting the need for greater focus on addressing these effects in LLM applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在零样本学习应用中表现出非凡的能力，仅使用预训练信息即可生成查询响应，而无需进行额外的微调。这与传统的机器学习方法有很大不同。先前的研究表明，LLM 可能表现出序列位置效应，例如首因偏差和近因偏差，这是人类心理学中众所周知的认知偏差。我们对各种任务和模型进行的广泛测试证实了这些影响的广泛存在，尽管它们的强度各不相同。我们还发现，虽然精心设计的提示可以在一定程度上减轻这些偏差，但它们的有效性并不一致。这些发现强调了序列位置效应在推理过程中的重要性，特别是在没有基本事实标签的情况下，突出了在 LLM 应用中更加注重解决这些影响的必要性。</li>
</ul>

<h3>Title: Title:
          Enhancing Cross-Document Event Coreference Resolution by Discourse Structure and Semantic Information</h3>
<ul>
<li><strong>Authors: </strong>Qiang Gao, Bobo Li, Zixiang Meng, Yunlong Li, Jun Zhou, Fei Li, Chong Teng, Donghong Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Enhancing Cross-Document Event Coreference Resolution by Discourse Structure and Semantic Information(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Existing cross-document event coreference resolution models, which either compute mention similarity directly or enhance mention representation by extracting event arguments (such as location, time, agent, and patient), lacking the ability to utilize document-level information. As a result, they struggle to capture long-distance dependencies. This shortcoming leads to their underwhelming performance in determining coreference for the events where their argument information relies on long-distance dependencies. In light of these limitations, we propose the construction of document-level Rhetorical Structure Theory (RST) trees and cross-document Lexical Chains to model the structural and semantic information of documents. Subsequently, cross-document heterogeneous graphs are constructed and GAT is utilized to learn the representations of events. Finally, a pair scorer calculates the similarity between each pair of events and co-referred events can be recognized using standard clustering algorithm. Additionally, as the existing cross-document event coreference datasets are limited to English, we have developed a large-scale Chinese cross-document event coreference dataset to fill this gap, which comprises 53,066 event mentions and 4,476 clusters. After applying our model on the English and Chinese datasets respectively, it outperforms all baselines by large margins.</li>
<li><strong>摘要：</strong>现有的跨文档事件共指解析模型要么直接计算提及相似度，要么通过提取事件参数（如位置、时间、代理和患者）来增强提及表示，但缺乏利用文档级信息的能力。因此，它们很难捕捉长距离依赖关系。这一缺点导致它们在确定事件的共指方面表现不佳，因为事件的参数信息依赖于长距离依赖关系。鉴于这些限制，我们建议构建文档级修辞结构理论 (RST) 树和跨文档词汇链来建模文档的结构和语义信息。随后，构建跨文档异构图并利用 GAT 来学习事件的表示。最后，配对评分器计算每对事件之间的相似度，并使用标准聚类算法识别共同指称的事件。此外，由于现有的跨文档事件共指数据集仅限于英文，我们开发了一个大规模中文跨文档事件共指数据集来填补这一空白，该数据集包含 53,066 个事件提及和 4,476 个簇。在分别将我们的模型应用于英文和中文数据集后，其表现远超所有基线。</li>
</ul>

<h3>Title: Title:
          Can LLM Graph Reasoning Generalize beyond Pattern Memorization?</h3>
<ul>
<li><strong>Authors: </strong>Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can LLM Graph Reasoning Generalize beyond Pattern Memorization?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate great potential for problems with implicit graphical structures, while recent works seek to enhance the graph reasoning capabilities of LLMs through specialized instruction tuning. The resulting 'graph LLMs' are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data. To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph reasoning generalization: whether LLMs could go beyond semantic, numeric, structural, reasoning patterns in the synthetic training data and improve utility on real-world graph-based tasks. Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures. We explore three strategies to improve LLM graph reasoning generalization, and we find that while post-training alignment is most promising for real-world tasks, empowering LLM graph reasoning to go beyond pattern memorization remains an open research question.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在解决隐式图形结构问题方面表现出巨大潜力，而最近的研究则试图通过专门的指令调整来增强 LLM 的图形推理能力。仅使用分布内设置对由此产生的“图形 LLM”进行评估，因此 LLM 是在学习可推广的图形推理技能还是仅仅记住合成训练数据中的模式仍未得到充分探索。为此，我们提出了 NLGift 基准，这是 LLM 图形推理泛化的评估套件：LLM 是否可以超越合成训练数据中的语义、数字、结构、推理模式，并提高现实世界基于图形的任务的实用性。在四个图形推理任务中使用两个 LLM 进行的大量实验表明，虽然对简单模式（语义、数字）的泛化有些令人满意，但 LLM 很难在推理和现实世界模式中进行泛化，这使人们对合成图形调整对具有底层网络结构的真实世界任务的好处产生了怀疑。我们探索了三种提高 LLM 图推理泛化的策略，我们发现虽然训练后对齐对于现实世界的任务最有希望，但使 LLM 图推理超越模式记忆仍然是一个悬而未决的研究问题。</li>
</ul>

<h3>Title: Title:
          Memorizing Documents with Guidance in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bumjin Park, Jaesik Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Memorizing Documents with Guidance in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Training data plays a pivotal role in AI models. Large language models (LLMs) are trained with massive amounts of documents, and their parameters hold document-related contents. Recently, several studies identified content-specific locations in LLMs by examining the parameters. Instead of the post hoc interpretation, we propose another approach. We propose document-wise memory architecture to track document memories in training. The proposed architecture maps document representations to memory entries, which softly mask memories in the forward process of LLMs. Additionally, we propose document guidance loss, which increases the likelihood of text with document memories and reduces the likelihood of the text with the memories of other documents. Experimental results on Wikitext-103-v1 with Pythia-1B show that the proposed methods provide different memory entries for documents and high recall of document-related content in generation with trained document-wise memories.</li>
<li><strong>摘要：</strong>训练数据在 AI 模型中起着关键作用。大型语言模型 (LLM) 经过大量文档的训练，其参数包含与文档相关的内容。最近，一些研究通过检查参数确定了 LLM 中内容特定的位置。我们提出了另一种方法，而不是事后解释。我们提出了基于文档的记忆架构来跟踪训练中的文档记忆。所提出的架构将文档表示映射到记忆条目，这会在 LLM 的前向过程中软屏蔽记忆。此外，我们提出了文档指导损失，这增加了文本具有文档记忆的可能性，并降低了文本具有其他文档记忆的可能性。使用 Pythia-1B 在 Wikitext-103-v1 上的实验结果表明，所提出的方法为文档提供了不同的记忆条目，并且在使用经过训练的基于文档的记忆生成时对文档相关内容具有很高的回忆率。</li>
</ul>

<h3>Title: Title:
          Distributed Rule Vectors is A Key Mechanism in Large Language Models' In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Bowen Zheng, Ming Ma, Zhongqiao Lin, Tianming Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Distributed Rule Vectors is A Key Mechanism in Large Language Models' In-Context Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable abilities, one of the most important being In-Context Learning (ICL). With ICL, LLMs can derive the underlying rule from a few demonstrations and provide answers that comply with the rule. Previous work hypothesized that the network creates a "task vector" in specific positions during ICL. Patching the "task vector" allows LLMs to achieve zero-shot performance similar to few-shot learning. However, we discover that such "task vectors" do not exist in tasks where the rule has to be defined through multiple demonstrations. Instead, the rule information provided by each demonstration is first transmitted to its answer position and forms its own rule vector. Importantly, all the rule vectors contribute to the output in a distributed manner. We further show that the rule vectors encode a high-level abstraction of rules extracted from the demonstrations. These results are further validated in a series of tasks that rely on rules dependent on multiple demonstrations. Our study provides novel insights into the mechanism underlying ICL in LLMs, demonstrating how ICL may be achieved through an information aggregation mechanism.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经展现出非凡的能力，其中最重要的一项能力就是情境学习 (ICL)。借助 ICL，LLM 可以从一些演示中得出底层规则并提供符合规则的答案。先前的研究假设网络在 ICL 期间在特定位置创建“任务向量”。修补“任务向量”可使 LLM 实现类似于少样本学习的零样本性能。然而，我们发现在必须通过多个演示来定义规则的任务中不存在这样的“任务向量”。相反，每个演示提供的规则信息首先传输到其答案位置并形成自己的规则向量。重要的是，所有规则向量都以分布式方式对输出做出贡献。我们进一步表明，规则向量编码了从演示中提取的规则的高级抽象。这些结果在一系列依赖于依赖于多个演示的规则的任务中得到进一步验证。我们的研究为 LLM 中 ICL 的底层机制提供了新的见解，展示了如何通过信息聚合机制实现 ICL。</li>
</ul>

<h3>Title: Title:
          Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T. Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs exhibit a U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 15 percentage points. These findings open up future directions in understanding LLM attention bias and its potential consequences.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM)，即使经过专门训练来处理长输入上下文，也很难捕捉到位于输入中间的相关信息。这种现象被称为“迷失在中间”问题。在这项工作中，我们做出了三点贡献。首先，我们着手了解导致这种现象的因素。在此过程中，我们建立了“迷失在中间”与 LLM 内在注意力偏差之间的联系：LLM 表现出 U 形注意力偏差，其中输入开头和结尾的标记会受到更高的关注，而不管它们的相关性如何。其次，我们通过一种校准机制（即“在中间找到”）来减轻这种位置偏差，该机制允许模型根据上下文的相关性忠实地关注上下文，即使它们位于中间。第三，我们表明，中间发现不仅在长上下文中定位相关信息方面取得了更好的表现，而且最终还提高了各种任务中的检索增强生成 (RAG) 性能，比现有方法高出多达 15 个百分点。这些发现为理解 LLM 注意力偏差及其潜在后果开辟了未来方向。</li>
</ul>

<h3>Title: Title:
          Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Planning, as the core module of agents, is crucial in various fields such as embodied agents, web navigation, and tool using. With the development of large language models (LLMs), some researchers treat large language models as intelligent agents to stimulate and evaluate their planning capabilities. However, the planning mechanism is still unclear. In this work, we focus on exploring the look-ahead planning mechanism in large language models from the perspectives of information flow and internal representations. First, we study how planning is done internally by analyzing the multi-layer perception (MLP) and multi-head self-attention (MHSA) components at the last token. We find that the output of MHSA in the middle layers at the last token can directly decode the decision to some extent. Based on this discovery, we further trace the source of MHSA by information flow, and we reveal that MHSA mainly extracts information from spans of the goal states and recent steps. According to information flow, we continue to study what information is encoded within it. Specifically, we explore whether future decisions have been encoded in advance in the representation of flow. We demonstrate that the middle and upper layers encode a few short-term future decisions to some extent when planning is successful. Overall, our research analyzes the look-ahead planning mechanisms of LLMs, facilitating future research on LLMs performing planning tasks.</li>
<li><strong>摘要：</strong>规划作为智能体的核心模块，在具体化智能体、网页导航、工具使用等各个领域都至关重要。随着大型语言模型（LLM）的发展，一些研究者将大型语言模型视为智能体，以模拟和评估其规划能力。然而，规划机制尚不明确。本文从信息流和内部表征两个角度探讨大型语言模型的前瞻规划机制。首先，通过分析最后一个 token 处的多层感知器（MLP）和多头自注意力（MHSA）组件，研究内部如何进行规划。我们发现，MHSA 在最后一个 token 处的中间层的输出可以在一定程度上直接解码决策。基于这一发现，我们进一步通过信息流追溯 MHSA 的来源，发现 MHSA 主要从目标状态和最近步骤的跨度中提取信息。根据信息流，我们继续研究它里面编码了什么信息。具体来说，我们探讨未来的决策是否已经提前编码在流的表征中。我们证明，当规划成功时，中层和上层会在一定程度上编码一些短期未来决策。总体而言，我们的研究分析了 LLM 的前瞻规划机制，有助于未来研究 LLM 执行规划任务。</li>
</ul>

<h3>Title: Title:
          FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu Li, Feiyu Xiong, Tong Xu, Matthew B. Blaschko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in generating coherent text, but they often struggle with context awareness, leading to inaccuracies in tasks requiring faithful adherence to provided information. We introduce FastMem, a novel method designed to enhance instruction fine-tuned LLMs' context awareness through fast memorization of the prompt. FastMem maximizes the likelihood of the prompt before inference by fine-tuning only the last Feed-Forward Network (FFN) module. This targeted approach ensures efficient optimization without overfitting, significantly improving the model's ability to comprehend and accurately follow the context. Our experiments demonstrate substantial gains in reading comprehension, text summarization and adherence to output structures. For instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP dataset from 59.1% to 71.6%, and reduces the output structure failure rate of Qwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight FastMem's potential to offer a robust solution to enhance the reliability and accuracy of LLMs in various applications. Our code is available at: this https URL</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 擅长生成连贯的文本，但它们往往难以感知上下文，导致在需要忠实遵循所提供信息的任务中出现不准确性。我们引入了 FastMem，这是一种新颖的方法，旨在通过快速记忆提示来增强指令微调 LLM 的上下文感知能力。FastMem 通过仅微调最后一个前馈网络 (FFN) 模块来最大化推理前提示的可能性。这种有针对性的方法可确保高效优化而不会过度拟合，从而显著提高模型理解和准确遵循上下文的能力。我们的实验表明，在阅读理解、文本摘要和遵循输出结构方面取得了显着的进步。例如，FastMem 将 NQ-SWAP 数据集上 Llama 3-8B-Inst 的准确率从 59.1% 提高到 71.6%，并将 Qwen 1.5-4B-Chat 的输出结构失败率从 34.9% 降低到 25.5%。大量实验结果凸显了 FastMem 的潜力，它能够提供强大的解决方案来提高 LLM 在各种应用中的可靠性和准确性。我们的代码可从以下网址获取：此 https URL</li>
</ul>

<h3>Title: Title:
          First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Multi-step reasoning is widely adopted in the community to explore the better performance of language models (LMs). We report on the systematic strategy that LMs use in this process. Our controlled experiments reveal that LMs rely more heavily on heuristics, such as lexical overlap, in the earlier stages of reasoning when more steps are required to reach an answer. Conversely, as LMs progress closer to the final answer, their reliance on heuristics decreases. This suggests that LMs track only a limited number of future steps and dynamically combine heuristic strategies with logical ones in tasks involving multi-step reasoning.</li>
<li><strong>摘要：</strong>社区广泛采用多步推理来探索语言模型 (LM) 的更好性能。我们报告了 LM 在此过程中使用的系统策略。我们的受控实验表明，在推理的早期阶段，当需要更多步骤才能得出答案时，LM 更依赖启发式方法（例如词汇重叠）。相反，随着 LM 越来越接近最终答案，它们对启发式方法的依赖就会减少。这表明 LM 仅跟踪有限数量的未来步骤，并在涉及多步推理的任务中动态地将启发式策略与逻辑策略相结合。</li>
</ul>

<h3>Title: Title:
          EERPD: Leveraging Emotion and Emotion Regulation for Improving Personality Detection</h3>
<ul>
<li><strong>Authors: </strong>Zheng Li, Dawei Zhu, Qilong Ma, Weimin Xiong, Sujian Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          EERPD: Leveraging Emotion and Emotion Regulation for Improving Personality Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Personality is a fundamental construct in psychology, reflecting an individual's behavior, thinking, and emotional patterns. Previous researches have made some progress in personality detection, primarily by utilizing the whole text to predict personality. However, these studies generally tend to overlook psychological knowledge: they rarely apply the well-established correlations between emotion regulation and personality. Based on this, we propose a new personality detection method called EERPD. This method introduces the use of emotion regulation, a psychological concept highly correlated with personality, for personality prediction. By combining this feature with emotion features, it retrieves few-shot examples and provides process CoTs for inferring labels from text. This approach enhances the understanding of LLM for personality within text and improves the performance in personality detection. Experimental results demonstrate that EERPD significantly enhances the accuracy and robustness of personality detection, outperforming previous SOTA by 15.05/4.29 in average F1 on the two benchmark datasets.</li>
<li><strong>摘要：</strong>性格是心理学的一个基本概念，反映了个体的行为、思维和情绪模式。先前的研究在性格检测方面取得了一些进展，主要是利用整个文本来预测性格。然而，这些研究通常倾向于忽视心理学知识：它们很少应用情绪调节和性格之间已建立的相关性。基于此，我们提出了一种新的性格检测方法，称为 EERPD。该方法引入了情绪调节这一与性格高度相关的心理概念，用于性格预测。通过将这一特征与情绪特征相结合，它检索少量样本示例并提供从文本中推断标签的过程 CoT。该方法增强了 LLM 对文本中性格的理解，并提高了性格检测的性能。实验结果表明，EERPD 显著提高了性格检测的准确性和鲁棒性，在两个基准数据集上的平均 F1 比之前的 SOTA 高出 15.05/4.29。</li>
</ul>

<h3>Title: Title:
          SEAM: A Stochastic Benchmark for Multi-Document Tasks</h3>
<ul>
<li><strong>Authors: </strong>Gili Lior, Avi Caciularu, Arie Cattan, Shahar Levy, Ori Shapira, Gabriel Stanovsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SEAM: A Stochastic Benchmark for Multi-Document Tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Various tasks, such as summarization, multi-hop question answering, or coreference resolution, are naturally phrased over collections of real-world documents. Such tasks present a unique set of challenges, revolving around the lack of coherent narrative structure across documents, which often leads to contradiction, omission, or repetition of information. Despite their real-world application and challenging properties, there is currently no benchmark which specifically measures the abilities of large language models (LLMs) on multi-document tasks. To bridge this gap, we present SEAM (a Stochastic Evaluation Approach for Multi-document tasks), a conglomerate benchmark over a diverse set of multi-document datasets, setting conventional evaluation criteria, input-output formats, and evaluation protocols. In particular, SEAM addresses the sensitivity of LLMs to minor prompt variations through repeated evaluations, where in each evaluation we sample uniformly at random the values of arbitrary factors (e.g., the order of documents). We evaluate different LLMs on SEAM finding that multi-document tasks pose a significant challenge for LLMs, even for state-of-the-art models with 70B parameters. In addition, we show that the stochastic approach uncovers underlying statistical trends which cannot be observed in a static benchmark. We hope that SEAM will spur progress via consistent and meaningful evaluation of multi-document tasks.</li>
<li><strong>摘要：</strong>各种任务，例如摘要、多跳问答或共指解析，都是在真实世界文档集合上自然表达的。此类任务提出了一系列独特的挑战，主要在于文档之间缺乏连贯的叙述结构，这通常会导致信息矛盾、遗漏或重复。尽管它们在现实世界中具有应用价值且具有挑战性，但目前还没有专门衡量大型语言模型 (LLM) 在多文档任务上的能力的基准。为了弥补这一差距，我们提出了 SEAM（一种用于多文档任务的随机评估方法），这是一种针对多种多文档数据集的综合基准，设置了传统的评估标准、输入输出格式和评估协议。具体而言，SEAM 通过重复评估解决了 LLM 对微小提示变化的敏感性，在每次评估中，我们都会随机均匀地抽取任意因素（例如文档顺序）的值。我们在 SEAM 上评估了不同的 LLM，发现多文档任务对 LLM 构成了重大挑战，即使对于具有 70B 个参数的最先进的模型也是如此。此外，我们还表明，随机方法揭示了静态基准无法观察到的潜在统计趋势。我们希望 SEAM 能够通过对多文档任务进行一致且有意义的评估来推动进步。</li>
</ul>

<h3>Title: Title:
          Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chulin Xie, Chiyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are typically multilingual due to pretraining on diverse multilingual corpora. But can these models relate corresponding concepts across languages, effectively being crosslingual? This study evaluates six state-of-the-art LLMs on inherently crosslingual tasks. We observe that while these models show promising surface-level crosslingual abilities on machine translation and embedding space analyses, they struggle with deeper crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in both general (MMLU benchmark) and domain-specific (Harry Potter quiz) contexts. We observe that simple inference-time mitigation methods offer only limited improvement. On the other hand, we propose fine-tuning of LLMs on mixed-language data, which effectively reduces these gaps, even when using out-of-domain datasets like WikiText. Our findings suggest the need for explicit optimization to unlock the full crosslingual potential of LLMs. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常是多语言的，因为它们在不同的多语言语料库上进行了预训练。但是这些模型能否将相应的概念跨语言关联起来，从而有效地实现跨语言？本研究评估了六种最先进的 LLM 在本质上是跨语言任务上的表现。我们观察到，虽然这些模型在机器翻译和嵌入空间分析上表现出良好的表面跨语言能力，但它们在更深层次的跨语言知识转移方面却举步维艰，揭示了在一般（MMLU 基准）和领域特定（哈利波特测验）环境中的跨语言知识障碍。我们观察到简单的推理时间缓解方法只能提供有限的改进。另一方面，我们建议在混合语言数据上对 LLM 进行微调，这可以有效地缩小这些差距，即使在使用 WikiText 等域外数据集时也是如此。我们的研究结果表明需要进行显式优化才能充分发挥 LLM 的跨语言潜力。我们的代码在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Title:
          Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step</h3>
<ul>
<li><strong>Authors: </strong>Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.</li>
<li><strong>摘要：</strong>目前的研究发现，大型语言模型 (LLM) 中存在早期回答问题，即模型在生成思维链 (CoT) 之前已经有了答案。这种现象表明预测答案和推理过程之间可能缺乏必要的依赖关系。因此，出现了两个重要问题：(1) 如果模型已经有了答案，那么 CoT 是否仍然有必要？(2) 答案的正确性是否可以作为 CoT 正确性的有效证据？为了解决这些问题，我们提出了一种名为探测链 (CoP) 的方法，用于探测模型推理过程中的思维变化。探测结果表明，在大量问答案例中，CoT 似乎是不必要的，这种必要性与任务的简单性相关，由所需的推理步骤定义。此外，通过分析思维变化的模式，我们可以检查模型推理的正确性。我们的验证表明，许多答案虽然最终答案正确，但其推理过程存在错误。为此，我们提出一种基于 CoP 的策略方法，在多个候选答案中优先考虑具有正确推理的答案，从而增强模型推理的可靠性。</li>
</ul>

<h3>Title: Title:
          Towards Region-aware Bias Evaluation Metrics</h3>
<ul>
<li><strong>Authors: </strong>Angana Borah, Aparna Garimella, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Region-aware Bias Evaluation Metrics(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>When exposed to human-generated data, language models are known to learn and amplify societal biases. While previous works introduced benchmarks that can be used to assess the bias in these models, they rely on assumptions that may not be universally true. For instance, a gender bias dimension commonly used by these metrics is that of family--career, but this may not be the only common bias in certain regions of the world. In this paper, we identify topical differences in gender bias across different regions and propose a region-aware bottom-up approach for bias assessment. Our proposed approach uses gender-aligned topics for a given region and identifies gender bias dimensions in the form of topic pairs that are likely to capture gender societal biases. Several of our proposed bias topic pairs are on par with human perception of gender biases in these regions in comparison to the existing ones, and we also identify new pairs that are more aligned than the existing ones. In addition, we use our region-aware bias topic pairs in a Word Embedding Association Test (WEAT)-based evaluation metric to test for gender biases across different regions in different data domains. We also find that LLMs have a higher alignment to bias pairs for highly-represented regions showing the importance of region-aware bias evaluation metric.</li>
<li><strong>摘要：</strong>众所周知，当接触到人类生成的数据时，语言模型会学习并放大社会偏见。虽然先前的研究引入了可用于评估这些模型中的偏见的基准，但它们依赖的假设可能并非普遍正确。例如，这些指标常用的性别偏见维度是家庭-职业，但这可能不是世界某些地区唯一的常见偏见。在本文中，我们确定了不同地区性别偏见的主题差异，并提出了一种区域感知的自下而上的偏见评估方法。我们提出的方法使用给定区域的性别一致主题，并以可能捕捉性别社会偏见的主题对的形式识别性别偏见维度。与现有偏见主题对相比，我们提出的几个偏见主题对与人类对这些地区性别偏见的感知相当，我们还确定了比现有偏见主题对更一致的新偏见主题对。此外，我们在基于词嵌入关联测试 (WEAT) 的评估指标中使用区域感知偏见主题对来测试不同数据域中不同地区的性别偏见。我们还发现，对于高度代表性的区域，LLM 与偏差对具有更高的一致性，这表明了区域感知偏差评估指标的重要性。</li>
</ul>

<h3>Title: Title:
          FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Harish Tayyar Madabushi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models. Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. We conduct experiments to demonstrate the effectiveness of this method both in terms of retrieval effectiveness and in terms of the relevance of the frames and frame relations automatically generated. Our results show that this novel mechanism of Frame Semantic-based retrieval, designed to improve Retrieval Augmented Generation (FS-RAG), is effective and offers potential for providing data-driven insights into frame semantics theory. We provide open access to our program code and prompts.</li>
<li><strong>摘要：</strong>我们提出了一种新的检索增强生成扩展方法，旨在减轻大型语言模型输出中的事实不准确性。具体来说，我们的方法借鉴了框架语义的认知语言理论，用于索引和检索与帮助大型语言模型回答查询相关的事实信息。我们进行了实验，以证明该方法在检索有效性以及自动生成的框架和框架关系的相关性方面的有效性。我们的结果表明，这种旨在改进检索增强生成 (FS-RAG) 的基于框架语义的检索新机制是有效的，并有可能为框架语义理论提供数据驱动的见解。我们开放我们的程序代码和提示。</li>
</ul>

<h3>Title: Title:
          LLMs' Classification Performance is Overclaimed</h3>
<ul>
<li><strong>Authors: </strong>Hanzi Xu, Renze Lou, Jiangshu Du, Vahid Mahzoon, Elmira Talebianaraki, Zhuoan Zhou, Elizabeth Garrison, Slobodan Vucetic, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLMs' Classification Performance is Overclaimed(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In many classification tasks designed for AI or human to solve, gold labels are typically included within the label space by default, often posed as "which of the following is correct?" This standard setup has traditionally highlighted the strong performance of advanced AI, particularly top-performing Large Language Models (LLMs), in routine classification tasks. However, when the gold label is intentionally excluded from the label space, it becomes evident that LLMs still attempt to select from the available label candidates, even when none are correct. This raises a pivotal question: Do LLMs truly demonstrate their intelligence in understanding the essence of classification tasks? In this study, we evaluate both closed-source and open-source LLMs across representative classification tasks, arguing that the perceived performance of LLMs is overstated due to their inability to exhibit the expected comprehension of the task. This paper makes a threefold contribution: i) To our knowledge, this is the first work to identify the limitations of LLMs in classification tasks when gold labels are absent. We define this task as Classify-w/o-Gold and propose it as a new testbed for LLMs. ii) We introduce a benchmark, Know-No, comprising two existing classification tasks and one new task, to evaluate Classify-w/o-Gold. iii) This work defines and advocates for a new evaluation metric, OmniAccuracy, which assesses LLMs' performance in classification tasks both when gold labels are present and absent.</li>
<li><strong>摘要：</strong>在许多为人工智能或人类设计的分类任务中，黄金标签通常默认包含在标签空间中，通常表示为“以下哪项是正确的？”这种标准设置传统上突出了高级人工智能，尤其是表现最佳的大型语言模型 (LLM) 在常规分类任务中的强大性能。然而，当黄金标签被故意排除在标签空间之外时，很明显 LLM 仍然会尝试从可用的标签候选中进行选择，即使没有一个是正确的。这提出了一个关键问题：LLM 是否真正展示了它们在理解分类任务本质方面的智慧？在本研究中，我们在代表性分类任务中评估了闭源和开源 LLM，认为 LLM 的感知性能被夸大了，因为它们无法表现出对任务的预期理解。本文做出了三方面的贡献：i) 据我们所知，这是第一篇确定在没有黄金标签的情况下 LLM 在分类任务中的局限性的工作。我们将此任务定义为 Classify-w/o-Gold，并提议将其作为 LLM 的新试验台。ii) 我们引入一个基准 Know-No，它包含两个现有的分类任务和一个新任务，以评估 Classify-w/o-Gold。iii) 这项工作定义并提倡一种新的评估指标 OmniAccuracy，它在存在和不存在黄金标签的情况下评估 LLM 在分类任务中的表现。</li>
</ul>

<h3>Title: Title:
          Multi-Objective Linguistic Control of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Jiuhai Chen, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multi-Objective Linguistic Control of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), despite their breakthroughs on many challenging benchmark tasks, lean to generate verbose responses and lack the controllability of output complexity, which is usually preferred by human users in practice. In this paper, we study how to precisely control multiple linguistic complexities of LLM output by finetuning using off-the-shelf data. To this end, we propose multi-control tuning (MCTune), which includes multiple linguistic complexity values of ground-truth responses as controls in the input for instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM datasets. Evaluations on widely used benchmarks demonstrate that our method does not only improve LLMs' multi-complexity controllability substantially but also retains or even enhances the quality of the responses as a side benefit.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 尽管在许多具有挑战性的基准任务上取得了突破，但倾向于生成冗长的响应，缺乏输出复杂度的可控性，而这在实践中通常是人类用户所偏爱的。在本文中，我们研究如何通过使用现成的数据进行微调来精确控制 LLM 输出的多种语言复杂度。为此，我们提出了多控制调优 (MCTune)，其中包括多个真实响应的语言复杂度值作为指令调优输入中的控制。我们在 Alpaca-GPT4 和 WizardLM 数据集上对 LLaMA2-7B 进行了微调。在广泛使用的基准测试上的评估表明，我们的方法不仅大大提高了 LLM 的多重复杂度可控性，而且作为附带好处，还保留甚至提高了响应的质量。</li>
</ul>

<h3>Title: Title:
          Preference Tuning For Toxicity Mitigation Generalizes Across Languages</h3>
<ul>
<li><strong>Authors: </strong>Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Preference Tuning For Toxicity Mitigation Generalizes Across Languages(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8% to 3.9% across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.</li>
<li><strong>摘要：</strong>由于多语言大型语言模型 (LLM) 在全球范围内的使用日益广泛，对其进行去毒化变得至关重要。在这项工作中，我们探索了去毒化 LLM 中偏好调整的零样本跨语言泛化。与之前的研究显示其他安全任务的跨语言泛化有限不同，我们证明仅使用英语数据进行直接偏好优化 (DPO) 训练可以显著降低多语言开放式生成中的毒性。例如，经过训练后，mGPT-1.3B 在 17 种不同语言中产生有毒延续的概率从 46.8% 下降到 3.9%。我们的结果还扩展到其他多语言 LLM，例如 BLOOM、Llama3 和 Aya-23。使用因果干预和激活分析等机械可解释性工具，我们确定了 LLM 中 MLP 层的双重多语言性属性，这解释了 DPO 的跨语言泛化。最后，我们表明双语句子检索可以预测 DPO 偏好调整的跨语言可迁移性。</li>
</ul>

<h3>Title: Title:
          LLMs assist NLP Researchers: Critique Paper (Meta-)Reviewing</h3>
<ul>
<li><strong>Authors: </strong>Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Jiayang Cheng, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLMs assist NLP Researchers: Critique Paper (Meta-)Reviewing(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work is motivated by two key trends. On one hand, large language models (LLMs) have shown remarkable versatility in various generative tasks such as writing, drawing, and question answering, significantly reducing the time required for many routine tasks. On the other hand, researchers, whose work is not only time-consuming but also highly expertise-demanding, face increasing challenges as they have to spend more time reading, writing, and reviewing papers. This raises the question: how can LLMs potentially assist researchers in alleviating their heavy workload? This study focuses on the topic of LLMs assist NLP Researchers, particularly examining the effectiveness of LLM in assisting paper (meta-)reviewing and its recognizability. To address this, we constructed the ReviewCritique dataset, which includes two types of information: (i) NLP papers (initial submissions rather than camera-ready) with both human-written and LLM-generated reviews, and (ii) each review comes with "deficiency" labels and corresponding explanations for individual segments, annotated by experts. Using ReviewCritique, this study explores two threads of research questions: (i) "LLMs as Reviewers", how do reviews generated by LLMs compare with those written by humans in terms of quality and distinguishability? (ii) "LLMs as Metareviewers", how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews? To our knowledge, this is the first work to provide such a comprehensive analysis.</li>
<li><strong>摘要：</strong>这项工作的动机有两个主要趋势。一方面，大型语言模型 (LLM) 在各种生成任务（如写作、绘画和问答）中表现出了非凡的多功能性，大大减少了许多日常任务所需的时间。另一方面，研究人员的工作不仅耗时，而且对专业知识要求很高，他们面临着越来越大的挑战，因为他们必须花更多时间阅读、撰写和审阅论文。这就提出了一个问题：LLM 如何潜在地帮助研究人员减轻繁重的工作量？本研究重点关注 LLM 协助 NLP 研究人员的主题，特别是研究 LLM 在协助论文（元）审阅方面的有效性及其可识别性。为了解决这个问题，我们构建了 ReviewCritique 数据集，其中包括两种类型的信息：(i) NLP 论文（初始提交而不是照相排版），包括人工撰写和 LLM 生成的评论，以及 (ii) 每篇评论都带有“缺陷”标签和专家注释的各个部分的相应解释。本研究使用 ReviewCritique 探索了两个研究问题：（i）“LLM 作为审稿人”，LLM 生成的评论与人类撰写的评论在质量和可区分性方面相比如何？（ii）“LLM 作为元审稿人”，LLM 如何有效地识别单个论文评论中的潜在问题，例如有缺陷或不专业的评论部分？据我们所知，这是第一篇提供如此全面分析的作品。</li>
</ul>

<h3>Title: Title:
          One Thousand and One Pairs: A "novel" challenge for long-context language models</h3>
<ul>
<li><strong>Authors: </strong>Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, Mohit Iyyer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          One Thousand and One Pairs: A "novel" challenge for long-context language models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.</li>
<li><strong>摘要：</strong>合成的长上下文 LLM 基准测试（例如“大海捞针”）仅测试表面级别的检索能力，但长上下文 LLM 在多大程度上能够检索、合成和推理书本长度输入中的信息？我们通过创建 NoCha 来解决这个问题，NoCha 是一个由 67 本最近出版的英文小说的人类读者撰写的 1,001 对真实和虚假声明的最小差异数据集。与现有的长上下文基准测试相比，我们的注释者确认 NoCha 中的大部分对需要对整本书进行全局推理才能验证。我们的实验表明，虽然人类读者可以轻松完成这项任务，但对于我们评估的所有十个长上下文 LLM 来说，这都是极具挑战性的：没有一个开放权重模型的表现超过随机机会（尽管它们在合成基准测试中表现强劲），而 GPT-4o 的准确率最高，为 55.8%。进一步分析表明：(1) 平均而言，模型在仅需要句子级检索的配对上的表现要好于全局推理；(2) 即使对于正确标记的声明，模型对其决策的生成解释通常也不准确；(3) 模型在包含大量世界构建的推测性小说书籍上的表现要差得多。NoCha 中提出的方法允许基准数据集的演变和未来模型的轻松分析。</li>
</ul>

<h3>Title: Title:
          Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Choonghyun Park, Hyuhng Joon Kim, Junyeob Kim, Youna Kim, Taeuk Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-goo Lee, Kang Min Yoo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>AI Generated Text (AIGT) detectors are developed with texts from humans and LLMs of common tasks. Despite the diversity of plausible prompt choices, these datasets are generally constructed with a limited number of prompts. The lack of prompt variation can introduce prompt-specific shortcut features that exist in data collected with the chosen prompt, but do not generalize to others. In this paper, we analyze the impact of such shortcuts in AIGT detection. We propose Feedback-based Adversarial Instruction List Optimization (FAILOpt), an attack that searches for instructions deceptive to AIGT detectors exploiting prompt-specific shortcuts. FAILOpt effectively drops the detection performance of the target detector, comparable to other attacks based on adversarial in-context examples. We also utilize our method to enhance the robustness of the detector by mitigating the shortcuts. Based on the findings, we further train the classifier with the dataset augmented by FAILOpt prompt. The augmented classifier exhibits improvements across generation models, tasks, and attacks. Our code will be available at this https URL.</li>
<li><strong>摘要：</strong>AI 生成文本 (AIGT) 检测器是使用人类文本和常见任务的 LLM 开发的。尽管合理的提示选择多种多样，但这些数据集通常是用有限数量的提示构建的。提示变化的缺乏可能会引入提示特定的快捷方式特征，这些特征存在于使用所选提示收集的数据中，但不会推广到其他数据。在本文中，我们分析了此类快捷方式对 AIGT 检测的影响。我们提出了基于反馈的对抗性指令列表优化 (FAILOpt)，这是一种利用提示特定快捷方式搜索欺骗 AIGT 检测器的指令的攻击。FAILOpt 有效地降低了目标检测器的检测性能，与其他基于对抗性上下文示例的攻击相当。我们还利用我们的方法通过缓解快捷方式来增强检测器的鲁棒性。根据研究结果，我们进一步使用由 FAILOpt 提示增强的数据集训练分类器。增强的分类器在生成模型、任务和攻击中表现出改进。我们的代码将在此 https URL 上提供。</li>
</ul>

<h3>Title: Title:
          PlagBench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection</h3>
<ul>
<li><strong>Authors: </strong>Jooyoung Lee, Toshini Agrawal, Adaku Uchendu, Thai Le, Jinghui Chen, Dongwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          PlagBench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent literature has highlighted potential risks to academic integrity associated with large language models (LLMs), as they can memorize parts of training instances and reproduce them in the generated texts without proper attribution. In addition, given their capabilities in generating high-quality texts, plagiarists can exploit LLMs to generate realistic paraphrases or summaries indistinguishable from original work. In response to possible malicious use of LLMs in plagiarism, we introduce PlagBench, a comprehensive dataset consisting of 46.5K synthetic plagiarism cases generated using three instruction-tuned LLMs across three writing domains. The quality of PlagBench is ensured through fine-grained automatic evaluation for each type of plagiarism, complemented by human annotation. We then leverage our proposed dataset to evaluate the plagiarism detection performance of five modern LLMs and three specialized plagiarism checkers. Our findings reveal that GPT-3.5 tends to generates paraphrases and summaries of higher quality compared to Llama2 and GPT-4. Despite LLMs' weak performance in summary plagiarism identification, they can surpass current commercial plagiarism detectors. Overall, our results highlight the potential of LLMs to serve as robust plagiarism detection tools.</li>
<li><strong>摘要：</strong>最近的文献强调了大型语言模型 (LLM) 对学术诚信的潜在风险，因为它们可以记住部分训练实例并在生成的文本中重现它们而无需适当归因。此外，鉴于它们能够生成高质量文本，剽窃者可以利用 LLM 生成与原作难以区分的真实释义或摘要。为了应对 LLM 可能被恶意用于剽窃的情况，我们推出了 PlagBench，这是一个全面的数据集，包含 46.5K 个合成剽窃案例，这些案例是使用三个指令调整的 LLM 在三个写作领域生成的。PlagBench 的质量通过对每种剽窃类型的细粒度自动评估以及人工注释来保证。然后，我们利用我们提出的数据集来评估五个现代 LLM 和三个专门的剽窃检查器的剽窃检测性能。我们的研究结果表明，与 Llama2 和 GPT-4 相比，GPT-3.5 倾向于生成更高质量的释义和摘要。尽管 LLM 在摘要抄袭识别方面表现不佳，但它们可以超越当前的商业抄袭检测器。总体而言，我们的结果凸显了 LLM 作为强大的抄袭检测工具的潜力。</li>
</ul>

<h3>Title: Title:
          LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments</h3>
<ul>
<li><strong>Authors: </strong>Zixia Jia, Mengmeng Wang, Baichen Tong, Song-Chun Zhu, Zilong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely on language descriptions as inputs. However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments. To address this gap, we introduce LangSuitE, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds. Compared with previous LLM-based testbeds, LangSuitE (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents' capacity to develop ``internalized world knowledge'' with embodied observations, and (iii) allows easy customization of communication and action strategies. To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t. history information. Comprehensive benchmark results illustrate challenges and insights of embodied planning. LangSuitE represents a significant step toward building embodied generalists in the context of language models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展在构建依赖语言描述作为输入的自主代理方面取得了令人鼓舞的成就。然而，目前尚不清楚 LLM 在动态交互环境中作为少样本或零样本具身代理的性能如何。为了解决这一差距，我们引入了 LangSuitE，这是一个多功能且无需模拟的测试平台，具有 6 个具有代表性的文本具身世界中的具身任务。与以前基于 LLM 的测试平台相比，LangSuitE (i) 无需多个模拟引擎即可适应各种环境，(ii) 评估代理通过具身观察开发“内化世界知识”的能力，以及 (iii) 允许轻松定制沟通和行动策略。为了应对具身挑战，我们设计了一种新颖的思路链 (CoT) 模式 EmMem，它总结了相对于历史信息的具身状态。全面的基准测试结果说明了具身规划的挑战和见解。LangSuitE 代表了在语言模型背景下构建具身通才的重要一步。</li>
</ul>

<h3>Title: Title:
          Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other</h3>
<ul>
<li><strong>Authors: </strong>Yifei Gao, Jie Ou, Lei Wang, Yuting Xiao, Zhiyuan Xiang, Ruiting Dai, Jun Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Emergent Large Language Models (LLMs) use their extraordinary performance and powerful deduction capacity to discern from traditional language models. However, the expenses of computational resources and storage for these LLMs are stunning, quantization then arises as a trending conversation. To address accuracy decay caused by quantization, two streams of works in post-training quantization methods stand out. One uses other weights to compensate existing quantization error, while the other transfers the quantization difficulty to other parts in the model. Combining both merits, we introduce Learnable Singular value Increment (LSI) as an advanced solution. LSI uses Singular Value Decomposition to extract singular values of the weights and make them learnable to help weights compensate each other conditioned on activation. Incorporating LSI with existing techniques, we achieve state-of-the-art performance in diverse quantization settings, no matter in weight-only, weight-activation or extremely low bit scenarios. By unleashing the potential of LSI, efficient finetuning on quantized model is no longer a prohibitive problem.</li>
<li><strong>摘要：</strong>新兴的大型语言模型 (LLM) 利用其非凡的性能和强大的推理能力来与传统语言模型区分开来。然而，这些 LLM 的计算资源和存储成本令人震惊，量化随之成为热门话题。为了解决量化导致的准确率下降问题，训练后量化方法中的两大流派脱颖而出。一种使用其他权重来补偿现有的量化误差，而另一种将量化难度转移到模型中的其他部分。结合这两种优点，我们引入了可学习奇异值增量 (LSI) 作为一种高级解决方案。LSI 使用奇异值分解来提取权重的奇异值并使其可学习，以帮助权重在激活条件下相互补偿。将 LSI 与现有技术相结合，我们在各种量化设置中实现了最佳性能，无论是在仅权重、权重激活还是极低位场景中。通过释放 LSI 的潜力，对量化模型进行有效微调不再是一个难以解决的问题。</li>
</ul>

<h3>Title: Title:
          Cascade Reward Sampling for Efficient Decoding-Time Alignment</h3>
<ul>
<li><strong>Authors: </strong>Bolian Li, Yifan Wang, Ananth Grama, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Cascade Reward Sampling for Efficient Decoding-Time Alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human preferences is critical for their deployment. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that requires no fine-tuning of model parameters. However, generating text that achieves both high reward and high likelihood remains a significant challenge. Existing methods often fail to generate high-reward text or incur substantial computational costs. In this paper, we propose Cascade Reward Sampling (CARDS) to address both issues, guaranteeing the generation of high-reward and high-likelihood text with significantly low costs. Based on our analysis of reward models (RMs) on incomplete text and our observation that high-reward prefixes induce high-reward complete text, we use rejection sampling to iteratively generate small semantic segments to form such prefixes. The segment length is dynamically determined by the predictive uncertainty of LLMs. This strategy guarantees desirable prefixes for subsequent generations and significantly reduces wasteful token re-generations and the number of reward model scoring. Our experiments demonstrate substantial gains in both generation efficiency and alignment ratings compared to the baselines, achieving five times faster text generation and 99\% win-ties in GPT-4/Claude-3 helpfulness evaluation.</li>
<li><strong>摘要：</strong>将大型语言模型 (LLM) 与人类偏好对齐对于其部署至关重要。最近，解码时间对齐已成为一种有效的即插即用技术，无需对模型参数进行微调。然而，生成同时实现高奖励和高可能性的文本仍然是一项重大挑战。现有方法通常无法生成高奖励文本或产生大量计算成本。在本文中，我们提出了级联奖励采样 (CARDS) 来解决这两个问题，保证以极低的成本生成高奖励和高可能性的文本。基于我们对不完整文本的奖励模型 (RM) 的分析以及我们对高奖励前缀会诱导高奖励完整文本的观察，我们使用拒绝采样迭代生成小的语义段以形成此类前缀。段长度由 LLM 的预测不确定性动态确定。此策略保证了后续生成的理想前缀，并显着减少了浪费的标记重新生成和奖励模型评分的次数。我们的实验表明，与基线相比，生成效率和对齐评级都有显著提高，在 GPT-4 / Claude-3 有用性评估中实现了五倍的文本生成速度和 99％ 的胜利率。</li>
</ul>

<h3>Title: Title:
          Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Yuu Jinnai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English, reflecting only the preferences of English-speaking annotators. Nevertheless, it is common practice to use the English preference data, either directly or by translating it into the target language, when aligning a multilingual language model. The question is whether such an alignment strategy marginalizes the preference of non-English speaking users. To this end, we investigate the effect of aligning Japanese language models with (mostly) English resources. In particular, we focus on evaluating whether the commonsense morality of the resulting fine-tuned models is aligned with Japanese culture using the JCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show that the fine-tuned model outperforms the SFT model. However, it does not demonstrate the same level of improvement as a model fine-tuned using the JCM, suggesting that while some aspects of commonsense morality are transferable, others may not be.</li>
<li><strong>摘要：</strong>将语言模型与人类偏好对齐是一种使语言模型对最终用户有用的常见方法。然而，大多数对齐工作都是用英语完成的，而人类偏好数据集以英语为主，仅反映了英语注释者的偏好。尽管如此，在对齐多语言语言模型时，通常使用英语偏好数据（直接使用或将其翻译成目标语言）。问题是这种对齐策略是否会边缘化非英语用户的偏好。为此，我们研究了将日语语言模型与（主要是）英语资源对齐的效果。具体来说，我们专注于使用 JCommonsenseMorality (JCM) 和 ETHICS 数据集评估所得微调模型的常识道德是否与日本文化相符。实验结果表明，微调模型优于 SFT 模型。然而，它并没有表现出与使用 JCM 微调的模型相同程度的改进，这表明虽然常识道德的某些方面是可以转移的，但其他方面可能不能。</li>
</ul>

<h3>Title: Title:
          What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Michal Golovanevsky, William Rudman, Vedant Palit, Ritambhara Singh, Carsten Eickhoff</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have gained community-spanning prominence due to their ability to integrate visual and textual inputs to perform complex tasks. Despite their success, the internal decision-making processes of these models remain opaque, posing challenges in high-stakes applications. To address this, we introduce NOTICE, the first Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE incorporates a Semantic Minimal Pairs (SMP) framework for image corruption and Symmetric Token Replacement (STR) for text. This approach enables semantically meaningful causal mediation analysis for both modalities, providing a robust method for analyzing multimodal integration within models like BLIP. Our experiments on the SVO-Probes, MIT-States, and Facial Expression Recognition datasets reveal crucial insights into VLM decision-making, identifying the significant role of middle-layer cross-attention heads. Further, we uncover a set of ``universal cross-attention heads'' that consistently contribute across tasks and modalities, each performing distinct functions such as implicit image segmentation, object inhibition, and outlier inhibition. This work paves the way for more transparent and interpretable multimodal systems.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 因其能够整合视觉和文本输入以执行复杂任务的能力而获得了社区的广泛认可。尽管取得了成功，但这些模型的内部决策过程仍然不透明，这对高风险应用构成了挑战。为了解决这个问题，我们推出了 NOTICE，这是第一个用于 VLM 中机械可解释性的无噪声文本图像损坏和评估管道。NOTICE 结合了用于图像损坏的语义最小对 (SMP) 框架和用于文本的对称标记替换 (STR)。这种方法可以对两种模态进行语义上有意义的因果中介分析，为分析 BLIP 等模型中的多模态集成提供了一种强大的方法。我们在 SVO-Probes、MIT-States 和面部表情识别数据集上的实验揭示了对 VLM 决策的重要见解，确定了中间层交叉注意力头的重要作用。此外，我们发现了一组“通用交叉注意力头”，它们在各种任务和模态中持续发挥作用，每个头都执行不同的功能，例如隐式图像分割、对象抑制和异常值抑制。这项工作为更透明、更易于解释的多模态系统铺平了道路。</li>
</ul>

<h3>Title: Title:
          Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging</h3>
<ul>
<li><strong>Authors: </strong>Deyuan Liu, Zhanyue Qin, Hairu Wang, Zhao Yang, Zecheng Wang, Fangying Rong, Qingbin Liu, Yanchao Hao, Xi Chen, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在许多领域都表现出色，但其复杂性和规模对资源有限环境中的部署提出了挑战。当前的压缩技术（例如参数修剪）通常无法有效利用修剪参数中的知识。为了应对这些挑战，我们提出了基于流形的知识对齐和层合并压缩 (MKA)，这是一种新颖的方法，它使用流形学习和规范化成对信息瓶颈 (NPIB) 度量来合并相似的层，从而减小模型大小，同时保留基本性能。我们在多个基准数据集和各种 LLM 上评估了 MKA。我们的研究结果表明，MKA 不仅保留了模型性能，而且还实现了显着的压缩比，优于传统的修剪方法。此外，当与量化结合时，MKA 可提供更大的压缩率。具体而言，在使用 Llama3-8B 模型的 MMLU 数据集上，MKA 实现了 43.75% 的压缩率，而性能仅下降了 2.82%。提出的 MKA 方法为 LLM 提供了一种资源高效且性能保持的模型压缩技术。</li>
</ul>

<h3>Title: Title:
          EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Yeonsu Kwon, Jiho Kim, Gyubok Lee, Seongsu Bae, Daeun Kyung, Wonchul Cha, Tom Pollard, Alistair Johnson, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs. EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 3,943 entities across 105 clinical notes checked against database entries for consistency. EHRCon has two versions, one using the original MIMIC-III schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings. The code is available at this https URL.</li>
<li><strong>摘要：</strong>电子健康记录 (EHR) 是存储全面患者医疗记录不可或缺的部分，它将结构化数据（例如药物）与详细的临床笔记（例如医生笔记）结合在一起。这些元素对于直接的数据检索至关重要，并能为患者护理提供深入的背景见解。然而，由于 EHR 系统设计不直观以及人为错误，它们经常会出现差异，对患者安全构成严重风险。为了解决这个问题，我们开发了 EHRCon，这是一种新的数据集和任务，专门用于确保 EHR 中的结构化表和非结构化笔记之间的数据一致性。EHRCon 是与医疗专业人士合作使用 MIMIC-III EHR 数据集制作的，包括 105 份临床笔记中 3,943 个实体的手动注释，并与数据库条目进行一致性检查。EHRCon 有两个版本，一个使用原始 MIMIC-III 模式，另一个使用 OMOP CDM 模式，以提高其适用性和通用性。此外，利用大型语言模型的功能，我们引入了 CheckEHR，这是一种用于验证临床记录和数据库表之间一致性的新框架。CheckEHR 采用八阶段流程，在少样本和零样本设置中均显示出良好的结果。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          ADVSCORE: A Metric for the Evaluation and Creation of Adversarial Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Yoo Yeon Sung, Eve Fleisig, Ishani Mondal, Jordan Lee Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ADVSCORE: A Metric for the Evaluation and Creation of Adversarial Benchmarks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Adversarial benchmarks validate model abilities by providing samples that fool models but not humans. However, despite the proliferation of datasets that claim to be adversarial, there does not exist an established metric to evaluate how adversarial these datasets are. To address this lacuna, we introduce ADVSCORE, a metric which quantifies how adversarial and discriminative an adversarial dataset is and exposes the features that make data adversarial. We then use ADVSCORE to underpin a dataset creation pipeline that incentivizes writing a high-quality adversarial dataset. As a proof of concept, we use ADVSCORE to collect an adversarial question answering (QA) dataset, ADVQA, from our pipeline. The high-quality questions in ADVQA surpasses three adversarial benchmarks across domains at fooling several models but not humans. We validate our result based on difficulty estimates from 9,347 human responses on four datasets and predictions from three models. Moreover, ADVSCORE uncovers which adversarial tactics used by human writers fool models (e.g., GPT-4) but not humans. Through ADVSCORE and its analyses, we offer guidance on revealing language model vulnerabilities and producing reliable adversarial examples.</li>
<li><strong>摘要：</strong>对抗性基准通过提供可以欺骗模型但不能欺骗人类的样本来验证模型能力。然而，尽管声称具有对抗性的数据集激增，但目前还没有一个既定的指标来评估这些数据集的对抗性。为了解决这个问题，我们引入了 ADVSCORE，这是一个量化对抗性数据集的对抗性和判别性并揭示使数据具有对抗性的特征的指标。然后，我们使用 ADVSCORE 来支持数据集创建管道，以激励编写高质量的对抗性数据集。作为概念验证，我们使用 ADVSCORE 从我们的管道中收集对抗性问答 (QA) 数据集 ADVQA。ADVQA 中的高质量问题在欺骗多个模型但欺骗不了人类方面超越了跨领域的三个对抗性基准。我们根据四个数据集上 9,347 个人类响应的难度估计和三个模型的预测来验证我们的结果。此外，ADVSCORE 还揭示了人类作家使用的哪些对抗策略可以欺骗模型（例如 GPT-4），但不能欺骗人类。通过 ADVSCORE 及其分析，我们提供了揭示语言模型漏洞和生成可靠对抗示例的指导。</li>
</ul>

<h3>Title: Title:
          Evaluation of Instruction-Following Ability for Large Language Models on Story-Ending Generation</h3>
<ul>
<li><strong>Authors: </strong>Rem Hida, Junki Ohmura, Toshiyuki Sekiya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluation of Instruction-Following Ability for Large Language Models on Story-Ending Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Instruction-tuned Large Language Models (LLMs) have achieved remarkable performance across various benchmark tasks. While providing instructions to LLMs for guiding their generations is user-friendly, assessing their instruction-following capabilities is still unclarified due to a lack of evaluation metrics. In this paper, we focus on evaluating the instruction-following ability of LLMs in the context of story-ending generation, which requires diverse and context-specific instructions. We propose an automatic evaluation pipeline that utilizes a machine reading comprehension (MRC) model to determine whether the generated story-ending reflects instruction. Our findings demonstrate that our proposed metric aligns with human evaluation. Furthermore, our experiments confirm that recent open-source LLMs can achieve instruction-following performance close to GPT-3.5, as assessed through automatic evaluation.</li>
<li><strong>摘要：</strong>指令调优的大型语言模型 (LLM) 在各种基准测试任务中都取得了显著的表现。虽然向 LLM 提供指令来指导其生成是用户友好的，但由于缺乏评估指标，评估其指令遵循能力仍不明确。在本文中，我们专注于在故事结尾生成的背景下评估 LLM 的指令遵循能力，这需要多样化和特定于上下文的指令。我们提出了一种自动评估流程，该流程利用机器阅读理解 (MRC) 模型来确定生成的故事结尾是否反映了指令。我们的研究结果表明，我们提出的指标与人工评估一致。此外，我们的实验证实，通过自动评估评估，最近的开源 LLM 可以实现接近 GPT-3.5 的指令遵循性能。</li>
</ul>

<h3>Title: Title:
          KEHRL: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongyang Li, Taolin Zhang, Longtao Huang, Chengyu Wang, Xiaofeng He, Hui Xue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          KEHRL: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation triples from knowledge graphs (KGs) and integrate these external data sources into language models via self-supervised learning. Previous works treat knowledge enhancement as two independent operations, i.e., knowledge injection and knowledge integration. In this paper, we propose to learn Knowledge-Enhanced language representations with Hierarchical Reinforcement Learning (KEHRL), which jointly addresses the problems of detecting positions for knowledge injection and integrating external knowledge into the model in order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a high-level reinforcement learning (RL) agent utilizes both internal and prior knowledge to iteratively detect essential positions in texts for knowledge injection, which filters out less meaningful entities to avoid diverting the knowledge learning direction. Once the entity positions are selected, a relevant triple filtration module is triggered to perform low-level RL to dynamically refine the triples associated with polysemic entities through binary-valued actions. Experiments validate KEHRL's effectiveness in probing factual knowledge and enhancing the model's performance on various natural language understanding tasks.</li>
<li><strong>摘要：</strong>知识增强型预训练语言模型 (KEPLM) 利用知识图谱 (KG) 中的关系三元组，并通过自监督学习将这些外部数据源集成到语言模型中。以前的研究将知识增强视为两个独立的操作，即知识注入和知识集成。在本文中，我们提出使用分层强化学习 (KEHRL) 来学习知识增强型语言表示，它共同解决了检测知识注入位置和将外部知识集成到模型中的问题，以避免注入不准确或不相关的知识。具体而言，高级强化学习 (RL) 代理利用内部和先验知识迭代地检测文本中的重要位置以进行知识注入，从而过滤掉意义较小的实体，以避免转移知识学习方向。一旦选择了实体位置，就会触发相关的三元组过滤模块来执行低级 RL，以通过二值动作动态细化与多义实体相关的三元组。实验验证了 KEHRL 在探索事实知识和提高模型在各种自然语言理解任务中的性能方面的有效性。</li>
</ul>

<h3>Title: Title:
          On the Transformations across Reward Model, Parameter Update, and In-Context Prompt</h3>
<ul>
<li><strong>Authors: </strong>Deng Cai, Huayang Li, Tingchen Fu, Siheng Li, Weiwen Xu, Shuaiyi Li, Bowen Cao, Zhisong Zhang, Xinting Huang, Leyang Cui, Yan Wang, Lemao Liu, Taro Watanabe, Shuming Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          On the Transformations across Reward Model, Parameter Update, and In-Context Prompt(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite the general capabilities of pre-trained large language models (LLMs), they still need further adaptation to better serve practical applications. In this paper, we demonstrate the interchangeability of three popular and distinct adaptation tools: parameter updating, reward modeling, and in-context prompting. This interchangeability establishes a triangular framework with six transformation directions, each of which facilitates a variety of applications. Our work offers a holistic view that unifies numerous existing studies and suggests potential research directions. We envision our work as a useful roadmap for future research on LLMs.</li>
<li><strong>摘要：</strong>尽管预训练大型语言模型 (LLM) 具有通用功能，但它们仍需要进一步调整才能更好地服务于实际应用。在本文中，我们展示了三种流行且不同的适应工具的可互换性：参数更新、奖励建模和上下文提示。这种可互换性建立了一个具有六个转换方向的三角框架，每个方向都促进了各种应用。我们的工作提供了一个整体观点，统一了众多现有研究并提出了潜在的研究方向。我们设想我们的工作是未来 LLM 研究的有用路线图。</li>
</ul>

<h3>Title: Title:
          UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhanyue Qin, Haochuan Wang, Deyuan Liu, Ziyang Song, Cunhang Fan, Zhao Lv, Jinlin Wu, Zhen Lei, Zhiying Tu, Dianhui Chu, Xiaoyan Yu, Dianbo Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Sequential decision-making refers to algorithms that take into account the dynamics of the environment, where early decisions affect subsequent decisions. With large language models (LLMs) demonstrating powerful capabilities between tasks, we can't help but ask: Can Current LLMs Effectively Make Sequential Decisions? In order to answer this question, we propose the UNO Arena based on the card game UNO to evaluate the sequential decision-making capability of LLMs and explain in detail why we choose UNO. In UNO Arena, We evaluate the sequential decision-making capability of LLMs dynamically with novel metrics based Monte Carlo methods. We set up random players, DQN-based reinforcement learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison testing. Furthermore, in order to improve the sequential decision-making capability of LLMs, we propose the TUTRI player, which can involves having LLMs reflect their own actions wtih the summary of game history and the game strategy. Numerous experiments demonstrate that the TUTRI player achieves a notable breakthrough in the performance of sequential decision-making compared to the vanilla LLM player.</li>
<li><strong>摘要：</strong>序贯决策是指考虑环境动态的算法，其中早期的决策会影响后续的决策。随着大型语言模型 (LLM) 在任务之间展示出强大的能力，我们不禁要问：当前的 LLM 能否有效地做出序贯决策？为了回答这个问题，我们提出了基于纸牌游戏 UNO 的 UNO Arena 来评估 LLM 的序贯决策能力，并详细解释了我们选择 UNO 的原因。在 UNO Arena 中，我们使用基于新指标的蒙特卡洛方法动态评估 LLM 的序贯决策能力。我们设置了随机玩家、基于 DQN 的强化学习玩家和 LLM 玩家（例如 GPT-4、Gemini-pro）进行比较测试。此外，为了提高 LLM 的序贯决策能力，我们提出了 TUTRI 玩家，这可以让 LLM 通过游戏历史摘要和游戏策略来反映自己的动作。大量实验表明，与原始 LLM 玩家相比，TUTRI 玩家在顺序决策性能方面取得了显著突破。</li>
</ul>

<h3>Title: Title:
          Multilingual Knowledge Editing with Language-Agnostic Factual Neurons</h3>
<ul>
<li><strong>Authors: </strong>Xue zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multilingual Knowledge Editing with Language-Agnostic Factual Neurons(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multilingual knowledge editing (MKE) aims to simultaneously revise factual knowledge across multilingual languages within large language models (LLMs). However, most existing MKE methods just adapt existing monolingual editing methods to multilingual scenarios, overlooking the deep semantic connections of the same factual knowledge between different languages, thereby limiting edit performance. To address this issue, we first investigate how LLMs represent multilingual factual knowledge and discover that the same factual knowledge in different languages generally activates a shared set of neurons, which we call language-agnostic factual neurons. These neurons represent the semantic connections between multilingual knowledge and are mainly located in certain layers. Inspired by this finding, we propose a new MKE method by locating and modifying Language-Agnostic Factual Neurons (LAFN) to simultaneously edit multilingual knowledge. Specifically, we first generate a set of paraphrases for each multilingual knowledge to be edited to precisely locate the corresponding language-agnostic factual neurons. Then we optimize the update values for modifying these located neurons to achieve simultaneous modification of the same factual knowledge in multiple languages. Experimental results on Bi-ZsRE and MzsRE benchmarks demonstrate that our method outperforms existing MKE methods and achieves remarkable edit performance, indicating the importance of considering the semantic connections among multilingual knowledge.</li>
<li><strong>摘要：</strong>多语言知识编辑 (MKE) 旨在在大型语言模型 (LLM) 中同时修改跨多语言语言的事实知识。然而，大多数现有的 MKE 方法只是将现有的单语言编辑方法应用于多语言场景，忽略了不同语言之间相同事实知识的深层语义联系，从而限制了编辑性能。为了解决这个问题，我们首先研究 LLM 如何表示多语言事实知识，并发现不同语言中的相同事实知识通常会激活一组共享的神经元，我们称之为语言不可知的事实神经元。这些神经元代表多语言知识之间的语义联系，主要位于某些层。受此发现的启发，我们提出了一种新的 MKE 方法，通过定位和修改语言不可知的事实神经元 (LAFN) 来同时编辑多语言知识。具体而言，我们首先为每个要编辑的多语言知识生成一组释义，以精确定位相应的语言不可知的事实神经元。然后我们优化了修改这些定位神经元的更新值，以实现对多种语言中相同事实知识的同时修改。在 Bi-ZsRE 和 MzsRE 基准上的实验结果表明，我们的方法优于现有的 MKE 方法并实现了显著的编辑性能，表明考虑多语言知识之间的语义联系的重要性。</li>
</ul>

<h3>Title: Title:
          UniCoder: Scaling Code Large Language Model via Universal Code</h3>
<ul>
<li><strong>Authors: </strong>Tao Sun, Linzheng Chai, Jian Yang, Yuwei Yin, Hongcheng Guo, Jiaheng Liu, Bing Wang, Liqun Yang, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          UniCoder: Scaling Code Large Language Model via Universal Code(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.</li>
<li><strong>摘要：</strong>中间推理或动作步骤已成功改进大型语言模型 (LLM)，使其能够处理各种下游自然语言处理 (NLP) 任务。在将 LLM 应用于代码生成时，最近的研究主要集中于指导模型表达中间自然语言推理步骤，如思路链 (CoT) 提示，然后使用自然语言或其他结构化的中间步骤输出代码。然而，这种输出不适合代码翻译或生成任务，因为标准 CoT 与代码具有不同的逻辑结构和表达形式。在这项工作中，我们引入通用代码 (UniCode) 作为中间表示。它是使用多种编程语言约定（例如赋值运算符、条件运算符和循环）来描述算法步骤。因此，我们收集了一个指令数据集 UniCoder-Instruct，以针对多任务学习目标训练我们的模型 UniCoder。UniCoder-Instruct 包括自然语言问题、代码解决方案和相应的通用代码。中间通用代码表示与最终代码解决方案之间的一致性显著提高了生成代码的质量。实验结果表明，带有通用代码的UniCoder的性能明显优于之前的提示方法，体现了伪代码中结构线索的有效性。</li>
</ul>

<h3>Title: Title:
          Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers</h3>
<ul>
<li><strong>Authors: </strong>Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models' parameter count and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFN), which are less studied than attention blocks. We consider three candidate linear layer approximations in the FFN by combining efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from the training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We first demonstrate they can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called \textit{self-guided training}, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Experiments on the large RefinedWeb dataset show that our methods are both efficient and effective for training and inference. Interestingly, these structured FFNs exhibit steeper scaling curves than the original models. Further applying self-guided training to the structured matrices with 32\% FFN parameters and 2.5$\times$ speed-up enables only a 0.4 perplexity increase under the same training FLOPs. Finally, we develop the wide and structured networks surpassing the current medium-sized and large-sized Transformer in perplexity and throughput performance. Our code is available at \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的最新成果通常依赖于规模，而规模会导致计算成本高昂。这引发了一项研究议程，以减少这些模型的参数数量和计算成本，而不会显著影响其性能。我们的研究重点是基于 Transformer 的 LLM，特别是针对计算密集型的前馈网络 (FFN)，这些网络的研究少于注意力模块。我们通过结合高效的低秩和块对角矩阵来考虑 FFN 中的三个候选线性层近似。与许多研究这些近似的先前研究不同，我们的研究 i) 从从头开始训练的角度探索这些结构，ii) 扩展到 13 亿个参数，iii) 在最近的基于 Transformer 的 LLM 而不是卷积架构中进行。我们首先证明它们可以在各种场景中带来实际的计算收益，包括使用预合并技术时的在线解码。此外，我们提出了一种名为 \textit{自引导训练} 的新训练方案，旨在改善这些近似值从初始化开始使用时表现出的较差训练动态。在大型 RefinedWeb 数据集上进行的实验表明，我们的方法对于训练和推理都是高效且有效的。有趣的是，这些结构化的 FFN 表现出比原始模型更陡峭的缩放曲线。进一步将自引导训练应用于具有 32\% FFN 参数和 2.5$\times$ 加速的结构化矩阵，在相同的训练 FLOP 下仅使困惑度增加 0.4。最后，我们开发了宽而结构化的网络，在困惑度和吞吐量性能方面超越了当前的中型和大型 Transformer。我们的代码可在 \url{此 https URL} 获得。</li>
</ul>

<h3>Title: Title:
          Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Yujin Baek, ChaeHun Park, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>To create culturally inclusive vision-language models (VLMs), the foremost requirement is developing a test benchmark that can diagnose the models' ability to respond to questions reflecting cultural elements. This paper addresses the necessity for such benchmarks, noting that existing research has relied on human annotators' manual efforts, which impedes diversity and efficiency. We propose a semi-automated pipeline for constructing cultural VLM benchmarks to enhance diversity and efficiency. This pipeline leverages human-VLM collaboration, where VLMs generate questions based on guidelines, human-annotated examples, and image-wise relevant knowledge, which are then reviewed by native speakers for quality and cultural relevance. The effectiveness of our adaptable pipeline is demonstrated through a specific application: creating a dataset tailored to Korean culture, dubbed K-Viscuit. The resulting benchmark features two types of questions: Type 1 questions measure visual recognition abilities, while Type 2 assess fine-grained visual reasoning skills. This ensures a thorough diagnosis of VLM models across various aspects. Our evaluation using K-Viscuit revealed that open-source models notably lag behind proprietary models in understanding Korean culture, highlighting areas for improvement. We provided diverse analyses of VLM performance across different cultural aspects. Besides, we explored the potential of incorporating external knowledge retrieval to enhance the generation process, suggesting future directions for improving cultural interpretation ability of VLMs. Our dataset and code will be made publicly available.</li>
<li><strong>摘要：</strong>要创建具有文化包容性的视觉语言模型 (VLM)，首要要求是开发一个测试基准，以诊断模型回答反映文化元素的问题的能力。本文讨论了此类基准的必要性，并指出现有研究依赖于人工注释者的手动工作，这阻碍了多样性和效率。我们提出了一种半自动化流程来构建文化 VLM 基准，以提高多样性和效率。该流程利用人机协作，其中 VLM 根据指南、人工注释的示例和图像相关知识生成问题，然后由母语人士审查这些问题的质量和文化相关性。我们适应性流程的有效性通过一个特定的应用得到证明：创建一个针对韩国文化量身定制的数据集，称为 K-Viscuit。由此产生的基准有两种类型的问题：第 1 类问题衡量视觉识别能力，而第 2 类问题评估细粒度的视觉推理能力。这确保了对 VLM 模型的各个方面进行全面诊断。我们使用 K-Viscuit 进行的评估表明，开源模型在理解韩国文化方面明显落后于专有模型，这突出了需要改进的地方。我们对不同文化方面的 VLM 性能进行了多样化分析。此外，我们探索了结合外部知识检索来增强生成过程的潜力，为提高 VLM 的文化解释能力提出了未来方向。我们的数据集和代码将公开发布。</li>
</ul>

<h3>Title: Title:
          EMMI -- Empathic Multimodal Motivational Interviews Dataset: Analyses and Annotations</h3>
<ul>
<li><strong>Authors: </strong>Lucie Galland, Catherine Pelachaud, Florian Pecune</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          EMMI -- Empathic Multimodal Motivational Interviews Dataset: Analyses and Annotations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The study of multimodal interaction in therapy can yield a comprehensive understanding of therapist and patient behavior that can be used to develop a multimodal virtual agent supporting therapy. This investigation aims to uncover how therapists skillfully blend therapy's task goal (employing classical steps of Motivational Interviewing) with the social goal (building a trusting relationship and expressing empathy). Furthermore, we seek to categorize patients into various ``types'' requiring tailored therapeutic approaches. To this intent, we present multimodal annotations of a corpus consisting of simulated motivational interviewing conversations, wherein actors portray the roles of patients and therapists. We introduce EMMI, composed of two publicly available MI corpora, AnnoMI and the Motivational Interviewing Dataset, for which we add multimodal annotations. We analyze these annotations to characterize functional behavior for developing a virtual agent performing motivational interviews emphasizing social and empathic behaviors. Our analysis found three clusters of patients expressing significant differences in behavior and adaptation of the therapist's behavior to those types. This shows the importance of a therapist being able to adapt their behavior depending on the current situation within the dialog and the type of user.</li>
<li><strong>摘要：</strong>研究治疗中的多模态交互可以全面了解治疗师和患者的行为，可用于开发支持治疗的多模态虚拟代理。本研究旨在揭示治疗师如何巧妙地将治疗的任务目标（采用动机访谈的经典步骤）与社会目标（建立信任关系和表达同理心）融合在一起。此外，我们试图将患者分为需要量身定制的治疗方法的各种“类型”。为此，我们展示了由模拟动机访谈对话组成的语料库的多模态注释，其中演员扮演患者和治疗师的角色。我们引入了 EMMI，它由两个公开可用的 MI 语料库 AnnoMI 和动机访谈数据集组成，我们为其添加了多模态注释。我们分析这些注释以表征功能行为，以开发一个虚拟代理，执行强调社交和共情行为的动机访谈。我们的分析发现三组患者表现出行为的显著差异以及治疗师的行为对这些类型的适应性。这表明治疗师能够根据对话中的当前情况和用户类型调整其行为的重要性。</li>
</ul>

<h3>Title: Title:
          eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure</h3>
<ul>
<li><strong>Authors: </strong>Hoorieh Sabzevari, Mohammadmostafa Rostamkhani, Sauleh Eetemadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study investigates the performance of the zero-shot method in classifying data using three large language models, alongside two models with large input token sizes and the two pre-trained models on legal data. Our main dataset comes from the domain of U.S. civil procedure. It includes summaries of legal cases, specific questions, potential answers, and detailed explanations for why each solution is relevant, all sourced from a book aimed at law students. By comparing different methods, we aimed to understand how effectively they handle the complexities found in legal datasets. Our findings show how well the zero-shot method of large language models can understand complicated data. We achieved our highest F1 score of 64% in these experiments.</li>
<li><strong>摘要：</strong>本研究使用三个大型语言模型、两个具有大输入标记大小的模型和两个针对法律数据的预训练模型，研究了零样本方法在数据分类中的表现。我们的主要数据集来自美国民事诉讼领域。它包括法律案件摘要、具体问题、潜在答案以及每个解决方案相关原因的详细解释，所有这些都来自一本针对法学院学生的书。通过比较不同的方法，我们旨在了解它们处理法律数据集中复杂性的有效性。我们的研究结果表明，大型语言模型的零样本方法可以很好地理解复杂数据。我们在这些实验中获得了最高的 F1 分数 64%。</li>
</ul>

<h3>Title: Title:
          OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser</h3>
<ul>
<li><strong>Authors: </strong>Jingze Shi, Ting Xie, Bingheng Wu, Chunjun Zheng, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent research has shown that combining Mamba with Transformer architecture, which has selective state space and quadratic self-attention mechanism, outperforms using Mamba or Transformer architecture alone in language modeling tasks. The quadratic self-attention mechanism effectively alleviates the shortcomings of selective state space in handling long-term dependencies of any element in the sequence. We propose a position information injection method that connects the selective state space model with the quadratic attention, and integrates these two architectures with hybrid experts with cross-sharing domains, so that we can enjoy the advantages of both. We design a new architecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser (OTCE), which can compete with well-known medium-scale open-source language models on a small scale in language modeling tasks.</li>
<li><strong>摘要：</strong>近期研究表明，将 Mamba 与具有选择性状态空间和二次自注意力机制的 Transformer 架构相结合，在语言建模任务中的表现优于单独使用 Mamba 或 Transformer 架构。二次自注意力机制有效地缓解了选择性状态空间在处理序列中任意元素的长期依赖关系方面的不足。我们提出了一种位置信息注入方法，将选择性状态空间模型与二次注意力连接起来，并将这两种架构与跨共享领域的混合专家相结合，让我们能够同时享受两者的优势。我们设计了一个更具仿生思想的新架构：观察者-思考者-构思者-表达者（OTCE），它可以在语言建模任务上在小规模上与知名的中型开源语言模型相媲美。</li>
</ul>

<h3>Title: Title:
          Large Vocabulary Size Improves Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sho Takase, Ryokan Ri, Shun Kiyono, Takuya Kato</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Vocabulary Size Improves Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.</li>
<li><strong>摘要：</strong>本文实证研究了子词词汇量与大型语言模型 (LLM) 性能之间的关系，以提供有关如何定义词汇量的信息。实验结果表明，词汇量越大，LLM 的性能越好。此外，我们考虑了一种持续训练场景，其中预训练的语言模型在不同的目标语言上进行训练。我们介绍了一种使用新词汇代替预定义词汇的简单方法。我们表明，使用新词汇的表现优于使用预训练词汇的模型。</li>
</ul>

<h3>Title: Title:
          Evaluating the Ability of Large Language Models to Reason about Cardinal Directions</h3>
<ul>
<li><strong>Authors: </strong>Anthony G Cohn, Robert E Blackwell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating the Ability of Large Language Models to Reason about Cardinal Directions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>We investigate the abilities of a representative set of Large language Models (LLMs) to reason about cardinal directions (CDs). To do so, we create two datasets: the first, co-created with ChatGPT, focuses largely on recall of world knowledge about CDs; the second is generated from a set of templates, comprehensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first , second or third person. Even with a temperature setting of zero, Our experiments show that although LLMs are able to perform well in the simpler dataset, in the second more complex dataset no LLM is able to reliably determine the correct CD, even with a temperature setting of zero.</li>
<li><strong>摘要：</strong>我们研究了一组具有代表性的大型语言模型 (LLM) 推理基本方向 (CD) 的能力。为此，我们创建了两个数据集：第一个数据集与 ChatGPT 共同创建，主要侧重于回忆有关 CD 的世界知识；第二个数据集由一组模板生成，全面测试 LLM 在特定场景下确定正确 CD 的能力。模板允许多种程度的变化，例如所涉及代理的运动方式，以及是否设置为第一人称、第二人称或第三人称。即使温度设置为零，我们的实验表明，尽管 LLM 能够在较简单的数据集中表现良好，但在第二个更复杂的数据集中，即使温度设置为零，也没有 LLM 能够可靠地确定正确的 CD。</li>
</ul>

<h3>Title: Title:
          Towards Better Graph-based Cross-document Relation Extraction via Non-bridge Entity Enhancement and Prediction Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Hao Yue, Shaopeng Lai, Chengyi Yang, Liang Zhang, Junfeng Yao, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Better Graph-based Cross-document Relation Extraction via Non-bridge Entity Enhancement and Prediction Debiasing(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Cross-document Relation Extraction aims to predict the relation between target entities located in different documents. In this regard, the dominant models commonly retain useful information for relation prediction via bridge entities, which allows the model to elaborately capture the intrinsic interdependence between target entities. However, these studies ignore the non-bridge entities, each of which co-occurs with only one target entity and offers the semantic association between target entities for relation prediction. Besides, the commonly-used dataset--CodRED contains substantial NA instances, leading to the prediction bias during inference. To address these issues, in this paper, we propose a novel graph-based cross-document RE model with non-bridge entity enhancement and prediction debiasing. Specifically, we use a unified entity graph to integrate numerous non-bridge entities with target entities and bridge entities, modeling various associations between them, and then use a graph recurrent network to encode this graph. Finally, we introduce a novel debiasing strategy to calibrate the original prediction distribution. Experimental results on the closed and open settings show that our model significantly outperforms all baselines, including the GPT-3.5-turbo and InstructUIE, achieving state-of-the-art performance. Particularly, our model obtains 66.23% and 55.87% AUC points in the official leaderboard\footnote{\url{this https URL}} under the two settings, respectively, ranking the first place in all submissions since December 2023. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>跨文档关系抽取旨在预测位于不同文档中的目标实体之间的关系。在这方面，主流模型通常通过桥接实体保留对关系预测有用的信息，这使模型能够精细地捕捉目标实体之间的内在相互依赖性。然而，这些研究忽略了非桥接实体，每个非桥接实体只与一个目标实体同时出现，并为关系预测提供目标实体之间的语义关联。此外，常用的数据集 CodRED 包含大量 NA 实例，导致推理过程中出现预测偏差。为了解决这些问题，本文提出了一种基于图的新型跨文档 RE 模型，该模型具有非桥接实体增强和预测去偏功能。具体而言，我们使用统一的实体图将众多非桥接实体与目标实体和桥接实体集成在一起，对它们之间的各种关联进行建模，然后使用图递归网络对该图进行编码。最后，我们引入了一种新颖的去偏策略来校准原始预测分布。在封闭和开放设置下的实验结果表明，我们的模型显著优于所有基线，包括 GPT-3.5-turbo 和 InstructUIE，达到了最先进的性能。特别是，我们的模型在两种设置下分别在官方排行榜\footnote{\url{此 https URL}} 中获得了 66.23% 和 55.87% 的 AUC 点数，在 2023 年 12 月以来的所有提交中排名第一。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          C-LLM: Learn to Check Chinese Spelling Errors Character by Character</h3>
<ul>
<li><strong>Authors: </strong>Kunting Li, Yong Hu, Liang He, Fandong Meng, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          C-LLM: Learn to Check Chinese Spelling Errors Character by Character(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Chinese Spell Checking (CSC) aims to detect and correct spelling errors in sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and are widely applied in various tasks, their performance on CSC is often unsatisfactory. We find that LLMs fail to meet the Chinese character-level constraints of the CSC task, namely equal length and phonetic similarity, leading to a performance bottleneck. Further analysis reveal that this issue stems from the granularity of tokenization, as current mixed character-word tokenization struggles to satisfy these character-level constraints. To address this issue, we propose C-LLM, a Large Language Model-based Chinese Spell Checking method that learns to check errors Character by Character. Character-level tokenization enables the model to learn character-level alignment, effectively mitigating issues related to character-level constraints. Furthermore, CSC is simplified to replication-dominated and substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate that C-LLM achieves an average improvement of 10% over existing methods. Specifically, it shows a 2.1% improvement in general scenarios and a significant 12% improvement in vertical domain scenarios, establishing state-of-the-art performance. The source code can be accessed at this https URL.</li>
<li><strong>摘要：</strong>中文拼写检查 (CSC) 旨在检测和纠正句子中的拼写错误。尽管大型语言模型 (LLM) 表现出强大的功能并广泛应用于各种任务，但它们在 CSC 上的表现往往不尽人意。我们发现 LLM 未能满足 CSC 任务的中文字符级约束，即等长和语音相似性，从而导致性能瓶颈。进一步分析表明，这个问题源于标记化的粒度，因为当前的混合字符单词标记化难以满足这些字符级约束。为了解决这个问题，我们提出了 C-LLM，一种基于大型语言模型的中文拼写检查方法，它可以学习逐个字符检查错误。字符级标记化使模型能够学习字符级对齐，从而有效缓解与字符级约束相关的问题。此外，CSC 被简化为以复制为主和以替换为辅的任务。在两个 CSC 基准上的实验表明，C-LLM 比现有方法平均提高了 10%。具体来说，它在一般场景中提升了 2.1%，在垂直领域场景中提升了 12%，达到了最佳性能。源代码可以通过此 https URL 访问。</li>
</ul>

<h3>Title: Title:
          LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated by this limit, we investigate building MoE models from existing dense large language models. Specifically, based on the well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert Construction, which partitions the parameters of original Feed-Forward Networks (FFNs) into multiple experts; (2) Continual Pre-training, which further trains the transformed MoE model and additional gate networks. In this paper, we comprehensively explore different methods for expert construction and various data sampling strategies for continual pre-training. After these stages, our LLaMA-MoE models could maintain language abilities and route the input tokens to specific experts with part of the parameters activated. Empirically, by training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense models that contain similar activation parameters. The source codes and models are available at this https URL .</li>
<li><strong>摘要：</strong>混合专家 (MoE) 作为一种有前途的扩展大型语言模型 (LLM) 的框架，越来越受欢迎。然而，在大规模环境中从头开始训练 MoE 仍然存在数据匮乏和不稳定的问题。受此限制的启发，我们研究从现有的密集大型语言模型构建 MoE 模型。具体来说，基于著名的 LLaMA-2 7B 模型，我们通过以下方式获得 MoE 模型：(1) 专家构建，将原始前馈网络 (FFN) 的参数划分为多个专家；(2) 持续预训练，进一步训练转换后的 MoE 模型和附加门网络。在本文中，我们全面探索了专家构建的不同方法和持续预训练的各种数据采样策略。经过这些阶段，我们的 LLaMA-MoE 模型可以保持语言能力并将输入标记路由到特定专家，同时激活部分参数。从经验上看，通过训练 2000 亿个 token，LLaMA-MoE-3.5B 模型的表现明显优于包含类似激活参数的密集模型。源代码和模型可从此 https URL 获得。</li>
</ul>

<h3>Title: Title:
          Data Augmentation of Multi-turn Psychological Dialogue via Knowledge-driven Progressive Thought Prompting</h3>
<ul>
<li><strong>Authors: </strong>Jiyue Jiang, Liheng Chen, Sheng Wang, Lingpeng Kong, Yu Li, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Data Augmentation of Multi-turn Psychological Dialogue via Knowledge-driven Progressive Thought Prompting(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Existing dialogue data augmentation (DA) techniques predominantly focus on augmenting utterance-level dialogues, which makes it difficult to take dialogue contextual information into account. The advent of large language models (LLMs) has simplified the implementation of multi-turn dialogues. Due to absence of professional understanding and knowledge, it remains challenging to deliver satisfactory performance in low-resource domain, like psychological dialogue dialogue. DA involves creating new training or prompting data based on the existing data, which help the model better understand and generate psychology-related responses. In this paper, we aim to address the issue of multi-turn dialogue data augmentation for boosted performance in the psychology domain. We propose a knowledge-driven progressive thought prompting method to guide LLM to generate multi-turn psychology-related dialogue. This method integrates a progressive thought generator, a psychology knowledge generator, and a multi-turn dialogue generator. The thought generated by the progressive thought generator serves as a prompt to prevent the generated dialogue from having significant semantic deviations, while the psychology knowledge generator produces psychological knowledge to serve as the dialogue history for the LLM, guiding the dialogue generator to create multi-turn psychological dialogue. To ensure the precision of multi-turn psychological dialogue generation by LLM, a meticulous professional evaluation is required. Extensive experiments conducted on three datasets related to psychological dialogue verify the effectiveness of the proposed method.</li>
<li><strong>摘要：</strong>现有的对话数据增强 (DA) 技术主要侧重于增强话语级对话，这使得很难考虑对话上下文信息。大型语言模型 (LLM) 的出现简化了多轮对话的实现。由于缺乏专业理解和知识，在心理对话等低资源领域提供令人满意的表现仍然具有挑战性。DA 涉及基于现有数据创建新的训练或提示数据，这有助于模型更好地理解和生成与心理学相关的反应。在本文中，我们旨在解决多轮对话数据增强的问题，以提高心理学领域的性能。我们提出了一种知识驱动的渐进式思维提示方法来指导 LLM 生成多轮与心理学相关的对话。该方法集成了渐进式思维生成器、心理学知识生成器和多轮对话生成器。渐进式思维生成器产生的思维作为提示，避免生成的对话出现重大的语义偏差，而心理知识生成器产生的心理知识作为LLM的对话历史，指导对话生成器创建多轮心理对话。为了确保LLM多轮心理对话生成的准确性，需要进行细致的专业评估。在三个与心理对话相关的数据集上进行的大量实验验证了所提方法的有效性。</li>
</ul>

<h3>Title: Title:
          CLEAR: Can Language Models Really Understand Causal Graphs?</h3>
<ul>
<li><strong>Authors: </strong>Sirui Chen, Mengying Xu, Kun Wang, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Chaochao Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CLEAR: Can Language Models Really Understand Causal Graphs?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Causal reasoning is a cornerstone of how humans interpret the world. To model and reason about causality, causal graphs offer a concise yet effective solution. Given the impressive advancements in language models, a crucial question arises: can they really understand causal graphs? To this end, we pioneer an investigation into language models' understanding of causal graphs. Specifically, we develop a framework to define causal graph understanding, by assessing language models' behaviors through four practical criteria derived from diverse disciplines (e.g., philosophy and psychology). We then develop CLEAR, a novel benchmark that defines three complexity levels and encompasses 20 causal graph-based tasks across these levels. Finally, based on our framework and benchmark, we conduct extensive experiments on six leading language models and summarize five empirical findings. Our results indicate that while language models demonstrate a preliminary understanding of causal graphs, significant potential for improvement remains. Our project website is at this https URL.</li>
<li><strong>摘要：</strong>因果推理是人类理解世界的基石。为了对因果关系进行建模和推理，因果图提供了一种简洁而有效的解决方案。鉴于语言模型的令人瞩目的进步，一个关键问题出现了：它们真的能理解因果图吗？为此，我们率先研究了语言模型对因果图的理解。具体来说，我们开发了一个框架来定义因果图理解，通过来自不同学科（例如哲学和心理学）的四个实际标准来评估语言模型的行为。然后，我们开发了 CLEAR，这是一个新颖的基准，它定义了三个复杂度级别，并涵盖了这些级别的 20 个基于因果图的任务。最后，基于我们的框架和基准，我们对六种领先的语言模型进行了广泛的实验，并总结了五个实证结果。我们的结果表明，虽然语言模型展示了对因果图的初步理解，但仍有很大的改进潜力。我们的项目网站是这个 https URL。</li>
</ul>

<h3>Title: Title:
          Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings</h3>
<ul>
<li><strong>Authors: </strong>Andrea Posada, Daniel Rueckert, Felix Meissen, Philip Müller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Since the emergence of the Transformer architecture, language model development has increased, driven by their promising potential. However, releasing these models into production requires properly understanding their behavior, particularly in sensitive domains such as medicine. Despite this need, the medical literature still lacks technical assessments of pre-trained language models, which are especially valuable in resource-constrained settings in terms of computational power or limited budget. To address this gap, we provide a comprehensive survey of language models in the medical domain. In addition, we selected a subset of these models for thorough evaluation, focusing on classification and text generation tasks. Our subset encompasses 53 models, ranging from 110 million to 13 billion parameters, spanning the three families of Transformer-based models and from diverse knowledge domains. This study employs a series of approaches for text classification together with zero-shot prompting instead of model training or fine-tuning, which closely resembles the limited resource setting in which many users of language models find themselves. Encouragingly, our findings reveal remarkable performance across various tasks and datasets, underscoring the latent potential of certain models to contain medical knowledge, even without domain specialization. Consequently, our study advocates for further exploration of model applications in medical contexts, particularly in resource-constrained settings. The code is available on this https URL.</li>
<li><strong>摘要：</strong>自 Transformer 架构出现以来，语言模型的开发因其巨大的潜力而不断增加。然而，将这些模型投入生产需要正确理解它们的行为，特别是在医学等敏感领域。尽管有这种需要，但医学文献仍然缺乏对预训练语言模型的技术评估，这些评估在计算能力或预算有限的资源受限的环境中尤其有价值。为了弥补这一差距，我们对医学领域的语言模型进行了全面调查。此外，我们选择了这些模型的一个子集进行全面评估，重点是分类和文本生成任务。我们的子集包含 53 个模型，参数范围从 1.1 亿到 130 亿，涵盖了基于 Transformer 的三个模型系列和不同的知识领域。本研究采用了一系列文本分类方法，并使用零样本提示代替模型训练或微调，这与许多语言模型用户所处的资源有限环境非常相似。令人鼓舞的是，我们的研究结果显示，该模型在各种任务和数据集上都表现出色，凸显了某些模型即使没有领域专业化，也具有包含医学知识的潜在潜力。因此，我们的研究提倡进一步探索模型在医学环境中的应用，特别是在资源受限的环境中。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          Large Language Models Are Cross-Lingual Knowledge-Free Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models Are Cross-Lingual Knowledge-Free Reasoners(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated impressive reasoning capabilities across multiple languages. However, the relationship between capabilities in different languages is less explored. In this work, we decompose the process of reasoning tasks into two separated parts: knowledge retrieval and knowledge-free reasoning, and analyze the cross-lingual transferability of them. With adapted and constructed knowledge-free reasoning datasets, we show that the knowledge-free reasoning capability can be nearly perfectly transferred across various source-target language directions despite the secondary impact of resource in some specific target languages, while cross-lingual knowledge retrieval significantly hinders the transfer. Moreover, by analyzing the hidden states and feed-forward network neuron activation during the reasoning tasks, we show that higher similarity of hidden representations and larger overlap of activated neurons could explain the better cross-lingual transferability of knowledge-free reasoning than knowledge retrieval. Thus, we hypothesize that knowledge-free reasoning embeds in some language-shared mechanism, while knowledge is stored separately in different languages.</li>
<li><strong>摘要：</strong>大型语言模型已经展示了跨多种语言的令人瞩目的推理能力。然而，不同语言之间能力之间的关系研究得较少。在本文中，我们将推理任务的过程分解为两个独立的部分：知识检索和无知识推理，并分析它们的跨语言可迁移性。通过调整和构建的无知识推理数据集，我们发现尽管在某些特定目标语言中资源会受到二次影响，但无知识推理能力可以在各种源-目标语言方向之间几乎完美地迁移，而跨语言知识检索会显著阻碍这种迁移。此外，通过分析推理任务期间的隐藏状态和前馈网络神经元激活，我们发现隐藏表示的相似度越高和激活神经元的重叠度越大可以解释无知识推理比知识检索具有更好的跨语言可迁移性。因此，我们假设无知识推理嵌入某种语言共享机制中，而知识则分别存储在不同的语言中。</li>
</ul>

<h3>Title: Title:
          CAVE: Controllable Authorship Verification Explanations</h3>
<ul>
<li><strong>Authors: </strong>Sahana Ramnath, Kartik Pandey, Elizabeth Boschee, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CAVE: Controllable Authorship Verification Explanations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>Authorship Verification (AV) (do two documents have the same author?) is essential for many sensitive real-life applications. AV is often used in proprietary domains that require a private, offline model, making SOTA online models like ChatGPT undesirable. Other SOTA systems use methods, e.g. Siamese Networks, that are uninterpretable, and hence cannot be trusted in high-stakes applications. In this work, we take the first step to address the above challenges with our model CAVE (Controllable Authorship Verification Explanations): CAVE generates free-text AV explanations that are controlled to be 1) structured (can be decomposed into sub-explanations with respect to relevant linguistic features), and 2) easily verified for explanation-label consistency (via intermediate labels in sub-explanations). In this work, we train a Llama-3-8B as CAVE; since there are no human-written corpora for AV explanations, we sample silver-standard explanations from GPT-4-TURBO and distill them into a pretrained Llama-3-8B. Results on three difficult AV datasets IMdB2, Blog-Auth, and FanFiction show that CAVE generates high quality explanations (as measured by automatic and human evaluation) as well as competitive task accuracies.</li>
<li><strong>摘要：</strong>作者验证 (AV)（两篇文档是否属于同一作者？）对于许多敏感的实际应用至关重要。AV 通常用于需要私有离线模型的专有领域，这使得 ChatGPT 等 SOTA 在线模型不受欢迎。其他 SOTA 系统使用无法解释的方法（例如暹罗网络），因此在高风险应用中不可信任。在这项工作中，我们迈出了使用我们的模型 CAVE（可控作者验证解释）解决上述挑战的第一步：CAVE 生成自由文本 AV 解释，这些解释可控制为 1）结构化（可以根据相关语言特征分解为子解释），2) 易于验证解释标签一致性（通过子解释中的中间标签）。在这项工作中，我们将 Llama-3-8B 训练为 CAVE；由于没有人工编写的 AV 解释语料库，我们从 GPT-4-TURBO 中抽取了银标准解释，并将其提炼到预训练的 Llama-3-8B 中。在三个困难的 AV 数据集 IMdB2、Blog-Auth 和 FanFiction 上的结果表明，CAVE 可以生成高质量的解释（通过自动和人工评估来衡量）以及具有竞争力的任务准确率。</li>
</ul>

<h3>Title: Title:
          Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Markus Frohmann, Igor Sterner, Ivan Vulić, Benjamin Minixhofer, Markus Schedl</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are available at this https URL under the MIT license.</li>
<li><strong>摘要：</strong>将文本分割成句子在许多 NLP 系统中起着早期和关键的作用。这通常是通过使用基于规则或统计的方法来实现的，这些方法依赖于标点符号等词汇特征。尽管最近的一些作品不再完全依赖标点符号，但我们发现之前的方法都无法同时实现 (i) 对缺失标点符号的鲁棒性、(ii) 对新领域的有效适应性和 (iii) 高效率。我们引入了一个新模型 - 分割任何文本 (SaT) - 来解决这个问题。为了增强鲁棒性，我们提出了一种新的预训练方案，以确保减少对标点符号的依赖。为了解决适应性问题，我们引入了一个额外的参数高效微调阶段，在歌词和法律文件等不同领域建立了最先进的性能。在此过程中，我们引入了架构修改，使速度比以前的最先进技术提高了三倍，并解决了对未来上下文的虚假依赖。最后，我们介绍了一种模型变体，该变体针对多种多语言的句子分段数据进行了微调，可作为现有分段工具的替代品和增强工具。总体而言，我们的贡献为任何文本的分段提供了一种通用方法。我们的方法在 8 个语料库中的表现优于所有基线（包括强大的 LLM），涵盖了不同的领域和语言，尤其是在文本格式不佳的实际相关情况下。我们的模型和代码（包括文档）可在 MIT 许可下的此 https URL 上获得。</li>
</ul>

<h3>Title: Title:
          Scaling Laws for Linear Complexity Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, Yiran Zhong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Scaling Laws for Linear Complexity Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for softmax attention for comparison. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.</li>
<li><strong>摘要：</strong>尽管线性复杂度模型的扩展能力仍不确定，但人们对大型语言模型的兴趣正在增加。在本研究中，我们介绍了线性复杂度语言模型的扩展规律，为其可扩展性奠定了基础。具体来说，我们研究了三种高效线性架构的扩展行为。这些包括 TNL，一种具有数据独立衰减的线性注意力模型；HGRN2，一种具有数据相关衰减的线性 RNN；以及 cosFormer2，一种没有衰减的线性注意力模型。我们还将 LLaMA 作为 softmax 注意力的基线架构进行比较。这些模型在 300B 标记语料库上使用六种变体进行了训练，参数范围从 70M 到 7B，并在各种下游任务中使用总共 1,376 个中间检查点进行了评估。这些任务包括验证损失、常识推理以及信息检索和生成。研究表明，现有的线性复杂度语言模型表现出与传统基于 Transformer 的模型类似的扩展能力，同时还表现出卓越的语言能力和知识保留能力。</li>
</ul>

<h3>Title: Title:
          Task Oriented In-Domain Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liang, Xinyu Hu, Simiao Zuo, Yeyun Gong, Qiang Lou, Yi Liu, Shao-Lun Huang, Jian Jiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Task Oriented In-Domain Data Augmentation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown superior performance in various applications and fields. To achieve better performance on specialized domains such as law and advertisement, LLMs are often continue pre-trained on in-domain data. However, existing approaches suffer from two major issues. First, in-domain data are scarce compared with general domain-agnostic data. Second, data used for continual pre-training are not task-aware, such that they may not be helpful to downstream applications. We propose TRAIT, a task-oriented in-domain data augmentation framework. Our framework is divided into two parts: in-domain data selection and task-oriented synthetic passage generation. The data selection strategy identifies and selects a large amount of in-domain data from general corpora, and thus significantly enriches domain knowledge in the continual pre-training data. The synthetic passages contain guidance on how to use domain knowledge to answer questions about downstream tasks. By training on such passages, the model aligns with the need of downstream applications. We adapt LLMs to two domains: advertisement and math. On average, TRAIT improves LLM performance by 8% in the advertisement domain and 7.5% in the math domain.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种应用和领域都表现出色。为了在法律和广告等专业领域取得更好的表现，LLM 通常会在领域内数据上进行持续预训练。然而，现有方法存在两个主要问题。首先，与一般的领域无关数据相比，领域内数据稀缺。其次，用于持续预训练的数据不具有任务感知能力，因此可能对下游应用没有帮助。我们提出了 TRAIT，一个面向任务的领域内数据增强框架。我们的框架分为两部分：领域内数据选择和面向任务的合成段落生成。数据选择策略从一般语料库中识别和选择大量领域内数据，从而显著丰富持续预训练数据中的领域知识。合成段落包含如何使用领域知识回答下游任务问题的指导。通过对这些段落进行训练，该模型与下游应用的需求保持一致。我们将 LLM 应用于两个领域：广告和数学。平均而言，TRAIT 在广告领域将 LLM 的表现提高了 8%，在数学领域将 LLM 的表现提高了 7.5%。</li>
</ul>

<h3>Title: Title:
          AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction-following or coding tasks. As these unexpected errors could lead to severe consequences in practical deployments, it is crucial to investigate the limitations within LLMs systematically. Traditional benchmarking approaches cannot thoroughly pinpoint specific model deficiencies, while manual inspections are costly and not scalable. In this paper, we introduce a unified framework, AutoDetect, to automatically expose weaknesses in LLMs across various tasks. Inspired by the educational assessment process that measures students' learning outcomes, AutoDetect consists of three LLM-powered agents: Examiner, Questioner, and Assessor. The collaboration among these three agents is designed to realize comprehensive and in-depth weakness identification. Our framework demonstrates significant success in uncovering flaws, with an identification success rate exceeding 30% in prominent models such as ChatGPT and Claude. More importantly, these identified weaknesses can guide specific model improvements, proving more effective than untargeted data augmentation methods like Self-Instruct. Our approach has led to substantial enhancements in popular LLMs, including the Llama series and Mistral-7b, boosting their performance by over 10% across several benchmarks. Code and data are publicly available at this https URL.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 的功能越来越强大，但它们仍然表现出显着但微妙的弱点，例如在遵循指令或编码任务中的错误。由于这些意外错误可能会在实际部署中导致严重后果，因此系统地调查 LLM 中的局限性至关重要。传统的基准测试方法无法彻底查明特定的模型缺陷，而人工检查成本高昂且不可扩展。在本文中，我们引入了一个统一的框架 AutoDetect，以自动发现各种任务中 LLM 的弱点。AutoDetect 受到衡量学生学习成果的教育评估过程的启发，由三个由 LLM 驱动的代理组成：考官、提问者和评估者。这三个代理之间的协作旨在实现全面深入的弱点识别。我们的框架在发现缺陷方面取得了显著的成功，在 ChatGPT 和 Claude 等知名模型中的识别成功率超过 30%。更重要的是，这些识别出的弱点可以指导特定的模型改进，证明比 Self-Instruct 等无针对性的数据增强方法更有效。我们的方法已显著增强了热门 LLM（包括 Llama 系列和 Mistral-7b）的性能，使其在多个基准测试中的性能提高了 10% 以上。代码和数据可在此 https URL 上公开获取。</li>
</ul>

<h3>Title: Title:
          Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) can effectively reduce harmful responses from LLMs, they often require high-quality datasets and heavy computational overhead during model training. Another way to align language models is to modify the logit of tokens in model outputs without heavy training. Recent studies have shown that contrastive decoding can enhance the performance of language models by reducing the likelihood of confused tokens. However, these methods require the manual selection of contrastive models or instruction templates. To this end, we propose Adversarial Contrastive Decoding (ACD), an optimization-based framework to generate two opposite system prompts for prompt-based contrastive decoding. ACD only needs to apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min for each model) without training the target model. Experiments conducted on extensive models and benchmarks demonstrate that the proposed method achieves much better safety performance than previous model training-free decoding methods without sacrificing its original generation ability.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的广泛应用，确保其安全性并防止有害响应已成为一个重要问题。虽然当前基于指令微调和人类反馈强化学习 (RLHF) 的安全对齐方法可以有效减少 LLM 的有害响应，但它们通常需要高质量的数据集和模型训练期间的大量计算开销。对齐语言模型的另一种方法是修改模型输出中标记的逻辑，而无需大量训练。最近的研究表明，对比解码可以通过降低混淆标记的可能性来提高语言模型的性能。但是，这些方法需要手动选择对比模型或指令模板。为此，我们提出了对抗对比解码 (ACD)，这是一个基于优化的框架，用于为基于提示的对比解码生成两个相反的系统提示。ACD 只需要在相当小的锚数据集上应用轻量级提示调整（每个模型 < 3 分钟），而无需训练目标模型。在大量模型和基准上进行的实验表明，所提出的方法在不牺牲其原有的生成能力的情况下，实现了比以前的无模型训练解码方法更好的安全性能。</li>
</ul>

<h3>Title: Title:
          Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers</h3>
<ul>
<li><strong>Authors: </strong>Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.</li>
<li><strong>摘要：</strong>由于自注意力机制固有的二次计算复杂度和大量 KV 内存要求，在自回归 Transformer 中有效地容纳长序列（尤其是在扩展上下文窗口内）带来了重大挑战。在这项工作中，我们引入了 SPARSEK Attention，这是一种新颖的稀疏注意力机制，旨在克服这些计算和内存障碍，同时保持性能。我们的方法集成了一个评分网络和一个可微分的 top-k 掩码运算符 SPARSEK，为每个查询选择恒定数量的 KV 对，从而实现基于梯度的优化。因此，SPARSEK Attention 在生成过程中提供线性时间复杂度和恒定的内存占用。实验结果表明，SPARSEK Attention 优于以前的稀疏注意力方法，并且在训练和推理过程中都提供了显着的速度提升，特别是在语言建模和下游任务中。此外，我们的方法可以无缝集成到预先训练的大型语言模型 (LLM) 中，只需进行最少的微调，为有效管理不同应用程序中的远程依赖关系提供了实用的解决方案。</li>
</ul>

<h3>Title: Title:
          Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters</h3>
<ul>
<li><strong>Authors: </strong>Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing and broadened their applicability across diverse commercial applications. However, the deployment of these models is constrained by high inference time in multilingual settings. To mitigate this challenge, this paper explores a training recipe of an assistant model in speculative decoding, which are leveraged to draft and-then its future tokens are verified by the target LLM. We show that language-specific draft models, optimized through a targeted pretrain-and-finetune strategy, substantially brings a speedup of inference time compared to the previous methods. We validate these models across various languages in inference time, out-of-domain speedup, and GPT-4o evaluation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了自然语言处理，并扩大了其在各种商业应用中的适用性。然而，这些模型的部署受到多语言环境中高推理时间的限制。为了应对这一挑战，本文探讨了推测解码中辅助模型的训练方法，该模型用于起草，然后由目标 LLM 验证其未来的标记。我们表明，与以前的方法相比，通过有针对性的预训练和微调策略优化的语言特定草稿模型大大加快了推理时间。我们在推理时间、域外加速和 GPT-4o 评估方面跨各种语言验证了这些模型。</li>
</ul>

<h3>Title: Title:
          The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories</h3>
<ul>
<li><strong>Authors: </strong>Xi Yu Huang, Krishnapriya Vishnubhotla, Frank Rudzicz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>The improved generative capabilities of large language models have made them a powerful tool for creative writing and storytelling. It is therefore important to quantitatively understand the nature of generated stories, and how they differ from human storytelling. We augment the Reddit WritingPrompts dataset with short stories generated by GPT-3.5, given the same prompts. We quantify and compare the emotional and descriptive features of storytelling from both generative processes, human and machine, along a set of six dimensions. We find that generated stories differ significantly from human stories along all six dimensions, and that human and machine generations display similar biases when grouped according to the narrative point-of-view and gender of the main protagonist. We release our dataset and code at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型的生成能力不断增强，使其成为创意写作和讲故事的有力工具。因此，定量地了解生成故事的性质以及它们与人类讲故事的区别非常重要。我们用 GPT-3.5 在给定相同提示的情况下生成的短篇故事扩充了 Reddit WritingPrompts 数据集。我们量化并比较了人类和机器生成过程在六个维度上讲故事的情感和描述特征。我们发现，生成的故事在所有六个维度上都与人类故事存在显著差异，并且当根据叙事视角和主角的性别分组时，人类和机器生成的故事表现出相似的偏见。我们在此 https URL 上发布了我们的数据集和代码。</li>
</ul>

<h3>Title: Title:
          OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?</h3>
<ul>
<li><strong>Authors: </strong>Zhen Huang, Zengzhi Wang, Shijie Xia, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>In this report, we pose the following question: Who is the most intelligent AI model to date, as measured by the OlympicArena (an Olympic-level, multi-discipline, multi-modal benchmark for superintelligent AI)? We specifically focus on the most recently released models: Claude-3.5-Sonnet, Gemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic medal Table approach to rank AI models based on their comprehensive performance across various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet shows highly competitive overall performance over GPT-4o, even surpassing GPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2) Gemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and Claude-3.5-Sonnet, but with a clear performance gap between them. (3) The performance of AI models from the open-source community significantly lags behind these proprietary models. (4) The performance of these models on this benchmark has been less than satisfactory, indicating that we still have a long way to go before achieving superintelligence. We remain committed to continuously tracking and evaluating the performance of the latest powerful models on this benchmark (available at this https URL).</li>
<li><strong>摘要：</strong>在本报告中，我们提出以下问题：根据 OlympicArena（奥运级别、多学科、多模态的超级智能 AI 基准）的衡量，谁是迄今为止最智能的 AI 模型？我们特别关注了最新发布的模型：Claude-3.5-Sonnet、Gemini-1.5-Pro 和 GPT-4o。我们首次提出使用奥运奖牌榜方法对 AI 模型进行排名，排名依据是它们在各个学科的综合表现。实证结果表明：（1）Claude-3.5-Sonnet 的整体性能远超 GPT-4o，甚至在物理、化学和生物等少数学科上超越了 GPT-4o。（2）Gemini-1.5-Pro 和 GPT-4V 的排名紧随 GPT-4o 和 Claude-3.5-Sonnet 之后，但它们之间的性能差距明显。（3）来自开源社区的 AI 模型的性能明显落后于这些专有模型。 （4）这些模型在此基准上的表现并不令人满意，这表明我们在实现超级智能之前还有很长的路要走。我们将继续致力于持续跟踪和评估最新的强大模型在此基准上的表现（可在此 https URL 上查看）。</li>
</ul>

<h3>Title: Title:
          Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024</h3>
<ul>
<li><strong>Authors: </strong>Sai Koneru, Thai-Binh Nguyen, Ngoc-Quan Pham, Danni Liu, Zhaolin Li, Alexander Waibel, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic Speech Recognition (ASR), Machine Translation (MT), and even End-to-End Speech Translation (ST). In this paper, we present KIT's offline submission in the constrained + LLM track by incorporating recently proposed techniques that can be added to any cascaded speech translation. Specifically, we integrate Mistral-7B\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to enhance it in two ways. Firstly, we refine the ASR outputs by utilizing the N-best lists generated by our system and fine-tuning the LLM to predict the transcript accurately. Secondly, we refine the MT outputs at the document level by fine-tuning the LLM, leveraging both ASR and MT predictions to improve translation quality. We find that integrating the LLM into the ASR and MT systems results in an absolute improvement of $0.3\%$ in Word Error Rate and $0.65\%$ in COMET for tst2019 test set. In challenging test sets with overlapping speakers and background noise, we find that integrating LLM is not beneficial due to poor ASR performance. Here, we use ASR with chunked long-form decoding to improve context usage that may be unavailable when transcribing with Voice Activity Detection segmentation alone.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 目前正被应用于各种任务，包括自动语音识别 (ASR)、机器翻译 (MT)，甚至端到端语音翻译 (ST)。在本文中，我们通过结合最近提出的可添加到任何级联语音翻译中的技术，展示了 KIT 在受限 + LLM 轨道中的离线提交。具体来说，我们将 Mistral-7B\footnote{mistralai/Mistral-7B-Instruct-v0.1} 集成到我们的系统中，以两种方式增强它。首先，我们利用系统生成的 N 个最佳列表来优化 ASR 输出，并微调 LLM 以准确预测成绩单。其次，我们通过微调 LLM 在文档级别优化 MT 输出，利用 ASR 和 MT 预测来提高翻译质量。我们发现，将 LLM 集成到 ASR 和 MT 系统中，对于 tst2019 测试集，单词错误率绝对改善了 $0.3\%$，COMET 绝对改善了 $0.65\%$。在具有重叠说话者和背景噪音的具有挑战性的测试集中，我们发现集成 LLM 并不有益，因为 ASR 性能较差。在这里，我们使用带有分块长格式解码的 ASR 来改进上下文使用，这在仅使用语音活动检测分段进行转录时可能无法实现。</li>
</ul>

<h3>Title: Title:
          Finding Transformer Circuits with Edge Pruning</h3>
<ul>
<li><strong>Authors: </strong>Adithya Bhaskar, Alexander Wettig, Dan Friedman, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Finding Transformer Circuits with Edge Pruning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>The path to interpreting a language model often proceeds via analysis of circuits -- sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they rely either on inefficient search algorithms or inaccurate approximations. In this paper, we frame automated circuit discovery as an optimization problem and propose *Edge Pruning* as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, it prunes the \emph{edges} between components. Our method finds circuits in GPT-2 that use less than half the number of edges compared to circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient even with as many as 100K examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale that prior methods operate on. We use this setting for a case study comparing the mechanisms behind instruction prompting and in-context learning. We find two circuits with more than 99.96% sparsity that match the performance of the full model and reveal that the mechanisms in the two settings overlap substantially. Our case study shows that Edge Pruning is a practical and scalable tool for interpretability and sheds light on behaviors that only emerge in large models.</li>
<li><strong>摘要：</strong>解释语言模型的途径通常是通过分析电路进行的——电路是模型的稀疏计算子图，可以捕捉其行为的特定方面。最近的工作已经使发现电路的任务自动化。然而，这些方法有实际的局限性，因为它们要么依赖于低效的搜索算法，要么依赖于不准确的近似值。在本文中，我们将自动电路发现定义为一个优化问题，并提出*边缘修剪*作为一种有效且可扩展的解决方案。边缘修剪利用基于梯度的修剪技术，但它不是删除神经元或组件，而是修剪组件之间的\emph{边}。我们的方法在 GPT-2 中找到的电路使用的边数不到以前方法找到的电路的一半，同时在标准电路查找任务上同样忠实于完整的模型预测。即使有多达 100K 个示例，边缘修剪也是有效的，在速度上优于以前的方法，并产生更好的电路。它还完美地恢复了用 Tracr 编译的两个模型中的真实电路。由于其效率高，我们将 Edge Pruning 扩展到 CodeLlama-13B，该模型的规模是之前方法的 100 倍以上。我们使用此设置进行案例研究，比较指令提示和上下文学习背后的机制。我们发现两个稀疏度超过 99.96% 的电路与完整模型的性能相匹配，并表明两种设置中的机制存在很大重叠。我们的案例研究表明，Edge Pruning 是一种实用且可扩展的可解释性工具，并揭示了仅在大型模型中出现的行为。</li>
</ul>

<h3>Title: Title:
          It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Sagi Shaier, Lawrence E Hunter, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Natural language processing has seen rapid progress over the past decade. Due to the speed of developments, some practices get established without proper evaluation. Considering one such case and focusing on reading comprehension, we ask our first research question: 1) How does the order of inputs -- i.e., question and context -- affect model performance? Additionally, given recent advancements in input emphasis, we ask a second research question: 2) Does emphasizing either the question, the context, or both enhance performance? Experimenting with 9 large language models across 3 datasets, we find that presenting the context before the question improves model performance, with an accuracy increase of up to $31\%$. Furthermore, emphasizing the context yields superior results compared to question emphasis, and in general, emphasizing parts of the input is particularly effective for addressing questions that models lack the parametric knowledge to answer. Experimenting with both prompt-based and attention-based emphasis methods, we additionally find that the best method is surprisingly simple: it only requires concatenating a few tokens to the input and results in an accuracy improvement of up to $36\%$, allowing smaller models to outperform their significantly larger counterparts.</li>
<li><strong>摘要：</strong>过去十年，自然语言处理取得了快速发展。由于发展速度太快，一些实践在没有经过适当评估的情况下就确立了。考虑到这样的一个案例，并专注于阅读理解，我们提出了第一个研究问题：1）输入的顺序（即问题和上下文）如何影响模型性能？此外，鉴于输入强调方面的最新进展，我们提出了第二个研究问题：2）强调问题、上下文还是两者兼而有之可以提高性能？在 3 个数据集上对 9 个大型语言模型进行了实验，我们发现在问题之前呈现上下文可以提高模型性能，准确率可提高高达 $31\%$。此外，与强调问题相比，强调上下文可以产生更好的结果，并且通常，强调输入的部分对于解决模型缺乏参数知识来回答的问题特别有效。通过对基于提示和基于注意力的强调方法进行实验，我们还发现最好的方法出奇的简单：它只需要将几个标记连接到输入中，即可将准确度提高高达 $36\%$，从而使较小的模型能够胜过明显较大的模型。</li>
</ul>

<h3>Title: Title:
          M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on high resource languages such as English. In this work, we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual, Multi-turn instruction finetuning dataset, called M2Lingual, to better align LLMs on a diverse set of languages and tasks. M2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds, covering 70 languages, 17 NLP tasks and general instruction-response pairs. LLMs finetuned with M2Lingual substantially outperform the majority of existing multilingual IFT datasets. Importantly, LLMs trained with M2Lingual consistently achieve competitive results across a wide variety of evaluation benchmarks compared to existing multilingual IFT datasets. Specifically, LLMs finetuned with M2Lingual achieve strong performance on our translated multilingual, multi-turn evaluation benchmark as well as a wide variety of multilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for its creation. M2Lingual repository - this https URL</li>
<li><strong>摘要：</strong>指令微调 (IFT) 对于对齐大型语言模型 (LLM) 以遵循指令至关重要。最近提出了许多有效的 IFT 数据集，但大多数都集中在资源丰富的语言上，例如英语。在这项工作中，我们提出了一个完全合成的、新分类法 (Evol) 引导的多语言、多轮指令微调数据集，称为 M2Lingual，以便更好地对齐 LLM 以适应各种语言和任务。M2Lingual 总共包含 182K 个 IFT 对，这些对基于不同的种子构建，涵盖 70 种语言、17 个 NLP 任务和一般指令-响应对。使用 M2Lingual 微调的 LLM 大大优于大多数现有的多语言 IFT 数据集。重要的是，与现有的多语言 IFT 数据集相比，使用 M2Lingual 训练的 LLM 在各种评估基准上始终取得有竞争力的结果。具体来说，使用 M2Lingual 微调的 LLM 在我们的翻译多语言、多轮评估基准以及各种多语言任务上取得了出色的表现。因此，我们做出了贡献，并为其创建使用了 2 步 Evol 分类法。M2Lingual 存储库 - 此 https URL</li>
</ul>

<h3>Title: Title:
          Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ashwinee Panda, Berivan Isik, Xiangyu Qi, Sanmi Koyejo, Tsachy Weissman, Prateek Mittal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Existing methods for adapting large language models (LLMs) to new tasks are not suited to multi-task adaptation because they modify all the model weights -- causing destructive interference between tasks. The resulting effects, such as catastrophic forgetting of earlier tasks, make it challenging to obtain good performance on multiple tasks at the same time. To mitigate this, we propose Lottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies and optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide range of challenging tasks such as instruction following, reasoning, math, and summarization. LoTA obtains better performance than full fine-tuning and low-rank adaptation (LoRA), and maintains good performance even after training on other tasks -- thus, avoiding catastrophic forgetting. By extracting and fine-tuning over \emph{lottery tickets} (or \emph{sparse task vectors}), LoTA also enables model merging over highly dissimilar tasks.</li>
<li><strong>摘要：</strong>现有的将大型语言模型 (LLM) 适配到新任务的方法不适合多任务适应，因为它们会修改所有模型权重，从而导致任务之间产生破坏性干扰。由此产生的后果，例如对早期任务的灾难性遗忘，使得同时在多个任务上获得良好性能变得具有挑战性。为了缓解这种情况，我们提出了彩票自适应 (LoTA)，这是一种稀疏自适应方法，仅识别和优化模型的稀疏子网络。我们在一系列具有挑战性的任务上评估了 LoTA，例如指令遵循、推理、数学和总结。LoTA 获得了比完全微调和低秩自适应 (LoRA) 更好的性能，并且即使在其他任务上训练后也能保持良好的性能，从而避免了灾难性遗忘。通过提取和微调 \emph{彩票}（或 \emph{稀疏任务向量}），LoTA 还能够在高度不同的任务上进行模型合并。</li>
</ul>

<h3>Title: Title:
          RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale</h3>
<ul>
<li><strong>Authors: </strong>Beck LaBash, August Rosedale, Alex Reents, Colin Wiel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>The instruction-following ability of Large Language Models (LLMs) has cultivated a class of LLM-based systems capable of approaching complex tasks such as making edits to large code repositories. Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems. We propose RES-Q, a natural language instruction-based benchmark for evaluating $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ystems, which consists of 100 repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system's ability to gather information and construct an edit that satisfies the criteria set by the instruction. We argue that evaluating LLMs in this way addresses issues with traditional benchmarks and provides a more holistic assessment of a model's abilities. We evaluate various state-of-the-art LLMs as language agents in a repository-editing system built on Qurrent OS, our language agent development software. Despite their 1% pass@1 performance difference on HumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's capacity to differentiate model capability as traditional benchmarks approach saturation. We further investigate token efficiency, performance relationships with existing benchmarks, and interesting disparities between closed and open-source LLMs. Code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的指令跟踪能力催生了一类基于 LLM 的系统，它们能够处理诸如编辑大型代码库之类的复杂任务。由于 LLM 行为对提示变化的响应具有高敏感性和不可预测性，因此需要强大的评估工具来推动这些系统的未来迭代。我们提出了 RES-Q，这是一个基于自然语言指令的基准，用于评估 $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ 系统，它由 100 个来自真实 GitHub 提交的存储库编辑任务组成。给定一个编辑指令和一个代码库，RES-Q 会评估 LLM 系统收集信息和构建满足指令设定的标准的编辑的能力。我们认为，以这种方式评估 LLM 可以解决传统基准的问题，并提供对模型能力的更全面评估。我们在基于我们的语言代理开发软件 Qurrent OS 构建的存储库编辑系统中评估了各种最先进的 LLM 作为语言代理。尽管它们在 HumanEval 上的 pass@1 性能差异为 1%，但我们发现 Claude Sonnet 3.5 在 RES-Q 上的表现比 GPT-4o 高出 12%，这表明 RES-Q 有能力在传统基准接近饱和时区分模型能力。我们进一步研究了 token 效率、与现有基准的性能关系以及闭源和开源 LLM 之间的有趣差异。代码和数据集可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          Understanding and Mitigating Tokenization Bias in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Buu Phan, Marton Havasi, Matthew Muckley, Karen Ullrich</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Understanding and Mitigating Tokenization Bias in Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>State-of-the-art language models are autoregressive and operate on subword units known as tokens. Specifically, one must encode the conditioning string into a list of tokens before passing to the language models for next-token prediction. We show that, for encoding schemes such as maximum prefix matching, tokenization induces a sampling bias that cannot be mitigated with more training or data. To counter this universal problem, we propose a novel algorithm to obtain unbiased estimates from a model that was trained on tokenized data. Our method does not require finetuning the model, and its complexity, defined as the number of model runs, scales linearly with the sequence length. As a consequence, we show that one can simulate token-free behavior from a tokenized language model. We empirically verify the correctness of our method through a Markov-chain setup, where it accurately recovers the transition probabilities, as opposed to the conventional method of directly prompting tokens into the language model.</li>
<li><strong>摘要：</strong>最先进的语言模型是自回归的，对称为标记的子词单元进行操作。具体来说，必须将条件字符串编码为标记列表，然后才能将其传递给语言模型进行下一个标记预测。我们表明，对于最大前缀匹配等编码方案，标记化会导致采样偏差，而这种偏差无法通过更多的训练或数据来缓解。为了解决这个普遍问题，我们提出了一种新算法，用于从在标记化数据上训练的模型中获得无偏估计。我们的方法不需要对模型进行微调，其复杂性（定义为模型运行次数）与序列长度成线性关系。因此，我们表明可以从标记化语言模型中模拟无标记行为。我们通过马尔可夫链设置通过经验验证了我们方法的正确性，它准确地恢复了转移概率，而不是直接将标记提示到语言模型中的传统方法。</li>
</ul>

<h3>Title: Title:
          USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations</h3>
<ul>
<li><strong>Authors: </strong>Mounika Marreddy, Subba Reddy Oota, Venkata Charan Chinni, Manish Gupta, Lucie Flek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising, and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) It is time-consuming and costly; 2) Conversation threads could be very long, increasing chances of noisy annotations; and 3) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human annotation process on the following two tasks while also providing reasoning: i) User Stance classification, which involves labeling a user's stance of a post in a conversation on a five-point scale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. The majority voting on zero-shot, one-shot, and few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available [https://anonymous.4open.science/r/USDC-0F7F].</li>
<li><strong>摘要：</strong>在有关各种主题的长对话线程中识别用户的观点和立场对于增强个性化、市场研究、政治运动、客户服务、冲突解决、定向广告和内容审核至关重要。因此，训练语言模型来自动化这项任务至关重要。但是，要训练这样的模型，收集手动注释面临多重挑战：1）耗时且成本高昂；2）对话线程可能非常长，增加了出现噪声注释的可能性；3）解释用户在对话中改变意见的情况很困难，因为这种转变通常很微妙且未明确表达。受大型语言模型 (LLM) 在复杂自然语言处理 (NLP) 任务中取得的最新成功的启发，我们利用 Mistral Large 和 GPT-4 来自动化以下两个任务的人工注释过程，同时提供推理：i）用户立场分类，涉及在五分量表上标记用户在对话中对帖子的立场； ii) 用户教条主义分类，它涉及在四点量表中标记用户在对话中的总体意见。在 764 个多用户 Reddit 对话中，对这两个 LLM 的零样本、一样本和少量样本注释的多数投票有助于我们整理 USDC 数据集。然后使用 USDC 对 5 类立场和 4 类教条主义分类任务的多个可部署小型语言模型进行微调和指令调整。我们公开提供代码和数据集 [https://anonymous.4open.science/r/USDC-0F7F]。</li>
</ul>

<h3>Title: Title:
          From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.</li>
<li><strong>摘要：</strong>现代大型语言模型 (LLM) 研究中最引人注目的发现之一是，在训练期间扩展计算可带来更好的结果。然而，人们较少关注在推理期间扩展计算的好处。本调查重点关注这些推理时间方法。我们在统一的数学形式下探索三个领域：标记级生成算法、元生成算法和高效生成。标记级生成算法通常称为解码算法，通过一次采样单个标记或构建标记级搜索空间然后选择输出来运行。这些方法通常假设可以访问语言模型的 logit、下一个标记分布或概率分数。元生成算法适用于部分或完整序列，结合领域知识，实现回溯并集成外部信息。高效生成方法旨在降低标记成本并提高生成速度。我们的调查统一了三个研究社区的观点：传统自然语言处理、现代 LLM 和机器学习系统。</li>
</ul>

<h3>Title: Title:
          Exploring Factual Entailment with NLI: A News Media Study</h3>
<ul>
<li><strong>Authors: </strong>Guy Mor-Lan, Effi Levi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Exploring Factual Entailment with NLI: A News Media Study(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>We explore the relationship between factuality and Natural Language Inference (NLI) by introducing FactRel -- a novel annotation scheme that models \textit{factual} rather than \textit{textual} entailment, and use it to annotate a dataset of naturally occurring sentences from news articles. Our analysis shows that 84\% of factually supporting pairs and 63\% of factually undermining pairs do not amount to NLI entailment or contradiction, respectively, suggesting that factual relationships are more apt for analyzing media discourse. We experiment with models for pairwise classification on the new dataset, and find that in some cases, generating synthetic data with GPT-4 on the basis of the annotated dataset can improve performance. Surprisingly, few-shot learning with GPT-4 yields strong results on par with medium LMs (DeBERTa) trained on the labelled dataset. We hypothesize that these results indicate the fundamental dependence of this task on both world knowledge and advanced reasoning abilities.</li>
<li><strong>摘要：</strong>我们通过引入 FactRel（一种对 \textit{事实} 而非 \textit{文本} 蕴涵进行建模的新型注释方案）来探索事实性与自然语言推理 (NLI) 之间的关系，并使用它来注释新闻文章中自然出现的句子的数据集。我们的分析表明，84% 的事实支持对和 63% 的事实破坏对分别不属于 NLI 蕴涵或矛盾，这表明事实关系更适合分析媒体话语。我们在新数据集上试验了成对分类模型，发现在某些情况下，基于注释数据集使用 GPT-4 生成合成数据可以提高性能。令人惊讶的是，使用 GPT-4 进行小样本学习可产生与在标记数据集上训练的中等 LM（DeBERTa）相当的强劲效果。我们假设这些结果表明这项任务对世界知识和高级推理能力的根本依赖。</li>
</ul>

<h3>Title: Title:
          RaTEScore: A Metric for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RaTEScore: A Metric for Radiology Report Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel, entity-aware metric, termed as Radiological Report (Text) Evaluation (RaTEScore), to assess the quality of medical reports generated by AI models. RaTEScore emphasizes crucial medical entities such as diagnostic outcomes and anatomical details, and is robust against complex medical synonyms and sensitive to negation expressions. Technically, we developed a comprehensive medical NER dataset, RaTE-NER, and trained an NER model specifically for this purpose. This model enables the decomposition of complex radiological reports into constituent medical entities. The metric itself is derived by comparing the similarity of entity embeddings, obtained from a language model, based on their types and relevance to clinical significance. Our evaluations demonstrate that RaTEScore aligns more closely with human preference than existing metrics, validated both on established public benchmarks and our newly proposed RaTE-Eval benchmark.</li>
<li><strong>摘要：</strong>本文介绍了一种新颖的实体感知指标，称为放射学报告（文本）评估（RaTEScore），用于评估 AI 模型生成的医疗报告的质量。RaTEScore 强调关键的医疗实体，例如诊断结果和解剖细节，并且对复杂的医学同义词具有鲁棒性，并且对否定表达式很敏感。从技术上讲，我们开发了一个全面的医学 NER 数据集 RaTE-NER，并专门为此训练了一个 NER 模型。该模型能够将复杂的放射学报告分解为组成医学实体。该指标本身是通过比较从语言模型获得的实体嵌入的相似性得出的，基于它们的类型和与临床意义的相关性。我们的评估表明，RaTEScore 比现有指标更符合人类偏好，这在既定的公共基准和我们新提出的 RaTE-Eval 基准上都得到了验证。</li>
</ul>

<h3>Title: Title:
          Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts</h3>
<ul>
<li><strong>Authors: </strong>Aditya Sharma, Michael Saxon, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context</a></li>
<li><strong>Abstract: </strong>We present LoCoVQA, a dynamic benchmark generator for evaluating long-context extractive reasoning in vision language models (VLMs). LoCoVQA augments test examples for mathematical reasoning, VQA, and character recognition tasks with increasingly long visual contexts composed of both in-distribution and out-of-distribution distractor images. Across these tasks, a diverse set of VLMs rapidly lose performance as the visual context length grows, often exhibiting a striking exponential decay trend. This test assesses how well VLMs can ignore irrelevant information when answering queries -- a task that is quite easy for language models (LMs) in the text domain -- demonstrating that current state-of-the-art VLMs lack this essential capability for many long-context applications.</li>
<li><strong>摘要：</strong>我们提出了 LoCoVQA，这是一种动态基准生成器，用于评估视觉语言模型 (VLM) 中的长上下文提取推理。LoCoVQA 增强了数学推理、VQA 和字符识别任务的测试示例，其视觉上下文越来越长，由分布内和分布外的干扰图像组成。在这些任务中，随着视觉上下文长度的增加，各种 VLM 的性能迅速下降，通常呈现出惊人的指数衰减趋势。该测试评估了 VLM 在回答查询时忽略不相关信息的能力——对于文本领域的语言模型 (LM) 来说，这项任务相当容易——这表明目前最先进的 VLM 缺乏许多长上下文应用程序所需的这种基本能力。</li>
</ul>

<h3>Title: Title:
          EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Inference with modern Large Language Models (LLMs) is expensive and time-consuming, and speculative sampling has proven to be an effective solution. Most speculative sampling methods such as EAGLE use a static draft tree, implicitly assuming that the acceptance rate of draft tokens depends only on their position. Interestingly, we found that the acceptance rate of draft tokens is also context-dependent. In this paper, building upon EAGLE, we propose EAGLE-2, which introduces a new technique of context-aware dynamic draft tree into drafting modeling. This improvement leverages the fact that the draft model of EAGLE is well-calibrated: the confidence scores from the draft model approximate acceptance rates with small errors. We conducted extensive evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also ensures that the distribution of the generated text remains unchanged, making it a lossless acceleration algorithm.</li>
<li><strong>摘要：</strong>使用现代大型语言模型 (LLM) 进行推理既昂贵又耗时，而推测性采样已被证明是一种有效的解决方案。大多数推测性采样方法（例如 EAGLE）都使用静态草稿树，隐含地假设草稿标记的接受率仅取决于其位置。有趣的是，我们发现草稿标记的接受率也与上下文有关。在本文中，我们基于 EAGLE 提出了 EAGLE-2，它将一种新的上下文感知动态草稿树技术引入到草稿建模中。这一改进利用了 EAGLE 草稿模型经过良好校准的事实：草稿模型的置信度得分近似于接受率，误差很小。我们对三个系列的 LLM 和六个任务进行了广泛的评估，EAGLE-2 实现了 3.05 倍至 4.26 倍的加速比，比 EAGLE-1 快 20% 至 40%。EAGLE-2 还确保生成文本的分布保持不变，使其成为一种无损加速算法。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
