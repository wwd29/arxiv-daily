<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-05</h1>
<h3>Title: CycleResearcher: Improving Automated Research via Automated Review</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00816">https://arxiv.org/abs/2411.00816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00816">https://arxiv.org/pdf/2411.00816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00816]] CycleResearcher: Improving Automated Research via Automated Review(https://arxiv.org/abs/2411.00816)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper revision. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves a 26.89\% improvement in mean absolute error (MAE) over individual human reviewers in predicting paper scores, indicating that LLMs can surpass expert-level performance in research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, surpassing the preprint level of 5.24 from human experts and approaching the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and advancing AI-driven research capabilities. The code, dataset and model weight are released at \url{http://github/minjun-zhu/Researcher}.</li>
<li><strong>摘要：</strong>实现科学发现的自动化一直是研究界的一个长期目标，其驱动力在于加速知识创造。虽然使用商业大型语言模型 (LLM) 作为研究助手或创意生成器已经取得了重大进展，但使用开源 LLM 实现整个研究过程自动化的可能性仍未得到充分探索。本文探讨了使用开源后训练 LLM 作为自主代理的可行性，这些代理能够执行从文献审查和手稿准备到同行评审和论文修订的全周期自动化研究和审查。我们的迭代偏好训练框架包括执行研究任务的 CycleResearcher 和模拟同行评审过程的 CycleReviewer，通过强化学习提供迭代反馈。为了训练这些模型，我们开发了两个新的数据集，Review-5k 和 Research-14k，反映了现实世界的机器学习研究和同行评审动态。我们的结果表明，CycleReviewer 在预测论文分数方面的平均绝对误差 (MAE) 比单个人类审阅者提高了 26.89%，这表明 LLM 在研究评估方面的表现可以超越专家级。在研究中，CycleResearcher 模型生成的论文在模拟同行评审中获得了 5.36 分，超过了人类专家的预印本水平 5.24，接近已接受论文的水平 5.69。这项工作代表着朝着完全自动化的科学探究迈出了重要一步，提供了道德保障并提高了人工智能驱动的研究能力。代码、数据集和模型权重在 \url{http://github/minjun-zhu/Researcher} 上发布。</li>
</ul>

<h3>Title: Survey of Cultural Awareness in Language Models: Text and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Siddhesh Pawar, Junyeong Park, Jiho Jin, Arnav Arora, Junho Myung, Srishti Yadav, Faiz Ghifari Haznitrama, Inhwa Song, Alice Oh, Isabelle Augenstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00860">https://arxiv.org/abs/2411.00860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00860">https://arxiv.org/pdf/2411.00860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00860]] Survey of Cultural Awareness in Language Models: Text and Beyond(https://arxiv.org/abs/2411.00860)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.</li>
<li><strong>摘要：</strong>在聊天机器人和虚拟助手等各种应用中大规模部署大型语言模型 (LLM) 需要 LLM 对用户具有文化敏感性，以确保包容性。文化在心理学和人类学中得到了广泛的研究，最近出现了大量关于如何使 LLM 在 LLM 中更具文化包容性的研究，这些研究超越了多语言性，并以心理学和人类学的研究成果为基础。在本文中，我们调查了将文化意识融入基于文本和多模态的 LLM 的努力。我们首先定义 LLM 中的文化意识，以人类学和心理学中的文化定义为出发点。然后，我们研究了用于创建跨文化数据集的方法、下游任务中的文化包容性策略以及用于对 LLM 中的文化意识进行基准测试的方法。此外，我们讨论了文化一致性的伦理影响、人机交互在推动 LLM 中的文化包容性方面的作用以及文化一致性在推动社会科学研究中的作用。最后，我们根据关于文献中差距的发现，为未来的研究提供了指引。</li>
</ul>

<h3>Title: Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation</h3>
<ul>
<li><strong>Authors: </strong>Chenyang An, Shima Imani, Feng Yao, Chengyu Dong, Ali Abbasi, Harsh Shrivastava, Samuel Buss, Jingbo Shang, Gayathri Mahalingam, Pramod Sharma, Maurice Diesendruck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00863">https://arxiv.org/abs/2411.00863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00863">https://arxiv.org/pdf/2411.00863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00863]] Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation(https://arxiv.org/abs/2411.00863)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the field of large language model (LLM)-based proof generation, despite being trained on extensive corpora such as OpenWebMath and Arxiv, these models still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the suboptimal order of each proof data used in training. Published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. However, this order aims to facilitate the verification of the proof's soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders are substantial -- with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order.</li>
<li><strong>摘要：</strong>在基于大型语言模型 (LLM) 的证明生成领域，尽管这些模型在 OpenWebMath 和 Arxiv 等大量语料库上进行了训练，但它们在中等难度的证明任务上仍然表现平平。我们认为，这部分是由于训练中使用的每个证明数据的顺序不是最优的。已发布的证明通常遵循纯逻辑顺序，其中每个步骤都基于演绎规则从前面的步骤逻辑地进行。然而，这种顺序旨在促进对证明的合理性的验证，而不是帮助人们和模型学习证明的发现过程。在证明生成中，我们认为，当证明中特定证明步骤的相关中间监督始终位于该证明步骤的左侧时，一个训练数据样本的最佳顺序就会发生。我们将这种顺序称为直观顺序。我们使用两个任务来验证我们的主张：直觉命题逻辑定理证明和数字乘法。我们的实验验证了顺序效应并为我们的解释提供了支持。我们证明，当证明按照直观的顺序进行时，训练是最有效的。此外，顺序效应和在不同数据顺序上训练的模型之间的性能差距是巨大的——在命题逻辑定理证明任务中，与按最差顺序训练的模型相比，按最佳顺序训练的模型的证明成功率提高了 11%。</li>
</ul>

<h3>Title: Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models</h3>
<ul>
<li><strong>Authors: </strong>Phil Wee, Riyadh Baghdadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00878">https://arxiv.org/abs/2411.00878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00878">https://arxiv.org/pdf/2411.00878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00878]] Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models(https://arxiv.org/abs/2411.00878)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Recently, there has been an explosion of large language models created through fine-tuning with data from larger models. These small models able to produce outputs that appear qualitatively similar to significantly larger models. However, one of the key limitations that have been observed with these models is their propensity to hallucinate significantly more often than larger models. In particular, they have been observed to generate coherent outputs that involve factually incorrect information and spread misinformation, toxicity, and stereotypes. There are many potential causes of hallucination, of which, one hypothesis is that fine-tuning a model on data produced by a larger model leads to a knowledge mismatch which contributes to hallucination. In particular, it is hypothesized that there is a mismatch between the knowledge that is fed to the model to fine-tune it and the knowledge that is already present in the graph. Fine-tuning the model on data that has such mismatch could contribute to an increased propensity to hallucinate. We show that on an unseen test set, a smaller model fine-tuned on data generated from a larger model produced more wrong answers when compared to models fine-tuned on data created by the small model, which confirms the hypothesis.</li>
<li><strong>摘要：</strong>最近，通过使用来自较大模型的数据进行微调而创建的大型语言模型数量激增。这些小型模型能够产生与大型模型在质量上相似的输出。然而，这些模型的一个关键限制是它们比大型模型更容易产生幻觉。具体来说，据观察，它们会产生连贯的输出，这些输出涉及事实上不正确的信息，并传播错误信息、毒性和刻板印象。幻觉有很多潜在原因，其中一种假设是，对大型模型产生的数据进行微调会导致知识不匹配，从而导致幻觉。具体来说，有人假设，输入模型进行微调的知识与图中已经存在的知识之间存在不匹配。对具有这种不匹配的数据进行模型微调可能会导致幻觉倾向增加。我们表明，在看不见的测试集上，与基于小模型创建的数据进行微调的模型相比，基于较大模型生成的数据进行微调的较小模型产生了更多的错误答案，这证实了这一假设。</li>
</ul>

<h3>Title: Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research</h3>
<ul>
<li><strong>Authors: </strong>Marcello Carammia, Stefano Maria Iacus, Giuseppe Porro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00890">https://arxiv.org/abs/2411.00890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00890">https://arxiv.org/pdf/2411.00890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00890]] Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research(https://arxiv.org/abs/2411.00890)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are distinguished by their architecture, which dictates their parameter size and performance capabilities. Social scientists have increasingly adopted LLMs for text classification tasks, which are difficult to scale with human coders. While very large, closed-source models often deliver superior performance, their use presents significant risks. These include lack of transparency, potential exposure of sensitive data, challenges to replicability, and dependence on proprietary systems. Additionally, their high costs make them impractical for large-scale research projects. In contrast, open-source models, although available in various sizes, may underperform compared to commercial alternatives if used without further fine-tuning. However, open-source models offer distinct advantages: they can be run locally (ensuring data privacy), fine-tuned for specific tasks, shared within the research community, and integrated into reproducible workflows. This study demonstrates that small, fine-tuned open-source LLMs can achieve equal or superior performance to models such as ChatGPT-4. We further explore the relationship between training set size and fine-tuning efficacy in open-source models. Finally, we propose a hybrid workflow that leverages the strengths of both open and closed models, offering a balanced approach to performance, transparency, and reproducibility.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 以其架构而著称，架构决定了它们的参数大小和性能能力。社会科学家越来越多地采用 LLM 进行文本分类任务，而这些任务很难通过人类编码员进行扩展。虽然非常大的闭源模型通常可以提供出色的性能，但使用它们会带来重大风险。这些风险包括缺乏透明度、敏感数据的潜在暴露、可复制性挑战以及对专有系统的依赖。此外，它们的高成本使它们不适用于大型研究项目。相比之下，开源模型虽然有各种尺寸，但如果不进一步微调，其性能可能会低于商业替代品。然而，开源模型具有明显的优势：它们可以在本地运行（确保数据隐私）、针对特定任务进行微调、在研究社区内共享以及集成到可重复的工作流程中。这项研究表明，小型、经过微调的开源 LLM 可以实现与 ChatGPT-4 等模型相同或更优异的性能。我们进一步探讨了开源模型中训练集大小与微调效果之间的关系。最后，我们提出了一种混合工作流程，利用开放和封闭模型的优势，在性能、透明度和可重复性之间提供一种平衡的方法。</li>
</ul>

<h3>Title: Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback</h3>
<ul>
<li><strong>Authors: </strong>Song Yu, Xiaofei Xu, Fangfei Xu, Li Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00897">https://arxiv.org/abs/2411.00897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00897">https://arxiv.org/pdf/2411.00897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00897]] Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback(https://arxiv.org/abs/2411.00897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Although large language models perform well in understanding and responding to user intent, their performance in specialized domains such as Traditional Chinese Medicine (TCM) remains limited due to lack of expertise. In addition, high-quality data related to TCM is scarce and difficult to obtain, making large language models ineffective in handling TCM tasks. In this work, we propose a framework to improve the performance of large language models for TCM tasks using only a small amount of data. First, we use medical case data for supervised fine-tuning of the large model, making it initially capable of performing TCM tasks. Subsequently, we further optimize the model's performance using reinforcement learning from AI feedback (RLAIF) to align it with the preference data. The ablation study also demonstrated the performance gain is attributed to both supervised fine-tuning and the direct policy optimization. The experimental results show that the model trained with a small amount of data achieves a significant performance improvement on a representative TCM task.</li>
<li><strong>摘要：</strong>尽管大型语言模型在理解和响应用户意图方面表现良好，但由于缺乏专业知识，它们在诸如中医 (TCM) 等专业领域的表现仍然有限。此外，与中医相关的高质量数据稀缺且难以获取，使得大型语言模型在处理中医任务时效果不佳。在本文中，我们提出了一个框架，仅使用少量数据即可提高大型语言模型在中医任务中的表现。首先，我们使用医疗案例数据对大型模型进行监督微调，使其最初能够执行中医任务。随后，我们使用来自 AI 反馈的强化学习 (RLAIF) 进一步优化模型的性能，使其与偏好数据保持一致。消融研究还表明性能提升归因于监督微调和直接策略优化。实验结果表明，使用少量数据训练的模型在代表性中医任务上实现了显著的性能提升。</li>
</ul>

<h3>Title: LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nam V. Nguyen, Thong T. Doan, Luong Tran, Van Nguyen, Quang Pham</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00918">https://arxiv.org/abs/2411.00918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00918">https://arxiv.org/pdf/2411.00918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00918]] LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models(https://arxiv.org/abs/2411.00918)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops \emph{LibMoE}, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: \url{this https URL}.</li>
<li><strong>摘要：</strong>专家混合模型 (MoE) 在开发更高效、更有效的大型语言模型 (LLM) 方面发挥着重要作用。由于巨大的资源需求，许多研究人员仍然无法研究大规模 MoE 算法。这项工作开发了 \emph{LibMoE}，这是一个全面的模块化框架，用于简化 MoE 算法的研究、训练和评估。LibMoE 基于三个核心原则：(i) 模块化设计、(ii) 高效训练；(iii) 全面评估，通过标准化训练和评估流程，LibMoE 使 LLM 中的 MoE 更容易被广泛的研究人员使用。使用 LibMoE，我们在零样本设置下对三个不同的 LLM 和 11 个数据集上的五种最先进的 MoE 算法进行了广泛的基准测试。结果表明，尽管 MoE 算法具有独特的特性，但在广泛的任务中平均表现大致相同。凭借模块化设计和广泛的评估，我们相信 LibMoE 将对研究人员在下一代 MoE 和 LLM 方面取得有意义的进展大有裨益。项目页面：\url{此 https URL}。</li>
</ul>

<h3>Title: ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Vardhan Dongre, Xiaocheng Yang, Emre Can Acikgoz, Suvodip Dey, Gokhan Tur, Dilek Hakkani-Tür</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00927">https://arxiv.org/abs/2411.00927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00927">https://arxiv.org/pdf/2411.00927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00927]] ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents(https://arxiv.org/abs/2411.00927)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agents have been increasingly used to interact with external environments (e.g., games, APIs, etc.) and solve tasks. However, current frameworks do not enable these agents to work with users and interact with them to align on the details of their tasks and reach user-defined goals; instead, in ambiguous situations, these agents may make decisions based on assumptions. This work introduces ReSpAct (Reason, Speak, and Act), a novel framework that synergistically combines the essential skills for building task-oriented "conversational" agents. ReSpAct addresses this need for agents, expanding on the ReAct approach. The ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions, and engage in dynamic dialogue to seek guidance, clarify ambiguities, understand user preferences, resolve problems, and use the intermediate feedback and responses of users to update their plans. We evaluated ReSpAct in environments supporting user interaction, such as task-oriented dialogue (MultiWOZ) and interactive decision-making (AlfWorld, WebShop). ReSpAct is flexible enough to incorporate dynamic user feedback and addresses prevalent issues like error propagation and agents getting stuck in reasoning loops. This results in more interpretable, human-like task-solving trajectories than relying solely on reasoning traces. In two interactive decision-making benchmarks, AlfWorld and WebShop, ReSpAct outperform the strong reasoning-only method ReAct by an absolute success rate of 6% and 4%, respectively. In the task-oriented dialogue benchmark MultiWOZ, ReSpAct improved Inform and Success scores by 5.5% and 3%, respectively.</li>
<li><strong>摘要：</strong>基于大型语言模型 (LLM) 的代理越来越多地用于与外部环境（例如游戏、API 等）交互并解决任务。然而，当前的框架不允许这些代理与用户合作并与他们交互以协调任务细节并实现用户定义的目标；相反，在模棱两可的情况下，这些代理可能会根据假设做出决策。这项工作引入了 ReSpAct（推理、说话和行动），这是一个新颖的框架，它协同结合了构建面向任务的“对话”代理的基本技能。ReSpAct 满足了代理的这种需求，扩展了 ReAct 方法。ReSpAct 框架使代理能够解释用户指令、推理复杂任务、执行适当的操作并参与动态对话以寻求指导、澄清歧义、理解用户偏好、解决问题，并使用用户的中间反馈和响应来更新他们的计划。我们在支持用户交互的环境中评估了 ReSpAct，例如面向任务的对话 (MultiWOZ) 和交互式决策 (AlfWorld、WebShop)。 ReSpAct 足够灵活，可以纳入动态用户反馈，并解决诸如错误传播和代理陷入推理循环等常见问题。与仅依赖推理轨迹相比，这可以产生更易于解释、更像人类的任务解决轨迹。在两个交互式决策基准测试 AlfWorld 和 WebShop 中，ReSpAct 的表现分别比仅使用强大推理的方法 ReAct 的绝对成功率高出 6% 和 4%。在面向任务的对话基准测试 MultiWOZ 中，ReSpAct 分别将 Inform 和 Success 得分提高了 5.5% 和 3%。</li>
</ul>

<h3>Title: Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO</h3>
<ul>
<li><strong>Authors: </strong>Macarious Hui, Jinda Zhang, Aanchan Mohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00980">https://arxiv.org/abs/2411.00980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00980">https://arxiv.org/pdf/2411.00980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00980]] Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO(https://arxiv.org/abs/2411.00980)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) frequently face challenges with articulation, leading to dysarthria and resulting in atypical speech patterns. In healthcare settings, coomunication breakdowns reduce the quality of care. While building an augmentative and alternative communication (AAC) tool to enable fluid communication we found that state-of-the-art (SOTA) automatic speech recognition (ASR) technology like Whisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack of training data. Our work looks to leverage SOTA ASR followed by domain specific error-correction. English dysarthric ASR performance is often evaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this dataset where phrases overlap between training and test speakers. Our work proposes an algorithm to break this prompt-overlap. After reducing prompt-overlap, results with SOTA ASR models produce extremely high word error rates for speakers with mild and severe dysarthria. Furthermore, to improve ASR, our work looks at the impact of n-gram language models and large-language model (LLM) based multi-modal generative error-correction algorithms like Whispering-LLaMA for a second pass ASR. Our work highlights how much more needs to be done to improve ASR for atypical speakers to enable equitable healthcare access both in-person and in e-health settings.</li>
<li><strong>摘要：</strong>患有脑瘫 (CP) 和肌萎缩侧索硬化症 (ALS) 的人经常面临发音困难，导致构音障碍和不正常的说话模式。在医疗保健环境中，沟通障碍会降低护理质量。在构建增强和替代沟通 (AAC) 工具以实现流畅沟通时，我们发现最先进的 (SOTA) 自动语音识别 (ASR) 技术（如 Whisper 和 Wav2vec2.0）在很大程度上由于缺乏训练数据而使非典型说话者边缘化。我们的工作旨在利用 SOTA ASR，然后进行特定领域的错误纠正。英语构音障碍 ASR 性能通常在 TORGO 数据集上进行评估。提示重叠是此数据集的一个众所周知的问题，即训练和测试说话者之间的短语重叠。我们的工作提出了一种打破这种提示重叠的算法。减少提示重叠后，SOTA ASR 模型的结果显示，对于患有轻度和重度构音障碍的说话者，单词错误率极高。此外，为了改进 ASR，我们的工作研究了 n-gram 语言模型和基于大语言模型 (LLM) 的多模态生成纠错算法（如 Whispering-LLaMA）对第二遍 ASR 的影响。我们的工作强调了还需要做多少工作来改进非典型说话者的 ASR，以便在面对面和电子医疗环境中实现公平的医疗保健。</li>
</ul>

<h3>Title: FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wu, Simin Chen, Yuzhe Yang, Yijiang Li, Shiyue Hou, Rui Jing, Zehua Wang, Wei Chen, Zijian Tian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00985">https://arxiv.org/abs/2411.00985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00985">https://arxiv.org/pdf/2411.00985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00985]] FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models(https://arxiv.org/abs/2411.00985)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have significantly advanced the field of natural language processing (NLP). By fine-tuning LLMs with data from specific scenarios, these foundation models can better adapt to various downstream tasks. However, the fine-tuning process poses privacy leakage risks, particularly in centralized data processing scenarios. To address user privacy concerns, federated learning (FL) has been introduced to mitigate the risks associated with centralized data collection from multiple sources. Nevertheless, the privacy of LLMs themselves is equally critical, as potential malicious attacks challenge their security, an issue that has received limited attention in current research. Consequently, establishing a trusted multi-party model fine-tuning environment is essential. Additionally, the local deployment of large LLMs incurs significant storage costs and high computational demands. To address these challenges, we propose for the first time a federated discrete and transferable prompt tuning, namely FedDTPT, for black-box large language models. In the client optimization phase, we adopt a token-level discrete prompt optimization method that leverages a feedback loop based on prediction accuracy to drive gradient-free prompt optimization through the MLM API. For server optimization, we employ an attention mechanism based on semantic similarity to filter all local prompt tokens, along with an embedding distance elbow detection and DBSCAN clustering strategy to enhance the filtering process. Experimental results demonstrate that, compared to state-of-the-art methods, our approach achieves higher accuracy, reduced communication overhead, and robustness to non-iid data in a black-box setting. Moreover, the optimized prompts are transferable.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 极大地推动了自然语言处理 (NLP) 领域的发展。通过使用特定场景的数据对 LLM 进行微调，这些基础模型可以更好地适应各种下游任务。然而，微调过程存在隐私泄露风险，尤其是在集中式数据处理场景中。为了解决用户隐私问题，引入了联邦学习 (FL) 来减轻与从多个来源进行集中式数据收集相关的风险。然而，LLM 本身的隐私同样至关重要，因为潜在的恶意攻击会挑战其安全性，而这个问题在当前的研究中并未受到足够的关注。因此，建立一个可信的多方模型微调环境至关重要。此外，大型 LLM 的本地部署会产生大量的存储成本和高计算需求。为了应对这些挑战，我们首次提出了一种用于黑盒大型语言模型的联邦离散可转移提示调优方法，即 FedDTPT。在客户端优化阶段，我们采用一种 token 级离散提示优化方法，利用基于预测准确性的反馈循环通过 MLM API 驱动无梯度提示优化。对于服务器优化，我们采用基于语义相似性的注意机制来过滤所有本地提示标记，并采用嵌入距离肘部检测和 DBSCAN 聚类策略来增强过滤过程。实验结果表明，与最先进的方法相比，我们的方法在黑盒设置中实现了更高的准确率、更低的通信开销和对非独立同分布数据的鲁棒性。此外，优化后的提示具有可迁移性。</li>
</ul>

<h3>Title: Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM Generation Output</h3>
<ul>
<li><strong>Authors: </strong>Hithesh Sankararaman, Mohammed Nasheed Yasin, Tanner Sorensen, Alessandro Di Bari, Andreas Stolcke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01022">https://arxiv.org/abs/2411.01022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01022">https://arxiv.org/pdf/2411.01022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01022]] Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM Generation Output(https://arxiv.org/abs/2411.01022)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>We present a light-weight approach for detecting nonfactual outputs from retrieval-augmented generation (RAG). Given a context and putative output, we compute a factuality score that can be thresholded to yield a binary decision to check the results of LLM-based question-answering, summarization, or other systems. Unlike factuality checkers that themselves rely on LLMs, we use compact, open-source natural language inference (NLI) models that yield a freely accessible solution with low latency and low cost at run-time, and no need for LLM fine-tuning. The approach also enables downstream mitigation and correction of hallucinations, by tracing them back to specific context chunks. Our experiments show high area under the ROC curve (AUC) across a wide range of relevant open source datasets, indicating the effectiveness of our method for fact-checking RAG output.</li>
<li><strong>摘要：</strong>我们提出了一种轻量级方法来检测检索增强生成 (RAG) 中的非事实输出。给定一个上下文和假定的输出，我们计算一个事实性分数，该分数可以进行阈值化以产生二元决策，以检查基于 LLM 的问答、摘要或其他系统的结果。与依赖 LLM 的事实性检查器不同，我们使用紧凑的开源自然语言推理 (NLI) 模型，这些模型可以产生一个可自由访问的解决方案，运行时延迟低、成本低，并且无需 LLM 微调。该方法还可以通过将幻觉追溯到特定的上下文块来实现下游的幻觉缓解和纠正。我们的实验表明，在广泛的相关开源数据集中，ROC 曲线下面积 (AUC) 很高，表明我们的方法对 RAG 输出进行事实核查的有效性。</li>
</ul>

<h3>Title: Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula</h3>
<ul>
<li><strong>Authors: </strong>Sam Blouir, Jimmy Smith, Antonios Anastasopoulos, Amarda Shehu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01030">https://arxiv.org/abs/2411.01030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01030">https://arxiv.org/pdf/2411.01030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01030]] Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula(https://arxiv.org/abs/2411.01030)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Efficient state space models (SSMs), including linear recurrent neural networks and linear attention variants, have emerged as potential alternative language models to Transformers. While efficient, SSMs struggle with tasks requiring in-context retrieval, such as text copying and associative recall, limiting their usefulness in practical settings. Prior work on how to meet this challenge has focused on the internal model architecture and not investigated the role of the training procedure. This paper proposes a new training procedure that strongly improves the performance of SSMs on retrieval-intensive tasks. This novel pre-training procedure combines a bidirectional processing of the input with dynamic mixtures of pre-training objectives to improve the utilization of the SSM's fixed-size state. Our experimental evaluations show that Birdie significantly improves performance on retrieval-intensive tasks that challenge current SSMs, such as phone book lookup, long paragraph question-answering, and infilling tasks. Our findings offer insights into a new direction to advance the training of SSMs to close the performance gap with Transformers.</li>
<li><strong>摘要：</strong>高效的状态空间模型 (SSM)，包括线性循环神经网络和线性注意变体，已成为 Transformers 的潜在替代语言模型。虽然 SSM 很高效，但它们在需要上下文检索的任务（例如文本复制和联想回忆）中却举步维艰，这限制了它们在实际环境中的实用性。关于如何应对这一挑战的先前研究主要集中在内部模型架构上，而没有研究训练过程的作用。本文提出了一种新的训练程序，可大大提高 SSM 在检索密集型任务上的性能。这种新颖的预训练程序将输入的双向处理与预训练目标的动态混合相结合，以提高 SSM 固定大小状态的利用率。我们的实验评估表明，Birdie 显著提高了对当前 SSM 构成挑战的检索密集型任务的性能，例如电话簿查找、长段落问答和填充任务。我们的研究结果为推进 SSM 训练以缩小与 Transformers 的性能差距的新方向提供了见解。</li>
</ul>

<h3>Title: Privacy Risks of Speculative Decoding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiankun Wei, Abdulrahman Abdulrazzag, Tianchen Zhang, Adel Muursepp, Gururaj Saileshwar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01076">https://arxiv.org/abs/2411.01076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01076">https://arxiv.org/pdf/2411.01076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01076]] Privacy Risks of Speculative Decoding in Large Language Models(https://arxiv.org/abs/2411.01076)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative decoding in large language models (LLMs) accelerates token generation by speculatively predicting multiple tokens cheaply and verifying them in parallel, and has been widely deployed. In this paper, we provide the first study demonstrating the privacy risks of speculative decoding. We observe that input-dependent patterns of correct and incorrect predictions can be leaked out to an adversary monitoring token generation times and packet sizes, leading to privacy breaches. By observing the pattern of correctly and incorrectly speculated tokens, we show that a malicious adversary can fingerprint queries and learn private user inputs with more than $90\%$ accuracy across three different speculative decoding techniques - BiLD (almost $100\%$ accuracy), LADE (up to $92\%$ accuracy), and REST (up to $95\%$ accuracy). We show that an adversary can also leak out confidential intellectual property used to design these techniques, such as data from data-stores used for prediction (in REST) at a rate of more than $25$ tokens per second, or even hyper-parameters used for prediction (in LADE). We also discuss mitigation strategies, such as aggregating tokens across multiple iterations and padding packets with additional bytes, to avoid such privacy or confidentiality breaches.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的推测解码通过低成本推测多个标记并并行验证它们来加速标记生成，并且已被广泛部署。在本文中，我们提供了第一份展示推测解码隐私风险的研究。我们观察到，与输入相关的正确和错误预测模式可能会泄露给监视标记生成时间和数据包大小的对手，从而导致隐私泄露。通过观察正确和错误推测的标记模式，我们表明恶意对手可以通过三种不同的推测解码技术（BiLD（准确率接近 100%）、LADE（准确率高达 92%）和 REST（准确率高达 95%））对查询进行指纹识别并以超过 90% 的准确率学习私人用户输入。我们表明，攻击者还可以泄露用于设计这些技术的机密知识产权，例如用于预测的数据存储数据（在 REST 中），速度超过每秒 25 个令牌，甚至用于预测的超参数（在 LADE 中）。我们还讨论了缓解策略，例如在多次迭代中聚合令牌并用额外的字节填充数据包，以避免此类隐私或机密性泄露。</li>
</ul>

<h3>Title: Emoji Attack: A Method for Misleading Judge LLMs in Safety Risk Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Wei, Yuqi Liu, N. Benjamin Erichson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01077">https://arxiv.org/abs/2411.01077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01077">https://arxiv.org/pdf/2411.01077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01077]] Emoji Attack: A Method for Misleading Judge LLMs in Safety Risk Detection(https://arxiv.org/abs/2411.01077)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Jailbreaking attacks show how Large Language Models (LLMs) can be tricked into generating harmful outputs using malicious prompts. To prevent these attacks, other LLMs are often used as judges to evaluate the harmfulness of the generated content. However, relying on LLMs as judges can introduce biases into the detection process, which in turn compromises the effectiveness of the evaluation. In this paper, we show that Judge LLMs, like other LLMs, are also affected by token segmentation bias. This bias occurs when tokens are split into smaller sub-tokens, altering their embeddings. This makes it harder for the model to detect harmful content. Specifically, this bias can cause sub-tokens to differ significantly from the original token in the embedding space, leading to incorrect "safe" predictions for harmful content. To exploit this bias in Judge LLMs, we introduce the Emoji Attack -- a method that places emojis within tokens to increase the embedding differences between sub-tokens and their originals. These emojis create new tokens that further distort the token embeddings, exacerbating the bias. To counter the Emoji Attack, we design prompts that help LLMs filter out unusual characters. However, this defense can still be bypassed by using a mix of emojis and other characters. The Emoji Attack can also be combined with existing jailbreaking prompts using few-shot learning, which enables LLMs to generate harmful responses with emojis. These responses are often mistakenly labeled as "safe" by Judge LLMs, allowing the attack to slip through. Our experiments with six state-of-the-art Judge LLMs show that the Emoji Attack allows 25\% of harmful responses to bypass detection by Llama Guard and Llama Guard 2, and up to 75\% by ShieldLM. These results highlight the need for stronger Judge LLMs to address this vulnerability.</li>
<li><strong>摘要：</strong>越狱攻击表明，大型语言模型 (LLM) 可能会被恶意提示诱骗生成有害输出。为了防止这些攻击，其他 LLM 通常被用作判断者来评估生成内容的有害性。但是，依赖 LLM 作为判断者可能会在检测过程中引入偏差，从而损害评估的有效性。在本文中，我们表明 Judge LLM 与其他 LLM 一样，也受到 token 分割偏差的影响。当 token 被分割成更小的子 token 时，就会发生这种偏差，从而改变它们的嵌入。这使得模型更难检测有害内容。具体而言，这种偏差可能导致子 token 与嵌入空间中的原始 token 存在显著差异，从而导致对有害内容的“安全”预测不正确。为了利用 Judge LLM 中的这种偏差，我们引入了表情符号攻击——一种将表情符号放置在 token 内以增加子 token 与其原始 token 之间的嵌入差异的方法。这些表情符号会创建新的标记，进一步扭曲标记嵌入，从而加剧偏差。为了应对表情符号攻击，我们设计了提示来帮助 LLM 过滤掉不常见的字符。但是，使用表情符号和其他字符的混合仍然可以绕过这种防御。表情符号攻击还可以与使用少样本学习的现有越狱提示相结合，这使 LLM 能够使用表情符号生成有害响应。这些响应经常被 Judge LLM 错误地标记为“安全”，从而让攻击得以溜走。我们对六台最先进的 Judge LLM 进行的实验表明，表情符号攻击允许 25% 的有害响应绕过 Llama Guard 和 Llama Guard 2 的检测，以及高达 75% 的 ShieldLM。这些结果强调需要更强大的 Judge LLM 来解决此漏洞。</li>
</ul>

<h3>Title: Plentiful Jailbreaks with String Compositions</h3>
<ul>
<li><strong>Authors: </strong>Brian R.Y. Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01084">https://arxiv.org/abs/2411.01084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01084">https://arxiv.org/pdf/2411.01084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01084]] Plentiful Jailbreaks with String Compositions(https://arxiv.org/abs/2411.01084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) remain vulnerable to a slew of adversarial attacks and jailbreaking methods. One common approach employed by white-hat attackers, or \textit{red-teamers}, is to process model inputs and outputs using string-level obfuscations, which can include leetspeak, rotary ciphers, Base64, ASCII, and more. Our work extends these encoding-based attacks by unifying them in a framework of invertible string transformations. With invertibility, we can devise arbitrary \textit{string compositions}, defined as sequences of transformations, that we can encode and decode end-to-end programmatically. We devise a automated best-of-n attack that samples from a combinatorially large number of string compositions. Our jailbreaks obtain competitive attack success rates on several leading frontier models when evaluated on HarmBench, highlighting that encoding-based attacks remain a persistent vulnerability even in advanced LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 仍然容易受到一系列对抗性攻击和越狱方法的攻击。白帽攻击者或 \textit{红队成员} 使用的一种常见方法是使用字符串级混淆来处理模型输入和输出，这些混淆可能包括 leetspeak、旋转密码、Base64、ASCII 等。我们的工作通过将这些基于编码的攻击统一到可逆字符串转换框架中来扩展它们。通过可逆性，我们可以设计任意 \textit{字符串组合}，定义为转换序列，我们可以以编程方式对其进行端到端编码和解码。我们设计了一种自动化的最佳 n 攻击，从大量组合的字符串组合中采样。在 HarmBench 上进行评估时，我们的越狱在几个领先的前沿模型上获得了具有竞争力的攻击成功率，这突显了即使在高级 LLM 中，基于编码的攻击仍然是一个持续存在的漏洞。</li>
</ul>

<h3>Title: TabVer: Tabular Fact Verification with Natural Logic</h3>
<ul>
<li><strong>Authors: </strong>Rami Aly, Andreas Vlachos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01093">https://arxiv.org/abs/2411.01093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01093">https://arxiv.org/pdf/2411.01093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01093]] TabVer: Tabular Fact Verification with Natural Logic(https://arxiv.org/abs/2411.01093)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fact verification on tabular evidence incentivises the use of symbolic reasoning models where a logical form is constructed (e.g. a LISP-style program), providing greater verifiability than fully neural approaches. However, these systems typically rely on well-formed tables, restricting their use in many scenarios. An emerging symbolic reasoning paradigm for textual evidence focuses on natural logic inference, which constructs proofs by modelling set-theoretic relations between a claim and its evidence in natural language. This approach provides flexibility and transparency but is less compatible with tabular evidence since the relations do not extend to arithmetic functions. We propose a set-theoretic interpretation of numerals and arithmetic functions in the context of natural logic, enabling the integration of arithmetic expressions in deterministic proofs. We leverage large language models to generate arithmetic expressions by generating questions about salient parts of a claim which are answered by executing appropriate functions on tables. In a few-shot setting on FEVEROUS, we achieve an accuracy of 71.4, outperforming both fully neural and symbolic reasoning models by 3.4 points. When evaluated on TabFact without any further training, our method remains competitive with an accuracy lead of 0.5 points.</li>
<li><strong>摘要：</strong>表格证据的事实验证鼓励使用符号推理模型，其中构建了逻辑形式（例如 LISP 样式的程序），提供比完全神经方法更高的可验证性。但是，这些系统通常依赖于格式良好的表格，这限制了它们在许多场景中的使用。一种新兴的文本证据符号推理范式侧重于自然逻辑推理，它通过用自然语言建模主张与其证据之间的集合论关系来构建证明。这种方法提供了灵活性和透明度，但与表格证据的兼容性较差，因为关系不扩展到算术函数。我们提出了一种在自然逻辑背景下对数字和算术函数的集合论解释，从而能够将算术表达式集成到确定性证明中。我们利用大型语言模型来生成算术表达式，方法是生成有关主张突出部分的问题，并通过在表上执行适当的函数来回答这些问题。在 FEVEROUS 的几次测试中，我们的准确率达到了 71.4，比完全神经推理模型和符号推理模型高出 3.4 个百分点。在未经进一步训练的情况下在 TabFact 上进行评估时，我们的方法仍然具有竞争力，准确率领先 0.5 个百分点。</li>
</ul>

<h3>Title: How Effective Is Self-Consistency for Long-Context Problems?</h3>
<ul>
<li><strong>Authors: </strong>Adam Byerly, Daniel Khashabi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01101">https://arxiv.org/abs/2411.01101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01101">https://arxiv.org/pdf/2411.01101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01101]] How Effective Is Self-Consistency for Long-Context Problems?(https://arxiv.org/abs/2411.01101)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Self-consistency (SC) has been demonstrated to enhance the performance of large language models (LLMs) across various tasks and domains involving short content. However, does this evidence support its effectiveness for long-context problems? This study examines the role of SC in long-context scenarios, where LLMs often struggle with position bias, hindering their ability to utilize information effectively from all parts of their long input context. We examine a range of design parameters, including different models, context lengths, prompt formats, and types of datasets and tasks. Our findings demonstrate that SC, while effective for short-context problems, fundamentally fails for long-context tasks -- not only does it fail to mitigate position bias, but it can also actively degrade performance. We observe that the effectiveness of SC varies with context length and model size but remains mainly unaffected by prompt format or task type. These results provide valuable insight into the limitations of current LLMs in long-context understanding and highlight the need for more sophisticated approaches to address position bias in these models.</li>
<li><strong>摘要：</strong>已经证明，自洽性 (SC) 可以提高大型语言模型 (LLM) 在涉及短内容的各种任务和领域的性能。但是，这些证据是否支持其对长上下文问题的有效性？本研究考察了 SC 在长上下文场景中的作用，其中 LLM 经常与位置偏差作斗争，阻碍了它们有效利用长输入上下文所有部分的信息的能力。我们研究了一系列设计参数，包括不同的模型、上下文长度、提示格式以及数据集和任务的类型。我们的研究结果表明，虽然 SC 对短上下文问题有效，但从根本上来说，它对长上下文任务不起作用——它不仅无法减轻位置偏差，而且还会主动降低性能。我们观察到 SC 的有效性随上下文长度和模型大小而变化，但主要不受提示格式或任务类型的影响。这些结果为当前 LLM 在长上下文理解方面的局限性提供了有价值的见解，并强调需要更复杂的方法来解决这些模型中的位置偏差。</li>
</ul>

<h3>Title: Do LLMs Know to Respect Copyright Notice?</h3>
<ul>
<li><strong>Authors: </strong>Jialiang Xu, Shenglan Li, Zhaozhuo Xu, Denghui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01136">https://arxiv.org/abs/2411.01136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01136">https://arxiv.org/pdf/2411.01136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01136]] Do LLMs Know to Respect Copyright Notice?(https://arxiv.org/abs/2411.01136)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prior study shows that LLMs sometimes generate content that violates copyright. In this paper, we study another important yet underexplored problem, i.e., will LLMs respect copyright information in user input, and behave accordingly? The research problem is critical, as a negative answer would imply that LLMs will become the primary facilitator and accelerator of copyright infringement behavior. We conducted a series of experiments using a diverse set of language models, user prompts, and copyrighted materials, including books, news articles, API documentation, and movie scripts. Our study offers a conservative evaluation of the extent to which language models may infringe upon copyrights when processing user input containing protected material. This research emphasizes the need for further investigation and the importance of ensuring LLMs respect copyright regulations when handling user input to prevent unauthorized use or reproduction of protected content. We also release a benchmark dataset serving as a test bed for evaluating infringement behaviors by LLMs and stress the need for future alignment.</li>
<li><strong>摘要：</strong>先前的研究表明，LLM 有时会生成侵犯版权的内容。在本文中，我们研究了另一个重要但尚未得到充分探索的问题，即 LLM 是否会尊重用户输入中的版权信息并采取相应的行动？这个研究问题至关重要，因为否定的答案意味着 LLM 将成为版权侵权行为的主要推动者和加速者。我们使用多种语言模型、用户提示和受版权保护的材料（包括书籍、新闻文章、API 文档和电影脚本）进行了一系列实验。我们的研究对语言模型在处理包含受保护材料的用户输入时可能侵犯版权的程度进行了保守的评估。这项研究强调了进一步调查的必要性，以及确保 LLM 在处理用户输入时尊重版权法规的重要性，以防止未经授权使用或复制受保护的内容。我们还发布了一个基准数据集，作为评估 LLM 侵权行为的试验台，并强调了未来协调的必要性。</li>
</ul>

<h3>Title: Dictionary Insertion Prompting for Multilingual Reasoning on Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyuan Lu, Zixuan Li, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01141">https://arxiv.org/abs/2411.01141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01141">https://arxiv.org/pdf/2411.01141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01141]] Dictionary Insertion Prompting for Multilingual Reasoning on Multilingual Large Language Models(https://arxiv.org/abs/2411.01141)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As current training data for Large Language Models (LLMs) are dominated by English corpus, they are English-centric and they present impressive performance on English reasoning tasks.\footnote{This paper primarily studies English-centric models, but our method could be universal by using the centric language in the dictionary for non-English-centric LLMs.} Yet, they usually suffer from lower performance in other languages. There are about 7,000 languages over the world, and many are low-resourced on English-centric LLMs. For the sake of people who primarily speak these languages, it is especially urgent to enable our LLMs in those languages. Model training is usually effective, but computationally expensive and requires experienced NLP practitioners. This paper presents a novel and simple yet effective method called \textbf{D}ictionary \textbf{I}nsertion \textbf{P}rompting (\textbf{DIP}). When providing a non-English prompt, DIP looks up a word dictionary and inserts words' English counterparts into the prompt for LLMs. It then enables better translation into English and better English model thinking steps which leads to obviously better results. We experiment with about 200 languages from FLORES-200. Since there are no adequate datasets, we use the NLLB translator to create synthetic multilingual benchmarks from the existing 4 English reasoning benchmarks such as GSM8K and AQuA. Despite the simplicity and computationally lightweight, we surprisingly found the effectiveness of DIP on math and commonsense reasoning tasks on multiple open-source and close-source LLMs.\footnote{Our dictionaries, code, and synthetic benchmarks will be open-sourced to facilitate future research.}</li>
<li><strong>摘要：</strong>目前大型语言模型 (LLM) 的训练数据以英语语料库为主，因此它们以英语为中心，在英语推理任务上表现出色。\footnote{本文主要研究以英语为中心的模型，但我们的方法可以通过使用词典中的中心语言来实现非以英语为中心的 LLM 的通用性。}然而，它们在其他语言中的表现通常较低。全世界大约有 7,000 种语言，许多语言在以英语为中心的 LLM 上的资源很少。为了主要讲这些语言的人的利益，让我们的 LLM 能够支持这些语言尤为紧迫。模型训练通常很有效，但计算成本高昂，并且需要经验丰富的 NLP 从业者。本文提出了一种新颖、简单而有效的方法，称为 \textbf{D}ictionary \textbf{I}nsertion \textbf{P}rompting (\textbf{DIP})。当提供非英语提示时，DIP 会查找词典并将单词的英语对应词插入 LLM 提示中。然后，它可以更好地翻译成英语，并更好地建立英语模型思维步骤，从而产生明显更好的结果。我们用 FLORES-200 中的大约 200 种语言进行了实验。由于没有足够的数据集，我们使用 NLLB 翻译器从现有的 4 个英语推理基准（如 GSM8K 和 AQuA）创建合成多语言基准。尽管简单且计算量小，但我们意外地发现 DIP 在多个开源和闭源 LLM 上的数学和常识推理任务上都很有效。\footnote{我们的词典、代码和合成基准将开源，以方便未来的研究。}</li>
</ul>

<h3>Title: CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research</h3>
<ul>
<li><strong>Authors: </strong>Sian-Yao Huang, Cheng-Lin Yang, Che-Yu Lin, Chun-Ying Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01176">https://arxiv.org/abs/2411.01176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01176">https://arxiv.org/pdf/2411.01176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01176]] CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research(https://arxiv.org/abs/2411.01176)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This research addresses command-line embedding in cybersecurity, a field obstructed by the lack of comprehensive datasets due to privacy and regulation concerns. We propose the first dataset of similar command lines, named CyPHER, for training and unbiased evaluation. The training set is generated using a set of large language models (LLMs) comprising 28,520 similar command-line pairs. Our testing dataset consists of 2,807 similar command-line pairs sourced from authentic command-line data. In addition, we propose a command-line embedding model named CmdCaliper, enabling the computation of semantic similarity with command lines. Performance evaluations demonstrate that the smallest version of CmdCaliper (30 million parameters) suppresses state-of-the-art (SOTA) sentence embedding models with ten times more parameters across various tasks (e.g., malicious command-line detection and similar command-line retrieval). Our study explores the feasibility of data generation using LLMs in the cybersecurity domain. Furthermore, we release our proposed command-line dataset, embedding models' weights and all program codes to the public. This advancement paves the way for more effective command-line embedding for future researchers.</li>
<li><strong>摘要：</strong>本研究针对网络安全中的命令行嵌入问题，由于隐私和监管问题，该领域因缺乏全面的数据集而受到阻碍。我们提出了第一个类似命令行数据集，名为 CyPHER，用于训练和无偏评估。训练集是使用一组包含 28,520 个类似命令行对的大型语言模型 (LLM) 生成的。我们的测试数据集由来自真实命令行数据的 2,807 个类似命令行对组成。此外，我们提出了一个名为 CmdCaliper 的命令行嵌入模型，可以计算与命令行的语义相似度。性能评估表明，最小版本的 CmdCaliper（3000 万个参数）在各种任务（例如恶意命令行检测和类似命令行检索）中抑制了具有十倍参数的最先进的 (SOTA) 句子嵌入模型。我们的研究探索了在网络安全领域使用 LLM 生成数据的可行性。此外，我们向公众发布了我们提出的命令行数据集、嵌入模型的权重和所有程序代码。这一进步为未来研究人员更有效的命令行嵌入铺平了道路。</li>
</ul>

<h3>Title: Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Gagan Bhatia, El Moatez Billah Nagoudi, Abdellah El Mekki, Fakhraddin Alwajih, Muhammad Abdul-Mageed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01192">https://arxiv.org/abs/2411.01192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01192">https://arxiv.org/pdf/2411.01192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01192]] Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks(https://arxiv.org/abs/2411.01192)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.</li>
<li><strong>摘要：</strong>我们推出了 Swan，这是一系列以阿拉伯语为中心的嵌入模型，可解决小规模和大规模用例。Swan 包括两个变体：基于 ARBERTv2 的 Swan-Small 和基于预训练的阿拉伯语大型语言模型 ArMistral 构建的 Swan-Large。为了评估这些模型，我们提出了 ArabiaMTEB，这是一个全面的基准套件，可评估跨语言、多方言、多领域和多文化的阿拉伯语文本嵌入性能，涵盖八个不同的任务和 94 个数据集。Swan-Large 取得了最先进的结果，在大多数阿拉伯语任务中的表现优于 Multilingual-E5-large，而 Swan-Small 始终超越 Multilingual-E5 基础。我们广泛的评估表明，Swan 模型既具有方言意识，又具有文化意识，在各种阿拉伯语领域表现出色，同时提供了显着的经济效率。这项工作极大地推动了阿拉伯语建模领域的发展，并为阿拉伯语自然语言处理的未来研究和应用提供了宝贵的资源。我们的模型和基准将向公众开放，以供研究。</li>
</ul>

<h3>Title: Transfer Learning for Finetuning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tobias Strangmann, Lennart Purucker, Jörg K.H. Franke, Ivo Rapant, Fabio Ferreira, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01195">https://arxiv.org/abs/2411.01195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01195">https://arxiv.org/pdf/2411.01195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01195]] Transfer Learning for Finetuning Large Language Models(https://arxiv.org/abs/2411.01195)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As the landscape of large language models expands, efficiently finetuning for specific tasks becomes increasingly crucial. At the same time, the landscape of parameter-efficient finetuning methods rapidly expands. Consequently, practitioners face a multitude of complex choices when searching for an optimal finetuning pipeline for large language models. To reduce the complexity for practitioners, we investigate transfer learning for finetuning large language models and aim to transfer knowledge about configurations from related finetuning tasks to a new task. In this work, we transfer learn finetuning by meta-learning performance and cost surrogate models for grey-box meta-optimization from a new meta-dataset. Counter-intuitively, we propose to rely only on transfer learning for new datasets. Thus, we do not use task-specific Bayesian optimization but prioritize knowledge transferred from related tasks over task-specific feedback. We evaluate our method on eight synthetic question-answer datasets and a meta-dataset consisting of 1,800 runs of finetuning Microsoft's Phi-3. Our transfer learning is superior to zero-shot, default finetuning, and meta-optimization baselines. Our results demonstrate the transferability of finetuning to adapt large language models more effectively.</li>
<li><strong>摘要：</strong>随着大型语言模型领域的扩展，针对特定任务进行高效微调变得越来越重要。与此同时，参数高效微调方法的领域迅速扩展。因此，从业者在为大型语言模型寻找最佳微调流程时面临着众多复杂的选择。为了降低从业者的复杂性，我们研究了用于微调大型语言模型的迁移学习，旨在将有关配置的知识从相关的微调任务转移到新任务。在这项工作中，我们通过从新的元数据集进行灰盒元优化的元学习性能和成本替代模型来迁移学习微调。与直觉相反，我们建议仅依靠新数据集的迁移学习。因此，我们不使用特定于任务的贝叶斯优化，而是优先考虑从相关任务转移的知识，而不是特定于任务的反馈。我们在八个合成问答数据集和一个由 1,800 次微调 Microsoft Phi-3 运行组成的元数据集上评估了我们的方法。我们的迁移学习优于零样本、默认微调和元优化基线。我们的结果证明了微调的可迁移性，可以更有效地适应大型语言模型。</li>
</ul>

<h3>Title: PRIMO: Progressive Induction for Multi-hop Open Rule Generation</h3>
<ul>
<li><strong>Authors: </strong>Jianyu Liu, Sheng Bi, Guilin Qi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01205">https://arxiv.org/abs/2411.01205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01205">https://arxiv.org/pdf/2411.01205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01205]] PRIMO: Progressive Induction for Multi-hop Open Rule Generation(https://arxiv.org/abs/2411.01205)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Open rule refer to the implication from premise atoms to hypothesis atoms, which captures various relations between instances in the real world. Injecting open rule knowledge into the machine helps to improve the performance of downstream tasks such as dialogue and relation extraction. Existing approaches focus on single-hop open rule generation, ignoring multi-hop scenarios, leading to logical inconsistencies between premise and hypothesis atoms, as well as semantic duplication of generated rule atoms. To address these issues, we propose a progressive multi-stage open rule generation method called PRIMO. We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure consisting of generation, extraction, and ranking modules to fully leverage the latent knowledge within the language model across multiple dimensions. Furthermore, we employ reinforcement learning from human feedback to further optimize model, enhancing the model's understanding of commonsense knowledge. Experiments show that compared to baseline models, PRIMO significantly improves rule quality and diversity while reducing the repetition rate of rule atoms.</li>
<li><strong>摘要：</strong>开放规则是指从前提原子到假设原子的蕴涵，它捕捉了现实世界中实例之间的各种关系。将开放规则知识注入机器有助于提升对话、关系抽取等下游任务的性能。现有方法侧重于单跳开放规则生成，忽略了多跳场景，导致前提原子和假设原子之间逻辑不一致，生成的规则原子语义重复。针对这些问题，我们提出了一种渐进式多阶段开放规则生成方法PRIMO。我们在规则生成阶段引入本体信息，减少歧义，提高规则准确率。PRIMO构建了由生成、提取和排序模块组成的多阶段结构，充分利用了语言模型中跨维度的潜在知识。此外，我们利用从人类反馈中进行的强化学习进一步优化模型，增强了模型对常识性知识的理解。实验表明，与基线模型相比，PRIMO在降低规则原子重复率的同时，显著提高了规则质量和多样性。</li>
</ul>

<h3>Title: One Arrow, Many Targets: Probing LLMs for Multi-Attribute Controllable Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Tathagato Roy, Rahul Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01213">https://arxiv.org/abs/2411.01213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01213">https://arxiv.org/pdf/2411.01213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01213]] One Arrow, Many Targets: Probing LLMs for Multi-Attribute Controllable Text Summarization(https://arxiv.org/abs/2411.01213)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Text summarization is a well-established task within the natural language processing (NLP) community. However, the focus on controllable summarization tailored to user requirements is gaining traction only recently. While several efforts explore controllability in text summarization, the investigation of Multi-Attribute Controllable Summarization (MACS) remains limited. This work addresses this gap by examining the MACS task through the lens of large language models (LLMs), using various learning paradigms, particularly low-rank adapters. We experiment with different popular adapter fine-tuning strategies to assess the effectiveness of the resulting models in retaining cues and patterns associated with multiple controllable attributes. Additionally, we propose and evaluate a novel hierarchical adapter fusion technique to integrate learnings from two distinct controllable attributes. Subsquently, we present our findings, discuss the challenges encountered, and suggest potential avenues for advancing the MACS task.</li>
<li><strong>摘要：</strong>文本摘要是自然语言处理 (NLP) 社区中一项成熟的任务。然而，针对用户需求的可控摘要最近才开始受到关注。虽然有多项研究探索了文本摘要的可控性，但对多属性可控摘要 (MACS) 的研究仍然有限。这项工作通过从大型语言模型 (LLM) 的视角研究 MACS 任务来解决这一问题，使用各种学习范式，特别是低秩适配器。我们尝试了不同的流行适配器微调策略，以评估生成的模型在保留与多个可控属性相关的线索和模式方面的有效性。此外，我们提出并评估了一种新颖的分层适配器融合技术，以整合来自两个不同可控属性的学习。随后，我们介绍了我们的发现，讨论了遇到的挑战，并提出了推进 MACS 任务的潜在途径。</li>
</ul>

<h3>Title: $B^4$: A Black-Box Scrubbing Attack on LLM Watermarks</h3>
<ul>
<li><strong>Authors: </strong>Baizhou Huang, Xiao Pu, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01222">https://arxiv.org/abs/2411.01222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01222">https://arxiv.org/pdf/2411.01222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01222]] $B^4$: A Black-Box Scrubbing Attack on LLM Watermarks(https://arxiv.org/abs/2411.01222)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Watermarking has emerged as a prominent technique for LLM-generated content detection by embedding imperceptible patterns. Despite supreme performance, its robustness against adversarial attacks remains underexplored. Previous work typically considers a grey-box attack setting, where the specific type of watermark is already known. Some even necessitates knowledge about hyperparameters of the watermarking method. Such prerequisites are unattainable in real-world scenarios. Targeting at a more realistic black-box threat model with fewer assumptions, we here propose $\mathcal{B}^4$, a black-box scrubbing attack on watermarks. Specifically, we formulate the watermark scrubbing attack as a constrained optimization problem by capturing its objectives with two distributions, a Watermark Distribution and a Fidelity Distribution. This optimization problem can be approximately solved using two proxy distributions. Experimental results across 12 different settings demonstrate the superior performance of $\mathcal{B}^4$ compared with other baselines.</li>
<li><strong>摘要：</strong>通过嵌入不可察觉的模式，水印已成为 LLM 生成内容检测的一种重要技术。尽管性能卓越，但它对对抗性攻击的鲁棒性仍未得到充分探索。以前的工作通常考虑灰盒攻击设置，其中水印的具体类型是已知的。有些甚至需要了解水印方法的超参数。这样的先决条件在现实世界中是无法实现的。针对具有更少假设的更现实的黑盒威胁模型，我们在此提出了 $\mathcal{B}^4$，一种针对水印的黑盒擦除攻击。具体而言，我们将水印擦除攻击表述为一个约束优化问题，通过使用两个分布（水印分布和保真度分布）来捕获其目标。可以使用两个代理分布近似地解决这个优化问题。在 12 种不同设置中的实验结果表明，与其他基线相比，$\mathcal{B}^4$ 具有更优越的性能。</li>
</ul>

<h3>Title: PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Dongxu Liu, Bing Xu, Yinzhuo Chen, Bufan Xu, Wenpeng Lu, Muyun Yang, Tiejun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01245">https://arxiv.org/abs/2411.01245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01245">https://arxiv.org/pdf/2411.01245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01245]] PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment(https://arxiv.org/abs/2411.01245)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has been proven to be an effective method for preference alignment of large language models (LLMs) and is widely used in the post-training process of LLMs. However, RLHF struggles with handling multiple competing preferences. This leads to a decrease in the alignment of LLMs with human preferences. To address this issue, we propose Preference Mixture of LoRAs (PMoL) from the perspective of model architecture, which can adapt to any number of preferences to mix. PMoL combines Mixture of Experts (MoE) and Low Rank Adaptor (LoRA). This architecture is innovatively applied to the research of preference alignment and has achieved significant performance improvement. The expert group soft loss is used to enable MoE with the ability to mix preferences. Through comprehensive evaluation by the reward model and GPT-4o, the experiment results show that PMoL has superior preference mixing capabilities compared to baseline methods. PMoL achieves better preference alignment with lower training costs.</li>
<li><strong>摘要：</strong>人类反馈强化学习（RLHF）已被证明是一种有效的大型语言模型（LLM）偏好对齐方法，并广泛应用于LLM的后训练过程。然而，RLHF在处理多个相互竞争的偏好时会遇到困难。这导致LLM与人类偏好的一致性降低。为了解决这个问题，我们从模型架构的角度提出了LoRA偏好混合（PMoL），它可以适应任意数量的偏好混合。PMoL结合了专家混合（MoE）和低秩适配器（LoRA），该架构创新性地应用于偏好对齐的研究，并取得了显著的性能提升。专家组软损失用于使MoE具有混合偏好的能力。通过奖励模型和GPT-4o的综合评估，实验结果表明PMoL与基线方法相比具有更优的偏好混合能力。PMoL以更低的训练成本实现了更好的偏好对齐。</li>
</ul>

<h3>Title: Diversidade lingu\'istica e inclus\~ao digital: desafios para uma ia brasileira</h3>
<ul>
<li><strong>Authors: </strong>Raquel Meister Ko Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01259">https://arxiv.org/abs/2411.01259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01259">https://arxiv.org/pdf/2411.01259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01259]] Diversidade lingu\'istica e inclus\~ao digital: desafios para uma ia brasileira(https://arxiv.org/abs/2411.01259)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Linguistic diversity is a human attribute which, with the advance of generative AIs, is coming under threat. This paper, based on the contributions of sociolinguistics, examines the consequences of the variety selection bias imposed by technological applications and the vicious circle of preserving a variety that becomes dominant and standardized because it has linguistic documentation to feed the large language models for machine learning.</li>
<li><strong>摘要：</strong>语言多样性是人类的一种属性，但随着生成式人工智能的发展，这种属性正受到威胁。本文基于社会语言学的贡献，探讨了技术应用造成的多样性选择偏差的后果，以及由于拥有语言文献，可以为机器学习的大型语言模型提供信息，从而保留某一成为主导和标准化的多样性的恶性循环。</li>
</ul>

<h3>Title: Varco Arena: A Tournament Approach to Reference-Free Benchmarking Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seonil Son, Ju-Min Oh, Heegon Jin, Cheolhun Jang, Jeongbeom Jeong, Kuntae Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01281">https://arxiv.org/abs/2411.01281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01281">https://arxiv.org/pdf/2411.01281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01281]] Varco Arena: A Tournament Approach to Reference-Free Benchmarking Large Language Models(https://arxiv.org/abs/2411.01281)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) necessitates robust evaluation methodologies. Current benchmarking approaches often rely on comparing model outputs against predefined prompts and reference outputs. Relying on predefined reference outputs hinders flexible adaptation of benchmarks to the rapidly evolving capabilities of LLMs. This limitation necessitates periodic efforts to prepare new benchmarks. To keep pace with rapidly evolving LLM capabilities, we propose a more flexible benchmarking approach. Our method, \textit{\textbf{Varco Arena}}, provides reference-free benchmarking of LLMs in tournament style. \textit{\textbf{Varco Arena}} directly compares LLM outputs across a diverse set of prompts, determining model rankings through a single-elimination tournament structure. This direct pairwise comparison offers two key advantages: (1) Direct comparison, unmediated by reference text, more effectively orders competing LLMs, resulting in more reliable rankings, and (2) reference-free approach to benchmarking adds flexibility in updating benchmark prompts by eliminating the need for quality references. Our empirical results, supported by simulation experiments, demonstrate that the \textit{\textbf{Varco Arena}} tournament approach aligns better with the current Elo model for benchmarking LLMs. The alignment is measured in terms of Spearman correlation, showing improvement over current practice of benchmarking that use reference outputs as comparison \textit{anchor}s.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展需要强大的评估方法。当前的基准测试方法通常依赖于将模型输出与预定义提示和参考输出进行比较。依赖预定义的参考输出会阻碍基准测试灵活地适应 LLM 快速发展的功能。这种限制需要定期努力准备新的基准测试。为了跟上快速发展的 LLM 功能，我们提出了一种更灵活的基准测试方法。我们的方法 \textit{\textbf{Varco Arena}} 以锦标赛风格提供 LLM 的无参考基准测试。 \textit{\textbf{Varco Arena}} 直接比较不同提示中的 LLM 输出，通过单淘汰锦标赛结构确定模型排名。这种直接的成对比较提供了两个主要优势：(1) 直接比较，不通过参考文本进行中介，可以更有效地对竞争的 LLM 进行排序，从而获得更可靠的排名，以及 (2) 无参考基准测试方法通过消除对质量参考的需求，增加了更新基准提示的灵活性。我们的实证结果（由模拟实验支持）表明，\textit{\textbf{Varco Arena}} 锦标赛方法与当前用于对 LLM 进行基准测试的 Elo 模型更加一致。这种一致性以 Spearman 相关性来衡量，与使用参考输出作为比较 \textit{anchor} 的当前基准测试实践相比有所改进。</li>
</ul>

<h3>Title: Can Multimodal Large Language Model Think Analogically?</h3>
<ul>
<li><strong>Authors: </strong>Diandian Guo, Cong Cao, Fangfang Yuan, Dakui Wang, Wei Ma, Yanbing Liu, Jianhui Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01307">https://arxiv.org/abs/2411.01307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01307">https://arxiv.org/pdf/2411.01307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01307]] Can Multimodal Large Language Model Think Analogically?(https://arxiv.org/abs/2411.01307)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Analogical reasoning, particularly in multimodal contexts, is the foundation of human perception and creativity. Multimodal Large Language Model (MLLM) has recently sparked considerable discussion due to its emergent capabilities. In this paper, we delve into the multimodal analogical reasoning capability of MLLM. Specifically, we explore two facets: \textit{MLLM as an explainer} and \textit{MLLM as a predictor}. In \textit{MLLM as an explainer}, we primarily focus on whether MLLM can deeply comprehend multimodal analogical reasoning problems. We propose a unified prompt template and a method for harnessing the comprehension capabilities of MLLM to augment existing models. In \textit{MLLM as a predictor}, we aim to determine whether MLLM can directly solve multimodal analogical reasoning problems. The experiments show that our approach outperforms existing methods on popular datasets, providing preliminary evidence for the analogical reasoning capability of MLLM.</li>
<li><strong>摘要：</strong>类比推理，特别是在多模态语境中，是人类感知和创造力的基础。多模态大型语言模型 (MLLM) 最近因其新兴能力而引发了广泛讨论。在本文中，我们深入研究了 MLLM 的多模态类比推理能力。具体来说，我们探索了两个方面：\textit{MLLM 作为解释器} 和 \textit{MLLM 作为预测器}。在 \textit{MLLM 作为解释器} 中，我们主要关注 MLLM 是否能够深入理解多模态类比推理问题。我们提出了一个统一的提示模板和一种利用 MLLM 的理解能力来增强现有模型的方法。在 \textit{MLLM 作为预测器} 中，我们旨在确定 MLLM 是否可以直接解决多模态类比推理问题。实验表明，我们的方法在流行数据集上的表现优于现有方法，为 MLLM 的类比推理能力提供了初步证据。</li>
</ul>

<h3>Title: AMREx: AMR for Explainable Fact Verification</h3>
<ul>
<li><strong>Authors: </strong>Chathuri Jayaweera, Sangpil Youm, Bonnie Dorr</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01343">https://arxiv.org/abs/2411.01343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01343">https://arxiv.org/pdf/2411.01343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01343]] AMREx: AMR for Explainable Fact Verification(https://arxiv.org/abs/2411.01343)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>With the advent of social media networks and the vast amount of information circulating through them, automatic fact verification is an essential component to prevent the spread of misinformation. It is even more useful to have fact verification systems that provide explanations along with their classifications to ensure accurate predictions. To address both of these requirements, we implement AMREx, an Abstract Meaning Representation (AMR)-based veracity prediction and explanation system for fact verification using a combination of Smatch, an AMR evaluation metric to measure meaning containment and textual similarity, and demonstrate its effectiveness in producing partially explainable justifications using two community standard fact verification datasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy showing the effectiveness of our approach for real-world claim verification. It follows an interpretable pipeline and returns an explainable AMR node mapping to clarify the system's veracity predictions when applicable. We further demonstrate that AMREx output can be used to prompt LLMs to generate natural-language explanations using the AMR mappings as a guide to lessen the probability of hallucinations.</li>
<li><strong>摘要：</strong>随着社交媒体网络的出现以及大量信息在其中传播，自动事实验证是防止错误信息传播的重要组成部分。拥有提供解释和分类的事实验证系统更为有用，以确保准确的预测。为了满足这两个要求，我们实施了 AMREx，这是一种基于抽象意义表示 (AMR) 的真实性预​​测和解释系统，用于事实验证，结合使用 Smatch（一种用于衡量意义包含和文本相似性的 AMR 评估指标），并使用两个社区标准事实验证数据集 FEVER 和 AVeriTeC 证明了其在产生部分可解释的理由方面的有效性。AMREx 超越了 AVeriTec 基线准确度，表明我们的方法对现实世界的声明验证是有效的。它遵循可解释的管道并返回可解释的 AMR 节点映射，以在适用时阐明系统的真实性预​​测。我们进一步证明，AMREx 输出可用于提示 LLM 使用 AMR 映射作为指导来生成自然语言解释，以减少出现幻觉的可能性。</li>
</ul>

<h3>Title: Artificial Intelligence Driven Course Generation: A Case Study Using ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Djaber Rouabhia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01369">https://arxiv.org/abs/2411.01369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01369">https://arxiv.org/pdf/2411.01369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01369]] Artificial Intelligence Driven Course Generation: A Case Study Using ChatGPT(https://arxiv.org/abs/2411.01369)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>This study explores Artificial Intelligence use, specifically ChatGPT, in creating educational content. The study aims to elaborate on using ChatGPT to create course materials. The main objective is to assess the efficiency, quality, and impact of AI-driven course generation, and to create a Multimedia Databases course as a case study. The study highlights the potential of AI to revolutionize educational content creation, making it more accessible, personalized, and efficient. The course content was generated in less than one day through iterative methods, using prompts for translation, content expansion, practical examples, assignments, supplementary materials, and LaTeX formatting. Each part was verified immediately after generation to ensure accuracy. Post-generation analysis with Detectia and Turnitin showed similarity rates of 8.7% and 13%, indicating high originality. Experts and university committees reviewed and approved the course, with English university teachers praising its language quality. ChatGPT also created a well-structured and diversified exam for the module. Key findings reveal significant time efficiency, comprehensive content coverage, and high flexibility. The study underscores AI's transformative potential in education, addressing challenges related to data privacy, technology dependence, content accuracy, and algorithmic biases. The conclusions emphasize the need for collaboration between educators, policymakers, and technology developers to harness AI's benefits in education fully.</li>
<li><strong>摘要：</strong>本研究探讨了人工智能（特别是 ChatGPT）在创建教育内容中的应用。该研究旨在详细说明如何使用 ChatGPT 创建课程材料。主要目标是评估人工智能驱动的课程生成的效率、质量和影响，并创建多媒体数据库课程作为案例研究。该研究强调了人工智能彻底改变教育内容创建的潜力，使其更易于访问、个性化和高效。课程内容通过迭代方法在不到一天的时间内生成，使用翻译提示、内容扩展、实际示例、作业、补充材料和 LaTeX 格式。每个部分在生成后立即进行验证以确保准确性。使用 Detectia 和 Turnitin 进行生成后分析显示相似率分别为 8.7% 和 13%，表明原创性很高。专家和大学委员会审查并批准了该课程，英语大学教师对其语言质量表示赞赏。ChatGPT 还为该模块创建了一个结构良好且多样化的考试。主要发现表明时间效率高、内容覆盖面广、灵活性高。该研究强调了人工智能在教育领域的变革潜力，解决了与数据隐私、技术依赖性、内容准确性和算法偏差相关的挑战。结论强调，教育工作者、政策制定者和技术开发人员需要合作，以充分利用人工智能在教育领域的优势。</li>
</ul>

<h3>Title: Teaching Models to Improve on Tape</h3>
<ul>
<li><strong>Authors: </strong>Liat Bezalel, Eyal Orgad, Amir Globerson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01483">https://arxiv.org/abs/2411.01483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01483">https://arxiv.org/pdf/2411.01483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01483]] Teaching Models to Improve on Tape(https://arxiv.org/abs/2411.01483)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often struggle when prompted to generate content under specific constraints. However, in such cases it is often easy to check whether these constraints are satisfied or violated. Recent works have shown that LLMs can benefit from such ``corrective feedback''. Here we claim that this skill of LLMs can be significantly enhanced via training. We introduce an RL framework for teaching models to use such rewards, by simulating interaction sessions, and rewarding the model according to its ability to satisfy the constraints. We refer to our method as CORGI (Controlled Generation with RL for Guided Interaction), and evaluate it on a variety of controlled generation tasks using unlabeled training data. We find that CORGI consistently outperforms the baseline reinforcement learning method that does not incorporate conversational feedback. Furthermore, CORGI's interactive framework enables meta-learning, allowing the LLM to generalize better to guided interaction in new tasks. Our results clearly show that conversational optimization, when combined with reinforcement learning, significantly improves the effectiveness of LLMs in controlled generation contexts.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在特定约束下生成内容时通常会遇到困难。但是，在这种情况下，通常很容易检查这些约束是否得到满足或违反。最近的研究表明，LLM 可以从这种“纠正反馈”中受益。我们在此声称，LLM 的这种技能可以通过训练得到显著增强。我们引入了一个 RL 框架，用于通过模拟交互会话并根据模型满足约束的能力奖励模型来教模型使用此类奖励。我们将我们的方法称为 CORGI（使用 RL 进行引导交互的受控生成），并使用未标记的训练数据在各种受控生成任务上对其进行评估。我们发现 CORGI 的表现始终优于不包含对话反馈的基线强化学习方法。此外，CORGI 的交互式框架支持元学习，使 LLM 能够更好地推广到新任务中的引导交互。我们的结果清楚地表明，对话优化与强化学习相结合，可显著提高 LLM 在受控生成环境中的有效性。</li>
</ul>

<h3>Title: DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in Endangered Uralic Languages using ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Mika Hämäläinen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01531">https://arxiv.org/abs/2411.01531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01531">https://arxiv.org/pdf/2411.01531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01531]] DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in Endangered Uralic Languages using ChatGPT(https://arxiv.org/abs/2411.01531)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>We showcase that ChatGPT can be used to disambiguate lemmas in two endangered languages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment our prompt by providing dictionary translations of the candidate lemmas to a majority language - Finnish in our case. This dictionary augmented generation approach results in 50\% accuracy for Skolt Sami and 41\% accuracy for Erzya. On a closer inspection, many of the error types were of the kind even an untrained human annotator would make.</li>
<li><strong>摘要：</strong>我们展示了 ChatGPT 可用于消除两种濒危语言（即 Erzya 和 Skolt Sami）的词条歧义，而 ChatGPT 并不精通这两种语言。我们通过提供候选词条的词典翻译来增强我们的提示，这些词条翻译成主流语言（在我们的例子中是芬兰语）。这种词典增强生成方法的 Skolt Sami 准确率为 50%，Erzya 准确率为 41%。仔细检查后发现，许多错误类型即使是未经训练的人类注释者也会犯。</li>
</ul>

<h3>Title: Enhancing LLM Evaluations: The Garbling Trick</h3>
<ul>
<li><strong>Authors: </strong>William F. Bradley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01533">https://arxiv.org/abs/2411.01533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01533">https://arxiv.org/pdf/2411.01533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01533]] Enhancing LLM Evaluations: The Garbling Trick(https://arxiv.org/abs/2411.01533)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly powerful, traditional evaluation metrics tend to saturate, making it challenging to distinguish between models based on their performance. We propose a general method to transform existing LLM evaluations into a series of progressively more difficult tasks. These enhanced evaluations emphasize reasoning capabilities and can reveal relative performance differences that are not apparent in the original assessments. To demonstrate the effectiveness of our approach, we create a new multiple-choice test corpus, extend it into a family of evaluations, and assess a collection of LLMs. Our results offer insights into the comparative reasoning abilities of these models, particularly highlighting distinctions between OpenAI's o1-preview and Google's gemini-pro-1.5-002.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 变得越来越强大，传统的评估指标趋于饱和，这使得根据模型的性能区分模型变得具有挑战性。我们提出了一种通用方法，将现有的 LLM 评估转变为一系列逐渐变得更加困难的任务。这些增强的评估强调推理能力，并且可以揭示原始评估中不明显的相对性能差异。为了证明我们方法的有效性，我们创建了一个新的多项选择测试语料库，将其扩展为一系列评估，并评估了一组 LLM。我们的结果提供了对这些模型的比较推理能力的见解，特别是突出了 OpenAI 的 o1-preview 和 Google 的 gemini-pro-1.5-002 之间的区别。</li>
</ul>

<h3>Title: LLMs and the Madness of Crowds</h3>
<ul>
<li><strong>Authors: </strong>William F. Bradley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01539">https://arxiv.org/abs/2411.01539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01539">https://arxiv.org/pdf/2411.01539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01539]] LLMs and the Madness of Crowds(https://arxiv.org/abs/2411.01539)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We investigate the patterns of incorrect answers produced by large language models (LLMs) during evaluation. These errors exhibit highly non-intuitive behaviors unique to each model. By analyzing these patterns, we measure the similarities between LLMs and construct a taxonomy that categorizes them based on their error correlations. Our findings reveal that the incorrect responses are not randomly distributed but systematically correlated across models, providing new insights into the underlying structures and relationships among LLMs.</li>
<li><strong>摘要：</strong>我们研究了大型语言模型 (LLM) 在评估过程中产生的错误答案的模式。这些错误表现出每个模型独有的非常不直观的行为。通过分析这些模式，我们测量了 LLM 之间的相似性，并构建了一个基于错误相关性对它们进行分类的分类法。我们的研究结果表明，错误答案并不是随机分布的，而是在各个模型之间系统地相关的，这为 LLM 之间的底层结构和关系提供了新的见解。</li>
</ul>

<h3>Title: Are LLMs good pragmatic speakers?</h3>
<ul>
<li><strong>Authors: </strong>Mingyue Jian, Siddharth Narayanaswamy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01562">https://arxiv.org/abs/2411.01562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01562">https://arxiv.org/pdf/2411.01562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01562]] Are LLMs good pragmatic speakers?(https://arxiv.org/abs/2411.01562)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained on data assumed to include natural language pragmatics, but do they actually behave like pragmatic speakers? We attempt to answer this question using the Rational Speech Act (RSA) framework, which models pragmatic reasoning in human communication. Using the paradigm of a reference game constructed from the TUNA corpus, we score candidate referential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and in the RSA model, comparing and contrasting these scores. Given that RSA requires defining alternative utterances and a truth-conditional meaning function, we explore such comparison for different choices of each of these requirements. We find that while scores from the LLM have some positive correlation with those from RSA, there isn't sufficient evidence to claim that it behaves like a pragmatic speaker. This initial study paves way for further targeted efforts exploring different models and settings, including human-subject evaluation, to see if LLMs truly can, or be made to, behave like pragmatic speakers.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是在假定包含自然语言语用的数据上进行训练的，但它们的行为是否真的像语用说话者？我们尝试使用理性言语行为 (RSA) 框架来回答这个问题，该框架模拟了人类交流中的语用推理。使用由 TUNA 语料库构建的参考游戏范式，我们在最先进的 LLM (Llama3-8B-Instruct) 和 RSA 模型中对候选指称话语进行评分，并比较和对比这些分数。鉴于 RSA 需要定义替代话语和真值条件意义函数，我们探索了针对每个要求的不同选择的这种比较。我们发现，虽然 LLM 的分数与 RSA 的分数呈正相关，但没有足够的证据表明它的行为像语用说话者。这项初步研究为进一步探索不同的模型和设置（包括人类受试者评估）铺平了道路，以查看 LLM 是否真的可以或被要求像语用说话者一样行事。</li>
</ul>

<h3>Title: Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM</h3>
<ul>
<li><strong>Authors: </strong>Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, Tagyoung Chung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01610">https://arxiv.org/abs/2411.01610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01610">https://arxiv.org/pdf/2411.01610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01610]] Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM(https://arxiv.org/abs/2411.01610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Contrastive decoding (CD) (Li et al., 2023) improves the next-token distribution of a large expert language model (LM) using a small amateur LM. Although CD is applied to various LMs and domains to enhance open-ended text generation, it is still unclear why CD often works well, when it could fail, and how we can make it better. To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM. We also highlight that the linear extrapolation could make CD unable to output the most obvious answers that have already been assigned high probabilities by the amateur LM. To overcome CD's limitation, we propose a new unsupervised decoding method called $\mathbf{A}$symptotic $\mathbf{P}$robability $\mathbf{D}$ecoding (APD). APD explicitly extrapolates the probability curves from the LMs of different sizes to infer the asymptotic probabilities from an infinitely large LM without inducing more inference costs than CD. In FactualityPrompts, an open-ended text generation benchmark, sampling using APD significantly boosts factuality in comparison to the CD sampling and its variants, and achieves state-of-the-art results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA datasets, APD is often significantly better than CD and achieves a similar effect of using a larger LLM. For example, the perplexity of APD on top of Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA and LAMBADA.</li>
<li><strong>摘要：</strong>对比解码 (CD) (Li et al., 2023) 使用小型业余语言模型 (LM) 改进大型专家语言模型 (LM) 的下一个标记分布。尽管 CD 已应用于各种语言模型和领域以增强开放式文本生成，但尚不清楚 CD 为何通常效果良好、何时会失败以及如何改进它。为了加深对 CD 的理解，我们首先从理论上证明 CD 可以被视为从庞大的假设语言模型中线性推断下一个标记逻辑。我们还强调，线性外推可能会使 CD 无法输出业余语言模型已经分配高概率的最明显的答案。为了克服 CD 的局限性，我们提出了一种新的无监督解码方法，称为 $\mathbf{A}$symptotic $\mathbf{P}$robability $\mathbf{D}$ecoding (APD)。 APD 明确地从不同大小的 LM 中推断出概率曲线，从而从无限大的 LM 中推断出渐近概率，而不会比 CD 产生更多的推理成本。在开放式文本生成基准 FactualityPrompts 中，使用 APD 进行采样与 CD 采样及其变体相比显著提高了事实性，并在 Pythia 6.9B 和 OPT 6.7B 中取得了最先进的结果。此外，在五个常识性 QA 数据集中，APD 通常明显优于 CD，并达到了使用更大 LLM 的类似效果。例如，在 CommonsenseQA 和 LAMBADA 中，APD 在 Pythia 6.9B 上的困惑度甚至低于 Pythia 12B 的困惑度。</li>
</ul>

<h3>Title: UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01703">https://arxiv.org/abs/2411.01703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01703">https://arxiv.org/pdf/2411.01703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01703]] UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models(https://arxiv.org/abs/2411.01703)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have revolutionized vision-language understanding but are vulnerable to multimodal jailbreak attacks, where adversaries meticulously craft inputs to elicit harmful or inappropriate responses. We propose UniGuard, a novel multimodal safety guardrail that jointly considers the unimodal and cross-modal harmful signals. UniGuard is trained such that the likelihood of generating harmful responses in a toxic corpus is minimized, and can be seamlessly applied to any input prompt during inference with minimal computational costs. Extensive experiments demonstrate the generalizability of UniGuard across multiple modalities and attack strategies. It demonstrates impressive generalizability across multiple state-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4, MiniGPT-4, and InstructBLIP, thereby broadening the scope of our solution.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 彻底改变了视觉语言理解，但容易受到多模态越狱攻击，攻击者会精心设计输入以引发有害或不适当的反应。我们提出了 UniGuard，这是一种新型的多模态安全护栏，它同时考虑了单模态和跨模态有害信号。UniGuard 经过训练，可在有毒语料库中产生有害反应的可能性降至最低，并且可以在推理过程中以最小的计算成本无缝应用于任何输入提示。大量实验证明了 UniGuard 在多种模态和攻击策略中的通用性。它在多个最先进的 MLLM（包括 LLaVA、Gemini Pro、GPT-4、MiniGPT-4 和 InstructBLIP）中表现出令人印象深刻的通用性，从而扩大了我们解决方案的范围。</li>
</ul>

<h3>Title: Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups</h3>
<ul>
<li><strong>Authors: </strong>Răzvan-Alexandru Smădu, David-Gabriel Ion, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01706">https://arxiv.org/abs/2411.01706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01706">https://arxiv.org/pdf/2411.01706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01706]] Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups(https://arxiv.org/abs/2411.01706)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Complex Word Identification (CWI) is an essential step in the lexical simplification task and has recently become a task on its own. Some variations of this binary classification task have emerged, such as lexical complexity prediction (LCP) and complexity evaluation of multi-word expressions (MWE). Large language models (LLMs) recently became popular in the Natural Language Processing community because of their versatility and capability to solve unseen tasks in zero/few-shot settings. Our work investigates LLM usage, specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show that LLMs struggle in certain conditions or achieve comparable results against existing methods. In addition, we provide some views on meta-learning combined with prompt learning. In the end, we conclude that the current state of LLMs cannot or barely outperform existing methods, which are usually much smaller.</li>
<li><strong>摘要：</strong>复杂词识别 (CWI) 是词汇简化任务中必不可少的步骤，最近已成为一项独立的任务。这种二元分类任务的一些变体已经出现，例如词汇复杂度预测 (LCP) 和多词表达的复杂度评估 (MWE)。大型语言模型 (LLM) 最近在自然语言处理社区中流行起来，因为它们具有多功能性和在零样本/少样本设置中解决未见过的任务的能力。我们的工作调查了 LLM 的使用情况，特别是开源模型（例如 Llama 2、Llama 3 和 Vicuna v1.5）和闭源模型（例如 ChatGPT-3.5-turbo 和 GPT-4o），在 CWI、LCP 和 MWE 设置中。我们评估了零样本、少样本和微调设置，并表明 LLM 在某些条件下会遇到困难或取得与现有方法相当的结果。此外，我们还提供了一些关于元学习与即时学习相结合的观点。最后，我们得出结论，当前的 LLM 状态无法或几乎无法超越现有的方法，因为现有的方法通常要小得多。</li>
</ul>

<h3>Title: DynaSaur: Large Language Agents Beyond Predefined Actions</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01747">https://arxiv.org/abs/2411.01747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01747">https://arxiv.org/pdf/2411.01747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01747]] DynaSaur: Large Language Agents Beyond Predefined Actions(https://arxiv.org/abs/2411.01747)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>现有的 LLM 代理系统通常在每个步骤中从固定和预定义的集合中选择操作。虽然这种方法在封闭的、范围狭窄的环境中是有效的，但我们认为，在实际场景中部署 LLM 代理时，它会带来两个主要挑战：(1) 从一组固定的操作中进行选择会严重限制 LLM 代理的规划和行动能力，(2) 这种方法需要大量的人力来列举和实施所有可能的操作，这在具有大量潜在操作的复杂环境中是不切实际的。在这项工作中，我们提出了一个 LLM 代理框架，该框架能够以在线方式动态创建和组合操作。在这个框架中，代理通过在每个步骤生成和执行用通用编程语言编写的程序来与环境交互。此外，生成的操作会随着时间的推移而累积以供将来重用。我们在 GAIA 基准上进行的大量实验表明，该框架提供了更大的灵活性，并且优于以前的方法。值得注意的是，它允许 LLM 代理在预定义集合中不存在相关操作或现有操作由于不可预见的边缘情况而失败的情况下恢复。在撰写本文时，我们在 GAIA 公共排行榜上名列前茅。我们的代码可以在 \href{此 https URL}{此 https URL} 中找到。</li>
</ul>

<h3>Title: RAGViz: Diagnose and Visualize Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Tevin Wang, Jingyuan He, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01751">https://arxiv.org/abs/2411.01751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01751">https://arxiv.org/pdf/2411.01751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01751]] RAGViz: Diagnose and Visualize Retrieval-Augmented Generation(https://arxiv.org/abs/2411.01751)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) combines knowledge from domain-specific sources into large language models to ground answer generation. Current RAG systems lack customizable visibility on the context documents and the model's attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool that visualizes the attentiveness of the generated tokens in retrieved documents. With a built-in user interface, retrieval index, and Large Language Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and document-level attention visualization, and (2) generation comparison upon context document addition and removal. As an open-source toolkit, RAGViz can be easily hosted with a custom embedding model and HuggingFace-supported LLM backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index, memory-efficient LLM inference tool, and custom context snippet method, RAGViz operates efficiently with a median query time of about 5 seconds on a moderate GPU node. Our code is available at this https URL. A demo video of RAGViz can be found at this https URL.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 将来自领域特定来源的知识组合到大型语言模型中，以支持答案生成。当前的 RAG 系统缺乏对上下文文档的可定制可见性以及模型对此类文档的注意力。我们提出了 RAGViz，这是一种 RAG 诊断工具，可可视化检索到的文档中生成的标记的注意力。凭借内置用户界面、检索索引和大型语言模型 (LLM) 主干，RAGViz 提供两项主要功能：(1) 标记和文档级注意力可视化，以及 (2) 在添加和删除上下文文档时的生成比较。作为一个开源工具包，RAGViz 可以轻松托管自定义嵌入模型和 HuggingFace 支持的 LLM 主干。使用混合 ANN（近似最近邻）索引、内存高效的 LLM 推理工具和自定义上下文片段方法，RAGViz 在中等 GPU 节点上高效运行，平均查询时间约为 5 秒。我们的代码可在此 https URL 上找到。RAGViz 的演示视频可在此 https URL 上找到。</li>
</ul>

<h3>Title: Towards Pedagogical LLMs with Supervised Fine Tuning for Computing Education</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Vassar, Jake Renzella, Emily Ross, Andrew Taylor</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01765">https://arxiv.org/abs/2411.01765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01765">https://arxiv.org/pdf/2411.01765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01765]] Towards Pedagogical LLMs with Supervised Fine Tuning for Computing Education(https://arxiv.org/abs/2411.01765)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper investigates supervised fine-tuning of large language models (LLMs) to improve their pedagogical alignment in computing education, addressing concerns that LLMs may hinder learning outcomes. The project utilised a proprietary dataset of 2,500 high quality question/answer pairs from programming course forums, and explores two research questions: the suitability of university course forums in contributing to fine-tuning datasets, and how supervised fine-tuning can improve LLMs' alignment with educational principles such as constructivism. Initial findings suggest benefits in pedagogical alignment of LLMs, with deeper evaluations required.</li>
<li><strong>摘要：</strong>本文探讨了大型语言模型 (LLM) 的监督微调，以改善其在计算机教育中的教学一致性，解决了人们对 LLM 可能阻碍学习成果的担忧。该项目利用了来自编程课程论坛的 2,500 个高质量问答对的专有数据集，并探讨了两个研究问题：大学课程论坛是否适合为微调数据集做出贡献，以及监督微调如何改善 LLM 与建构主义等教育原则的一致性。初步研究结果表明 LLM 的教学一致性具有优势，但需要进行更深入的评估。</li>
</ul>

<h3>Title: Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback</h3>
<ul>
<li><strong>Authors: </strong>Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01834">https://arxiv.org/abs/2411.01834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01834">https://arxiv.org/pdf/2411.01834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01834]] Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback(https://arxiv.org/abs/2411.01834)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.</li>
<li><strong>摘要：</strong>尽管无文本口语语言模型 (SLM) 在端到端语音到语音建模中显示出潜力，但它们在语义连贯性和相关性方面仍然落后于基于文本的大型语言模型 (LLM)。这项工作引入了 Align-SLM 框架，该框架利用受人工智能反馈强化学习 (RLAIF) 启发的偏好优化来增强对 SLM 的语义理解。我们的方法从给定的提示中生成多个语音延续，并使用语义指标为直接偏好优化 (DPO) 创建偏好数据。我们使用 ZeroSpeech 2021 基准对词汇和句法建模、StoryCloze 数据集的口语版本对语义连贯性以及其他语音生成指标（包括 GPT4-o 分数和人工评估）来评估该框架。实验结果表明，我们的方法在大多数基准上都实现了 SLM 的最佳性能，凸显了偏好优化对于改进 SLM 语义的重要性。</li>
</ul>

<h3>Title: Can Language Models Learn to Skip Steps?</h3>
<ul>
<li><strong>Authors: </strong>Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01855">https://arxiv.org/abs/2411.01855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01855">https://arxiv.org/pdf/2411.01855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01855]] Can Language Models Learn to Skip Steps?(https://arxiv.org/abs/2411.01855)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and model behaviors. In this work, we study the ability to skip steps in reasoning - a hallmark of human expertise developed through practice. Unlike humans, who may skip steps to enhance efficiency or to reduce cognitive load, models do not inherently possess such motivations to minimize reasoning steps. To address this, we introduce a controlled framework that stimulates step-skipping behavior by iteratively refining models to generate shorter and accurate reasoning paths. Empirical results indicate that models can develop the step skipping ability under our guidance. Moreover, after fine-tuning on expanded datasets that include both complete and skipped reasoning sequences, the models can not only resolve tasks with increased efficiency without sacrificing accuracy, but also exhibit comparable and even enhanced generalization capabilities in out-of-domain scenarios. Our work presents the first exploration into human-like step-skipping ability and provides fresh perspectives on how such cognitive abilities can benefit AI models.</li>
<li><strong>摘要：</strong>语言模型在大量人类语言语料上进行训练后，展现出类似人类的推理能力。然而，它们距离真正的智能还很远，这为探索人类和模型行为的相似之处提供了有趣的机会。在这项工作中，我们研究了跳过推理步骤的能力——这是人类通过实践发展起来的专业知识的标志。与人类可能会跳过步骤来提高效率或减少认知负荷不同，模型本身并不具有最小化推理步骤的动机。为了解决这个问题，我们引入了一个受控框架，通过迭代优化模型来生成更短、更准确的推理路径，从而刺激跳过步骤的行为。实证结果表明，在我们的指导下，模型可以发展出跳过步骤的能力。此外，在包含完整和跳过的推理序列的扩展数据集上进行微调后，模型不仅可以在不牺牲准确性的情况下以更高的效率解决任务，而且在领域外的场景中也表现出相当甚至增强的泛化能力。我们的工作首次探索了类似人类的跳步能力，并为这种认知能力如何使人工智能模型受益提供了新的视角。</li>
</ul>

<h3>Title: Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task</h3>
<ul>
<li><strong>Authors: </strong>Hoonick Lee, Mogan Gim, Donghyeon Park, Donghee Choi, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.01996">https://arxiv.org/abs/2411.01996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.01996">https://arxiv.org/pdf/2411.01996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.01996]] Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task(https://arxiv.org/abs/2411.01996)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) have shown promise in various creative domains, including culinary arts. However, many LLMs still struggle to deliver the desired level of culinary creativity, especially when tasked with adapting recipes to meet specific cultural requirements. This study focuses on cuisine transfer-applying elements of one cuisine to another-to assess LLMs' culinary creativity. We employ a diverse set of LLMs to generate and evaluate culturally adapted recipes, comparing their evaluations against LLM and human judgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark to evaluate LLMs' recipe generation abilities in the cuisine transfer task, assessing their cultural accuracy and creativity in the culinary domain. Our findings reveal crucial insights into both generative and evaluative capabilities of LLMs in the culinary domain, highlighting strengths and limitations in understanding and applying cultural nuances in recipe creation. The code and dataset used in this project will be openly available in \url{this http URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现已在包括烹饪艺术在内的各种创意领域中展现出前景。然而，许多 LLM 仍然难以达到所需的烹饪创造力水平，尤其是在需要调整食谱以满足特定文化要求时。本研究侧重于美食迁移——将一种美食的元素应用于另一种美食——以评估 LLM 的烹饪创造力。我们使用了一组不同的 LLM 来生成和评估文化适应的食谱，将它们的评估与 LLM 和人类的判断进行比较。我们引入了 ASH（真实性、敏感性、和谐性）基准来评估 LLM 在美食迁移任务中的食谱生成能力，评估它们在烹饪领域的文化准确性和创造力。我们的研究结果揭示了 LLM 在烹饪领域的生成和评估能力的重要见解，突出了在理解和应用食谱创作中的文化细微差别方面的优势和局限性。本项目中使用的代码和数据集将在 \url{此 http URL} 中公开提供。</li>
</ul>

<h3>Title: Shortcut Learning in In-Context Learning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Rui Song, Yingji Li, Fausto Giunchiglia, Hao Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02018">https://arxiv.org/abs/2411.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02018">https://arxiv.org/pdf/2411.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02018]] Shortcut Learning in In-Context Learning: A Survey(https://arxiv.org/abs/2411.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Shortcut learning refers to the phenomenon where models employ simple, non-robust decision rules in practical tasks, which hinders their generalization and robustness. With the rapid development of large language models (LLMs) in recent years, an increasing number of studies have shown the impact of shortcut learning on LLMs. This paper provides a novel perspective to review relevant research on shortcut learning in In-Context Learning (ICL). It conducts a detailed exploration of the types of shortcuts in ICL tasks, their causes, available benchmarks, and strategies for mitigating shortcuts. Based on corresponding observations, it summarizes the unresolved issues in existing research and attempts to outline the future research landscape of shortcut learning.</li>
<li><strong>摘要：</strong>捷径学习是指模型在实际任务中采用简单、非鲁棒的决策规则，从而阻碍其泛化和鲁棒性的现象。近年来，随着大型语言模型（LLM）的快速发展，越来越多的研究显示了捷径学习对LLM的影响。本文提供了一个新颖的视角来回顾情境学习（ICL）中捷径学习的相关研究，详细探讨了ICL任务中的捷径类型、其原因、可用的基准以及缓解捷径的策略，并在此基础上总结了现有研究中尚未解决的问题，并试图勾勒出捷径学习的未来研究前景。</li>
</ul>

<h3>Title: Explainable cognitive decline detection in free dialogues with a Machine Learning approach based on pre-trained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Francisco de Arriba-Pérez, Silvia García-Méndez, Javier Otero-Mosquera, Francisco J. González-Castaño</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02036">https://arxiv.org/abs/2411.02036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02036">https://arxiv.org/pdf/2411.02036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02036]] Explainable cognitive decline detection in free dialogues with a Machine Learning approach based on pre-trained Large Language Models(https://arxiv.org/abs/2411.02036)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Cognitive and neurological impairments are very common, but only a small proportion of affected individuals are diagnosed and treated, partly because of the high costs associated with frequent screening. Detecting pre-illness stages and analyzing the progression of neurological disorders through effective and efficient intelligent systems can be beneficial for timely diagnosis and early intervention. We propose using Large Language Models to extract features from free dialogues to detect cognitive decline. These features comprise high-level reasoning content-independent features (such as comprehension, decreased awareness, increased distraction, and memory problems). Our solution comprises (i) preprocessing, (ii) feature engineering via Natural Language Processing techniques and prompt engineering, (iii) feature analysis and selection to optimize performance, and (iv) classification, supported by automatic explainability. We also explore how to improve Chatgpt's direct cognitive impairment prediction capabilities using the best features in our models. Evaluation metrics obtained endorse the effectiveness of a mixed approach combining feature extraction with Chatgpt and a specialized Machine Learning model to detect cognitive decline within free-form conversational dialogues with older adults. Ultimately, our work may facilitate the development of an inexpensive, non-invasive, and rapid means of detecting and explaining cognitive decline.</li>
<li><strong>摘要：</strong>认知和神经系统障碍非常常见，但只有一小部分受影响的个体得到诊断和治疗，部分原因是频繁筛查的成本很高。通过有效和高效的智能系统检测疾病前阶段并分析神经系统疾病的进展，有利于及时诊断和早期干预。我们建议使用大型语言模型从自由对话中提取特征来检测认知衰退。这些特征包括高级推理内容独立特征（例如理解、意识下降、注意力分散和记忆问题）。我们的解决方案包括 (i) 预处理、(ii) 通过自然语言处理技术和提示工程进行特征工程、(iii) 特征分析和选择以优化性能，以及 (iv) 由自动可解释性支持的分类。我们还探索了如何使用我们模型中的最佳特征来提高 Chatgpt 的直接认知障碍预测能力。获得的评估指标证明了将特征提取与 Chatgpt 和专门的机器学习模型相结合的混合方法的有效性，可以检测与老年人自由形式的对话中的认知衰退。最终，我们的工作可能促进开发一种廉价、非侵入性且快速的检测和解释认知能力下降的方法。</li>
</ul>

<h3>Title: Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention</h3>
<ul>
<li><strong>Authors: </strong>Xingtai Lv, Ning Ding, Kaiyan Zhang, Ermo Hua, Ganqu Cui, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02063">https://arxiv.org/abs/2411.02063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02063">https://arxiv.org/pdf/2411.02063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02063]] Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention(https://arxiv.org/abs/2411.02063)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Improving the effectiveness and efficiency of large language models (LLMs) simultaneously is a critical yet challenging research goal. In this paper, we find that low-rank pre-training, normally considered as efficient methods that will compromise performance, can be scalably effective when reduced parameters are precisely targeted. Specifically, applying the low-dimensional module only to the attention layer -- resolves this issue and enhances both effectiveness and efficiency. We refer to this structure as Low-dimensional Projected Attention (LPA) and provide an explanatory analysis. Through extensive experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we have validated the effectiveness and scalability of LPA. Our results show that LPA model can save up to 12.4% in time while achieving an approximate 5% improvement in test perplexity (ppl) and on downstream tasks compared with the vanilla Transformer.</li>
<li><strong>摘要：</strong>同时提高大型语言模型（LLM）的有效性和效率是一个关键而又具有挑战性的研究目标。在本文中，我们发现低秩预训练通常被认为是一种会影响性能的有效方法，但当精准减少参数时，它可以具有可扩展的有效性。具体而言，将低维模块仅应用于注意层 - 解决了这个问题并提高了有效性和效率。我们将这种结构称为低维投影注意力（LPA）并提供解释性分析。通过在 130M、370M 和扩展到 3B 的参数规模上进行大量实验，我们验证了 LPA 的有效性和可扩展性。我们的结果表明，与 vanilla Transformer 相比，LPA 模型可以节省高达 12.4% 的时间，同时在测试困惑度（ppl）和下游任务上实现约 5% 的改进。</li>
</ul>

<h3>Title: Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jonas Zausinger, Lars Pennig, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Mohan Singh, Michael Morris Danziger, Jannis Born</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE, cs.LG, cs.MS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02083">https://arxiv.org/abs/2411.02083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02083">https://arxiv.org/pdf/2411.02083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02083]] Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models(https://arxiv.org/abs/2411.02083)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving reasoning over quantities, especially arithmetics. This has particular relevance in scientific datasets where combinations of text and numerical data are abundant. One fundamental limitation is the nature of the CE loss, which assumes a nominal (categorical) scale and thus cannot convey proximity between generated number tokens. As a remedy, we here present two versions of a number token loss. The first is based on an $L_p$ loss between the ground truth token value and the weighted sum of the predicted class probabilities. The second loss minimizes the Wasserstein-1 distance between the distribution of the predicted output probabilities and the ground truth distribution. These regression-like losses can easily be added to any language model and extend the CE objective during training. We compare the proposed schemes on a mathematics dataset against existing tokenization, encoding, and decoding schemes for improving number representation in language models. Our results reveal a significant improvement in numerical accuracy when equipping a standard T5 model with the proposed loss schemes.</li>
<li><strong>摘要：</strong>虽然语言模型在文本生成方面具有出色的能力，但它们缺乏自然的归纳数字偏差，因此在涉及数量推理（尤其是算术）的任务中会遇到困难。这在科学数据集中尤其重要，因为科学数据集中文本和数字数据的组合非常丰富。一个根本的限制是 CE 损失的性质，它假设了一个名义（分类）尺度，因此无法传达生成的数字标记之间的接近度。作为补救措施，我们在此介绍了两个版本的数字标记损失。第一个版本基于真实标记值与预测类概率的加权和之间的 $L_p$ 损失。第二个损失最小化了预测输出概率分布与真实分布之间的 Wasserstein-1 距离。这些类似回归的损失可以轻松添加到任何语言模型中，并在训练期间扩展 CE 目标。我们将数学数据集上的提议方案与现有的标记化、编码和解码方案进行比较，以改进语言模型中的数字表示。我们的结果表明，当为标准 T5 模型配备提议的损失方案时，数值准确性显著提高。</li>
</ul>

<h3>Title: Advancements and limitations of LLMs in replicating human color-word associations</h3>
<ul>
<li><strong>Authors: </strong>Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.GR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02116">https://arxiv.org/abs/2411.02116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02116">https://arxiv.org/pdf/2411.02116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02116]] Advancements and limitations of LLMs in replicating human color-word associations(https://arxiv.org/abs/2411.02116)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT- 4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and words from eight categories in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category, particularly when using visual inputs rather than text-based color codes. However, the highest median performance was approximately 50% even for GPT4-o with visual inputs (chance level is 10%), and the performance levels varied significantly across word categories and colors, indicating a failure to fully replicate human color-word associations. On the other hand, color discrimination ability estimated from our color-word association data showed that LLMs demonstrated high correlation with human color discrimination patterns, similarly to previous studies. Our study highlights both the advancements in LLM capabilities and their persistent limitations, suggesting differences in semantic memory structures between humans and LLMs in representing color-word associations.</li>
<li><strong>摘要：</strong>颜色词联想在人类认知和设计应用中起着根本性的作用。大型语言模型 (LLM) 已广泛应用，并在各种基准测试中表现出具有自然对话技能的智能行为。然而，它们复制人类颜色词联想的能力仍未得到充分研究。我们使用从 10,000 多名日本参与者收集的数据，比较了多代 LLM（从 GPT-3 到 GPT-4o）与人类颜色词联想，涉及日语中 8 个类别的 17 种颜色和单词。我们的研究结果显示，LLM 的性能在各代之间有明显的进步，GPT-4o 在预测每种颜色和类别的最佳投票词方面实现了最高的准确率，尤其是在使用视觉输入而不是基于文本的颜色代码时。然而，即使是使用视觉输入的 GPT4-o（概率水平为 10%），最高中位性能也约为 50%，并且不同词类和颜色的性能水平差异很大，表明无法完全复制人类的颜色词联想。另一方面，根据我们根据颜色词关联数据估计的颜色辨别能力，LLM 与人类的颜色辨别模式具有高度相关性，这与之前的研究类似。我们的研究既强调了 LLM 能力的进步，也强调了其持续存在的局限性，表明人类和 LLM 在表示颜色词关联方面的语义记忆结构存在差异。</li>
</ul>

<h3>Title: AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zichen Song, Yuxin Wu, Sitan Huang, Zhongfeng Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02117">https://arxiv.org/abs/2411.02117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02117">https://arxiv.org/pdf/2411.02117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02117]] AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis(https://arxiv.org/abs/2411.02117)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer's contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures.</li>
<li><strong>摘要：</strong>深度学习中层重要性的评估一直是一个活跃的研究领域，对模型优化和可解释性具有重要意义。最近，大型语言模型 (LLM) 在各个领域都获得了突出地位，但很少有研究探索 LLM 中各个层的功能重要性和性能贡献，尤其是从激活分布的角度来看。在这项工作中，我们提出了激活方差-稀疏度分数 (AVSS)，这是一种结合归一化激活方差和稀疏度的新指标，用于评估每个层对模型性能的贡献。通过基于 AVSS 识别和删除大约最低的 25% 的层，我们在问答、语言建模和情感分类等任务中实现了超过 90% 的原始模型性能，这表明这些层可能不是必需的。我们的方法提供了一种识别不太重要的层的系统方法，有助于实现高效的大型语言模型架构。</li>
</ul>

<h3>Title: Positive Experience Reflection for Agents in Interactive Text Environments</h3>
<ul>
<li><strong>Authors: </strong>Philip Lippmann, Matthijs T.J. Spaan, Jie Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02223">https://arxiv.org/abs/2411.02223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02223">https://arxiv.org/pdf/2411.02223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02223]] Positive Experience Reflection for Agents in Interactive Text Environments(https://arxiv.org/abs/2411.02223)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Intelligent agents designed for interactive environments face significant challenges in text-based games, a domain that demands complex reasoning and adaptability. While agents based on large language models (LLMs) using self-reflection have shown promise, they struggle when initially successful and exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour, a novel approach that addresses these limitations in existing reflection methods by incorporating positive experiences and managed memory to enrich the context available to the agent at decision time. Our comprehensive analysis spans both closed- and open-source LLMs and demonstrates the effectiveness of Sweet&Sour in improving agent performance, particularly in scenarios where previous approaches fall short.</li>
<li><strong>摘要：</strong>为交互式环境设计的智能代理在基于文本的游戏中面临重大挑战，因为该领域需要复杂的推理和适应性。虽然基于使用自我反思的大型语言模型 (LLM) 的代理已显示出良好的前景，但它们在最初成功时会遇到困难，并且在使用较小的 LLM 时会降低效率。我们引入了 Sweet&Sour，这是一种新颖的方法，它通过结合积极的经验和托管内存来丰富代理在决策时可用的上下文，从而解决了现有反思方法中的这些限制。我们的全面分析涵盖了闭源和开源 LLM，并展示了 Sweet&Sour 在提高代理性能方面的有效性，特别是在以前的方法不足的情况下。</li>
</ul>

<h3>Title: The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units</h3>
<ul>
<li><strong>Authors: </strong>Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02280">https://arxiv.org/abs/2411.02280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02280">https://arxiv.org/pdf/2411.02280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02280]] The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units(https://arxiv.org/abs/2411.02280)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units -- but not random units -- leads to drastic deficits in language tasks. Correspondingly, language-selective LLM units are more aligned to brain recordings from the human language system than random units. Finally, we investigate whether our localization method extends to other cognitive domains: while we find specialized networks in some LLMs for reasoning and social capabilities, there are substantial differences among models. These findings provide functional and causal evidence for specialization in large language models, and highlight parallels with the functional organization in the brain.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 不仅在语言任务上表现出非凡的能力，而且在各种非语言性质的任务上也表现出非凡的能力，例如逻辑推理和社会推理。在人类大脑中，神经科学已经确定了一个核心语言系统，该系统有选择地和因果地支持语言处理。我们在此询问 LLM 中是否也出现了类似的语言专业化。我们使用与神经科学相同的定位方法，在 18 个流行的 LLM 中识别语言选择性单元。然后，我们通过证明消除 LLM 语言选择性单元（而不是随机单元）会导致语言任务的严重缺陷来确定这些单元的因果作用。相应地，语言选择性 LLM 单元比随机单元更符合人类语言系统的大脑记录。最后，我们研究我们的定位方法是否可以扩展到其他认知领域：虽然我们在某些 LLM 中发现了用于推理和社交能力的专门网络，但模型之间存在很大差异。这些发现为大型语言模型的专业化提供了功能和因果证据，并强调了与大脑功能组织的相似性。</li>
</ul>

<h3>Title: CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments</h3>
<ul>
<li><strong>Authors: </strong>Kung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02305">https://arxiv.org/abs/2411.02305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02305">https://arxiv.org/pdf/2411.02305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02305]] CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments(https://arxiv.org/abs/2411.02305)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 40% of the tasks with ReAct prompting, and less than 55% even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments. CRMArena is an open challenge to the community: systems that can reliably complete tasks showcase direct business value in a popular work environment.</li>
<li><strong>摘要：</strong>客户关系管理 (CRM) 系统对于现代企业至关重要，为管理客户互动和数据提供了基础。将 AI 代理集成到 CRM 系统中可以自动化常规流程并增强个性化服务。但是，由于缺乏反映现实世界 CRM 任务复杂性的现实基准，部署和评估这些代理具有挑战性。为了解决这个问题，我们引入了 CRMArena，这是一种新颖的基准，旨在评估 AI 代理在专业工作环境中的现实任务上的表现。根据 CRM 专家和行业最佳实践的指导，我们设计了 CRMArena，其中包含九项客户服务任务，分布在三个角色中：服务代理、分析师和经理。基准包括 16 个常用的工业对象（例如，帐户、订单、知识文章、案例），具有高度互联性，以及潜在变量（例如，投诉习惯、政策违规），以模拟现实数据分布。实验结果表明，最先进的 LLM 代理在 ReAct 提示下成功完成的任务不到 40%，即使具有函数调用能力，成功率也低于 55%。我们的研究结果强调，需要在实际工作环境中部署增强的代理功能，包括函数调用和规则遵循。 CRMArena 是面向社区的公开挑战：能够可靠地完成任务的系统在受欢迎的工作环境中展示直接的商业价值。</li>
</ul>

<h3>Title: MdEval: Massively Multilingual Code Debugging</h3>
<ul>
<li><strong>Authors: </strong>Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02310">https://arxiv.org/abs/2411.02310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02310">https://arxiv.org/pdf/2411.02310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02310]] MdEval: Massively Multilingual Code Debugging(https://arxiv.org/abs/2411.02310)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Code large language models (LLMs) have made significant progress in code debugging by directly generating the correct code based on the buggy code snippet. Programming benchmarks, typically consisting of buggy code snippet and their associated test cases, are used to assess the debugging capabilities of LLMs. However, many existing benchmarks primarily focus on Python and are often limited in terms of language diversity (e.g., DebugBench and DebugEval). To advance the field of multilingual debugging with LLMs, we propose the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task. Further, we introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs into the correct multilingual queries and solutions (xDebugGen). Further, a multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong baseline specifically to handle the bugs of a wide range of programming languages (e.g. "Missing Mut" in language Rust and "Misused Macro Definition" in language C). Our extensive experiments on MDEVAL reveal a notable performance gap between open-source models and closed-source LLMs (e.g., GPT and Claude series), highlighting huge room for improvement in multilingual code debugging scenarios.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过直接根据有缺陷的代码片段生成正确的代码，在代码调试方面取得了重大进展。编程基准测试通常由有缺陷的代码片段及其相关的测试用例组成，用于评估 LLM 的调试能力。然而，许多现有的基准测试主要关注 Python，并且通常在语言多样性方面受到限制（例如，DebugBench 和 DebugEval）。为了推动使用 LLM 进行多语言调试领域的发展，我们提出了第一个大规模多语言调试基准测试，它包括 18 种编程语言的 3.6K 测试样本，涵盖自动程序修复 (APR) 任务、代码审查 (CR) 任务和错误识别 (BI) 任务。此外，我们通过将错误注入正确的多语言查询和解决方案（xDebugGen）来引入调试指令语料库 MDEVAL-INSTRUCT。此外，多语言调试器 xDebugCoder 在 MDEVAL-INSTRUCT 上进行了训练，作为强大的基线，专门用于处理各种编程语言的错误（例如 Rust 语言中的“Missing Mut”和 C 语言中的“Misused Macro Definition”）。我们对 MDEVAL 进行的大量实验表明，开源模型和闭源 LLM（例如 GPT 和 Claude 系列）之间存在明显的性能差距，凸显了多语言代码调试场景中巨大的改进空间。</li>
</ul>

<h3>Title: Evaluating Creative Short Story Generation in Humans and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02316">https://arxiv.org/abs/2411.02316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02316">https://arxiv.org/pdf/2411.02316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02316]] Evaluating Creative Short Story Generation in Humans and Large Language Models(https://arxiv.org/abs/2411.02316)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Storytelling is a fundamental aspect of human communication, relying heavily on creativity to produce narratives that are novel, appropriate, and surprising. While large language models (LLMs) have recently demonstrated the ability to generate high-quality stories, their creative capabilities remain underexplored. Previous research has either focused on creativity tests requiring short responses or primarily compared model performance in story generation to that of professional writers. However, the question of whether LLMs exhibit creativity in writing short stories on par with the average human remains unanswered. In this work, we conduct a systematic analysis of creativity in short story generation across LLMs and everyday people. Using a five-sentence creative story task, commonly employed in psychology to assess human creativity, we automatically evaluate model- and human-generated stories across several dimensions of creativity, including novelty, surprise, and diversity. Our findings reveal that while LLMs can generate stylistically complex stories, they tend to fall short in terms of creativity when compared to average human writers.</li>
<li><strong>摘要：</strong>讲故事是人类交流的一个基本方面，在很大程度上依赖于创造力来创作新颖、恰当和令人惊讶的故事。虽然大型语言模型 (LLM) 最近展示了生成高质量故事的能力，但它们的创造能力仍未得到充分开发。先前的研究要么集中在需要简短回答的创造力测试上，要么主要将模型在故事生成方面的表现与专业作家进行比较。然而，LLM 在写短篇小说时是否表现出与普通人相当的创造力，这个问题仍未得到解答。在这项工作中，我们对 LLM 和普通人的短篇小说生成创造力进行了系统分析。使用心理学中常用来评估人类创造力的五句创意故事任务，我们自动评估模型和人类生成的故事在创造力的几个维度上的表现，包括新颖性、惊喜和多样性。我们的研究结果表明，虽然 LLM 可以生成风格复杂的故事，但与普通人类作家相比，它们在创造力方面往往有所欠缺。</li>
</ul>

<h3>Title: WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02337">https://arxiv.org/abs/2411.02337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02337">https://arxiv.org/pdf/2411.02337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02337]] WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning(https://arxiv.org/abs/2411.02337)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出作为自主代理的巨大潜力，尤其是在基于 Web 的任务中。然而，现有的 LLM Web 代理严重依赖昂贵的专有 LLM API，而开放的 LLM 缺乏必要的决策能力。本文介绍了 WebRL，这是一个自我进化的在线课程强化学习框架，旨在使用开放的 LLM 训练高性能 Web 代理。WebRL 解决了构建 LLM Web 代理的三个关键挑战，包括训练任务的稀缺性、稀疏的反馈信号和在线学习中的策略分布漂移。具体来说，WebRL 结合了 1) 一个从不成功的尝试中生成新任务的自我进化课程，2) 一个强大的结果监督奖励模型 (ORM)，以及 3) 自适应强化学习策略以确保持续改进。我们应用 WebRL 将开放的 Llama-3.1 和 GLM-4 模型转换为熟练的 Web 代理。在 WebArena-Lite 上，WebRL 将 Llama-3.1-8B 的成功率从 4.8% 提高到 42.4%，将 GLM-4-9B 的成功率从 6.1% 提高到 43%。这些开放模型的性能显著超越了 GPT-4-Turbo（17.6%）和 GPT-4o（13.9%），并且优于之前在开放 LLM 上训练的最先进的 Web 代理（AutoWebGLM，18.2%）。我们的研究结果表明，WebRL 能够有效地弥合开放和专有的基于 LLM 的 Web 代理之间的差距，为更易于访问和更强大的自主 Web 交互系统铺平道路。</li>
</ul>

<h3>Title: Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02382">https://arxiv.org/abs/2411.02382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02382">https://arxiv.org/pdf/2411.02382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02382]] Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models(https://arxiv.org/abs/2411.02382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in various scientific domains, from natural language processing to complex problem-solving tasks. Their ability to understand and generate human-like text has opened up new possibilities for advancing scientific research, enabling tasks such as data analysis, literature review, and even experimental design. One of the most promising applications of LLMs in this context is hypothesis generation, where they can identify novel research directions by analyzing existing knowledge. However, despite their potential, LLMs are prone to generating ``hallucinations'', outputs that are plausible-sounding but factually incorrect. Such a problem presents significant challenges in scientific fields that demand rigorous accuracy and verifiability, potentially leading to erroneous or misleading conclusions. To overcome these challenges, we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that enhances LLM hypothesis generation by integrating external, structured knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured reasoning process, organizing their output as a chain of ideas (CoI), and includes a KG-supported module for the detection of hallucinations. With experiments on our newly constructed hypothesis generation dataset, we demonstrate that KG-CoI not only improves the accuracy of LLM-generated hypotheses but also reduces the hallucination in their reasoning chains, highlighting its effectiveness in advancing real-world scientific research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在从自然语言处理到复杂问题解决任务等各个科学领域都表现出了卓越的能力。它们理解和生成类似人类文本的能力为推进科学研究开辟了新的可能性，使数据分析、文献综述甚至实验设计等任务成为可能。在这种情况下，LLM 最有前途的应用之一是假设生成，它们可以通过分析现有知识来确定新的研究方向。然而，尽管 LLM 潜力巨大，但它很容易产生“幻觉”，即输出听起来合理但事实错误的输出。这样的问题对要求严格准确性和可验证性的科学领域提出了重大挑战，可能会导致错误或误导性的结论。为了克服这些挑战，我们提出了 KG-CoI（知识根基思想链），这是一种新颖的系统，它通过集成来自知识图谱 (KG) 的外部结构化知识来增强 LLM 假设生成。 KG-CoI 引导 LLM 完成结构化推理过程，将其输出组织为思想链 (CoI)，并包含一个 KG 支持的幻觉检测模块。通过对我们新构建的假设生成数据集进行实验，我们证明 KG-CoI 不仅提高了 LLM 生成的假设的准确性，而且还减少了其推理链中的幻觉，凸显了其在推动现实世界科学研究方面的有效性。</li>
</ul>

<h3>Title: Attacking Vision-Language Computer Agents via Pop-ups</h3>
<ul>
<li><strong>Authors: </strong>Yanzhe Zhang, Tao Yu, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02391">https://arxiv.org/abs/2411.02391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02391">https://arxiv.org/pdf/2411.02391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02391]] Attacking Vision-Language Computer Agents via Pop-ups(https://arxiv.org/abs/2411.02391)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups, which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing the tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack.</li>
<li><strong>摘要：</strong>搭载大型视觉和语言模型 (VLM) 的自主代理在完成日常计算机任务方面表现出巨大潜力，例如浏览网页预订旅行和操作桌面软件，这需要代理理解这些界面。尽管此类视觉输入越来越多地融入代理应用程序，但它们周围存在哪些类型的风险和攻击仍不清楚。在这项研究中，我们证明 VLM 代理很容易受到一组精心设计的对抗性弹出窗口的攻击，而人类用户通常会识别并忽略这些弹出窗口。这种干扰会导致代理点击这些弹出窗口，而不是像往常一样执行任务。将这些弹出窗口集成到现有的代理测试环境（如 OSWorld 和 VisualWebArena）中会导致攻击成功率（代理点击弹出窗口的频率）平均为 86%，任务成功率降低 47%。要求代理忽略弹出窗口或包含广告通知等基本防御技术对攻击无效。</li>
</ul>

<h3>Title: Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin Script Languages</h3>
<ul>
<li><strong>Authors: </strong>Hoang Nguyen, Khyati Mahajan, Vikas Yadav, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02398">https://arxiv.org/abs/2411.02398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02398">https://arxiv.org/pdf/2411.02398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02398]] Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin Script Languages(https://arxiv.org/abs/2411.02398)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Multilingual LLMs have achieved remarkable benchmark performance, but we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates that integrating phonemic signals improves performance across both non-Latin and Latin languages, with a particularly significant impact on closing the performance gap between the two. Through detailed experiments, we show that phonemic and orthographic scripts retrieve distinct examples for in-context learning (ICL). This motivates our proposed Mixed-ICL retrieval strategy, where further aggregation leads to our significant performance improvements for both Latin script languages (up to 12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL retrieval.</li>
<li><strong>摘要：</strong>多语言 LLM 已取得显著的基准性能，但我们发现它们在当代 LLM 系列的非拉丁文字语言上仍然表现不佳。这种差异源于这样一个事实：LLM 是用正字法脚本进行预训练的，而正字法脚本以拉丁字符为主，掩盖了它们与非拉丁文字的共同音系。我们建议利用音素转录作为互补信号来诱导脚本不变表示。我们的研究表明，整合音素信号可以提高非拉丁和拉丁语言的性能，对缩小两者之间的性能差距具有特别显著的影响。通过详细的实验，我们表明音素和正字法脚本可以检索不同的示例以进行上下文学习 (ICL)。这激发了我们提出的混合 ICL 检索策略，与随机 ICL 检索相比，进一步聚合可显著提高拉丁文字语言（高达 12.6%）和非拉丁文字语言（高达 15.1%）的性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
