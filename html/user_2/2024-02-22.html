<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-22</h1>
<h3>Title: Spontaneous Theory of Mind for Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Nikolos Gurney, David V. Pynadath, Volkan Ustun</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13272">https://arxiv.org/abs/2402.13272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13272">https://arxiv.org/pdf/2402.13272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13272]] Spontaneous Theory of Mind for Artificial Intelligence(https://arxiv.org/abs/2402.13272)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Existing approaches to Theory of Mind (ToM) in Artificial Intelligence (AI) overemphasize prompted, or cue-based, ToM, which may limit our collective ability to develop Artificial Social Intelligence (ASI). Drawing from research in computer science, cognitive science, and related disciplines, we contrast prompted ToM with what we call spontaneous ToM -- reasoning about others' mental states that is grounded in unintentional, possibly uncontrollable cognitive functions. We argue for a principled approach to studying and developing AI ToM and suggest that a robust, or general, ASI will respond to prompts \textit{and} spontaneously engage in social reasoning.</li>
<li><strong>摘要：</strong>人工智能 (AI) 中的现有心智理论 (ToM) 方法过分强调提示或基于线索的 ToM，这可能会限制我们开发人工智能 (ASI) 的集体能力。根据计算机科学、认知科学和相关学科的研究，我们将提示性思维理论与我们所说的自发思维理论进行了对比——对他人基于无意识、可能无法控制的认知功能的心理状态进行推理。我们主张采用一种有原则的方法来研究和开发 AI ToM，并建议强大的或通用的 ASI 将响应提示\textit{并}自发地参与社会推理。</li>
</ul>

<h3>Title: Operational Collective Intelligence of Humans and Machines</h3>
<ul>
<li><strong>Authors: </strong>Nikolos Gurney, Fred Morstatter, David V. Pynadath, Adam Russell, Gleb Satyukov</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13273">https://arxiv.org/abs/2402.13273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13273">https://arxiv.org/pdf/2402.13273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13273]] Operational Collective Intelligence of Humans and Machines(https://arxiv.org/abs/2402.13273)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We explore the use of aggregative crowdsourced forecasting (ACF) as a mechanism to help operationalize ``collective intelligence'' of human-machine teams for coordinated actions. We adopt the definition for Collective Intelligence as: ``A property of groups that emerges from synergies among data-information-knowledge, software-hardware, and individuals (those with new insights as well as recognized authorities) that enables just-in-time knowledge for better decisions than these three elements acting alone.'' Collective Intelligence emerges from new ways of connecting humans and AI to enable decision-advantage, in part by creating and leveraging additional sources of information that might otherwise not be included. Aggregative crowdsourced forecasting (ACF) is a recent key advancement towards Collective Intelligence wherein predictions (X\% probability that Y will happen) and rationales (why I believe it is this probability that X will happen) are elicited independently from a diverse crowd, aggregated, and then used to inform higher-level decision-making. This research asks whether ACF, as a key way to enable Operational Collective Intelligence, could be brought to bear on operational scenarios (i.e., sequences of events with defined agents, components, and interactions) and decision-making, and considers whether such a capability could provide novel operational capabilities to enable new forms of decision-advantage.</li>
<li><strong>摘要：</strong>我们探索使用聚合众包预测（ACF）作为一种机制来帮助实施人机团队的“集体智慧”以协调行动。我们采用集体智慧的定义为：“数据-信息-知识、软件-硬件和个人（具有新见解和公认权威的人）之间的协同作用产生的群体属性，能够实现及时集体智慧源于连接人类和人工智能的新方式，以实现决策优势，部分是通过创建和利用其他可能不包含的信息源来实现。聚合众包预测 (ACF) 是集体智能的最新关键进展，其中预测（Y 发生的 X\% 概率）和基本原理（为什么我相信 X 发生的概率）是从不同人群中独立得出的，聚合，然后用于为高层决策提供信息。本研究询问 ACF 作为实现作战集体智能的关键方式，是否可以应用于作战场景（即具有定义的代理、组件和交互的事件序列）和决策，并考虑这种能力是否具有可以提供新颖的运营能力，以实现新形式的决策优势。</li>
</ul>

<h3>Title: Grounding from an AI and Cognitive Science Lens</h3>
<ul>
<li><strong>Authors: </strong>Goonmeet Bajaj, Srinivasan Parthasarathy, Valerie L. Shalin, Amit Sheth</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13290">https://arxiv.org/abs/2402.13290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13290">https://arxiv.org/pdf/2402.13290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13290]] Grounding from an AI and Cognitive Science Lens(https://arxiv.org/abs/2402.13290)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Grounding is a challenging problem, requiring a formal definition and different levels of abstraction. This article explores grounding from both cognitive science and machine learning perspectives. It identifies the subtleties of grounding, its significance for collaborative agents, and similarities and differences in grounding approaches in both communities. The article examines the potential of neuro-symbolic approaches tailored for grounding tasks, showcasing how they can more comprehensively address grounding. Finally, we discuss areas for further exploration and development in grounding.</li>
<li><strong>摘要：</strong>接地是一个具有挑战性的问题，需要正式的定义和不同的抽象级别。本文从认知科学和机器学习的角度探讨了基础知识。它确定了接地的微妙之处、其对协作代理的重要性以及两个社区接地方法的异同。本文探讨了针对接地任务量身定制的神经符号方法的潜力，展示了它们如何更全面地解决接地问题。最后，我们讨论了接地方面进一步探索和发展的领域。</li>
</ul>

<h3>Title: Enhanced Hallucination Detection in Neural Machine Translation through  Simple Detector Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, Nuno M. Guerreiro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13331">https://arxiv.org/abs/2402.13331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13331">https://arxiv.org/pdf/2402.13331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13331]] Enhanced Hallucination Detection in Neural Machine Translation through  Simple Detector Aggregation(https://arxiv.org/abs/2402.13331)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>Hallucinated translations pose significant threats and safety concerns when it comes to the practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems.</li>
<li><strong>摘要：</strong>在机器翻译系统的实际部署方面，幻觉翻译会带来重大威胁和安全问题。先前的研究工作已经发现，探测器表现出互补的性能，不同的探测器擅长探测不同类型的幻觉。在本文中，我们建议通过组合单个检测器并引入一种聚合多个检测器的简单方法来解决单个检测器的局限性。我们的结果证明了聚合检测器的有效性，为迈向更加可靠的机器翻译系统迈出了有希望的一步。</li>
</ul>

<h3>Title: Incentivized Exploration via Filtered Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Anand Kalvit, Aleksandrs Slivkins, Yonatan Gur</a></li>
<li><strong>Subjects: </strong>cs.LG, econ.TH</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13338">https://arxiv.org/abs/2402.13338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13338">https://arxiv.org/pdf/2402.13338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13338]] Incentivized Exploration via Filtered Posterior Sampling(https://arxiv.org/abs/2402.13338)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We study "incentivized exploration" (IE) in social learning problems where the principal (a recommendation algorithm) can leverage information asymmetry to incentivize sequentially-arriving agents to take exploratory actions. We identify posterior sampling, an algorithmic approach that is well known in the multi-armed bandits literature, as a general-purpose solution for IE. In particular, we expand the existing scope of IE in several practically-relevant dimensions, from private agent types to informative recommendations to correlated Bayesian priors. We obtain a general analysis of posterior sampling in IE which allows us to subsume these extended settings as corollaries, while also recovering existing results as special cases.</li>
<li><strong>摘要：</strong>我们研究社会学习问题中的“激励探索”（IE），其中委托人（推荐算法）可以利用信息不对称来激励顺序到达的代理采取探索性行动。我们将后采样（一种在多臂老虎机文献中众所周知的算法方法）确定为 IE 的通用解决方案。特别是，我们在几个实际相关的维度上扩展了 IE 的现有范围，从私人代理类型到信息推荐再到相关的贝叶斯先验。我们获得了 IE 中后验采样的一般分析，这使我们能够将这些扩展设置作为推论，同时也将现有结果恢复为特殊情况。</li>
</ul>

<h3>Title: A Simple but Effective Approach to Improve Structured Language Model  Output for Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Li, Rampi Ramprasad, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13364">https://arxiv.org/abs/2402.13364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13364">https://arxiv.org/pdf/2402.13364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13364]] A Simple but Effective Approach to Improve Structured Language Model  Output for Information Extraction(https://arxiv.org/abs/2402.13364)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在根据指令生成非结构化自然语言方面表现出了令人印象深刻的能力。然而，当负责生成符合特定结构化格式的文本时，它们的性能可能不一致，这在命名实体识别 (NER) 或关系提取 (RE) 等应用中至关重要。为了解决这个问题，本文介绍了一种有效的方法——G&O，来增强其结构化文本生成能力。它将生成过程分为两步：最初，法学硕士以自然语言生成答案作为中间响应。随后，法学硕士被要求使用中间响应作为上下文，将输出组织成所需的结构。 G&O 有效地将内容生成与结构化过程分开，减轻了同时完成两个正交任务的压力。在零样本 NER 和 RE 上进行测试，结果表明，以最少的额外努力即可显着提高 LLM 性能。这种简单且适应性强的提示技术还可以与其他策略（例如自我一致性）相结合，以进一步提升 LLM 在各种结构化文本生成任务中的能力。</li>
</ul>

<h3>Title: EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human  Adversaries</h3>
<ul>
<li><strong>Authors: </strong>Jing Han Sun, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13372">https://arxiv.org/abs/2402.13372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13372">https://arxiv.org/pdf/2402.13372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13372]] EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human  Adversaries(https://arxiv.org/abs/2402.13372)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT's capabilities, we expand our task instances from 182 to 3,691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limitations and the value of dynamic datasets in uncovering them.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在 Winograd 模式挑战 (WSC) 中表现出色，这是一项通过代词消歧来测试常识推理的共指解析任务，但它们在处理具有细微更改或改写的实例时遇到了困难。为了解决这个问题，我们引入了 EvoGrad，这是一个开源平台，它利用人机交互方法来创建针对此类改变的 WSC 实例量身定制的动态数据集。利用 ChatGPT 的功能，我们将任务实例从 182 个扩展到 3,691 个，为多样化的常识推理数据集树立了新的基准。此外，我们引入了错误深度度量，评估动态任务中的模型稳定性。我们的结果强调了 EvoGrad 带来的挑战：即使是表现最好的 LLM GPT-3.5，也能达到 65.0% 的准确率，平均误差深度为 7.2，这与人类在没有扰动误差的情况下 92. 8% 的准确率形成鲜明对比。这凸显了持续存在的模型局限性以及动态数据集在发现这些局限性方面的价值。</li>
</ul>

<h3>Title: Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Ivan Sekulić, Silvia Terragni, Victor Guimarães, Nghia Khau, Bruna Guedes, Modestas Filipavicius, André Ferreira Manso, Roland Mathis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13374">https://arxiv.org/abs/2402.13374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13374">https://arxiv.org/pdf/2402.13374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13374]] Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems(https://arxiv.org/abs/2402.13374)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>In the realm of dialogue systems, user simulation techniques have emerged as a game-changer, redefining the evaluation and enhancement of task-oriented dialogue (TOD) systems. These methods are crucial for replicating real user interactions, enabling applications like synthetic data augmentation, error detection, and robust evaluation. However, existing approaches often rely on rigid rule-based methods or on annotated data. This paper introduces DAUS, a Domain-Aware User Simulator. Leveraging large language models, we fine-tune DAUS on real examples of task-oriented dialogues. Results on two relevant benchmarks showcase significant improvements in terms of user goal fulfillment. Notably, we have observed that fine-tuning enhances the simulator's coherence with user goals, effectively mitigating hallucinations -- a major source of inconsistencies in simulator responses.</li>
<li><strong>摘要：</strong>在对话系统领域，用户模拟技术已经成为游戏规则的改变者，重新定义了面向任务的对话（TOD）系统的评估和增强。这些方法对于复制真实的用户交互、实现合成数据增强、错误检测和稳健评估等应用至关重要。然而，现有的方法通常依赖于严格的基于规则的方法或注释数据。本文介绍了 DAUS，一种领域感知用户模拟器。利用大型语言模型，我们根据面向任务的对话的真实示例对 DAUS 进行微调。两个相关基准的结果显示了用户目标实现方面的显着改进。值得注意的是，我们观察到微调增强了模拟器与用户目标的一致性，有效地减轻了幻觉——这是模拟器响应不一致的主要根源。</li>
</ul>

<h3>Title: Learning and Sustaining Shared Normative Systems via Bayesian Rule  Induction in Markov Games</h3>
<ul>
<li><strong>Authors: </strong>Ninell Oldenburg, Tan Zhi-Xuan</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13399">https://arxiv.org/abs/2402.13399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13399">https://arxiv.org/pdf/2402.13399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13399]] Learning and Sustaining Shared Normative Systems via Bayesian Rule  Induction in Markov Games(https://arxiv.org/abs/2402.13399)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants to rapidly learn those norms. We formalize this framework in the context of Markov games and demonstrate its operation in a multi-agent environment via approximately Bayesian rule induction of obligative and prohibitive norms. Using our approach, agents are able to rapidly learn and sustain a variety of cooperative institutions, including resource management norms and compensation for pro-social labor, promoting collective welfare while still allowing agents to act in their own interests.</li>
<li><strong>摘要：</strong>人类社会的一个普遍特征是采用为合作目的服务的规则和规范体系。我们如何构建具有相同功能的学习代理，以便它们可以灵活地与它们所处的人类机构合作？我们假设，代理人可以通过假设存在一套大多数其他人在追求个人愿望时遵守的共同规范来实现这一目标，即使他们不知道这些规范的确切内容。通过假设共享规范，新引入的代理可以从对遵守和违规行为的观察中推断出现有群体的规范。此外，代理群体可以收敛到一套共享的规范，即使他们最初对规范的信念存在分歧。这反过来又保证了规范系统的稳定性：由于代理可以引导规范的常识，这导致规范被广泛遵守，使新进入者能够快速学习这些规范。我们在马尔可夫博弈的背景下形式化了这个框架，并通过强制性和禁止性规范的近似贝叶斯规则归纳来证明其在多智能体环境中的运行。使用我们的方法，代理人能够快速学习和维持各种合作制度，包括资源管理规范和亲社会劳动补偿，促进集体福利，同时仍然允许代理人按照自己的利益行事。</li>
</ul>

<h3>Title: A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set  Expansion and Taxonomy Expansion</h3>
<ul>
<li><strong>Authors: </strong>Yanzhen Shen, Yu Zhang, Yunyi Zhang, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13405">https://arxiv.org/abs/2402.13405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13405">https://arxiv.org/pdf/2402.13405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13405]] A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set  Expansion and Taxonomy Expansion(https://arxiv.org/abs/2402.13405)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across all three tasks.</li>
<li><strong>摘要：</strong>实体集扩展、分类法扩展和种子引导分类法构建是三个代表性任务，可用于使用新实体自动填充现有分类法。然而，以前的方法通常使用异构技术分别解决这些任务，缺乏统一的视角。为了解决这个问题，在本文中，我们从分类结构的角度确定了这些任务所需的共同关键技能——寻找“兄弟姐妹”和寻找“父母”——并提出了一个统一的分类引导的指令调整框架，以共同解决三项任务。具体来说，通过利用现有的分类法作为实体关系的丰富来源，我们利用指令调整来微调大型语言模型以生成父实体和兄弟实体。对多个基准数据集的大量实验证明了 TaxoInstruct 的有效性，它在所有三个任务中都优于特定于任务的基线。</li>
</ul>

<h3>Title: Healthcare Copilot: Eliciting the Power of General LLMs for Medical  Consultation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyao Ren, Yibing Zhan, Baosheng Yu, Liang Ding, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13408">https://arxiv.org/abs/2402.13408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13408">https://arxiv.org/pdf/2402.13408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13408]] Healthcare Copilot: Eliciting the Power of General LLMs for Medical  Consultation(https://arxiv.org/abs/2402.13408)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The copilot framework, which aims to enhance and tailor large language models (LLMs) for specific complex tasks without requiring fine-tuning, is gaining increasing attention from the community. In this paper, we introduce the construction of a Healthcare Copilot designed for medical consultation. The proposed Healthcare Copilot comprises three main components: 1) the Dialogue component, responsible for effective and safe patient interactions; 2) the Memory component, storing both current conversation data and historical patient information; and 3) the Processing component, summarizing the entire dialogue and generating reports. To evaluate the proposed Healthcare Copilot, we implement an auto-evaluation scheme using ChatGPT for two roles: as a virtual patient engaging in dialogue with the copilot, and as an evaluator to assess the quality of the dialogue. Extensive results demonstrate that the proposed Healthcare Copilot significantly enhances the capabilities of general LLMs for medical consultations in terms of inquiry capability, conversational fluency, response accuracy, and safety. Furthermore, we conduct ablation studies to highlight the contribution of each individual module in the Healthcare Copilot. Code will be made publicly available on GitHub.</li>
<li><strong>摘要：</strong>copilot 框架旨在为特定的复杂任务增强和定制大型语言模型（LLM），而不需要进行微调，正在获得社区越来越多的关注。在本文中，我们介绍了一个专为医疗咨询而设计的医疗副驾驶的构建。拟议的医疗保健副驾驶由三个主要部分组成：1）对话部分，负责有效和安全的患者互动； 2）Memory组件，存储当前对话数据和历史患者信息； 3) 处理组件，总结整个对话并生成报告。为了评估拟议的医疗保健副驾驶，我们使用 ChatGPT 实施自动评估方案，扮演两个角色：作为与副驾驶对话的虚拟患者，以及作为评估对话质量的评估者。大量结果表明，拟议的医疗保健副驾驶显着增强了普通法学硕士在医疗咨询方面的询问能力、对话流畅性、反应准确性和安全性方面的能力。此外，我们还进行消融研究，以突出医疗保健副驾驶中每个单独模块的贡献。代码将在 GitHub 上公开发布。</li>
</ul>

<h3>Title: Harnessing Large Language Models as Post-hoc Correctors</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13414">https://arxiv.org/abs/2402.13414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13414">https://arxiv.org/pdf/2402.13414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13414]] Harnessing Large Language Models as Post-hoc Correctors(https://arxiv.org/abs/2402.13414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions. Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%.</li>
<li><strong>摘要：</strong>随着机器学习 (ML) 模型规模的不断扩大并需要更高质量的训练数据，与重新训练和微调这些模型相关的费用正在迅速增加。受近年来大型语言模型（LLM）在不同领域取得的令人瞩目的成就的启发，本文深入探讨了这样一个问题：LLM 能否以最小的成本有效地提高机器学习的性能？我们表明，通过我们提出的免训练框架 LlmCorr，LLM 可以作为事后校正器，为任意 ML 模型的预测提出校正。特别是，我们通过合并数据集的标签信息和 ML 模型对验证数据集的预测来形成上下文知识数据库。利用法学硕士的上下文学习能力，我们要求法学硕士总结ML模型出错的实例以及主要预测和真实标签之间的相关性。此后，法学硕士可以转移其获得的知识，以建议对 ML 模型的预测进行更正。我们对具有挑战性的分子预测的实验结果表明，LlmCorr 将许多模型的性能提高了高达 39%。</li>
</ul>

<h3>Title: Structure Guided Prompt: Instructing Large Language Model in Multi-Step  Reasoning by Exploring Graph Structure of the Text</h3>
<ul>
<li><strong>Authors: </strong>Kewei Cheng, Nesreen K. Ahmed, Theodore Willke, Yizhou Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13415">https://arxiv.org/abs/2402.13415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13415">https://arxiv.org/pdf/2402.13415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13415]] Structure Guided Prompt: Instructing Large Language Model in Multi-Step  Reasoning by Exploring Graph Structure of the Text(https://arxiv.org/abs/2402.13415)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）擅长解决简单的推理任务，但由于一系列因素而面临更复杂的多步骤推理时，它们经常遇到困难。首先，自然语言通常包含实体之间的复杂关系，这使得在较长跨度内保持清晰的推理链具有挑战性。其次，丰富的语言多样性意味着相同的实体和关系可以使用不同的术语和结构来表达，从而使识别和建立多条信息之间的联系的任务变得复杂。图提供了一种有效的解决方案来表示富含关系信息的数据并捕获实体之间的长期依赖关系。为了利用图的潜力，我们的论文引入了结构引导提示，这是一种创新的三阶段任务无关提示框架，旨在提高法学硕士在零样本设置中的多步推理能力。该框架通过法学硕士明确地将非结构化文本转换为图表，并指示他们使用特定于任务的策略来导航该图表以制定响应。通过有效地组织信息和指导导航，它使法学硕士能够提供更准确和上下文感知的响应。我们的实验表明，该框架显着增强了法学硕士的推理能力，使他们能够在更广泛的自然语言场景中表现出色。</li>
</ul>

<h3>Title: Reward Bound for Behavioral Guarantee of Model-based Planning Agents</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu An, Xianzhong Ding, Wan Du</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13419">https://arxiv.org/abs/2402.13419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13419">https://arxiv.org/pdf/2402.13419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13419]] Reward Bound for Behavioral Guarantee of Model-based Planning Agents(https://arxiv.org/abs/2402.13419)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Recent years have seen an emerging interest in the trustworthiness of machine learning-based agents in the wild, especially in robotics, to provide safety assurance for the industry. Obtaining behavioral guarantees for these agents remains an important problem. In this work, we focus on guaranteeing a model-based planning agent reaches a goal state within a specific future time step. We show that there exists a lower bound for the reward at the goal state, such that if the said reward is below that bound, it is impossible to obtain such a guarantee. By extension, we show how to enforce preferences over multiple goals.</li>
<li><strong>摘要：</strong>近年来，人们对基于机器学习的智能体（尤其是机器人技术）的可信度产生了浓厚的兴趣，以为行业提供安全保证。为这些代理人获得行为保证仍然是一个重要问题。在这项工作中，我们专注于保证基于模型的规划代理在特定的未来时间步内达到目标状态。我们证明，目标状态下的奖励存在一个下限，因此，如果所述奖励低于该下限，则不可能获得这样的保证。通过扩展，我们展示了如何强制执行对多个目标的偏好。</li>
</ul>

<h3>Title: Explaining Relationships Among Research Papers</h3>
<ul>
<li><strong>Authors: </strong>Xiangci Li, Jessica Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13426">https://arxiv.org/abs/2402.13426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13426">https://arxiv.org/pdf/2402.13426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13426]] Explaining Relationships Among Research Papers(https://arxiv.org/abs/2402.13426)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Due to the rapid pace of research publications, keeping up to date with all the latest related papers is very time-consuming, even with daily feed tools. There is a need for automatically generated, short, customized literature reviews of sets of papers to help researchers decide what to read. While several works in the last decade have addressed the task of explaining a single research paper, usually in the context of another paper citing it, the relationship among multiple papers has been ignored; prior works have focused on generating a single citation sentence in isolation, without addressing the expository and transition sentences needed to connect multiple papers in a coherent story. In this work, we explore a feature-based, LLM-prompting approach to generate richer citation texts, as well as generating multiple citations at once to capture the complex relationships among research papers. We perform an expert evaluation to investigate the impact of our proposed features on the quality of the generated paragraphs and find a strong correlation between human preference and integrative writing style, suggesting that humans prefer high-level, abstract citations, with transition sentences between them to provide an overall story.</li>
<li><strong>摘要：</strong>由于研究出版物的更新速度很快，即使使用日常提要工具，跟上所有最新的相关论文也是非常耗时的。需要自动生成、简短、定制的论文集文献综述，以帮助研究人员决定阅读哪些内容。虽然过去十年中的几部著作都解决了解释一篇研究论文的任务，但通常是在另一篇论文引用它的背景下，多篇论文之间的关系却被忽视了；先前的工作侧重于单独生成单个引文句子，而没有解决在连贯的故事中连接多篇论文所需的说明性句子和过渡句。在这项工作中，我们探索了一种基于特征的 LLM 提示方法来生成更丰富的引文文本，以及一次生成多个引文以捕获研究论文之间的复杂关系。我们进行了专家评估，以调查我们提出的特征对生成段落质量的影响，并发现人类偏好和综合写作风格之间存在很强的相关性，这表明人类更喜欢高级的、抽象的引用，以及它们之间的过渡句子提供一个整体的故事。</li>
</ul>

<h3>Title: DrBenchmark: A Large Language Understanding Evaluation Benchmark for  French Biomedical Domain</h3>
<ul>
<li><strong>Authors: </strong>Yanis Labrak, Adrien Bazoge, Oumaima El Khettari, Mickael Rouvier, Pacome Constant dit Beaufils, Natalia Grabar, Beatrice Daille, Solen Quiniou, Emmanuel Morin, Pierre-Antoine Gourraud, Richard Dufour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13432">https://arxiv.org/abs/2402.13432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13432">https://arxiv.org/pdf/2402.13432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13432]] DrBenchmark: A Large Language Understanding Evaluation Benchmark for  French Biomedical Domain(https://arxiv.org/abs/2402.13432)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity recognition, part-of-speech tagging, question-answering, semantic textual similarity, and classification. We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.</li>
<li><strong>摘要：</strong>生物医学领域引发了人们对自然语言处理 (NLP) 领域的浓厚兴趣，该领域通过预训练语言模型 (PLM) 取得了重大进展。然而，由于不同模型的评估协议存在差异，比较这些模型已被证明具有挑战性。一个公平的解决方案是将不同的下游任务聚合到一个基准中，以便从不同的角度评估 PLM 的内在质量。尽管仍仅限于少数语言，但这一举措已在生物医学领域开展，特别是英语和中文。这种限制阻碍了对最新法国生物医学模型的评估，因为它们要么使用非标准化协议对最少数量的任务进行评估，要么使用一般下游任务进行评估。为了弥补这一研究差距并考虑到法语的独特敏感性，我们推出了第一个公开的法语生物医学语言理解基准，称为 DrBenchmark。它包含 20 种多样化的任务，包括命名实体识别、词性标注、问答、语义文本相似度和分类。我们根据一般数据和生物医学特定数据评估 8 个最先进的预训练掩蔽语言模型 (MLM)，以及英语特定的 MLM，以评估其跨语言能力。我们的实验表明，没有一个模型能够胜任所有任务，而通才模型有时仍然具有竞争力。</li>
</ul>

<h3>Title: A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and  Probabilistic Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Chitra Subramanian, Miao Liu, Naweed Khan, Jonathan Lenchner, Aporva Amarnath, Sarathkrishna Swaminathan, Ryan Riegel, Alexander Gray</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13440">https://arxiv.org/abs/2402.13440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13440">https://arxiv.org/pdf/2402.13440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13440]] A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and  Probabilistic Decision Making(https://arxiv.org/abs/2402.13440)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Multi-agent reinforcement learning (MARL) is well-suited for runtime decision-making in optimizing the performance of systems where multiple agents coexist and compete for shared resources. However, applying common deep learning-based MARL solutions to real-world problems suffers from issues of interpretability, sample efficiency, partial observability, etc. To address these challenges, we present an event-driven formulation, where decision-making is handled by distributed co-operative MARL agents using neuro-symbolic methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN) framework serves as a function approximator for the RL, to train a rules-based policy that is both logical and interpretable by construction. To enable decision-making under uncertainty and partial observability, we developed a novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural Networks (PLNN), which combines the capabilities of logical reasoning with probabilistic graphical models. In PLNN, the upward/downward inference strategy, inherited from LNN, is coupled with belief bounds by setting the activation function for the logical operator associated with each neural network node to a probability-respecting generalization of the Fr\'echet inequalities. These PLNN nodes form the unifying element that combines probabilistic logic and Bayes Nets, permitting inference for variables with unobserved states. We demonstrate our contributions by addressing key MARL challenges for power sharing in a system-on-chip application.</li>
<li><strong>摘要：</strong>多智能体强化学习 (MARL) 非常适合运行时决策，可优化多个智能体共存并竞争共享资源的系统性能。然而，将常见的基于深度学习的 MARL 解决方案应用于现实世界的问题会遇到可解释性、样本效率、部分可观察性等问题。为了解决这些挑战，我们提出了一种事件驱动的公式，其中决策由分布式处理使用神经符号方法的合作 MARL 代理。最近引入的神经符号逻辑神经网络 (LNN) 框架充当强化学习的函数逼近器，用于训练基于规则的策略，该策略既符合逻辑又可通过构造进行解释。为了在不确定性和部分可观察性下做出决策，我们开发了一种新颖的概率神经符号框架，即概率逻辑神经网络（PLNN），它将逻辑推理与概率图形模型的功能结合起来。在 PLNN 中，继承自 LNN 的向上/向下推理策略通过将与每个神经网络节点关联的逻辑运算符的激活函数设置为 Fr'echet 不等式的概率推广，与置信边界相结合。这些 PLNN 节点形成了结合概率逻辑和贝叶斯网络的统一元素，允许对具有未观察到的状态的变量进行推断。我们通过解决片上系统应用中功率共享的关键 MARL 挑战来展示我们的贡献。</li>
</ul>

<h3>Title: Large Language Models for Data Annotation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13446">https://arxiv.org/abs/2402.13446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13446">https://arxiv.org/pdf/2402.13446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13446]] Large Language Models for Data Annotation: A Survey(https://arxiv.org/abs/2402.13446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at \url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.</li>
<li><strong>摘要：</strong>数据注释是用相关信息对原始数据进行标记或标记，这对于提高机器学习模型的效率至关重要。然而，该过程是劳动密集型且昂贵的。以 GPT-4 为代表的高级大型语言模型 (LLM) 的出现为彻底改变和自动化复杂的数据注释过程提供了前所未有的机会。虽然现有的调查广泛涵盖了法学硕士架构、培训和一般应用，但本文特别关注它们在数据注释方面的特定用途。这项调查致力于三个核心方面：基于 LLM 的数据注释、评估 LLM 生成的注释以及使用 LLM 生成的注释进行学习。此外，本文还对采用法学硕士进行数据注释的方法进行了深入的分类，对结合法学硕士生成的注释的模型的学习策略进行了全面回顾，并详细讨论了与使用法学硕士进行数据注释相关的主要挑战和限制。作为一个关键指南，这项调查旨在指导研究人员和从业者探索最新法学硕士在数据注释方面的潜力，促进这一关键领域的未来进步。我们在 \url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git} 提供了全面的论文列表。</li>
</ul>

<h3>Title: ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance</h3>
<ul>
<li><strong>Authors: </strong>Liwen Sun, Abhineet Agarwal, Aaron Kornblith, Bin Yu, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13448">https://arxiv.org/abs/2402.13448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13448">https://arxiv.org/pdf/2402.13448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13448]] ED-Copilot: Reduce Emergency Department Wait Time with Language Model  Diagnostic Assistance(https://arxiv.org/abs/2402.13448)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learning to minimize ED wait time and maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours. Ablation studies demonstrate the importance of model scale and use of a bio-medical language model. Further analyses reveal the necessity of personalized laboratory test suggestions for diagnosing patients with severe cases, as well as the potential of ED-Copilot in providing ED clinicians with informative laboratory test recommendations. Our code is available at https://github.com/cxcscmu/ED-Copilot.</li>
<li><strong>摘要：</strong>在急诊科 (ED)，患者在诊断前接受分诊和多项实验室检查。这个过程非常耗时，并且会导致急诊室拥挤，从而显着影响患者死亡率、医疗错误、员工倦怠等。这项工作提出了（时间）成本效益高的诊断辅助，探索人工智能 (AI) 系统在协助急诊室方面的潜力临床医生能够做出省时、准确的诊断。利用公开的患者数据，我们与急诊科临床医生合作制定 MIMIC-ED-Assist，这是一个基准，用于衡量人工智能系统建议实验室测试的能力，以最大限度地减少急诊室等待时间，同时正确预测死亡等关键结果。我们开发了 ED-Copilot，它可以依次建议患者特定的实验室测试并做出诊断预测。 ED-Copilot 使用预先训练的生物医学语言模型对患者信息和强化学习进行编码，以最大限度地减少 ED 等待时间并最大限度地提高关键结果的预测准确性。在 MIMIC-ED-Assist 上，ED-Copilot 提高了基线的预测准确性，同时将平均等待时间从 4 小时减少到 2 小时。消融研究证明了模型规模和生物医学语言模型使用的重要性。进一步的分析揭示了个性化实验室测试建议对于诊断重症患者的必要性，以及 ED-Copilot 在为 ED 临床医生提供信息丰富的实验室测试建议方面的潜力。我们的代码可在 https://github.com/cxcscmu/ED-Copilot 获取。</li>
</ul>

<h3>Title: CAMELoT: Towards Large Language Models with Training-Free Consolidated  Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, Rogerio Feris</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13449">https://arxiv.org/abs/2402.13449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13449">https://arxiv.org/pdf/2402.13449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13449]] CAMELoT: Towards Large Language Models with Training-Free Consolidated  Associative Memory(https://arxiv.org/abs/2402.13449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks. This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations.</li>
<li><strong>摘要：</strong>由于内存和运行时成本较高，大型语言模型 (LLM) 很难处理长输入序列。记忆增强模型已成为解决这一问题的有希望的解决方案，但当前的方法受到有限的记忆容量的阻碍，并且需要昂贵的重新训练才能与新的法学硕士集成。在这项工作中，我们引入了一个关联记忆模块，它可以耦合到任何预训练（冻结）的基于注意力的 LLM，无需重新训练，使其能够处理任意长的输入序列。与以前的方法不同，我们的关联内存模块将各个标记的表示合并到非参数分布模型中，通过适当平衡传入数据的新颖性和新近性来动态管理。通过从这种整合的联想记忆中检索信息，与在标准基准上评估的其他基线相比，基础法学硕士可以在长上下文建模中显着降低困惑度（在 Arxiv 上高达 29.7%）。这种架构，我们称之为 CAMELoT（统一关联记忆增强型长变换器），即使在 128 个标记的微小上下文窗口中也能展现出卓越的性能，并且还可以通过更大的演示集来改进上下文学习。</li>
</ul>

<h3>Title: Learning to Poison Large Language Models During Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13459">https://arxiv.org/abs/2402.13459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13459">https://arxiv.org/pdf/2402.13459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13459]] Learning to Poison Large Language Models During Instruction Tuning(https://arxiv.org/abs/2402.13459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\%. Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding LLMs against these more sophisticated attacks. The source code can be found on this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的出现标志着语言处理和推理能力方面取得了重大成就。尽管取得了进步，法学硕士仍面临数据中毒攻击的漏洞，对手会在训练数据中插入后门触发器，以出于恶意目的操纵输出。这项工作通过设计一种专为利用指令调整过程而定制的新数据中毒攻击，进一步识别了法学硕士中的其他安全风险。我们提出了一种新颖的梯度引导后门触发学习方法，可以有效识别对抗性触发因素，确保逃避传统防御的检测，同时保持内容完整性。通过对各种法学硕士和任务的实验验证，我们的策略在损害模型输出方面表现出很高的成功率；仅中毒 4,000 个指令调整样本中的 1\% 会导致性能下降率 (PDR) 约为 80\%。我们的工作强调了对数据中毒攻击采取更强有力的防御措施的必要性，并为保护法学硕士免受这些更复杂的攻击提供了见解。源代码可以在此 GitHub 存储库中找到：https://github.com/RookieZxy/GBTL/blob/main/README.md。</li>
</ul>

<h3>Title: Potential and Challenges of Model Editing for Social Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yan, Futing Wang, Yafu Li, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13462">https://arxiv.org/abs/2402.13462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13462">https://arxiv.org/pdf/2402.13462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13462]] Potential and Challenges of Model Editing for Social Debiasing(https://arxiv.org/abs/2402.13462)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.</li>
<li><strong>摘要：</strong>在大量语料库上训练的大型语言模型（LLM）不可避免地会出现刻板印象偏差。通过微调来减轻这些偏差可能既昂贵又需要数据。模型编辑方法侧重于以事后方式修改法学硕士，在解决去偏问题方面具有巨大潜力。然而，它缺乏全面​​的研究来促进内部和外部模型编辑方法，支持各种偏差类型，以及了解将编辑方法应用于刻板去偏差的利弊。为了缩小这一差距，我们仔细地将社会去偏见纳入一个编辑问题，并对七种现有的模型编辑算法进行刻板去偏见（即去偏见编辑）的基准测试。我们在三种情况下的研究结果揭示了去偏见编辑的潜力和挑战：（1）现有的模型编辑方法可以有效地保留知识并减轻偏见，而从编辑的句子到语义等效句子的去偏见效应的泛化是有限的。（2）顺序编辑凸显了 SERAC 的稳健性（Mitchell et al. 2022b），而内部编辑方法则随着编辑次数的增加而退化。 (3) 模型编辑算法实现了对同一类型内和不同类型中看不见的偏差的泛化。根据这些发现，我们进一步提出了两种简单但有效的方法来改进去偏差编辑，并通过实验证明了所提出方法的有效性。</li>
</ul>

<h3>Title: RefuteBench: Evaluating Refuting Instruction-Following for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yan, Yun Luo, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13463">https://arxiv.org/abs/2402.13463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13463">https://arxiv.org/pdf/2402.13463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13463]] RefuteBench: Evaluating Refuting Instruction-Following for Large  Language Models(https://arxiv.org/abs/2402.13463)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses. We further propose a recall-and-repeat prompts as a simple and effective way to enhance the model's responsiveness to feedback.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的应用范围日益扩大。在实际使用中，用户可能会根据模型的输出提供反馈，希望有一个响应式模型能够根据他们的反馈完成响应。该模型是否能够适当地响应用户的反驳反馈并始终如一地贯彻执行尚未得到彻底分析。鉴于此，本文提出了一个综合基准RefuteBench，涵盖问答、机器翻译和电子邮件撰写等任务。该评估旨在评估模型是否能够积极接受反驳指令形式的反馈，以及是否能够在整个对话过程中始终坚持用户需求。我们对众多法学硕士进行了评估，发现法学硕士很顽固，即表现出倾向于内部知识，往往不遵守用户反馈。此外，随着对话长度的增加，模型会逐渐忘记用户所说的反馈并回滚到自己的响应。我们进一步提出召回和重复提示作为增强模型对反馈的响应能力的简单有效的方法。</li>
</ul>

<h3>Title: STENCIL: Submodular Mutual Information Based Weak Supervision for  Cold-Start Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Nathan Beck, Adithya Iyer, Rishabh Iyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13468">https://arxiv.org/abs/2402.13468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13468">https://arxiv.org/pdf/2402.13468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13468]] STENCIL: Submodular Mutual Information Based Weak Supervision for  Cold-Start Active Learning(https://arxiv.org/abs/2402.13468)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on multiple text classification datasets over common active learning methods within the class-imbalanced cold-start setting.</li>
<li><strong>摘要：</strong>随着 NLP 应用程序中预训练模型的监督微调越来越受欢迎，需要更大的注释数据语料库，特别是随着大型语言模型中参数数量的增加。主动学习尝试挖掘和注释未标记的实例以最大程度地快速提高模型性能，是降低注释成本的常见选择；然而，大多数方法通常忽略类别不平衡，并且要么假设访问初始注释数据，要么在改进稀有类别之前需要多轮主动学习选择。我们提出了 STENCIL，它利用一组文本样本和最近提出的子模块互信息来选择一组弱标记的稀有类实例，然后由注释器对其进行强标记。我们表明，与类不平衡冷分类中常见的主动学习方法相比，STENCIL 在多个文本分类数据集上将整体准确度提高了 $10\%-24\%$，稀有类 F-1 分数提高了 $17\%-40\%$。开始设置。</li>
</ul>

<h3>Title: How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction?</h3>
<ul>
<li><strong>Authors: </strong>Aviv Brokman, Ramakanth Kavuluru</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13470">https://arxiv.org/abs/2402.13470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13470">https://arxiv.org/pdf/2402.13470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13470]] How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction?(https://arxiv.org/abs/2402.13470)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs</li>
<li><strong>摘要：</strong>在一般 NLP 领域开发的尖端技术通常随后应用于高价值、数据丰富的生物医学领域。在过去的几年里，生成语言模型（LM）、指令微调和小样本学习成为 NLP 研究的焦点。因此，在生物医学语料库上预训练的生成式语言模型已经激增，并且也尝试了生物医学指令微调，所有这些都希望领域特异性能够提高下游任务的性能。鉴于训练此类模型所付出的巨大努力，我们研究了它们在关系提取的关键生物医学 NLP 任务中有何益处（如果有的话）。具体来说，我们解决两个问题：（1）在生物医学语料库上训练的语言模型是否优于在通用领域语料库上训练的语言模型？ (2) 在生物医学数据集上微调的模型指令是否优于在各种数据集上微调的模型指令或简单预训练的模型指令？我们使用现有的 LM 来解决这些问题，并在四个数据集上进行测试。令人惊讶的结果是，通用领域模型通常优于生物医学领域模型。然而，生物医学指令微调将性能提高到与一般指令微调相似的程度，尽管指令数量少了几个数量级。我们的研究结果表明，将研究工作集中在通用语言模型的大规模生物医学指令微调上可能比构建特定领域的生物医学语言模型更富有成效</li>
</ul>

<h3>Title: Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks</h3>
<ul>
<li><strong>Authors: </strong>Minju Seo, Jinheon Baek, James Thorne, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13482">https://arxiv.org/abs/2402.13482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13482">https://arxiv.org/pdf/2402.13482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13482]] Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks(https://arxiv.org/abs/2402.13482)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing LLM-powered data augmentation baselines.</li>
<li><strong>摘要：</strong>尽管最近的语言模型在各种任务上取得了巨大成功，但在可用训练数据有限的资源匮乏环境中，它们的性能严重下降。许多现有的工作通过从训练数据生成合成数据，然后在其上训练模型来解决这个问题，最近使用了大型语言模型（LLM）。然而，在资源匮乏的环境中，用于数据增强的种子数据样本量非常小，这使得生成的样本不是最优的且多样性较差。为了应对这一挑战，我们提出了一种新颖的方法，通过结合其他数据集中的大量示例以及给定的训练数据来增强训练数据。具体来说，我们首先根据与给定种子数据的相似性从其他数据集中检索相关实例，例如它们的输入输出对或上下文，然后提示法学硕士使用原始数据内部和外部的上下文信息生成新样本。检索到的样本。这种方法可以确保生成的数据不仅相关，而且比单独使用有限的种子数据所能实现的数据更加多样化。我们在训练和测试时数据增强场景的低资源设置下，在多个数据集上验证了我们提出的检索增强数据增强（RADA）框架，该框架的性能优于现有的 LLM 支持的数据增强基线。</li>
</ul>

<h3>Title: ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13485">https://arxiv.org/abs/2402.13485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13485">https://arxiv.org/pdf/2402.13485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13485]] ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel  Decoding(https://arxiv.org/abs/2402.13485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the computation and parallelism of the verification phase in real-time and maximize the overall efficiency across different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD consistently outperforms existing decoding algorithms by 1.1-3.2x.</li>
<li><strong>摘要：</strong>生成式大语言模型 (LLM) 的最新进展显着提高了自然语言处理任务的性能。然而，它们的效率受到自回归令牌生成的固有限制的阻碍。虽然已经提出使用令牌树验证的并行解码（例如 Medusa）来提高解码并行性和效率，但由于其独立的令牌预测方法，它经常难以维持上下文关系，并且会产生大量的验证开销，特别是在树大小较大和批处理的情况下。在本文中，我们提出了ProPD，一种基于动态令牌树修剪和生成的高效LLM并行解码框架。 ProPD具有先进的早期剪枝机制，可有效消除无希望的令牌序列，从而提高验证效率。此外，它还引入了动态令牌树生成算法，以实时平衡验证阶段的计算和并行性，并最大限度地提高不同批量大小、序列长度和任务等的整体效率。数据集、LLM 和批量大小，并证明 ProPD 始终优于现有解码算法 1.1-3.2 倍。</li>
</ul>

<h3>Title: Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval  Augmentation to Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13492">https://arxiv.org/abs/2402.13492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13492">https://arxiv.org/pdf/2402.13492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13492]] Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval  Augmentation to Language Models(https://arxiv.org/abs/2402.13492)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LM) 表现出了卓越的性能，但当查询超出预训练记忆范围的信息时，它们在提供准确响应方面遇到了挑战。尽管用相关的外部信息来增强它们可以缓解这些问题，但不考虑检索的必要性可能会对整体性能产生不利影响。先前的研究主要集中于研究实体如何影响语言模型中的检索模型和知识回忆，而其他方面相对未得到探索。在这项工作中，我们的目标是通过探索实体和关系组合的影响来提供更详细、以事实为中心的分析。为了实现这一点，我们构建了一个新的问答（QA）数据集，称为 WiTQA（维基百科三重问答）。该数据集包含有关不同受欢迎程度的实体和关系的问题，每个问题都附有支持段落。我们对不同的 LM 和检索器进行的广泛实验表明，从以事实为中心的流行度的角度来看，检索并不能始终如一地增强 LM。为了证实早期的发现，我们观察到较大的 LM 在回忆流行事实方面表现出色。然而，与检索器相比，它们在处理不常见的实体关系对时尤其遇到困难。有趣的是，它们可以有效地保留不常见实体的流行关系。我们通过自适应检索系统展示了更细粒度的度量和见解的有效性，该系统根据问题中实体和关系的频率有选择地采用检索和回忆。</li>
</ul>

<h3>Title: GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13494">https://arxiv.org/abs/2402.13494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13494">https://arxiv.org/pdf/2402.13494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13494]] GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient  Analysis(https://arxiv.org/abs/2402.13494)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting unsafe prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 面临不安全提示的威胁。现有的检测不安全提示的方法主要是在线审核 API 或微调的 LLM。然而，这些策略通常需要广泛且资源密集的数据收集和培训过程。在这项研究中，我们提出了 GradSafe，它通过仔细检查法学硕士中安全关键参数的梯度来有效检测不安全提示。我们的方法基于一个关键的观察：法学硕士因不安全提示而损失的梯度与合规响应相结合，在某些安全关键参数上表现出类似的模式。相反，安全提示会导致明显不同的渐变模式。在此观察的基础上，GradSafe 分析提示的梯度（与合规性响应配对），以准确检测不安全的提示。我们表明，GradSafe 在未经进一步训练的情况下应用于 Llama-2，在检测不安全提示方面优于 Llama Guard，尽管它使用大型数据集进行了广泛的微调。正如我们对 ToxicChat 和 XSTest 的评估所证明的那样，这种卓越的性能在零样本和适应场景中都是一致的。源代码可在 https://github.com/xyq7/GradSafe 获取。</li>
</ul>

<h3>Title: The Lay Person's Guide to Biomedicine: Orchestrating Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Zheheng Luo, Qianqian Xie, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13498">https://arxiv.org/abs/2402.13498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13498">https://arxiv.org/pdf/2402.13498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13498]] The Lay Person's Guide to Biomedicine: Orchestrating Large Language  Models(https://arxiv.org/abs/2402.13498)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Automated lay summarisation (LS) aims to simplify complex technical documents into a more accessible format to non-experts. Existing approaches using pre-trained language models, possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation. Moreover, automated methods that can effectively assess the `layness' of generated summaries are lacking. Recently, large language models (LLMs) have demonstrated a remarkable capacity for text simplification, background information generation, and text evaluation. This has motivated our systematic exploration into using LLMs to generate and evaluate lay summaries of biomedical articles. We propose a novel \textit{Explain-then-Summarise} LS framework, which leverages LLMs to generate high-quality background knowledge to improve supervised LS. We also evaluate the performance of LLMs for zero-shot LS and propose two novel LLM-based LS evaluation metrics, which assess layness from multiple perspectives. Finally, we conduct a human assessment of generated lay summaries. Our experiments reveal that LLM-generated background information can support improved supervised LS. Furthermore, our novel zero-shot LS evaluation metric demonstrates a high degree of alignment with human preferences. We conclude that LLMs have an important part to play in improving both the performance and evaluation of LS methods.</li>
<li><strong>摘要：</strong>自动外行摘要 (LS) 旨在将复杂的技术文档简化为非专家更容易理解的格式。使用预先训练的语言模型（可能通过外部背景知识进行增强）的现有方法往往难以有效简化和解释。此外，缺乏能够有效评估生成摘要的“外行性”的自动化方法。最近，大型语言模型（LLM）在文本简化、背景信息生成和文本评估方面表现出了卓越的能力。这促使我们系统地探索使用法学硕士来生成和评估生物医学文章的简明摘要。我们提出了一种新颖的 \textit{Explain-then-Summarise} LS 框架，该框架利用 LLM 生成高质量的背景知识来改进监督 LS。我们还评估了零样本 LS 的法学硕士的性能，并提出了两种基于法学硕士的新型 LS 评估指标，从多个角度评估外行性。最后，我们对生成的外行摘要进行人工评估。我们的实验表明，LLM 生成的背景信息可以支持改进的监督 LS。此外，我们新颖的零样本 LS 评估指标表明与人类偏好高度一致。我们的结论是，法学硕士在提高 LS 方法的性能和评估方面可以发挥重要作用。</li>
</ul>

<h3>Title: From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers</h3>
<ul>
<li><strong>Authors: </strong>M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, Samet Oymak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13512">https://arxiv.org/abs/2402.13512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13512">https://arxiv.org/pdf/2402.13512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13512]] From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers(https://arxiv.org/abs/2402.13512)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from an initial prompt. We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by self-attention collapses into sampling a limited subset of tokens due to its non-mixing nature. This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text. In summary, the equivalence to CCMC provides a simple but powerful framework to study self-attention and its properties.</li>
<li><strong>摘要：</strong>现代语言模型依赖于 Transformer 架构和注意力机制来执行语言理解和文本生成。在这项工作中，我们研究从一组提示和从模型中采样的相关输出数据中学习单层自注意力模型。我们首先在自注意力机制和马尔可夫模型之间建立精确的映射：向模型输入提示，根据上下文条件马尔可夫链（CCMC）对输出标记进行采样，该链对基本马尔可夫链的转移矩阵进行加权。此外，合并位置编码会导致转移概率的位置相关缩放。在此形式主义的基础上，我们为即时分布开发了可识别性/覆盖条件，以保证一致的估计并在 IID 样本下建立样本复杂性保证。最后，我们研究了从初始提示生成的单个输出轨迹中学习的问题。我们描述了一种有趣的赢家通吃现象，其中通过自注意力实现的生成过程由于其非混合性质而崩溃为对有限的令牌子集进行采样。这为现代法学硕士生成重复文本的趋势提供了数学解释。总之，CCMC 的等价物提供了一个简单但强大的框架来研究自注意力及其属性。</li>
</ul>

<h3>Title: Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer  for Compositional Unknown Questions</h3>
<ul>
<li><strong>Authors: </strong>Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Guanhua Chen, Huimin Wang, Kam-fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13514">https://arxiv.org/abs/2402.13514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13514">https://arxiv.org/pdf/2402.13514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13514]] Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer  for Compositional Unknown Questions(https://arxiv.org/abs/2402.13514)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines.</li>
<li><strong>摘要：</strong>检索然后阅读和生成然后阅读是开放域问答中处理未知和已知问题的两种典型解决方案，前者检索必要的外部知识，后者提示大型语言模型生成编码的内部已知知识在参数中。然而，以前的作品很少考虑由几个已知或未知子问题组成的组合未知问题。因此，简单的二元分类（已知或未知）变得次优且效率低下，因为它会对每个组成的未知问题过度调用外部检索。为此，我们提出了第一个组合未知问答数据集（CuQA），并引入了自分而治之（Self-DC）框架，使法学硕士能够按需自适应地调用不同的方法，从而获得更好的性能和效率。两个数据集（CuQA 和 FreshQA）上的实验结果表明，与几个强大的基线相比，Self-DC 可以以更少的检索时间实现可比甚至更好的性能。</li>
</ul>

<h3>Title: ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13516">https://arxiv.org/abs/2402.13516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13516">https://arxiv.org/pdf/2402.13516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13516]] ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models(https://arxiv.org/abs/2402.13516)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along sine curves in multiple stages. This can enhance activation sparsity and alleviate performance degradation by avoiding radical shifts in activation distribution. With ProSparse, we obtain high sparsity of 89.32% and 88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable performance to their original Swish-activated versions. Our inference acceleration experiments further demonstrate the practical acceleration brought by higher activation sparsity.</li>
<li><strong>摘要：</strong>激活稀疏性是指激活输出中存在相当多的弱贡献元素。作为使用 ReLU 激活函数的模型的普遍属性，它已被证明是提高模型推理效率的有前途的范例。然而，大多数大型语言模型（LLM）采用没有内在激活稀疏性的激活函数（例如 GELU 和 Swish）。最近的一些努力探索引入ReLU或其变体作为替代激活函数来帮助LLM实现激活稀疏性和推理加速，但很少有人能够同时获得高稀疏性和可比较的模型性能。本文介绍了一种名为“ProSparse”的有效稀疏化方法，以在不降低模型性能的情况下推动 LLM 获得更高的激活稀疏度。具体来说，ProSparse用ReLU替代LLM的激活函数后，采用渐进稀疏正则化，因子沿多阶段正弦曲线平滑增加。这可以通过避免激活分布的根本变化来增强激活稀疏性并减轻性能下降。通过 ProSparse，我们分别为 LLaMA2-7B 和 LLaMA2-13B 获得了 89.32% 和 88.80% 的高稀疏度，实现了与原始 Swish 激活版本相当的性能。我们的推理加速实验进一步证明了更高的激活稀疏性带来的实际加速。</li>
</ul>

<h3>Title: Round Trip Translation Defence against Large Language Model Jailbreaking  Attacks</h3>
<ul>
<li><strong>Authors: </strong>Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13517">https://arxiv.org/abs/2402.13517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13517">https://arxiv.org/pdf/2402.13517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13517]] Round Trip Translation Defence against Large Language Model Jailbreaking  Attacks(https://arxiv.org/abs/2402.13517)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 很容易受到人类可解释的社会工程攻击，但需要 LLM 具有高水平的理解能力才能抵御。现有的防御措施最多只能缓解其中不到一半的攻击。为了解决这个问题，我们提出了往返翻译（RTT）方法，这是第一个专门为防御法学硕士的社会工程攻击而设计的算法。 RTT 解释了对抗性提示并概括了所传达的想法，使法学硕士更容易发现诱发的有害行为。这种方法用途广泛、轻量级，并且可以转移到不同的法学硕士。我们的防御成功缓解了超过 70% 的即时自动迭代细化 (PAIR) 攻击，这是目前据我们所知最有效的防御。我们也是第一个尝试缓解 MathsAttack 的人，并将其攻击成功率降低了近 40%。我们的代码可在 https://github.com/Cancanxxx/Round_Trip_Translation_Defence 上公开获取</li>
</ul>

<h3>Title: RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal  State</h3>
<ul>
<li><strong>Authors: </strong>Takashi Kodama, Hirokazu Kiyomaru, Yin Jou Huang, Sadao Kurohashi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13522">https://arxiv.org/abs/2402.13522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13522">https://arxiv.org/pdf/2402.13522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13522]] RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal  State(https://arxiv.org/abs/2402.13522)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Humans pay careful attention to the interlocutor's internal state in dialogues. For example, in recommendation dialogues, we make recommendations while estimating the seeker's internal state, such as his/her level of knowledge and interest. Since there are no existing annotated resources for the analysis, we constructed RecMind, a Japanese movie recommendation dialogue dataset with annotations of the seeker's internal state at the entity level. Each entity has a subjective label annotated by the seeker and an objective label annotated by the recommender. RecMind also features engaging dialogues with long seeker's utterances, enabling a detailed analysis of the seeker's internal state. Our analysis based on RecMind reveals that entities that the seeker has no knowledge about but has an interest in contribute to recommendation success. We also propose a response generation framework that explicitly considers the seeker's internal state, utilizing the chain-of-thought prompting. The human evaluation results show that our proposed method outperforms the baseline method in both consistency and the success of recommendations.</li>
<li><strong>摘要：</strong>人类在对话中会密切关注对话者的内部状态。例如，在推荐对话中，我们在估计寻求者的内部状态（例如他/她的知识水平和兴趣水平）的同时提出推荐。由于没有现有的用于分析的注释资源，我们构建了 RecMind，一个日本电影推荐对话数据集，在实体级别对搜索者的内部状态进行了注释。每个实体都有一个由搜索者注释的主观标签和一个由推荐者注释的客观标签。 RecMind 还具有与长期探索者的话语进行对话的功能，可以对探索者的内部状态进行详细分析。我们基于 RecMind 的分析表明，搜索者不了解但感兴趣的实体有助于推荐成功。我们还提出了一个响应生成框架，利用思想链提示明确考虑寻求者的内部状态。人类评估结果表明，我们提出的方法在一致性和推荐成功度方面都优于基线方法。</li>
</ul>

<h3>Title: OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Meng Xu, Shuo Wang, Liner Yang, Haoyu Wang, Zhenghao Liu, Cunliang Kong, Yun Chen, Yang Liu, Maosong Sun, Erhong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13524">https://arxiv.org/abs/2402.13524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13524">https://arxiv.org/pdf/2402.13524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13524]] OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models(https://arxiv.org/abs/2402.13524)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to automatically score different model outputs, which is shown closely related to human evaluation. We evaluate several representative multilingual LLMs on the proposed OMGEval, which we believe will provide a valuable reference for the community to further understand and improve the multilingual capability of LLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.</li>
<li><strong>摘要：</strong>现代大语言模型（LLM）通常应该使来自世界各地不同文化背景的个人受益。然而，最近针对法学硕士的高级生成评估基准主要集中在英语上。为此，我们推出了 OMGEval，这是第一个开源多语言生成测试集，可以评估法学硕士使用不同语言的能力。对于每种语言，OMGEval 提供了 804 个开放式问题，涵盖了 LLM 的广泛重要能力，例如常识、逻辑推理等。每个问题都经过人工注释者的严格验证。值得注意的是，为了充分体现LLM在不同文化背景下的兼容性，我们对每种非英语语言进行了本地化。具体来说，当前版本的 OMGEval 包括 5 种语言（即 Zh、Ru、Fr、Es、Ar）。继AlpacaEval之后，我们使用GPT-4作为裁决器来自动对不同的模型输出进行评分，这与人类评估密切相关。我们在拟议的OMGEval上评估了几位具有代表性的多语言LLM，我们相信这将为社会进一步了解和提高LLM的多语言能力提供有价值的参考。 OMGEval 可在 https://github.com/blcuicall/OMGEval 获取。</li>
</ul>

<h3>Title: Backdoor Attacks on Dense Passage Retrievers for Disseminating  Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, Sinno Jialin Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13532">https://arxiv.org/abs/2402.13532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13532">https://arxiv.org/pdf/2402.13532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13532]] Backdoor Attacks on Dense Passage Retrievers for Disseminating  Misinformation(https://arxiv.org/abs/2402.13532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Dense retrievers and retrieval-augmented language models have been widely used in various NLP applications. Despite being designed to deliver reliable and secure outcomes, the vulnerability of retrievers to potential attacks remains unclear, raising concerns about their security. In this paper, we introduce a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisement, through a retrieval system. To achieve this, we propose a perilous backdoor attack triggered by grammar errors in dense passage retrieval. Our approach ensures that attacked models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries. Extensive experiments demonstrate the effectiveness and stealthiness of our proposed attack method. When a user query is error-free, our model consistently retrieves accurate information while effectively filtering out misinformation from the top-k results. However, when a query contains grammar errors, our system shows a significantly higher success rate in fetching the targeted content.</li>
<li><strong>摘要：</strong>密集检索器和检索增强语言模型已广泛应用于各种 NLP 应用中。尽管旨在提供可靠和安全的结果，但检索器对潜在攻击的脆弱性仍不清楚，这引发了人们对其安全性的担忧。在本文中，我们介绍了一种新颖的场景，攻击者旨在通过检索系统秘密传播有针对性的错误信息，例如仇恨言论或广告。为了实现这一目标，我们提出了一种由密集段落检索中的语法错误触发的危险后门攻击。我们的方法确保受攻击的模型可以正常运行标准查询，但当用户在查询中无意中犯语法错误时，会被操纵以返回攻击者指定的段落。大量的实验证明了我们提出的攻击方法的有效性和隐蔽性。当用户查询没有错误时，我们的模型能够一致地检索准确的信息，同时有效地从前 k 个结果中过滤掉错误信息。然而，当查询包含语法错误时，我们的系统在获取目标内容方面显示出明显更高的成功率。</li>
</ul>

<h3>Title: FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models  for Financial Applications with High-Performance Computing</h3>
<ul>
<li><strong>Authors: </strong>Xiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, Anwar Walid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13533">https://arxiv.org/abs/2402.13533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13533">https://arxiv.org/pdf/2402.13533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13533]] FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models  for Financial Applications with High-Performance Computing(https://arxiv.org/abs/2402.13533)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetune LLMs for financial applications. We replace one conventional linear layer of the transformer structure with two narrower linear layers, which allows us to reduce the number of parameters by several orders of magnitude. By quantizing the parameters into low precision (8-bit and 4-bit), the memory consumption of the resulting model is further reduced. Compared with existing LLMs, our methods achieve a speedup of 1.3X and a model compression ratio of 2.64X for pretaining without accuracy drop. For finetuning, our methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks and financial tasks, respectively, and GPU memory consumption ratio of 6.3X. The sizes of our models are smaller than 0.59 GB, allowing inference on a smartphone.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是计算密集型的。计算工作量和内存占用量随维度（层宽度）呈二次方增长。 LLM 的大部分参数来自变压器结构的线性层，并且冗余度很高。这些线性层贡献了超过 80% 的计算工作量和 99% 的模型大小。为了有效地预训练和微调 LLM，需要解决三个主要挑战：1）减少线性层的冗余； 2）减少GPU内存占用； 3）使用分布式训练时提高GPU利用率。先前的方法，例如 LoRA 和 QLoRA，分别利用低秩矩阵和量化来减少可训练参数的数量和模型大小。然而，生成的模型仍然消耗大量 GPU 内存。在本文中，我们提出了基于 GPU 的高性能方法，该方法利用低秩结构来预训练和微调金融应用程序的 LLM。我们用两个较窄的线性层替换变压器结构的一个传统线性层，这使我们能够将参数数量减少几个数量级。通过将参数量化为低精度（8 位和 4 位），所得模型的内存消耗进一步减少。与现有的 LLM 相比，我们的方法在预保留方面实现了 1.3 倍的加速和 2.64 倍的模型压缩比，且精度没有下降。对于微调，我们的方法在一般任务和金融任务中的平均准确率分别提高了 6.3% 和 24.0%，GPU 内存消耗比率为 6.3 倍。我们模型的大小小于 0.59 GB，允许在智能手机上进行推理。</li>
</ul>

<h3>Title: ARL2: Aligning Retrievers for Black-box Large Language Models via  Self-guided Adaptive Relevance Labeling</h3>
<ul>
<li><strong>Authors: </strong>Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13542">https://arxiv.org/abs/2402.13542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13542">https://arxiv.org/pdf/2402.13542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13542]] ARL2: Aligning Retrievers for Black-box Large Language Models via  Self-guided Adaptive Relevance Labeling(https://arxiv.org/abs/2402.13542)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.</li>
<li><strong>摘要：</strong>检索增强生成通过合并来自外部知识源的相关信息来增强大型语言模型（LLM）。这使得法学硕士能够适应特定领域并减轻知识密集型任务中的幻觉。然而，由于其单独的训练过程和法学硕士的黑盒性质，现有的检索器经常与法学硕士不一致。为了应对这一挑战，我们提出了 ARL2，一种利用法学硕士作为标记器的检索器学习技术。 ARL2 利用 LLM 对相关证据进行注释和评分，从而能够从强大的 LLM 监督中学习检索器。此外，ARL2使用自适应自我训练策略来管理高质量和多样化的相关数据，可以有效降低注释成本。大量实验证明了 ARL2 的有效性，与最先进的方法相比，在 NQ 上实现了 5.4% 的准确率提高，在 MMLU 上实现了 4.6% 的准确率提高。此外，ARL2 还表现出强大的迁移学习能力和强大的零样本泛化能力。我们的代码将发布在\url{https://github.com/zhanglingxi-cs/ARL2}。</li>
</ul>

<h3>Title: LLMs Meet Long Video: Advancing Long Video Comprehension with An  Interactive Visual Adapter in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Li, Xinyu Chen, Baotain Hu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13546">https://arxiv.org/abs/2402.13546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13546">https://arxiv.org/pdf/2402.13546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13546]] LLMs Meet Long Video: Advancing Long Video Comprehension with An  Interactive Visual Adapter in LLMs(https://arxiv.org/abs/2402.13546)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings.</li>
<li><strong>摘要：</strong>长视频理解是多媒体和人工智能交叉领域的一个重大且持续的挑战。采用大型语言模型（LLM）来理解视频成为一种新兴且有前途的方法。然而，由于大量的视频标记，这种方法会产生很高的计算成本，由于标记聚合而导致视觉清晰度降低，并且在回答视频相关问题时面临不相关的视觉标记带来的挑战。为了缓解这些问题，我们在法学硕士中提出了交互式视觉适配器（IVA），旨在增强与细粒度视觉元素的交互。具体来说，我们首先利用视觉编码器和预训练的因果变换器将长视频转换为时间视频标记，然后将它们与视频指令一起输入到法学硕士中。随后，我们将 IVA（包含轻量级时间帧选择器和空间特征交互器）集成到 LLM 的内部模块中，以捕获指令感知和细粒度的视觉信号。因此，所提出的视频法学硕士通过适当的长视频建模和精确的视觉交互促进了对长视频内容的全面理解。我们对九个视频理解基准进行了广泛的实验，实验结果表明，我们的交互式视觉适配器显着提高了视频法学硕士在长视频 QA 任务上的性能。消融研究进一步验证了 IVA 在长短视频理解中的有效性。</li>
</ul>

<h3>Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13547">https://arxiv.org/abs/2402.13547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13547">https://arxiv.org/pdf/2402.13547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13547]] ActiveRAG: Revealing the Treasures of Knowledge via Active Learning(https://arxiv.org/abs/2402.13547)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 引入了大型语言模型 (LLM) 的新范式，有助于解决知识密集型任务。然而，当前的RAG模型将法学硕士定位为被动的知识接受者，从而限制了他们学习和理解外部知识的能力。在本文中，我们提出了 ActiveRAG，这是一种创新的 RAG 框架，它从被动的知识获取转变为主动的学习机制。这种方法利用知识构建机制，通过将外部知识与先前获取或记忆的知识相关联来加深对外部知识的理解。随后，设计了认知联结机制，将思想链和知识建构的结果结合起来，从而校准法学硕士的内在认知。我们的实验结果表明，ActiveRAG 超越了之前的 RAG 模型，在问答数据集上实现了 5% 的改进。所有数据和代码均可在 https://github.com/OpenMatch/ActiveRAG 获取。</li>
</ul>

<h3>Title: Are LLMs Effective Negotiators? Systematic Evaluation of the  Multifaceted Capabilities of LLMs in Negotiation Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Deuksin Kwon, Emily Weiss, Tara Kulshrestha, Kushal Chawla, Gale M. Lucas, Jonathan Gratch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13550">https://arxiv.org/abs/2402.13550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13550">https://arxiv.org/pdf/2402.13550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13550]] Are LLMs Effective Negotiators? Systematic Evaluation of the  Multifaceted Capabilities of LLMs in Negotiation Dialogues(https://arxiv.org/abs/2402.13550)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as strategic reasoning and effective communication, making it challenging for automated systems. Given the remarkable performance of LLMs across a variety of NLP tasks, in this work, we aim to understand how LLMs can advance different aspects of negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. To this end, we devise a methodology to analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios covering all the time stages of a typical negotiation interaction. Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous.</li>
<li><strong>摘要：</strong>成功的谈判需要对对话上下文的深入理解、推断合作伙伴动机的思维理论 (ToM) 技能、以及战略推理和有效的沟通，这对自动化系统来说具有挑战性。鉴于法学硕士在各种 NLP 任务中的出色表现，在这项工作中，我们的目标是了解法学硕士如何推进谈判研究的不同方面，从设计对话系统到提供教学反馈和扩大数据收集实践。为此，我们设计了一种方法来分析法学硕士在不同对话场景中的多方面能力，涵盖典型谈判互动的所有时间阶段。我们的分析为 GPT-4 在各种任务中的优越性提供了越来越多的证据，同时也提供了对法学硕士仍然困难的特定任务的见解。例如，在对谈判对话进行主观评估时，这些模型与人类参与者的相关性很差，并且常常难以生成适合上下文且具有战略优势的响应。</li>
</ul>

<h3>Title: Graph Representation of Narrative Context: Coherence Dependency via  Retrospective Questions</h3>
<ul>
<li><strong>Authors: </strong>Liyan Xu, Jiangnan Li, Mo Yu, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13551">https://arxiv.org/abs/2402.13551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13551">https://arxiv.org/pdf/2402.13551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13551]] Graph Representation of Narrative Context: Coherence Dependency via  Retrospective Questions(https://arxiv.org/abs/2402.13551)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Experiments suggest that our approaches leveraging NARCO yield performance boost across all three tasks.</li>
<li><strong>摘要：</strong>这项工作引入了一种新颖且实用的叙事理解范式，源于对叙事中各个段落往往紧密相关而不是孤立的观察。因此，我们建议根据称为 NARCO 的叙述制定一个图表，该图表描绘了整个上下文的与任务无关的连贯性依赖性。特别是，NARCO 中的边缘包含两个上下文片段之间的回顾性自由形式问题，反映了高级别的连贯关系，其灵感来自人类不断从先前上下文中恢复相关事件的认知感知。重要的是，我们的图表是通过我们设计的两阶段 LLM 提示来实例化的，因此不依赖于人工注释。我们针对其实用性提出了三项独特的研究，通过回顾识别来检查边缘功效，通过图检索来增强本地上下文，以及通过长文档质量保证来验证更广泛的应用。实验表明，我们利用 NARCO 的方法可以提高所有三项任务的性能。</li>
</ul>

<h3>Title: Inductive Graph Alignment Prompt: Bridging the Gap between Graph  Pre-training and Inductive Fine-tuning From Spectral Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Yan, Peiyan Zhang, Zheng Fang, Qingqing Long</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13556">https://arxiv.org/abs/2402.13556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13556">https://arxiv.org/pdf/2402.13556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13556]] Inductive Graph Alignment Prompt: Bridging the Gap between Graph  Pre-training and Inductive Fine-tuning From Spectral Perspective(https://arxiv.org/abs/2402.13556)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph pre-training frameworks and analyze the essence of graph pre-training from graph spectral theory. Then we identify the two sources of the data gap in inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on the insight of graph pre-training, we propose to bridge the graph signal gap and the graph structure gap with learnable prompts in the spectral space. A theoretical analysis ensures the effectiveness of our method. At last, we conduct extensive experiments among nodes classification and graph classification tasks under the transductive, semi-inductive and inductive settings. The results demonstrate that our proposed method can successfully bridge the data gap under different settings.</li>
<li><strong>摘要：</strong>“图预训练和微调”范式通过捕获一般知识而无需对下游任务进行手动注释，显着改进了图神经网络（GNN）。然而，由于预训练和微调阶段之间数据和任务的巨大差距，模型性能仍然受到限制。受自然语言处理（NLP）快速微调的启发，人们做出了许多努力来弥合图领域的差距。但现有方法只是将微调任务的形式重新表述为预训练任务。在预训练图与微调图兼容的前提下，这些方法通常在传导设置中运行。为了将图预训练推广到微调图可能与预训练图显着不同的归纳场景，我们提出了一种基于图提示的新颖方法，称为归纳图对齐提示（IGAP）。首先，我们统一主流图预训练框架，从图谱理论分析图预训练的本质。然后，我们确定归纳设置中数据差距的两个来源：（i）图信号差距和（ii）图结构差距。基于图预训练的见解，我们建议通过谱空间中的可学习提示来弥合图信号间隙和图结构间隙。理论分析确保了我们方法的有效性。最后，我们在传导、半感应和感应设置下对节点分类和图分类任务进行了广泛的实验。结果表明，我们提出的方法可以成功地弥合不同设置下的数据差距。</li>
</ul>

<h3>Title: Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension  with Enhanced Visual Knowledge Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13561">https://arxiv.org/abs/2402.13561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13561">https://arxiv.org/pdf/2402.13561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13561]] Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension  with Enhanced Visual Knowledge Alignment(https://arxiv.org/abs/2402.13561)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.</li>
<li><strong>摘要：</strong>评估和重新思考大型多模态模型 (LMM) 的现状，我们观察到广泛使用的视觉语言投影方法（例如 Q-former 或 MLP）侧重于图像文本描述的对齐，但忽略了视觉知识维度对齐，即将视觉效果与其相关知识联系起来。视觉知识在分析、推断和解释视觉信息方面发挥着重要作用，有助于提高基于知识的视觉问题答案的准确性。在本文中，我们主要探索通过视觉语言知识对齐来改进 LMM，特别是针对挑战基于知识的视觉问答（VQA）。为此，我们提出了一个认知视觉语言映射器（CVLM），其中包含一个预训练的视觉知识对齐器（VKA）和一个用于多模式指令调整阶段的细粒度知识适配器（FKA）。具体来说，我们基于小语言模型和视觉编码器之间的交互设计了VKA，在收集的图像知识对上对其进行训练，以实现视觉知识获取和投影。 FKA 用于提取图像的细粒度视觉知识并将其注入大型语言模型 (LLM)。我们对基于知识的 VQA 基准进行了广泛的实验，实验结果表明 CVLM 显着提高了 LMM 在基于知识的 VQA 上的性能（平均增益 5.0%）。消融研究也分别验证了 VKA 和 FKA 的有效性。</li>
</ul>

<h3>Title: Analysis of Multi-Source Language Training in Cross-Lingual Transfer</h3>
<ul>
<li><strong>Authors: </strong>Seong Hoon Lim, Taejun Yun, Jinhyeon Kim, Jihun Choi, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13562">https://arxiv.org/abs/2402.13562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13562">https://arxiv.org/pdf/2402.13562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13562]] Analysis of Multi-Source Language Training in Cross-Lingual Transfer(https://arxiv.org/abs/2402.13562)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness. In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process. Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information. On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance. We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness.</li>
<li><strong>摘要：</strong>多语言语言模型 (LM) 能否成功适应特定的语言任务对，关键取决于针对该条件定制的数据的可用性。虽然跨语言传输（XLT）方法有助于解决这一数据稀缺问题，但对其有效性背后的机制仍然存在持续的争论。在这项工作中，我们重点关注有关 XLT 内部运作的一个有前景的假设，即它鼓励多语言 LM 更加重视与语言无关或特定于任务的功能。我们通过检查 XLT 模式如何随着过程中涉及的不同数量的源语言而变化来测试这个假设。我们的实验结果表明，在 XLT 中使用多种源语言（我们称之为多源语言训练 (MSLT)）的技术会导致不同语言的嵌入空间混合增加，这支持了 XLT 受益于使用语言的说法。独立信息。另一方面，我们发现使用源语言的任意组合并不总是能保证更好的性能。我们建议使用简单的启发式方法来识别 MSLT 的有效语言组合，并通过经验证明其有效性。</li>
</ul>

<h3>Title: BBA: Bi-Modal Behavioral Alignment for Reasoning with Large  Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xueliang Zhao, Xinting Huang, Tingchen Fu, Qintong Li, Shansan Gong, Lemao Liu, Wei Bi, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13577">https://arxiv.org/abs/2402.13577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13577">https://arxiv.org/pdf/2402.13577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13577]] BBA: Bi-Modal Behavioral Alignment for Reasoning with Large  Vision-Language Models(https://arxiv.org/abs/2402.13577)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Multimodal reasoning stands as a pivotal capability for large vision-language models (LVLMs). The integration with Domain-Specific Languages (DSL), offering precise visual representations, equips these models with the opportunity to execute more accurate reasoning in complex and professional domains. However, the vanilla Chain-of-Thought (CoT) prompting method faces challenges in effectively leveraging the unique strengths of visual and DSL representations, primarily due to their differing reasoning mechanisms. Additionally, it often falls short in addressing critical steps in multi-step reasoning tasks. To mitigate these challenges, we introduce the \underline{B}i-Modal \underline{B}ehavioral \underline{A}lignment (BBA) prompting method, designed to maximize the potential of DSL in augmenting complex multi-modal reasoning tasks. This method initiates by guiding LVLMs to create separate reasoning chains for visual and DSL representations. Subsequently, it aligns these chains by addressing any inconsistencies, thus achieving a cohesive integration of behaviors from different modalities. Our experiments demonstrate that BBA substantially improves the performance of GPT-4V(ision) on geometry problem solving ($28.34\% \to 34.22\%$), chess positional advantage prediction ($42.08\% \to 46.99\%$) and molecular property prediction ($77.47\% \to 83.52\%$).</li>
<li><strong>摘要：</strong>多模态推理是大型视觉语言模型 (LVLM) 的关键功能。与特定领域语言 (DSL) 的集成提供精确的视觉表示，使这些模型有机会在复杂和专业的领域执行更准确的推理。然而，普通的思想链 (CoT) 提示方法在有效利用视觉和 DSL 表示的独特优势方面面临着挑战，这主要是由于它们的推理机制不同。此外，它通常无法解决多步骤推理任务中的关键步骤。为了缓解这些挑战，我们引入了 \underline{B}i-Modal \underline{B}ehavioral \underline{A}lignment (BBA) 提示方法，旨在最大限度地发挥 DSL 在增强复杂多模态推理任务方面的潜力。该方法首先引导 LVLM 为视觉和 DSL 表示创建单独的推理链。随后，它通过解决任何不一致的问题来调整这些链，从而实现不同模式行为的紧密结合。我们的实验表明，BBA 显着提高了 GPT-4V(ision) 在几何问题解决（$28.34\% \to 34.22\%$）、国际象棋位置优势预测（$42.08\% \to 46.99\%$）和分子属性方面的性能预测（$77.47\% \到 83.52\%$）。</li>
</ul>

<h3>Title: Mastering the Game of Guandan with Deep Reinforcement Learning and  Behavior Regulating</h3>
<ul>
<li><strong>Authors: </strong>Yifan Yanggong, Hao Pan, Lei Wang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13582">https://arxiv.org/abs/2402.13582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13582">https://arxiv.org/pdf/2402.13582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13582]] Mastering the Game of Guandan with Deep Reinforcement Learning and  Behavior Regulating(https://arxiv.org/abs/2402.13582)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Games are a simplified model of reality and often serve as a favored platform for Artificial Intelligence (AI) research. Much of the research is concerned with game-playing agents and their decision making processes. The game of Guandan (literally, "throwing eggs") is a challenging game where even professional human players struggle to make the right decision at times. In this paper we propose a framework named GuanZero for AI agents to master this game using Monte-Carlo methods and deep neural networks. The main contribution of this paper is about regulating agents' behavior through a carefully designed neural network encoding scheme. We then demonstrate the effectiveness of the proposed framework by comparing it with state-of-the-art approaches.</li>
<li><strong>摘要：</strong>游戏是现实的简化模型，通常是人工智能 (AI) 研究的首选平台。许多研究都与游戏代理及其决策过程有关。关蛋游戏（字面意思是“扔鸡蛋”）是一种具有挑战性的游戏，即使是职业玩家有时也很难做出正确的决定。在本文中，我们提出了一个名为guanZero的框架，供人工智能代理使用蒙特卡罗方法和深度神经网络来掌握这个游戏。本文的主要贡献是通过精心设计的神经网络编码方案来调节代理的行为。然后，我们通过与最先进的方法进行比较来证明所提出的框架的有效性。</li>
</ul>

<h3>Title: LongWanjuan: Towards Systematic Measurement for Long Text Quality</h3>
<ul>
<li><strong>Authors: </strong>Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13583">https://arxiv.org/abs/2402.13583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13583">https://arxiv.org/pdf/2402.13583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13583]] LongWanjuan: Towards Systematic Measurement for Long Text Quality(https://arxiv.org/abs/2402.13583)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks. The code and dataset are available at https://github.com/OpenLMLab/LongWanjuan.</li>
<li><strong>摘要：</strong>训练数据的质量对于增强基础模型的长文本能力至关重要。尽管现有的努力通过启发式规则和基于数据多样性和难度的评估来提高数据质量，但缺乏专门用于评估长文本的系统方法。为了解决这一差距，我们的工作通过评估三个基本语言维度：连贯性、凝聚力和复杂性来系统地衡量长文本的质量。受上述三个维度的启发，我们引入了一套旨在评估长文本质量的指标，包括统计指标和基于预训练语言模型的指标。利用这些指标，我们推出了 LongWanjuan，这是一个专门为增强具有超过 160B 个标记的长文本任务的语言模型训练而定制的双语数据集。在龙万卷中，我们将长文本分为整体型、聚合型和混沌型，从而可以对长文本质量进行详细分析。此外，我们设计了一种数据混合配方，可以策略性地平衡龙万卷内不同类型的长文本，从而显着提高长文本任务的模型性能。代码和数据集可在 https://github.com/OpenLMLab/LongWanjuan 获取。</li>
</ul>

<h3>Title: WinoViz: Probing Visual Properties of Objects Under Different States</h3>
<ul>
<li><strong>Authors: </strong>Woojeong Jin, Tejas Srinivasan, Jesse Thomason, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13584">https://arxiv.org/abs/2402.13584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13584">https://arxiv.org/pdf/2402.13584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13584]] WinoViz: Probing Visual Properties of Objects Under Different States(https://arxiv.org/abs/2402.13584)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Humans perceive and comprehend different visual properties of an object based on specific contexts. For instance, we know that a banana turns brown ``when it becomes rotten,'' whereas it appears green ``when it is unripe.'' Previous studies on probing visual commonsense knowledge have primarily focused on examining language models' understanding of typical properties (e.g., colors and shapes) of objects. We present WinoViz, a text-only evaluation dataset, consisting of 1,380 examples that probe the reasoning abilities of language models regarding variant visual properties of objects under different contexts or states. Our task is challenging since it requires pragmatic reasoning (finding intended meanings) and visual knowledge reasoning. We also present multi-hop data, a more challenging version of our data, which requires multi-step reasoning chains to solve our task. In our experimental analysis, our findings are: a) Large language models such as GPT-4 demonstrate effective performance, but when it comes to multi-hop data, their performance is significantly degraded. b) Large models perform well on pragmatic reasoning, but visual knowledge reasoning is a bottleneck in our task. c) Vision-language models outperform their language-model counterparts. d) A model with machine-generated images performs poorly in our task. This is due to the poor quality of the generated images.</li>
<li><strong>摘要：</strong>人类根据特定的上下文感知和理解物体的不同视觉属性。例如，我们知道香蕉“腐烂时”会变成棕色，而“未成熟时”会呈现绿色。之前关于探索视觉常识知识的研究主要集中在检查语言模型对典型事物的理解。物体的属性（例如颜色和形状）。我们提出了 WinoViz，一个纯文本评估数据集，由 1,380 个示例组成，这些示例探讨了语言模型关于不同上下文或状态下对象的不同视觉属性的推理能力。我们的任务具有挑战性，因为它需要实用推理（找到预期含义）和视觉知识推理。我们还提出了多跳数据，这是我们数据的更具挑战性的版本，它需要多步骤推理链来解决我们的任务。在我们的实验分析中，我们的发现是：a）诸如GPT-4之类的大型语言模型表现出有效的性能，但是当涉及多跳数据时，它们的性能显着下降。 b）大型模型在语用推理上表现良好，但视觉知识推理是我们任务的瓶颈。 c) 视觉语言模型优于相应的语言模型。 d）具有机器生成图像的模型在我们的任务中表现不佳。这是由于生成的图像质量较差造成的。</li>
</ul>

<h3>Title: A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13587">https://arxiv.org/abs/2402.13587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13587">https://arxiv.org/pdf/2402.13587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13587]] A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation(https://arxiv.org/abs/2402.13587)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种从图像生成产品描述的新设置，并通过营销关键词进行增强。它利用视觉和文本信息的综合力量来创建更适合产品独特功能的描述。对于这种设置，以前的方法利用视觉和文本编码器对图像和关键字进行编码，并采用基于语言模型的解码器来生成产品描述。然而，由于同品类产品具有相似的文案，生成的描述往往不准确且笼统，并且在大规模样本上优化整体框架使得模型专注于常用词而忽略了产品特征。为了缓解这个问题，我们提出了一种简单有效的多模态上下文调优方法，名为 ModICT，该方法引入类似的产品样本作为参考，并利用语言模型的上下文学习能力来生成描述。在训练过程中，我们保持视觉编码器和语言模型冻结，专注于优化负责创建多模式上下文引用和动态提示的模块。这种方法保留了大型语言模型（LLM）的语言生成能力，促进了描述多样性的大幅增加。为了评估 ModICT 在各种语言模型规模和类型中的有效性，我们从电子商务领域内的三个不同产品类别收集数据。大量实验表明，与传统方法相比，ModICT 显着提高了生成结果的准确性（在 Rouge-L 上提高了 3.3%）和多样性（在 D-5 上提高了 9.4%）。我们的研究结果强调了 ModICT 作为一种有价值的工具的潜力，可以在广泛的应用中增强产品描述的自动生成。</li>
</ul>

<h3>Title: Knowledge Graph Enhanced Large Language Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13593">https://arxiv.org/abs/2402.13593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13593">https://arxiv.org/pdf/2402.13593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13593]] Knowledge Graph Enhanced Large Language Model Editing(https://arxiv.org/abs/2402.13593)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 对于推进自然语言处理 (NLP) 任务至关重要，但其有效性因不准确和过时的知识而受到阻碍。模型编辑成为应对这些挑战的一种有前景的解决方案。然而，现有的编辑方法很难跟踪和合并与编辑相关的知识变化，这限制了 postit 法学硕士在处理编辑知识方面的泛化能力。为了解决这些问题，我们提出了一种利用知识图来增强 LLM 编辑的新颖模型编辑方法，即 GLAME。具体来说，我们首先利用知识图增强模块来揭示因编辑而改变的相关知识，获取其在法学硕士中的内部表示。这种方法允许法学硕士内的知识变更通过外部图结构反映出来。随后，我们设计了一个基于图的知识编辑模块，将结构化知识集成到模型编辑中。这确保更新的参数不仅反映编辑知识的修改，而且反映编辑过程导致的其他相关知识的变化。在 GPT-J 和 GPT-2 XL 上进行的综合实验表明，GLAME 显着提高了编辑后法学硕士在使用编辑知识方面的泛化能力。</li>
</ul>

<h3>Title: User-LLM: Efficient LLM Contextualization with User Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O'Banion, Jun Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13598">https://arxiv.org/abs/2402.13598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13598">https://arxiv.org/pdf/2402.13598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13598]] User-LLM: Efficient LLM Contextualization with User Embeddings(https://arxiv.org/abs/2402.13598)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了自然语言处理。然而，有效整合复杂且可能有噪音的用户交互数据仍然是一个挑战。为了解决这个问题，我们提出了 User-LLM，这是一种利用用户嵌入来将 LLM 情境化的新颖框架。这些嵌入是使用自监督预训练从不同的用户交互中提取出来的，捕获潜在的用户偏好及其随时间的演变。我们通过交叉注意力和软提示将这些用户嵌入与 LLM 集成，使 LLM 能够动态适应用户上下文。我们对 MovieLens、Amazon Review 和 Google Local Review 数据集进行的综合实验表明，各种任务的性能都有显着提升。值得注意的是，我们的方法在长序列任务和需要深入用户理解同时计算效率高的任务上优于基于文本提示的上下文化。我们进一步合并感知器层来简化用户编码器和 LLM 之间的集成，从而减少计算需求。</li>
</ul>

<h3>Title: Breaking the HISCO Barrier: Automatic Occupational Standardization with  OccCANINE</h3>
<ul>
<li><strong>Authors: </strong>Christian Møller Dahl, Christian Vedel</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.EM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13604">https://arxiv.org/abs/2402.13604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13604">https://arxiv.org/pdf/2402.13604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13604]] Breaking the HISCO Barrier: Automatic Occupational Standardization with  OccCANINE(https://arxiv.org/abs/2402.13604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system. The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming. We finetune a preexisting language model (CANINE) to do this automatically thereby performing in seconds and minutes what previously took days and weeks. The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources. Our approach is shown to have accuracy, recall and precision above 90 percent. Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history and various related disciplines.</li>
<li><strong>摘要：</strong>本文介绍了一种新工具 OccCANINE，可以自动将职业描述转换为 HISCO 分类系统。职业描述的处理和分类所涉及的手工工作容易出错、乏味且耗时。我们对预先存在的语言模型 (CANINE) 进行了微调，使其自动执行此操作，从而在几秒钟和几分钟内完成了以前需要几天和几周的时间。该模型接受了来自 22 个不同来源的 1400 万对职业描述和 13 种不同语言的 HISCO 代码的训练。我们的方法的准确度、召回率和精确度均超过 90%。我们的工具打破了 HISCO 的隐喻障碍，使这些数据易于用于职业结构分析，在经济学、经济史和各种相关学科中具有广泛的适用性。</li>
</ul>

<h3>Title: KorNAT: LLM Alignment Benchmark for Korean Social Values and Common  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won, Hwaran Lee, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13605">https://arxiv.org/abs/2402.13605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13605">https://arxiv.org/pdf/2402.13605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13605]] KorNAT: LLM Alignment Benchmark for Korean Social Values and Common  Knowledge(https://arxiv.org/abs/2402.13605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>For Large Language Models (LLMs) to be effectively deployed in a specific country, they must possess an understanding of the nation's culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first benchmark that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a large-scale survey involving 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for social value and common knowledge, respectively. Our dataset creation process is meticulously designed and based on statistical sampling theory and was refined through multiple rounds of human review. The experiment results of seven LLMs reveal that only a few models met our reference score, indicating a potential for further enhancement. KorNAT has received government approval after passing an assessment conducted by a government-affiliated organization dedicated to evaluating dataset quality. Samples and detailed evaluation protocols of our dataset can be found in \url{https://selectstar.ai/ko/papers-national-alignment#}</li>
<li><strong>摘要：</strong>为了在特定国家有效部署大型语言模型（LLM），他们必须了解该国家的文化和基础知识。为此，我们引入国家一致性，从社会价值一致性和常识一致性两个方面衡量LLM与目标国家之间的一致性。社会价值一致性评估模型对特定国家社会价值观的理解程度，而常识一致性则检查模型捕获与国家相关的基本知识的程度。我们构建了 KorNAT，这是衡量国家与韩国一致性的第一个基准。对于社会价值数据集，我们从一项涉及 6,174 名韩国参与者的大规模调查中获得了真实标签。对于常识数据集，我们根据韩国教科书和GED参考资料构建了样本。 KorNAT 包含分别针对社会价值和常识的 4K 和 6K 多项选择题。我们的数据集创建过程经过精心设计，基于统计抽样理论，并通过多轮人工审核进行完善。七个法学硕士的实验结果表明，只有少数模型符合我们的参考分数，表明有进一步增强的潜力。 KorNAT 在通过了专门评估数据集质量的政府附属组织进行的评估后获得了政府批准。我们数据集的样本和详细评估协议可以在 \url{https://selectstar.ai/ko/papers-national-alignment#} 中找到</li>
</ul>

<h3>Title: A Comprehensive Study of Multilingual Confidence Estimation on Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boyang Xue, Hongru Wang, Weichao Wang, Rui Wang, Sheng Wang, Zeming Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13606">https://arxiv.org/abs/2402.13606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13606">https://arxiv.org/pdf/2402.13606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13606]] A Comprehensive Study of Multilingual Confidence Estimation on Large  Language Models(https://arxiv.org/abs/2402.13606)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.</li>
<li><strong>摘要：</strong>大型语言模型容易产生幻觉并表现出对预测的过度自信，这引起了人们对其可靠性的担忧。表明模型响应可信程度的置信度或不确定性估计对于开发可靠的人工智能系统至关重要。目前的研究主要集中在英语法学硕士的置信度估计上，对于其他广泛使用的语言来说仍然是一个空白，并阻碍了可靠的人工智能应用程序的全球发展。本文介绍了法学硕士多语言置信度估计（MlingConf）的全面研究。首先，我们介绍一个精心设计且经过专家检查的多语言 QA 数据集。其次，我们深入研究置信度评估的表现，并研究这些置信度分数如何通过跨不同语言的自我完善来提高法学硕士的表现。最后，我们提出了一种跨语言置信度估计方法来获得更精确的置信度分数。实验结果展示了不同语言中各种置信度估计方法的性能，并表明我们提出的跨语言置信度估计技术显着增强了置信度估计并优于几种基线方法。</li>
</ul>

<h3>Title: Data-driven Discovery with Large Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, Peter Clark</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13610">https://arxiv.org/abs/2402.13610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13610">https://arxiv.org/pdf/2402.13610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13610]] Data-driven Discovery with Large Generative Models(https://arxiv.org/abs/2402.13610)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</li>
<li><strong>摘要：</strong>随着数据以前所未有的速度积累，其推动科学发现的潜力呈指数级增长。本立场文件敦促机器学习 (ML) 社区利用大型生成模型 (LGM) 的功能来开发用于端到端数据驱动发现的自动化系统，这是一种涵盖纯粹从数据中搜索和验证假设的范式。一组提供的数据集，无需额外的数据收集或物理实验。我们首先概述了理想的数据驱动发现系统的几个需求。然后，通过 DATAVOYAGER（一种利用 GPT-4 的概念验证），我们演示了 LGM 如何满足其中一些需求（这是以前无法实现的壮举），同时也强调了当前系统中的重要局限性，这些局限性为新颖的 ML 研究开辟了机会。我们认为，仅通过 LGM 当前的功能来实现准确、可靠和强大的端到端发现系统具有挑战性。相反，我们提倡防故障的工具集成，并通过反馈机制进行积极的用户审核，以促进数据驱动的科学发现的效率和可重复性。</li>
</ul>

<h3>Title: FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sahil Mishra, Ujjwal Sudev, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13623">https://arxiv.org/abs/2402.13623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13623">https://arxiv.org/pdf/2402.13623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13623]] FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models(https://arxiv.org/abs/2402.13623)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Taxonomies represent an arborescence hierarchical structure that establishes relationships among entities to convey knowledge within a specific domain. Each edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find utility in various real-world applications, such as e-commerce search engines and recommendation systems. Consequently, there arises a necessity to enhance these taxonomies over time. However, manually curating taxonomies with neoteric data presents challenges due to limitations in available human resources and the exponential growth of data. Therefore, it becomes imperative to develop automatic taxonomy expansion methods. Traditional supervised taxonomy expansion approaches encounter difficulties stemming from limited resources, primarily due to the small size of existing taxonomies. This scarcity of training data often leads to overfitting. In this paper, we propose FLAME, a novel approach for taxonomy expansion in low-resource environments by harnessing the capabilities of large language models that are trained on extensive real-world knowledge. LLMs help compensate for the scarcity of domain-specific knowledge. Specifically, FLAME leverages prompting in few-shot settings to extract the inherent knowledge within the LLMs, ascertaining the hypernym entities within the taxonomy. Furthermore, it employs reinforcement learning to fine-tune the large language models, resulting in more accurate predictions. Experiments on three real-world benchmark datasets demonstrate the effectiveness of FLAME in real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy and 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate the strengths and weaknesses of FLAME through an extensive case study, error analysis and ablation studies on the benchmarks.</li>
<li><strong>摘要：</strong>分类法表示树状层次结构，它在实体之间建立关系以传达特定领域内的知识。分类中的每条边都表示上位词-下位词关系。分类法在各种现实应用中都有用处，例如电子商务搜索引擎和推荐系统。因此，有必要随着时间的推移增强这些分类法。然而，由于可用人力资源的限制和数据的指数增长，使用新数据手动管理分类法提出了挑战。因此，开发自动分类扩展方法势在必行。传统的监督分类法扩展方法遇到了由于资源有限而遇到的困难，这主要是由于现有分类法的规模较小。训练数据的缺乏常常导致过度拟合。在本文中，我们提出了 FLAME，这是一种通过利用经过广泛的现实世界知识训练的大型语言模型的功能，在资源匮乏的环境中进行分类法扩展的新方法。法学硕士有助于弥补特定领域知识的稀缺。具体来说，FLAME 利用少量设置中的提示来提取 LLM 中的固有知识，确定分类法中的上位词实体。此外，它采用强化学习来微调大型语言模型，从而产生更准确的预测。对三个真实世界基准数据集的实验证明了 FLAME 在真实场景中的有效性，与八个基线相比，准确率显着提高了 18.5%，Wu & Palmer 指标显着提高了 12.3%。此外，我们通过广泛的案例研究、错误分析和基准消融研究来阐明 FLAME 的优点和缺点。</li>
</ul>

<h3>Title: MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13625">https://arxiv.org/abs/2402.13625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13625">https://arxiv.org/pdf/2402.13625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13625]] MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning(https://arxiv.org/abs/2402.13625)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.</li>
<li><strong>摘要：</strong>由于常识信息的记录频率明显低于其存在频率，因此通过文本生成预训练的语言模型很难学习足够的常识知识。一些研究利用文本检索来增强模型的常识能力。与文本不同，图像本质上捕获常识信息，但人们很少努力有效地利用它们。在这项工作中，我们提出了一种新颖的多模态检索（MORE）增强框架，利用文本和图像来增强语言模型的常识能力。 Common-Gen 任务的大量实验证明了基于单模态和多模态预训练模型的 MORE 的有效性。</li>
</ul>

<h3>Title: UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language</h3>
<ul>
<li><strong>Authors: </strong>Yufei He, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13630">https://arxiv.org/abs/2402.13630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13630">https://arxiv.org/pdf/2402.13630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13630]] UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language(https://arxiv.org/abs/2402.13630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations. We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM). We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.</li>
<li><strong>摘要：</strong>ChatGPT 和 GPT-4 等基础模型彻底改变了人工智能，表现出非凡的能力，可以泛化超出其最初训练目标的广泛任务和应用程序。然而，当这个概念应用于图学习时，就出现了鲜明的对比。图学习主要关注针对特定任务或数据集定制的单图模型，缺乏将学到的知识转移到不同领域的能力。这种限制源于图结构固有的复杂性和多样性，以及图数据特定的不同特征和标签空间。在本文中，我们提出了 UniGraph 框架，旨在训练能够泛化到跨不同领域的未见过的图和任务的图基础模型。与使用不同维度的预先计算的节点特征作为输入的单图模型不同，我们的方法利用文本属性图（TAG）来统一节点表示。我们提出了语言模型（LM）和图神经网络（GNN）的级联架构作为骨干网络，具有基于掩蔽图建模（MGM）的自监督训练目标。我们引入使用大型语言模型（LLM）的图指令调整来实现零样本预测能力。我们在各种图学习任务和领域的综合实验证明了该模型在未见过的图上的自监督表示学习、少样本上下文迁移和零样本迁移方面的有效性，甚至超越或匹配经过监督的 GNN 的性能对目标数据集进行训练。</li>
</ul>

<h3>Title: Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions</h3>
<ul>
<li><strong>Authors: </strong>Lei Pan, Yunshi Lan, Yang Li, Weining Qian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13647">https://arxiv.org/abs/2402.13647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13647">https://arxiv.org/pdf/2402.13647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13647]] Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions(https://arxiv.org/abs/2402.13647)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency. Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.</li>
<li><strong>摘要：</strong>无监督文本风格迁移（UTST）已成为自然语言处理（NLP）领域的一项关键任务，旨在将句子的一种风格转变为另一种风格，而不改变其语义、语法或其他属性。鉴于本质上缺乏并行文本配对，这项任务尤其具有挑战性。在现有的 UTST 任务方法中，注意力掩蔽方法和大型语言模型（LLM）被认为是两种开创性的方法。然而，它们分别在生成不流畅的句子和改变原始内容方面存在缺陷。在本文中，我们研究是否可以有效地将这两种方法结合起来。我们提出了四种交互方式，即具有调整顺序的管道框架；从法学硕士到注意力掩蔽模型的知识蒸馏；通过构建的并行示例进行上下文学习。我们的经验表明，这些多向交互可以在风格强度、内容保留和文本流畅性的某些方面提高基线。实验还表明，简单地进行提示，然后进行基于注意力掩蔽的修订可以始终超越其他系统，包括监督文本样式转换系统。在 Yelp-clean 和 Amazon-clean 数据集上，它将之前的最佳平均指标分别提高了 0.5 和 3.0 绝对百分比，并取得了新的 SOTA 结果。</li>
</ul>

<h3>Title: PQA: Zero-shot Protein Question Answering for Free-form Scientific  Enquiry with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eli M Carrami, Sahand Sharifzadeh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13653">https://arxiv.org/abs/2402.13653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13653">https://arxiv.org/pdf/2402.13653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13653]] PQA: Zero-shot Protein Question Answering for Free-form Scientific  Enquiry with Large Language Models(https://arxiv.org/abs/2402.13653)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs). We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs. Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA. Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is openly accessible on github.com/EMCarrami/Pika, promoting wider research and application in the field.</li>
<li><strong>摘要：</strong>我们介绍了零样本蛋白质问答（PQA）的新任务，用于自由形式的科学探究。给定一个以前未见过的蛋白质序列和一个自然语言问题，任务是提供科学上准确的答案。这项任务不仅支持未来的生物学研究，还可以为评估大语言模型（LLM）的科学精度提供一个测试平台。我们贡献了第一个用于 PQA 模型训练的专用数据集，其中包含 257K 个蛋白质序列，并用 197 万个科学问答对进行注释。此外，我们还提出并研究了几种新颖的生物学相关的科学 PQA 基准。采用两种强大的多模态架构，我们为 PQA 建立了最初最先进的性能，并通过消融研究揭示了关键性能因素。我们的综合 PQA 框架名为 Pika，包括数据集、代码、模型检查点和用户友好的演示，可在 github.com/EMCarrami/Pika 上公开访问，促进该领域更广泛的研究和应用。</li>
</ul>

<h3>Title: GCOF: Self-iterative Text Generation for Copywriting Using Large  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jianghui Zhou, Ya Gao, Jie Liu, Xuemin Zhao, Zhaohua Yang, Yue Wu, Lirong Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13667">https://arxiv.org/abs/2402.13667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13667">https://arxiv.org/pdf/2402.13667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13667]] GCOF: Self-iterative Text Generation for Copywriting Using Large  Language Model(https://arxiv.org/abs/2402.13667)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge. In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation. We conduct explicit feature engineering within the prompts of LLM. Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering. This integration facilitates a self-iterative refinement of the marketing copy. Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50\%$.</li>
<li><strong>摘要：</strong>ChatGPT 等大型语言模型 (LLM) 极大地简化了营销文案的生成，但生成满足特定领域要求（例如有效吸引客户）的内容仍然是一项重大挑战。在这项工作中，我们引入了遗传文案优化框架（GCOF），旨在提高营销文案创建的效率和参与度。我们在法学硕士的提示下进行显式特征工程。此外，我们修改了遗传算法（GA）中的交叉算子，将其集成到 GCOF 中以实现自动特征工程。这种集成有助于营销文案的自我迭代完善。与人工策划的副本相比，在线结果表明，我们的框架生成的副本的点击率 (CTR) 平均增加了超过 50\%$。</li>
</ul>

<h3>Title: Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13669">https://arxiv.org/abs/2402.13669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13669">https://arxiv.org/pdf/2402.13669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13669]] Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning(https://arxiv.org/abs/2402.13669)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at \url{https://github.com/sail-sg/sdft}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的激增彻底改变了自然语言处理，但针对特定任务对其进行微调通常会在平衡性能和保持一般指令跟踪能力方面遇到挑战。在本文中，我们认为任务数据集和法学硕士之间的分配差距是主要原因。为了解决这个问题，我们引入了自蒸馏微调（SDFT），这是一种新颖的方法，通过使用模型本身生成的蒸馏数据集进行微调以匹配其原始分布来指导微调，从而弥补分布差距。 Llama-2-chat 模型在各种基准上的实验结果表明，与普通微调相比，SDFT 有效地减轻了灾难性遗忘，同时在下游任务上实现了相当或更好的性能。此外，SDFT 展示了保持法学硕士的有用性和安全性的潜力。我们的代码可在 \url{https://github.com/sail-sg/sdft} 获取。</li>
</ul>

<h3>Title: KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual  Machine-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Michal Spiegel, Dominik Macko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13671">https://arxiv.org/abs/2402.13671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13671">https://arxiv.org/pdf/2402.13671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13671]] KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual  Machine-Generated Text Detection(https://arxiv.org/abs/2402.13671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.</li>
<li><strong>摘要：</strong>SemEval-2024 任务 8 专注于多生成器、多域和多语言黑盒机器生成的文本检测。这种检测对于防止大型语言模型 (LLM) 的潜在滥用非常重要，最新的大型语言模型非常能够生成多语言的类人文本。我们以多种方式应对这项任务，利用语言识别和小型法学硕士的参数高效微调进行文本分类。我们进一步使用每种语言的分类阈值校准来独特地将微调模型预测与统计检测指标结合起来，以提高系统检测性能的泛化。我们提交的方法取得了有竞争力的结果，排名第四，仅落后获胜者不到 1 个百分点。</li>
</ul>

<h3>Title: Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand  for Multilingual Instructions?</h3>
<ul>
<li><strong>Authors: </strong>Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, Mehdi Ali</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13703">https://arxiv.org/abs/2402.13703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13703">https://arxiv.org/pdf/2402.13703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13703]] Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand  for Multilingual Instructions?(https://arxiv.org/abs/2402.13703)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios.</li>
<li><strong>摘要：</strong>将多语言预训练大语言模型 (LLM) 改编为雄辩且乐于助人的助手对于促进其在不同语言区域的使用至关重要。本着这种精神，我们是第一个在精选的最常用印欧语言的并行、多轮指令调优基准上对多语言模型的性能进行广泛研究的人。我们通过在并行指令调整数据集上进行指令调整，系统地检查语言和指令数据集大小对中型多语言 LLM 的影响。我们的结果表明，并行指令调整而不是单语言语料库可使跨语言指令跟踪能力提高高达 4.6%。此外，我们表明表面对齐假设一般不成立，因为所研究的多语言 7B 参数模型提出了一个需要大规模指令调整数据集的反例。最后，我们进行了人工注释研究，以了解多语言聊天场景中基于人类的评估和基于 GPT-4 的评估之间的一致性。</li>
</ul>

<h3>Title: SaGE: Evaluating Moral Consistency in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam Kumaraguru, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13709">https://arxiv.org/abs/2402.13709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13709">https://arxiv.org/pdf/2402.13709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13709]] SaGE: Evaluating Moral Consistency in Large Language Models(https://arxiv.org/abs/2402.13709)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.</li>
<li><strong>摘要：</strong>尽管最近的进展展示了大型语言模型（LLM）在会话系统中令人印象深刻的能力，但我们表明，即使是最先进的 LLM 在他们的世代中在道德上也不一致，质疑它们的可靠性（以及总体上的可信度）。法学硕士评估的先前工作重点是开发真实数据以衡量特定任务的准确性。然而，对于通常缺乏普遍同意的答案的道德场景，模型响应的一致性对其可靠性变得至关重要。为了解决这个问题，我们提出了一种称为语义图熵（SaGE）的信息论度量，它基于“经验法则”（RoT）的概念来衡量模型的道德一致性。 RoT 是模型学习到的抽象原理，可以帮助有效地解释其决策策略。为此，我们构建了道德一致性语料库（MCC），其中包含 50K 个道德问题、法学硕士对这些问题的回答以及这些模型遵循的 RoT。此外，为了说明 SaGE 的普遍性，我们用它来研究两个流行数据集——TruthfulQA 和 HellaSwag 上的 LLM 一致性。我们的结果表明，任务准确性和一致性是独立的问题，迫切需要进一步研究这些问题。</li>
</ul>

<h3>Title: Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character  Role-Playing Agent</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao, Liehuang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13717">https://arxiv.org/abs/2402.13717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13717">https://arxiv.org/pdf/2402.13717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13717]] Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character  Role-Playing Agent(https://arxiv.org/abs/2402.13717)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at https://github.com/weiyifan1023/Neeko.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了开放域对话代理，但在多角色角色扮演 (MCRP) 场景中遇到了挑战。为了解决这个问题，我们推出了 Neeko，一个专为高效的多角色模仿而设计的创新框架。与现有方法不同，Neeko 采用动态低阶适配器 (LoRA) 策略，使其能够无缝适应不同的角色。我们的框架将角色扮演过程分解为代理预训练、多个角色扮演和角色增量学习，有效地处理可见和不可见的角色。这种动态方法与每个角色的独特 LoRA 块相结合，增强了 Neeko 对独特属性、个性和说话模式的适应性。因此，Neeko 在 MCRP 中展示了优于大多数现有方法的性能，提供了更具吸引力和多功能的用户交互体验。代码和数据可在 https://github.com/weiyifan1023/Neeko 获取。</li>
</ul>

<h3>Title: $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens</h3>
<ul>
<li><strong>Authors: </strong>Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13718">https://arxiv.org/abs/2402.13718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13718">https://arxiv.org/pdf/2402.13718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13718]] $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens(https://arxiv.org/abs/2402.13718)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, agent</a></li>
<li><strong>Abstract: </strong>Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.</li>
<li><strong>摘要：</strong>长上下文的处理和推理对于大型语言模型（LLM）的许多实际应用至关重要，例如文档理解和代理构建。尽管最近在使法学硕士处理具有超过 100K 个令牌的上下文方面取得了长足进步，但目前仍缺乏评估这种长上下文能力的标准化基准。现有的公共基准通常侧重于 10K 代币左右的上下文，限制了 LLM 在处理更长上下文时的评估和比较。在本文中，我们提出了 $\infty$Bench，这是第一个平均数据长度超过 100K 代币的 LLM 基准。 $\infty$Bench 包含跨越不同领域的综合和现实任务，以英文和中文呈现。 $\infty$Bench 中的任务旨在要求充分理解上下文中的长依赖关系，并且仅从上下文中检索有限数量的段落不足以完成这些任务。在我们的实验中，基于 $\infty$Bench，我们评估了为处理长上下文而定制的最先进的专有和开源 LLM。结果表明，现有的长上下文法学硕士仍需要重大进步才能有效处理 100K+ 上下文。我们进一步提出了关于法学硕士处理长上下文行为的三个有趣的分析。</li>
</ul>

<h3>Title: Ouroboros: Speculative Decoding with Large Model Enhanced Drafting</h3>
<ul>
<li><strong>Authors: </strong>Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13720">https://arxiv.org/abs/2402.13720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13720">https://arxiv.org/pdf/2402.13720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13720]] Ouroboros: Speculative Decoding with Large Model Enhanced Drafting(https://arxiv.org/abs/2402.13720)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model. Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts. The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively. The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros.</li>
<li><strong>摘要：</strong>推测解码等先起草后验证的解码方法是广泛采用的免训练方法，可加速大型语言模型 (LLM) 的推理。推测性解码最初使用高效的小模型创建草稿，而不是采用自回归过程顺序解码令牌。然后法学硕士需要以非自回归的方式进行验证和纠正，以尽量减少时间开销。一旦验证，生成更长的草稿可以带来更显着的加速，但如果失败也会产生大量的试错成本。由于验证失败概率较高，现有的解码方法无法一次起草太多的内容进行验证，从而实现了次优的推理加速。在本文中，我们介绍了 Ouroboros，它根据 LLM 的验证过程构建了一个短语候选池，为小模型的草稿生成提供候选。从而，Ouroboros可以进一步提高初稿的效率和效果。典型文本生成任务的实验结果表明，与前瞻解码和推测解码相比，Ouroboros 的加速分别高达 1.9 倍和 2.8 倍。 Ouroboros 的源代码可在 https://github.com/thunlp/Ouroboros 获取。</li>
</ul>

<h3>Title: The Da Vinci Code of Large Pre-trained Language Models: Deciphering  Degenerate Knowledge Neurons</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Chen, Pengfei Cao, Yubo Chen, Yining Wang, Shengping Liu, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13731">https://arxiv.org/abs/2402.13731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13731">https://arxiv.org/pdf/2402.13731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13731]] The Da Vinci Code of Large Pre-trained Language Models: Deciphering  Degenerate Knowledge Neurons(https://arxiv.org/abs/2402.13731)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon.</li>
<li><strong>摘要：</strong>本研究探讨了预训练语言模型（PLM）中事实知识存储的机制。先前的研究表明，事实知识存储在多层感知器权重中，并且一些存储单元表现出简并性，称为简并知识神经元（DKN）。本文提供了 DKN 的全面定义，涵盖结构和功能方面，开创了 PLM 事实知识存储单元结构的研究。在此基础上，我们引入了神经拓扑聚类方法，该方法允许形成任意数量和结构的DKN，从而获得更准确的DKN。此外，我们还引入了神经退化分析框架，该框架独特地集成了模型稳健性、可进化性和复杂性，以对 PLM 进行整体评估。在此框架内，我们在 2 个 PLM、4 个数据集和 6 个设置中执行了 34 项实验，凸显了 DKN 的关键作用。该代码即将推出。</li>
</ul>

<h3>Title: From Text to CQL: Bridging Natural Language and Corpus Search Engine</h3>
<ul>
<li><strong>Authors: </strong>Luming Lu, Jiyuan An, Yujie Wang, Liner yang, Cunliang Kong, Zhenghao Liu, Shuo Wang, Haozhe Lin, Mingwei Fang, Yaping Huang, Erhong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13740">https://arxiv.org/abs/2402.13740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13740">https://arxiv.org/pdf/2402.13740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13740]] From Text to CQL: Bridging Natural Language and Corpus Search Engine(https://arxiv.org/abs/2402.13740)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Natural Language Processing (NLP) technologies have revolutionized the way we interact with information systems, with a significant focus on converting natural language queries into formal query languages such as SQL. However, less emphasis has been placed on the Corpus Query Language (CQL), a critical tool for linguistic research and detailed analysis within text corpora. The manual construction of CQL queries is a complex and time-intensive task that requires a great deal of expertise, which presents a notable challenge for both researchers and practitioners. This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL. We present a comprehensive framework for this task, including a specifically curated large-scale dataset and methodologies leveraging large language models (LLMs) for effective text-to-CQL task. In addition, we established advanced evaluation metrics to assess the syntactic and semantic accuracy of the generated queries. We created innovative LLM-based conversion approaches and detailed experiments. The results demonstrate the efficacy of our methods and provide insights into the complexities of text-to-CQL task.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 技术彻底改变了我们与信息系统交互的方式，重点是将自然语言查询转换为正式查询语言（例如 SQL）。然而，语料库查询语言 (CQL) 却不受重视，它是文本语料库中语言研究和详细分析的关键工具。手动构建 CQL 查询是一项复杂且耗时的任务，需要大量专业知识，这对研究人员和从业者来说都是一个巨大的挑战。本文提出了第一个文本到 CQL 任务，旨在将自然语言自动翻译为 CQL。我们为此任务提供了一个全面的框架，包括专门策划的大规模数据集和利用大型语言模型 (LLM) 来执行有效的文本到 CQL 任务的方法。此外，我们建立了高级评估指标来评估生成的查询的语法和语义准确性。我们创建了基于法学硕士的创新转换方法和详细的实验。结果证明了我们方法的有效性，并提供了对文本到 CQL 任务复杂性的见解。</li>
</ul>

<h3>Title: Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction</h3>
<ul>
<li><strong>Authors: </strong>Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji, Jiajun Liu, Ziyu Shang, Qiqing Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13741">https://arxiv.org/abs/2402.13741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13741">https://arxiv.org/pdf/2402.13741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13741]] Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction(https://arxiv.org/abs/2402.13741)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures. Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples.</li>
<li><strong>摘要：</strong>用于关系三元组提取（RTE）的上下文学习（ICL）已经取得了可喜的性能，但仍然遇到两个关键挑战：（1）如何设计有效的提示和（2）如何选择适当的演示。然而，现有的方法无法适当地应对这些挑战。一方面，他们通常将 RTE 任务重新转换为文本到文本的提示格式，这是不自然的，并导致预训练时的输出格式与大型语言模型（LLM）的推理时间不匹配。另一方面，它们仅利用表面自然语言特征，在样本选择时缺乏对三重语义的考虑。这些问题阻碍了 RTE ICL 性能的提高，因此我们的目标是同时解决快速设计和样本选择的挑战。为此，我们设计了 RTE 的表格提示（\textsc{TableIE}），它将 RTE 任务构建为表格生成任务，以将显式结构化信息合并到 ICL 中，从而促进输出到 RTE 结构的转换。然后，我们提出了指导性上下文学习（I$^2$CL），它仅考虑大量未标记样本中的内部三元组语义来选择和注释一些样本。</li>
</ul>

<h3>Title: LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</h3>
<ul>
<li><strong>Authors: </strong>Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13753">https://arxiv.org/abs/2402.13753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13753">https://arxiv.org/pdf/2402.13753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13753]] LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens(https://arxiv.org/abs/2402.13753)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.</li>
<li><strong>摘要：</strong>大上下文窗口是大型语言模型 (LLM) 中理想的功能。然而，由于微调成本高、长文本稀缺以及新标记位置引入的灾难性值，当前的扩展上下文窗口仅限于大约 128k 个标记。本文介绍了 LongRoPE，它首次将预训练 LLM 的上下文窗口扩展到令人印象深刻的 2048k 令牌，在 256k 训练长度内最多仅需要 1k 微调步骤，同时保持原始短上下文窗口的性能。这是通过三个关键创新实现的：（i）我们通过有效的搜索识别和利用位置插值中的两种形式的非均匀性，为微调提供更好的初始化，并在非微调场景中实现 8 倍扩展； (ii) 我们引入了一种渐进扩展策略，首先微调 256k 长度的 LLM，然后对微调的扩展 LLM 进行第二次位置插值，以实现 2048k 上下文窗口； (iii) 我们在 8k 长度上重新调整 LongRoPE 以恢复短上下文窗口性能。在 LLaMA2 和 Mistral 上进行的各种任务的广泛实验证明了我们方法的有效性。通过 LongRoPE 扩展的模型保留了原始架构，并对位置嵌入进行了少量修改，并且可以重用大多数预先存在的优化。</li>
</ul>

<h3>Title: Factual Consistency Evaluation of Summarisation in the Era of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheheng Luo, Qianqian Xie, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13758">https://arxiv.org/abs/2402.13758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13758">https://arxiv.org/pdf/2402.13758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13758]] Factual Consistency Evaluation of Summarisation in the Era of Large  Language Models(https://arxiv.org/abs/2402.13758)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.</li>
<li><strong>摘要：</strong>自动生成的摘要中与源文档的事实不一致可能会导致错误信息或带来风险。现有的事实一致性（FC）指标受到其性能、效率和可解释性的限制。大语言模型 (LLM) 的最新进展在文本评估方面展示了巨大的潜力，但其在评估摘要 FC 方面的有效性仍有待探索。之前的研究主要集中在专有的法学硕士上，而没有探索影响其评估能力的基本因素。此外，当前的 FC 评估基准仅限于新闻文章，这使人们对在其上测试的 FC 方法的通用性产生怀疑。在本文中，我们首先通过引入 TreatFact 来解决这一差距，这是由法学硕士生成的临床文本摘要的数据集，由领域专家为 FC 进行注释。此外，我们对新闻和临床领域的 11 个法学硕士进行 FC 评估进行基准测试，并分析模型大小、提示、预训练和微调数据的影响。我们的研究结果表明，尽管专有模型在该任务中占主导地位，但开源法学硕士却落后了。尽管如此，通过增加模型大小、扩展预训练数据和开发精心策划的微调数据，仍有可能提高开源法学硕士的性能。 TreatFact 的实验表明，之前的方法和基于 LLM 的评估者都无法捕获临床总结中事实的不一致，这对 FC 评估提出了新的挑战。</li>
</ul>

<h3>Title: CriticBench: Evaluating Large Language Models as Critic</h3>
<ul>
<li><strong>Authors: </strong>Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-ling Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13764">https://arxiv.org/abs/2402.13764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13764">https://arxiv.org/pdf/2402.13764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13764]] CriticBench: Evaluating Large Language Models as Critic(https://arxiv.org/abs/2402.13764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/CriticBench}.</li>
<li><strong>摘要：</strong>批判能力对于大型语言模型（LLM）的可扩展监督和自我改进至关重要。虽然最近许多研究探讨了法学硕士判断和完善代际缺陷的批判能力，但如何全面、可靠地衡量法学硕士的批判能力尚未得到充分探索。本文介绍了 \shortname，这是一个新颖的基准，旨在全面、可靠地评估法学硕士的四个关键批判能力维度：反馈、比较、细化和元反馈。 \shortname~包含九项不同的任务，每项任务都评估法学硕士以不同质量粒度级别批评回答的能力。我们对开源和闭源法学硕士的广泛评估揭示了批判能力和任务、响应质量和模型规模之间有趣的关系。 \shortname~的数据集、资源和评估工具包将在\url{https://github.com/gmftbyGMFTBY/CriticBench}公开发布。</li>
</ul>

<h3>Title: Synthesis of Hierarchical Controllers Based on Deep Reinforcement  Learning Policies</h3>
<ul>
<li><strong>Authors: </strong>Florent Delgrange, Guy Avni, Anna Lukina, Christian Schilling, Ann Nowé, Guillermo A. Pérez</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13785">https://arxiv.org/abs/2402.13785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13785">https://arxiv.org/pdf/2402.13785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13785]] Synthesis of Hierarchical Controllers Based on Deep Reinforcement  Learning Policies(https://arxiv.org/abs/2402.13785)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs). Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a "room". We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for modeling rooms. We address this challenge by developing a DRL procedure to train concise "latent" policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a model distillation step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation amid moving obstacles.</li>
<li><strong>摘要：</strong>我们提出了一种新的方法来解决建模为马尔可夫决策过程（MDP）的环境的控制器设计问题。具体来说，我们将分层 MDP 视为一个图，其中每个顶点都由称为“房间”的 MDP 填充。我们首先应用深度强化学习（DRL）来获取每个房间的低级策略，扩展到结构未知的大房间。然后，我们应用反应式综合来获得一个高级规划器，该规划器选择在每个房间中执行哪个低级策略。综合规划器的主要挑战是对建模室的需求。我们通过开发 DRL 程序来训练简洁的“潜在”策略以及 PAC 对其性能的保证来应对这一挑战。与以前的方法不同，我们的方法绕过了模型蒸馏步骤。我们的方法解决了 DRL 中奖励稀疏的问题，并实现了低级策略的可重用性。我们通过涉及移动障碍物中代理导航的案例研究证明了可行性。</li>
</ul>

<h3>Title: Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering  Dehumanizing Language</h3>
<ul>
<li><strong>Authors: </strong>Hezhao Zhang, Lasana Harris, Nafise Sadat Moosavi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13818">https://arxiv.org/abs/2402.13818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13818">https://arxiv.org/pdf/2402.13818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13818]] Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering  Dehumanizing Language(https://arxiv.org/abs/2402.13818)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups. Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models. However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task.</li>
<li><strong>摘要：</strong>非人化是仇恨言论的一种微妙但有害的表现形式，涉及否认个人的人性品质，并常常导致针对边缘群体的暴力。尽管自然语言处理在各个领域取得了重大进展，但其在检测非人性语言方面的应用受到限制，这很大程度上是由于该领域公开可用的注释数据的稀缺。本文评估了尖端 NLP 模型（包括 GPT-4、GPT-3.5 和 LLAMA-2）在识别非人性语言方面的性能。我们的研究结果表明，虽然这些模型显示出潜力，在区分非人性语言和更广泛的仇恨言论方面达到了 70% 的准确率，但它们也表现出了偏见。他们过于敏感地将其他形式的仇恨言论归类为特定目标群体子集的非人化，而更常见的是未能识别其他目标群体的非人化的明显案例。此外，利用性能最好的模型之一，我们自动注释了更大的数据集，以训练更易于访问的模型。然而，我们的研究结果表明，这些模型目前不满足此任务所需的高质量数据生成阈值。</li>
</ul>

<h3>Title: Kuaiji: the First Chinese Accounting Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jiayuan Luo, Songhua Yang, Xiaoling Qiu, Panyu Chen, Yufei Nai, Wenxuan Zeng, Wentao Zhang, Xinke Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13866">https://arxiv.org/abs/2402.13866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13866">https://arxiv.org/pdf/2402.13866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13866]] Kuaiji: the First Chinese Accounting Large Language Model(https://arxiv.org/abs/2402.13866)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.</li>
<li><strong>摘要：</strong>ChatGPT 和 GPT-4 等大型语言模型 (LLM) 在理解和生成自然语言方面表现出了令人印象深刻的熟练程度。然而，当他们负责适应会计等专业领域时，他们会遇到困难。为了应对这一挑战，我们引入了会稽，一种量身定制的会计大语言模型。会稽使用百川框架进行精心微调，其中包括持续的预训练和监督微调过程。在包含大量真实会计师与客户对话的数据集 CAtAcctQA 的支持下，会稽表现出了卓越的准确性和响应速度。我们的贡献包括创建第一个中国会计数据集、将会稽建立为领先的开源中国会计法学硕士，以及通过现实世界的会计场景验证其有效性。</li>
</ul>

<h3>Title: An Explainable Transformer-based Model for Phishing Email Detection: A  Large Language Model Approach</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Amaz Uddin, Iqbal H. Sarker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13871">https://arxiv.org/abs/2402.13871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13871">https://arxiv.org/pdf/2402.13871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13871]] An Explainable Transformer-based Model for Phishing Email Detection: A  Large Language Model Approach(https://arxiv.org/abs/2402.13871)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.</li>
<li><strong>摘要：</strong>网络钓鱼电子邮件是一种严重的网络威胁，它试图通过发送虚假电子邮件来欺骗用户，目的是窃取机密信息或造成经济损失。攻击者通常冒充值得信赖的实体，利用技术进步和复杂性使网络钓鱼的检测和预防变得更具挑战性。尽管进行了广泛的学术研究，网络钓鱼检测仍然是网络安全领域持续且艰巨的挑战。大型语言模型 (LLM) 和屏蔽语言模型 (MLM) 拥有巨大的潜力，可以提供创新解决方案来应对长期存在的挑战。在这篇研究论文中，我们提出了一种优化、微调的基于 Transformer 的 DistilBERT 模型，专为检测网络钓鱼电子邮件而设计。在检测过程中，我们使用网络钓鱼电子邮件数据集，并利用预处理技术来清理和解决不平衡类问题。通过我们的实验，我们发现我们的模型有效地实现了高精度，展示了其良好表现的能力。最后，我们使用可解释人工智能 (XAI) 技术（例如本地可解释模型不可知解释 (LIME) 和 Transformer Interpret）演示我们的微调模型，以解释我们的模型如何在网络钓鱼电子邮件的文本分类上下文中进行预测。</li>
</ul>

<h3>Title: $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Furu Wei, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13874">https://arxiv.org/abs/2402.13874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13874">https://arxiv.org/pdf/2402.13874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13874]] $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning(https://arxiv.org/abs/2402.13874)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在上下文学习（ICL）方面的卓越能力需要通过演示示例来激活。先前的工作广泛探索了 ICL 示例的选择，主要遵循“选择然后组织”范式，此类方法往往忽略示例之间的内部关系，并且存在训练和推理之间的不一致。在本文中，我们将问题表述为 $\textit{se}$quential $\textit{se}$lection 问题，并引入 $\texttt{Se}^2$，这是一种顺序感知方法，利用了法学硕士的反馈不同的上下文，有助于捕获示例之间的相互关系和顺序信息，显着丰富 ICL 提示的上下文和相关性。同时，我们利用波束搜索来寻找和构建示例序列，从而提高质量和多样性。来自 8 个不同类别的 23 个 NLP 任务的广泛实验表明，$\texttt{Se}^2$ 显着超越了竞争基线，并比随机选择实现了 42% 的相对改进。进一步深入的分析显示了所提出策略的有效性，突出了 $\texttt{Se}^2$ 在各种场景下卓越的稳定性和适应性。我们的代码将被发布以方便未来的研究。</li>
</ul>

<h3>Title: Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Lyu, Minghao Wu, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13887">https://arxiv.org/abs/2402.13887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13887">https://arxiv.org/pdf/2402.13887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13887]] Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models(https://arxiv.org/abs/2402.13887)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种应用中展示了卓越的功能，从根本上重塑了自然语言处理 (NLP) 研究的格局。然而，最近的评估框架通常依赖于法学硕士的输出概率进行预测，这主要是由于计算限制，与现实世界的法学硕士使用场景不同。尽管广泛采用，这些基于概率的评估策略的有效性仍然是一个悬而未决的研究问题。本研究旨在在使用法学硕士进行多项选择题（MCQ）的背景下审查这种基于概率的评估方法的有效性，强调其固有的局限性。我们的实证调查表明，流行的基于概率的评估方法与基于世代的预测不够一致。此外，由于计算限制，当前的评估框架通常通过基于输出概率的预测任务来评估法学硕士，而不是直接生成响应。我们说明这些基于概率的方法并不能有效地对应生成预测。我们的研究结果可以增强对法学硕士评估方法的理解，并为该领域的未来研究提供见解。</li>
</ul>

<h3>Title: Calibrating Large Language Models with Sample Consistency</h3>
<ul>
<li><strong>Authors: </strong>Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13904">https://arxiv.org/abs/2402.13904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13904">https://arxiv.org/pdf/2402.13904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13904]] Calibrating Large Language Models with Sample Consistency(https://arxiv.org/abs/2402.13904)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.</li>
<li><strong>摘要：</strong>准确衡量大型语言模型 (LLM) 预测的置信度对于其可靠应用至关重要。然而，法学硕士通常本质上是未经校准的，并且由于其专有性质和大规模而无法使用传统的校准技术。在这项工作中，我们探索了通过三个一致性度量从多个随机采样模型生成的分布中获取置信度的潜力。我们对九个推理数据集的各种开源和闭源模型进行了广泛的评估。结果表明，基于一致性的校准方法优于现有的事后方法。同时，我们发现中间解释、模型缩放和较大样本量等因素增强了校准，而指令调整使校准变得更加困难。此外，从一致性获得的置信度分数有可能提高模型性能。最后，我们针对各种 LM 的特性，提供了选择合适的一致性度量进行校准的实用指南。</li>
</ul>

<h3>Title: What Linguistic Features and Languages are Important in LLM Translation?</h3>
<ul>
<li><strong>Authors: </strong>Ryandito Diandaru, Lucky Susanto, Zilu Tang, Ayu Purwarianti, Derry Wijaya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13917">https://arxiv.org/abs/2402.13917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13917">https://arxiv.org/pdf/2402.13917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13917]] What Linguistic Features and Languages are Important in LLM Translation?(https://arxiv.org/abs/2402.13917)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 展示了跨多种任务的强大能力，包括机器翻译。我们的研究重点是评估 Llama2 的机器翻译能力，并探索翻译如何依赖于其训练数据中的语言。我们的实验表明，7B Llama2 模型对于它见过的所有语言都能产生高于 10 的 BLEU 分数，但对于它没有见过的语言则并非总是如此。与使用聊天版本或添加镜头计数相比，通过模型规模观察到的那些看不见的语言的大部分收益最多。此外，我们的语言距离分析表明，句法相似性并不总是决定翻译质量的主要语言因素。有趣的是，我们发现在特定情况下，某些语言尽管训练数据明显少于英语，但表现出与英语相当的强相关性。我们的发现为当前法学硕士的现状提供了新的视角，提高了以英语以外的语言为中心的法学硕士可能为多语言模式提供更有效基础的可能性。</li>
</ul>

<h3>Title: SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in  Clinical Summarization</h3>
<ul>
<li><strong>Authors: </strong>Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining Wang, Vidhi Dhaval Mody, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13919">https://arxiv.org/abs/2402.13919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13919">https://arxiv.org/pdf/2402.13919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13919]] SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in  Clinical Summarization(https://arxiv.org/abs/2402.13919)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality. This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback. Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality.</li>
<li><strong>摘要：</strong>GPT 和 Llama 等大型语言模型 (LLM) 在摘要任务中取得了显着的成就，但与事实不准确作斗争，这是临床 NLP 应用中的一个关键问题，错误可能会导致严重后果。为了解决用于事实对齐的专家注释数据的高成本和有限的可用性，本研究引入了一种创新管道，利用 GPT-3.5 和 GPT-4 生成高质量的反馈，旨在增强临床记录摘要中的事实一致性。我们的研究主要集中在编辑反馈上，反映了医疗专业人员在不需要额外注释的情况下改进人工智能系统输出的实际场景。尽管 GPT 在各种临床 NLP 任务（例如医疗执照考试）方面拥有经过验证的专业知识，但对其提供专家级编辑反馈以提高较弱的 LM 或 LLM 生成质量的能力的研究还很少。这项工作利用 GPT 在临床 NLP 方面的先进功能来提供专家级的编辑反馈。通过使用基于 GPT 编辑反馈的两种不同的对齐算法（DPO 和 SALT），我们的目标是减少幻觉并与医学事实紧密结合，努力缩小人工智能生成的内容与事实准确性之间的差距。这凸显了 GPT 编辑在增强临床事实一致性方面的巨大潜力。</li>
</ul>

<h3>Title: Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content</h3>
<ul>
<li><strong>Authors: </strong>Federico Bianchi, James Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13926">https://arxiv.org/abs/2402.13926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13926">https://arxiv.org/pdf/2402.13926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13926]] Large Language Models are Vulnerable to Bait-and-Switch Attacks for  Generating Harmful Content(https://arxiv.org/abs/2402.13926)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）生成欺骗性和破坏性内容所带来的风险一直是大量研究的主题，但即使是安全的生成也可能导致有问题的下游影响。在我们的研究中，我们将重点转移到如何通过诱饵和转换攻击轻松地将来自法学硕士的安全文本变成潜在危险的内容。在此类攻击中，用户首先向法学硕士提出安全问题，然后采用简单的查找和替换事后技术将输出操纵为有害的叙述。这种方法在产生有毒内容方面的惊人功效凸显了为法学硕士开发可靠的安全护栏所面临的重大挑战。我们特别强调，仅仅关注法学硕士逐字输出的安全性是不够的，我们还需要考虑事后转型。</li>
</ul>

<h3>Title: Enhancing Reinforcement Learning Agents with Local Guides</h3>
<ul>
<li><strong>Authors: </strong>Paul Daoudi, Bogdan Robu, Christophe Prieur, Ludovic Dos Santos, Merwan Barlier</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13930">https://arxiv.org/abs/2402.13930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13930">https://arxiv.org/pdf/2402.13930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13930]] Enhancing Reinforcement Learning Agents with Local Guides(https://arxiv.org/abs/2402.13930)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.</li>
<li><strong>摘要：</strong>本文解决了将本地指导策略集成到强化学习代理中的问题。为此，我们在引入基于噪声策略切换过程的新算法之前，展示如何使现有算法适应这种设置。这种方法建立在适当的近似政策评估（APE）方案的基础上，提供扰动，仔细引导当地指南采取更好的行动。我们在一组经典的强化学习问题上评估了我们的方法，包括安全关键系统，在这些系统中，代理无法进入某些区域，从而有引发灾难性后果的风险。在所有提出的环境中，我们的代理被证明能够有效地利用这些策略来提高任何基于 APE 的强化学习算法的性能，尤其是在其第一个学习阶段。</li>
</ul>

<h3>Title: Do Efficient Transformers Really Save Computation?</h3>
<ul>
<li><strong>Authors: </strong>Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13934">https://arxiv.org/abs/2402.13934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13934">https://arxiv.org/pdf/2402.13934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13934]] Do Efficient Transformers Really Save Computation?(https://arxiv.org/abs/2402.13934)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.</li>
<li><strong>摘要：</strong>随着基于 Transformer 的语言模型在越来越大的数据集和大量参数上进行训练，寻找标准 Transformer 的更有效替代方案变得非常有价值。虽然已经提出了许多高效的 Transformer 和 Transformer 替代方案，但没有一个提供理论保证它们是标准 Transformer 的合适替代品。这使得确定何时使用特定模型以及优先考虑哪些方向以进行进一步调查变得具有挑战性。在本文中，我们的目标是了解高效 Transformer 的功能和局限性，特别是稀疏 Transformer 和线性 Transformer。我们专注于他们的思维链（CoT）提示所展示的推理能力，并遵循以前的工作将它们建模为动态规划（DP）问题。我们的结果表明，虽然这些模型具有足够的表达能力来解决一般的 DP 任务，但与预期相反，它们需要一个随问题规模而缩放的模型规模。尽管如此，我们还是确定了一类 DP 问题，对于这些问题，这些模型比标准 Transformer 更有效。我们通过代表性 DP 任务的实验证实了我们的理论结果，增加了对高效 Transformers 实际优势和劣势的理解。</li>
</ul>

<h3>Title: AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning</h3>
<ul>
<li><strong>Authors: </strong>Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13946">https://arxiv.org/abs/2402.13946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13946">https://arxiv.org/pdf/2402.13946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13946]] AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning(https://arxiv.org/abs/2402.13946)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits. In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all GNNs considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated. For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against GNNs for all classes of problems.</li>
<li><strong>摘要：</strong>机器学习在解决几个关键的硬件安全问题方面显示出了巨大的前景。特别是，研究人员开发了基于图神经网络 (GNN) 的新颖技术，用于检测知识产权 (IP) 盗版、检测硬件木马 (HT) 和逆向工程电路等。这些技术表现出了出色的准确性，并受到了社区的广泛关注。然而，由于这些技术用于安全应用，因此必须对其进行彻底评估并确保它们稳健且不会损害集成电路的安全性。在这项工作中，我们提出了 AttackGNN，这是红队对硬件安全中基于 GNN 的技术的第一个攻击。为此，我们设计了一种新颖的强化学习（RL）代理，它可以针对基于 GNN 的技术生成对抗性示例，即电路。我们克服了与有效性、可扩展性和通用性相关的三个挑战，设计了一个有效的 RL 代理。我们针对硬件安全中四类关键问题的五种基于 GNN 的技术：IP 盗版、检测/本地化 HT、逆向工程和硬件混淆。通过我们的方法，我们设计了可以愚弄本工作中考虑的所有 GNN 的电路。例如，为了逃避 IP 盗版检测，我们生成对抗性盗版电路，欺骗基于 GNN 的防御，将我们精心设计的电路归类为非盗版。为了攻击 HT 定位 GNN，我们的攻击生成了 HT 感染的电路，欺骗了所有测试电路上的防御。对于所有类别的问题，我们都获得了与 GNN 类似的 100% 成功率。</li>
</ul>

<h3>Title: Making Reasoning Matter: Measuring and Improving Faithfulness of  Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13950">https://arxiv.org/abs/2402.13950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13950">https://arxiv.org/pdf/2402.13950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13950]] Making Reasoning Matter: Measuring and Improving Faithfulness of  Chain-of-Thought Reasoning(https://arxiv.org/abs/2402.13950)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已被证明在回答问题之前被要求逐步推理时表现更好。然而，尚不清楚模型的最终答案在多大程度上忠实于所述推理步骤。在本文中，我们对 12 个法学硕士进行了因果中介分析，以检查法学硕士生成的中间推理步骤如何影响最终结果，并发现法学硕士在生成答案时并未可靠地使用其中间推理步骤。为了解决这个问题，我们引入了 FRODO，这是一个定制小型 LM 的框架，用于生成正确的推理步骤，并对这些步骤进行稳健的推理。 FRODO 由一个推理模块和一个推理模块组成，推理模块学习使用隐式因果奖励函数生成正确的推理步骤，推理模块学习使用反事实和因果偏好目标忠实地推理这些中间推理。我们的实验表明，FRODO 的性能明显优于四个竞争基准。此外，FRODO 提高了推理 LM 的鲁棒性和泛化能力，在分布外测试集上产生更高的性能。最后，我们发现 FRODO 的基本原理比标准监督微调更忠实于其最终答案预测。</li>
</ul>

<h3>Title: Measuring Social Biases in Masked Language Models by Proxy of Prediction  Quality</h3>
<ul>
<li><strong>Authors: </strong>Rahul Zalkikar, Kanchan Chandra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13954">https://arxiv.org/abs/2402.13954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13954">https://arxiv.org/pdf/2402.13954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13954]] Measuring Social Biases in Masked Language Models by Proxy of Prediction  Quality(https://arxiv.org/abs/2402.13954)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings). Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications. In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions, and assess the preference of MLMs towards disadvantaged and advantaged groups. We compare bias estimations with those produced by other evaluation methods using two benchmark datasets, finding relatively high religious and disability biases across considered MLMs and low gender bias in one dataset relative to the other. Our measures outperform others in their agreement with human annotators. We extend on previous work by evaluating social biases introduced after re-training an MLM under the masked language modeling objective (w.r.t. the model's pre-trained base), and find that proposed measures produce more accurate estimations of relative preference for biased sentences between transformers than others based on our methods.</li>
<li><strong>摘要：</strong>社会和政治科学家通常旨在发现和测量文本数据表示（嵌入）中的明显偏见。基于 Transformer 的创新语言模型可生成上下文感知的标记嵌入，并在各种自然语言任务中实现了最先进的性能，但已被证明可以为下游应用程序编码不需要的偏差。在本文中，我们使用迭代掩蔽实验中提出的代理函数来评估由掩蔽语言建模目标训练的变压器编码的社会偏见，以衡量变压器模型预测的质量，并评估传销对弱势群体和优势群体的偏好。我们使用两个基准数据集将偏差估计与其他评估方法产生的偏差估计进行比较，发现所考虑的传销中相对较高的宗教和残疾偏差，而一个数据集中相对于另一个数据集的性别偏差较低。我们的措施在与人类注释者的一致性方面优于其他措施。我们通过评估在掩蔽语言建模目标（w.r.t.模型的预训练基础）下重新训练 MLM 后引入的社会偏见来扩展之前的工作，并发现所提出的措施可以比 Transformer 之间对偏见句子的相对偏好产生更准确的估计。其他基于我们的方法。</li>
</ul>

<h3>Title: Towards Building Multilingual Language Model for Medicine</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13963">https://arxiv.org/abs/2402.13963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13963">https://arxiv.org/pdf/2402.13963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13963]] Towards Building Multilingual Language Model for Medicine(https://arxiv.org/abs/2402.13963)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.</li>
<li><strong>摘要：</strong>在本文中，我们的目标是开发一种开源的多语言医学语言模型，使来自不同地区的更广泛的、语言多样的受众受益。总的来说，我们从以下几个方面提出了贡献：首先，针对多语言医学特定的适应，我们构建了一个新的多语言医学语料库，其中包含大约 25.5B 个包含 6 种主要语言的标记，称为 MMedC，可以进行自回归训练对于现有的普通法学硕士。其次，为了监测医学领域多语言法学硕士的发展，我们提出了一个新的具有基本原理的多语言医学多选问答基准，称为MMedBench；第三，我们在基准测试中评估了一些流行的开源大语言模型 (LLM)，以及在 MMedC 上进一步训练的自回归模型，因此，我们的最终模型称为 MMedLM 2，只有 7B 个参数，与所有其他开源模型相比，它实现了卓越的性能，甚至可以在 MMedBench 上与 GPT-4 相媲美。我们将公开资源，包括代码、模型权重和数据集。</li>
</ul>

<h3>Title: Analysing The Impact of Sequence Composition on Language Model  Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhao, Yuanbin Qu, Konrad Staniszewski, Szymon Tworkowski, Wei Liu, Piotr Miłoś, Yuxiang Wu, Pasquale Minervini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13991">https://arxiv.org/abs/2402.13991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13991">https://arxiv.org/pdf/2402.13991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13991]] Analysing The Impact of Sequence Composition on Language Model  Pre-Training(https://arxiv.org/abs/2402.13991)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve in-context learning (+11.6\%), knowledge memorisation (+9.8\%), and context utilisation (+7.2\%) abilities of language models without sacrificing efficiency.</li>
<li><strong>摘要：</strong>大多数语言模型预训练框架将多个文档连接成固定长度的序列，并使用因果屏蔽来计算每个标记在给定上下文的情况下的可能性；该策略因其简单和高效而被广泛采用。然而，迄今为止，预训练序列组成策略对模型泛化特性的影响仍未得到充分探索。在这项工作中，我们发现应用因果屏蔽可能会导致在预训练期间包含来自先前文档的干扰信息，这会对模型在语言建模和下游任务上的性能产生负面影响。在文档内因果屏蔽中，每个标记的可能性仅取决于同一文档中的先前标记，从而消除了先前文档中潜在的干扰信息，并显着提高了性能。此外，我们发现连接相关文档可以减少预训练期间的一些潜在干扰，并且我们提出的基于检索的高效序列构建方法 BM25Chunk 可以改善上下文学习（+11.6\%）、知识记忆（+9.8\ %) 和语言模型的上下文利用 (+7.2\%) 能力，而不牺牲效率。</li>
</ul>

<h3>Title: Hallucinations or Attention Misdirection? The Path to Strategic Value  Extraction in Business Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aline Ioste</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14002">https://arxiv.org/abs/2402.14002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14002">https://arxiv.org/pdf/2402.14002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14002]] Hallucinations or Attention Misdirection? The Path to Strategic Value  Extraction in Business Using Large Language Models(https://arxiv.org/abs/2402.14002)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs.</li>
<li><strong>摘要：</strong>具有变压器架构的大型语言模型彻底改变了文本生成领域，设定了前所未有的基准。尽管法学硕士拥有令人印象深刻的能力，但他们仍因产生偏离事实准确性或表现出逻辑不一致的结果而受到批评，这种现象通常被称为幻觉。然而，这个术语经常被误用于任何偏离教师期望的结果，本文将其定义为注意力误导，而不是真正的幻觉。了解幻觉和注意力误导之间的区别在商业环境中变得越来越重要，在商业环境中，此类错误的后果可能会显着影响从这些固有的预训练模型中提取价值。本文重点介绍了 PGI、角色、分组和智能方法的最佳实践，该战略框架在 GPT 为应对实际业务挑战而生成的 4,000 个响应中实现了仅为 3.15% 的惊人错误率。它强调，通过为实验配备知识，企业可以通过使用这些本地预先训练的模型来释放创新机会。这强化了这样一种观念：基于熟练团队的战略应用可以最大限度地发挥法学硕士等新兴技术的优势。</li>
</ul>

<h3>Title: Can Watermarks Survive Translation? On the Cross-lingual Consistency of  Text Watermark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14007">https://arxiv.org/abs/2402.14007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14007">https://arxiv.org/pdf/2402.14007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14007]] Can Watermarks Survive Translation? On the Cross-lingual Consistency of  Text Watermark for Large Language Models(https://arxiv.org/abs/2402.14007)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.</li>
<li><strong>摘要：</strong>文本水印技术旨在标记和识别大型语言模型 (LLM) 生成的内容，以防止滥用。在本研究中，我们在文本水印中引入了“跨语言一致性”的概念，它评估文本水印在翻译成其他语言后保持其有效性的能力。两个法学硕士和三种水印方法的初步实证结果表明，当前的文本水印技术在文本翻译成各种语言时缺乏一致性。基于这一观察，我们提出了一种跨语言水印去除攻击（CWRA）来绕过水印，首先从主语言的法学硕士那里获得响应，然后将其翻译成目标语言。 CWRA 可以通过将曲线下面积 (AUC) 从 0.95 降低到 0.67 来有效去除水印，而不会造成性能损失。此外，我们分析了有助于文本水印跨语言一致性的两个关键因素，并提出了一种防御方法，将 CWRA 下的 AUC 从 0.67 提高到 0.88。</li>
</ul>

<h3>Title: OlympiadBench: A Challenging Benchmark for Promoting AGI with  Olympiad-Level Bilingual Multimodal Scientific Problems</h3>
<ul>
<li><strong>Authors: </strong>Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14008">https://arxiv.org/abs/2402.14008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14008">https://arxiv.org/pdf/2402.14008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14008]] OlympiadBench: A Challenging Benchmark for Promoting AGI with  Olympiad-Level Bilingual Multimodal Scientific Problems(https://arxiv.org/abs/2402.14008)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors.</li>
<li><strong>摘要：</strong>最近的进展表明，大型语言模型（LLM）和大型多模态模型（LMM）在各种任务中超越了一般人类的能力，接近多个领域的人类专家的熟练水平。随着传统基准对这些模型的挑战性越来越小，新的严格挑战对于衡量其先进能力至关重要。在这项工作中，我们推出了 OlympiadBench，这是一个奥林匹克级双语多模式科学基准，包含来自奥林匹克级数学和物理竞赛（包括中国高考）的 8,952 个问题。每个问题都配有专家级注释，用于逐步推理。在 OlympiadBench 上评估顶级模型时，我们实施了全面的评估方法来准确评估模型响应。值得注意的是，表现最好的模型 GPT-4V 在 OlympiadBench 上的平均得分为 17.23%，而物理得分仅为 11.28%，凸显了基准测试的严谨性和物理推理的复杂性。我们针对 GPT-4V 的分析指出了幻觉、知识遗漏和逻辑谬误等普遍问题。我们希望我们具有挑战性的基准能够成为帮助未来 AGI 研究工作的宝贵资源。</li>
</ul>

<h3>Title: Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment</h3>
<ul>
<li><strong>Authors: </strong>Vyas Raina, Adian Liusie, Mark Gales</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14016">https://arxiv.org/abs/2402.14016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14016">https://arxiv.org/pdf/2402.14016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14016]] Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on  Zero-shot LLM Assessment(https://arxiv.org/abs/2402.14016)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5. This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是功能强大的零样本评估器，并且越来越多地用于现实世界中，例如笔试或基准测试系统。尽管如此，现有的工作还没有分析法官法学硕士针对试图操纵输出的对手的脆弱性。这项工作提出了第一个关于评估法学硕士的对抗性鲁棒性的研究，其中我们搜索简短的通用短语，当附加到文本中时可以欺骗法学硕士提供高评估分数。 SummEval 和 TopicalChat 上的实验表明，LLM 评分和成对 LLM 比较评估都容易受到简单串联攻击，特别是 LLM 评分非常容易受到影响，并且无论输入文本质量如何，都可以产生最大评估分数。有趣的是，此类攻击是可转移的，在小型开源 LLM 上学到的短语可以应用于更大的闭源模型，例如 GPT3.5。这凸显了不同法官法学硕士规模、类别和方法中对抗性漏洞的普遍性。我们的研究结果引起了人们对法学硕士作为法官方法的可靠性的严重担忧，并强调了在高风险的现实场景中部署之前解决法学硕士评估方法中的漏洞的重要性。</li>
</ul>

<h3>Title: Coercing LLMs to do and reveal (almost) anything</h3>
<ul>
<li><strong>Authors: </strong>Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14020">https://arxiv.org/abs/2402.14020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14020">https://arxiv.org/pdf/2402.14020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14020]] Coercing LLMs to do and reveal (almost) anything(https://arxiv.org/abs/2402.14020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.</li>
<li><strong>摘要：</strong>最近的研究表明，对大型语言模型 (LLM) 的对抗性攻击可以“越狱”模型，从而做出有害的语句。在这项工作中，我们认为针对法学硕士的对抗性攻击的范围远大于单纯的越狱。我们提供了可能的攻击面和攻击目标的广泛概述。基于一系列具体示例，我们对强制各种非预期行为（例如误导、模型控制、拒绝服务或数据提取）的攻击进行讨论、分类和系统化。我们在受控实验中分析这些攻击，发现其中许多攻击源于预训练具有编码能力的 LLM 的做法，以及常见 LLM 词汇表中持续存在的奇怪“故障”标记，出于安全原因应将其删除。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
