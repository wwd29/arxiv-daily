<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-12</h1>
<h3>Title: Identifying and Decomposing Compound Ingredients in Meal Plans Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Leon Kopitar, Leon Bedrac, Larissa J Strath, Jiang Bian, Gregor Stiglic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05892">https://arxiv.org/abs/2411.05892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05892">https://arxiv.org/pdf/2411.05892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05892]] Identifying and Decomposing Compound Ingredients in Meal Plans Using Large Language Models(https://arxiv.org/abs/2411.05892)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This study explores the effectiveness of Large Language Models in meal planning, focusing on their ability to identify and decompose compound ingredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral (8x7b)-to assess their proficiency in recognizing and breaking down complex ingredient combinations. Preliminary results indicate that while Llama-3 (70b) and GPT-4o excels in accurate decomposition, all models encounter difficulties with identifying essential elements like seasonings and oils. Despite strong overall performance, variations in accuracy and completeness were observed across models. These findings underscore LLMs' potential to enhance personalized nutrition but highlight the need for further refinement in ingredient decomposition. Future research should address these limitations to improve nutritional recommendations and health outcomes.</li>
<li><strong>摘要：</strong>本研究探讨了大型语言模型在膳食计划中的有效性，重点关注其识别和分解复合成分的能力。我们评估了三种模型 - GPT-4o、Llama-3 (70b) 和 Mixtral (8x7b) - 以评估它们识别和分解复杂成分组合的能力。初步结果表明，虽然 Llama-3 (70b) 和 GPT-4o 在准确分解方面表现出色，但所有模型在识别调味料和油等基本元素方面都遇到了困难。尽管整体性能强劲，但不同模型的准确性和完整性存在差异。这些发现强调了 LLM 增强个性化营养的潜力，但也强调了进一步改进成分分解的必要性。未来的研究应解决这些局限性，以改善营养建议和健康结果。</li>
</ul>

<h3>Title: SSSD: Simply-Scalable Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Michele Marzollo, Jiawei Zhuang, Niklas Roemer, Lorenz K. Müller, Lukas Cavigelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05894">https://arxiv.org/abs/2411.05894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05894">https://arxiv.org/pdf/2411.05894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05894]] SSSD: Simply-Scalable Speculative Decoding(https://arxiv.org/abs/2411.05894)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Over the past year, Speculative Decoding has gained popularity as a technique for accelerating Large Language Model inference. While several methods have been introduced, most struggle to deliver satisfactory performance at batch sizes typical for data centers ($\geq 8$) and often involve significant deployment complexities. In this work, we offer a theoretical explanation of how Speculative Decoding can be effectively utilized with larger batch sizes. We also introduce a method that integrates seamlessly into existing systems without additional training or the complexity of deploying a small LLM. In a continuous batching setting, we achieve a 4x increase in throughput without any latency impact for short context generation, and a 1.7-2x improvement in both latency and throughput for longer contexts.</li>
<li><strong>摘要：</strong>在过去的一年中，推测解码作为一种加速大型语言模型推理的技术而广受欢迎。虽然已经引入了几种方法，但大多数方法都难以在数据中心典型的批量大小 ($\geq 8$) 下提供令人满意的性能，并且通常涉及相当大的部署复杂性。在这项工作中，我们从理论角度解释了如何有效地将推测解码用于更大的批量大小。我们还介绍了一种无缝集成到现有系统中的方法，无需额外的培训或部署小型 LLM 的复杂性。在连续批处理设置中，我们在短上下文生成中实现了 4 倍的吞吐量提升，且没有任何延迟影响，在较长上下文中，延迟和吞吐量均提高了 1.7-2 倍。</li>
</ul>

<h3>Title: One Small and One Large for Document-level Event Argument Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jiaren Peng, Hongda Sun, Wenzhong Yang, Fuyuan Wei, Liang He, Liejun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05895">https://arxiv.org/abs/2411.05895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05895">https://arxiv.org/pdf/2411.05895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05895]] One Small and One Large for Document-level Event Argument Extraction(https://arxiv.org/abs/2411.05895)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Document-level Event Argument Extraction (EAE) faces two challenges due to increased input length: 1) difficulty in distinguishing semantic boundaries between events, and 2) interference from redundant information. To address these issues, we propose two methods. The first method introduces the Co and Structure Event Argument Extraction model (CsEAE) based on Small Language Models (SLMs). CsEAE includes a co-occurrences-aware module, which integrates information about all events present in the current input through context labeling and co-occurrences event prompts extraction. Additionally, CsEAE includes a structure-aware module that reduces interference from redundant information by establishing structural relationships between the sentence containing the trigger and other sentences in the document. The second method introduces new prompts to transform the extraction task into a generative task suitable for Large Language Models (LLMs), addressing gaps in EAE performance using LLMs under Supervised Fine-Tuning (SFT) conditions. We also fine-tuned multiple datasets to develop an LLM that performs better across most datasets. Finally, we applied insights from CsEAE to LLMs, achieving further performance improvements. This suggests that reliable insights validated on SLMs are also applicable to LLMs. We tested our models on the Rams, WikiEvents, and MLEE datasets. The CsEAE model achieved improvements of 2.1\%, 2.3\%, and 3.2\% in the Arg-C F1 metric compared to the baseline, PAIE~\cite{PAIE}. For LLMs, we demonstrated that their performance on document-level datasets is comparable to that of SLMs~\footnote{All code is available at this https URL}.</li>
<li><strong>摘要：</strong>由于输入长度增加，文档级事件论元提取 (EAE) 面临两大挑战：1) 难以区分事件之间的语义边界，2) 冗余信息的干扰。为了解决这些问题，我们提出了两种方法。第一种方法引入了基于小型语言模型 (SLM) 的共现和结构事件论元提取模型 (CsEAE)。CsEAE 包含一个共现感知模块，该模块通过上下文标记和共现事件提示提取整合了当前输入中存在的所有事件的信息。此外，CsEAE 还包括一个结构感知模块，该模块通过在包含触发器的句子和文档中的其他句子之间建立结构关系来减少冗余信息的干扰。第二种方法引入了新的提示，将提取任务转换为适合大型语言模型 (LLM) 的生成任务，解决了在监督微调 (SFT) 条件下使用 LLM 的 EAE 性能差距。我们还对多个数据集进行了微调，以开发出在大多数数据集上表现更好的 LLM。最后，我们将 CsEAE 的见解应用于 LLM，从而进一步提高了性能。这表明在 SLM 上验证的可靠见解也适用于 LLM。我们在 Rams、WikiEvents 和 MLEE 数据集上测试了我们的模型。与基线 PAIE~\cite{PAIE} 相比，CsEAE 模型在 Arg-C F1 指标上实现了 2.1\%、2.3\% 和 3.2\% 的改进。对于 LLM，我们证明了它们在文档级数据集上的性能与 SLM 相当~\footnote{所有代码均可在此 https URL 上找到}。</li>
</ul>

<h3>Title: Humans Continue to Outperform Large Language Models in Complex Clinical Decision-Making: A Study with Medical Calculators</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Wan, Qiao Jin, Joey Chan, Guangzhi Xiong, Serina Applebaum, Aidan Gilson, Reid McMurry, R. Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05897">https://arxiv.org/abs/2411.05897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05897">https://arxiv.org/pdf/2411.05897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05897]] Humans Continue to Outperform Large Language Models in Complex Clinical Decision-Making: A Study with Medical Calculators(https://arxiv.org/abs/2411.05897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have been assessed for general medical knowledge using medical licensing exams, their ability to effectively support clinical decision-making tasks, such as selecting and using medical calculators, remains uncertain. Here, we evaluate the capability of both medical trainees and LLMs to recommend medical calculators in response to various multiple-choice clinical scenarios such as risk stratification, prognosis, and disease diagnosis. We assessed eight LLMs, including open-source, proprietary, and domain-specific models, with 1,009 question-answer pairs across 35 clinical calculators and measured human performance on a subset of 100 questions. While the highest-performing LLM, GPT-4o, provided an answer accuracy of 74.3% (CI: 71.5-76.9%), human annotators, on average, outperformed LLMs with an accuracy of 79.5% (CI: 73.5-85.0%). With error analysis showing that the highest-performing LLMs continue to make mistakes in comprehension (56.6%) and calculator knowledge (8.1%), our findings emphasize that humans continue to surpass LLMs on complex clinical tasks such as calculator recommendation.</li>
<li><strong>摘要：</strong>尽管已经使用医师执照考试对大型语言模型 (LLM) 的一般医学知识进行了评估，但它们是否能够有效支持临床决策任务（例如选择和使用医学计算器），仍不确定。在这里，我们评估了医学实习生和 LLM 针对各种多项选择临床场景（例如风险分层、预后和疾病诊断）推荐医学计算器的能力。我们评估了八个 LLM，包括开源、专有和领域特定模型，其中包含 35 个临床计算器中的 1,009 个问答对，并在 100 个问题子集上测量了人类表现。虽然表现最高的 LLM GPT-4o 提供了 74.3%（CI：71.5-76.9%）的答案准确率，但人类注释者的平均表现优于 LLM，准确率为 79.5%（CI：73.5-85.0%）。错误分析显示，即使是表现最好的 LLM 仍然会在理解（56.6%）和计算器知识（8.1%）方面犯错误，我们的研究结果强调，人类在计算器推荐等复杂的临床任务上继续超越 LLM。</li>
</ul>

<h3>Title: Reducing Distraction in Long-Context Language Models by Focused Learning</h3>
<ul>
<li><strong>Authors: </strong>Zijun Wu, Bingyuan Liu, Ran Yan, Lei Chen, Thomas Delteil</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05928">https://arxiv.org/abs/2411.05928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05928">https://arxiv.org/pdf/2411.05928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05928]] Reducing Distraction in Long-Context Language Models by Focused Learning(https://arxiv.org/abs/2411.05928)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant information dominates lengthy contexts, causing LLMs to lose focus on the most relevant segments. To address this, we propose a novel training method that enhances LLMs' ability to discern relevant information through a unique combination of retrieval-based data augmentation and contrastive learning. Specifically, during fine-tuning with long contexts, we employ a retriever to extract the most relevant segments, serving as augmented inputs. We then introduce an auxiliary contrastive learning objective to explicitly ensure that outputs from the original context and the retrieved sub-context are closely aligned. Extensive experiments on long single-document and multi-document QA benchmarks demonstrate the effectiveness of our proposed method.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展显著增强了其处理长上下文的能力。然而，由于干扰问题，有效利用这种长上下文仍然是一项挑战，其中无关信息主导着长上下文，导致 LLM 无法关注最相关的片段。为了解决这个问题，我们提出了一种新颖的训练方法，通过基于检索的数据增强和对比学习的独特组合，增强了 LLM 辨别相关信息的能力。具体而言，在使用长上下文进行微调时，我们使用检索器提取最相关的片段，作为增强输入。然后，我们引入辅助对比学习目标，以明确确保原始上下文和检索到的子上下文的输出紧密一致。在长单文档和多文档 QA 基准上进行的大量实验证明了我们提出的方法的有效性。</li>
</ul>

<h3>Title: BERTrend: Neural Topic Modeling for Emerging Trends Detection</h3>
<ul>
<li><strong>Authors: </strong>Allaa Boutaleb, Jerome Picault, Guillaume Grosjean</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05930">https://arxiv.org/abs/2411.05930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05930">https://arxiv.org/pdf/2411.05930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05930]] BERTrend: Neural Topic Modeling for Emerging Trends Detection(https://arxiv.org/abs/2411.05930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Detecting and tracking emerging trends and weak signals in large, evolving text corpora is vital for applications such as monitoring scientific literature, managing brand reputation, surveilling critical infrastructure and more generally to any kind of text-based event detection. Existing solutions often fail to capture the nuanced context or dynamically track evolving patterns over time. BERTrend, a novel method, addresses these limitations using neural topic modeling in an online setting. It introduces a new metric to quantify topic popularity over time by considering both the number of documents and update frequency. This metric classifies topics as noise, weak, or strong signals, flagging emerging, rapidly growing topics for further investigation. Experimentation on two large real-world datasets demonstrates BERTrend's ability to accurately detect and track meaningful weak signals while filtering out noise, offering a comprehensive solution for monitoring emerging trends in large-scale, evolving text corpora. The method can also be used for retrospective analysis of past events. In addition, the use of Large Language Models together with BERTrend offers efficient means for the interpretability of trends of events.</li>
<li><strong>摘要：</strong>在大型、不断发展的文本语料库中检测和跟踪新兴趋势和弱信号对于监测科学文献、管理品牌声誉、监视关键基础设施以及更广泛地说任何类型的基于文本的事件检测等应用至关重要。现有的解决方案通常无法捕捉细微的上下文或动态跟踪随时间演变的模式。BERTrend 是一种新颖的方法，它使用在线环境中的神经主题建模来解决这些限制。它引入了一种新的指标，通过考虑文档数量和更新频率来量化主题随时间变化的流行度。该指标将主题分为噪声、弱信号或强信号，标记新兴、快速增长的主题以供进一步研究。在两个大型真实数据集上进行的实验证明了 BERTrend 能够准确检测和跟踪有意义的弱信号，同时滤除噪声，为监测大规模、不断发展的文本语料库中的新兴趋势提供了全面的解决方案。该方法还可用于回顾性分析过去的事件。此外，将大型语言模型与 BERTrend 结合使用为解释事件趋势提供了有效的方法。</li>
</ul>

<h3>Title: NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts</h3>
<ul>
<li><strong>Authors: </strong>Yen-Ting Lin, Chao-Han Huck Yang, Zhehuai Chen, Piotr Zelasko, Xuesong Yang, Zih-Ching Chen, Krishna C Puvvada, Szu-Wei Fu, Ke Hu, Jun Wei Chiu, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05945">https://arxiv.org/abs/2411.05945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05945">https://arxiv.org/pdf/2411.05945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05945]] NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts(https://arxiv.org/abs/2411.05945)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an ``expert'' of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative $5.0$% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to $27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.</li>
<li><strong>摘要：</strong>构建通用的后识别错误校正器提出了一个关键问题：我们如何才能最有效地在大量混合领域数据集上训练模型？答案在于学习特定于数据集的特征并在单个模型中消化它们的知识。以前的方法通过使用单独的校正语言模型来实现这一点，从而导致参数显著增加。在这项工作中，我们提出了混合专家作为一种解决方案，强调 MoE 不仅仅是一个可扩展性工具。我们提出了一种多任务校正 MoE，通过学习将每个数据集的标记路由到其映射的专家，我们训练专家成为语音到文本、语言到文本和视觉到文本数据集的“专家”。在 Open ASR Leaderboard 上的实验表明，我们通过实现平均相对 $5.0$% WER 减少和语音和翻译任务的 BLEU 分数的显着提高，探索了新的最先进性能。在零样本评估中，NeKo 的表现优于 GPT-3.5 和 Claude-Opus，在 Hyporadise 基准中相对 WER 降低了 $15.5$% 至 $27.6$%。作为多任务模型，NeKo 在语法和 OCR 后校正方面表现出色。</li>
</ul>

<h3>Title: The Empirical Impact of Data Sanitization on Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anwesan Pal, Radhika Bhargava, Kyle Hinsz, Jacques Esterhuizen, Sudipta Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05978">https://arxiv.org/abs/2411.05978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05978">https://arxiv.org/pdf/2411.05978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05978]] The Empirical Impact of Data Sanitization on Language Models(https://arxiv.org/abs/2411.05978)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Data sanitization in the context of language modeling involves identifying sensitive content, such as personally identifiable information (PII), and redacting them from a dataset corpus. It is a common practice used in natural language processing (NLP) to maintain privacy. Nevertheless, the impact of data sanitization on the language understanding capability of a language model remains less studied. This paper empirically analyzes the effects of data sanitization across several benchmark language-modeling tasks including comprehension question answering (Q&A), entailment, sentiment analysis, and text classification. Our experiments cover a wide spectrum comprising finetuning small-scale language models, to prompting large language models (LLMs), on both original and sanitized datasets, and comparing their performance across the tasks. Interestingly, our results suggest that for some tasks such as sentiment analysis or entailment, the impact of redaction is quite low, typically around 1-5%, while for tasks such as comprehension Q&A there is a big drop of >25% in performance observed in redacted queries as compared to the original. For tasks that have a higher impact, we perform a deeper dive to inspect the presence of task-critical entities. Finally, we investigate correlation between performance and number of redacted entities, and also suggest a strategy to repair an already redacted dataset by means of content-based subsampling. Additional details are available at this https URL.</li>
<li><strong>摘要：</strong>语言建模环境中的数据清理涉及识别敏感内容（例如个人身份信息 (PII)）并将其从数据集语料库中删除。这是自然语言处理 (NLP) 中用于保护隐私的常见做法。然而，数据清理对语言模型的语言理解能力的影响仍然研究较少。本文通过实证分析了数据清理对几个基准语言建模任务的影响，包括理解问答 (Q&A)、蕴涵、情绪分析和文本分类。我们的实验涵盖了广泛的范围，包括在原始数据集和清理后的数据集上微调小规模语言模型，以提示大型语言模型 (LLM)，并比较它们在任务中的表现。有趣的是，我们的结果表明，对于某些任务（例如情绪分析或蕴涵），编辑的影响非常低，通常在 1-5% 左右，而对于理解问答等任务，与原始查询相比，编辑后的查询的性能大幅下降了 25% 以上。对于影响较大的任务，我们会进行更深入的研究，以检查任务关键实体的存在情况。最后，我们研究了性能与已编辑实体数量之间的相关性，并提出了一种通过基于内容的子采样来修复已编辑数据集的策略。更多详细信息请访问此 https URL。</li>
</ul>

<h3>Title: FactLens: Benchmarking Fine-Grained Fact Verification</h3>
<ul>
<li><strong>Authors: </strong>Kushan Mitra, Dan Zhang, Sajjadur Rahman, Estevam Hruschka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05980">https://arxiv.org/abs/2411.05980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05980">https://arxiv.org/pdf/2411.05980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05980]] FactLens: Benchmarking Fine-Grained Fact Verification(https://arxiv.org/abs/2411.05980)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift toward fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在语言生成和理解方面表现出了令人印象深刻的能力，但它们产生幻觉和产生事实错误信息的倾向仍然是一个关键限制。为了验证 LLM 生成的内容和来自其他来源的声明，传统的验证方法通常依赖于整体模型，这些模型为复杂的声明分配单一的事实标签，这可能会掩盖细微的错误。在本文中，我们提倡转向细粒度验证，将复杂的声明分解为较小的子声明进行单独验证，从而可以更精确地识别不准确之处、提高透明度并减少证据检索中的歧义。然而，生成子声明带来了挑战，例如保持上下文并确保与原始声明的语义等价性。我们引入了 FactLens，这是一个评估细粒度事实验证的基准，具有子声明质量的指标和自动评估器。基准数据是手动整理的，以确保高质量的基本事实。我们的结果表明自动 FactLens 评估器与人工判断一致，并且我们讨论了子索赔特征对整体验证性能的影响。</li>
</ul>

<h3>Title: GUIDEQ: Framework for Guided Questioning for progressive informational collection and classification</h3>
<ul>
<li><strong>Authors: </strong>Priya Mishra, Suraj Racha, Kaustubh Ponkshe, Adit Akarsh, Ganesh Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.05991">https://arxiv.org/abs/2411.05991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.05991">https://arxiv.org/pdf/2411.05991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.05991]] GUIDEQ: Framework for Guided Questioning for progressive informational collection and classification(https://arxiv.org/abs/2411.05991)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Question Answering (QA) is an important part of tasks like text classification through information gathering. These are finding increasing use in sectors like healthcare, customer support, legal services, etc., to collect and classify responses into actionable categories. LLMs, although can support QA systems, they face a significant challenge of insufficient or missing information for classification. Although LLMs excel in reasoning, the models rely on their parametric knowledge to answer. However, questioning the user requires domain-specific information aiding to collect accurate information. Our work, GUIDEQ, presents a novel framework for asking guided questions to further progress a partial information. We leverage the explainability derived from the classifier model for along with LLMs for asking guided questions to further enhance the information. This further information helps in more accurate classification of a text. GUIDEQ derives the most significant key-words representative of a label using occlusions. We develop GUIDEQ's prompting strategy for guided questions based on the top-3 classifier label outputs and the significant words, to seek specific and relevant information, and classify in a targeted manner. Through our experimental results, we demonstrate that GUIDEQ outperforms other LLM-based baselines, yielding improved F1-Score through the accurate collection of relevant further information. We perform various analytical studies and also report better question quality compared to our method.</li>
<li><strong>摘要：</strong>问答 (QA) 是通过信息收集进行文本分类等任务的重要组成部分。在医疗保健、客户支持、法律服务等领域，问答的使用越来越广泛，用于收集回复并将其分类为可操作的类别。虽然 LLM 可以支持 QA 系统，但它们面临着分类信息不足或缺失的重大挑战。虽然 LLM 在推理方面表现出色，但模型依靠其参数知识来回答。然而，询问用户需要特定领域的信息来帮助收集准确的信息。我们的工作 GUIDEQ 提出了一个新颖的框架，用于提出引导性问题以进一步推进部分信息。我们利用从分类器模型中获得的可解释性以及 LLM 来提出引导性问题以进一步增强信息。这些进一步的信息有助于更准确地对文本进行分类。GUIDEQ 使用遮挡得出代表标签的最重要关键词。我们根据前 3 个分类器标签输出和重要词开发了 GUIDEQ 的引导性问题提示策略，以寻找具体和相关的信息，并有针对性地进行分类。通过我们的实验结果，我们证明了 GUIDEQ 优于其他基于 LLM 的基线，通过准确收集相关的进一步信息，获得了更高的 F1 分数。我们进行了各种分析研究，并报告了与我们的方法相比更好的问题质量。</li>
</ul>

<h3>Title: The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses</h3>
<ul>
<li><strong>Authors: </strong>Wiktoria Mieleszczenko-Kowszewicz, Dawid Płudowski, Filip Kołodziejczyk, Jakub Świstak, Julian Sienkiewicz, Przemysław Biecek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06008">https://arxiv.org/abs/2411.06008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06008">https://arxiv.org/pdf/2411.06008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06008]] The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses(https://arxiv.org/abs/2411.06008)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study explores how the Large Language Models (LLMs) adjust linguistic features to create personalized persuasive outputs. While research showed that LLMs personalize outputs, a gap remains in understanding the linguistic features of their persuasive capabilities. We identified 13 linguistic features crucial for influencing personalities across different levels of the Big Five model of personality. We analyzed how prompts with personality trait information influenced the output of 19 LLMs across five model families. The findings show that models use more anxiety-related words for neuroticism, increase achievement-related words for conscientiousness, and employ fewer cognitive processes words for openness to experience. Some model families excel at adapting language for openness to experience, others for conscientiousness, while only one model adapts language for neuroticism. Our findings show how LLMs tailor responses based on personality cues in prompts, indicating their potential to create persuasive content affecting the mind and well-being of the recipients.</li>
<li><strong>摘要：</strong>本研究探讨了大型语言模型 (LLM) 如何调整语言特征以创建个性化的说服输出。虽然研究表明 LLM 可以个性化输出，但在理解其说服能力的语言特征方面仍然存在差距。我们确定了 13 个语言特征，这些特征对于影响大五人格模型不同层次的性格至关重要。我们分析了带有性格特征信息的提示如何影响五个模型系列中 19 个 LLM 的输出。研究结果表明，模型使用更多与焦虑相关的词语来表示神经质，增加与成就相关的词语来表示尽责性，使用更少的认知过程词语来表示开放性。一些模型系列擅长调整语言以适应开放性，另一些模型系列擅长调整语言以适应尽责性，而只有一个模型调整语言以适应神经质。我们的研究结果表明 LLM 如何根据提示中的性格线索来定制响应，表明它们有可能创建影响接受者思想和幸福感的说服性内容。</li>
</ul>

<h3>Title: LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output</h3>
<ul>
<li><strong>Authors: </strong>Elise Karinshak, Amanda Hu, Kewen Kong, Vishwanatha Rao, Jingren Wang, Jindong Wang, Yi Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06032">https://arxiv.org/abs/2411.06032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06032">https://arxiv.org/pdf/2411.06032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06032]] LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output(https://arxiv.org/abs/2411.06032)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Immense effort has been dedicated to minimizing the presence of harmful or biased generative content and better aligning AI output to human intention; however, research investigating the cultural values of LLMs is still in very early stages. Cultural values underpin how societies operate, providing profound insights into the norms, priorities, and decision making of their members. In recognition of this need for further research, we draw upon cultural psychology theory and the empirically-validated GLOBE framework to propose the LLM-GLOBE benchmark for evaluating the cultural value systems of LLMs, and we then leverage the benchmark to compare the values of Chinese and US LLMs. Our methodology includes a novel "LLMs-as-a-Jury" pipeline which automates the evaluation of open-ended content to enable large-scale analysis at a conceptual level. Results clarify similarities and differences that exist between Eastern and Western cultural value systems and suggest that open-generation tasks represent a more promising direction for evaluation of cultural values. We interpret the implications of this research for subsequent model development, evaluation, and deployment efforts as they relate to LLMs, AI cultural alignment more broadly, and the influence of AI cultural value systems on human-AI collaboration outcomes.</li>
<li><strong>摘要：</strong>人们付出了巨大的努力来尽量减少有害或有偏见的生成内容，并更好地使人工智能输出与人类意图保持一致；然而，对法学硕士文化价值观的研究仍处于非常早期的阶段。文化价值观支撑着社会的运作方式，为其成员的规范、优先事项和决策提供了深刻的见解。认识到需要进一步研究，我们借鉴文化心理学理论和经实证验证的 GLOBE 框架，提出了用于评估法学硕士文化价值体系的 LLM-GLOBE 基准，然后利用该基准比较中国和美国法学硕士的价值观。我们的方法包括一个新颖的“法学硕士陪审团”流程，它可以自动评估开放式内容，从而实现概念层面的大规模分析。结果阐明了东西方文化价值体系之间的相似之处和差异，并表明开放生成任务代表了评估文化价值观的更有希望的方向。我们解释了这项研究对后续模型开发、评估和部署工作的影响，因为它们与 LLM、更广泛的人工智能文化协调以及人工智能文化价值体系对人机协作结果的影响有关。</li>
</ul>

<h3>Title: Sufficient Context: A New Lens on Retrieval Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, Cyrus Rashtchian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06037">https://arxiv.org/abs/2411.06037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06037">https://arxiv.org/pdf/2411.06037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06037]] Sufficient Context: A New Lens on Retrieval Augmented Generation Systems(https://arxiv.org/abs/2411.06037)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, hallucination, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a way to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that proprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, open-source LLMs (Llama, Mistral, Gemma) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2-10% for Gemini, GPT, and Gemma.</li>
<li><strong>摘要：</strong>使用上下文增强 LLM 可提高许多应用程序的性能。尽管对检索增强生成 (RAG) 系统进行了大量研究，但一个悬而未决的问题是，错误是否是因为 LLM 未能利用检索中的上下文或上下文本身不足以回答查询而产生的。为了阐明这一点，我们开发了一种新的充分上下文概念，以及一种对具有足够信息来回答查询的实例进行分类的方法。然后，我们使用充分上下文来分析多个模型和数据集。通过根据上下文充分性对错误进行分层，我们发现专有 LLM（Gemini、GPT、Claude）在上下文充足时擅长回答查询，但在上下文不足时，通常会输出错误答案而不是弃权。另一方面，开源 LLM（Llama、Mistral、Gemma）即使有足够的上下文，也经常产生幻觉或弃权。我们进一步对上下文有用的情况进行分类，并提高准确性，即使它不能完全回答查询，并且模型在没有上下文的情况下会出错。基于我们的发现，我们探索了减少 RAG 系统中幻觉的方法，包括一种新的选择性生成方法，该方法利用足够的上下文信息进行引导弃权。我们的方法将 Gemini、GPT 和 Gemma 模型响应时正确答案的比例提高了 2-10%。</li>
</ul>

<h3>Title: Zyda-2: a 5 Trillion Token High-Quality Dataset</h3>
<ul>
<li><strong>Authors: </strong>Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06068">https://arxiv.org/abs/2411.06068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06068">https://arxiv.org/pdf/2411.06068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06068]] Zyda-2: a 5 Trillion Token High-Quality Dataset(https://arxiv.org/abs/2411.06068)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this technical report, we present Zyda-2: a five trillion token dataset for language model pretraining. Zyda-2 was used to train our Zamba2 series of models which are state-of-the-art for their weight class. We build Zyda-2 by collating high-quality open-source tokens such as FineWeb and DCLM, then distilling them to the highest-quality subset via cross-deduplication and model-based quality filtering. Zyda-2 is released under a permissive open license, and is available at this https URL</li>
<li><strong>摘要：</strong>在本技术报告中，我们介绍了 Zyda-2：一个用于语言模型预训练的五万亿个 token 数据集。Zyda-2 用于训练我们的 Zamba2 系列模型，这些模型是其权重类别中最先进的。我们通过整理 FineWeb 和 DCLM 等高质量开源 token，然后通过交叉重复数据删除和基于模型的质量过滤将它们提炼为最高质量的子集来构建 Zyda-2。Zyda-2 是根据宽松的开放许可发布的，可在此 https URL 上获取</li>
</ul>

<h3>Title: ZhoBLiMP: a Systematic Assessment of Language Models with Linguistic Minimal Pairs in Chinese</h3>
<ul>
<li><strong>Authors: </strong>Yikang Liu, Yeting Shen, Hongao Zhu, Lilong Xu, Zhiheng Qian, Siyuan Song, Kejia Zhang, Jialong Tang, Pei Zhang, Baosong Yang, Rui Wang, Hai Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06096">https://arxiv.org/abs/2411.06096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06096">https://arxiv.org/pdf/2411.06096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06096]] ZhoBLiMP: a Systematic Assessment of Language Models with Linguistic Minimal Pairs in Chinese(https://arxiv.org/abs/2411.06096)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Whether and how language models (LMs) acquire the syntax of natural languages has been widely evaluated under the minimal pair paradigm. However, a lack of wide-coverage benchmarks in languages other than English has constrained systematic investigations into the issue. Addressing it, we first introduce ZhoBLiMP, the most comprehensive benchmark of linguistic minimal pairs for Chinese to date, with 118 paradigms, covering 15 linguistic phenomena. We then train 20 LMs of different sizes (14M to 1.4B) on Chinese corpora of various volumes (100M to 3B tokens) and evaluate them along with 14 off-the-shelf LLMs on ZhoBLiMP. The overall results indicate that Chinese grammar can be mostly learned by models with around 500M parameters, trained on 1B tokens with one epoch, showing limited benefits for further scaling. Most (N=95) linguistic paradigms are of easy or medium difficulty for LMs, while there are still 13 paradigms that remain challenging even for models with up to 32B parameters. In regard to how LMs acquire Chinese grammar, we observe a U-shaped learning pattern in several phenomena, similar to those observed in child language acquisition.</li>
<li><strong>摘要：</strong>在最小对范式下，语言模型 (LM) 是否以及如何获得自然语言的语法已得到广泛评估。然而，由于缺乏除英语以外的其他语言的广泛覆盖基准，因此限制了对该问题的系统研究。为了解决这个问题，我们首先引入了 ZhoBLiMP，这是迄今为止最全面的中文语言最小对基准，包含 118 个范式，涵盖 15 种语言现象。然后，我们在不同规模（1 亿到 30 亿个 token）的中文语料库上训练 20 个不同规模（1400 万到 14 亿）的 LM，并在 ZhoBLiMP 上对它们和 14 个现成的 LLM 进行评估。总体结果表明，中文语法主要可以通过具有约 5 亿个参数的模型来学习，这些模型在一个 epoch 内对 10 亿个 token 进行训练，进一步扩展的优势有限。大多数（N=95）语言范式对于语言模型来说都属于简单或中等难度，但仍有 13 个范式即使对于具有多达 32B 个参数的模型来说也仍然具有挑战性。关于语言模型如何习得中文语法，我们在几个现象中观察到 U 型学习模式，类似于儿童语言习得中观察到的模式。</li>
</ul>

<h3>Title: Detecting Reference Errors in Scientific Literature with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianmai M. Zhang, Neil F. Abernethy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06101">https://arxiv.org/abs/2411.06101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06101">https://arxiv.org/pdf/2411.06101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06101]] Detecting Reference Errors in Scientific Literature with Large Language Models(https://arxiv.org/abs/2411.06101)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Reference errors, such as citation and quotation errors, are common in scientific papers. Such errors can result in the propagation of inaccurate information, but are difficult and time-consuming to detect, posing a significant challenge to scientific publishing. To support automatic detection of reference errors, this work evaluated the ability of large language models in OpenAI's GPT family to detect quotation errors. Specifically, we prepared an expert-annotated, general-domain dataset of statement-reference pairs from journal articles. Large language models were evaluated in different settings with varying amounts of reference information provided by retrieval augmentation. Our results showed that large language models are able to detect erroneous citations with limited context and without fine-tuning. This study contributes to the growing literature that seeks to utilize artificial intelligence to assist in the writing, reviewing, and publishing of scientific papers. Potential avenues for further improvements in this task are also discussed.</li>
<li><strong>摘要：</strong>参考文献错误（例如引用和引文错误）在科学论文中很常见。此类错误可能导致不准确信息的传播，但很难检测且耗时，对科学出版构成了重大挑战。为了支持自动检测参考文献错误，这项工作评估了 OpenAI 的 GPT 系列中大型语言模型检测引文错误的能力。具体来说，我们准备了一个专家注释的通用领域数据集，其中包含期刊文章中的语句-参考对。大型语言模型在不同环境中进行了评估，检索增强提供了不同数量的参考信息。我们的结果表明，大型语言模型能够在有限的上下文中检测到错误的引用，而无需进行微调。这项研究为越来越多的文献做出了贡献，这些文献试图利用人工智能来协助撰写、审阅和发表科学论文。我们还讨论了进一步改进这项任务的潜在途径。</li>
</ul>

<h3>Title: Building an Efficient Multilingual Non-Profit IR System for the Islamic Domain Leveraging Multiprocessing Design in Rust</h3>
<ul>
<li><strong>Authors: </strong>Vera Pavlova, Mohammed Makhlouf</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06151">https://arxiv.org/abs/2411.06151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06151">https://arxiv.org/pdf/2411.06151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06151]] Building an Efficient Multilingual Non-Profit IR System for the Islamic Domain Leveraging Multiprocessing Design in Rust(https://arxiv.org/abs/2411.06151)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The widespread use of large language models (LLMs) has dramatically improved many applications of Natural Language Processing (NLP), including Information Retrieval (IR). However, domains that are not driven by commercial interest often lag behind in benefiting from AI-powered solutions. One such area is religious and heritage corpora. Alongside similar domains, Islamic literature holds significant cultural value and is regularly utilized by scholars and the general public. Navigating this extensive amount of text is challenging, and there is currently no unified resource that allows for easy searching of this data using advanced AI tools. This work focuses on the development of a multilingual non-profit IR system for the Islamic domain. This process brings a few major challenges, such as preparing multilingual domain-specific corpora when data is limited in certain languages, deploying a model on resource-constrained devices, and enabling fast search on a limited budget. By employing methods like continued pre-training for domain adaptation and language reduction to decrease model size, a lightweight multilingual retrieval model was prepared, demonstrating superior performance compared to larger models pre-trained on general domain data. Furthermore, evaluating the proposed architecture that utilizes Rust Language capabilities shows the possibility of implementing efficient semantic search in a low-resource setting.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的广泛使用极大地改善了自然语言处理 (NLP) 的许多应用，包括信息检索 (IR)。然而，不受商业利益驱动的领域往往在从人工智能解决方案中受益方面落后。宗教和遗产语料库就是其中之一。与类似领域一样，伊斯兰文学具有重要的文化价值，经常被学者和普通民众使用。浏览如此大量的文本具有挑战性，目前没有统一的资源可以使用高级人工智能工具轻松搜索这些数据。这项工作的重点是为伊斯兰领域开发一个多语言非营利性 IR 系统。这个过程带来了一些重大挑战，例如当某些语言的数据有限时准备多语言领域特定语料库、在资源受限的设备上部署模型以及在有限的预算下实现快速搜索。通过采用持续预训练以适应领域和语言减少以减小模型大小等方法，准备了一个轻量级的多语言检索模型，与在一般领域数据上预训练的大型模型相比，该模型表现出了卓越的性能。此外，评估利用 Rust 语言功能的所提出的架构表明在低资源环境中实现有效的语义搜索的可能性。</li>
</ul>

<h3>Title: From References to Insights: Collaborative Knowledge Minigraph Agents for Automating Scholarly Literature Review</h3>
<ul>
<li><strong>Authors: </strong>Zhi Zhang, Yan Liu, Sheng-hua Zhong, Gong Chen, Yu Yang, Jiannong Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06159">https://arxiv.org/abs/2411.06159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06159">https://arxiv.org/pdf/2411.06159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06159]] From References to Insights: Collaborative Knowledge Minigraph Agents for Automating Scholarly Literature Review(https://arxiv.org/abs/2411.06159)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Literature reviews play a crucial role in scientific research for understanding the current state of research, identifying gaps, and guiding future studies on specific topics. However, the process of conducting a comprehensive literature review is yet time-consuming. This paper proposes a novel framework, collaborative knowledge minigraph agents (CKMAs), to automate scholarly literature reviews. A novel prompt-based algorithm, the knowledge minigraph construction agent (KMCA), is designed to identify relationships between information pieces from academic literature and automatically constructs knowledge minigraphs. By leveraging the capabilities of large language models on constructed knowledge minigraphs, the multiple path summarization agent (MPSA) efficiently organizes information pieces and relationships from different viewpoints to generate literature review paragraphs. We evaluate CKMAs on three benchmark datasets. Experimental results demonstrate that the proposed techniques generate informative, complete, consistent, and insightful summaries for different research problems, promoting the use of LLMs in more professional fields.</li>
<li><strong>摘要：</strong>文献综述在科学研究中起着至关重要的作用，它有助于了解当前的研究状况、发现差距并指导特定主题的未来研究。然而，进行全面的文献综述的过程却非常耗时。本文提出了一个新颖的框架——协作知识微图代理 (CKMA)，以自动化学术文献综述。知识微图构建代理 (KMCA) 是一种基于提示的新型算法，旨在识别学术文献中信息片段之间的关系并自动构建知识微图。通过利用构建的知识微图上的大型语言模型的功能，多路径摘要代理 (MPSA) 可以有效地组织来自不同视角的信息片段和关系以生成文献综述段落。我们在三个基准数据集上评估了 CKMA。实验结果表明，所提出的技术可以为不同的研究问题生成信息丰富、完整、一致且富有洞察力的摘要，从而促进 LLM 在更专业领域的使用。</li>
</ul>

<h3>Title: SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinghan He, Haiyun Guo, Kuan Zhu, Zihan Zhao, Ming Tang, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06171">https://arxiv.org/abs/2411.06171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06171">https://arxiv.org/pdf/2411.06171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06171]] SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models(https://arxiv.org/abs/2411.06171)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Continual learning (CL) is crucial for language models to dynamically adapt to the evolving real-world demands. To mitigate the catastrophic forgetting problem in CL, data replay has been proven a simple and effective strategy, and the subsequent data-replay-based distillation can further enhance the performance. However, existing methods fail to fully exploit the knowledge embedded in models from previous tasks, resulting in the need for a relatively large number of replay samples to achieve good results. In this work, we first explore and emphasize the importance of attention weights in knowledge retention, and then propose a SElective attEntion-guided Knowledge Retention method (SEEKR) for data-efficient replay-based continual learning of large language models (LLMs). Specifically, SEEKR performs attention distillation on the selected attention heads for finer-grained knowledge retention, where the proposed forgettability-based and task-sensitivity-based measures are used to identify the most valuable attention heads. Experimental results on two continual learning benchmarks for LLMs demonstrate the superiority of SEEKR over the existing methods on both performance and efficiency. Explicitly, SEEKR achieves comparable or even better performance with only 1/10 of the replayed data used by other methods, and reduces the proportion of replayed data to 1%.</li>
<li><strong>摘要：</strong>持续学习 (CL) 对于语言模型动态适应不断变化的现实需求至关重要。为了缓解 CL 中的灾难性遗忘问题，数据重放已被证明是一种简单有效的策略，随后基于数据重放的蒸馏可以进一步提高性能。然而，现有的方法未能充分利用模型中嵌入的先前任务的知识，因此需要相对大量的重放样本才能获得良好的结果。在本文中，我们首先探索并强调注意力权重在知识保留中的重要性，然后提出一种选择性注意力引导知识保留方法 (SEEKR)，用于数据高效的基于重放的大型语言模型 (LLM) 持续学习。具体来说，SEEKR 对选定的注意力头进行注意力蒸馏，以实现更细粒度的知识保留，其中提出的基于可遗忘性和基于任务敏感性的度量用于识别最有价值的注意力头。在两个 LLM 持续学习基准上的实验结果表明，SEEKR 在性能和效率上均优于现有方法。具体而言，SEEKR 仅使用其他方法 1/10 的重放数据就获得了相当甚至更好的性能，并将重放数据的比例降低到 1%。</li>
</ul>

<h3>Title: Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shan Zhong, Jiahao Zeng, Yongxin Yu, Bohong Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06175">https://arxiv.org/abs/2411.06175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06175">https://arxiv.org/pdf/2411.06175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06175]] Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs(https://arxiv.org/abs/2411.06175)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative semi-supervised learning approach for text classification, addressing the challenge of abundant data but limited labeled examples. Our methodology integrates few-shot learning with retrieval-augmented generation (RAG) and conventional statistical clustering, enabling effective learning from a minimal number of labeled instances while generating high-quality labeled data. To the best of our knowledge, we are the first to incorporate RAG alongside clustering in text data generation. Our experiments on the Reuters and Web of Science datasets demonstrate state-of-the-art performance, with few-shot augmented data alone producing results nearly equivalent to those achieved with fully labeled datasets. Notably, accuracies of 95.41\% and 82.43\% were achieved for complex text document classification tasks, where the number of categories can exceed 100.</li>
<li><strong>摘要：</strong>本文介绍了一种创新的半监督学习文本分类方法，解决了数据丰富但标记示例有限的挑战。我们的方法将小样本学习与检索增强生成 (RAG) 和传统统计聚类相结合，能够从最少数量的标记实例中进行有效学习，同时生成高质量的标记数据。据我们所知，我们是第一个将 RAG 与聚类结合到文本数据生成的。我们在路透社和 Web of Science 数据集上进行的实验展示了最先进的性能，仅使用小样本增强数据就能产生几乎与使用完全标记数据集所获得的结果相当的结果。值得注意的是，对于复杂的文本文档分类任务，其中类别数量可能超过 100，准确率分别达到 95.41% 和 82.43%。</li>
</ul>

<h3>Title: Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment</h3>
<ul>
<li><strong>Authors: </strong>Zhen Zhang, Xinyu Wang, Yong Jiang, Zhuo Chen, Feiteng Mu, Mengting Hu, Pengjun Xie, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06207">https://arxiv.org/abs/2411.06207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06207">https://arxiv.org/pdf/2411.06207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06207]] Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment(https://arxiv.org/abs/2411.06207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly recognized for their practical applications. However, these models often encounter challenges in dynamically changing knowledge, as well as in managing unknown static knowledge. Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a significant impact on LLMs. Actually, we find that the impact of RAG on the question answering capabilities of LLMs can be categorized into three groups: beneficial, neutral, and harmful. By minimizing retrieval requests that yield neutral or harmful results, we can effectively reduce both time and computational costs, while also improving the overall performance of LLMs. This insight motivates us to differentiate between types of questions using certain metrics as indicators, to decrease the retrieval ratio without compromising performance. In our work, we propose a method that is able to identify different types of questions from this view by training a Knowledge Boundary Model (KBM). Experiments conducted on 11 English and Chinese datasets illustrate that the KBM effectively delineates the knowledge boundary, significantly decreasing the proportion of retrievals required for optimal end-to-end performance. Specifically, we evaluate the effectiveness of KBM in three complex scenarios: dynamic knowledge, long-tail static knowledge, and multi-hop problems, as well as its functionality as an external LLM plug-in.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其实际应用而受到越来越多的认可。然而，这些模型在动态变化的知识以及管理未知的静态知识方面经常遇到挑战。检索增强生成 (RAG) 解决了这一挑战，并对 LLM 产生了重大影响。实际上，我们发现 RAG 对 LLM 问答能力的影响可以分为三类：有益、中性和有害。通过最小化产生中性或有害结果的检索请求，我们可以有效地减少时间和计算成本，同时还可以提高 LLM 的整体性能。这种见解促使我们使用某些指标作为指标来区分问题类型，以在不影响性能的情况下降低检索率。在我们的工作中，我们提出了一种方法，通过训练知识边界模型 (KBM)，可以从这个角度识别不同类型的问题。在11个英文和中文数据集上进行的实验表明，KBM有效地勾勒出知识边界，显著降低了实现最佳端到端性能所需的检索比例。具体来说，我们在动态知识、长尾静态知识和多跳问题三种复杂场景中评估了KBM的有效性，以及其作为外部LLM插件的功能。</li>
</ul>

<h3>Title: IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06208">https://arxiv.org/abs/2411.06208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06208">https://arxiv.org/pdf/2411.06208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06208]] IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization(https://arxiv.org/abs/2411.06208)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 领域，随着越来越多的代理和应用程序利用 LLM 进行构建，模型准确遵循指令的能力至关重要，而指令的复杂性正在迅速增加。然而，一方面，复杂指令评估数据只有一定数量；另一方面，没有专门的算法来提高遵循复杂指令的能力。为此，本文引入了 TRACE，一个用于改进和评估复杂指令遵循能力的基准，它由 120K 训练数据和 1K 评估数据组成。此外，我们提出了 IOPO（输入输出偏好优化）对齐方法，该方法同时考虑输入和输出偏好对，其中 LLM 不仅可以快速与响应偏好对齐，还可以细致地探索指令偏好。在域内和域外数据集上进行的大量实验证实了IOPO的有效性，与SFT和DPO相比，域内数据上的性能分别提高了8.15%、2.18%，域外数据上的性能分别提高了6.29%、3.13%。</li>
</ul>

<h3>Title: Incorporating Human Explanations for Robust Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Jennifer L. Chen, Faisal Ladhak, Daniel Li, Noémie Elhadad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06213">https://arxiv.org/abs/2411.06213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06213">https://arxiv.org/pdf/2411.06213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06213]] Incorporating Human Explanations for Robust Hate Speech Detection(https://arxiv.org/abs/2411.06213)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Given the black-box nature and complexity of large transformer language models (LM), concerns about generalizability and robustness present ethical implications for domains such as hate speech (HS) detection. Using the content rich Social Bias Frames dataset, containing human-annotated stereotypes, intent, and targeted groups, we develop a three stage analysis to evaluate if LMs faithfully assess hate speech. First, we observe the need for modeling contextually grounded stereotype intents to capture implicit semantic meaning. Next, we design a new task, Stereotype Intent Entailment (SIE), which encourages a model to contextually understand stereotype presence. Finally, through ablation tests and user studies, we find a SIE objective improves content understanding, but challenges remain in modeling implicit intent.</li>
<li><strong>摘要：</strong>鉴于大型 Transformer 语言模型 (LM) 的黑箱性质和复杂性，对通用性和稳健性的担忧对仇恨言论 (HS) 检测等领域带来了伦理影响。使用内容丰富的社交偏见框架数据集（包含人工注释的刻板印象、意图和目标群体），我们开发了一个三阶段分析来评估 LM 是否忠实地评估仇恨言论。首先，我们观察到需要对基于上下文的刻板印象意图进行建模以捕捉隐含的语义含义。接下来，我们设计了一个新任务，即刻板印象意图蕴涵 (SIE)，它鼓励模型根据上下文理解刻板印象的存在。最后，通过消融测试和用户研究，我们发现 SIE 目标可以提高内容理解，但在建模隐含意图方面仍然存在挑战。</li>
</ul>

<h3>Title: Robust Detection of LLM-Generated Text: A Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yongye Su, Yuqing Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06248">https://arxiv.org/abs/2411.06248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06248">https://arxiv.org/pdf/2411.06248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06248]] Robust Detection of LLM-Generated Text: A Comparative Analysis(https://arxiv.org/abs/2411.06248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The ability of large language models to generate complex texts allows them to be widely integrated into many aspects of life, and their output can quickly fill all network resources. As the impact of LLMs grows, it becomes increasingly important to develop powerful detectors for the generated text. This detector is essential to prevent the potential misuse of these technologies and to protect areas such as social media from the negative effects of false content generated by LLMS. The main goal of LLM-generated text detection is to determine whether text is generated by an LLM, which is a basic binary classification task. In our work, we mainly use three different classification methods based on open source datasets: traditional machine learning techniques such as logistic regression, k-means clustering, Gaussian Naive Bayes, support vector machines, and methods based on converters such as BERT, and finally algorithms that use LLMs to detect LLM-generated text. We focus on model generalization, potential adversarial attacks, and accuracy of model evaluation. Finally, the possible research direction in the future is proposed, and the current experimental results are summarized.</li>
<li><strong>摘要：</strong>大型语言模型生成复杂文本的能力使得它们被广泛地融入到生活的方方面面，其输出可以迅速填满所有的网络资源。随着LLMs的影响越来越大，开发强大的生成文本检测器变得越来越重要。这种检测器对于防止这些技术的潜在滥用以及保护社交媒体等领域免受LLMS生成的虚假内容的负面影响至关重要。LLM生成文本检测的主要目标是确定文本是否由LLM生成，这是一个基本的二分类任务。在我们的工作中，我们主要使用三种基于开源数据集的不同分类方法：传统的机器学习技术，如逻辑回归、k均值聚类、高斯朴素贝叶斯、支持向量机，以及基于转换器的方法，如BERT，最后是使用LLM检测LLM生成文本的算法。我们关注模型泛化、潜在的对抗性攻击和模型评估的准确性。最后提出了未来可能的研究方向，并总结了目前的实验结果。</li>
</ul>

<h3>Title: Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaojun Wu, Junxi Liu, Huanyi Su, Zhouchi Lin, Yiyan Qi, Chengjin Xu, Jiajun Su, Jiajie Zhong, Fuwei Wang, Saizhuo Wang, Fengrui Hua, Jia Li, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06272">https://arxiv.org/abs/2411.06272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06272">https://arxiv.org/pdf/2411.06272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06272]] Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models(https://arxiv.org/abs/2411.06272)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose "Golden Touchstone", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific this http URL research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at \url{this https URL}, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.</li>
<li><strong>摘要：</strong>随着大型语言模型在金融领域的应用越来越广泛，迫切需要一种标准化的方法来全面评估它们的性能。然而，现有的金融基准往往存在语言和任务覆盖范围有限、数据集质量低下、LLM 评估适应性不足等挑战。为了解决这些限制，我们提出了“Golden Touchstone”，这是第一个全面的金融 LLM 双语基准，它结合了八个核心金融 NLP 任务中来自中文和英文的代表性数据集。该基准基于广泛的开源数据收集和行业特定需求而开发，包括各种金融任务，旨在全面评估模型的语言理解和生成能力。通过对基准上的主要模型（如 GPT-4o Llama3、FinGPT 和 FinMA）的比较分析，我们揭示了它们在处理复杂金融信息方面的优势和局限性。此外，我们开源了 Touchstone-GPT，这是一个通过持续预训练和金融指令调整训练的金融 LLM，它在双语基准上表现出色，但在特定研究方面仍然存在局限性，这不仅为金融大型语言模型提供了实用的评估工具，还指导了未来研究的开发和优化。Golden Touchstone 的源代码和 Touchstone-GPT 的模型权重已在 \url{this https URL} 上公开，为 FinLLM 的持续发展做出了贡献，并促进了这一关键领域的进一步研究。</li>
</ul>

<h3>Title: Prompts Matter: Comparing ML/GAI Approaches for Generating Inductive Qualitative Coding Results</h3>
<ul>
<li><strong>Authors: </strong>John Chen, Alexandros Lotsos, Lexie Zhao, Grace Wang, Uri Wilensky, Bruce Sherin, Michael Horn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06316">https://arxiv.org/abs/2411.06316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06316">https://arxiv.org/pdf/2411.06316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06316]] Prompts Matter: Comparing ML/GAI Approaches for Generating Inductive Qualitative Coding Results(https://arxiv.org/abs/2411.06316)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Inductive qualitative methods have been a mainstay of education research for decades, yet it takes much time and effort to conduct rigorously. Recent advances in artificial intelligence, particularly with generative AI (GAI), have led to initial success in generating inductive coding results. Like human coders, GAI tools rely on instructions to work, and how to instruct it may matter. To understand how ML/GAI approaches could contribute to qualitative coding processes, this study applied two known and two theory-informed novel approaches to an online community dataset and evaluated the resulting coding results. Our findings show significant discrepancies between ML/GAI approaches and demonstrate the advantage of our approaches, which introduce human coding processes into GAI prompts.</li>
<li><strong>摘要：</strong>归纳定性方法几十年来一直是教育研究的支柱，但要严格实施需要花费大量时间和精力。人工智能的最新进展，尤其是生成式人工智能 (GAI)，已在生成归纳编码结果方面取得了初步成功。与人类编码员一样，GAI 工具依靠指令来工作，而如何指导它可能很重要。为了了解 ML/GAI 方法如何促进定性编码过程，本研究将两种已知方法和两种理论指导的新方法应用于在线社区数据集，并评估了产生的编码结果。我们的研究结果显示 ML/GAI 方法之间存在显著差异，并证明了我们的方法的优势，即将人类编码过程引入 GAI 提示中。</li>
</ul>

<h3>Title: LLM Vocabulary Compression for Low-Compute Environments</h3>
<ul>
<li><strong>Authors: </strong>Sreeram Vennam, Anish Joishy, Ponnurangam Kumaraguru</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06371">https://arxiv.org/abs/2411.06371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06371">https://arxiv.org/pdf/2411.06371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06371]] LLM Vocabulary Compression for Low-Compute Environments(https://arxiv.org/abs/2411.06371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We present a method to compress the final linear layer of language models, reducing memory usage by up to 3.4x without significant performance loss. By grouping tokens based on Byte Pair Encoding (BPE) merges, we prevent materialization of the memory-intensive logits tensor. Evaluations on the TinyStories dataset show that our method performs on par with GPT-Neo and GPT2 while significantly improving throughput by up to 3x, making it suitable for low-compute environments.</li>
<li><strong>摘要：</strong>我们提出了一种压缩语言模型最终线性层的方法，将内存使用量减少高达 3.4 倍，同时不会造成显著的性能损失。通过基于字节对编码 (BPE) 合并对标记进行分组，我们可以防止内存密集型 logits 张量的实现。在 TinyStories 数据集上的评估表明，我们的方法性能与 GPT-Neo 和 GPT2 相当，同时显著提高了高达 3 倍的吞吐量，使其适用于低计算环境。</li>
</ul>

<h3>Title: Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sultan Alrashed, Dmitrii Khizbullin, David R. Pugh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06402">https://arxiv.org/abs/2411.06402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06402">https://arxiv.org/pdf/2411.06402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06402]] Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language Models(https://arxiv.org/abs/2411.06402)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) grow and develop, so do their data demands. This is especially true for multilingual LLMs, where the scarcity of high-quality and readily available data online has led to a multitude of synthetic dataset generation approaches. A key technique in this space is machine translation (MT), where high-quality English text is adapted to a target, comparatively low-resource language. This report introduces FineWeb-Edu-Ar, a machine-translated version of the exceedingly popular (deduplicated) FineWeb-Edu dataset from HuggingFace. To the best of our knowledge, FineWeb-Edu-Ar is the largest publicly available machine-translated Arabic dataset out there, with its size of 202B tokens of an Arabic-trained tokenizer.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的增长和发展，它们对数据的需求也在增长。对于多语言 LLM 来说尤其如此，由于在线高质量且随时可用的数据稀缺，因此出现了大量合成数据集生成方法。该领域的一项关键技术是机器翻译 (MT)，即将高质量的英语文本改编为资源相对较少的目标语言。本报告介绍了 FineWeb-Edu-Ar，这是 HuggingFace 非常流行的（去重）FineWeb-Edu 数据集的机器翻译版本。据我们所知，FineWeb-Edu-Ar 是目前最大的公开可用的机器翻译阿拉伯语数据集，其大小为 202B 个阿拉伯语训练标记器标记。</li>
</ul>

<h3>Title: PLM-Based Discrete Diffusion Language Models with Entropy-Adaptive Gibbs Sampling</h3>
<ul>
<li><strong>Authors: </strong>Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06438">https://arxiv.org/abs/2411.06438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06438">https://arxiv.org/pdf/2411.06438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06438]] PLM-Based Discrete Diffusion Language Models with Entropy-Adaptive Gibbs Sampling(https://arxiv.org/abs/2411.06438)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, discrete diffusion language models have demonstrated promising results in NLP. However, there has been limited research on integrating Pretrained Language Models (PLMs) into discrete diffusion models, resulting in underwhelming performance in downstream NLP generation tasks. This integration is particularly challenging because of the discrepancy between step-wise denoising strategy of diffusion models and single-step mask prediction approach of MLM-based PLMs. In this paper, we introduce Diffusion-EAGS, a novel approach that effectively integrates PLMs with the diffusion models. Furthermore, as it is challenging for PLMs to determine where to apply denoising during the diffusion process, we integrate an entropy tracking module to assist them. Finally, we propose entropy-based noise scheduling in the forward process to improve the effectiveness of entropy-adaptive sampling throughout the generation phase. Experimental results show that Diffusion-EAGS outperforms existing diffusion baselines in downstream generation tasks, achieving high text quality and diversity with precise token-level control. We also show that our model is capable of adapting to bilingual and low-resource settings, which are common in real-world applications.</li>
<li><strong>摘要：</strong>最近，离散扩散语言模型在 NLP 中表现出了良好的效果。然而，将预训练语言模型 (PLM) 集成到离散扩散模型中的研究有限，导致下游 NLP 生成任务中的表现不佳。由于扩散模型的逐步去噪策略与基于 MLM 的 PLM 的单步掩码预测方法之间存在差异，这种集成尤其具有挑战性。在本文中，我们介绍了 Diffusion-EAGS，这是一种有效将 PLM 与扩散模型集成的新方法。此外，由于 PLM 很难确定在扩散过程中在哪里应用去噪，我们集成了一个熵跟踪模块来协助它们。最后，我们提出了基于熵的正向噪声调度，以提高整个生成阶段熵自适应采样的有效性。实验结果表明，Diffusion-EAGS 在下游生成任务中的表现优于现有的扩散基线，通过精确的 token 级控制实现了高文本质量和多样性。我们还表明，我们的模型能够适应实际应用中常见的双语和低资源设置。</li>
</ul>

<h3>Title: Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce Hallucination and to Improve Reproducibility in Scientific Text Generation Using Stochastic Optimisation Techniques</h3>
<ul>
<li><strong>Authors: </strong>Daniil Sulimov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06445">https://arxiv.org/abs/2411.06445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06445">https://arxiv.org/pdf/2411.06445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06445]] Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce Hallucination and to Improve Reproducibility in Scientific Text Generation Using Stochastic Optimisation Techniques(https://arxiv.org/abs/2411.06445)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly adopted for complex scientific text generation tasks, yet they often suffer from limitations in accuracy, consistency, and hallucination control. This thesis introduces a Parameter-Efficient Fine-Tuning (PEFT) approach tailored for GPT-like models, aiming to mitigate hallucinations and enhance reproducibility, particularly in the computational domain of mass spectrometry. We implemented Low-Rank Adaptation (LoRA) adapters to refine GPT-2, termed MS-GPT, using a specialized corpus of mass spectrometry literature. Through novel evaluation methods applied to LLMs, including BLEU, ROUGE, and Perplexity scores, the fine-tuned MS-GPT model demonstrated superior text coherence and reproducibility compared to the baseline GPT-2, confirmed through statistical analysis with the Wilcoxon rank-sum test. Further, we propose a reproducibility metric based on cosine similarity of model outputs under controlled prompts, showcasing MS-GPT's enhanced stability. This research highlights PEFT's potential to optimize LLMs for scientific contexts, reducing computational costs while improving model reliability.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于复杂的科学文本生成任务，但它们通常在准确性、一致性和幻觉控制方面受到限制。本论文介绍了一种针对 GPT 类模型量身定制的参数高效微调 (PEFT) 方法，旨在减轻幻觉并提高可重复性，特别是在质谱计算领域。我们使用专门的质谱文献语料库实现了低秩自适应 (LoRA) 适配器来改进 GPT-2，称为 MS-GPT。通过应用于 LLM 的新型评估方法，包括 BLEU、ROUGE 和困惑度分数，经过微调的 MS-GPT 模型与基线 GPT-2 相比表现出更好的文本连贯性和可重复性，这通过 Wilcoxon 秩和检验的统计分析得到证实。此外，我们提出了一种基于受控提示下模型输出余弦相似度的可重复性指标，展示了 MS-GPT 增强的稳定性。这项研究强调了 PEFT 在科学背景下优化 LLM 的潜力，降低计算成本，同时提高模型可靠性。</li>
</ul>

<h3>Title: ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?</h3>
<ul>
<li><strong>Authors: </strong>Canyu Chen, Jian Yu, Shan Chen, Che Liu, Zhongwei Wan, Danielle Bitterman, Fei Wang, Kai Shu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06469">https://arxiv.org/abs/2411.06469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06469">https://arxiv.org/pdf/2411.06469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06469]] ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?(https://arxiv.org/abs/2411.06469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) hold great promise to revolutionize current clinical systems for their superior capacities on medical text processing tasks and medical licensing exams. Meanwhile, traditional ML models such as SVM and XGBoost have still been mainly adopted in clinical prediction tasks. An emerging question is Can LLMs beat traditional ML models in clinical prediction? Thus, we build a new benchmark ClinicalBench to comprehensively study the clinical predictive modeling capacities of both general-purpose and medical LLMs, and compare them with traditional ML models. ClinicalBench embraces three common clinical prediction tasks, two databases, 14 general-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through extensive empirical investigation, we discover that both general-purpose and medical LLMs, even with different model scales, diverse prompting or fine-tuning strategies, still cannot beat traditional ML models in clinical prediction yet, shedding light on their potential deficiency in clinical reasoning and decision-making. We call for caution when practitioners adopt LLMs in clinical applications. ClinicalBench can be utilized to bridge the gap between LLMs' development for healthcare and real-world clinical practice.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其在医学文本处理任务和行医执照考试中的出色能力而有望彻底改变当前的临床系统。同时，传统的 ML 模型（例如 SVM 和 XGBoost）仍然主要应用于临床预测任务。一个新出现的问题是 LLM 能否在临床预测中击败传统 ML 模型？因此，我们建立了一个新的基准 ClinicalBench，以全面研究通用和医学 LLM 的临床预测建模能力，并将其与传统 ML 模型进行比较。ClinicalBench 包含三个常见的临床预测任务、两个数据库、14 个通用 LLM、8 个医学 LLM 和 11 个传统 ML 模型。通过广泛的实证调查，我们发现通用和医学 LLM 即使具有不同的模型规模、不同的提示或微调策略，仍然无法在临床预测中击败传统 ML 模型，这表明它们在临床推理和决策方面存在潜在缺陷。我们呼吁从业者在临床应用中采用 LLM 时要谨慎。 ClinicalBench 可用于弥合 LLM 医疗保健发展与现实世界临床实践之间的差距。</li>
</ul>

<h3>Title: Epistemic Integrity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bijean Ghafouri, Shahrad Mohammadzadeh, James Zhou, Pratheeksha Nair, Jacob-Junqi Tian, Mayank Goel, Reihaneh Rabbany, Jean-François Godbout, Kellin Pelrine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06528">https://arxiv.org/abs/2411.06528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06528">https://arxiv.org/pdf/2411.06528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06528]] Epistemic Integrity in Large Language Models(https://arxiv.org/abs/2411.06528)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\unicode{x2013}$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地被用作信息来源，但它们倾向于以高置信度生成虚假或误导性陈述，这对用户和社会构成了风险。在本文中，我们面对的是认知误校准 $\unicode{x2013}$ 这一关键问题，即模型的语言自信度无法反映其真正的内部确定性。我们引入了一种新的人工标记数据集和一种用于测量大型语言模型 (LLM) 语言自信度的新方法，与以前的基准相比，该方法可将错误率降低 50% 以上。通过在多个数据集上进行验证，我们的方法揭示了模型在语言上呈现信息的自信度与其实际准确性之间存在严重偏差。进一步的人工评估证实了这种误校准的严重性。这一证据强调了 LLM 所持有的过分强调的确定性的紧迫风险，这可能会大规模误导用户。我们的框架在诊断这种错误校准方面迈出了重要的一步，为纠正错误校准提供了一条途径，并为跨领域提供更值得信赖的人工智能。</li>
</ul>

<h3>Title: Explore the Reasoning Capability of LLMs in the Chess Testbed</h3>
<ul>
<li><strong>Authors: </strong>Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, Ying Nian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06655">https://arxiv.org/abs/2411.06655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06655">https://arxiv.org/pdf/2411.06655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06655]] Explore the Reasoning Capability of LLMs in the Chess Testbed(https://arxiv.org/abs/2411.06655)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated by chess experts for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.</li>
<li><strong>摘要：</strong>推理是人类智能的核心能力。近年来，随着大规模数据集的出现，预训练的大型语言模型已经出现，具有推理等新功能。然而，这些模型仍然难以完成长期、复杂的推理任务，比如下棋。根据对专业棋手采用长期战略游戏与短期战术游戏以及语言解释相结合的双重方法的观察，我们提出通过整合带注释的战略和战术来提高国际象棋大型语言模型的推理能力。具体来说，我们收集了一个名为 MATE 的数据集，其中包含 100 万个国际象棋位置，国际象棋专家为战略和战术注释了候选动作。我们对 LLaMA-3-8B 模型进行了微调，并将其与最先进的商业语言模型进行比较，以选择更好的国际象棋动作。我们的实验表明，我们的模型比 GPT、Claude 和 Gemini 模型表现更好。我们发现语言解释可以增强大型语言模型的推理能力。</li>
</ul>

<h3>Title: Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation</h3>
<ul>
<li><strong>Authors: </strong>Qiao Qiao, Yuepei Li, Qing Wang, Kang Zhou, Qi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06660">https://arxiv.org/abs/2411.06660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06660">https://arxiv.org/pdf/2411.06660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06660]] Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation(https://arxiv.org/abs/2411.06660)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge graph completion (KGC) is a task of inferring missing triples based on existing Knowledge Graphs (KGs). Both structural and semantic information are vital for successful KGC. However, existing methods only use either the structural knowledge from the KG embeddings or the semantic information from pre-trained language models (PLMs), leading to suboptimal model performance. Moreover, since PLMs are not trained on KGs, directly using PLMs to encode triples may be inappropriate. To overcome these limitations, we propose a novel framework called Bridge, which jointly encodes structural and semantic information of KGs. Specifically, we strategically encode entities and relations separately by PLMs to better utilize the semantic knowledge of PLMs and enable structured representation learning via a structural learning principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a self-supervised representation learning method called BYOL to fine-tune PLMs with two different views of a triple. Unlike BYOL, which uses augmentation methods to create two semantically similar views of the same image, potentially altering the semantic information. We strategically separate the triple into two parts to create different views, thus avoiding semantic alteration. Experiments demonstrate that Bridge outperforms the SOTA models on three benchmark datasets.</li>
<li><strong>摘要：</strong>知识图谱补全 (KGC) 是一项基于现有知识图谱 (KG) 推断缺失三元组的任务。结构和语义信息对于成功的 KGC 都至关重要。然而，现有方法仅使用来自 KG 嵌入的结构知识或来自预训练语言模型 (PLM) 的语义信息，导致模型性能不佳。此外，由于 PLM 不是在 KG 上训练的，因此直接使用 PLM 编码三元组可能不合适。为了克服这些限制，我们提出了一个名为 Bridge 的新框架，它联合编码 KG 的结构和语义信息。具体而言，我们策略性地通过 PLM 分别编码实体和关系，以更好地利用 PLM 的语义知识并通过结构学习原理实现结构化表示学习。此外，为了弥合 KG 和 PLM 之间的差距，我们采用了一种名为 BYOL 的自监督表示学习方法，通过三元组的两种不同视图来微调 PLM。与 BYOL 不同，BYOL 使用增强方法创建同一图像的两个语义相似的视图，这可能会改变语义信息。我们策略性地将三元组分成两部分以创建不同的视图，从而避免语义改变。实验表明，Bridge 在三个基准数据集上的表现优于 SOTA 模型。</li>
</ul>

<h3>Title: What Should Baby Models Read? Exploring Sample-Efficient Data Composition on Model Performance</h3>
<ul>
<li><strong>Authors: </strong>Hong Meng Yam, Nathan J Paek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06672">https://arxiv.org/abs/2411.06672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06672">https://arxiv.org/pdf/2411.06672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06672]] What Should Baby Models Read? Exploring Sample-Efficient Data Composition on Model Performance(https://arxiv.org/abs/2411.06672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We explore the impact of pre-training data composition on the performance of small language models in a sample-efficient setting. Using datasets limited to 10 million words, we evaluate several dataset sources, including child-directed speech (CHILDES), classic books (Gutenberg), synthetic data (TinyStories), and a mix of these (Mix) across different model sizes ranging from 18 million to 705 million parameters. Our experiments show that smaller models (e.g., GPT2-97M, GPT2-705M, Llama-360M) perform better when trained on more complex and rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories datasets underperformed across all model sizes. These findings suggest that the optimal dataset for sample efficient training depends on the model size, and that neither child-directed speech nor simplified stories are optimal for language models of all sizes. We highlight the importance of considering both dataset composition and model capacity for effective sample efficient language model training.</li>
<li><strong>摘要：</strong>我们探索了预训练数据组成对样本高效环境下小型语言模型性能的影响。使用限制为 1000 万个单词的数据集，我们评估了几个数据集来源，包括儿童导向语音 (CHILDES)、经典书籍 (Gutenberg)、合成数据 (TinyStories) 以及这些的混合 (Mix)，涉及从 1800 万到 7.05 亿个参数的不同模型大小。我们的实验表明，较小的模型 (例如 GPT2-97M、GPT2-705M、Llama-360M) 在更复杂和丰富的数据集 (如 Gutenberg) 上训练时表现更好。在 CHILDES 和 TinyStories 数据集上训练的模型在所有模型大小上表现不佳。这些发现表明，样本高效训练的最佳数据集取决于模型大小，并且儿童导向语音和简化故事都不是所有规模的语言模型的最佳选择。我们强调了考虑数据集组成和模型容量对于有效样本高效语言模型训练的重要性。</li>
</ul>

<h3>Title: Reverse Prompt Engineering</h3>
<ul>
<li><strong>Authors: </strong>Hanqing Li, Diego Klabjan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06729">https://arxiv.org/abs/2411.06729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06729">https://arxiv.org/pdf/2411.06729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06729]] Reverse Prompt Engineering(https://arxiv.org/abs/2411.06729)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This paper explores a new black-box, zero-shot language model inversion problem and proposes an innovative framework for prompt reconstruction using only text outputs from a language model. Leveraging a large language model alongside an optimization algorithm, the proposed method effectively recovers prompts with minimal resources. Experimental results on several datasets derived from public sources indicate that the proposed approach achieves high-quality prompt recovery and generates prompts more similar to the originals than current state-of-the-art methods. Additionally, the use-case study demonstrates the method's strong potential for generating high-quality text data.</li>
<li><strong>摘要：</strong>本文探讨了一种新的黑盒零样本语言模型逆问题，并提出了一种创新的提示重建框架，仅使用语言模型的文本输出即可完成提示重建。利用大型语言模型和优化算法，该方法可以以最少的资源有效地恢复提示。在来自公共来源的多个数据集上进行的实验结果表明，与当前最先进的方法相比，该方法实现了高质量的提示恢复，并且生成的提示与原始提示更相似。此外，用例研究还展示了该方法在生成高质量文本数据方面的巨大潜力。</li>
</ul>

<h3>Title: PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Duan, Yonghong Yu, Xiaoming Zhao, Yichang Wu, Wenbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06767">https://arxiv.org/abs/2411.06767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06767">https://arxiv.org/pdf/2411.06767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06767]] PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing(https://arxiv.org/abs/2411.06767)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the "disorientation" in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.</li>
<li><strong>摘要：</strong>Code 大型语言模型（Code LLM），例如 Code llama 和 DeepSeek-Coder，在代码生成任务中表现出色。然而，大多数现有模型专注于生成正确代码的能力，但经常在错误修复方面遇到困难。我们介绍了一套方法来增强 LLM 的 SQL 错误修复能力。这些方法主要由两部分组成：从头开始的渐进式数据集构建（PDC）和动态掩码监督微调（DM-SFT）。PDC 分别从广度优先和深度优先的角度提出了两种数据扩展方法。DM-SFT 引入了一种高效的错误修复监督学习方法，可有效减少总训练步骤并缓解 SQL 代码错误修复训练中的“迷失方向”现象。在我们的评估中，使用这两种方法训练的代码 LLM 模型已经超越了所有当前表现最佳的模型，而这些模型的规模要大得多。</li>
</ul>

<h3>Title: AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant</h3>
<ul>
<li><strong>Authors: </strong>Yujia Zhou, Zheng Liu, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06805">https://arxiv.org/abs/2411.06805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06805">https://arxiv.org/pdf/2411.06805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06805]] AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant(https://arxiv.org/abs/2411.06805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as "hallucination". Initial retrieval-augmented generation (RAG) methods like the "Retrieve-Read" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach, Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现大大推进了自然语言处理，但这些模型通常会生成事实上不正确的信息，即所谓的“幻觉”。最初的检索增强生成 (RAG) 方法（如“检索-阅读”框架）不足以完成复杂的推理任务。随后基于提示的 RAG 策略和监督微调 (SFT) 方法提高了性能，但需要频繁重新训练，并且有改变基础 LLM 功能的风险。为了应对这些挑战，我们提出了基于助手的检索增强生成 (AssistRAG)，将智能信息助手集成到 LLM 中。该助手通过工具使用、动作执行、记忆构建和计划规范来管理记忆和知识。使用两阶段训练方法、课程助手学习和强化偏好优化。AssistRAG 增强了信息检索和决策能力。实验表明，AssistRAG 的表现明显优于基准，尤其是对不太先进的 LLM 有益，因为它提供了卓越的推理能力和准确的响应。</li>
</ul>

<h3>Title: HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yannis Belkhiter, Giulio Zizzo, Sergio Maffeis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06835">https://arxiv.org/abs/2411.06835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06835">https://arxiv.org/pdf/2411.06835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06835]] HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment(https://arxiv.org/abs/2411.06835)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>With the introduction of the transformers architecture, LLMs have revolutionized the NLP field with ever more powerful models. Nevertheless, their development came up with several challenges. The exponential growth in computational power and reasoning capabilities of language models has heightened concerns about their security. As models become more powerful, ensuring their safety has become a crucial focus in research. This paper aims to address gaps in the current literature on jailbreaking techniques and the evaluation of LLM vulnerabilities. Our contributions include the creation of a novel dataset designed to assess the harmfulness of model outputs across multiple harm levels, as well as a focus on fine-grained harm-level analysis. Using this framework, we provide a comprehensive benchmark of state-of-the-art jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model. Additionally, we examine how quantization techniques, such as AWQ and GPTQ, influence the alignment and robustness of models, revealing trade-offs between enhanced robustness with regards to transfer attacks and potential increases in vulnerability on direct ones. This study aims to demonstrate the influence of harmful input queries on the complexity of jailbreaking techniques, as well as to deepen our understanding of LLM vulnerabilities and improve methods for assessing model robustness when confronted with harmful content, particularly in the context of compression strategies.</li>
<li><strong>摘要：</strong>随着 transformers 架构的引入，LLM 以更强大的模型彻底改变了 NLP 领域。然而，它们的发展也带来了一些挑战。语言模型的计算能力和推理能力呈指数级增长，加剧了人们对其安全性的担忧。随着模型变得越来越强大，确保其安全性已成为研究的关键重点。本文旨在解决当前越狱技术和 LLM 漏洞评估文献中的空白。我们的贡献包括创建一个新的数据集，旨在评估模型输出在多个危害级别的危害性，以及专注于细粒度危害级别分析。使用这个框架，我们提供了一个全面的基准，用于最先进的越狱攻击，特别是针对 Vicuna 13B v1.5 模型。此外，我们研究了量化技术（如 AWQ 和 GPTQ）如何影响模型的对齐和稳健性，揭示了增强的稳健性与直接攻击的潜在漏洞增加之间的权衡。本研究旨在展示有害输入查询对越狱技术复杂性的影响，并加深我们对 LLM 漏洞的理解，并改进在面对有害内容时评估模型稳健性的方法，特别是在压缩策略的背景下。</li>
</ul>

<h3>Title: Persuasion with Large Language Models: a Survey</h3>
<ul>
<li><strong>Authors: </strong>Alexander Rogiers, Sander Noels, Maarten Buyl, Tijl De Bie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06837">https://arxiv.org/abs/2411.06837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06837">https://arxiv.org/pdf/2411.06837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06837]] Persuasion with Large Language Models: a Survey(https://arxiv.org/abs/2411.06837)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid rise of Large Language Models (LLMs) has created new disruptive possibilities for persuasive communication, by enabling fully-automated personalized and interactive content generation at an unprecedented scale. In this paper, we survey the research field of LLM-based persuasion that has emerged as a result. We begin by exploring the different modes in which LLM Systems are used to influence human attitudes and behaviors. In areas such as politics, marketing, public health, e-commerce, and charitable giving, such LLM Systems have already achieved human-level or even super-human persuasiveness. We identify key factors influencing their effectiveness, such as the manner of personalization and whether the content is labelled as AI-generated. We also summarize the experimental designs that have been used to evaluate progress. Our survey suggests that the current and future potential of LLM-based persuasion poses profound ethical and societal risks, including the spread of misinformation, the magnification of biases, and the invasion of privacy. These risks underscore the urgent need for ethical guidelines and updated regulatory frameworks to avoid the widespread deployment of irresponsible and harmful LLM Systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的迅速崛起为说服性沟通创造了新的颠覆性可能性，它实现了前所未有的规模的全自动个性化和交互式内容生成。在本文中，我们调查了由此出现的基于 LLM 的说服研究领域。我们首先探索 LLM 系统用于影响人类态度和行为的不同模式。在政治、营销、公共卫生、电子商务和慈善捐赠等领域，此类 LLM 系统已经达到了人类水平甚至超人的说服力。我们确定了影响其有效性的关键因素，例如个性化方式以及内容是否被标记为 AI 生成。我们还总结了用于评估进展的实验设计。我们的调查表明，基于 LLM 的说服的当前和未来潜力带来了巨大的道德和社会风险，包括错误信息的传播、偏见的放大以及隐私的侵犯。这些风险凸显了制定道德准则和更新监管框架的迫切需要，以避免不负责任和有害的 LLM 系统的广泛部署。</li>
</ul>

<h3>Title: LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06839">https://arxiv.org/abs/2411.06839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06839">https://arxiv.org/pdf/2411.06839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06839]] LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models(https://arxiv.org/abs/2411.06839)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel LLM-Neo framework that efficiently transfers knowledge from a large language model (LLM) teacher to a compact student. Initially, we revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and argue that they share the same paradigm. Inspired by this observation, we explore the strategy that combines LoRA and KD to enhance the efficiency of knowledge transfer. We first summarize some guidelines for this design and further develop the LLM-Neo. Experimental results on compressing Llama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further analysis demonstrates the robustness of the proposed LLM-Neo on variants of LoRA. The trained models have been available at \href{this https URL}{this repository}.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种新颖的 LLM-Neo 框架，可以有效地将知识从大型语言模型 (LLM) 教师转移到紧凑型学生。首先，我们重新审视知识蒸馏 (KD) 和低秩自适应 (LoRA)，并认为它们共享相同的范式。受此观察的启发，我们探索了结合 LoRA 和 KD 来提高知识转移效率的策略。我们首先总结了一些设计指南，并进一步开发了 LLM-Neo。压缩 Llama 2 和 Llama 3 的实验结果表明 LLM-Neo 优于各种基线。进一步的分析证明了所提出的 LLM-Neo 在 LoRA 变体上的稳健性。训练好的模型已在 \href{此 https URL}{此存储库} 上提供。</li>
</ul>

<h3>Title: 1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jebish Purbey, Siddartha Pullakhandam, Kanwal Mehreen, Muhammad Arham, Drishti Sharma, Ashay Srivastava, Ram Mohan Rao Kadiyala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06850">https://arxiv.org/abs/2411.06850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06850">https://arxiv.org/pdf/2411.06850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06850]] 1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs(https://arxiv.org/abs/2411.06850)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper presents a detailed system description of our entry for the CHiPSAL 2025 shared task, focusing on language detection, hate speech identification, and target detection in Devanagari script languages. We experimented with a combination of large language models and their ensembles, including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like focal loss to address challenges in the natural understanding of Devanagari languages, such as multilingual processing and class imbalance. Our approach achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804 for Sub-tasks A, B, and C respectively. This work provides insights into the effectiveness of transformer models in tasks with domain-specific and linguistic challenges, as well as areas for potential improvement in future iterations.</li>
<li><strong>摘要：</strong>本文详细介绍了我们参加 CHiPSAL 2025 共享任务的参赛作品的系统描述，重点关注语言检测、仇恨言论识别和梵文脚本语言中的目标检测。我们尝试了大型语言模型及其集成（包括 MuRIL、IndicBERT 和 Gemma-2）的组合，并利用焦点损失等独特技术来解决梵文自然理解方面的挑战，例如多语言处理和类别不平衡。我们的方法在所有任务中都取得了有竞争力的结果：子任务 A、B 和 C 的 F1 分别为 0.9980、0.7652 和 0.6804。这项工作深入了解了 Transformer 模型在具有领域特定和语言挑战的任务中的有效性，以及未来迭代中可能改进的领域。</li>
</ul>

<h3>Title: Evaluating Large Language Models on Financial Report Summarization: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Xinqi Yang, Scott Zang, Yong Ren, Dingjie Peng, Zheng Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06852">https://arxiv.org/abs/2411.06852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06852">https://arxiv.org/pdf/2411.06852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06852]] Evaluating Large Language Models on Financial Report Summarization: An Empirical Study(https://arxiv.org/abs/2411.06852)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model's capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmarks for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model's output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 在各种应用中表现出了非凡的多功能性，包括自然语言理解、领域特定知识任务等。然而，将 LLM 应用于金融等复杂、高风险领域需要进行严格评估，以确保可靠性、准确性和符合行业标准。为了满足这一需求，我们对三种最先进的 LLM、GLM-4、Mistral-NeMo 和 LLaMA3.1 进行了全面的比较研究，重点关注它们在生成自动财务报告方面的有效性。我们的主要动机是探索如何在金融领域利用这些模型，金融领域要求精确度、上下文相关性和对错误或误导信息的鲁棒性。通过检查每个模型的功能，我们旨在对它们的优势和局限性进行深刻的评估。我们的论文为财务报告分析提供了基准，涵盖了 ROUGE-1、BERT 分数和 LLM 分数等拟议指标。我们引入了一种创新的评估框架，该框架整合了定量指标（例如精确度、召回率）和定性分析（例如上下文契合度、一致性），以提供对每个模型输出质量的整体看法。此外，我们公开提供财务数据集，邀请研究人员和从业人员通过更广泛的社区参与和协作改进来利用、审查和增强我们的研究成果。我们的数据集可在 huggingface 上找到。</li>
</ul>

<h3>Title: LongSafetyBench: Long-Context LLMs Struggle with Safety Issues</h3>
<ul>
<li><strong>Authors: </strong>Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06899">https://arxiv.org/abs/2411.06899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06899">https://arxiv.org/pdf/2411.06899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06899]] LongSafetyBench: Long-Context LLMs Struggle with Safety Issues(https://arxiv.org/abs/2411.06899)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the development of large language models (LLMs), the sequence length of these models continues to increase, drawing significant attention to long-context language models. However, the evaluation of these models has been primarily limited to their capabilities, with a lack of research focusing on their safety. Existing work, such as ManyShotJailbreak, has to some extent demonstrated that long-context language models can exhibit safety concerns. However, the methods used are limited and lack comprehensiveness. In response, we introduce \textbf{LongSafetyBench}, the first benchmark designed to objectively and comprehensively evaluate the safety of long-context models. LongSafetyBench consists of 10 task categories, with an average length of 41,889 words. After testing eight long-context language models on LongSafetyBench, we found that existing models generally exhibit insufficient safety capabilities. The proportion of safe responses from most mainstream long-context LLMs is below 50\%. Moreover, models' safety performance in long-context scenarios does not always align with that in short-context scenarios. Further investigation revealed that long-context models tend to overlook harmful content within lengthy texts. We also proposed a simple yet effective solution, allowing open-source models to achieve performance comparable to that of top-tier closed-source models. We believe that LongSafetyBench can serve as a valuable benchmark for evaluating the safety capabilities of long-context language models. We hope that our work will encourage the broader community to pay attention to the safety of long-context models and contribute to the development of solutions to improve the safety of long-context LLMs.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的发展，模型的序列长度不断增加，长上下文语言模型受到广泛关注。但对该类模型的评估主要局限于其能力，对其安全性的研究较少。现有工作如ManyShotJailbreak在一定程度上证明了长上下文语言模型存在安全性隐患，但所采用的方法有限且不够全面。针对此，我们引入了\textbf{LongSafetyBench}，这是第一个旨在客观、全面评估长上下文模型安全性的基准测试。LongSafetyBench包含10个任务类别，平均长度为41,889个单词。在LongSafetyBench上测试了8个长上下文语言模型后，我们发现现有模型普遍存在安全能力不足的问题，主流的长上下文LLM模型的安全响应比例都在50\%以下，而且模型在长上下文场景下的安全表现并不一定与短上下文场景下的一致。进一步的调查发现，长上下文模型往往会忽略长文本中的有害内容。我们还提出了一个简单而有效的解决方案，让开源模型达到与顶级闭源模型相当的性能。我们相信 LongSafetyBench 可以作为评估长上下文语言模型安全能力的宝贵基准。我们希望我们的工作能够鼓励更广泛的社区关注长上下文模型的安全性，并为开发提高长上下文 LLM 安全性的解决方案做出贡献。</li>
</ul>

<h3>Title: Cancer-Answer: Empowering Cancer Care with Advanced Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aniket Deroy, Subhankar Maity</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06946">https://arxiv.org/abs/2411.06946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06946">https://arxiv.org/pdf/2411.06946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06946]] Cancer-Answer: Empowering Cancer Care with Advanced Large Language Models(https://arxiv.org/abs/2411.06946)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Gastrointestinal (GI) tract cancers account for a substantial portion of the global cancer burden, where early diagnosis is critical for improved management and patient outcomes. The complex aetiologies and overlapping symptoms across GI cancers often delay diagnosis, leading to suboptimal treatment strategies. Cancer-related queries are crucial for timely diagnosis, treatment, and patient education, as access to accurate, comprehensive information can significantly influence outcomes. However, the complexity of cancer as a disease, combined with the vast amount of available data, makes it difficult for clinicians and patients to quickly find precise answers. To address these challenges, we leverage large language models (LLMs) such as GPT-3.5 Turbo to generate accurate, contextually relevant responses to cancer-related queries. Pre-trained with medical data, these models provide timely, actionable insights that support informed decision-making in cancer diagnosis and care, ultimately improving patient outcomes. We calculate two metrics: A1 (which represents the fraction of entities present in the model-generated answer compared to the gold standard) and A2 (which represents the linguistic correctness and meaningfulness of the model-generated answer with respect to the gold standard), achieving maximum values of 0.546 and 0.881, respectively.</li>
<li><strong>摘要：</strong>胃肠道 (GI) 癌症占全球癌症负担的很大一部分，早期诊断对于改善管理和患者治疗效果至关重要。胃肠道癌症的复杂病因和重叠症状往往会延迟诊断，导致治疗策略不理想。癌症相关查询对于及时诊断、治疗和患者教育至关重要，因为获取准确、全面的信息会显著影响治疗效果。然而，癌症作为一种疾病的复杂性，加上大量可用数据，使得临床医生和患者难以快速找到准确的答案。为了应对这些挑战，我们利用大型语言模型 (LLM)（例如 GPT-3.5 Turbo）来生成准确、与上下文相关的癌症相关查询的响应。这些模型经过医疗数据的预先训练，可提供及时、可操作的见解，支持在癌症诊断和护理中做出明智的决策，最终改善患者的治疗效果。我们计算了两个指标：A1（表示与黄金标准相比，模型生成的答案中存在的实体的比例）和A2（表示相对于黄金标准的模型生成的答案的语言正确性和意义），分别达到了最大值0.546和0.881。</li>
</ul>

<h3>Title: Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual Alignment with Human Smell Experiences</h3>
<ul>
<li><strong>Authors: </strong>Shu Zhong, Zetao Zhou, Christopher Dawes, Giada Brianz, Marianna Obrist</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06950">https://arxiv.org/abs/2411.06950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06950">https://arxiv.org/pdf/2411.06950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06950]] Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual Alignment with Human Smell Experiences(https://arxiv.org/abs/2411.06950)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aligning AI with human intent is important, yet perceptual alignment-how AI interprets what we see, hear, or smell-remains underexplored. This work focuses on olfaction, human smell experiences. We conducted a user study with 40 participants to investigate how well AI can interpret human descriptions of scents. Participants performed "sniff and describe" interactive tasks, with our designed AI system attempting to guess what scent the participants were experiencing based on their descriptions. These tasks evaluated the Large Language Model's (LLMs) contextual understanding and representation of scent relationships within its internal states - high-dimensional embedding space. Both quantitative and qualitative methods were used to evaluate the AI system's performance. Results indicated limited perceptual alignment, with biases towards certain scents, like lemon and peppermint, and continued failing to identify others, like rosemary. We discuss these findings in light of human-AI alignment advancements, highlighting the limitations and opportunities for enhancing HCI systems with multisensory experience integration.</li>
<li><strong>摘要：</strong>将人工智能与人类意图相结合非常重要，但感知结合（即人工智能如何解释我们看到、听到或闻到的东西）仍未得到充分探索。这项工作的重点是嗅觉，即人类的嗅觉体验。我们进行了一项有 40 名参与者的用户研究，以调查人工智能如何很好地解释人类对气味的描述。参与者执行“嗅闻和描述”交互式任务，我们设计的人工智能系统试图根据参与者的描述猜测他们正在体验什么气味。这些任务评估了大型语言模型 (LLM) 对内部状态（高维嵌入空间）中气味关系的上下文理解和表示。定量和定性方法都用于评估人工智能系统的性能。结果表明，感知结合有限，偏向某些气味，如柠檬和薄荷，并且仍然无法识别其他气味，如迷迭香。我们根据人机结合的进步讨论了这些发现，强调了通过多感官体验集成增强人机交互系统的局限性和机会。</li>
</ul>

<h3>Title: Token2Wave</h3>
<ul>
<li><strong>Authors: </strong>Xin Zhang, Victor S. Sheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.06989">https://arxiv.org/abs/2411.06989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.06989">https://arxiv.org/pdf/2411.06989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.06989]] Token2Wave(https://arxiv.org/abs/2411.06989)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper provides an in-depth analysis of Token2Wave, a novel token representation method derived from the Wave Network, designed to capture both global and local semantics of input text through wave-inspired complex vectors. In Token2Wave, each token is represented with a magnitude component, capturing the global semantics of the entire input text, and a phase component, encoding the relationships between individual tokens and the global semantics. Building on prior research that demonstrated the effectiveness of wave-like operations, such as interference and modulation, during forward propagation, this study investigates the convergence behavior, backpropagation characteristics, and embedding independence within the Token2Wave framework. A detailed computational complexity analysis shows that Token2Wave can significantly reduce video memory usage and training time compared to BERT. Gradient comparisons for the [CLS] token, total input text, and classifier parameters further highlight Token2Wave's unique characteristics. This research offers new insights into wave-based token representations, demonstrating their potential to enable efficient and computationally friendly language model architectures.</li>
<li><strong>摘要：</strong>本文对 Token2Wave 进行了深入分析，这是一种源自 Wave 网络的新型 token 表示方法，旨在通过受波启发的复杂向量捕获输入文本的全局和局部语义。在 Token2Wave 中，每个 token 都用一个幅度分量表示，捕获整个输入文本的全局语义，以及一个相位分量，编码各个 token 和全局语义之间的关系。基于先前的研究，证明了干扰和调制等波状操作在前向传播过程中的有效性，本研究调查了 Token2Wave 框架内的收敛行为、反向传播特性和嵌入独立性。详细的计算复杂度分析表明，与 BERT 相比，Token2Wave 可以显着减少视频内存使用量和训练时间。[CLS] token、总输入文本和分类器参数的梯度比较进一步突出了 Token2Wave 的独特特性。这项研究为基于波的 token 表示提供了新的见解，展示了它们实现高效且计算友好的语言模型架构的潜力。</li>
</ul>

<h3>Title: LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07037">https://arxiv.org/abs/2411.07037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07037">https://arxiv.org/pdf/2411.07037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07037]] LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios(https://arxiv.org/abs/2411.07037)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) continue to advance in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become crucial for real-world applications. While existing benchmarks assess various LLM capabilities, they rarely focus on instruction-following in long-context scenarios or stability on different inputs. In response, we introduce the Long-context Instruction-Following Benchmark (LIFBench), a scalable dataset designed to evaluate LLMs' instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, supported by 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment framework that provides precise, automated scoring of complex LLM responses without relying on LLM-assisted evaluations or human judgments. This approach facilitates a comprehensive analysis of model performance and stability across various perspectives. We conduct extensive experiments on 20 notable LLMs across six length intervals, analyzing their instruction-following capabilities and stability. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex, long-context settings, providing insights that can inform future LLM development.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在自然语言处理 (NLP) 领域的不断进步，它们在长上下文输入中稳定遵循指令的能力对于实际应用至关重要。虽然现有的基准测试评估了各种 LLM 功能，但它们很少关注长上下文场景中的指令遵循或不同输入的稳定性。为此，我们引入了长上下文指令遵循基准 (LIFBench)，这是一个可扩展的数据集，旨在评估 LLM 在长上下文中的指令遵循能力和稳定性。LIFBench 包含三个长上下文场景和 11 个不同的任务，由通过长度、表达和变量三个维度的自动扩展方法生成的 2,766 条指令支持。对于评估，我们提出了 LIFEval，这是一个基于评分标准的评估框架，它可以精确、自动地对复杂的 LLM 响应进行评分，而无需依赖 LLM 辅助评估或人工判断。这种方法有助于从各个角度全面分析模型性能和稳定性。我们对 20 个著名的 LLM 进行了广泛的实验，涵盖六个长度间隔，分析了它们的指令遵循能力和稳定性。我们的工作贡献了 LIFBench 和 LIFEval 作为评估 LLM 在复杂、长上下文环境中表现的强大工具，提供了可以为未来 LLM 开发提供参考的见解。</li>
</ul>

<h3>Title: On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qian Sun, Hanpeng Wu, Xi Sheryl Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07070">https://arxiv.org/abs/2411.07070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07070">https://arxiv.org/pdf/2411.07070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07070]] On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models(https://arxiv.org/abs/2411.07070)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>The pretraining and fine-tuning approach has become the leading technique for various NLP applications. However, recent studies reveal that fine-tuning data, due to their sensitive nature, domain-specific characteristics, and identifiability, pose significant privacy concerns. To help develop more privacy-resilient fine-tuning models, we introduce a novel active privacy auditing framework, dubbed Parsing, designed to identify and quantify privacy leakage risks during the supervised fine-tuning (SFT) of language models (LMs). The framework leverages improved white-box membership inference attacks (MIAs) as the core technology, utilizing novel learning objectives and a two-stage pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the exposure of privacy risks. Additionally, we have improved the effectiveness of MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our research aims to provide the SFT community of LMs with a reliable, ready-to-use privacy auditing tool, and to offer valuable insights into safeguarding privacy during the fine-tuning process. Experimental results confirm the framework's efficiency across various models and tasks, emphasizing notable privacy concerns in the fine-tuning process. Project code available for this https URL.</li>
<li><strong>摘要：</strong>预训练和微调方法已成为各种 NLP 应用的领先技术。然而，最近的研究表明，由于微调数据的敏感性、领域特定特征和可识别性，微调数据带来了严重的隐私问题。为了帮助开发更具隐私弹性的微调模型，我们引入了一种新颖的主动隐私审计框架，称为 Parsing，旨在识别和量化语言模型 (LM) 监督微调 (SFT) 期间的隐私泄露风险。该框架利用改进的白盒成员推理攻击 (MIA) 作为核心技术，利用新颖的学习目标和两阶段管道来监控 LM 微调过程的隐私，最大限度地暴露隐私风险。此外，我们还提高了 MIA 对大型 LM 的有效性，包括 GPT-2、Llama2 及其某些变体。我们的研究旨在为 LM 的 SFT 社区提供可靠、随时可用的隐私审计工具，并为在微调过程中保护隐私提供宝贵见解。实验结果证实了该框架在各种模型和任务中的效率，强调了微调过程中值得注意的隐私问题。项目代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Transformer verbatim in-context retrieval across time and scale</h3>
<ul>
<li><strong>Authors: </strong>Kristijan Armeni, Marko Pranjić, Senja Pollak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07075">https://arxiv.org/abs/2411.07075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07075">https://arxiv.org/pdf/2411.07075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07075]] Transformer verbatim in-context retrieval across time and scale(https://arxiv.org/abs/2411.07075)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>To predict upcoming text, language models must in some cases retrieve in-context information verbatim. In this report, we investigated how the ability of language models to retrieve arbitrary in-context nouns developed during training (across time) and as language models trained on the same dataset increase in size (across scale). We then asked whether learning of in-context retrieval correlates with learning of more challenging zero-shot benchmarks. Furthermore, inspired by semantic effects in human short-term memory, we evaluated the retrieval with respect to a major semantic component of target nouns, namely whether they denote a concrete or abstract entity, as rated by humans. We show that verbatim in-context retrieval developed in a sudden transition early in the training process, after about 1% of the training tokens. This was observed across model sizes (from 14M and up to 12B parameters), and the transition occurred slightly later for the two smallest models. We further found that the development of verbatim in-context retrieval is positively correlated with the learning of zero-shot benchmarks. Around the transition point, all models showed the advantage of retrieving concrete nouns as opposed to abstract nouns. In all but two smallest models, the advantage dissipated away toward the end of training.</li>
<li><strong>摘要：</strong>为了预测即将到来的文本，语言模型在某些情况下必须逐字检索上下文信息。在本报告中，我们研究了语言模型在训练期间（跨时间）以及在同一数据集上训练的语言模型规模增加（跨规模）时检索任意上下文名词的能力。然后，我们询问上下文检索的学习是否与更具挑战性的零样本基准的学习相关。此外，受人类短期记忆中语义效应的启发，我们根据目标名词的主要语义成分评估了检索，即它们是否表示人类评定的具体或抽象实体。我们表明，在训练过程的早期，大约 1% 的训练标记之后，逐字上下文检索在突然转变中发展。在各种模型大小（从 14M 到 12B 参数）中都观察到了这一点，并且对于两个最小的模型，转变发生得稍晚一些。我们进一步发现，逐字上下文检索的发展与零样本基准的学习呈正相关。在转换点附近，所有模型都显示出检索具体名词而非抽象名词的优势。除了两个最小的模型外，其他所有模型的优势都在训练结束时消失。</li>
</ul>

<h3>Title: Training Neural Networks as Recognizers of Formal Languages</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07107">https://arxiv.org/abs/2411.07107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07107">https://arxiv.org/pdf/2411.07107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07107]] Training Neural Networks as Recognizers of Formal Languages(https://arxiv.org/abs/2411.07107)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves a discrepancy between experiments and the formal claims they are meant to support. The problem is that formal language theory pertains specifically to recognizers: machines that receive a string as input and classify whether it belongs to a language. On the other hand, it is common to instead use proxy tasks that are similar in only an informal sense, such as language modeling or sequence-to-sequence transduction. We correct this mismatch by training and evaluating neural networks directly as binary classifiers of strings, using a general method that can be applied to a wide variety of languages. As part of this, we extend an algorithm recently proposed by Snæbjarnarson et al. (2024) to do length-controlled sampling of strings from regular languages, with much better asymptotic time complexity than previous methods. We provide results on a variety of languages across the Chomsky hierarchy for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer. We find that the RNN and LSTM often outperform the transformer, and that auxiliary training objectives such as language modeling can help, although no single objective uniformly improves performance across languages and architectures. Our contributions will facilitate theoretically sound empirical testing of language recognition claims in future work. We have released our datasets as a benchmark called FLaRe (Formal Language Recognition), along with our code.</li>
<li><strong>摘要：</strong>用形式语言理论来描述神经网络架构的计算能力仍然是一个关键的研究方向，因为它描述了现代人工智能推理能力的下限和上限。然而，在对这些界限进行实证测试时，现有的研究往往会导致实验与它们旨在支持的正式声明之间存在差异。问题在于，形式语言理论专门适用于识别器：接收字符串作为输入并对其进行分类是否属于某种语言的机器。另一方面，人们通常使用仅在非正式意义上相似的代理任务，例如语言建模或序列到序列的转换。我们通过直接将神经网络作为字符串的二元分类器进行训练和评估来纠正这种不匹配，使用一种可以应用于各种语言的通用方法。作为其中的一部分，我们扩展了 Snæbjarnarson 等人 (2024) 最近提出的一种算法，以对来自常规语言的字符串进行长度控制采样，其渐近时间复杂度比以前的方法要好得多。我们针对三种神经架构提供了 Chomsky 层次结构中多种语言的结果：简单 RNN、LSTM 和因果掩蔽转换器。我们发现 RNN 和 LSTM 通常比转换器表现更好，语言建模等辅助训练目标可以提供帮助，尽管没有一个单一目标能够统一提高跨语言和架构的性能。我们的贡献将有助于在未来的工作中对语言识别声明进行理论上合理的实证测试。我们已将我们的数据集作为称为 FLaRe（形式语言识别）的基准发布，并附带了我们的代码。</li>
</ul>

<h3>Title: Building a Taiwanese Mandarin Spoken Language Model: A First Attempt</h3>
<ul>
<li><strong>Authors: </strong>Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07111">https://arxiv.org/abs/2411.07111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07111">https://arxiv.org/pdf/2411.07111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07111]] Building a Taiwanese Mandarin Spoken Language Model: A First Attempt(https://arxiv.org/abs/2411.07111)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.</li>
<li><strong>摘要：</strong>本技术报告介绍了我们为台湾普通话构建口语大型语言模型 (LLM) 的初步尝试，该模型专门用于实现多轮对话中的实时语音对语音交互。我们的端到端模型采用了仅解码器的转换器架构，旨在实现无缝交互，同时保留对话流程，包括允许同时说话和聆听的全双工功能。本文还详细介绍了训练过程，包括使用合成对话进行数据准备和实时交互调整。我们还开发了一个平台来评估多轮对话中的对话流畅性和响应连贯性。我们希望该报告的发布能够为台湾普通话口语 LLM 的未来发展做出贡献。</li>
</ul>

<h3>Title: SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ruben Härle, Felix Friedrich, Manuel Brack, Björn Deiseroth, Patrick Schramowski, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07122">https://arxiv.org/abs/2411.07122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07122">https://arxiv.org/pdf/2411.07122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07122]] SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering in LLMs(https://arxiv.org/abs/2411.07122)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, but their output may not be aligned with the user or even produce harmful content. This paper presents a novel approach to detect and steer concepts such as toxicity before generation. We introduce the Sparse Conditioned Autoencoder (SCAR), a single trained module that extends the otherwise untouched LLM. SCAR ensures full steerability, towards and away from concepts (e.g., toxic content), without compromising the quality of the model's text generation on standard evaluation benchmarks. We demonstrate the effective application of our approach through a variety of concepts, including toxicity, safety, and writing style alignment. As such, this work establishes a robust framework for controlling LLM generations, ensuring their ethical and safe deployment in real-world applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出生成类人文本的卓越能力，但它们的输出可能与用户不一致，甚至会产生有害内容。本文介绍了一种在生成之前检测和控制诸如毒性之类的概念的新方法。我们引入了稀疏条件自动编码器 (SCAR)，这是一个经过单独训练的模块，可以扩展原本未受影响的 LLM。SCAR 确保完全可控性，无论是朝向还是远离概念（例如，有毒内容），而不会损害模型在标准评估基准上的文本生成质量。我们通过各种概念（包括毒性、安全性和写作风格对齐）展示了我们方法的有效应用。因此，这项工作建立了一个控制 LLM 生成的强大框架，确保它们在实际应用中合乎道德且安全的部署。</li>
</ul>

<h3>Title: Benchmarking LLMs' Judgments with No Gold Standard</h3>
<ul>
<li><strong>Authors: </strong>Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07127">https://arxiv.org/abs/2411.07127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07127">https://arxiv.org/pdf/2411.07127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07127]] Benchmarking LLMs' Judgments with No Gold Standard(https://arxiv.org/abs/2411.07127)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review. GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner. We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.</li>
<li><strong>摘要：</strong>我们引入了 GEM（相互信息生成估计器），这是一种评估大型语言模型 (LLM) 语言生成的评估指标，特别是在生成信息性判断方面，无需黄金标准参考。GEM 扩大了我们可以对 LLM 生成性能进行基准测试的场景——从传统的机器翻译和摘要（其中黄金标准参考随时可用）到没有明确黄金标准的主观任务（例如学术同行评审）。GEM 使用生成模型来估计候选和参考响应之间的相互信息，而无需参考是黄金标准。在对人工注释数据集进行的实验中，与最先进的 GPT-4o Examiner 相比，GEM 表现出与人工评分的竞争相关性，并且优于所有其他基线。此外，GEM 对策略性操纵（例如改写或延长）的鲁棒性更强，这些操纵可能会在 GPT-4o Examiner 下人为地提高分数。我们还介绍了 GRE-bench（生成评审评估基准），该基准根据 LLM 为学术研究论文生成高质量同行评审的能力对其进行评估。由于 GRE-bench 基于 GEM，因此它继承了 GEM 的稳健性。此外，GRE-bench 利用每年不断涌入的新开放获取研究论文和同行评审来规避数据污染问题（或数据泄漏）。我们使用 ICLR2023 数据集展示了各种热门 LLM 的同行评审能力的 GRE-bench 结果。</li>
</ul>

<h3>Title: Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Kaijian Zou, Muhammad Khalifa, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07130">https://arxiv.org/abs/2411.07130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07130">https://arxiv.org/pdf/2411.07130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07130]] Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation(https://arxiv.org/abs/2411.07130)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have demonstrated an improved capacity to handle long-context information, yet existing long-context benchmarks primarily measure LMs' retrieval abilities with extended inputs, e.g., pinpointing a short phrase from long-form text. Therefore, they may fall short when evaluating models' global context understanding capacity, such as synthesizing and reasoning over content across input to generate the response. In this paper, we study long-context language model (LCLM) evaluation through many-shot in-context learning (ICL). Concretely, we identify the skills each ICL task requires, and examine models' long-context capabilities on them. We first ask: What types of ICL tasks benefit from additional demonstrations, and are these tasks effective at evaluating LCLMs? We find that classification and summarization tasks show notable performance improvements with additional demonstrations, while translation and reasoning tasks do not exhibit clear trends. This suggests the classification tasks predominantly test models' retrieval skills. Next, we ask: To what extent does each task require retrieval skills versus global context understanding from LCLMs? We develop metrics to categorize ICL tasks into two groups: (i) retrieval tasks that require strong retrieval ability to pinpoint relevant examples, and (ii) global context understanding tasks that necessitate a deeper comprehension of the full input. We find that not all datasets can effectively evaluate these long-context capabilities. To address this gap, we introduce a new many-shot ICL benchmark, MANYICLBENCH, designed to characterize LCLMs' retrieval and global context understanding capabilities separately. Benchmarking 11 open-weight LCLMs with MANYICLBENCH, we find that while state-of-the-art models perform well in retrieval tasks up to 64k tokens, many show significant drops in global context tasks at just 16k tokens.</li>
<li><strong>摘要：</strong>语言模型 (LM) 已证明其处理长上下文信息的能力有所提高，但现有的长上下文基准主要衡量 LM 在扩展输入下的检索能力，例如从长文本中精确定位短语。因此，它们在评估模型的全局上下文理解能力（例如对输入内容进行综合和推理以生成响应）时可能会有所不足。在本文中，我们通过多镜头上下文学习 (ICL) 研究长上下文语言模型 (LCLM) 评估。具体来说，我们确定每个 ICL 任务所需的技能，并检查模型在这些任务上的长上下文能力。我们首先问：哪些类型的 ICL 任务可以从额外的演示中受益，这些任务在评估 LCLM 方面是否有效？我们发现分类和总结任务在额外的演示下表现出显着的性能提升，而翻译和推理任务没有表现出明显的趋势。这表明分类任务主要测试模型的检索技能。接下来，我们要问：每项任务在多大程度上需要 LCLM 的检索技能和全局上下文理解？我们制定了指标，将 ICL 任务分为两类：(i) 需要强大检索能力来精确定位相关示例的检索任务，以及 (ii) 需要更深入地理解完整输入的全局上下文理解任务。我们发现并非所有数据集都能有效地评估这些长上下文能力。为了解决这一差距，我们引入了一个新的多样本 ICL 基准 MANYICLBENCH，旨在分别表征 LCLM 的检索和全局上下文理解能力。使用 MANYICLBENCH 对 11 个开放权重 LCLM 进行基准测试，我们发现虽然最先进的模型在多达 64k 个标记的检索任务中表现良好，但许多模型在仅 16k 个标记的全局上下文任务中表现出显着下降。</li>
</ul>

<h3>Title: Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Hui Huang, Weixun Wang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Xuepeng Liu, Dekai Sun, Wenbo Su, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07140">https://arxiv.org/abs/2411.07140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07140">https://arxiv.org/pdf/2411.07140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07140]] Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models(https://arxiv.org/abs/2411.07140)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.</li>
<li><strong>摘要：</strong>新的 LLM 评估基准对于与大型语言模型 (LLM) 的快速发展保持一致非常重要。在这项工作中，我们提出了中文 SimpleQA，这是第一个全面的中文基准，用于评估语言模型回答简短问题的真实性能力，中文 SimpleQA 主要具有五个属性（即中文、多样化、高质量、静态、易于评估）。具体来说，首先，我们专注于 6 个主要主题的中文，其中包含 99 个不同的子主题。其次，我们进行了全面的质量控制流程，以实现高质量的问题和答案，其中参考答案是静态的，不会随时间而改变。第三，遵循 SimpleQA，问题和答案非常简短，并且评分过程基于 OpenAI API 易于评估。基于中文 SimpleQA，我们对现有 LLM 的真实性能力进行了全面评估。最后，我们希望中文 SimpleQA 可以指导开发人员更好地了解其模型的中文真实性能力并促进基础模型的发展。</li>
</ul>

<h3>Title: A Primer on Word Embeddings: AI Techniques for Text Analysis in Social Work</h3>
<ul>
<li><strong>Authors: </strong>Brian E. Perron, Kelley A. Rivenburgh, Bryan G. Victor, Zia Qi, Hui Luan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07156">https://arxiv.org/abs/2411.07156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07156">https://arxiv.org/pdf/2411.07156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07156]] A Primer on Word Embeddings: AI Techniques for Text Analysis in Social Work(https://arxiv.org/abs/2411.07156)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Word embeddings represent a transformative technology for analyzing text data in social work research, offering sophisticated tools for understanding case notes, policy documents, research literature, and other text-based materials. This methodological paper introduces word embeddings to social work researchers, explaining how these mathematical representations capture meaning and relationships in text data more effectively than traditional keyword-based approaches. We discuss fundamental concepts, technical foundations, and practical applications, including semantic search, clustering, and retrieval augmented generation. The paper demonstrates how embeddings can enhance research workflows through concrete examples from social work practice, such as analyzing case notes for housing instability patterns and comparing social work licensing examinations across languages. While highlighting the potential of embeddings for advancing social work research, we acknowledge limitations including information loss, training data constraints, and potential biases. We conclude that successfully implementing embedding technologies in social work requires developing domain-specific models, creating accessible tools, and establishing best practices aligned with social work's ethical principles. This integration can enhance our ability to analyze complex patterns in text data while supporting more effective services and interventions.</li>
<li><strong>摘要：</strong>词嵌入代表了社会工作研究中分析文本数据的变革性技术，为理解案例记录、政策文件、研究文献和其他基于文本的材料提供了复杂的工具。这篇方法论文向社会工作研究人员介绍了词嵌入，解释了这些数学表示如何比传统的基于关键字的方法更有效地捕捉文本数据中的含义和关系。我们讨论了基本概念、技术基础和实际应用，包括语义搜索、聚类和检索增强生成。本文通过社会工作实践中的具体示例展示了嵌入如何增强研究工作流程，例如分析住房不稳定模式的案例记录和比较不同语言的社会工作许可考试。在强调嵌入在推进社会工作研究方面的潜力的同时，我们承认嵌入的局限性，包括信息丢失、训练数据限制和潜在偏见。我们得出结论，成功地在社会工作中实施嵌入技术需要开发特定领域的模型、创建可访问的工具，并建立符合社会工作道德原则的最佳实践。这种整合可以增强我们分析文本数据中复杂模式的能力，同时支持更有效的服务和干预。</li>
</ul>

<h3>Title: Continual Memorization of Factoids in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07175">https://arxiv.org/abs/2411.07175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07175">https://arxiv.org/pdf/2411.07175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07175]] Continual Memorization of Factoids in Large Language Models(https://arxiv.org/abs/2411.07175)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the world has become popular, though it risks disrupting the model's original capabilities. We study this fragility in the context of continual memorization, where the model is trained on a small set of long-tail factoids (factual associations) and must retain these factoids after multiple stages of subsequent training on other datasets. Through extensive experiments, we show that LLMs suffer from forgetting across a wide range of subsequent tasks, and simple replay techniques do not fully prevent forgetting, especially when the factoid datasets are trained in the later stages. We posit that there are two ways to alleviate forgetting: 1) protect the memorization process as the model learns the factoids, or 2) reduce interference from training in later stages. With this insight, we develop an effective mitigation strategy: REMIX (Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic data sampled from pretraining corpora or even randomly generated word sequences during each stage, despite being unrelated to the memorized factoids in the first stage. REMIX can recover performance from severe forgetting, often outperforming replay-based methods that have access to the factoids from the first stage. We then analyze how REMIX alters the learning process and find that successful forgetting prevention is associated with a pattern: the model stores factoids in earlier layers than usual and diversifies the set of layers that store these factoids. The efficacy of REMIX invites further investigation into the underlying dynamics of memorization and forgetting, opening exciting possibilities for future research.</li>
<li><strong>摘要：</strong>大型语言模型可以通过预训练吸收大量知识，但预训练对于获取长尾或专业事实效率低下。因此，对反映世界变化的专业或新知识进行微调已变得流行，尽管它可能会破坏模型的原始功能。我们在持续记忆的背景下研究了这种脆弱性，其中模型在一小组长尾事实（事实关联）上进行训练，并且必须在对其他数据集进行多个阶段的后续训练后保留这些事实。通过大量实验，我们表明 LLM 在后续的广泛任务中都会遭受遗忘，而简单的重放技术并不能完全防止遗忘，尤其是在后期训练事实数据集时。我们假设有两种方法可以缓解遗忘：1) 在模型学习事实时保护记忆过程，或 2) 减少后期训练的干扰。基于这一见解，我们开发了一种有效的缓解策略：REMIX（随机和通用数据混合）。 REMIX 通过在每个阶段混合从预训练语料库中采样的通用数据或甚至随机生成的单词序列来防止遗忘，尽管这些数据与第一阶段记忆的事实无关。REMIX 可以从严重遗忘中恢复性能，通常优于可以访问第一阶段事实的基于重放的方法。然后，我们分析 REMIX 如何改变学习过程，并发现成功的遗忘预防与一种模式有关：该模型将事实存储在比平常更早的层中，并使存储这些事实的层集多样化。REMIX 的有效性促使人们进一步研究记忆和遗忘的潜在动态，为未来的研究开辟了令人兴奋的可能性。</li>
</ul>

<h3>Title: More Expressive Attention with Negative Weights</h3>
<ul>
<li><strong>Authors: </strong>Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07176">https://arxiv.org/abs/2411.07176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07176">https://arxiv.org/pdf/2411.07176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07176]] More Expressive Attention with Negative Weights(https://arxiv.org/abs/2411.07176)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We propose a novel attention mechanism, named Cog Attention, that enables attention weights to be negative for enhanced expressiveness, which stems from two key factors: (1) Cog Attention can shift the token deletion and copying function from a static OV matrix to dynamic QK inner products, with the OV matrix now focusing more on refinement or modification. The attention head can simultaneously delete, copy, or retain tokens by assigning them negative, positive, or minimal attention weights, respectively. As a result, a single attention head becomes more flexible and expressive. (2) Cog Attention improves the model's robustness against representational collapse, which can occur when earlier tokens are over-squashed into later positions, leading to homogeneous representations. Negative weights reduce effective information paths from earlier to later tokens, helping to mitigate this issue. We develop Transformer-like models which use Cog Attention as attention modules, including decoder-only models for language modeling and U-ViT diffusion models for image generation. Experiments show that models using Cog Attention exhibit superior performance compared to those employing traditional softmax attention modules. Our approach suggests a promising research direction for rethinking and breaking the entrenched constraints of traditional softmax attention, such as the requirement for non-negative weights.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的注意力机制，即 Cog Attention，该机制使注意力权重为负，以增强表现力，这源于两个关键因素：（1）Cog Attention 可以将 token 删除和复制功能从静态 OV 矩阵转移到动态 QK 内积，OV 矩阵现在更侧重于细化或修改。注意力头可以同时删除、复制或保留 token，方法是分别为它们分配负、正或最小注意力权重。因此，单个注意力头变得更加灵活和富有表现力。（2）Cog Attention 提高了模型对表征崩溃的鲁棒性，表征崩溃可能发生在早期 token 被过度挤压到后期位置时，从而导致同质表征。负权重减少了从早期 token 到后期 token 的有效信息路径，有助于缓解此问题。我们开发了使用 Cog Attention 作为注意力模块的类似 Transformer 的模型，包括用于语言建模的仅解码器模型和用于图像生成的 U-ViT 扩散模型。实验表明，使用 Cog Attention 的模型比使用传统 softmax 注意力模块的模型表现出更好的性能。我们的方法为重新思考和打破传统 softmax 注意力的根深蒂固的限制（例如对非负权重的要求）提供了一个有前途的研究方向。</li>
</ul>

<h3>Title: Counterfactual Generation from Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07180">https://arxiv.org/abs/2411.07180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07180">https://arxiv.org/pdf/2411.07180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07180]] Counterfactual Generation from Language Models(https://arxiv.org/abs/2411.07180)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.</li>
<li><strong>摘要：</strong>理解和操纵语言模型中的因果生成机制对于控制其行为至关重要。以前的工作主要依赖于诸如表征手术之类的技术（例如，模型消融或操纵与特定概念相关的线性子空间）来干预这些模型。为了准确理解干预的影响，检查反事实很有用——例如，如果给定的句子是由模型在特定干预后生成的，它会是什么样子。我们强调，反事实推理在概念上不同于干预，正如 Pearl 的因果层次结构所阐明的那样。基于这一观察，我们提出了一个生成真实字符串反事实的框架，即将语言模型重新表述为广义结构方程。使用 Gumbel-max 技巧的模型。这使我们能够对原始字符串及其反事实的联合分布进行建模，这些反事实是由采样噪声的相同实例产生的。我们开发了一种基于事后甘贝尔抽样的算法，该算法使我们能够推断潜在噪声变量并生成观察到的字符串的反事实。我们的实验表明，该方法产生了有意义的反事实，同时表明常用的干预技术具有相当大的不良副作用。</li>
</ul>

<h3>Title: The Super Weight in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07191">https://arxiv.org/abs/2411.07191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07191">https://arxiv.org/pdf/2411.07191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07191]] The Super Weight in Large Language Models(https://arxiv.org/abs/2411.07191)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text -- increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.</li>
<li><strong>摘要：</strong>最近的研究显示了一个令人惊讶的结果：大型语言模型 (LLM) 参数异常值的一小部分对模型质量异常重要。LLM 包含数十亿个参数，因此这些小部分（例如 0.01%）意味着数十万个参数。在这项研究中，我们提出了一个更令人惊讶的发现：修剪少至一个参数就会破坏 LLM 生成文本的能力 -- 使困惑度增加 3 个数量级，并将零样本准确度降低为猜测。我们提出了一种无需数据的方法来识别此类参数，称为超级权重，使用单次前向传递通过模型。我们还发现，这些超级权重会相应产生罕见且较大的激活异常值，称为超级激活。当以高精度保存时，超级激活可以改进简单的四舍五入量化，从而与最先进的方法相媲美。对于权重量化，我们同样发现，通过保留超权重并裁剪其他权重异常值，四舍五入量化可以扩展到比以前考虑的更大的块大小。为了便于进一步研究超权重，我们为常见的、公开可用的 LLM 提供了超权重坐标索引。</li>
</ul>

<h3>Title: Contextualized Evaluations: Taking the Guesswork Out of Language Model Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Chaitanya Malaviya, Joseph Chee Chang, Dan Roth, Mohit Iyyer, Mark Yatskar, Kyle Lo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07237">https://arxiv.org/abs/2411.07237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07237">https://arxiv.org/pdf/2411.07237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07237]] Contextualized Evaluations: Taking the Guesswork Out of Language Model Evaluations(https://arxiv.org/abs/2411.07237)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Language model users often issue queries that lack specification, where the context under which a query was issued -- such as the user's identity, the query's intent, and the criteria for a response to be useful -- is not explicit. For instance, a good response to a subjective query like "What book should I read next?" would depend on the user's preferences, and a good response to an open-ended query like "How do antibiotics work against bacteria?" would depend on the user's expertise. This makes evaluation of responses to such queries an ill-posed task, as evaluators may make arbitrary judgments about the response quality. To remedy this, we present contextualized evaluations, a protocol that synthetically constructs context surrounding an underspecified query and provides it during evaluation. We find that the presence of context can 1) alter conclusions drawn from evaluation, even flipping win rates between model pairs, 2) nudge evaluators to make fewer judgments based on surface-level criteria, like style, and 3) provide new insights about model behavior across diverse contexts. Specifically, our procedure uncovers an implicit bias towards WEIRD contexts in models' "default" responses and we find that models are not equally sensitive to following different contexts, even when they are provided in prompts.</li>
<li><strong>摘要：</strong>语言模型用户经常会发出缺乏规范的查询，其中发出查询的上下文（例如用户的身份、查询的意图以及有用的响应标准）并不明确。例如，对主观查询（如“我接下来应该读什么书？”）的良好响应取决于用户的偏好，而对开放式查询（如“抗生素如何对抗细菌？”）的良好响应取决于用户的专业知识。这使得对此类查询的响应的评估成为一项不适定的任务，因为评估者可能会对响应质量做出任意判断。为了解决这个问题，我们提出了情境化评估，这是一种围绕未明确指定的查询综合构建上下文并在评估期间提供的协议。我们发现，上下文的存在可以 1) 改变从评估中得出的结论，甚至在模型对之间翻转胜率，2) 促使评估者根据风格等表面标准做出更少的判断，3) 提供有关跨不同上下文的模型行为的新见解。具体来说，我们的程序揭示了模型“默认”响应中对 WEIRD 上下文的隐性偏见，并且我们发现模型对遵循不同上下文的敏感度并不相同，即使它们是在提示中提供的。</li>
</ul>

<h3>Title: OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, Jillaphat Jaroenkantasima</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07238">https://arxiv.org/abs/2411.07238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07238">https://arxiv.org/pdf/2411.07238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07238]] OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model(https://arxiv.org/abs/2411.07238)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5, finetuned on over 2,000,000 Thai instruction pairs. This report provides an engineering perspective on the model's development, capabilities, and performance. We discuss the model's architecture, training process, and key features, including multi-turn conversation support, Retrieval Augmented Generation (RAG) compatibility, and tool-calling functionality. Benchmark results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various Thai language tasks, outperforming other open-source Thai language models. We also address practical considerations such as GPU memory requirements and deployment strategies.</li>
<li><strong>摘要：</strong>OpenThaiGPT 1.5 是基于 Qwen v2.5 的高级泰语聊天模型，已针对超过 2,000,000 对泰语指令进行了微调。本报告从工程角度介绍了该模型的开发、功能和性能。我们讨论了该模型的架构、训练过程和主要功能，包括多轮对话支持、检索增强生成 (RAG) 兼容性和工具调用功能。基准测试结果表明 OpenThaiGPT 1.5 在各种泰语任务上都表现出色，优于其他开源泰语模型。我们还讨论了 GPU 内存要求和部署策略等实际考虑因素。</li>
</ul>

<h3>Title: UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts</h3>
<ul>
<li><strong>Authors: </strong>Bo Yang, Qingping Yang, Runtao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.07240">https://arxiv.org/abs/2411.07240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.07240">https://arxiv.org/pdf/2411.07240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.07240]] UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts(https://arxiv.org/abs/2411.07240)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI). While Large Language Models (LLMs) have shown impressive performance in solving mathematical problems, existing benchmarks such as GSM8K and MATH present limitations, including narrow problem definitions with specific numbers and reliance on predetermined rules that hinder accurate assessments of reasoning and adaptability. This paper introduces the UTMath Benchmark, which robustly evaluates the models through extensive unit tests. It consists of 1,053 problems across 9 mathematical domains, with over 68 test cases per this http URL propose an innovative evaluation framework inspired by unit testing in software development, focusing on both accuracy and reliability of results. Furthermore, we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which encourages LLMs to perform explicit reasoning before generating code, leading to generating more advanced solution and improved performance. Furthermore, we are releasing not only the UTMath benchmark but also the UTMath-Train training dataset (more than 70k samples), to support the community in further exploring mathematical reasoning.</li>
<li><strong>摘要：</strong>数学推理能力的评估对于推进通用人工智能 (AGI) 至关重要。虽然大型语言模型 (LLM) 在解决数学问题方面表现出色，但现有的基准测试（如 GSM8K 和 MATH）存在局限性，包括问题定义狭窄、数字具体以及依赖预定规则，这些都阻碍了对推理和适应性的准确评估。本文介绍了 UTMath 基准测试，该基准测试通过广泛的单元测试对模型进行了稳健的评估。它包含 9 个数学领域的 1,053 个问题，每个 http URL 有超过 68 个测试用例，提出了一个受软件开发单元测试启发的创新评估框架，重点关注结果的准确性和可靠性。此外，我们引入了从推理到思想编码 (RCoT) 方法，该方法鼓励 LLM 在生成代码之前执行显式推理，从而生成更高级的解决方案并提高性能。此外，我们不仅发布了 UTMath 基准测试，还发布了 UTMath-Train 训练数据集（超过 70k 个样本），以支持社区进一步探索数学推理。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
