<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-07</h1>
<h3>Title: Optimizing Humor Generation in Large Language Models: Temperature Configurations and Architectural Trade-offs</h3>
<ul>
<li><strong>Authors: </strong>Evgenii Evstafev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02858">https://arxiv.org/abs/2504.02858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02858">https://arxiv.org/pdf/2504.02858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02858]] Optimizing Humor Generation in Large Language Models: Temperature Configurations and Architectural Trade-offs(https://arxiv.org/abs/2504.02858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate increasing capabilities in creative text generation, yet systematic evaluations of their humor production remain underexplored. This study presents a comprehensive analysis of 13 state-of-the-art LLMs across five architectural families, evaluating their performance in generating technically relevant humor for software developers. Through a full factorial design testing 715 unique configurations of temperature settings and prompt variations, we assess model outputs using five weighted criteria: humor quality, domain relevance, concept originality, tone precision, and delivery efficiency. Our methodology employs rigorous statistical analysis including ANOVA, correlation studies, and quadratic regression to identify optimal configurations and architectural influences. Results reveal significant performance variations across models, with certain architectures achieving 21.8% superiority over baseline systems. Temperature sensitivity analysis demonstrates that 73% of models achieve peak performance at lower stochasticity settings (<= 0.5), though optimal ranges vary substantially by architecture. We identify distinct model clusters: compact high-performers maintaining efficiency-quality balance versus verbose specialists requiring longer outputs for marginal gains. Statistical validation confirms model architecture explains 38.7% of performance variance, with significant correlations between humor quality and concept originality. The study establishes practical guidelines for model selection and configuration, demonstrating how temperature adjustments and architectural considerations impact humor generation effectiveness. These findings advance understanding of LLM capabilities in creative technical writing and provide empirically validated configuration strategies for developers implementing humor-generation systems.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）表明，在创意文本生成中的功能越来越多，但对其幽默产生的系统评估仍未得到充实。这项研究对五个建筑家庭的13个最先进的LLM进行了全面分析，评估了它们在为软件开发人员带来技术相关的幽默方面的表现。通过完整的阶乘设计测试715温度设置和及时变化的独特配置，我们使用五个加权标准评估模型输出：幽默质量，域相关性，概念独创性，音调精度和交付效率。我们的方法采用了严格的统计分析，包括方差分析，相关研究和二次回归，以识别最佳配置和建筑影响。结果揭示了整个模型的显着性能变化，某些架构比基线系统实现了21.8％的优势。温度灵敏度分析表明，有73％的模型在较低的随机性设置下实现峰值性能（<= 0.5），尽管最佳范围范围差异很大。我们确定不同的模型簇：保持效率质量平衡的紧凑型高性能与需要更长的边际收益产出的详细专家。统计验证确认模型体系结构解释了38.7％的性能差异，幽默质量和概念独创性之间存在显着相关性。该研究为模型选择和配置建立了实用指南，证明了温度调整和体系结构考虑如何影响幽默的产生有效性。这些发现提高了对创意技术写作中LLM功能的了解，并为实施幽默生成系统的开发人员提供了经验验证的配置策略。</li>
</ul>

<h3>Title: The Illusionist's Prompt: Exposing the Factual Vulnerabilities of Large Language Models with Linguistic Nuances</h3>
<ul>
<li><strong>Authors: </strong>Yining Wang, Yuquan Wang, Xi Li, Mi Zhang, Geng Hong, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02865">https://arxiv.org/abs/2504.02865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02865">https://arxiv.org/pdf/2504.02865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02865]] The Illusionist's Prompt: Exposing the Factual Vulnerabilities of Large Language Models with Linguistic Nuances(https://arxiv.org/abs/2504.02865)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) continue to advance, they are increasingly relied upon as real-time sources of information by non-expert users. To ensure the factuality of the information they provide, much research has focused on mitigating hallucinations in LLM responses, but only in the context of formal user queries, rather than maliciously crafted ones. In this study, we introduce The Illusionist's Prompt, a novel hallucination attack that incorporates linguistic nuances into adversarial queries, challenging the factual accuracy of LLMs against five types of fact-enhancing strategies. Our attack automatically generates highly transferrable illusory prompts to induce internal factual errors, all while preserving user intent and semantics. Extensive experiments confirm the effectiveness of our attack in compromising black-box LLMs, including commercial APIs like GPT-4o and Gemini-2.0, even with various defensive mechanisms.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLMS）继续发展，非专家用户越来越依赖它们作为实时信息来源。为了确保其提供的信息的事实，许多研究集中在减轻LLM响应中的幻觉上，但仅在正式的用户查询的背景下而不是恶性制作的情况下。在这项研究中，我们介绍了幻觉主义者的提示，这是一种新颖的幻觉攻击，将语言细微差别纳入了对抗性查询中，从而挑战了LLMS对五种事实增强策略的事实准确性。我们的攻击会自动产生高度转移的虚幻提示，以引起内部事实错误，同时保留用户意图和语义。广泛的实验证实了我们攻击在损害黑盒LLM中的有效性，包括GPT-4O和Gemini-2.0等商业API，即使具有各种防御机制。</li>
</ul>

<h3>Title: Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications</h3>
<ul>
<li><strong>Authors: </strong>Hongliu Cao, Ilias Driouich, Robin Singh, Eoin Thomas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02867">https://arxiv.org/abs/2504.02867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02867">https://arxiv.org/pdf/2504.02867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02867]] Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications(https://arxiv.org/abs/2504.02867)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performance across diverse domains, yet they still encounter challenges such as insufficient domain-specific knowledge, biases, and hallucinations. This underscores the need for robust evaluation methodologies to accurately assess LLM-based applications. Traditional evaluation methods, which rely on word overlap or text embeddings, are inadequate for capturing the nuanced semantic information necessary to evaluate dynamic, open-ended text generation. Recent research has explored leveraging LLMs to mimic human reasoning and decision-making processes for evaluation purposes known as LLM-as-a-judge framework. However, these existing frameworks have two significant limitations. First, they lack the flexibility to adapt to different text styles, including various answer and ground truth styles, thereby reducing their generalization performance. Second, the evaluation scores produced by these frameworks are often skewed and hard to interpret, showing a low correlation with human judgment. To address these challenges, we propose a novel dynamic multi-agent system that automatically designs personalized LLM judges for various natural language generation applications. This system iteratively refines evaluation prompts and balances the trade-off between the adaptive requirements of downstream tasks and the alignment with human perception. Our experimental results show that the proposed multi-agent LLM Judge framework not only enhances evaluation accuracy compared to existing methods but also produces evaluation scores that better align with human perception.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在不同领域表现出了令人印象深刻的表现，但它们仍然遇到挑战，例如特定于领域的知识，偏见和幻觉。这强调了对可靠的评估方法的需求，以准确评估基于LLM的应用程序。依靠单词重叠或文本嵌入的传统评估方法不足以捕获评估动态，开放式文本生成所需的细微差别语义信息。最近的研究探索了利用LLM模仿人类推理和决策过程的评估目的，称为LLM-AS-A-A-Gudge框架。但是，这些现有的框架有两个重大局限性。首先，他们缺乏适应不同文本样式的灵活性，包括各种答案和地面真相样式，从而降低了它们的概括性能。其次，这些框架产生的评估得分通常偏向且难以解释，显示出与人类判断的低相关性。为了应对这些挑战，我们提出了一个新型的动态多机构系统，该系统会自动为各种自然语言生成应用设计个性化的LLM法官。该系统迭代地完善评估的提示和平衡了下游任务的适应性要求与与人类感知的一致性之间的权衡。我们的实验结果表明，与现有方法相比，拟议的多代理LLM法官框架不仅提高了评估精度，而且还产生了与人类感知更好一致的评估评分。</li>
</ul>

<h3>Title: AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening</h3>
<ul>
<li><strong>Authors: </strong>Frank P.-W. Lo, Jianing Qiu, Zeyu Wang, Haibao Yu, Yeming Chen, Gao Zhang, Benny Lo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02870">https://arxiv.org/abs/2504.02870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02870">https://arxiv.org/pdf/2504.02870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02870]] AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening(https://arxiv.org/abs/2504.02870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows.</li>
<li><strong>摘要：</strong>恢复筛查是人才获取的关键但时间密集的过程，要求招聘人员分析大量的工作应用程序，同时保持客观，准确和公平。随着大语言模型（LLM）的进步，其推理能力和广泛的知识库展示了简化和自动化招聘工作流程的新机会。在这项工作中，我们提出了一个多代理框架，用于使用LLMS进行系统处理和评估简历的恢复筛查。该框架由四个核心代理组成，包括简历提取器，一个评估者，摘要和分数格式化器。为了增强候选人评估的上下文相关性，我们在简历评估者中整合了检索功能的生成（RAG），允许纳入外部知识来源，例如特定于行业的专业知识，专业认证，大学排名和公司特定的雇用标准。这种动态的适应能够实现个性化的招聘，弥合了AI自动化和人才掌握之间的差距。我们通过将AI生成的分数与人力资源专业人员在匿名在线简历的数据集上进行比较，评估了方法的有效性。这些发现突出了多代理RAG-LLM系统在自动化简历筛选中的潜力，从而实现了更有效，可扩展的招聘工作流程。</li>
</ul>

<h3>Title: Synthesized Annotation Guidelines are Knowledge-Lite Boosters for Clinical Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Enshuo Hsu, Martin Ugbala, Krishna Kumar Kookal, Zouaidi Kawtar, Nicholas L. Rider, Muhammad F. Walji, Kirk Roberts</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02871">https://arxiv.org/abs/2504.02871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02871">https://arxiv.org/pdf/2504.02871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02871]] Synthesized Annotation Guidelines are Knowledge-Lite Boosters for Clinical Information Extraction(https://arxiv.org/abs/2504.02871)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Generative information extraction using large language models, particularly through few-shot learning, has become a popular method. Recent studies indicate that providing a detailed, human-readable guideline-similar to the annotation guidelines traditionally used for training human annotators can significantly improve performance. However, constructing these guidelines is both labor- and knowledge-intensive. Additionally, the definitions are often tailored to meet specific needs, making them highly task-specific and often non-reusable. Handling these subtle differences requires considerable effort and attention to detail. In this study, we propose a self-improving method that harvests the knowledge summarization and text generation capacity of LLMs to synthesize annotation guidelines while requiring virtually no human input. Our zero-shot experiments on the clinical named entity recognition benchmarks, 2012 i2b2 EVENT, 2012 i2b2 TIMEX, 2014 i2b2, and 2018 n2c2 showed 25.86%, 4.36%, 0.20%, and 7.75% improvements in strict F1 scores from the no-guideline baseline. The LLM-synthesized guidelines showed equivalent or better performance compared to human-written guidelines by 1.15% to 4.14% in most tasks. In conclusion, this study proposes a novel LLM self-improving method that requires minimal knowledge and human input and is applicable to multiple biomedical domains.</li>
<li><strong>摘要：</strong>使用大型语言模型的生成信息提取，尤其是通过几次学习，已成为一种流行的方法。最近的研究表明，提供详细的，可读的准则类似于传统训练人类注释者的注释指南可以显着提高性能。但是，构建这些准则既是劳动和知识密集型的。此外，这些定义通常是为满足特定需求而量身定制的，使其高度特定于任务，而且通常不可用。处理这些微妙的差异需要大量的努力和对细节的关注。在这项研究中，我们提出了一种自我改善方法，该方法可以收获LLM的知识摘要和文本生成能力，以合成注释指南，同时几乎不需要人类的投入。我们在临床指定实体识别基准，2012 I2B2事件，2012 I2B2 TimeX，2014 I2B2和2018 N2C2上进行的零射击实验显示25.86％，4.36％，0.20％和7.75％的scores scores scores score score vrom-not-not-not-noce scores scores scores scores scores scores scores scores的提高了25.86％，4.36％，0.20％。在大多数任务中，LLM合成指南与人写的指南相比表现出同等或更好的性能。总之，这项研究提出了一种新型的LLM自我改善方法，该方法需要最少的知识和人类的输入，并且适用于多个生物医学领域。</li>
</ul>

<h3>Title: Short-PHD: Detecting Short LLM-generated Text with Topological Data Analysis After Off-topic Content Insertion</h3>
<ul>
<li><strong>Authors: </strong>Dongjun Wei, Minjia Mao, Xiao Fang, Michael Chau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02873">https://arxiv.org/abs/2504.02873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02873">https://arxiv.org/pdf/2504.02873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02873]] Short-PHD: Detecting Short LLM-generated Text with Topological Data Analysis After Off-topic Content Insertion(https://arxiv.org/abs/2504.02873)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The malicious usage of large language models (LLMs) has motivated the detection of LLM-generated texts. Previous work in topological data analysis shows that the persistent homology dimension (PHD) of text embeddings can serve as a more robust and promising score than other zero-shot methods. However, effectively detecting short LLM-generated texts remains a challenge. This paper presents Short-PHD, a zero-shot LLM-generated text detection method tailored for short texts. Short-PHD stabilizes the estimation of the previous PHD method for short texts by inserting off-topic content before the given input text and identifies LLM-generated text based on an established detection threshold. Experimental results on both public and generated datasets demonstrate that Short-PHD outperforms existing zero-shot methods in short LLM-generated text detection. Implementation codes are available online.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的恶意用法激发了LLM生成的文本的检测。拓扑数据分析中的先前工作表明，与其他零射击方法相比，文本嵌入的持续同源性维度（PHD）可以作为更强和有希望的分数。但是，有效检测简短的LLM生成的文本仍然是一个挑战。本文介绍了短时间的零照片LLM生成的文本检测方法，该方法是针对短文本量身定制的。短phD通过在给定输入文本之前插入非主题内容，并根据已建立的检测阈值识别LLM生成的文本，从而稳定了先前的PHD方法的估计。公共和生成数据集的实验结果表明，在简短的LLM生成的文本检测中，短时间phD的表现优于现有的零摄像方法。实施代码可在线提供。</li>
</ul>

<h3>Title: TheBlueScrubs-v1, a comprehensive curated medical dataset derived from the internet</h3>
<ul>
<li><strong>Authors: </strong>Luis Felipe, Carlos Garcia, Issam El Naqa, Monique Shotande, Aakash Tripathi, Vivek Rudrapatna, Ghulam Rasool, Danielle Bitterman, Gilmer Valdes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02874">https://arxiv.org/abs/2504.02874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02874">https://arxiv.org/pdf/2504.02874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02874]] TheBlueScrubs-v1, a comprehensive curated medical dataset derived from the internet(https://arxiv.org/abs/2504.02874)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The need for robust and diverse data sets to train clinical large language models (cLLMs) is critical given that currently available public repositories often prove too limited in size or scope for comprehensive medical use. While resources like PubMed provide foundational medical literature, they capture only a narrow range of formal publications and omit the broader medical discourse on the internet. To address these deficits, we introduce TheBlueScrubs-v1, a curated dataset of over 25 billion medical tokens - nearly three times larger than PubMed - drawn from a broad-scale internet corpus. Our two-stage filtering pipeline employs a Logistic Regression model for document screening (achieving an AUC of approximately 0.95 on external validation), followed by verification via a 70B-parameter Llama 3.1 instruct model. Each text is assigned three LLM-based quality scores encompassing medical relevance, precision and factual detail, and safety and ethical standards. Clinician reviews confirm high concordance with these automated evaluations, and a specialized cancer classifier further labels approximately 11 billion oncology tokens. Two demonstration tasks highlight the dataset's practical value: first, we distill the safety evaluations to a smaller BERT-style model that reaches an AUC near 0.96 on unseen data; second, we fine-tune a compact LLM on a filtered subset, showing measurable improvements over standard baselines in medical benchmarks as well as private ones. This Data Descriptor details the dataset's creation and validation, underscoring its potential utility for medical AI research.</li>
<li><strong>摘要：</strong>鉴于目前可用的公共存储库通常被证明过于有限或范围，无法进行全面的医疗用途，因此对培训临床大语言模型（CLLM）的强大和多样化数据集的需求至关重要。尽管PubMed等资源提供了基础医学文献，但它们仅捕获了狭窄的正式出版物，并忽略了互联网上更广泛的医学话语。为了解决这些赤字，我们介绍了thebluescrubs -v1，这是一个超过250亿个医疗令牌的数据集（比PubMed大的三倍），是从广泛的互联网语料库中绘制的。我们的两阶段过滤管道采用逻辑回归模型进行文档筛选（在外部验证时达到约0.95的AUC），然后通过70B参数Llama 3.1指示模型进行验证。每个文本都分配了三个基于LLM的质量分数，其中包括医学相关性，精度和事实细节以及安全性和道德标准。临床医生的评论证实了与这些自动化评估的高度一致性，并且专门的癌症分类器进一步标记了约110亿个肿瘤学标记。两个演示任务突出了数据集的实际价值：首先，我们将安全性评估提炼成一个较小的BERT风格模型，该模型在看不见的数据上达到了接近0.96的AUC；其次，我们在过滤的子集上微调了紧凑型LLM，显示了对医疗基准和私人基准中标准基准的可测量改进。该数据描述符详细介绍了数据集的创建和验证，并强调了其在医疗AI研究中的潜在实用性。</li>
</ul>

<h3>Title: Revisiting Funnel Transformers for Modern LLM Architectures with Comprehensive Ablations in Training and Inference Configurations</h3>
<ul>
<li><strong>Authors: </strong>DongHyun Choi, Lucas Spangher, Chris Hidey, Peter Grabowski, Ramy Eskander</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02877">https://arxiv.org/abs/2504.02877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02877">https://arxiv.org/pdf/2504.02877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02877]] Revisiting Funnel Transformers for Modern LLM Architectures with Comprehensive Ablations in Training and Inference Configurations(https://arxiv.org/abs/2504.02877)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Transformer-based Large Language Models, which suffer from high computational costs, advance so quickly that techniques proposed to streamline earlier iterations are not guaranteed to benefit more modern models. Building upon the Funnel Transformer proposed by Dai and Le (2020), which progressively compresses intermediate representations, we investigate the impact of funneling in contemporary Gemma2 Transformer architectures. We systematically evaluate various funnel configurations and recovery methods, comparing: (1) standard pretraining to funnel-aware pretraining strategies, (2) the impact of funnel-aware fine-tuning, and (3) the type of sequence recovery operation. Our results demonstrate that funneling creates information bottlenecks that propagate through deeper network layers, particularly in larger models (e.g., Gemma 7B), leading to at times unmanageable performance lost. However, carefully selecting the funneling layer and employing effective recovery strategies, can substantially mitigate performance losses, achieving up to a 44\% reduction in latency. Our findings highlight key trade-offs between computational efficiency and model accuracy, providing practical guidance for deploying funnel-based approaches in large-scale natural language applications.</li>
<li><strong>摘要：</strong>基于变形金刚的大型语言模型遭受了高计算成本的损失，因此无法确保提议简化较早迭代的技术会受益于更现代的模型。在Dai和Le（2020）提出的漏斗变压器的基础上，该漏斗逐渐压缩了中间表示，我们研究了漏斗在当代Gemma2变形金刚结构中的影响。我们系统地评估了各种漏斗配置和恢复方法，比较：（1）标准预处理与漏斗意识的预处理策略，（2）漏斗感知的微调的影响，以及（3）序列恢复操作的类型。我们的结果表明，漏斗会创建信息瓶颈，这些信息通过更深的网络层传播，尤其是在较大的模型（例如Gemma 7b）中，导致有时无法控制的性能丢失。但是，仔细选择漏斗层并采用有效的恢复策略，可以大大减轻绩效损失，从而减少44％的潜伏期。我们的发现突出了计算效率和模型准确性之间的关键权衡，为在大型自然语言应用中部署基于漏斗的方法提供了实用的指导。</li>
</ul>

<h3>Title: Better Bill GPT: Comparing Large Language Models against Legal Invoice Reviewers</h3>
<ul>
<li><strong>Authors: </strong>Nick Whitehouse, Nicole Lincoln, Stephanie Yiu, Lizzie Catterson, Rivindu Perera</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02881">https://arxiv.org/abs/2504.02881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02881">https://arxiv.org/pdf/2504.02881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02881]] Better Bill GPT: Comparing Large Language Models against Legal Invoice Reviewers(https://arxiv.org/abs/2504.02881)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Legal invoice review is a costly, inconsistent, and time-consuming process, traditionally performed by Legal Operations, Lawyers or Billing Specialists who scrutinise billing compliance line by line. This study presents the first empirical comparison of Large Language Models (LLMs) against human invoice reviewers - Early-Career Lawyers, Experienced Lawyers, and Legal Operations Professionals-assessing their accuracy, speed, and cost-effectiveness. Benchmarking state-of-the-art LLMs against a ground truth set by expert legal professionals, our empirically substantiated findings reveal that LLMs decisively outperform humans across every metric. In invoice approval decisions, LLMs achieve up to 92% accuracy, surpassing the 72% ceiling set by experienced lawyers. On a granular level, LLMs dominate line-item classification, with top models reaching F-scores of 81%, compared to just 43% for the best-performing human group. Speed comparisons are even more striking - while lawyers take 194 to 316 seconds per invoice, LLMs are capable of completing reviews in as fast as 3.6 seconds. And cost? AI slashes review expenses by 99.97%, reducing invoice processing costs from an average of $4.27 per invoice for human invoice reviewers to mere cents. These results highlight the evolving role of AI in legal spend management. As law firms and corporate legal departments struggle with inefficiencies, this study signals a seismic shift: The era of LLM-powered legal spend management is not on the horizon, it has arrived. The challenge ahead is not whether AI can perform as well as human reviewers, but how legal teams will strategically incorporate it, balancing automation with human discretion.</li>
<li><strong>摘要：</strong>法律发票审查是一项昂贵，不一致且耗时的过程，传统上是由法律运营，律师或计费专家进行的，他们仔细检查按线计费合规性。这项研究介绍了大语模型（LLM）与人类发票审稿人的首次经验比较 - 早期职业律师，经验丰富的律师和法律运营专业人士，并评估其准确性，速度和成本效益。我们的经验证实的发现，根据专家法律专业人士设定的基本真理进行基准测试，表明LLM果断地超过了每个指标的人类。在发票批准的决定中，LLMS的准确性高达92％，超过了经验丰富的律师设定的72％上限。在颗粒状水平上，LLM占主导地位的行分类，顶级模型达到81％，而表现最佳的人类群体仅为43％。速度比较甚至更为惊人 - 虽然律师每张发票需要194至316秒，但LLMS能够在3.6秒内完成评论。费用？ AI将审查费用削减了99.97％，将发票处理成本从平均每张发票4.27美元降低到人类发票审稿人的平均$ 4.27。这些结果突出了AI在法律支出管理中的不断发展的作用。随着律师事务所和公司法律部门与效率低下的斗争，这项研究标志着地震转变：LLM驱动的法律支出管理的时代没有到来，它已经到来了。面临的挑战不是AI是否可以像人类审稿人一样执行，而是法律团队将如何战略性地纳入自动化与人类自由裁量权之间。</li>
</ul>

<h3>Title: DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sunghee Jung, Donghun Lee, Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Junrae Cho, Kihyun Kim, Eunggyun Kim, Myeongcheol Shin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02882">https://arxiv.org/abs/2504.02882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02882">https://arxiv.org/pdf/2504.02882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02882]] DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models(https://arxiv.org/abs/2504.02882)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.</li>
<li><strong>摘要：</strong>工具增强的Larage语言模型（TA-LLMS）已在现实世界应用中显示出希望，但是在处理不完整的查询和越来越多的请求时面临挑战。尽管现有方法主要依赖于专家轨迹的监督微调，但我们提出了Diatool-DPO，这是一种新颖的方法，可以通过直接偏好优化来增强TA-LLM的对话能力。我们将TA-LLM交互作用模型为Markov决策过程，该过程具有5个不同的对话状态，并根据其状态过渡轨迹将用户查询分为3种类型。我们会自动构建正确和不正确的对话流的配对轨迹数据集，并引入对话控制的专门客观损失。我们的全面评估表明，Diatool-DPO的方法是GPT-4O的绩效（在信息收集中为94.8％，工具呼叫拒绝的91％），比基线进行了实质性改进（分别为44％和9.6％），同时保持核心功能。我们的方法为开发可以处理各种现实世界情景的ta-llms开辟了新的可能性，而无需其他专家演示或人类标签。</li>
</ul>

<h3>Title: SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02883">https://arxiv.org/abs/2504.02883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02883">https://arxiv.org/pdf/2504.02883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02883]] SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models(https://arxiv.org/abs/2504.02883)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce SemEval-2025 Task 4: unlearning sensitive content from Large Language Models (LLMs). The task features 3 subtasks for LLM unlearning spanning different use cases: (1) unlearn long form synthetic creative documents spanning different genres; (2) unlearn short form synthetic biographies containing personally identifiable information (PII), including fake names, phone number, SSN, email and home addresses, and (3) unlearn real documents sampled from the target model's training dataset. We received over 100 submissions from over 30 institutions and we summarize the key techniques and lessons in this paper.</li>
<li><strong>摘要：</strong>我们介绍了Semeval-2025任务4：来自大语言模型（LLMS）的敏感内容。该任务具有3个针对LLM学习的子任务，涵盖了不同的用例：（1）跨越不同类型的综合形式的综合创意文档； （2）未读书的简短形式的合成传记，其中包含个人身份信息（PII），包括伪造名称，电话号码，SSN，电子邮件和家庭地址，以及（3）从目标模型的培训数据集中采样的未了解的真实文档。我们收到了来自30多个机构的100多种提交，并总结了本文的关键技术和课程。</li>
</ul>

<h3>Title: LVMed-R2: Perception and Reflection-driven Complex Reasoning for Medical Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Shuchang Ye, Jinghao Lin, Usman Naseem, Jinman Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02885">https://arxiv.org/abs/2504.02885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02885">https://arxiv.org/pdf/2504.02885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02885]] LVMed-R2: Perception and Reflection-driven Complex Reasoning for Medical Report Generation(https://arxiv.org/abs/2504.02885)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVMs) hold a great promise for automating medical report generation, potentially reducing the burden of manual reporting. State-of-the-art (SOTA) research fine-tunes general LVMs with medical data to align radiology images to corresponding medical reports. However, there are two key factors that limit these LVM's performance. Firstly, LVMs lack complex reasoning capability that leads to logical inconsistencies and potential diagnostic errors in generated reports. Secondly, LVMs lack reflection mechanism that leads to an inability to discover errors in the thinking process. To address these gaps, we propose LVMed-R2, a new fine-tuning strategy that introduces complex reasoning and reflection mechanisms for LVMs to enhance medical report generation. To the best of our knowledge, this is the first work to introduce complex reasoning to the medical report generation (MRG) task. Our proposed complex reasoning contains medical knowledge injection and perception-enhancing modules which improve the accuracy of LVMs diagnosis, coupled with a perception tree to provide guidance to limit the perception range. Further, the reflection mechanism forces self-verification for outputs to correct for potential errors. We experimented by fine-tuning LVMs with our proposed LVMed-R2 strategy, using IU-Xray and MIMIC-CXR datasets. Our results, measured on natural language generation (NLG) metrics and clinical efficacy (CE) metrics, demonstrate that LVMs fine-tuned with the proposed reflection mechanism possess the ability to correct outputs and complex reasoning effectively and improve LVMs performance for MRG.</li>
<li><strong>摘要：</strong>大型视觉模型（LVM）对自动化医学报告的生成有一个巨大的希望，可能会减轻手动报告的负担。最先进的（SOTA）研究通过医学数据进行微型LVM，以使放射学图像与相应的医学报告保持一致。但是，有两个关键因素限制了这些LVM的性能。首先，LVM缺乏复杂的推理能力，从而导致逻辑上的不一致和生成报告中的潜在诊断错误。其次，LVM缺乏反射机制，从而导致无法在思维过程中发现错误。为了解决这些差距，我们提出了LVMED-R2，这是一种新的微调策略，介绍了LVM的复杂推理和反思机制，以增强医疗报告的生成。据我们所知，这是第一项将复杂推理引入医疗报告（MRG）任务的工作。我们提出的复杂推理包含医学知识注入和感知增强模块，可提高LVMS诊断的准确性，并与感知树相结合，以提供指导以限制感知范围。此外，反射机制迫使输出自我验证以纠正潜在误差。我们使用IU-XRAR和MIMIC-CXR数据集对我们提出的LVMED-R2策略进行了微调LVM的实验。我们的结果是根据自然语言产生（NLG）指标和临床功效（CE）指标衡量的，表明，使用拟议的反射机制对LVM进行了微调，具有有效地纠正输出和复杂推理并改善MRG的LVMS性能的能力。</li>
</ul>

<h3>Title: Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding of Online Discourse Datasets</h3>
<ul>
<li><strong>Authors: </strong>John Chen, Alexandros Lotsos, Grace Wang, Lexie Zhao, Bruce Sherin, Uri Wilensky, Michael Horn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02887">https://arxiv.org/abs/2504.02887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02887">https://arxiv.org/pdf/2504.02887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02887]] Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding of Online Discourse Datasets(https://arxiv.org/abs/2504.02887)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Open coding, a key inductive step in qualitative research, discovers and constructs concepts from human datasets. However, capturing extensive and nuanced aspects or "coding moments" can be challenging, especially with large discourse datasets. While some studies explore machine learning (ML)/Generative AI (GAI)'s potential for open coding, few evaluation studies exist. We compare open coding results by five recently published ML/GAI approaches and four human coders, using a dataset of online chat messages around a mobile learning software. Our systematic analysis reveals ML/GAI approaches' strengths and weaknesses, uncovering the complementary potential between humans and AI. Line-by-line AI approaches effectively identify content-based codes, while humans excel in interpreting conversational dynamics. We discussed how embedded analytical processes could shape the results of ML/GAI approaches. Instead of replacing humans in open coding, researchers should integrate AI with and according to their analytical processes, e.g., as parallel co-coders.</li>
<li><strong>摘要：</strong>开放编码是定性研究的关键归纳步骤，它从人类数据集发现并构建了概念。但是，捕获广泛而细微的方面或“编码时刻”可能具有挑战性，尤其是在大型话语数据集的情况下。尽管一些研究探索机器学习（ML）/生成AI（GAI）的开放编码潜力，但很少有评估研究。我们使用围绕移动学习软件的在线聊天消息数据集比较了五个最近发布的ML/GAI方法和四个人类编码器的开放编码结果。我们的系统分析揭示了ML/GAI方法的优势和劣势，从而发现了人类与AI之间的互补潜力。逐线AI方法有效地识别基于内容的代码，而人类在解释对话动态方面表现出色。我们讨论了嵌入式分析过程如何塑造ML/GAI方法的结果。研究人员不应在开放编码中取代人类，而应与他们的分析过程（例如平行的共同编码器）相结合并根据其分析过程进行整合。</li>
</ul>

<h3>Title: A Status Quo Investigation of Large Language Models towards Cost-Effective CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek</h3>
<ul>
<li><strong>Authors: </strong>Wenkang Wang, Ran Xu, Jingsen Feng, Qingfu Zhang, Xu Chu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02888">https://arxiv.org/abs/2504.02888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02888">https://arxiv.org/pdf/2504.02888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02888]] A Status Quo Investigation of Large Language Models towards Cost-Effective CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek(https://arxiv.org/abs/2504.02888)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>We evaluated the performance of OpenFOAMGPT incorporating multiple large-language models. Some of the present models efficiently manage different CFD tasks such as adjusting boundary conditions, turbulence models, and solver configurations, although their token cost and stability vary. Locally deployed smaller models like QwQ-32B struggled with generating valid solver files for complex processes. Zero-shot prompting commonly failed in simulations with intricate settings, even for large models. Challenges with boundary conditions and solver keywords stress the requirement for expert supervision, indicating that further development is needed to fully automate specialized CFD simulations.</li>
<li><strong>摘要：</strong>我们评估了合并多个大语模型的OpenFoAmgpt的性能。当前的某些模型有效地管理不同的CFD任务，例如调整边界条件，湍流模型和求解器配置，尽管它们的代币成本和稳定性各不相同。本地部署的较小型号（例如QWQ-32B）努力为复杂过程生成有效的求解器文件。零射击促使通常在具有复杂设置的模拟中，即使对于大型模型也是如此。边界条件和求解器关键字的挑战强调了专家监督的要求，表明需要进一步开发才能完全自动化专业的CFD模拟。</li>
</ul>

<h3>Title: Scaling Test-time Compute for Low-resource Languages: Multilingual Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02890">https://arxiv.org/abs/2504.02890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02890">https://arxiv.org/pdf/2504.02890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02890]] Scaling Test-time Compute for Low-resource Languages: Multilingual Reasoning in LLMs(https://arxiv.org/abs/2504.02890)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent advances in test-time compute scaling have enabled Large Language Models (LLMs) to tackle deep reasoning tasks by generating a chain-of-thought (CoT) that includes trial and error, backtracking, and intermediate reasoning steps before producing the final answer. However, these techniques have been applied predominantly to popular languages, such as English, leaving reasoning in low-resource languages underexplored and misaligned. In this work, we investigate the multilingual mechanism by which LLMs internally operate in a latent space biased toward their inherently dominant language. To leverage this phenomenon for low-resource languages, we train models to generate the CoT in English while outputting the final response in the target language, given input in the low-resource language. Our experiments demonstrate that this approach, named English-Pivoted CoT Training, outperforms other baselines, including training to generate both the CoT and the final response solely in the target language, with up to 28.33% improvement. Further analysis provides novel insights into the relationships between reasoning and multilinguality of LLMs, prompting for better approaches in developing multilingual large reasoning models</li>
<li><strong>摘要：</strong>测试时间计算缩放的最新进展使大型语言模型（LLMS）通过生成包括试验和错误，回溯和中间推理步骤的经验链（COT）来解决深层推理任务，然后再产生最终答案。但是，这些技术主要应用于流行语言，例如英语，以低资源语言的推理却没有被忽视和对齐。在这项工作中，我们研究了LLM在内部在潜在空间中运行的多语言机制，该空间偏向于其固有的主导语言。为了利用这种现象来使用低资源语言，我们训练模型以英语生成COT，同时输出目标语言的最终响应，并以低资源语言给出输入。我们的实验表明，这种方法称为英语居民的COT培训，优于其他基准，包括仅以目标语言来产生COT和最终响应的训练，提高了28.33％。进一步的分析提供了对LLM的推理与多语言之间关系的新见解，促使在开发多语言大型推理模型方面采取更好的方法</li>
</ul>

<h3>Title: Automated Survey Collection with LLM-based Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Kurmanbek Kaiyrbekov, Nicholas J Dobbins, Sean D Mooney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02891">https://arxiv.org/abs/2504.02891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02891">https://arxiv.org/pdf/2504.02891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02891]] Automated Survey Collection with LLM-based Conversational Agents(https://arxiv.org/abs/2504.02891)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs). Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience. Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction. Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.</li>
<li><strong>摘要：</strong>目的：传统的基于电话的调查是收集生物医学和医疗保健数据最容易访问和广泛使用的方法之一，但是，它们通常是昂贵的，劳动密集型且难以有效扩展的。为了克服这些限制，我们提出了一个由对话式大语模型（LLM）驱动的端到端调查收集框架。材料和方法：我们的框架由负责设计调查和招募参与者的研究人员，由LLM供电的对话式电话代理，呼吁参与者并管理调查，第二个LLM（GPT-4O）分析了在调查过程中产生的对话成绩单，以及用于存储和组织结果的数据库。为了测试我们的框架，我们招募了8位由5名本地和3位非母语的人组成的参与者，并进行了40次调查。我们评估了LLM生成的对话笔录的正确性，GPT-4O推断的调查响应的准确性以及整体参与者的经验。结果：尽管成绩单的平均每条单词错误率为7.7％，但GPT-4O从对话转录本中成功提取了调查响应，平均准确性为98％。尽管参与者指出了对话性LLM代理商偶尔出现的错误，但他们报告说，该代理商有效地传达了调查的目的，证明了良好的理解并保持了引人入胜的互动。结论：我们的研究强调了LLM代理在进行和分析医疗保健应用的电话调查中的潜力。通过减少人类面试官的工作量并提供可扩展的解决方案，这种方法为现实世界中，端到端AI驱动的电话调查收集系统铺平了道路。</li>
</ul>

<h3>Title: OnRL-RAG: Real-Time Personalized Mental Health Dialogue System</h3>
<ul>
<li><strong>Authors: </strong>Ahsan Bilal, Beiyu Lin, Mehdi Zaeifi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02894">https://arxiv.org/abs/2504.02894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02894">https://arxiv.org/pdf/2504.02894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02894]] OnRL-RAG: Real-Time Personalized Mental Health Dialogue System(https://arxiv.org/abs/2504.02894)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已被广泛用于各种任务和应用程序。但是，LLM和微调仅限于预训练的数据。例如，直到2021年，Chatgpt的世界知识可能已经过时或不准确。为了增强LLM的功能，提议提高检索功能的生成（RAG），以增强LLMS，并提供其他新的，最新的详细信息和信息向LLMS提供。尽管RAG提供了正确的信息，但它可能无法最好地呈现，尤其是对于具有个性化的不同人群群体。从人类反馈（RLHF）中学习通过反馈循环将模型响应与人类偏好调整来适应用户需求。在现实生活中的应用程序（例如心理健康问题）中，基于动态和反馈的模型将不断适应新信息，并由于每日环境中波动的复杂因素而提供个性化的援助。因此，我们提出了一个基于在线学习的基于学习的检索增强生成（ONRL-rag）系统，以检测和个性化响应系统，以应对心理健康问题，例如压力，焦虑和抑郁。我们使用来自2028个大学生收集的开源数据集，每个学生有28个调查问题，以证明我们建议的系统与现有系统的性能。与标准抹布和简单的LLM相比，我们的系统通过GPT-4O，GPT-4O-MINI，GEMINI-1.5和GPT-3.5实现了卓越的性能。这项工作将开辟LLMS在日常环境中为个性化服务的现实生活应用的可能性。结果还将帮助社会学，心理学和神经科学领域的研究人员更加与实际的人类日常环境保持一致。</li>
</ul>

<h3>Title: A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content</h3>
<ul>
<li><strong>Authors: </strong>Lele Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02898">https://arxiv.org/abs/2504.02898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02898">https://arxiv.org/pdf/2504.02898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02898]] A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content(https://arxiv.org/abs/2504.02898)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.</li>
<li><strong>摘要：</strong>AI生成的内容的进步导致广泛采用了大型语言模型，基于扩散的视觉发生器和合成音频工具。但是，这些事态发展引起了人们对错误信息，侵犯版权，安全威胁和公共信任的侵蚀的关键关注。在本文中，我们探讨了旨在检测​​和减轻AI生成的文本，视觉和音频内容的广泛方法。我们首先讨论与基于AI的内容产生相关的动机和潜在影响，包括现实世界的风险和道德困境。然后，我们概述了涵盖基于观察的策略，语言和统计分析，基于模型的管道，水印和指纹以及新兴的集合接近的检测技术。我们还介绍了有关鲁棒性，适应快速改善生成架构的新观点以及人类在循环验证的关键作用。通过调查最先进的研究并强调学术，新闻，法律和工业环境中的案例研究，本文旨在为强有力的解决方案和决策提供信息。最后，我们讨论了公开挑战，包括对抗性转型，领域的概括和道德问题，从而为研究人员，从业人员和监管者提供了整体指南，以保持内容真实性，面对日益成熟的AI生成的媒体。</li>
</ul>

<h3>Title: Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liangjie Huang, Dawei Li, Huan Liu, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02902">https://arxiv.org/abs/2504.02902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02902">https://arxiv.org/pdf/2504.02902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02902]] Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models(https://arxiv.org/abs/2504.02902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）表现出了显着的自我完善能力，可以通过自我生成的反馈进行迭代修改其输出。尽管这种反思性机制在提高任务绩效方面已显示出希望，但最近的研究表明，它也可能引入不良偏见，值得注意的是自偏见或LLMS倾向于偏爱自己先前的输出的趋势。在这项工作中，我们通过调查对置信度估计的影响来扩展此询问线。我们评估了三个代表性的自我改进范式提示，经过思考链（COT）提示和基于调整的方法，并发现迭代性自我改善会导致系统性过度控制，这证明了预期的预期校准误差（ECE）稳定增长，并且具有较高的准确性。然后，我们进一步探讨了置信校准技术与自我完善的整合。具体而言，我们比较了三种策略：（1）在自我完善多轮后进行校准，（2）在自我完善前进行校准，以及（3）在每个自我完善步骤中迭代应用校准。我们的结果表明，迭代校准最有效地减少ECE，从而改善校准。我们的工作从校准的角度开创了对自我改善LLM的研究，从而为平衡模型性能和可靠性提供了宝贵的见解。</li>
</ul>

<h3>Title: How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</h3>
<ul>
<li><strong>Authors: </strong>Hongzhe Du, Weikai Li, Min Cai, Karim Saraipour, Zimin Zhang, Himabindu Lakkaraju, Yizhou Sun, Shichang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02904">https://arxiv.org/abs/2504.02904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02904">https://arxiv.org/pdf/2504.02904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02904]] How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence(https://arxiv.org/abs/2504.02904)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by linear vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training.</li>
<li><strong>摘要：</strong>训练后对于大型语言模型（LLM）的成功至关重要，将预训练的基本模型转化为更有用的，训练的后模型。尽管大量作品研究了训练后算法并通过其输出评估了训练后模型，但它仍然研究了培训后如何在内部进行培训。在本文中，我们从四个角度将基础和训练后的LLMS进行了比较，以更好地了解培训后效果。我们跨模型家庭和数据集的发现表明：（1）培训后培训不会改变事实知识存储位置，并且在开发新知识表示的同时，可以调整基本模型的知识表示； （2）在隐藏表示空间中的线性向量可以表示真实和拒绝。基础模型和训练后模型之间的真实方向高度相似，并且可以有效地转移干预措施。 （3）拒绝方向在基础模型和训练后模型之间有所不同，并且显示出有限的前进性转移性； （4）基础模型和训练后模型之间的置信度差异不能归因于熵神经元。我们的研究提供了有关在训练后保留和改变的基本机制的见解，促进了诸如模型转向之类的下游任务，并有可能使未来在解释性和LLM训练后培训方面有益于未来的研究。</li>
</ul>

<h3>Title: Enhancing Chart-to-Code Generation in Multimodal Large Language Models via Iterative Dual Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhihan Zhang, Yixin Cao, Lizi Liao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02906">https://arxiv.org/abs/2504.02906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02906">https://arxiv.org/pdf/2504.02906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02906]] Enhancing Chart-to-Code Generation in Multimodal Large Language Models via Iterative Dual Preference Learning(https://arxiv.org/abs/2504.02906)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Chart-to-code generation, the process of converting chart images into executable plotting scripts, provides a lossless representation of chart information, requiring models to accurately capture and summarize all visual and structural elements. However, this remains a significant challenge for multimodal large language models (MLLMs), which are not inherently well-aligned with code generation tasks. To bridge this gap, we introduce Chart2Code, a novel iterative dual preference learning framework designed to enhance MLLMs' chart-to-code generation capabilities through structured code variant generation and fine-grained dual reward signals. We validate Chart2Code across three MLLMs and find that iterative preference learning consistently improves out-of-distribution chart-to-code generation quality. Throughout this process, our dual scoring method, which evaluates both the textual code structure and its visual representation, leads to greater performance improvements, even with a reduced preference dataset size. Further analysis explores the key components of our framework and highlights the interplay between chart-to-code generation and broader chart reasoning, paving the way for future advancements in chart comprehension.</li>
<li><strong>摘要：</strong>图表到代码生成，将图表图像转换为可执行的绘图脚本的过程提供了图表信息的无损表示，需要模型准确捕获和总结所有视觉和结构元素。但是，对于多模式大语言模型（MLLM）而言，这仍然是一个重大挑战，这些模型本质上与代码生成任务固有良好结合。为了弥合这一差距，我们介绍了Chart2Code，这是一个新型的迭代双重偏好学习框架，旨在通过结构化代码变体生成和细粒的双奖励信号来增强MLLMS的图表到代码生成功能。我们验证了跨三个MLLM的Chart2代码，发现迭代偏好学习始终提高分布外的图表到代码的生成质量。在整个过程中，我们的双评分方法评估了文本代码结构及其视觉表示，即使偏好数据集大小降低，也会导致更大的性能改进。进一步的分析探讨了我们框架的关键组成部分，并突出了图表到编码生成与更广泛的图表推理之间的相互作用，为未来的图表理解铺平了道路。</li>
</ul>

<h3>Title: Noiser: Bounded Input Perturbations for Attributing Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Reza Ghasemi Madani, Aryo Pradipta Gema, Gabriele Sarti, Yu Zhao, Pasquale Minervini, Andrea Passerini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02911">https://arxiv.org/abs/2504.02911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02911">https://arxiv.org/pdf/2504.02911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02911]] Noiser: Bounded Input Perturbations for Attributing Large Language Models(https://arxiv.org/abs/2504.02911)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Feature attribution (FA) methods are common post-hoc approaches that explain how Large Language Models (LLMs) make predictions. Accordingly, generating faithful attributions that reflect the actual inner behavior of the model is crucial. In this paper, we introduce Noiser, a perturbation-based FA method that imposes bounded noise on each input embedding and measures the robustness of the model against partially noised input to obtain the input attributions. Additionally, we propose an answerability metric that employs an instructed judge model to assess the extent to which highly scored tokens suffice to recover the predicted output. Through a comprehensive evaluation across six LLMs and three tasks, we demonstrate that Noiser consistently outperforms existing gradient-based, attention-based, and perturbation-based FA methods in terms of both faithfulness and answerability, making it a robust and effective approach for explaining language model predictions.</li>
<li><strong>摘要：</strong>特征归因（FA）方法是简单的事后方法，可以解释大型语言模型（LLMS）做出预测。因此，产生反映模型实际内在行为的忠实归因至关重要。在本文中，我们引入了Noiser，这是一种基于扰动的FA方法，该方法在每个输入嵌入式上施加有界的噪声，并测量模型对部分液体输入的鲁棒性，以获得输入属性。此外，我们提出了一个回答性指标，该指标采用指示法官模型来评估高度评分的代币足以恢复预测的产出的程度。通过对六个LLM和三个任务进行的全面评估，我们证明，噪声始终优于现有的基于梯度的，基于注意力的基于注意力和基于扰动的FA方法，从忠诚和答复性方面，这是一种强大而有效的方法来解释语言模型预测。</li>
</ul>

<h3>Title: Bias in Large Language Models Across Clinical Applications: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Thanathip Suenghataiphorn, Narisara Tribuddharat, Pojsakorn Danpanichkul, Narathorn Kulthamrongsri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02917">https://arxiv.org/abs/2504.02917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02917">https://arxiv.org/pdf/2504.02917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02917]] Bias in Large Language Models Across Clinical Applications: A Systematic Review(https://arxiv.org/abs/2504.02917)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Background: Large language models (LLMs) are rapidly being integrated into healthcare, promising to enhance various clinical tasks. However, concerns exist regarding their potential for bias, which could compromise patient care and exacerbate health inequities. This systematic review investigates the prevalence, sources, manifestations, and clinical implications of bias in LLMs. Methods: We conducted a systematic search of PubMed, OVID, and EMBASE from database inception through 2025, for studies evaluating bias in LLMs applied to clinical tasks. We extracted data on LLM type, bias source, bias manifestation, affected attributes, clinical task, evaluation methods, and outcomes. Risk of bias was assessed using a modified ROBINS-I tool. Results: Thirty-eight studies met inclusion criteria, revealing pervasive bias across various LLMs and clinical applications. Both data-related bias (from biased training data) and model-related bias (from model training) were significant contributors. Biases manifested as: allocative harm (e.g., differential treatment recommendations); representational harm (e.g., stereotypical associations, biased image generation); and performance disparities (e.g., variable output quality). These biases affected multiple attributes, most frequently race/ethnicity and gender, but also age, disability, and language. Conclusions: Bias in clinical LLMs is a pervasive and systemic issue, with a potential to lead to misdiagnosis and inappropriate treatment, particularly for marginalized patient populations. Rigorous evaluation of the model is crucial. Furthermore, the development and implementation of effective mitigation strategies, coupled with continuous monitoring in real-world clinical settings, are essential to ensure the safe, equitable, and trustworthy deployment of LLMs in healthcare.</li>
<li><strong>摘要：</strong>背景：大型语言模型（LLM）迅速纳入医疗保健，有望增强各种临床任务。但是，人们对它们偏见的潜力存在担忧，这可能会损害患者护理并加剧健康不平等。该系统评价调查了LLMS偏见的流行，来源，表现和临床意义。方法：我们从数据库成立到2025年对PubMed，Ovid和Embase进行了系统的搜索，用于评估应用于临床任务的LLMS中的偏差。我们提取了有关LLM类型，偏置源，偏差表现，影响属性，临床任务，评估方法和结果的数据。使用修改后的Robins-I工具评估了偏差的风险。结果：38项研究符合纳入标准，揭示了各种LLM和临床应用的普遍偏见。数据相关的偏见（来自偏见的培训数据）和与模型相关的偏见（来自模型培训）都是重要的贡献者。偏见表现为：分配危害（例如，差异治疗建议）；代表性危害（例如，刻板印象关联，偏见的图像产生）；和性能差异（例如，可变输出质量）。这些偏见影响了多个属性，最经常种族/种族和性别，但同时也是年龄，残疾和语言。结论：临床LLMS的偏见是一个普遍存在的系统问题，有可能导致误诊和不适当的治疗，特别是对于边缘化的患者人群。对模型的严格评估至关重要。此外，开发和实施有效的缓解策略，再加上在现实世界中的临床环境中的持续监测，对于确保LLMS在医疗保健中的安全，公平和值得信赖的部署至关重要。</li>
</ul>

<h3>Title: HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse</h3>
<ul>
<li><strong>Authors: </strong>Yuwei An, Yihua Cheng, Seo Jin Park, Junchen Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02921">https://arxiv.org/abs/2504.02921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02921">https://arxiv.org/pdf/2504.02921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02921]] HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse(https://arxiv.org/abs/2504.02921)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the performance of large language models (LLMs) by integrating external knowledge into the generation process. A key component of RAG pipelines is the reranker, which selects the most relevant documents from a pool of retrieved candidates and significantly improves the quality of the generated responses. While rerankers refine the selection of retrieved documents in RAG pipelines, they introduce computational challenges that hinder high throughput and low latency. To address this problem, we propose HyperRAG, a system that optimizes the trade-off between quality and efficiency in RAG pipelines by leveraging KV-cache reuse for efficient reranker inference. By reusing document-side KV-cache, HyperRAG achieves both high-quality generation and system-level efficiency. To fully realize the benefits of KV-cache reuse, HyperRAG incorporates a range of system-level optimizations designed to enhance efficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3 throughput improvement with decoder-only rerankers while also delivering higher downstream performance compared with traditional RAG service.</li>
<li><strong>摘要：</strong>通过将外部知识整合到生成过程中，检索授课的生成（RAG）已成为一种强大的范式，用于增强大语言模型（LLM）的性能。 RAG管道的一个关键组成部分是Reranker，它从检索到的候选人池中选择最相关的文档，并显着提高了生成的响应的质量。当Rerankers完善了在RAG管道中检索到的文档的选择，但它们引入了计算挑战，从而阻碍了高吞吐量和低潜伏期。为了解决这个问题，我们提出了HyperRag，该系统通过利用KV-CACHE REUSE进行有效的Reranker推断来优化RAG管道中质量和效率之间的权衡。通过重复文档侧KV-CACHE，HyperRag可以达到高质量的生成和系统级效率。为了充分实现KV-CACHE重用的好处，HyperRag结合了一系列旨在提高效率和可扩展性的系统级优化。实验表明，HyperRag通过仅解码器的重读者实现2-3个吞吐量的改进，同时与传统的抹布服务相比，下游性能也更高。</li>
</ul>

<h3>Title: Cultural Learning-Based Culture Adaptation of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Cecilia Liu, Anna Korhonen, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02953">https://arxiv.org/abs/2504.02953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02953">https://arxiv.org/pdf/2504.02953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02953]] Cultural Learning-Based Culture Adaptation of Language Models(https://arxiv.org/abs/2504.02953)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially causing harm to others. In this paper, we present CLCA, a novel framework for enhancing LLM alignment with cultural values based on cultural learning. The framework leverages simulated social interactions to generate conversations in which LLMs engage in role-playing within culturally adapted social scenarios, capturing implicit cultural norms for model fine-tuning. CLCA improves cultural value alignment across various model architectures measured using World Value Survey data, demonstrating the effectiveness of our proposed approach. Our results provide early evidence that understanding intent and social interactions can enhance cultural value adaptation in LLMs, highlighting the promise of training approaches based on cultural learning.</li>
<li><strong>摘要：</strong>将大型语言模型（LLM）调整为多种文化价值是一项艰巨的任务，因为现有的LLM默认情况下通常反映了特定群体的价值，并可能对他人造成伤害。在本文中，我们提出了CLCA，这是一个新的框架，用于增强基于文化学习的文化价值的LLM对齐。该框架利用模拟的社交互动来产生对话，在这种对话中，LLMS在文化适应的社会场景中从事角色扮演，捕获了模型微调的隐性文化规范。 CLCA改善了使用世界价值调查数据测量的各种模型体系结构之间的文化价值对齐，证明了我们提出的方法的有效性。我们的结果提供了早期证据，表明理解意图和社会互动可以增强LLM中的文化价值适应，从而强调了基于文化学习的培训方法的希望。</li>
</ul>

<h3>Title: CoLa -- Learning to Interactively Collaborate with Large LMs</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Sharma, Dan Goldwasser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02965">https://arxiv.org/abs/2504.02965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02965">https://arxiv.org/pdf/2504.02965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02965]] CoLa -- Learning to Interactively Collaborate with Large LMs(https://arxiv.org/abs/2504.02965)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>LLMs' remarkable ability to tackle a wide range of language tasks opened new opportunities for collaborative human-AI problem solving. LLMs can amplify human capabilities by applying their intuitions and reasoning strategies at scale. We explore whether human guides can be simulated, by generalizing from human demonstrations of guiding an AI system to solve complex language problems. We introduce CoLa, a novel self-guided learning paradigm for training automated $\textit{guides}$ and evaluate it on two QA datasets, a puzzle-solving task, and a constrained text generation task. Our empirical results show that CoLa consistently outperforms competitive approaches across all domains. Moreover, a small-sized trained guide outperforms a strong model like GPT-4 when acting as a guide. We compare the strategies employed by humans and automated guides by conducting a human study on a QA dataset. We show that automated guides outperform humans by adapting their strategies to reasoners' capabilities and conduct qualitative analyses highlighting distinct differences in guiding strategies.</li>
<li><strong>摘要：</strong>LLMS的非凡能力解决广泛的语言任务为解决人类问题解决问题打开了新的机会。 LLM可以通过大规模应用其直觉和推理策略来扩大人类能力。我们通过从人类的演示中概括指导AI系统来解决复杂的语言问题，探索是否可以模拟人类指南。我们介绍了Cola，这是一种新颖的自助学习范式，用于培训自动化的$ \ textit {guides} $，并在两个QA数据集，一个拼图解决任务和约束文本生成任务上对其进行评估。我们的经验结果表明，可乐在所有领域始终超过竞争方法。此外，在充当指南时，一个小型训练有素的指南优于GPT-4这样的强大模型。我们通过对质量检查数据集进行人体研究来比较人类和自动指南所采用的策略。我们表明，自动指南通过将策略调整为推理能力并进行定性分析，突出了指导策略的明显差异，从而超过了人类的表现。</li>
</ul>

<h3>Title: A Bayesian account of pronoun and neopronoun acquisition</h3>
<ul>
<li><strong>Authors: </strong>Cassandra L. Jacobs, Morgan Grobol</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02973">https://arxiv.org/abs/2504.02973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02973">https://arxiv.org/pdf/2504.02973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02973]] A Bayesian account of pronoun and neopronoun acquisition(https://arxiv.org/abs/2504.02973)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A major challenge to equity among members of queer communities is the use of one's chosen forms of reference, such as personal names or pronouns. Speakers often dismiss their misuses of pronouns as "unintentional", and claim that their errors reflect many decades of fossilized mainstream language use, as well as attitudes or expectations about the relationship between one's appearance and acceptable forms of reference. We argue for explicitly modeling individual differences in pronoun selection and present a probabilistic graphical modeling approach based on the nested Chinese Restaurant Franchise Process (nCRFP) (Ahmed et al., 2013) to account for flexible pronominal reference such as chosen names and neopronouns while moving beyond form-to-meaning mappings and without lexical co-occurrence statistics to learn referring expressions, as in contemporary language models. We show that such a model can account for variability in how quickly pronouns or names are integrated into symbolic knowledge and can empower computational systems to be both flexible and respectful of queer people with diverse gender expression.</li>
<li><strong>摘要：</strong>酷儿社区成员之间的公平挑战是使用自己选择的参考形式，例如个人名称或代词。演讲者经常将其滥用代词视为“无意的”，并声称他们的错误反映了数十年的化石主流语言使用，以及对外观和可接受的参考形式之间关系的态度或期望。我们主张在代词选择中明确建模个体差异，并根据嵌套的中国餐厅特许经营过程（NCRFP）（Ahmed等，2013）提出了一种概率的图形建模方法（Ahmed等人，2013年），以考虑灵活的名称和Neopronouns的灵活性参考，例如在不超越形式上的映射映射和无用的模型中，以了解同时的模型，以了解同时的同时参考文献，以参考参考文献，以介绍型号。我们表明，这样的模型可以说明代词或名称如何集成到符号知识中的可变性，并可以使计算系统能够既灵活又尊重具有多种性别表达的同性恋者。</li>
</ul>

<h3>Title: Hummus: A Dataset of Humorous Multimodal Metaphor Use</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Tong, Zhi Zhang, Martha Lewis, Ekaterina Shutova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.02983">https://arxiv.org/abs/2504.02983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.02983">https://arxiv.org/pdf/2504.02983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.02983]] Hummus: A Dataset of Humorous Multimodal Metaphor Use(https://arxiv.org/abs/2504.02983)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Metaphor and humor share a lot of common ground, and metaphor is one of the most common humorous mechanisms. This study focuses on the humorous capacity of multimodal metaphors, which has not received due attention in the community. We take inspiration from the Incongruity Theory of humor, the Conceptual Metaphor Theory, and the annotation scheme behind the VU Amsterdam Metaphor Corpus, and developed a novel annotation scheme for humorous multimodal metaphor use in image-caption pairs. We create the Hummus Dataset of Humorous Multimodal Metaphor Use, providing expert annotation on 1k image-caption pairs sampled from the New Yorker Caption Contest corpus. Using the dataset, we test state-of-the-art multimodal large language models (MLLMs) on their ability to detect and understand humorous multimodal metaphor use. Our experiments show that current MLLMs still struggle with processing humorous multimodal metaphors, particularly with regard to integrating visual and textual information. We release our dataset and code at this http URL.</li>
<li><strong>摘要：</strong>隐喻和幽默有很多共同点，隐喻是最常见的幽默机制之一。这项研究的重点是多模式隐喻的幽默能力，该隐喻在社区中没有得到适当关注。我们从不一致的幽默理论，概念隐喻理论以及VU阿姆斯特丹隐喻语料库背后的注释方案中汲取灵感，并为图像捕捉对中的幽默多模式隐喻使用开发了一种新颖的注释方案。我们创建了幽默的多模式隐喻使用的鹰嘴豆泥数据集，为从纽约客字幕竞赛语料库采样的1K图像扣对上提供了专家注释。使用数据集，我们测试了最先进的多模式大语言模型（MLLM），以检测和理解幽默的多模式隐喻的使用能力。我们的实验表明，当前的MLLM仍在处理幽默的多模式隐喻方面，尤其是在整合视觉和文本信息方面。我们在此HTTP URL上发布数据集和代码。</li>
</ul>

<h3>Title: IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zébulon Goriely, Paula Buttery</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03036">https://arxiv.org/abs/2504.03036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03036">https://arxiv.org/pdf/2504.03036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03036]] IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling(https://arxiv.org/abs/2504.03036)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce two resources: (i) G2P+, a tool for converting orthographic datasets to a consistent phonemic representation; and (ii) IPA CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior tools for grapheme-to-phoneme conversion result in phonemic vocabularies that are inconsistent with established phonemic inventories, an issue which G2P+ addresses by leveraging the inventories in the Phoible database. Using this tool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES. This new resource fills several gaps in existing phonemic datasets, which often lack multilingual coverage, spontaneous speech, and a focus on child-directed language. We demonstrate the utility of this dataset for phonological research by training phoneme language models on 11 languages and probing them for distinctive features, finding that the distributional properties of phonemes are sufficient to learn major class and place features cross-lingually.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了两个资源：（i）G2P+，一种将拼字数据集转换为一致的音调表示的工具； （ii）IPA Childes，这是一种以31种语言为中心的语音的音素数据集。字符到音量转换的先前工具会导致语音词汇与既定的音素清单不一致，这是G2P+通过利用Phoible数据库中的库存来解决的问题。使用此工具，我们增加了具有音素转录的Childes来产生IPA Childes。这种新资源填补了现有的音调数据集中的几个空白，这些数据集通常缺乏多语言覆盖范围，自发的语音以及专注于儿童指导语言的空白。我们通过在11种语言上训练音素语言模型并探索它们的独特特征，以证明该数据集在语音研究中的实用性，发现音素的分布属性足以在交叉上学习主要类别和位置特征。</li>
</ul>

<h3>Title: Extending CREAMT: Leveraging Large Language Models for Literary Translation Post-Editing</h3>
<ul>
<li><strong>Authors: </strong>Antonio Castaldo, Sheila Castilho, Joss Moorkens, Johanna Monti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03045">https://arxiv.org/abs/2504.03045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03045">https://arxiv.org/pdf/2504.03045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03045]] Extending CREAMT: Leveraging Large Language Models for Literary Translation Post-Editing(https://arxiv.org/abs/2504.03045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Post-editing machine translation (MT) for creative texts, such as literature, requires balancing efficiency with the preservation of creativity and style. While neural MT systems struggle with these challenges, large language models (LLMs) offer improved capabilities for context-aware and creative translation. This study evaluates the feasibility of post-editing literary translations generated by LLMs. Using a custom research tool, we collaborated with professional literary translators to analyze editing time, quality, and creativity. Our results indicate that post-editing LLM-generated translations significantly reduces editing time compared to human translation while maintaining a similar level of creativity. The minimal difference in creativity between PE and MT, combined with substantial productivity gains, suggests that LLMs may effectively support literary translators working with high-resource languages.</li>
<li><strong>摘要：</strong>诸如文学之类的创意文本的后编辑机器翻译（MT）需要平衡效率与保护创造力和风格。尽管神经MT系统面临这些挑战，但大型语言模型（LLMS）为上下文感知和创造性翻译提供了改进的功能。这项研究评估了LLMS产生的后编辑文学翻译的可行性。使用定制研究工具，我们与专业文学翻译人员合作，分析编辑时间，质量和创造力。我们的结果表明，与人类翻译相比，在保持相似的创造力的同时，与人翻译相比，LLM生成的翻译会大大减少编辑时间。 PE和MT之间的创造力差异最小，结合了实质性的生产率提高，这表明LLM可以有效地支持使用高资源语言工作的文学翻译。</li>
</ul>

<h3>Title: Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chengyang He, Wenlong Zhang, Violet Xinying Chen, Yue Ning, Ping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03051">https://arxiv.org/abs/2504.03051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03051">https://arxiv.org/pdf/2504.03051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03051]] Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language Models(https://arxiv.org/abs/2504.03051)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Accurate medical symptom coding from unstructured clinical text, such as vaccine safety reports, is a critical task with applications in pharmacovigilance and safety monitoring. Symptom coding, as tailored in this study, involves identifying and linking nuanced symptom mentions to standardized vocabularies like MedDRA, differentiating it from broader medical coding tasks. Traditional approaches to this task, which treat symptom extraction and linking as independent workflows, often fail to handle the variability and complexity of clinical narratives, especially for rare cases. Recent advancements in Large Language Models (LLMs) offer new opportunities but face challenges in achieving consistent performance. To address these issues, we propose Task as Context (TACO) Prompting, a novel framework that unifies extraction and linking tasks by embedding task-specific context into LLM prompts. Our study also introduces SYMPCODER, a human-annotated dataset derived from Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage evaluation framework to comprehensively assess both symptom linking and mention fidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat, Jackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's effectiveness in improving flexibility and accuracy for tailored tasks like symptom coding, paving the way for more specific coding tasks and advancing clinical text processing methodologies.</li>
<li><strong>摘要：</strong>从非结构化临床文本（例如疫苗安全报告）中编码的准确医学症状是一项至关重要的任务。根据本研究的量身定制的症状编码涉及识别和联系细微的症状提及与Meddra等标准化词汇，将其与更广泛的医疗编码任务区分开来。这项任务的传统方法将症状提取并将其视为独立的工作流程，通常无法处理临床叙事的可变性和复杂性，尤其是对于极少数情况。大型语言模型（LLMS）的最新进步提供了新的机会，但在达到一致的绩效方面面临着挑战。为了解决这些问题，我们将任务作为上下文（TACO）提示，这是一个新颖的框架，通过将特定于任务的上下文嵌入LLM提示中，将提取和链接到任务统一。我们的研究还介绍了SympCoder，这是一种源自疫苗不良事件报告系统（VAERS）报告的人类通知数据集，以及一个两阶段评估框架，可全面评估症状联系和提及的忠诚度。我们对包括Llama2-Chat，Jackalope-7B，GPT-3.5 Turbo，GPT-4 Turbo和GPT-4O在内的多个LLM的全面评估表明，炸玉米饼在提高量身定制任务（例如症状编码）的灵活性和准确性方面的有效性，例如症状编码，为更具体的编码任务铺平了更多特定的编码任务，并推进了更多的编码任务，并推进了更多的临床文本处理方法。</li>
</ul>

<h3>Title: AD-GPT: Large Language Models in Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Liu, Lintao Tang, Zeliang Sun, Zhengliang Liu, Yanjun Lyu, Wei Ruan, Yangshuang Xu, Liang Shan, Jiyoon Shin, Xiaohe Chen, Dajiang Zhu, Tianming Liu, Rongjie Liu, Chao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03071">https://arxiv.org/abs/2504.03071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03071">https://arxiv.org/pdf/2504.03071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03071]] AD-GPT: Large Language Models in Alzheimer's Disease(https://arxiv.org/abs/2504.03071)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as powerful tools for medical information retrieval, yet their accuracy and depth remain limited in specialized domains such as Alzheimer's disease (AD), a growing global health challenge. To address this gap, we introduce AD-GPT, a domain-specific generative pre-trained transformer designed to enhance the retrieval and analysis of AD-related genetic and neurobiological information. AD-GPT integrates diverse biomedical data sources, including potential AD-associated genes, molecular genetic information, and key gene variants linked to brain regions. We develop a stacked LLM architecture combining Llama3 and BERT, optimized for four critical tasks in AD research: (1) genetic information retrieval, (2) gene-brain region relationship assessment, (3) gene-AD relationship analysis, and (4) brain region-AD relationship mapping. Comparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's superior precision and reliability across these tasks, underscoring its potential as a robust and specialized AI tool for advancing AD research and biomarker discovery.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已成为医疗信息检索的强大工具，但它们的准确性和深度仍然有限​​，例如阿尔茨海默氏病（AD）等专业领域，这是日益增长的全球健康挑战。为了解决这一差距，我们引入了AD-GPT，这是一种域特异性生成的预训练的变压器，旨在增强与AD相关的遗传和神经生物学信息的检索和分析。 AD-GPT整合了不同的生物医学数据源，包括潜在的AD相关基因，分子遗传信息以及与大脑区域相关的关键基因变异。我们开发了一个结合Llama3和BERT的堆叠LLM结构，该结构针对AD研究的四个关键任务进行了优化：（1）遗传信息检索，（2）基因 - 脑区域关系评估，（3）基因AD关系分析和（4）（4）（4）大脑区域-AD -AD关系映射。针对最先进的LLM的比较评估表明，AD-GPT在这些任务中的优异精度和可靠性，强调了其潜力是一种强大而专业的AI工具，用于推进AD研究和生物标志物发现。</li>
</ul>

<h3>Title: Single-Pass Document Scanning for Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Weili Cao, Jianyou Wang, Youze Zheng, Longtian Bao, Qirui Zheng, Taylor Berg-Kirkpatrick, Ramamohan Paturi, Leon Bergen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03101">https://arxiv.org/abs/2504.03101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03101">https://arxiv.org/pdf/2504.03101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03101]] Single-Pass Document Scanning for Question Answering(https://arxiv.org/abs/2504.03101)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Handling extremely large documents for question answering is challenging: chunk-based embedding methods often lose track of important global context, while full-context transformers can be prohibitively expensive for hundreds of thousands of tokens. We propose a single-pass document scanning approach that processes the entire text in linear time, preserving global coherence while deciding which sentences are most relevant to the query. On 41 QA benchmarks, our single-pass scanner consistently outperforms chunk-based embedding methods and competes with large language models at a fraction of the computational cost. By conditioning on the entire preceding context without chunk breaks, the method preserves global coherence, which is especially important for long documents. Overall, single-pass document scanning offers a simple solution for question answering over massive text. All code, datasets, and model checkpoints are available at this https URL</li>
<li><strong>摘要：</strong>处理非常大的文档以进行提问是具有挑战性的：基于块的嵌入方法通常会失去对重要的全球环境的跟踪，而全文变形金刚对于数十万个代币而言可能会非常昂贵。我们提出了一种单通行文档扫描方法，该方法在线性时间内处理整个文本，在确定哪些句子与查询最相关的同时保持全局相干性。在41个QA基准测试中，我们的单通扫描仪始终优于基于块的嵌入方法，并以计算成本的一小部分与大语言模型竞争。通过在整个上下文中没有任何碎片断裂的条件，该方法可以保留全球连贯性，这对于长文档尤其重要。总体而言，单通道文档扫描提供了一个简单的解决方案，以解决大规模文本的回答。所有代码，数据集和模型检查点都可以在此HTTPS URL上找到</li>
</ul>

<h3>Title: Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)</h3>
<ul>
<li><strong>Authors: </strong>Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, Jiarui Wu, Shu Yang, Daoan Zhang, Chen Chen, Lianggong Bruce Wen, Zhang Liu, Jiebo Luo, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03151">https://arxiv.org/abs/2504.03151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03151">https://arxiv.org/pdf/2504.03151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03151]] Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)(https://arxiv.org/abs/2504.03151)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.</li>
<li><strong>摘要：</strong>推理是人类智能的核心，使跨不同任务的结构化问题解决。大型语言模型（LLM）的最新进展极大地增强了其在算术，常识和符号领域中的推理能力。但是，有效地将这些功能扩展到多模式上下文中 - 模型必须将视觉和文本输入范围集成为一个重大挑战。多模式推理引入了复杂性，例如处理跨模式的相互矛盾的信息，这些信息需要模型采用先进的解释策略。解决这些挑战不仅涉及复杂的算法，还涉及可评估推理准确性和连贯性的强大方法。本文提供了文本和多模式LLM中推理技术的简洁而有见地的概述。通过彻底和最新的比较，我们清楚地提出了核心推理挑战和机遇，突出了训练后优化和测试时间推理的实用方法。我们的工作提供了宝贵的见解和指导，弥合了理论框架和实际实施，并为未来的研究设定了明确的方向。</li>
</ul>

<h3>Title: Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction</h3>
<ul>
<li><strong>Authors: </strong>Junlang Qian, Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Zepeng Zhai, Kezhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03159">https://arxiv.org/abs/2504.03159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03159">https://arxiv.org/pdf/2504.03159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03159]] Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction(https://arxiv.org/abs/2504.03159)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Zero-shot text classification typically relies on prompt engineering, but the inherent prompt brittleness of large language models undermines its reliability. Minor changes in prompt can cause significant discrepancies in model performance. We attribute this prompt brittleness largely to the narrow focus on nexttoken probabilities in existing methods. To address this, we propose Placeholding Parallel Prediction (P3), a novel approach that predicts token probabilities across multiple positions and simulates comprehensive sampling of generation paths in a single run of a language model. Experiments show improved accuracy and up to 98% reduction in the standard deviation across prompts, boosting robustness. Even without a prompt, P3 maintains comparable performance, reducing the need for prompt engineering.</li>
<li><strong>摘要：</strong>零声文本分类通常依赖于及时的工程，但是大型语言模型的固有及时迅速脆弱性破坏了其可靠性。提示的微小变化可能会导致模型性能的严重差异。我们将这种提示性的脆性归因于现有方法中对隔壁概率的狭窄关注。为了解决这个问题，我们提出了一个新颖的方法（P3），这是一种新的方法，可以预测跨多个位置的令牌概率，并模拟单个语言模型的单个运行中的发电路径的全面抽样。实验表明，跨提示的标准偏差提高了准确性，并提高了98％的降低，从而提高了鲁棒性。即使没有提示，P3仍保持可比的性能，从而减少了及时工程的需求。</li>
</ul>

<h3>Title: Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation</h3>
<ul>
<li><strong>Authors: </strong>Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03165">https://arxiv.org/abs/2504.03165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03165">https://arxiv.org/pdf/2504.03165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03165]] Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation(https://arxiv.org/abs/2504.03165)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge integration during large language model (LLM) inference in recent years. However, current RAG implementations face challenges in effectively addressing noise, repetition and redundancy in retrieved content, primarily due to their limited ability to exploit fine-grained inter-document relationships. To address these limitations, we propose an \textbf{E}fficient \textbf{D}ynamic \textbf{C}lustering-based document \textbf{C}ompression framework (\textbf{EDC\textsuperscript{2}-RAG}) that effectively utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The results show that this method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets can be found at this https URL.</li>
<li><strong>摘要：</strong>近年来，在大型语言模型（LLM）推论期间，检索授课的生成（RAG）已成为一种广泛采用的知识整合方法。但是，当前的抹布实现在有效地解决了检索内容中的噪声，重复和冗余方面面临挑战，这主要是由于它们利用细粒度跨文档之间关系的能力有限。 To address these limitations, we propose an \textbf{E}fficient \textbf{D}ynamic \textbf{C}lustering-based document \textbf{C}ompression framework (\textbf{EDC\textsuperscript{2}-RAG}) that effectively utilizes latent inter-document relationships while simultaneously removing无关的信息和冗余内容。我们在广泛使用的知识QA和幻觉检测的数据集上验证了基于GPT-3.5的方法。结果表明，该方法在各种情况和实验环境中都能达到一致的性能改进，表明了强大的鲁棒性和适用性。我们的代码和数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Multi-lingual Multi-turn Automated Red Teaming for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Singhania, Christophe Dupuy, Shivam Mangale, Amani Namboori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03174">https://arxiv.org/abs/2504.03174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03174">https://arxiv.org/pdf/2504.03174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03174]] Multi-lingual Multi-turn Automated Red Teaming for LLMs(https://arxiv.org/abs/2504.03174)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Language Model Models (LLMs) have improved dramatically in the past few years, increasing their adoption and the scope of their capabilities over time. A significant amount of work is dedicated to ``model alignment'', i.e., preventing LLMs to generate unsafe responses when deployed into customer-facing applications. One popular method to evaluate safety risks is \textit{red-teaming}, where agents attempt to bypass alignment by crafting elaborate prompts that trigger unsafe responses from a model. Standard human-driven red-teaming is costly, time-consuming and rarely covers all the recent features (e.g., multi-lingual, multi-modal aspects), while proposed automation methods only cover a small subset of LLMs capabilities (i.e., English or single-turn). We present Multi-lingual Multi-turn Automated Red Teaming (\textbf{MM-ART}), a method to fully automate conversational, multi-lingual red-teaming operations and quickly identify prompts leading to unsafe responses. Through extensive experiments on different languages, we show the studied LLMs are on average 71\% more vulnerable after a 5-turn conversation in English than after the initial turn. For conversations in non-English languages, models display up to 195\% more safety vulnerabilities than the standard single-turn English approach, confirming the need for automated red-teaming methods matching LLMs capabilities.</li>
<li><strong>摘要：</strong>在过去的几年中，语言模型模型（LLMS）已大大改善，随着时间的推移，其能力的范围增加了。大量工作专门用于``模型对齐''，即阻止LLMS在部署到面向客户的应用程序中时产生不安全的响应。评估安全风险的一种流行方法是\ textIt {红色团队}，在该方法中，代理商试图通过制作精心设计的提示来绕过对准，从而触发模型的不安全响应。标准的人类驱动的红色团队成本高昂，耗时，很少涵盖所有最近的功能（例如，多语言，多模式方面），而拟议的自动化方法仅涵盖了LLMS的一小部分（即英语或单端）。我们提出了多种语言多转弯自动红色团队（\ textbf {mm-art}），这是一种完全自动化对话的，多语性的红色团队操作并快速识别提示导致不安全响应的提示。通过对不同语言的广泛实验，我们表明所研究的LLM平均在5转英语对话后比初始转弯时要多71 \％。对于非英语语言的对话，模型显示出比标准单转英语方法多达195 \％的安全漏洞，这证实了对匹配LLMS功能的自动红色团队方法的需求。</li>
</ul>

<h3>Title: Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents</h3>
<ul>
<li><strong>Authors: </strong>Jaymari Chua, Chen Wang, Lina Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03185">https://arxiv.org/abs/2504.03185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03185">https://arxiv.org/pdf/2504.03185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03185]] Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents(https://arxiv.org/abs/2504.03185)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings.</li>
<li><strong>摘要：</strong>可概括的对齐是安全部署大型语言模型（LLMS）的核心挑战。当前的一致性方法，包括从人类反馈（RLHF）中学习的强化方法，由于其依赖隐式，事后的偏好，通常无法保证其训练分布以外的限制满意度。受到调整之前的范式转移到首次策划数据的启发，我们引入了一个新的安全语言一致性框架，该框架从正面和负面的演示中学习自然语言约束是主要步骤。通过推断特定于任务的奖励功能和潜在约束功能，我们的方法促进了对新型安全要求和域移动和对抗输入下的鲁棒性概括的适应。我们在受约束的马尔可夫决策过程（CMDP）中对框架进行形式化，并通过基于文本的导航环境对其进行验证，并证明了对改变危险区域的安全适应。我们的实验表明，遵循安全导航路径时，域转移的违规行为较少，并且通过将学习的约束应用于蒸馏的BERT模型作为微调技术，我们实现了零违规行为。这项工作为实用的NLP设置建立了关键的安全性和更具概括性的LLM提供了有希望的途径。</li>
</ul>

<h3>Title: Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation</h3>
<ul>
<li><strong>Authors: </strong>Jaewoo Park, Jungyang Park, Dongju Jang, Jiwan Chung, Byungwoo Yoo, Jaewoo Shin, Seonjoon Park, Taehyeong Kim, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03197">https://arxiv.org/abs/2504.03197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03197">https://arxiv.org/pdf/2504.03197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03197]] Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation(https://arxiv.org/abs/2504.03197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of mathematical reasoning capabilities in large language models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: visual explanation. In real-world instructional contexts, human tutors routinely employ visual aids-such as diagrams, markings, and highlights-to enhance conceptual clarity. To bridge this gap, we introduce a novel task of visual solution explanation, which requires not only solving problems but also generating explanations that incorporate newly introduced visual elements essential for understanding (e.g., auxiliary lines, annotations, or geometric constructions). To evaluate model performance on this task, we propose MathExplain, a multimodal benchmark consisting of 997 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that while some closed-source models demonstrate promising capabilities on visual solution-explaining, current open-source general-purpose models perform inconsistently, particularly in identifying relevant visual components and producing coherent keypoint-based explanations. We expect that visual solution-explaining and the MathExplain dataset will catalyze further research on multimodal LLMs in education and advance their deployment as effective, explanation-oriented AI tutors. Code and data will be released publicly.</li>
<li><strong>摘要：</strong>随着大语言模型（LLMS）数学推理能力的快速发展，在教育环境中，AI系统越来越多地采用，以支持学生对解决问题的过程的理解。然而，在当前LLM生成的解释中，关键组件仍未得到充实：视觉解释。在现实世界的教学环境中，人的导师通常使用视觉辅助工具，例如图，标记和亮点，以增强概念清晰度。为了弥合这一差距，我们介绍了一项新颖的视觉解决方案解释任务，该任务不仅需要解决问题，还需要产生解释，这些解释包含了新介绍的对理解必不可少的视觉元素（例如，辅助线，注释或几何结构）。为了评估该任务上的模型性能，我们提出了MathExplain，这是一种多模式基准，该基准由997个数学问题组成，并用Visual Keypoints注释和相应的解释性文本，引用了这些元素。我们的经验结果表明，尽管一些封闭源模型在视觉解决方案解释方面表现出有希望的能力，但当前的开源通用通用模型表现不一致，尤其是在识别相关的视觉组件并产生基于相干关键的解释方面。我们预计，视觉解决方案解释和数学数据集将促进对教育中多模式LLM的进一步研究，并将其部署作为有效的，以解释为导向的AI导师。代码和数据将公开发布。</li>
</ul>

<h3>Title: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</h3>
<ul>
<li><strong>Authors: </strong>Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03206">https://arxiv.org/abs/2504.03206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03206">https://arxiv.org/pdf/2504.03206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03206]] Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward(https://arxiv.org/abs/2504.03206)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Effective conversational agents must be able to personalize their behavior to suit a user's preferences, personality, and attributes, whether they are assisting with writing tasks or operating in domains like education or healthcare. Current training methods like Reinforcement Learning from Human Feedback (RLHF) prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized interactions. Traditional approaches to personalization often rely on extensive user history, limiting their effectiveness for new or context-limited users. To overcome these limitations, we propose to incorporate an intrinsic motivation to improve the conversational agents's model of the user as an additional reward alongside multi-turn RLHF. This reward mechanism encourages the agent to actively elicit user traits by optimizing conversations to increase the accuracy of its user model. Consequently, the policy agent can deliver more personalized interactions through obtaining more information about the user. We applied our method both education and fitness settings, where LLMs teach concepts or recommend personalized strategies based on users' hidden learning style or lifestyle attributes. Using LLM-simulated users, our approach outperformed a multi-turn RLHF baseline in revealing information about the users' preferences, and adapting to them.</li>
<li><strong>摘要：</strong>有效的对话代理必须能够个性化其行为，以适合用户的偏好，个性和属性，无论他们是在撰写任务还是在教育或医疗保健等领域中运行。当前的培训方法，例如从人类反馈中学习（RLHF）的强化学习优先级的帮助和安全性，但在促进真正的善解人意，适应性和个性化互动方面缺乏。传统的个性化方法通常依赖于广泛的用户历史记录，从而限制了新的或上下文有限的用户的有效性。为了克服这些局限性，我们建议纳入一种内在的动机，以将对话代理的模型改善用户的模型，并将其作为多转变RLHF的额外奖励。这种奖励机制鼓励代理商通过优化对话以提高其用户模型的准确性来积极地引起用户特征。因此，政策代理可以通过获取有关用户的更多信息来提供更多个性化的交互。我们采用了教育和健身环境，LLM会根据用户的隐藏学习风格或生活方式属性来教授概念或推荐个性化策略。使用LLM模拟的用户，我们的方法在揭示有关用户偏好的信息并适应它们的信息方面优于多转化的RLHF基线。</li>
</ul>

<h3>Title: Think When You Need: Self-Adaptive Chain-of-Thought Learning</h3>
<ul>
<li><strong>Authors: </strong>Junjie Yang, Ke Lin, Xing Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03234">https://arxiv.org/abs/2504.03234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03234">https://arxiv.org/pdf/2504.03234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03234]] Think When You Need: Self-Adaptive Chain-of-Thought Learning(https://arxiv.org/abs/2504.03234)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain of Thought (CoT) reasoning enhances language models' performance but often leads to inefficient "overthinking" on simple problems. We identify that existing approaches directly penalizing reasoning length fail to account for varying problem complexity. Our approach constructs rewards through length and quality comparisons, guided by theoretical assumptions that jointly enhance solution correctness with conciseness. Moreover, we further demonstrate our method to fuzzy tasks where ground truth is unavailable. Experiments across multiple reasoning benchmarks demonstrate that our method maintains accuracy while generating significantly more concise explanations, effectively teaching models to "think when needed."</li>
<li><strong>摘要：</strong>思想链（COT）推理增强了语言模型的性能，但通常会导致对简单问题的效率低下。我们确定直接惩罚推理长度的现有方法无法解决问题的复杂性。我们的方法在理论假设的指导下，通过长度和质量比较来构建奖励，这些假设共同增强了解决方案的正确性，并简洁。此外，我们进一步证明了我们无法实现地面真理的模糊任务的方法。跨多个推理基准的实验表明，我们的方法保持准确性，同时产生更明显的解释，从而有效地教授“在需要时进行思考”的模型。</li>
</ul>

<h3>Title: Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task</h3>
<ul>
<li><strong>Authors: </strong>Bingqian Wang, Quan Fang, Jiachen Sun, Xiaoxiao Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03295">https://arxiv.org/abs/2504.03295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03295">https://arxiv.org/pdf/2504.03295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03295]] Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task(https://arxiv.org/abs/2504.03295)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Formulating statements that support diverse or controversial stances on specific topics is vital for platforms that enable user expression, reshape political discourse, and drive social critique and information dissemination. With the rise of Large Language Models (LLMs), controllable text generation towards specific stances has become a promising research area with applications in shaping public opinion and commercial marketing. However, current datasets often focus solely on pure texts, lacking multimodal content and effective context, particularly in the context of stance detection. In this paper, we formally define and study the new problem of stance-driven controllable content generation for tweets with text and images, where given a multimodal post (text and image/video), a model generates a stance-controlled response. To this end, we create the Multimodal Stance Generation Dataset (StanceGen2024), the first resource explicitly designed for multimodal stance-controllable text generation in political discourse. It includes posts and user comments from the 2024 U.S. presidential election, featuring text, images, videos, and stance annotations to explore how multimodal political content shapes stance expression. Furthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework that integrates weighted fusion of multimodal features and stance guidance to improve semantic consistency and stance control. We release the dataset and code (this https URL) for public use and further research.</li>
<li><strong>摘要：</strong>对支持特定主题的多样或有争议的立场的制定陈述对于能够表达用户表达，重塑政治话语并推动社会批评和信息传播的平台至关重要。随着大型语言模型（LLM）的兴起，对特定立场的可控文本生成已成为一个有前途的研究领域，并在塑造公众舆论和商业营销方面的应用。但是，当前的数据集通常只关注纯文本，缺乏多模式内容和有效的环境，尤其是在立场检测的情况下。在本文中，我们正式定义和研究了带有文本和图像的推文的立场驱动的可控内容生成的新问题，其中给定了多模式帖子（文本和图像/视频），模型会生成姿态控制的响应。为此，我们创建了多模式立场生成数据集（StanceGen2024），这是第一个针对政治话语中多模式立场可控制的文本生成的明确设计的资源。它包括2024年美国总统大选的帖子和用户评论，其中包含文本，图像，视频和立场注释，以探讨多模式政治内容如何形成立场表达方式。此外，我们提出了一个由立场驱动的多模式生成（SDMG）框架，该框架集成了多模式特征的加权融合和立场指导，以提高语义一致性和立场控制。我们发布数据集和代码（此HTTPS URL）供公众使用和进一步研究。</li>
</ul>

<h3>Title: Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03302">https://arxiv.org/abs/2504.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03302">https://arxiv.org/pdf/2504.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03302]] Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models(https://arxiv.org/abs/2504.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通常会产生不准确或误导性的内容震荡。为了应对这一挑战，我们介绍了噪声增强的微调（Noingfit），这是一个新型框架，该框架利用基于信噪比（SNR）（SNR）的自适应噪声注入以增强模型的鲁棒性。特别是，使用动态缩放的高斯噪声，噪声贴有选择性地将其鉴定为高SNR（更健壮）或低SNR（可能不足的）。我们进一步提出了混合损失，结合了标准的跨凝结，软横膜和一致性正则化，以确保在嘈杂的训练条件下稳定而准确的输出。我们的理论分析表明，自适应噪声注入既是公正的又是差异，为期望的融合提供了强大的保证。多个测试和基准数据集的经验结果表明，噪声贴有显着降低幻觉率，通常在关键任务中改善或匹配基线性能。这些发现突出了噪声驱动的策略的希望，可以实现强大的，值得信赖的语言建模，而不会产生过度的计算开销。鉴于我们实验的全面和详细的性质，我们已在W＆B，Hugging Face和Github在线公开发布了微调日志，基准评估文物和源代码，以促进进一步的研究，可访问性和可重复性。</li>
</ul>

<h3>Title: Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices</h3>
<ul>
<li><strong>Authors: </strong>Luís Couto Seller, Íñigo Sanz Torres, Adrián Vogel-Fernández, Carlos González Carballo, Pedro Miguel Sánchez Sánchez, Adrián Carruana Martín, Enrique de Miguel Ambite</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03312">https://arxiv.org/abs/2504.03312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03312">https://arxiv.org/pdf/2504.03312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03312]] Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices(https://arxiv.org/abs/2504.03312)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance</li>
<li><strong>摘要：</strong>大型语言模型具有显着高级的自然语言处理，可以在语言产生，翻译和推理等任务中取得出色的表现。但是，它们的实质计算要求将部署限制在高端系统上，从而限制了消费级设备的可访问性。对于水资源不足的语言，例如伊比利亚半岛所说的语言尤其明显，在这种语言中，语言资源和基准相对有限，阻碍了有效的评估。这项工作对针对伊比利亚语言量身定制的几种基本NLP任务进行了全面评估紧凑的最新LLM。结果表明，尽管某些模型在某些任务中始终出色，但仍然存在明显的性能差距，尤其是对于巴斯克等语言。这些发现凸显了需要进一步研究平衡模型紧凑性与强大的多语言性能的需求</li>
</ul>

<h3>Title: BabyLM's First Words: Word Segmentation as a Phonological Probing Task</h3>
<ul>
<li><strong>Authors: </strong>Zébulon Goriely</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03338">https://arxiv.org/abs/2504.03338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03338">https://arxiv.org/pdf/2504.03338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03338]] BabyLM's First Words: Word Segmentation as a Phonological Probing Task(https://arxiv.org/abs/2504.03338)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Language models provide a key framework for studying linguistic theories based on prediction, but phonological analysis using large language models (LLMs) is difficult; there are few phonological benchmarks beyond English and the standard input representation used in LLMs (subwords of graphemes) is not suitable for analyzing the representation of phonemes. In this work, we demonstrate how word segmentation can be used as a phonological probing task, allowing us to study the representations learned by phoneme-based language models trained on child-directed speech across 31 languages. Following computational models of word segmentation, we present unsupervised methods for extracting word boundaries from a trained model using the observation that prediction-error peaks at the start of words. We also use linear probes to identify that these models implicitly track word boundaries, even when they do not appear in training. This cross-lingual work corroborates statistical learning theories of acquisition and empirically motivates new methods for training subword tokenizers.</li>
<li><strong>摘要：</strong>语言模型为基于预测研究语言理论提供了一个关键框架，但是使用大语言模型（LLM）的语音分析很困难。英语以外的语音基准很少，LLMS中使用的标准输入表示（素描的子字）不适合分析音素的表示。在这项工作中，我们演示了如何将单词分割用作语音探测任务，从而使我们能够研究由基于音素的语言模型在31种语言中对儿童指导语音进行培训的表述。遵循单词分割的计算模型，我们提出了使用训练有素模型从训练有素模型中提取单词边界的无监督方法，该观察值是在单词开始时预测峰值峰值。我们还使用线性探针来确定这些模型即使在训练中没有出现，即使它们不出现。这项跨语言工作证实了统计学习理论的获取理论，并在经验上促进了培训子字引物的新方法。</li>
</ul>

<h3>Title: Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings</h3>
<ul>
<li><strong>Authors: </strong>Kaustubh Shivshankar Shejole, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03352">https://arxiv.org/abs/2504.03352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03352">https://arxiv.org/pdf/2504.03352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03352]] Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings(https://arxiv.org/abs/2504.03352)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Stereotypes are known to be highly pernicious, making their detection critically important. However, current research predominantly focuses on detecting and evaluating stereotypical biases in LLMs, leaving the study of stereotypes in its early stages. Many studies have failed to clearly distinguish between stereotypes and stereotypical biases, which has significantly slowed progress in advancing research in this area. Stereotype and anti-stereotype detection is a problem that requires knowledge of society; hence, it is one of the most difficult areas in Responsible AI. This work investigates this task, where we propose a four-tuple definition and provide precise terminology distinguishing stereotype, anti-stereotype, stereotypical bias, and bias, offering valuable insights into their various aspects. In this paper, we propose StereoDetect, a high-quality benchmarking dataset curated for this task by optimally utilizing current datasets such as StereoSet and WinoQueer, involving a manual verification process and the transfer of semantic information. We demonstrate that language models for reasoning with fewer than 10B parameters often get confused when detecting anti-stereotypes. We also demonstrate the critical importance of well-curated datasets by comparing our model with other current models for stereotype detection. The dataset and code is available at this https URL.</li>
<li><strong>摘要：</strong>已知刻板印象是高度有害的，使其至关重要。但是，当前的研究主要集中在LLMS中检测和评估刻板印象的偏见，从而使刻板印象的早期阶段的研究。许多研究未能清楚地区分刻板印象和刻板印象的偏见，这在推进该领域的研究方面的进展显着减慢。刻板印象和反疾病型检测是一个需要社会知识的问题。因此，它是负责人AI中最困难的领域之一。这项工作调查了这项任务，我们提出了一个四翼定义，并提供了确切的术语，以区分刻板印象，反风度像，刻板印象偏见和偏见，为其各个方面提供宝贵的见解。在本文中，我们提出了立体探测，这是一种通过最佳利用当前数据集（如Stereoset和Winoqueer）来策划此任务的高质量基准测试数据集，涉及手动验证过程以及语义信息的传输。我们证明，在检测抗疾病型时，少于10b参数的推理的语言模型通常会感到困惑。我们还通过将我们的模型与其他当前的刻板印象检测模型进行比较，证明了精心策划数据集的重要性。该数据集和代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03380">https://arxiv.org/abs/2504.03380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03380">https://arxiv.org/pdf/2504.03380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03380]] Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning(https://arxiv.org/abs/2504.03380)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks show that balanced online filtering yields an additional 10% in AIME and 4% improvements in average over plain GRPO. Moreover, further analysis shows the gains in sample efficiency and training time efficiency, exceeding the maximum reward of plain GRPO within 60% training time and the volume of the training set.</li>
<li><strong>摘要：</strong>面向推理的增强学习（RORL）增强了大语言模型（LLMS）的推理能力。但是，由于RORL奖励的稀疏性，有效的培训高度取决于选择适当难度的问题。尽管课程学习试图通过调整难度来解决这一问题，但它通常依赖于静态时间表，即使是最近的在线过滤方法也缺乏理论基础和对其有效性的系统理解。在这项工作中，我们从理论上和经验上表明，策划批次的问题是训练模型即时实现中间精度的问题可以最大程度地提高RORL培训的有效性，即平衡在线难度过滤。我们首先得出，初始策略和最佳策略之间的KL差异的下限可以以采样精度的差异表示。在这些见解的基础上，我们表明平衡过滤可以最大化下限，从而提高性能。五个挑战性数学推理基准的实验结果表明，平衡的在线过滤会在AIME中额外提高10％，而平均GRPO的平均效果为4％。此外，进一步的分析表明，样本效率和训练时间效率的提高，超过了60％的训练时间和训练集量之内普通GRPO的最大奖励。</li>
</ul>

<h3>Title: Locations of Characters in Narratives: Andersen and Persuasion Datasets</h3>
<ul>
<li><strong>Authors: </strong>Batuhan Ozyurt, Roya Arkhmammadova, Deniz Yuret</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03434">https://arxiv.org/abs/2504.03434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03434">https://arxiv.org/pdf/2504.03434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03434]] Locations of Characters in Narratives: Andersen and Persuasion Datasets(https://arxiv.org/abs/2504.03434)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The ability of machines to grasp spatial understanding within narrative contexts is an intriguing aspect of reading comprehension that continues to be studied. Motivated by the goal to test the AI's competence in understanding the relationship between characters and their respective locations in narratives, we introduce two new datasets: Andersen and Persuasion. For the Andersen dataset, we selected fifteen children's stories from "Andersen's Fairy Tales" by Hans Christian Andersen and manually annotated the characters and their respective locations throughout each story. Similarly, for the Persuasion dataset, characters and their locations in the novel "Persuasion" by Jane Austen were also manually annotated. We used these datasets to prompt Large Language Models (LLMs). The prompts are created by extracting excerpts from the stories or the novel and combining them with a question asking the location of a character mentioned in that excerpt. Out of the five LLMs we tested, the best-performing one for the Andersen dataset accurately identified the location in 61.85% of the examples, while for the Persuasion dataset, the best-performing one did so in 56.06% of the cases.</li>
<li><strong>摘要：</strong>机器在叙事环境中掌握空间理解的能力是继续研究的阅读理解的一个有趣的方面。由测试人工智能在理解角色及其各自叙事中各自位置之间关系的能力的目标的动机，我们介绍了两个新的数据集：安德森和说服力。对于Andersen数据集，我们从Hans Christian Andersen的《安德森的童话故事》中选择了15个儿童故事，并在每个故事中手动注释了角色及其各自的位置。同样，对于说服数据集，简·奥斯丁（Jane Austen）在小说“说服”中的角色及其位置也被手动注释。我们使用这些数据集提示大型语言模型（LLMS）。提示是通过从故事或小说中提取摘录，并将它们与询问该摘录中提到的角色的位置相结合的问题来创建的。在我们测试的五个LLM中，Andersen数据集的表现最佳，可以准确地识别了61.85％的示例中的位置，而对于说服数据集，表现最好的示例中最出色的案例中的一个案例中的案例最佳。</li>
</ul>

<h3>Title: SpectR: Dynamically Composing LM Experts with Spectral Routing</h3>
<ul>
<li><strong>Authors: </strong>William Fleshman, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03454">https://arxiv.org/abs/2504.03454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03454">https://arxiv.org/pdf/2504.03454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03454]] SpectR: Dynamically Composing LM Experts with Spectral Routing(https://arxiv.org/abs/2504.03454)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.</li>
<li><strong>摘要：</strong>培训大型通用语言模型带来了重大挑战。从审计的特定任务或域进行微调的专业专家模型的可用性不断增长，提供了一种有希望的替代方案。利用这些现有专家模型在实际应用程序中的潜力需要有效的方法来选择或合并最适合给定任务的模型。本文介绍了Spectr，这是一种在推理过程中的每个时间步骤中动态组成专家模型的方法。值得注意的是，我们的方法不需要额外的培训，并且可以启用灵活的，令牌和层的模型组合。我们的实验结果表明，SPECT提高了与替代培训方法相对于替代培训方法的路由准确性，从而提高了跨专家领域的任务绩效。</li>
</ul>

<h3>Title: Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej</h3>
<ul>
<li><strong>Authors: </strong>Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03486">https://arxiv.org/abs/2504.03486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03486">https://arxiv.org/pdf/2504.03486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03486]] Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej(https://arxiv.org/abs/2504.03486)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>Automating legal document drafting can significantly enhance efficiency, reduce manual effort, and streamline legal workflows. While prior research has explored tasks such as judgment prediction and case summarization, the structured generation of private legal documents in the Indian legal domain remains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej, a novel, anonymized dataset of private legal documents, and develop NyayaShilp, a fine-tuned legal document generation model specifically adapted to Indian legal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework that first generates structured section titles and then iteratively produces content while leveraging retrieval-based mechanisms to ensure coherence and factual accuracy. We benchmark multiple open-source LLMs, including instruction-tuned and domain-adapted versions, alongside proprietary models for comparison. Our findings indicate that while direct fine-tuning on small datasets does not always yield improvements, our structured wrapper significantly enhances coherence, factual adherence, and overall document quality while mitigating hallucinations. To ensure real-world applicability, we developed a Human-in-the-Loop (HITL) Document Generation System, an interactive user interface that enables users to specify document types, refine section details, and generate structured legal drafts. This tool allows legal professionals and researchers to generate, validate, and refine AI-generated legal documents efficiently. Extensive evaluations, including expert assessments, confirm that our framework achieves high reliability in structured legal drafting. This research establishes a scalable and adaptable foundation for AI-assisted legal drafting in India, offering an effective approach to structured legal document generation.</li>
<li><strong>摘要：</strong>自动化法律文件的起草可以显着提高效率，降低手动努力并简化法律工作流程。虽然先前的研究探讨了诸如判断预测和案件摘要之类的任务，但印度法律领域中私人法律文件的结构化生成在很大程度上仍未得到解决。为了弥合这一差距，我们介绍了Vidhikdastaavej，这是一部小说，匿名的私人法律文件数据集，并开发了Nyayashilp，这是一种精心调整的法律文件生成模型，专门适用于印度法律文本。我们提出了一个模型不合局包装器（MAW），这是一个两步框架，首先生成结构化的部分标题，然后迭代产生内容，同时利用基于检索的机制来确保相干性和事实准确性。我们基准了多个开源LLM，包括指导调整和域适应版的版本，以及专有模型以进行比较。我们的发现表明，尽管在小型数据集上进行直接微调并不总是会改善，但我们的结构化包装器可显着提高连贯性，事实依从性和整体文档质量，同时减轻幻觉。为了确保现实世界中的适用性，我们开发了一个人类的文档（hitl）文档生成系统，这是一个交互式用户界面，使用户能够指定文档类型，完善部分详细信息并生成结构化的法律草稿。该工具允许法律专业人员和研究人员有效地生成，验证和完善AI生成的法律文件。包括专家评估在内的广泛评估，确认我们的框架在结构化的法律制图中可靠。这项研究为印度的AI辅助法律起草建立了可扩展且适应性的基础，为结构化法律文件生成提供了有效的方法。</li>
</ul>

<h3>Title: Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles</h3>
<ul>
<li><strong>Authors: </strong>Chen Wei Kuo, Kevin Chu, Nouar AlDahoul, Hazem Ibrahim, Talal Rahwan, Yasir Zaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03520">https://arxiv.org/abs/2504.03520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03520">https://arxiv.org/pdf/2504.03520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03520]] Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles(https://arxiv.org/abs/2504.03520)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Bias in news reporting significantly impacts public perception, particularly regarding crime, politics, and societal issues. Traditional bias detection methods, predominantly reliant on human moderation, suffer from subjective interpretations and scalability constraints. Here, we introduce an AI-driven framework leveraging advanced large language models (LLMs), specifically GPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to systematically identify and mitigate biases in news articles. To this end, we collect an extensive dataset consisting of over 30,000 crime-related articles from five politically diverse news sources spanning a decade (2013-2023). Our approach employs a two-stage methodology: (1) bias detection, where each LLM scores and justifies biased content at the paragraph level, validated through human evaluation for ground truth establishment, and (2) iterative debiasing using GPT-4o Mini, verified by both automated reassessment and human reviewers. Empirical results indicate GPT-4o Mini's superior accuracy in bias detection and effectiveness in debiasing. Furthermore, our analysis reveals temporal and geographical variations in media bias correlating with socio-political dynamics and real-world events. This study contributes to scalable computational methodologies for bias mitigation, promoting fairness and accountability in news reporting.</li>
<li><strong>摘要：</strong>新闻报道的偏见显着影响公众的看法，尤其是对犯罪，政治和社会问题的看法。传统的偏见检测方法主要依赖于人类节制，受主观解释和可伸缩性约束。在这里，我们介绍了一个利用高级大语模型（LLM）的AI驱动框架，特别是GPT-4O，GPT-4O Mini，Gemini Pro，Gemini Flash，Gemini Flash，Llama 8b和Llama 3B，以系统地识别和减轻新闻文章中的偏见。为此，我们收集了一个广泛的数据集，该数据集由跨越十年（2013  -  2023年）的五种政治多元化新闻来源的30,000多种与犯罪相关的文章组成。我们的方法采用了两阶段的方法：（1）偏见检测，每个LLM分数并证明段落级别的偏见内容，通过人类评估的地面真理建立来验证，以及（2）使用GPT-4O MINI进行迭代性偏见，并通过两者自动重新评估和人类审核者进行验证。经验结果表明GPT-4O MINI在偏置中的偏置检测和有效性方面的卓越精度。此外，我们的分析揭示了媒体偏见与社会政治动态和现实世界事件相关的时间和地理变化。这项研究为缓解偏见的可扩展计算方法有助于促进新闻报道中的公平性和问责制。</li>
</ul>

<h3>Title: Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing</h3>
<ul>
<li><strong>Authors: </strong>Mayank Kothyari, Sunita Sarawagi, Soumen Chakrabarti, Gaurav Arora, Srujana Merugu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03541">https://arxiv.org/abs/2504.03541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03541">https://arxiv.org/pdf/2504.03541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03541]] Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing(https://arxiv.org/abs/2504.03541)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>LLMs are increasingly used as seq2seq translators from natural language utterances to structured programs, a process called semantic interpretation. Unlike atomic labels or token sequences, programs are naturally represented as abstract syntax trees (ASTs). Such structured representation raises novel issues related to the design and selection of in-context examples (ICEs) presented to the LLM. We focus on decomposing the pool of available ICE trees into fragments, some of which may be better suited to solving the test instance. Next, we propose how to use (additional invocations of) an LLM with prompted syntax constraints to automatically map the fragments to corresponding utterances. Finally, we adapt and extend a recent method for diverse ICE selection to work with whole and fragmented ICE instances. We evaluate our system, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing visible accuracy gains from our proposed decomposed diverse demonstration method. Benefits are particularly notable for smaller LLMs, ICE pools having larger labeled trees, and programs in lower resource languages.</li>
<li><strong>摘要：</strong>LLM越来越多地用作SEQ2SEQ翻译人员从自然语言到结构化程序，这是一个称为语义解释的过程。与原子标签或令牌序列不同，程序自然表示为抽象语法树（ASTS）。这种结构化表示提出了与呈现给LLM的外观示例（ICE）的设计和选择有关的新颖问题。我们专注于将可用的冰树池分解为碎片，其中一些可能更适合解决测试实例。接下来，我们建议使用具有促进语法约束的LLM使用（其他调用），以将片段自动映射到相应的话语中。最后，我们适应并扩展了一种用于各种冰选择的方法，可以与整个冰块实例合作。我们将系统Scud4ICL评估在流行的不同语义解析基准上，显示了我们提出的分解多样的演示方法的可见准确性提高。对于较小的LLM，具有较大标签树的冰池以及较低资源语言的程序，好处尤其值得注意。</li>
</ul>

<h3>Title: Agentic Knowledgeable Self-awareness</h3>
<ul>
<li><strong>Authors: </strong>Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03553">https://arxiv.org/abs/2504.03553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03553">https://arxiv.org/pdf/2504.03553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03553]] Agentic Knowledgeable Self-awareness(https://arxiv.org/abs/2504.03553)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在各种代理计划任务中都取得了相当大的性能。但是，传统的代理计划方法采用了一种“洪水灌溉”方法，该方法不论将黄金轨迹，外部反馈和域知识注入代理模型。这种做法忽略了决策过程中情境自我意识的基本认知原则 - 能够动态评估情境需求并在决策过程中策略性地利用资源。我们提出了代理知识渊博的自我意识来解决这一差距，这是一种新颖的范式，使基于LLM的代理可以自主调节知识利用。具体来说，我们提出了一种以数据为中心的方法，该方法应用了像人类一样知识渊博的自我意识的代理。具体而言，我们设计了一个启发式情况判断标准，以标记代理商的自我探索轨迹以收集培训数据的特殊令牌。通过两个阶段的训练过程，代理模型可以通过产生特定的特殊令牌在不同情况下切换，从而实现最佳的计划效果，而成本最低。我们的实验表明，知识自我可以在不同的任务和模型上胜过各种强大的基线，而最少使用外部知识。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</h3>
<ul>
<li><strong>Authors: </strong>Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03561">https://arxiv.org/abs/2504.03561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03561">https://arxiv.org/pdf/2504.03561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03561]] SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement(https://arxiv.org/abs/2504.03561)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at this https URL.</li>
<li><strong>摘要：</strong>在代理与其环境之间的互动中，代理通过计划和执行行动来扩展其能力。但是，基于LLM的代理商在新型环境中部署或驾驶非常规的行动空间时面临重大挑战。为了授权代理人自主探索环境，优化工作流程并增强对动作的理解，我们提出了SynWorld，该框架使代理可以通过在动作空间中使用多步骤调用来合成可能的场景，并在当前环境中有效地改进其动作知识。我们的实验表明，Synworld是在新环境中学习行动知识的有效和一般方法。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline</h3>
<ul>
<li><strong>Authors: </strong>Peter Baile Chen, Tomer Wolfson, Michael Cafarella, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03598">https://arxiv.org/abs/2504.03598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03598">https://arxiv.org/pdf/2504.03598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03598]] EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline(https://arxiv.org/abs/2504.03598)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Existing information retrieval systems excel in cases where the language of target documents closely matches that of the user query. However, real-world retrieval systems are often required to implicitly reason whether a document is relevant. For example, when retrieving technical texts or tables, their relevance to the user query may be implied through a particular jargon or structure, rather than explicitly expressed in their content. Large language models (LLMs) hold great potential in identifying such implied relevance by leveraging their reasoning skills. Nevertheless, current LLM-augmented retrieval is hindered by high latency and computation cost, as the LLM typically computes the query-document relevance online, for every query anew. To tackle this issue we introduce EnrichIndex, a retrieval approach which instead uses the LLM offline to build semantically-enriched retrieval indices, by performing a single pass over all documents in the retrieval corpus once during ingestion time. Furthermore, the semantically-enriched indices can complement existing online retrieval approaches, boosting the performance of LLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving passages and tables, and found that it outperforms strong online LLM-based retrieval systems, with an average improvement of 11.7 points in recall @ 10 and 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online calls to the LLM, it processes 293.3 times fewer tokens which greatly reduces the online latency and cost. Overall, EnrichIndex is an effective way to build better retrieval indices offline by leveraging the strong reasoning skills of LLMs.</li>
<li><strong>摘要：</strong>在目标文档的语言与用户查询密切匹配的情况下，现有信息检索系统脱颖而出。但是，通常需要需要现实的检索系统来隐式推理文档是否相关。例如，当检索技术文本或表格时，它们与用户查询的相关性可以通过特定的术语或结构暗示，而不是在其内容中明确表示。大型语言模型（LLMS）通过利用其推理技能来确定这种隐含相关性的潜力很大。尽管如此，由于LLM通常会在线计算查询文件相关性，但对于每个查询，当前的LLM扬声器检索受到高潜伏期和计算成本的阻碍。为了解决此问题，我们引入了RenichIndex，这是一种检索方法，它通过在摄入期间一次对检索语料库中的所有文档进行一次通过，将LLM离线使用LLM离线构建富含语言的检索指数。此外，语义增强的指数可以补充现有的在线检索方法，从而提高LLM重新级别的性能。我们评估了涉及段落和表格的五个检索任务的丰富inchindex，并发现它表现优于强大的在线LLM检索系统，与强碱基相比，NDCG @ 10中的召回 @ 10和10.6分的平均改善为11.7分。在在线电话方面，它处理的代币少293.3倍，这大大降低了在线延迟和成本。总体而言，富集是一种有效的方法，可以利用LLM的强大推理能力来脱机更好地检索指数。</li>
</ul>

<h3>Title: APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay</h3>
<ul>
<li><strong>Authors: </strong>Akshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Shiyu Wang, Zhiwei Liu, Tulika Awalgaonkar, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03601">https://arxiv.org/abs/2504.03601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03601">https://arxiv.org/pdf/2504.03601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03601]] APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay(https://arxiv.org/abs/2504.03601)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at this https URL and project website is this https URL</li>
<li><strong>摘要：</strong>培训有效的AI代理进行多转交互作用需要高质量的数据，以捕获现实的人类代理动力学，但是手动收集的数据却很少且昂贵。我们介绍了Apigen-MT，这是一个两阶段的框架，生成可验证和多样化的多转变代理数据。在第一阶段，我们的代理管道通过基本操作产生详细的任务蓝图，利用LLM审阅者委员会和迭代反馈循环。然后通过模拟的人类代理相互作用将这些蓝图转化为完整的相互作用轨迹。我们训练一个模型家族 -  XLAM-2-FC-R系列，尺寸从1B到70B参数不等。我们的模型优于$ \ tau $ -bench和BFCL基准等诸如GPT-4O和Claude 3.5之类的边界模型，较小的型号超过了较大的对应物，尤其是在多转弯设置中，同时在多个试验中保持卓越的一致性。全面的实验表明，我们经过验证的蓝图到详细方法可产生高质量的培训数据，从而能够开发更可靠，有效和有能力的代理。我们开放收集的合成数据和训练有素的XLAM-2-FC-R模型，以推动AI代理的研究。在此HTTPS URL的HuggingFace上可以找到型号，项目网站是此HTTPS URL</li>
</ul>

<h3>Title: AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset</h3>
<ul>
<li><strong>Authors: </strong>Bingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen Hong, Longtao Huang, Hui Xue, Ganqu Cui, Wanxiang Che, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03612">https://arxiv.org/abs/2504.03612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03612">https://arxiv.org/pdf/2504.03612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03612]] AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset(https://arxiv.org/abs/2504.03612)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Preference learning is critical for aligning large language models (LLMs) with human values, yet its success hinges on high-quality datasets comprising three core components: Preference \textbf{A}nnotations, \textbf{I}nstructions, and \textbf{R}esponse Pairs. Current approaches conflate these components, obscuring their individual impacts and hindering systematic optimization. In this work, we propose \textbf{AIR}, a component-wise analysis framework that systematically isolates and optimizes each component while evaluating their synergistic effects. Through rigorous experimentation, AIR reveals actionable principles: annotation simplicity (point-wise generative scoring), instruction inference stability (variance-based filtering across LLMs), and response pair quality (moderate margins + high absolute scores). When combined, these principles yield +5.3 average gains over baseline method, even with only 14k high-quality pairs. Our work shifts preference dataset design from ad hoc scaling to component-aware optimization, offering a blueprint for efficient, reproducible alignment.</li>
<li><strong>摘要：</strong>偏好学习对于使大语言模型（LLM）与人类价值观保持一致至关重要，但是其成功取决于包括三个核心组成部分的高质量数据集上：prexperion \ textbf {a} nnotations，\ textbf {i} nStructions和\ textbf {textbf {r} spepsess spepse sossess pot。当前的方法将这些组件混为一谈，掩盖了它们的个人影响并阻碍系统优化。在这项工作中，我们提出了\ textbf {air}，这是一个组件分析框架，在评估其协同效果的同时，系统地隔离和优化了每个组件。通过严格的实验，空气揭示了可行的原理：注释简单性（点的生成评分），指导推理稳定性（基于方差的LLMS基于方差的过滤）和响应对质量（中等边缘 +高绝对分数）。合并后，这些原理在基线方法上的平均收益+5.3，即使只有14K高质量对。我们的工作将偏好数据集设计从临时缩放缩放到组件感知优化，提供了蓝图，以实现高效，可重复的比对。</li>
</ul>

<h3>Title: Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Ranaldi, Barry Haddow, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03616">https://arxiv.org/abs/2504.03616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03616">https://arxiv.org/pdf/2504.03616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03616]] Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task(https://arxiv.org/abs/2504.03616)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has become a cornerstone of contemporary NLP, enhancing large language models (LLMs) by allowing them to access richer factual contexts through in-context retrieval. While effective in monolingual settings, especially in English, its use in multilingual tasks remains unexplored. This paper investigates the effectiveness of RAG across multiple languages by proposing novel approaches for multilingual open-domain question-answering. We evaluate the performance of various multilingual RAG strategies, including question-translation (tRAG), which translates questions into English before retrieval, and Multilingual RAG (MultiRAG), where retrieval occurs directly across multiple languages. Our findings reveal that tRAG, while useful, suffers from limited coverage. In contrast, MultiRAG improves efficiency by enabling multilingual retrieval but introduces inconsistencies due to cross-lingual variations in the retrieved content. To address these issues, we propose Crosslingual RAG (CrossRAG), a method that translates retrieved documents into a common language (e.g., English) before generating the response. Our experiments show that CrossRAG significantly enhances performance on knowledge-intensive tasks, benefiting both high-resource and low-resource languages.</li>
<li><strong>摘要：</strong>检索仪式（RAG）已成为当代NLP的基石，通过允许他们通过秘密检索访问更丰富的事实环境，从而增强了大型语言模型（LLMS）。虽然在单语设置中有效，尤其是在英语中，但在多语言任务中的使用仍未得到探索。本文通过提出多种语言开放域问答的新方法来研究跨多种语言的抹布的有效性。我们评估了各种多语言抹布策略的性能，包括问题翻译（TRAG），该策略在检索前将问题转化为英语，以及多语言抹布（多拉格），其中检索直接发生在多种语言上。我们的发现表明，trag虽然有用，但覆盖范围有限。相比之下，多齿可以通过启用多语言检索来提高效率，但由于检索到的含量的跨语性变化而引起的不一致。为了解决这些问题，我们提出了跨语言抹布（CrossRag），该方法在产生响应之前将检索到通用语言（例如英语）转换为通用语言（例如英语）。我们的实验表明，CrossRag显着提高了知识密集型任务的性能，从而使高资源和低资源语言受益。</li>
</ul>

<h3>Title: Align to Structure: Aligning Large Language Models with Structural Information</h3>
<ul>
<li><strong>Authors: </strong>Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03622">https://arxiv.org/abs/2504.03622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03622">https://arxiv.org/pdf/2504.03622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03622]] Align to Structure: Aligning Large Language Models with Structural Information(https://arxiv.org/abs/2504.03622)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at this https URL.</li>
<li><strong>摘要：</strong>对于大型语言模型（LLM）来说，生成长而连贯的文本仍然是一个挑战，因为它们缺乏层次结构计划和在话语产生中的结构性组织。我们介绍了结构对齐，这是一种新颖的方法，将LLM与人类的话语结构保持一致，以增强长篇文本生成。通过将语言基础的话语框架整合到加强学习中，我们的方法指导模型以产生连贯且组织良好的产出。我们在近端政策优化框架内采用密集的奖励计划，根据与人写作相对的话语独特性，分配了细粒度的，令牌级别的奖励。评估了两个互补的奖励模型：第一个通过评分表面级的文本特征来提高可读性，以提供明确的结构，而第二个则通过分析层次结构的话语主题来分析全球话语模式，从而增强了更深的连贯性和修辞性，从而超过了标准和RLHF-Espers-tasks intpers and tasks and Essemiment and Essiment anderpoy andocemand andocemand andocemand Generatimation sumpers command command command command command command command command sumpsar command command summbar。所有培训数据和代码将在此HTTPS URL上公开共享。</li>
</ul>

<h3>Title: Bonsai: Interpretable Tree-Adaptive Grounded Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kate Sanders, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.03640">https://arxiv.org/abs/2504.03640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.03640">https://arxiv.org/pdf/2504.03640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.03640]] Bonsai: Interpretable Tree-Adaptive Grounded Reasoning(https://arxiv.org/abs/2504.03640)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.</li>
<li><strong>摘要：</strong>为了开发通用协作代理，人类需要可靠的AI系统，该系统可以（1）适应新领域，以及（2）透明的理由以不确定性允许验证和校正。 Black-Box模型表现出强大的数据处理能力，但由于其不透明，领域的特异性和缺乏不确定性意识而无法满足这些标准。我们介绍了盆景，这是一种组成和概率的推理系统，通过检索相关的基础证据并使用它来计算从更广泛的自然语言推断中得出的亚声称的可能性，从而产生适应性的推理树。盆景的推理能力在测试时间可以通过证据缩放来调节，它证明了对各种域的可靠处理，包括成绩单，照片，视频，音频和数据库。提问和人类对齐实验表明，盆景与域特异性黑盒方法的性能相匹配，同时生成可解释，接地和不确定性的推理痕迹。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
