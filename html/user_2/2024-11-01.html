<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-01</h1>
<h3>Title: Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists</h3>
<ul>
<li><strong>Authors: </strong>Michał Pietruszka, Łukasz Borchmann, Aleksander Jędrosz, Paweł Morawiecki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23331">https://arxiv.org/abs/2410.23331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23331">https://arxiv.org/pdf/2410.23331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23331]] Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists(https://arxiv.org/abs/2410.23331)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We present a benchmark for large language models designed to tackle one of the most knowledge-intensive tasks in data science: writing feature engineering code, which requires domain knowledge in addition to a deep understanding of the underlying problem and data structure. The model is provided with a dataset description in a prompt and asked to generate code transforming it. The evaluation score is derived from the improvement achieved by an XGBoost model fit on the modified dataset compared to the original data. By an extensive evaluation of state-of-the-art models and comparison to well-established benchmarks, we demonstrate that the FeatEng of our proposal can cheaply and efficiently assess the broad capabilities of LLMs, in contrast to the existing methods.</li>
<li><strong>摘要：</strong>我们为大型语言模型提供了一个基准，旨在解决数据科学中最需要知识的任务之一：编写特征工程代码，这需要领域知识以及对底层问题和数据结构的深刻理解。该模型在提示中提供了数据集描述，并被要求生成对其进行转换的代码。评估分数来自 XGBoost 模型在修改后的数据集上与原始数据相比所取得的改进。通过对最先进模型的广泛评估并与完善的基准进行比较，我们证明了与现有方法相比，我们提案的 FeatEng 可以廉价而有效地评估 LLM 的广泛能力。</li>
</ul>

<h3>Title: Leveraging Language Models and Bandit Algorithms to Drive Adoption of Battery-Electric Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Keiichi Namikoshi, David A. Shamma, Rumen Iliev, Jingchao Fang, Alexandre Filipowicz, Candice L Hogan, Charlene Wu, Nikos Arechiga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23371">https://arxiv.org/abs/2410.23371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23371">https://arxiv.org/pdf/2410.23371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23371]] Leveraging Language Models and Bandit Algorithms to Drive Adoption of Battery-Electric Vehicles(https://arxiv.org/abs/2410.23371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Behavior change interventions are important to coordinate societal action across a wide array of important applications, including the adoption of electrified vehicles to reduce emissions. Prior work has demonstrated that interventions for behavior must be personalized, and that the intervention that is most effective on average across a large group can result in a backlash effect that strengthens opposition among some subgroups. Thus, it is important to target interventions to different audiences, and to present them in a natural, conversational style. In this context, an important emerging application domain for large language models (LLMs) is conversational interventions for behavior change. In this work, we leverage prior work on understanding values motivating the adoption of battery electric vehicles. We leverage new advances in LLMs, combined with a contextual bandit, to develop conversational interventions that are personalized to the values of each study participant. We use a contextual bandit algorithm to learn to target values based on the demographics of each participant. To train our bandit algorithm in an offline manner, we leverage LLMs to play the role of study participants. We benchmark the persuasive effectiveness of our bandit-enhanced LLM against an unaided LLM generating conversational interventions without demographic-targeted values.</li>
<li><strong>摘要：</strong>行为改变干预对于协调社会行动在一系列重要应用中的重要性不言而喻，包括采用电动汽车来减少排放。先前的研究表明，行为干预必须个性化，平均而言对大群体最有效的干预可能会导致反弹效应，从而加强某些亚群体之间的反对。因此，重要的是针对不同的受众实施干预，并以自然的对话风格呈现它们。在这种情况下，大型语言模型 (LLM) 的一个重要新兴应用领域是行为改变的对话干预。在这项工作中，我们利用先前的研究来理解激励人们采用电动汽车的价值观。我们利用 LLM 的新进展，结合上下文强盗，开发出针对每个研究参与者的价值观进行个性化的对话干预。我们使用上下文强盗算法来学习根据每个参与者的人口统计数据来定位价值观。为了以离线方式训练我们的强盗算法，我们利用 LLM 扮演研究参与者的角色。我们对经过老虎机增强的 LLM 的说服效力与未经辅助的 LLM 产生的不具有人口统计目标值的对话干预进行了对比。</li>
</ul>

<h3>Title: Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?</h3>
<ul>
<li><strong>Authors: </strong>Yue Huang, Zhengqing Yuan, Yujun Zhou, Kehan Guo, Xiangqi Wang, Haomin Zhuang, Weixiang Sun, Lichao Sun, Jindong Wang, Yanfang Ye, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23426">https://arxiv.org/abs/2410.23426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23426">https://arxiv.org/pdf/2410.23426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23426]] Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?(https://arxiv.org/abs/2410.23426)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly employed for simulations, enabling applications in role-playing agents and Computational Social Science (CSS). However, the reliability of these simulations is under-explored, which raises concerns about the trustworthiness of LLMs in these applications. In this paper, we aim to answer ``How reliable is LLM-based simulation?'' To address this, we introduce TrustSim, an evaluation dataset covering 10 CSS-related topics, to systematically investigate the reliability of the LLM simulation. We conducted experiments on 14 LLMs and found that inconsistencies persist in the LLM-based simulated roles. In addition, the consistency level of LLMs does not strongly correlate with their general performance. To enhance the reliability of LLMs in simulation, we proposed Adaptive Learning Rate Based ORPO (AdaORPO), a reinforcement learning-based algorithm to improve the reliability in simulation across 7 LLMs. Our research provides a foundation for future studies to explore more robust and trustworthy LLM-based simulations.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于模拟，从而可以在角色扮演代理和计算社会科学 (CSS) 中应用。然而，这些模拟的可靠性尚未得到充分探索，这引发了人们对 LLM 在这些应用中的可信度的担忧。在本文中，我们旨在回答“基于 LLM 的模拟有多可靠？”为了解决这个问题，我们引入了 TrustSim，这是一个涵盖 10 个 CSS 相关主题的评估数据集，以系统地研究 LLM 模拟的可靠性。我们对 14 个 LLM 进行了实验，发现基于 LLM 的模拟角色中仍然存在不一致性。此外，LLM 的一致性水平与其总体性能没有很强的相关性。为了提高 LLM 在模拟中的可靠性，我们提出了基于自适应学习率的 ORPO (AdaORPO)，这是一种基于强化学习的算法，可提高 7 个 LLM 的模拟可靠性。我们的研究为未来探索更稳健、更可信的基于 LLM 的模拟奠定了基础。</li>
</ul>

<h3>Title: Graph-Augmented Relation Extraction Model with LLMs-Generated Support Document</h3>
<ul>
<li><strong>Authors: </strong>Vicky Dong, Hao Yu, Yao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23452">https://arxiv.org/abs/2410.23452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23452">https://arxiv.org/pdf/2410.23452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23452]] Graph-Augmented Relation Extraction Model with LLMs-Generated Support Document(https://arxiv.org/abs/2410.23452)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study introduces a novel approach to sentence-level relation extraction (RE) that integrates Graph Neural Networks (GNNs) with Large Language Models (LLMs) to generate contextually enriched support documents. By harnessing the power of LLMs to generate auxiliary information, our approach crafts an intricate graph representation of textual data. This graph is subsequently processed through a Graph Neural Network (GNN) to refine and enrich the embeddings associated with each entity ensuring a more nuanced and interconnected understanding of the data. This methodology addresses the limitations of traditional sentence-level RE models by incorporating broader contexts and leveraging inter-entity interactions, thereby improving the model's ability to capture complex relationships across sentences. Our experiments, conducted on the CrossRE dataset, demonstrate the effectiveness of our approach, with notable improvements in performance across various domains. The results underscore the potential of combining GNNs with LLM-generated context to advance the field of relation extraction.</li>
<li><strong>摘要：</strong>本研究介绍了一种新颖的句子级关系提取 (RE) 方法，该方法将图神经网络 (GNN) 与大型语言模型 (LLM) 相结合，以生成上下文丰富的支持文档。通过利用 LLM 的强大功能来生成辅助信息，我们的方法可以制作出文本数据的复杂图形表示。随后通过图神经网络 (GNN) 处理该图，以细化和丰富与每个实体相关的嵌入，确保对数据的理解更加细致入微和互联互通。该方法通过结合更广泛的上下文并利用实体间交互来解决传统句子级 RE 模型的局限性，从而提高了模型捕获跨句子复杂关系的能力。我们在 CrossRE 数据集上进行的实验证明了我们方法的有效性，并且在各个领域的性能都有显着提高。结果强调了将 GNN 与 LLM 生成的上下文相结合以推进关系提取领域的潜力。</li>
</ul>

<h3>Title: MDCure: A Scalable Pipeline for Multi-Document Instruction-Following</h3>
<ul>
<li><strong>Authors: </strong>Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23463">https://arxiv.org/abs/2410.23463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23463">https://arxiv.org/pdf/2410.23463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23463]] MDCure: A Scalable Pipeline for Multi-Document Instruction-Following(https://arxiv.org/abs/2410.23463)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Multi-document (MD) processing is crucial for LLMs to handle real-world tasks such as summarization and question-answering across large sets of documents. While LLMs have improved at processing long inputs, MD contexts still present challenges, such as managing inter-document dependencies, redundancy, and incoherent structures. We introduce MDCure, a scalable and effective fine-tuning pipeline to enhance the MD capabilities of LLMs without the computational cost of pre-training or reliance on human annotated data. MDCure is based on generation of high-quality synthetic MD instruction data from sets of related articles via targeted prompts. We further introduce MDCureRM, a multi-objective reward model which filters generated data based on their training utility for MD settings. With MDCure, we fine-tune a variety of LLMs, from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size. Extensive evaluations on a wide range of MD and long-context benchmarks spanning various tasks show MDCure consistently improves performance over pre-trained baselines and over corresponding base models by up to 75.5%. Our code, datasets, and models are available at this https URL.</li>
<li><strong>摘要：</strong>多文档 (MD) 处理对于 LLM 处理现实世界任务（例如跨大量文档的摘要和问答）至关重要。虽然 LLM 在处理长输入方面有所改进，但 MD 上下文仍然存在挑战，例如管理文档间依赖关系、冗余和不连贯的结构。我们引入了 MDCure，这是一种可扩展且有效的微调管道，可增强 LLM 的 MD 功能，而无需预先训练或依赖人工注释数据的计算成本。MDCure 基于通过有针对性的提示从相关文章集生成高质量的合成 MD 指令数据。我们进一步介绍了 MDCureRM，这是一种多目标奖励模型，它根据 MD 设置的训练效用过滤生成的数据。借助 MDCure，我们可以微调各种 LLM，包括 FlanT5、Qwen2 和 LLAMA3.1 模型系列，最多可容纳 70B 个参数。对涵盖各种任务的广泛 MD 和长上下文基准进行的广泛评估表明，MDCure 的性能始终比预训练基线和相应的基础模型提高高达 75.5%。我们的代码、数据集和模型可在此 https URL 上找到。</li>
</ul>

<h3>Title: Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs</h3>
<ul>
<li><strong>Authors: </strong>Sireesh Gururaja, Yueheng Zhang, Guannan Tang, Tianhao Zhang, Kevin Murphy, Yu-Tsen Yi, Junwon Seo, Anthony Rollett, Emma Strubell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23478">https://arxiv.org/abs/2410.23478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23478">https://arxiv.org/pdf/2410.23478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23478]] Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs(https://arxiv.org/abs/2410.23478)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recent years in NLP have seen the continued development of domain-specific information extraction tools for scientific documents, alongside the release of increasingly multimodal pretrained transformer models. While the opportunity for scientists outside of NLP to evaluate and apply such systems to their own domains has never been clearer, these models are difficult to compare: they accept different input formats, are often black-box and give little insight into processing failures, and rarely handle PDF documents, the most common format of scientific publication. In this work, we present Collage, a tool designed for rapid prototyping, visualization, and evaluation of different information extraction models on scientific PDFs. Collage allows the use and evaluation of any HuggingFace token classifier, several LLMs, and multiple other task-specific models out of the box, and provides extensible software interfaces to accelerate experimentation with new models. Further, we enable both developers and users of NLP-based tools to inspect, debug, and better understand modeling pipelines by providing granular views of intermediate states of processing. We demonstrate our system in the context of information extraction to assist with literature review in materials science.</li>
<li><strong>摘要：</strong>近年来，NLP 领域中，针对科学文档的特定领域信息提取工具不断发展，同时发布了越来越多的多模态预训练转换器模型。虽然 NLP 以外的科学家评估此类系统并将其应用于自己领域的机会从未如此清晰，但这些模型很难进行比较：它们接受不同的输入格式，通常是黑盒的，几乎无法洞察处理失败，并且很少处理 PDF 文档（最常见的科学出版物格式）。在这项工作中，我们介绍了 Collage，这是一种旨在快速原型设计、可视化和评估科学 PDF 上不同信息提取模型的工具。Collage 允许使用和评估任何 HuggingFace 标记分类器、多个 LLM 和多个其他特定于任务的模型，并提供可扩展的软件接口以加速新模型的实验。此外，我们通过提供处理中间状态的细粒度视图，使基于 NLP 的工具的开发人员和用户能够检查、调试和更好地理解建模流程。我们在信息提取的背景下演示了我们的系统，以协助材料科学的文献综述。</li>
</ul>

<h3>Title: Smaller Large Language Models Can Do Moral Self-Correction</h3>
<ul>
<li><strong>Authors: </strong>Guangliang Liu, Zhiyu Xue, Rongrong Wang, Kristen Marie Johnson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23496">https://arxiv.org/abs/2410.23496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23496">https://arxiv.org/pdf/2410.23496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23496]] Smaller Large Language Models Can Do Moral Self-Correction(https://arxiv.org/abs/2410.23496)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Self-correction is one of the most amazing emerging capabilities of Large Language Models (LLMs), enabling LLMs to self-modify an inappropriate output given a natural language feedback which describes the problems of that output. Moral self-correction is a post-hoc approach correcting unethical generations without requiring a gradient update, making it both computationally lightweight and capable of preserving the language modeling ability. Previous works have shown that LLMs can self-debias, and it has been reported that small models, i.e., those with less than 22B parameters, are not capable of moral self-correction. However, there is no direct proof as to why such smaller models fall short of moral self-correction, though previous research hypothesizes that larger models are skilled in following instructions and understanding abstract social norms. In this paper, we empirically validate this hypothesis in the context of social stereotyping, through meticulous prompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs with proper safety alignment fine-tuning can achieve very good moral self-correction performance, highlighting the significant effects of safety alignment; and (ii) small LLMs are indeed weaker than larger-scale models in terms of comprehending social norms and self-explanation through CoT, but all scales of LLMs show bad self-correction performance given unethical instructions.</li>
<li><strong>摘要：</strong>自我纠正是大型语言模型 (LLM) 最令人惊奇的新兴功能之一，它使 LLM 能够在给出描述该输出问题的自然语言反馈的情况下自我修改不适当的输出。道德自我纠正是一种事后方法，无需梯度更新即可纠正不道德的生成，使其在计算上既轻量又能够保留语言建模能力。之前的研究表明，LLM 可以自我纠正偏见，据报道，小型模型（即参数少于 22B 的模型）不具备道德自我纠正能力。然而，没有直接证据表明为什么这种较小的模型无法进行道德自我纠正，尽管之前的研究假设较大的模型擅长遵循指示和理解抽象的社会规范。在本文中，我们通过细致的提示，在社会刻板印象的背景下对这一假设进行了实证验证。我们的实验结果表明：（i）令人惊讶的是，经过适当安全对齐微调的 3.8B LLM 可以实现非常好的道德自我纠正性能，凸显了安全对齐的显著作用；（ii）小型 LLM 在理解社会规范和通过 CoT 进行自我解释方面确实比大型模型弱，但所有规模的 LLM 在给出不道德指令时都表现出糟糕的自我纠正性能。</li>
</ul>

<h3>Title: Dynamic Strategy Planning for Efficient Question Answering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tanmay Parekh, Pradyot Prakash, Alexander Radovic, Akshay Shekher, Denis Savenkov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23511">https://arxiv.org/abs/2410.23511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23511">https://arxiv.org/pdf/2410.23511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23511]] Dynamic Strategy Planning for Efficient Question Answering with Large Language Models(https://arxiv.org/abs/2410.23511)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought), planning (e.g., SelfAsk), and retrieval augmented generation strategies to improve the performance of Large Language Models (LLMs) on various tasks, such as question answering. However, using a single fixed strategy to answer different kinds of questions is suboptimal in performance and inefficient in terms of generated output tokens and performed retrievals. In our work, we propose a novel technique DyPlan, to induce a dynamic strategy selection process in LLMs, to improve performance and reduce costs in question-answering. DyPlan incorporates an initial decision step to select the most suitable strategy conditioned on the input question and guides the LLM's response generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal verification and correction process to further enrich the generated answer. Experiments on three prominent multi-hop question answering (MHQA) datasets reveal how DyPlan can improve model performance by 7-13% while reducing the cost by 11-32% relative to the best baseline model.</li>
<li><strong>摘要：</strong>研究表明，推理（例如，思维链）、规划（例如，SelfAsk）和检索增强生成策略可有效提高大型语言模型 (LLM) 在各种任务（例如问答）上的性能。但是，使用单一固定策略回答不同类型的问题在性能上不是最优的，而且在生成的输出标记和执行的检索方面效率低下。在我们的工作中，我们提出了一种新技术 DyPlan，以在 LLM 中引入动态策略选择过程，从而提高性能并降低问答成本。DyPlan 结合了初始决策步骤，以根据输入问题选择最合适的策略，并相应地指导 LLM 的响应生成。我们将 DyPlan 扩展为 DyPlan-verify，添加了内部验证和更正过程以进一步丰富生成的答案。在三个著名的多跳问答 (MHQA) 数据集上进行的实验表明，与最佳基线模型相比，DyPlan 如何将模型性能提高 7-13%，同时将成本降低 11-32%。</li>
</ul>

<h3>Title: Neural spell-checker: Beyond words with synthetic data generation</h3>
<ul>
<li><strong>Authors: </strong>Matej Klemen, Martin Božič, Špela Arhar Holdt, Marko Robnik-Šikonja</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23514">https://arxiv.org/abs/2410.23514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23514">https://arxiv.org/pdf/2410.23514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23514]] Neural spell-checker: Beyond words with synthetic data generation(https://arxiv.org/abs/2410.23514)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Spell-checkers are valuable tools that enhance communication by identifying misspelled words in written texts. Recent improvements in deep learning, and in particular in large language models, have opened new opportunities to improve traditional spell-checkers with new functionalities that not only assess spelling correctness but also the suitability of a word for a given context. In our work, we present and compare two new spell-checkers and evaluate them on synthetic, learner, and more general-domain Slovene datasets. The first spell-checker is a traditional, fast, word-based approach, based on a morphological lexicon with a significantly larger word list compared to existing spell-checkers. The second approach uses a language model trained on a large corpus with synthetically inserted errors. We present the training data construction strategies, which turn out to be a crucial component of neural spell-checkers. Further, the proposed neural model significantly outperforms all existing spell-checkers for Slovene in both precision and recall.</li>
<li><strong>摘要：</strong>拼写检查器是一种有价值的工具，它通过识别书面文本中的拼写错误单词来增强交流。深度学习（尤其是大型语言模型）的最新改进为改进传统拼写检查器提供了新的机会，这些新功能不仅可以评估拼写的正确性，还可以评估单词是否适合给定的上下文。在我们的工作中，我们介绍并比较了两个新的拼写检查器，并在合成、学习者和更通用领域的斯洛文尼亚语数据集上对它们进行了评估。第一个拼写检查器是一种传统的、快速的、基于单词的方法，基于形态词典，与现有的拼写检查器相比，它的单词列表要大得多。第二种方法使用在大型语料库上训练的语言模型，其中插入了合成错误。我们介绍了训练数据构建策略，这些策略是神经拼写检查器的重要组成部分。此外，所提出的神经模型在准确率和召回率方面都远远优于所有现有的斯洛文尼亚语拼写检查器。</li>
</ul>

<h3>Title: LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hieu Tran, Junda Wang, Yujan Ting, Weijing Huang, Terrence Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23526">https://arxiv.org/abs/2410.23526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23526">https://arxiv.org/pdf/2410.23526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23526]] LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models(https://arxiv.org/abs/2410.23526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable capabilities in various natural language processing tasks, yet they often struggle with maintaining factual accuracy, particularly in knowledge-intensive domains like healthcare. This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking, a novel approach designed to enhance the factual reliability of LLMs, with a focus on medical question answering (QA). LEAF utilizes a dual strategy to enhance the factual accuracy of responses from models such as Llama 3 70B Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG, improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking results to guide the retrieval process without updating model parameters. The second strategy, Learning from Fact-Checks via Self-Training, involves supervised fine-tuning (SFT) on fact-checked responses or applying Simple Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both updating LLM parameters from supervision. These findings suggest that integrating fact-checked responses whether through RAG enhancement or self-training enhances the reliability and factual correctness of LLM outputs, offering a promising solution for applications where information accuracy is crucial.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种自然语言处理任务中表现出非凡的能力，但它们往往难以保持事实准确性，尤其是在医疗保健等知识密集型领域。本研究介绍了 LEAF：通过事实核查增强的学习和评估，这是一种旨在提高 LLM 事实可靠性的新方法，重点是医学问答 (QA)。LEAF 采用双重策略来提高 Llama 3 70B Instruct 和 Llama 3 8B Instruct 等模型的响应的事实准确性。第一种策略，即 Fact-Check-Then-RAG，通过结合事实核查结果来指导检索过程而无需更新模型参数，从而改进了检索增强生成 (RAG)。第二种策略，即通过自我训练从事实核查中学习，涉及对事实核查的响应进行监督微调 (SFT) 或应用简单偏好优化 (SimPO) 和事实核查作为排名机制，两者都从监督中更新 LLM 参数。这些发现表明，无论是通过 RAG 增强还是自我训练，整合经过事实核查的回应都可以提高 LLM 输出的可靠性和事实正确性，为信息准确性至关重要的应用提供了一个有希望的解决方案。</li>
</ul>

<h3>Title: Large Language Models for Patient Comments Multi-Label Classification</h3>
<ul>
<li><strong>Authors: </strong>Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23528">https://arxiv.org/abs/2410.23528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23528">https://arxiv.org/pdf/2410.23528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23528]] Large Language Models for Patient Comments Multi-Label Classification(https://arxiv.org/abs/2410.23528)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Patient experience and care quality are crucial for a hospital's sustainability and reputation. The analysis of patient feedback offers valuable insight into patient satisfaction and outcomes. However, the unstructured nature of these comments poses challenges for traditional machine learning methods following a supervised learning paradigm. This is due to the unavailability of labeled data and the nuances these texts encompass. This research explores leveraging Large Language Models (LLMs) in conducting Multi-label Text Classification (MLTC) of inpatient comments shared after a stay in the hospital. GPT-4o-Turbo was leveraged to conduct the classification. However, given the sensitive nature of patients' comments, a security layer is introduced before feeding the data to the LLM through a Protected Health Information (PHI) detection framework, which ensures patients' de-identification. Additionally, using the prompt engineering framework, zero-shot learning, in-context learning, and chain-of-thought prompting were experimented with. Results demonstrate that GPT-4o-Turbo, whether following a zero-shot or few-shot setting, outperforms traditional methods and Pre-trained Language Models (PLMs) and achieves the highest overall performance with an F1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the few-shot learning results. Subsequently, the results' association with other patient experience structured variables (e.g., rating) was conducted. The study enhances MLTC through the application of LLMs, offering healthcare practitioners an efficient method to gain deeper insights into patient feedback and deliver prompt, appropriate responses.</li>
<li><strong>摘要：</strong>患者体验和护理质量对于医院的可持续性和声誉至关重要。对患者反馈的分析可以提供有关患者满意度和治疗效果的宝贵见解。然而，这些评论的非结构化性质对遵循监督学习范式的传统机器学习方法构成了挑战。这是由于标记数据的不可获得性以及这些文本所包含的细微差别。本研究探索利用大型语言模型 (LLM) 对住院后分享的住院评论进行多标签文本分类 (MLTC)。利用 GPT-4o-Turbo 进行分类。然而，考虑到患者评论的敏感性，在通过受保护的健康信息 (PHI) 检测框架将数据提供给 LLM 之前引入了一个安全层，以确保患者的身份识别。此外，使用提示工程框架，尝试了零样本学习、上下文学习和思路链提示。结果表明，无论是在零样本还是少样本设置下，GPT-4o-Turbo 的表现都优于传统方法和预训练语言模型 (PLM)，并且总体表现最高，F1 得分为 76.12%，加权 F1 得分为 73.61%，紧随其后的是少样本学习结果。随后，研究了结果与其他患者体验结构化变量（例如评分）的关联。该研究通过应用 LLM 增强了 MLTC，为医疗从业者提供了一种有效的方法来更深入地了解患者的反馈并做出及时、适当的回应。</li>
</ul>

<h3>Title: Simulating User Agents for Embodied Conversational-AI</h3>
<ul>
<li><strong>Authors: </strong>Daniel Philipov, Vardhan Dongre, Gokhan Tur, Dilek Hakkani-Tür</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23535">https://arxiv.org/abs/2410.23535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23535">https://arxiv.org/pdf/2410.23535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23535]] Simulating User Agents for Embodied Conversational-AI(https://arxiv.org/abs/2410.23535)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Embodied agents designed to assist users with tasks must engage in natural language interactions, interpret instructions, execute actions, and communicate effectively to resolve issues. However, collecting large-scale, diverse datasets of situated human-robot dialogues to train and evaluate such agents is expensive, labor-intensive, and time-consuming. To address this challenge, we propose building a large language model (LLM)-based user agent that can simulate user behavior during interactions with an embodied agent in a virtual environment. Given a user goal (e.g., make breakfast), at each time step, the user agent may observe" the robot actions or speak" to either intervene with the robot or answer questions. Such a user agent assists in improving the scalability and efficiency of embodied dialogues dataset generation and is critical for enhancing and evaluating the robot's interaction and task completion ability, as well as for research in reinforcement learning using AI feedback. We evaluate our user agent's ability to generate human-like behaviors by comparing its simulated dialogues with the TEACh dataset. We perform three experiments: zero-shot prompting to predict dialogue acts, few-shot prompting, and fine-tuning on the TEACh training subset. Results show the LLM-based user agent achieves an F-measure of 42% with zero-shot prompting and 43.4% with few-shot prompting in mimicking human speaking behavior. Through fine-tuning, performance in deciding when to speak remained stable, while deciding what to say improved from 51.1% to 62.5%. These findings showcase the feasibility of the proposed approach for assessing and enhancing the effectiveness of robot task completion through natural language communication.</li>
<li><strong>摘要：</strong>旨在协助用户完成任务的具身代理必须参与自然语言交互、解释指令、执行操作并有效沟通以解决问题。然而，收集大规模、多样化的情境人机对话数据集来训练和评估此类代理既昂贵又耗费人力和时间。为了应对这一挑战，我们建议构建一个基于大型语言模型 (LLM) 的用户代理，该代理可以在虚拟环境中模拟用户与具身代理交互时的行为。给定一个用户目标（例如做早餐），在每个时间步骤，用户代理可以观察“机器人动作或说话”以干预机器人或回答问题。这样的用户代理有助于提高具身对话数据集生成的可扩展性和效率，对于增强和评估机器人的交互和任务完成能力以及使用 AI 反馈进行强化学习的研究至关重要。我们通过将其模拟对话与 TEACh 数据集进行比较来评估我们的用户代理生成类似人类行为的能力。我们进行了三项实验：零次提示预测对话行为、少次提示和在 TEACh 训练子集上进行微调。结果表明，基于 LLM 的用户代理在模仿人类说话行为时，零次提示的 F 测量值为 42%，少次提示的 F 测量值为 43.4%。通过微调，决定何时说话的表现保持稳定，而决定说什么的表现从 51.1% 提高到 62.5%。这些发现证明了所提出的方法在评估和提高通过自然语言交流完成机器人任务的有效性方面的可行性。</li>
</ul>

<h3>Title: From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents</h3>
<ul>
<li><strong>Authors: </strong>Nalin Tiwary, Vardhan Dongre, Sanil Arun Chawla, Ashwin Lamani, Dilek Hakkani-Tür</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23555">https://arxiv.org/abs/2410.23555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23555">https://arxiv.org/pdf/2410.23555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23555]] From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents(https://arxiv.org/abs/2410.23555)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Model (LLM)-based frameworks have extended their capabilities to complex real-world applications, such as interactive web navigation. These systems, driven by user commands, navigate web browsers to complete tasks through multi-turn dialogues, offering both innovative opportunities and significant challenges. Despite the introduction of benchmarks for conversational web navigation, a detailed understanding of the key contextual components that influence the performance of these agents remains elusive. This study aims to fill this gap by analyzing the various contextual elements crucial to the functioning of web navigation agents. We investigate the optimization of context management, focusing on the influence of interaction history and web page representation. Our work highlights improved agent performance across out-of-distribution scenarios, including unseen websites, categories, and geographic locations through effective context management. These findings provide insights into the design and optimization of LLM-based agents, enabling more accurate and effective web navigation in real-world applications.</li>
<li><strong>摘要：</strong>基于大型语言模型 (LLM) 的框架的最新进展已将其功能扩展到复杂的实际应用，例如交互式 Web 导航。这些系统由用户命令驱动，通过多轮对话引导 Web 浏览器完成任务，既提供了创新机会，也带来了重大挑战。尽管引入了对话式 Web 导航的基准，但对影响这些代理性能的关键上下文组件的详细了解仍然难以捉摸。本研究旨在通过分析对 Web 导航代理功能至关重要的各种上下文元素来填补这一空白。我们研究了上下文管理的优化，重点关注交互历史和网页表示的影响。我们的工作重点是通过有效的上下文管理在分布外场景（包括未见过的网站、类别和地理位置）中提高代理性能。这些发现为基于 LLM 的代理的设计和优化提供了见解，使实际应用中的 Web 导航更加准确和有效。</li>
</ul>

<h3>Title: Dynamic Uncertainty Ranking: Enhancing In-Context Learning for Long-Tail Knowledge in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shuyang Yu, Runxue Bao, Parminder Bhatia, Taha Kass-Hout, Jiayu Zhou, Cao Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23605">https://arxiv.org/abs/2410.23605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23605">https://arxiv.org/pdf/2410.23605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23605]] Dynamic Uncertainty Ranking: Enhancing In-Context Learning for Long-Tail Knowledge in LLMs(https://arxiv.org/abs/2410.23605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can learn vast amounts of knowledge from diverse domains during pre-training. However, long-tail knowledge from specialized domains is often scarce and underrepresented, rarely appearing in the models' memorization. Prior work has shown that in-context learning (ICL) with retriever augmentation can help LLMs better capture long-tail knowledge, reducing their reliance on pre-trained data. Despite these advances, we observe that LLM predictions for long-tail questions remain uncertain to variations in retrieved samples. To take advantage of the uncertainty in ICL for guiding LLM predictions toward correct answers on long-tail samples, we propose a reinforcement learning-based dynamic uncertainty ranking method for ICL that accounts for the varying impact of each retrieved sample on LLM predictions. Our approach prioritizes more informative and stable samples while demoting misleading ones, updating rankings based on the feedback from the LLM w.r.t. each retrieved sample. To enhance training efficiency and reduce query costs, we introduce a learnable dynamic ranking threshold, adjusted when the model encounters negative prediction shifts. Experimental results on various question-answering datasets from different domains show that our method outperforms the best baseline by $2.76\%$, with a notable $5.96\%$ boost in accuracy on long-tail questions that elude zero-shot inference.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可以在预训练期间从不同领域学习大量知识。然而，来自专业领域的长尾知识通常稀缺且代表性不足，很少出现在模型的记忆中。先前的研究表明，具有检索器增强的上下文学习 (ICL) 可以帮助 LLM 更好地捕获长尾知识，从而减少对预训练数据的依赖。尽管取得了这些进展，但我们观察到 LLM 对长尾问题的预测仍然不确定，因为检索到的样本发生了变化。为了利用 ICL 中的不确定性来引导 LLM 预测长尾样本的正确答案，我们提出了一种基于强化学习的 ICL 动态不确定性排名方法，该方法考虑了每个检索到的样本对 LLM 预测的不同影响。我们的方法优先考虑更具信息性和稳定性的样本，同时降低误导性样本，并根据 LLM 对每个检索到的样本的反馈更新排名。为了提高训练效率并降低查询成本，我们引入了一个可学习的动态排名阈值，当模型遇到负预测变化时进行调整。在不同领域的各种问答数据集上的实验结果表明，我们的方法比最佳基线高出 $2.76\%$，在逃避零样本推理的长尾问题上，准确率显著提高了 $5.96\%$。</li>
</ul>

<h3>Title: On Positional Bias of Faithfulness for Long-form Summarization</h3>
<ul>
<li><strong>Authors: </strong>David Wan, Jesse Vig, Mohit Bansal, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23609">https://arxiv.org/abs/2410.23609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23609">https://arxiv.org/pdf/2410.23609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23609]] On Positional Bias of Faithfulness for Long-form Summarization(https://arxiv.org/abs/2410.23609)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often exhibit positional bias in long-context settings, under-attending to information in the middle of inputs. We investigate the presence of this bias in long-form summarization, its impact on faithfulness, and various techniques to mitigate this bias. To consistently evaluate faithfulness, we first compile a benchmark of eight human-annotated long-form summarization datasets and perform a meta-evaluation of faithfulness metrics. We show that LLM-based faithfulness metrics, though effective with full-context inputs, remain sensitive to document order, indicating positional bias. Analyzing LLM-generated summaries across six datasets, we find a "U-shaped" trend in faithfulness, where LLMs faithfully summarize the beginning and end of documents but neglect middle content. Perturbing document order similarly reveals models are less faithful when important documents are placed in the middle of the input. We find that this behavior is partly due to shifting focus with context length: as context increases, summaries become less faithful, but beyond a certain length, faithfulness improves as the model focuses on the end. Finally, we experiment with different generation techniques to reduce positional bias and find that prompting techniques effectively direct model attention to specific positions, whereas more sophisticated approaches offer limited improvements. Our data and code are available in this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常在长上下文设置中表现出位置偏差，对输入中间的信息关注不足。我们研究了这种偏差在长格式摘要中的存在、它对忠实度的影响以及减轻这种偏差的各种技术。为了一致地评估忠实度，我们首先编制了八个人工注释的长格式摘要数据集的基准，并对忠实度指标进行了元评估。我们表明，基于 LLM 的忠实度指标虽然对全上下文输入有效，但仍然对文档顺序敏感，这表明存在位置偏差。通过分析六个数据集中 LLM 生成的摘要，我们发现忠实度呈“U 形”趋势，其中 LLM 忠实地总结了文档的开头和结尾，但忽略了中间内容。扰乱文档顺序同样表明，当重要文档放在输入中间时，模型的忠实度较低。我们发现这种行为部分是由于焦点随着上下文长度而转移：随着上下文的增加，摘要变得不那么忠实，但超过一定长度，由于模型将注意力集中在结尾，忠实度会提高。最后，我们尝试了不同的生成技术来减少位置偏差，发现提示技术可以有效地将模型的注意力引导到特定位置，而更复杂的方法只能提供有限的改进。我们的数据和代码可在此 https URL 中找到。</li>
</ul>

<h3>Title: Morphological Typology in BPE Subword Productivity and Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Iñigo Parra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23656">https://arxiv.org/abs/2410.23656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23656">https://arxiv.org/pdf/2410.23656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23656]] Morphological Typology in BPE Subword Productivity and Language Modeling(https://arxiv.org/abs/2410.23656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study investigates the impact of morphological typology on tokenization and language modeling performance. We focus on languages with synthetic and analytical morphological structures and examine their productivity when tokenized using the byte-pair encoding (BPE) algorithm. We compare the performance of models trained with similar amounts of data in different languages. Our experiments reveal that languages with synthetic features exhibit greater subword regularity and productivity with BPE tokenization and achieve better results in language modeling tasks. We also observe that the typological continuum from linguistic theory is reflected in several experiments. These findings suggest a correlation between morphological typology and BPE tokenization efficiency.</li>
<li><strong>摘要：</strong>本研究调查了形态类型学对标记化和语言建模性能的影响。我们专注于具有合成和分析形态结构的语言，并检查使用字节对编码 (BPE) 算法进行标记化时的生产力。我们比较了使用不同语言的类似数据量训练的模型的性能。我们的实验表明，具有合成特征的语言在 BPE 标记化中表现出更高的子词规律性和生产力，并在语言建模任务中取得了更好的结果。我们还观察到，语言理论的类型学连续性反映在几个实验中。这些发现表明形态类型学和 BPE 标记化效率之间存在相关性。</li>
</ul>

<h3>Title: Kernel Looping: Eliminating Synchronization Boundaries for Peak Inference Performance</h3>
<ul>
<li><strong>Authors: </strong>David Koeplinger, Darshan Gandhi, Pushkar Nandkar, Nathan Sheeley, Matheen Musaddiq, Leon Zhang, Reid Goodbar, Matthew Shaffer, Han Wang, Angela Wang, Mingran Wang, Raghu Prabhakar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23668">https://arxiv.org/abs/2410.23668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23668">https://arxiv.org/pdf/2410.23668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23668]] Kernel Looping: Eliminating Synchronization Boundaries for Peak Inference Performance(https://arxiv.org/abs/2410.23668)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Token generation speed is critical to power the next wave of AI inference applications. GPUs significantly underperform during token generation due to synchronization overheads at kernel boundaries, utilizing only 21% of their peak memory bandwidth. While recent dataflow architectures mitigate these overheads by enabling aggressive fusion of decoder layers into a single kernel, they too leave performance on the table due to synchronization penalties at layer boundaries. This paper presents kernel looping, a specialized global optimization technique which exploits an optimization opportunity brought by combining the unique layer-level fusion possible in modern dataflow architectures with the repeated layer structure found in language models. Kernel looping eliminates synchronization costs between consecutive calls to the same kernel by transforming these calls into a single call to a modified kernel containing a pipelined outer loop. We evaluate kernel looping on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI. Experiments demonstrate that kernel looping speeds up the decode phase of a wide array of powerful open-source models by up to 2.2$\times$ on SN40L. Kernel looping allows scaling of decode performance over multiple SN40L sockets, achieving speedups of up to 2.5$\times$. Finally, kernel looping enables SN40L to achieve over 90% of peak performance on 8 and 16 sockets and achieve a speedup of up to 3.7$\times$ over DGX H100. Kernel looping, as well as the models evaluated in this paper, are deployed in production in a commercial AI inference cloud.</li>
<li><strong>摘要：</strong>令牌生成速度对于推动下一波 AI 推理应用至关重要。由于内核边界的同步开销，GPU 在令牌生成过程中的表现明显不佳，仅利用了其峰值内存带宽的 21%。虽然最近的数据流架构通过将解码器层积极融合到单个内核中来减轻这些开销，但由于层边界的同步惩罚，它们也会将性能置于不利地位。本文介绍了内核循环，这是一种专门的全局优化技术，它利用了现代数据流架构中可能存在的独特层级融合与语言模型中发现的重复层结构相结合所带来的优化机会。内核循环通过将这些调用转换为对包含流水线外循环的修改内核的单个调用，消除了对同一内核的连续调用之间的同步成本。我们在 SambaNova SN40L 可重构数据流单元 (RDU) 上评估了内核循环，这是一种用于 AI 的商用数据流加速器。实验表明，内核循环将 SN40L 上各种强大的开源模型的解码阶段速度提高了 2.2$\times$。内核循环允许在多个 SN40L 插槽上扩展解码性能，实现高达 2.5 倍的加速。最后，内核循环使 SN40L 在 8 和 16 个插槽上实现超过 90% 的峰值性能，并实现比 DGX H100 高达 3.7 倍的加速。内核循环以及本文评估的模型已在商业 AI 推理云中投入生产。</li>
</ul>

<h3>Title: Pseudo-Conversation Injection for LLM Goal Hijacking</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chen, Buhui Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23678">https://arxiv.org/abs/2410.23678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23678">https://arxiv.org/pdf/2410.23678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23678]] Pseudo-Conversation Injection for LLM Goal Hijacking(https://arxiv.org/abs/2410.23678)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Goal hijacking is a type of adversarial attack on Large Language Models (LLMs) where the objective is to manipulate the model into producing a specific, predetermined output, regardless of the user's original input. In goal hijacking, an attacker typically appends a carefully crafted malicious suffix to the user's prompt, which coerces the model into ignoring the user's original input and generating the target response. In this paper, we introduce a novel goal hijacking attack method called Pseudo-Conversation Injection, which leverages the weaknesses of LLMs in role identification within conversation contexts. Specifically, we construct the suffix by fabricating responses from the LLM to the user's initial prompt, followed by a prompt for a malicious new task. This leads the model to perceive the initial prompt and fabricated response as a completed conversation, thereby executing the new, falsified prompt. Following this approach, we propose three Pseudo-Conversation construction strategies: Targeted Pseudo-Conversation, Universal Pseudo-Conversation, and Robust Pseudo-Conversation. These strategies are designed to achieve effective goal hijacking across various scenarios. Our experiments, conducted on two mainstream LLM platforms including ChatGPT and Qwen, demonstrate that our proposed method significantly outperforms existing approaches in terms of attack effectiveness.</li>
<li><strong>摘要：</strong>目标劫持是一种针对大型语言模型 (LLM) 的对抗性攻击，其目的是操纵模型产生特定的、预定的输出，而不管用户的原始输入是什么。在目标劫持中，攻击者通常会在用户的提示后附加一个精心制作的恶意后缀，迫使模型忽略用户的原始输入并生成目标响应。在本文中，我们介绍了一种称为伪对话注入的新型目标劫持攻击方法，它利用了 LLM 在对话上下文中角色识别的弱点。具体来说，我们通过伪造 LLM 对用户初始提示的响应来构建后缀，然后提示恶意的新任务。这导致模型将初始提示和伪造的响应视为完整的对话，从而执行新的伪造提示。按照这种方法，我们提出了三种伪对话构建策略：有针对性的伪对话、通用伪对话和鲁棒伪对话。这些策略旨在实现在各种场景中的有效目标劫持。我们在 ChatGPT 和 Qwen 等两个主流 LLM 平台上进行的实验表明，我们提出的方法在攻击效果方面明显优于现有方法。</li>
</ul>

<h3>Title: Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers</h3>
<ul>
<li><strong>Authors: </strong>Eugene Jang, Kimin Lee, Jin-Woo Chung, Keuntae Park, Seungwon Shin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23684">https://arxiv.org/abs/2410.23684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23684">https://arxiv.org/pdf/2410.23684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23684]] Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers(https://arxiv.org/abs/2410.23684)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Tokenization is a crucial step that bridges human-readable text with model-readable discrete tokens. However, recent studies have revealed that tokenizers can be exploited to elicit unwanted model behaviors. In this work, we investigate incomplete tokens, i.e., undecodable tokens with stray bytes resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize that such tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens. To demonstrate this vulnerability, we introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency. Our experiments show that improbable bigrams are significantly prone to hallucinatory behaviors. Surprisingly, alternative tokenizations of the same phrases result in drastically lower rates of hallucination (93% reduction in Llama3.1). We caution against the potential vulnerabilities introduced by byte-level BPE tokenizers, which may impede the development of trustworthy language models.</li>
<li><strong>摘要：</strong>标记化是将人类可读的文本与模型可读的离散标记联系起来的关键步骤。然而，最近的研究表明，标记器可以被利用来引发不必要的模型行为。在这项工作中，我们研究了不完整的标记，即由字节级字节对编码 (BPE) 标记化产生的带有杂散字节的不可解码标记。我们假设此类标记严重依赖于其相邻标记，并且在与不熟悉的标记配对时很脆弱。为了证明这种脆弱性，我们引入了不可能的二元组：旨在利用其依赖性的不完整标记的分布外组合。我们的实验表明，不可能的二元组很容易产生幻觉行为。令人惊讶的是，对相同短语进行替代标记化会导致幻觉率大幅降低（Llama3.1 中减少了 93%）。我们警告字节级 BPE 标记器引入的潜在漏洞，这可能会阻碍可信语言模型的开发。</li>
</ul>

<h3>Title: Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction</h3>
<ul>
<li><strong>Authors: </strong>Peizhi Tang, Chuang Yang, Tong Xing, Xiaohang Xu, Renhe Jiang, Kaoru Sezaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23692">https://arxiv.org/abs/2410.23692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23692">https://arxiv.org/pdf/2410.23692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23692]] Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction(https://arxiv.org/abs/2410.23692)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Human mobility prediction plays a critical role in applications such as disaster response, urban planning, and epidemic forecasting. Traditional methods often rely on designing crafted, domain-specific models, and typically focus on short-term predictions, which struggle to generalize across diverse urban environments. In this study, we introduce Llama-3-8B-Mob, a large language model fine-tuned with instruction tuning, for long-term citywide mobility prediction -- in a Q&A manner. We validate our approach using large-scale human mobility data from four metropolitan areas in Japan, focusing on predicting individual trajectories over the next 15 days. The results demonstrate that Llama-3-8B-Mob excels in modeling long-term human mobility -- surpassing the state-of-the-art on multiple prediction metrics. It also displays strong zero-shot generalization capabilities -- effectively generalizing to other cities even when fine-tuned only on limited samples from a single city. Source codes are available at this https URL.</li>
<li><strong>摘要：</strong>人类流动性预测在灾害响应、城市规划和流行病预测等应用中起着至关重要的作用。传统方法通常依赖于设计精心设计的、特定领域的模型，并且通常侧重于短期预测，而这些模型很难推广到不同的城市环境中。在本研究中，我们引入了 Llama-3-8B-Mob，这是一个经过指令调整的大型语言模型，用于以问答方式进行长期全市流动性预测。我们使用来自日本四个大都市地区的大规模人类流动性数据验证了我们的方法，重点是预测未来 15 天的个人轨迹。结果表明，Llama-3-8B-Mob 在长期人类流动性建模方面表现出色——在多个预测指标上超越了最先进的技术。它还表现出强大的零样本泛化能力——即使仅对单个城市的有限样本进行微调，也能有效地推广到其他城市。源代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: GigaCheck: Detecting LLM-generated Content</h3>
<ul>
<li><strong>Authors: </strong>Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Aleksandr Gordeev, Vladimir Dokholyan, Maksim Kuprashevich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23728">https://arxiv.org/abs/2410.23728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23728">https://arxiv.org/pdf/2410.23728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23728]] GigaCheck: Detecting LLM-generated Content(https://arxiv.org/abs/2410.23728)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>With the increasing quality and spread of LLM-based assistants, the amount of artificially generated content is growing rapidly. In many cases and tasks, such texts are already indistinguishable from those written by humans, and the quality of generation tends to only increase. At the same time, detection methods are developing more slowly, making it challenging to prevent misuse of these technologies. In this work, we investigate the task of generated text detection by proposing the GigaCheck. Our research explores two approaches: (i) distinguishing human-written texts from LLM-generated ones, and (ii) detecting LLM-generated intervals in Human-Machine collaborative texts. For the first task, our approach utilizes a general-purpose LLM, leveraging its extensive language abilities to fine-tune efficiently for the downstream task of LLM-generated text detection, achieving high performance even with limited data. For the second task, we propose a novel approach that combines computer vision and natural language processing techniques. Specifically, we use a fine-tuned general-purpose LLM in conjunction with a DETR-like detection model, adapted from computer vision, to localize artificially generated intervals within text. We evaluate the GigaCheck on five classification datasets with English texts and three datasets designed for Human-Machine collaborative text analysis. Our results demonstrate that GigaCheck outperforms previous methods, even in out-of-distribution settings, establishing a strong baseline across all datasets.</li>
<li><strong>摘要：</strong>随着基于 LLM 的助手质量的提高和普及，人工生成内容的数量正在迅速增长。在许多情况和任务中，此类文本已经与人类编写的文本难以区分，而且生成的文本质量趋于只增不减。与此同时，检测方法的发展速度较慢，因此很难防止这些技术的滥用。在这项工作中，我们通过提出 GigaCheck 来研究生成文本检测任务。我们的研究探索了两种方法：(i) 区分人类编写的文本和 LLM 生成的文本，以及 (ii) 在人机协作文本中检测 LLM 生成的间隔。对于第一个任务，我们的方法利用通用 LLM，利用其广泛的语言能力对下游 LLM 生成文本检测任务进行有效微调，即使在数据有限的情况下也能实现高性能。对于第二个任务，我们提出了一种结合计算机视觉和自然语言处理技术的新方法。具体来说，我们使用经过微调的通用 LLM 结合 DETR 类检测模型（改编自计算机视觉）来定位文本中人工生成的间隔。我们在五个包含英文文本的分类数据集和三个用于人机协作文本分析的数据集上评估了 GigaCheck。我们的结果表明，即使在分布外的设置中，GigaCheck 也优于以前的方法，在所有数据集上建立了强大的基线。</li>
</ul>

<h3>Title: What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Yanhong Li, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23743">https://arxiv.org/abs/2410.23743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23743">https://arxiv.org/pdf/2410.23743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23743]] What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective(https://arxiv.org/abs/2410.23743)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: this https URL.</li>
<li><strong>摘要：</strong>是什么导致了 LLM 的后期训练有所不同？我们通过梯度的视角，研究了在使用不同响应和初始模型进行训练时，大型语言模型 (LLM) 中不同层的训练模式。鉴于最近在诸如思路链 (CoT) 和过程奖励等推理路径上训练 LLM 的流行，我们特别感兴趣的是快速思维与慢速思维如何影响分层梯度。在我们的研究中，没有 CoT 的快速思维比慢速思维 (详细 CoT) 导致更大的梯度和跨层梯度差异，这表明后者带来的学习稳定性。此外，与指令调整的 LLM 相比，预训练的 LLM 受快速思维不稳定性的影响较小。此外，我们研究了在使用慢速思维和快速思维路径训练不同 LLM 时，梯度模式是否可以反映响应的正确性。结果表明，慢速思维的梯度可以区分正确和不相关的推理路径。作为比较，我们对非推理知识学习任务进行了类似的梯度分析，然而，在这些任务中，稍微增加响应长度不会导致类似的慢思考行为。我们的研究加强了对 LLM 训练的基本理解，并对其效率和稳定性提供了新的见解，为构建可推广的 System-2 代理铺平了道路。我们的代码、数据和梯度统计数据可以在这个 https URL 中找到。</li>
</ul>

<h3>Title: DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xinyi Yang, Yulin Yuan, Lidia S. Chao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23746">https://arxiv.org/abs/2410.23746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23746">https://arxiv.org/pdf/2410.23746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23746]] DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios(https://arxiv.org/abs/2410.23746)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating advanced prompt usages, human revisions like word substitutions, and writing errors. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors. Data and code are publicly available at: this https URL.</li>
<li><strong>摘要：</strong>检测大型语言模型 (LLM) 生成的文本是最近备受关注的话题。借助 DetectGPT 等零样本方法，检测能力已达到令人印象深刻的水平。然而，现有检测器在实际应用中的可靠性仍未得到充分探索。在这项研究中，我们提出了一个新的基准 DetectRL，强调即使是最先进的 (SOTA) 检测技术在这项任务中仍然表现不佳。我们从 LLM 特别容易被滥用的领域收集了人工编写的数据集。使用流行的 LLM，我们生成了与实际应用更一致的数据。与以前的研究不同，我们使用启发式规则来创建对抗性 LLM 生成的文本，模拟高级提示用法、人工修订（如单词替换）和书写错误。我们对 DetectRL 的开发揭示了当前 SOTA 检测器的优势和局限性。更重要的是，我们分析了写作风格、模型类型、攻击方法、文本长度和现实世界的人类写作因素对不同类型检测器的潜在影响。我们相信 DetectRL 可以作为评估现实场景中检测器的有效基准，随着高级攻击方法的发展而不断发展，从而提供更具压力的评估，以推动更高效检测器的开发。数据和代码可在以下网址公开获取：此 https URL。</li>
</ul>

<h3>Title: The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams</h3>
<ul>
<li><strong>Authors: </strong>Yunqi Zhu, Wen Tang, Ying Sun, Xuebing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23769">https://arxiv.org/abs/2410.23769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23769">https://arxiv.org/pdf/2410.23769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23769]] The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams(https://arxiv.org/abs/2410.23769)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Recent research on large language models (LLMs) has primarily focused on their adaptation and application in specialized domains. The application of LLMs in the medical field is mainly concentrated on tasks such as the automation of medical report generation, summarization, diagnostic reasoning, and question-and-answer interactions between doctors and patients. The challenge of becoming a good teacher is more formidable than that of becoming a good student, and this study pioneers the application of LLMs in the field of medical education. In this work, we investigate the extent to which LLMs can generate medical qualification exam questions and corresponding answers based on few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic diseases, we tasked the LLMs with generating open-ended questions and answers based on a subset of sampled admission reports across eight widely used LLMs, including ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and Mistral. Furthermore, we engaged medical experts to manually evaluate these open-ended questions and answers across multiple dimensions. The study found that LLMs, after using few-shot prompts, can effectively mimic real-world medical qualification exam questions, whereas there is room for improvement in the correctness, evidence-based statements, and professionalism of the generated answers. Moreover, LLMs also demonstrate a decent level of ability to correct and rectify reference answers. Given the immense potential of artificial intelligence in the medical field, the task of generating questions and answers for medical qualification exams aimed at medical students, interns and residents can be a significant focus of future research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新研究主要集中在其在专业领域的适应和应用。LLM 在医学领域的应用主要集中在医疗报告生成、总结、诊断推理和医生与患者之间的问答互动的自动化任务上。成为一名好老师的挑战比成为一名好学生的挑战更大，这项研究开创了 LLM 在医学教育领域的应用。在这项工作中，我们调查了 LLM 在多大程度上能够基于少量提示生成医学资格考试问题和相应的答案。利用真实的中国老年慢性病数据集，我们要求 LLM 基于八个广泛使用的 LLM 中的一组抽样入学报告生成开放式问题和答案，包括 ERNIE 4、ChatGLM 4、Doubao、Hunyuan、Spark 4、Qwen、Llama 3 和 Mistral。此外，我们聘请了医学专家从多个维度手动评估这些开放式问题和答案。研究发现，LLM 在使用少量提示后，可以有效模拟现实世界的医学资格考试问题，但生成答案的正确性、基于证据的陈述和专业性仍有改进空间。此外，LLM 还展示了相当好的纠正和修正参考答案的能力。鉴于人工智能在医学领域的巨大潜力，针对医学生、实习医生和住院医生的医学资格考试生成问题和答案的任务可以成为未来研究的重要重点。</li>
</ul>

<h3>Title: What is Wrong with Perplexity for Long-context Language Modeling?</h3>
<ul>
<li><strong>Authors: </strong>Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, Yisen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23771">https://arxiv.org/abs/2410.23771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23771">https://arxiv.org/pdf/2410.23771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23771]] What is Wrong with Perplexity for Long-context Language Modeling?(https://arxiv.org/abs/2410.23771)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Handling long-context inputs is crucial for large language models (LLMs) in tasks such as extended conversations, document summarization, and many-shot in-context learning. While recent approaches have extended the context windows of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has proven unreliable for assessing long-context capabilities. The underlying cause of this limitation has remained unclear. In this work, we provide a comprehensive explanation for this issue. We find that PPL overlooks key tokens, which are essential for long-context understanding, by averaging across all tokens and thereby obscuring the true performance of models in long-context scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that focuses on key tokens by employing a long-short context contrastive method to identify them. Our experiments demonstrate that LongPPL strongly correlates with performance on various long-context benchmarks (e.g., Pearson correlation of -0.96), significantly outperforming traditional PPL in predictive accuracy. Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a re-weighting strategy for fine-tuning that prioritizes key tokens, leading to consistent improvements across diverse benchmarks. In summary, these contributions offer deeper insights into the limitations of PPL and present effective solutions for accurately evaluating and enhancing the long-context capabilities of LLMs. Code is available at this https URL.</li>
<li><strong>摘要：</strong>对于大型语言模型 (LLM)，处理长上下文输入至关重要，因为这些模型可用于执行诸如扩展对话、文档摘要和多镜头上下文学习等任务。虽然最近的方法扩展了 LLM 的上下文窗口，并使用困惑度 (PPL) 作为标准评估指标，但事实证明 PPL 无法可靠地评估长上下文能力。造成这种限制的根本原因仍不清楚。在本文中，我们对这个问题进行了全面的解释。我们发现 PPL 忽略了对长上下文理解至关重要的关键标记，因为它对所有标记进行平均，从而掩盖了模型在长上下文场景中的真实性能。为了解决这个问题，我们提出了 \textbf{LongPPL}，这是一种新颖的指标，它通过采用长短上下文对比方法来识别关键标记，从而专注于关键标记。我们的实验表明，LongPPL 与各种长上下文基准测试上的表现密切相关（例如，皮尔逊相关性为 -0.96），在预测准确性方面明显优于传统 PPL。此外，我们引入了 \textbf{LongCE}（长上下文交叉熵）损失，这是一种重新加权的微调策略，可优先考虑关键标记，从而实现跨不同基准测试的持续改进。总之，这些贡献提供了对 PPL 局限性的更深入见解，并提出了准确评估和增强 LLM 长上下文功能的有效解决方案。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages</h3>
<ul>
<li><strong>Authors: </strong>Amir Hossein Kargaran, François Yvon, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23825">https://arxiv.org/abs/2410.23825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23825">https://arxiv.org/pdf/2410.23825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23825]] GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages(https://arxiv.org/abs/2410.23825)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it - including the pipeline, language identification model, and filters - available to the research community. Corpus v. 1.0 this https URL, Pipeline v. 3.0 this https URL.</li>
<li><strong>摘要：</strong>随着预训练语言模型的出现，尤其是这些模型的缩放定律的发现，对大型文本语料库的需求日益增加。大多数可用语料库仅对具有大量主导社区的语言具有足够的数据。但是，没有可用的语料库能够 (i) 涵盖广泛的少数民族语言；(ii) 由开源可重现的管道生成；(iii) 严格清除噪音，使其值得信赖。我们提出了 GlotCC，这是一个干净的、文档级的、2TB 通用领域语料库，源自 CommonCrawl，涵盖 1000 多种语言。我们向研究界开放了 GlotCC 和用于生成它的系统（包括管道、语言识别模型和过滤器）。语料库 v. 1.0 此 https URL，管道 v. 3.0 此 https URL。</li>
</ul>

<h3>Title: Commonsense Knowledge Editing Based on Free-Text in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiusheng Huang, Yequan Wang, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23844">https://arxiv.org/abs/2410.23844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23844">https://arxiv.org/pdf/2410.23844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23844]] Commonsense Knowledge Editing Based on Free-Text in LLMs(https://arxiv.org/abs/2410.23844)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.</li>
<li><strong>摘要：</strong>知识编辑技术对于保持大型语言模型（LLM）的准确性和时效性至关重要，但该任务的设置忽略了现实世界中大量基于自由文本的常识性知识，其特点是知识范围广、内容长、非实例化。先前的方法（例如MEMIT）的编辑对象是单个token或entity，不适用于自由文本形式的常识性知识。针对上述挑战，我们从知识本地化和知识编辑两个角度进行了实验。首先，我们引入了自由文本的知识本地化（KLFT）方法，揭示了常识性知识在MLP和Attention层中的分布以及分散分布所带来的挑战。接下来，我们提出一种动态感知编辑方法（DEM），利用动态感知模块定位与常识性知识相对应的参数位置，并使用知识编辑模块来更新知识。 DEM方法充分挖掘了MLP和Attention层的潜力，成功实现了基于自由文本的常识性知识的编辑，实验结果表明DEM能够取得优异的编辑性能。</li>
</ul>

<h3>Title: Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?</h3>
<ul>
<li><strong>Authors: </strong>Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, Bo Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23856">https://arxiv.org/abs/2410.23856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23856">https://arxiv.org/pdf/2410.23856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23856]] Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?(https://arxiv.org/abs/2410.23856)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, base LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more drastically by 2.2%-40.4% with inaccurate thoughts. Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: this https URL.</li>
<li><strong>摘要：</strong>本文探讨了大型语言模型 (LLM) 中一个尚未得到充分探索的挑战：带有嘈杂原理的思路链提示，其中包括用于上下文学习的示例中不相关或不准确的推理想法。我们构建了 NoRa 数据集，专门用于评估在存在嘈杂原理的情况下推理的稳健性。我们在 NoRa 数据集上的发现揭示了当前 LLM 普遍容易受到此类噪声的影响，而现有的稳健方法（如自我校正和自我一致性）的有效性有限。值得注意的是，与使用干净原理的提示相比，基础 LLM 在不相关想法的准确率下降了 1.4%-19.8%，在不准确想法的准确率下降了 2.2%-40.4%。应对这一挑战需要外部监督，而这种监督应该在实践中是可以实现的。在这里，我们提出了一种带有噪声思路链 (CD-CoT) 的对比去噪方法。它通过将嘈杂的原理与只有一个干净的原理进行对比来增强 LLM 的去噪推理能力，这可能是去噪目的提示的最低要求。该方法遵循探索和利用的原则：（1）在输入空间中重新表述和选择原理以实现显式去噪；（2）探索不同的推理路径并在输出空间中对答案进行投票。从经验上看，CD-CoT 的准确率比基础模型平均提高了 17.8%，并且显示出比基线方法强得多的去噪能力。源代码可公开获取：此 https URL。</li>
</ul>

<h3>Title: Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23861">https://arxiv.org/abs/2410.23861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23861">https://arxiv.org/pdf/2410.23861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23861]] Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models(https://arxiv.org/abs/2410.23861)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Multimodal Models (LMMs) have demonstrated the ability to interact with humans under real-world conditions by combining Large Language Models (LLMs) and modality encoders to align multimodal information (visual and auditory) with text. However, such models raise new safety challenges of whether models that are safety-aligned on text also exhibit consistent safeguards for multimodal inputs. Despite recent safety-alignment research on vision LMMs, the safety of audio LMMs remains under-explored. In this work, we comprehensively red team the safety of five advanced audio LMMs under three settings: (i) harmful questions in both audio and text formats, (ii) harmful questions in text format accompanied by distracting non-speech audio, and (iii) speech-specific jailbreaks. Our results under these settings demonstrate that open-source audio LMMs suffer an average attack success rate of 69.14% on harmful audio questions, and exhibit safety vulnerabilities when distracted with non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro achieve an attack success rate of 70.67% on the harmful query benchmark. We provide insights on what could cause these reported safety-misalignments. Warning: this paper contains offensive examples.</li>
<li><strong>摘要：</strong>大型多模态模型 (LMM) 已证明能够通过结合大型语言模型 (LLM) 和模态编码器将多模态信息（视觉和听觉）与文本对齐，在现实条件下与人类进行交互。然而，这样的模型提出了新的安全挑战，即在文本上安全对齐的模型是否也对多模态输入表现出一致的保护措施。尽管最近对视觉 LMM 进行了安全对齐研究，但音频 LMM 的安全性仍未得到充分探索。在这项工作中，我们在三种设置下全面对五种高级音频 LMM 的安全性进行了红队测试：(i) 音频和文本格式的有害问题，(ii) 伴有分散注意力的非语音音频的文本格式的有害问题，以及 (iii) 特定于语音的越狱。我们在这些设置下的结果表明，开源音频 LMM 在有害音频问题上的平均攻击成功率为 69.14%，并且在受到非语音音频噪音的干扰时表现出安全漏洞。我们对 Gemini-1.5-Pro 上的语音特定越狱在有害查询基准上实现了 70.67% 的攻击成功率。我们提供了有关可能导致这些报告的安全失调的原因的见解。警告：本文包含令人反感的示例。</li>
</ul>

<h3>Title: 'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Rena Gao, Xuetong Wu, Siwen Luo, Caren Han, Feng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23883">https://arxiv.org/abs/2410.23883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23883">https://arxiv.org/pdf/2410.23883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23883]] 'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue(https://arxiv.org/abs/2410.23883)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection in multimodal contexts is essential for identifying deviations in combined inputs from different modalities, particularly in applications like open-domain dialogue systems or real-life dialogue interactions. This paper aims to improve the user experience that involves multi-round long dialogues by efficiently detecting OOD dialogues and images. We introduce a novel scoring framework named Dialogue Image Aligning and Enhancing Framework (DIAEF) that integrates the visual language models with the novel proposed scores that detect OOD in two key scenarios (1) mismatches between the dialogue and image input pair and (2) input pairs with previously unseen labels. Our experimental results, derived from various benchmarks, demonstrate that integrating image and multi-round dialogue OOD detection is more effective with previously unseen labels than using either modality independently. In the presence of mismatched pairs, our proposed score effectively identifies these mismatches and demonstrates strong robustness in long dialogues. This approach enhances domain-aware, adaptive conversational agents and establishes baselines for future studies.</li>
<li><strong>摘要：</strong>多模态环境中的分布外 (OOD) 检测对于识别不同模态组合输入中的偏差至关重要，尤其是在开放域对话系统或现实对话交互等应用中。本文旨在通过有效检测 OOD 对话和图像来改善涉及多轮长对话的用户体验。我们引入了一种名为“对话图像对齐和增强框架” (DIAEF) 的新型评分框架，该框架将视觉语言模型与新提出的分数相结合，可在两种关键场景中检测 OOD (1) 对话和图像输入对不匹配和 (2) 输入对具有以前未见过的标签。我们的实验结果来自各种基准，表明集成图像和多轮对话 OOD 检测比单独使用任何一种模态更有效。在存在不匹配对的情况下，我们提出的分数可以有效识别这些不匹配，并在长对话中表现出强大的鲁棒性。这种方法增强了领域感知、自适应的对话代理，并为未来的研究建立了基线。</li>
</ul>

<h3>Title: Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource languages</h3>
<ul>
<li><strong>Authors: </strong>Séamus Lankford, Andy Way</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23890">https://arxiv.org/abs/2410.23890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23890">https://arxiv.org/pdf/2410.23890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23890]] Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource languages(https://arxiv.org/abs/2410.23890)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In an evolving landscape of crisis communication, the need for robust and adaptable Machine Translation (MT) systems is more pressing than ever, particularly for low-resource languages. This study presents a comprehensive exploration of leveraging Large Language Models (LLMs) and Multilingual LLMs (MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique challenges posed by crisis situations where speed, accuracy, and the ability to handle a wide range of languages are paramount, this research outlines a novel approach that combines the cutting-edge capabilities of LLMs with fine-tuning techniques and community-driven corpus development strategies. At the core of this study is the development and empirical evaluation of MT systems tailored for two low-resource language pairs, illustrating the process from initial model selection and fine-tuning through to deployment. Bespoke systems are developed and modelled on the recent Covid-19 pandemic. The research highlights the importance of community involvement in creating highly specialised, crisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM models. It identifies fine-tuned MLLM models as offering superior performance compared with their LLM counterparts. A scalable and replicable model for rapid MT system development in crisis scenarios is outlined. Our approach enhances the field of humanitarian technology by offering a blueprint for developing multilingual communication systems during emergencies.</li>
<li><strong>摘要：</strong>在不断发展的危机沟通环境中，对强大且适应性强的机器翻译 (MT) 系统的需求比以往任何时候都更加迫切，尤其是对于资源匮乏的语言。本研究全面探索了如何利用大型语言模型 (LLM) 和多语言 LLM (MLLM) 来增强此类场景中的 MT 功能。通过关注危机情况下带来的独特挑战，在这些情况下，速度、准确性和处理多种语言的能力至关重要，本研究概述了一种新颖的方法，该方法将 LLM 的尖端功能与微调技术和社区驱动的语料库开发策略相结合。本研究的核心是为两对资源匮乏的语言量身定制的 MT 系统的开发和实证评估，说明了从初始模型选择和微调到部署的过程。定制系统是根据最近的 Covid-19 大流行开发的和建模的。该研究强调了社区参与在创建高度专业化、危机特定的数据集方面的重要性，并将定制 GPT 与适应 NLLB 的 MLLM 模型进行了比较。它指出经过微调的 MLLM 模型与 LLM 模型相比具有更优异的性能。概述了危机情况下快速开发 MT 系统的可扩展且可复制的模型。我们的方法通过提供在紧急情况下开发多语言通信系统的蓝图来增强人道主义技术领域。</li>
</ul>

<h3>Title: Responsible Retrieval Augmented Generation for Climate Decision Making from Documents</h3>
<ul>
<li><strong>Authors: </strong>Matyas Juhasz, Kalyan Dutia, Henry Franks, Conor Delahunty, Patrick Fawbert Mills, Harrison Pim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23902">https://arxiv.org/abs/2410.23902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23902">https://arxiv.org/pdf/2410.23902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23902]] Responsible Retrieval Augmented Generation for Climate Decision Making from Documents(https://arxiv.org/abs/2410.23902)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Climate decision making is constrained by the complexity and inaccessibility of key information within lengthy, technical, and multi-lingual documents. Generative AI technologies offer a promising route for improving the accessibility of information contained within these documents, but suffer from limitations. These include (1) a tendency to hallucinate or mis-represent information, (2) difficulty in steering or guaranteeing properties of generated output, and (3) reduced performance in specific technical domains. To address these challenges, we introduce a novel evaluation framework with domain-specific dimensions tailored for climate-related documents. We then apply this framework to evaluate Retrieval-Augmented Generation (RAG) approaches and assess retrieval- and generation-quality within a prototype tool that answers questions about individual climate law and policy documents. In addition, we publish a human-annotated dataset and scalable automated evaluation tools, with the aim of facilitating broader adoption and robust assessment of these systems in the climate domain. Our findings highlight the key components of responsible deployment of RAG to enhance decision-making, while also providing insights into user experience (UX) considerations for safely deploying such systems to build trust with users in high-risk domains.</li>
<li><strong>摘要：</strong>气候决策受到冗长、技术性和多语言文档中关键信息的复杂性和不可访问性的制约。生成式人工智能技术为提高这些文档中所含信息的可访问性提供了一种有希望的途径，但也存在局限性。这些局限性包括 (1) 容易产生幻觉或歪曲信息，(2) 难以控制或保证生成输出的属性，以及 (3) 特定技术领域的性能下降。为了应对这些挑战，我们引入了一个新颖的评估框架，该框架具有针对气候相关文档量身定制的领域特定维度。然后，我们应用该框架来评估检索增强生成 (RAG) 方法，并在原型工具中评估检索和生成质量，该工具可回答有关单个气候法律和政策文件的问题。此外，我们发布了人工注释的数据集和可扩展的自动评估工具，旨在促进这些系统在气候领域的更广泛采用和稳健评估。我们的研究结果强调了负责任地部署 RAG 以增强决策能力的关键组成部分，同时还提供了对用户体验 (UX) 考虑的见解，以便安全地部署此类系统以与高风险领域的用户建立信任。</li>
</ul>

<h3>Title: BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments</h3>
<ul>
<li><strong>Authors: </strong>Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23918">https://arxiv.org/abs/2410.23918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23918">https://arxiv.org/pdf/2410.23918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23918]] BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments(https://arxiv.org/abs/2410.23918)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from \textit{capability} to \textit{availability}, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce \textbf{BitStack}, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经彻底改变了众多应用程序，但它们的部署仍然受到本地设备内存限制的挑战。虽然缩放定律增强了 LLM 功能，但主要瓶颈已从 \textit{能力} 转移到 \textit{可用性}，强调了高效内存管理的需求。传统的压缩方法（例如量化）通常需要预定义的压缩率和针对每种设置的单独压缩过程，这使得在可变内存环境中的部署变得复杂。在本文中，我们介绍了 \textbf{BitStack}，这是一种新颖的、无需训练的权重压缩方法，可在内存使用量和模型性能之间实现兆字节级的权衡。通过利用权重分解，BitStack 可以动态调整模型大小，同时最大限度地减少运行内存和存储设备之间的传输。我们的方法在考虑每个参数的重要性的同时迭代分解权重矩阵，从而在每次分解迭代中产生每个参数大约 1 位的残差块。这些块作为基本传输单元在存储中进行排序和堆叠，根据当前内存可用性加载不同的数量。针对各种任务进行的大量实验表明，尽管提供了细粒度的尺寸控制，但 BitStack 始终达到或超过强量化基线，尤其是在极端压缩率下。据我们所知，这是第一种基于分解的方法，可以有效地弥补与量化等实用压缩技术的差距。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Language Models can Self-Lengthen to Generate Long Texts</h3>
<ul>
<li><strong>Authors: </strong>Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23933">https://arxiv.org/abs/2410.23933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23933">https://arxiv.org/pdf/2410.23933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23933]] Language Models can Self-Lengthen to Generate Long Texts(https://arxiv.org/abs/2410.23933)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet a notable gap remains in generating long, aligned outputs. This limitation stems from a training gap where pre-training lacks effective instructions for long-text generation, and post-training data primarily consists of short query-response pairs. Current approaches, such as instruction backtranslation and behavior imitation, face challenges including data quality, copyright issues, and constraints on proprietary model usage. In this paper, we introduce an innovative iterative training framework called Self-Lengthen that leverages only the intrinsic knowledge and skills of LLMs without the need for auxiliary data or proprietary models. The framework consists of two roles: the Generator and the Extender. The Generator produces the initial response, which is then split and expanded by the Extender. This process results in a new, longer response, which is used to train both the Generator and the Extender iteratively. Through this process, the models are progressively trained to handle increasingly longer responses. Experiments on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long-text generation, when applied to top open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展显著增强了其处理长上下文的能力，但在生成长对齐输出方面仍然存在明显差距。这种限制源于训练差距，即预训练缺乏有效的长文本生成指令，而训练后数据主要由短查询-响应对组成。当前的方法，例如指令回译和行为模仿，面临着数据质量、版权问题和专有模型使用限制等挑战。在本文中，我们介绍了一种名为 Self-Lengthen 的创新迭代训练框架，它仅利用 LLM 的固有知识和技能，而无需辅助数据或专有模型。该框架由两个角色组成：生成器和扩展器。生成器生成初始响应，然后由扩展器对其进行拆分和扩展。此过程会产生一个新的更长的响应，用于迭代训练生成器和扩展器。通过此过程，模型逐渐得到训练以处理越来越长的响应。基准测试和人工评估的实验表明，当应用于 Qwen2 和 LLaMA3 等顶级开源 LLM 时，Self-Lengthen 在长文本生成方面的表现优于现有方法。我们的代码在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single Source Language</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, Yihong Chen, Raphael Tang, Pontus Stenetorp</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23956">https://arxiv.org/abs/2410.23956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23956">https://arxiv.org/pdf/2410.23956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23956]] Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single Source Language(https://arxiv.org/abs/2410.23956)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>English, as a very high-resource language, enables the pretraining of high-quality large language models (LLMs). The same cannot be said for most other languages, as leading LLMs still underperform for non-English languages, likely due to a gap in the quality and diversity of the available multilingual pretraining corpora. In this work, we find that machine-translated text from a single high-quality source language can contribute significantly to the pretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality English web dataset, into French, German, and Spanish, resulting in a final 300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter model, CuatroLLM, from scratch on this dataset. Across five non-English reasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art multilingual models trained using closed data, such as Llama3.2 and Gemma2, despite using an order of magnitude less data, such as about 6% of the tokens used for Llama3.2's training. We further demonstrate that with additional domain-specific pretraining, amounting to less than 1% of TransWeb-Edu, CuatroLLM surpasses the state of the art in multilingual reasoning. To promote reproducibility, we release our corpus, models, and training pipeline under open licenses at this http URL.</li>
<li><strong>摘要：</strong>英语是一种资源非常丰富的语言，可以预训练高质量的大型语言模型 (LLM)。但大多数其他语言的情况并非如此，因为领先的 LLM 在非英语语言方面仍然表现不佳，这可能是由于可用的多语言预训练语料库的质量和多样性存在差距。在这项工作中，我们发现来自单一高质量源语言的机器翻译文本可以为多语言 LLM 的预训练做出重大贡献。我们将高质量的英语网络数据集 FineWeb-Edu 翻译成法语、德语和西班牙语，最终得到一个 300B 标记数据集，我们称之为 TransWeb-Edu，并在此数据集上从头开始预训练一个 1.3B 参数模型 CuatroLLM。在五项非英语推理任务中，我们表明 CuatroLLM 的表现与使用封闭数据训练的最先进的多语言模型（如 Llama3.2 和 Gemma2）相当或更胜一筹，尽管使用的数据量少了一个数量级，例如训练 Llama3.2 所用的标记的约 6%。我们进一步证明，通过额外的领域特定预训练（不到 TransWeb-Edu 的 1%），CuatroLLM 在多语言推理方面超越了最先进的水平。为了提高可重复性，我们在此 http URL 上以开放许可发布了我们的语料库、模型和训练流程。</li>
</ul>

<h3>Title: Speech is More Than Words: Do Speech-to-Text Translation Systems Leverage Prosody?</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Tsiamas, Matthias Sperber, Andrew Finch, Sarthak Garg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24019">https://arxiv.org/abs/2410.24019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24019">https://arxiv.org/pdf/2410.24019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24019]] Speech is More Than Words: Do Speech-to-Text Translation Systems Leverage Prosody?(https://arxiv.org/abs/2410.24019)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The prosody of a spoken utterance, including features like stress, intonation and rhythm, can significantly affect the underlying semantics, and as a consequence can also affect its textual translation. Nevertheless, prosody is rarely studied within the context of speech-to-text translation (S2TT) systems. In particular, end-to-end (E2E) systems have been proposed as well-suited for prosody-aware translation because they have direct access to the speech signal when making translation decisions, but the understanding of whether this is successful in practice is still limited. A main challenge is the difficulty of evaluating prosody awareness in translation. To address this challenge, we introduce an evaluation methodology and a focused benchmark (named ContraProST) aimed at capturing a wide range of prosodic phenomena. Our methodology uses large language models and controllable text-to-speech (TTS) to generate contrastive examples. Through experiments in translating English speech into German, Spanish, and Japanese, we find that (a) S2TT models possess some internal representation of prosody, but the prosody signal is often not strong enough to affect the translations, (b) E2E systems outperform cascades of speech recognition and text translation systems, confirming their theoretical advantage in this regard, and (c) certain cascaded systems also capture prosodic information in the translation, but only to a lesser extent that depends on the particulars of the transcript's surface form.</li>
<li><strong>摘要：</strong>口语的韵律，包括重音、语调和节奏等特征，会显著影响底层语义，从而影响文本翻译。尽管如此，在语音到文本翻译 (S2TT) 系统的背景下，韵律很少得到研究。特别是，端到端 (E2E) 系统被认为非常适合韵律感知翻译，因为它们在做出翻译决策时可以直接访问语音信号，但对于这在实践中是否成功的理解仍然有限。一个主要的挑战是难以评估翻译中的韵律意识。为了应对这一挑战，我们引入了一种评估方法和一个重点基准（名为 ContraProST），旨在捕捉各种韵律现象。我们的方法使用大型语言模型和可控的文本到语音 (TTS) 来生成对比示例。通过将英语语音翻译成德语、西班牙语和日语的实验，我们发现：（a）S2TT 模型具有一些韵律的内部表示，但韵律信号通常不足以影响翻译；（b）E2E 系统的表现优于语音识别和文本翻译系统的级联，证实了它们在这方面的理论优势；（c）某些级联系统也会在翻译中捕获韵律信息，但程度较小，这取决于成绩单表面形式的细节。</li>
</ul>

<h3>Title: Detecting text level intellectual influence with knowledge graph embeddings</h3>
<ul>
<li><strong>Authors: </strong>Lucian Li, Eryclis Silva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24021">https://arxiv.org/abs/2410.24021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24021">https://arxiv.org/pdf/2410.24021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24021]] Detecting text level intellectual influence with knowledge graph embeddings(https://arxiv.org/abs/2410.24021)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Introduction: Tracing the spread of ideas and the presence of influence is a question of special importance across a wide range of disciplines, ranging from intellectual history to cultural analytics, computational social science, and the science of science. Method: We collect a corpus of open source journal articles, generate Knowledge Graph representations using the Gemini LLM, and attempt to predict the existence of citations between sampled pairs of articles using previously published methods and a novel Graph Neural Network based embedding model. Results: We demonstrate that our knowledge graph embedding method is superior at distinguishing pairs of articles with and without citation. Once trained, it runs efficiently and can be fine-tuned on specific corpora to suit individual researcher needs. Conclusion(s): This experiment demonstrates that the relationships encoded in a knowledge graph, especially the types of concepts brought together by specific relations can encode information capable of revealing intellectual influence. This suggests that further work in analyzing document level knowledge graphs to understand latent structures could provide valuable insights.</li>
<li><strong>摘要：</strong>简介：追踪思想的传播和影响力的存在是一个跨学科的重要问题，涉及从思想史到文化分析、计算社会科学和科学科学等各个领域。方法：我们收集开源期刊文章语料库，使用 Gemini LLM 生成知识图谱表示，并尝试使用以前发布的方法和基于图神经网络的新型嵌入模型来预测抽样文章对之间引用的存在。结果：我们证明我们的知识图谱嵌入方法在区分有引文和无引文的文章对方面更胜一筹。经过训练后，它可以高效运行，并且可以在特定语料库上进行微调以满足个别研究人员的需求。结论：该实验表明，知识图谱中编码的关系，尤其是由特定关系汇集在一起​​的概念类型，可以编码能够揭示智力影响的信息。这表明，进一步分析文档级知识图谱以了解潜在结构可以提供有价值的见解。</li>
</ul>

<h3>Title: Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs</h3>
<ul>
<li><strong>Authors: </strong>Muhammed Saeed, Elgizouli Mohamed, Mukhtar Mohamed, Shaina Raza, Shady Shehata, Muhammad Abdul-Mageed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24049">https://arxiv.org/abs/2410.24049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24049">https://arxiv.org/pdf/2410.24049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24049]] Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs(https://arxiv.org/abs/2410.24049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are widely used but raise ethical concerns due to embedded social biases. This study examines LLM biases against Arabs versus Westerners across eight domains, including women's rights, terrorism, and anti-Semitism and assesses model resistance to perpetuating these biases. To this end, we create two datasets: one to evaluate LLM bias toward Arabs versus Westerners and another to test model safety against prompts that exaggerate negative traits ("jailbreaks"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA 3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases displaying negative biases toward Arabs, with LlaMA 3.1-405B being the most biased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being an optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except Claude exhibit attack success rates above 87% in three categories. We also find Claude 3.5 Sonnet the safest, but it still displays biases in seven of eight categories. Despite being an optimized version of GPT4, We find GPT-4o to be more prone to biases and jailbreaks, suggesting optimization flaws. Our findings underscore the pressing need for more robust bias mitigation strategies and strengthened security measures in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 被广泛使用，但由于其根深蒂固的社会偏见而引发了道德问题。本研究考察了 LLM 在八个领域对阿拉伯人和西方人的偏见，包括妇女权利、恐怖主义和反犹太主义，并评估了模型对延续这些偏见的抵抗力。为此，我们创建了两个数据集：一个用于评估 LLM 对阿拉伯人与西方人的偏见，另一个用于测试模型对夸大负面特征的提示（“越狱”）的安全性。我们评估了六个 LLM——GPT-4、GPT-4o、LlaMA 3.1 (8B 和 405B)、Mistral 7B 和 Claude 3.5 Sonnet。我们发现 79% 的案例表现出对阿拉伯人的负面偏见，其中 LlaMA 3.1-405B 的偏见最严重。我们的越狱测试显示，尽管 GPT-4o 是优化版本，但它是最脆弱的，其次是 LlaMA 3.1-8B 和 Mistral 7B。除 Claude 之外，所有 LLM 在三个类别中的攻击成功率均超过 87%。我们还发现 Claude 3.5 Sonnet 最安全，但它在八个类别中的七个中仍然表现出偏见。尽管是 GPT4 的优化版本，但我们发现 GPT-4o 更容易出现偏见和越狱，这表明存在优化缺陷。我们的研究结果强调了 LLM 迫切需要更强大的偏见缓解策略和加强安全措施。</li>
</ul>

<h3>Title: Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jinghan Zhang, Fengran Mo, Xiting Wang, Kunpeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24155">https://arxiv.org/abs/2410.24155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24155">https://arxiv.org/pdf/2410.24155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24155]] Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning(https://arxiv.org/abs/2410.24155)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated their potential in handling complex reasoning tasks, which are usually achieved by constructing a thought chain to guide the model to solve the problem with multi-step thinking. However, existing methods often remain confined to previously explored solution spaces and thus overlook the critical blind spot within LLMs' cognitive range. To address these issues, we design the Thought Space Explorer (TSE), a novel framework to expand and optimize thought structures to guide LLMs to explore their blind spots of thinking. By generating new reasoning steps and branches based on the original thought structure with various designed strategies, TSE broadens the thought space and alleviates the impact of blind spots for LLM reasoning. Experimental results on multiple levels of reasoning tasks demonstrate the efficacy of TSE. We also conduct extensive analysis to understand how structured and expansive thought can contribute to unleashing the potential of LLM reasoning capabilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展已展示出其在处理复杂推理任务方面的潜力，这通常是通过构建思维链来引导模型以多步骤思维解决问题来实现的。然而，现有的方法往往局限于先前探索过的解决方案空间，从而忽略了 LLM 认知范围内的关键盲点。为了解决这些问题，我们设计了思维空间探索器 (TSE)，这是一个扩展和优化思维结构的新框架，可引导 LLM 探索其思维盲点。通过使用各种设计策略，TSE 基于原始思维结构生成新的推理步骤和分支，拓宽了思维空间并减轻了盲点对 LLM 推理的影响。在多个层次的推理任务上的实验结果证明了 TSE 的有效性。我们还进行了广泛的分析，以了解结构化和扩展性思维如何有助于释放 LLM 推理能力的潜力。</li>
</ul>

<h3>Title: GPT or BERT: why not both?</h3>
<ul>
<li><strong>Authors: </strong>Lucas Georges Gabriel Charpentier, David Samuel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24159">https://arxiv.org/abs/2410.24159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24159">https://arxiv.org/pdf/2410.24159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24159]] GPT or BERT: why not both?(https://arxiv.org/abs/2410.24159)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack: GPT-BERT can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code.</li>
<li><strong>摘要：</strong>我们提出了一种将掩码语言建模与因果语言建模相结合的简单方法。这种混合训练目标产生的模型将两种建模范式的优势结合在一个转换器堆栈中：GPT-BERT 可以像任何标准因果或掩码语言模型一样透明地使用。我们在 BabyLM Challenge 2024 上测试了实现这种灵活行为的预训练过程。结果表明，混合预训练优于仅掩码或仅因果模型。我们公开发布了模型、训练语料库和代码。</li>
</ul>

<h3>Title: Constraint Back-translation Improves Complex Instruction Following of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24175">https://arxiv.org/abs/2410.24175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24175">https://arxiv.org/pdf/2410.24175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24175]] Constraint Back-translation Improves Complex Instruction Following of Large Language Models(https://arxiv.org/abs/2410.24175)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 难以遵循格式、长度等复杂约束的指令。按照传统的指令调优做法，先前的研究对通过将复杂指令输入高级 LLM 生成的复杂指令-响应对进行后训练。然而，即使是高级 LLM 也无法很好地遵循复杂指令，从而限制了生成数据的质量。在这项工作中，我们发现现有数据集固有地包含隐式复杂约束，并提出了一种新颖的数据生成技术——约束反向翻译。具体来说，我们采用现有数据集中的高质量指令-响应对，仅采用高级 LLM 将响应已经满足的复杂约束添加到指令中，这自然会降低成本和数据噪音。在实验中，我们采用 Llama3-70B-Instruct 反向翻译约束并创建一个名为 CRAB 的高质量复杂指令-响应数据集。我们表明，在 CRAB 上进行后训练可以提高多个骨干 LLM 的复杂指令跟随能力，并在广泛的指令跟随基准上进行了评估。我们进一步发现，约束反向翻译在后期训练中也是一个有用的辅助训练目标。我们的代码、数据和模型将发布以促进未来的研究。</li>
</ul>

<h3>Title: Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters</h3>
<ul>
<li><strong>Authors: </strong>Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24190">https://arxiv.org/abs/2410.24190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24190">https://arxiv.org/pdf/2410.24190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24190]] Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters(https://arxiv.org/abs/2410.24190)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>How could LLMs influence our democracy? We investigate LLMs' political leanings and the potential influence of LLMs on voters by conducting multiple experiments in a U.S. presidential election context. Through a voting simulation, we first demonstrate 18 open- and closed-weight LLMs' political preference for a Democratic nominee over a Republican nominee. We show how this leaning towards the Democratic nominee becomes more pronounced in instruction-tuned models compared to their base versions by analyzing their responses to candidate-policy related questions. We further explore the potential impact of LLMs on voter choice by conducting an experiment with 935 U.S. registered voters. During the experiments, participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse. This effect is larger than many previous studies on the persuasiveness of political campaigns, which have shown minimal effects in presidential elections. Many users also expressed a desire for further political interaction with LLMs. Which aspects of LLM interactions drove these shifts in voter choice requires further study. Lastly, we explore how a safety method can make LLMs more politically neutral, while leaving some open questions.</li>
<li><strong>摘要：</strong>LLM 如何影响我们的民主？我们通过在美国总统选举背景下进行多项实验，研究了 LLM 的政治倾向及其对选民的潜在影响。通过投票模拟，我们首先展示了 18 名开放和封闭权重的 LLM 对民主党候选人的政治偏好，而不是共和党候选人。我们通过分析他们对候选人政策相关问题的回答，展示了这种对民主党候选人的倾向在指令调整模型中如何比其基本版本更加明显。我们通过对 935 名美国登记选民进行实验，进一步探索了 LLM 对选民选择的潜在影响。在实验过程中，参与者与 LLM（Claude-3、Llama-3 和 GPT-4）进行了五次交流。实验结果显示，在与 LLM 互动后，选民的选择转向了民主党候选人，投票率从 0.7% 扩大到 4.6%，尽管 LLM 在讨论过程中没有被要求说服用户支持民主党候选人。这种影响比许多之前关于政治竞选说服力的研究要大，这些研究表明，政治竞选在总统选举中的影响微乎其微。许多用户还表示希望与 LLM 进行进一步的政治互动。LLM 互动的哪些方面推动了选民选择的转变，这需要进一步研究。最后，我们探讨了安全方法如何使 LLM 在政治上更加中立，同时留下一些悬而未决的问题。</li>
</ul>

<h3>Title: SelfCodeAlign: Self-Alignment for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24198">https://arxiv.org/abs/2410.24198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24198">https://arxiv.org/pdf/2410.24198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24198]] SelfCodeAlign: Self-Alignment for Code Generation(https://arxiv.org/abs/2410.24198)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.</li>
<li><strong>摘要：</strong>指令调优是一种监督式微调方法，可显著提高大型语言模型 (LLM) 遵循人类指令的能力。我们提出了 SelfCodeAlign，这是第一个完全透明且宽容的自对齐代码 LLM 管道，无需大量人工注释或提炼。SelfCodeAlign 在整个数据生成过程中使用相同的基础模型进行推理。它首先从高质量的种子片段中提取不同的编码概念来生成新任务。然后，它对每个任务采样多个响应，将每个响应与测试用例配对，并在沙盒环境中对其进行验证。最后，选择通过的示例进行指令调优。在我们的主要实验中，我们使用 SelfCodeAlign 和 CodeQwen1.5-7B 来生成一个包含 74k 个指令-响应对的数据集。对该数据集进行微调可使模型在 HumanEval+ 上实现 67.1 pass@1，超越 CodeLlama-70B-Instruct，尽管规模小十倍。在所有基准测试中，这个经过微调的模型始终优于使用 OctoPack 训练的原始版本，OctoPack 是之前最先进的无需人工注释或提炼的指令调整方法。此外，我们表明 SelfCodeAlign 适用于各种大小的 LLM，从 3B 到 33B，并且基础模型可以从与自身数据分布的对齐中受益更多。我们进一步验证了管道中每个组件的有效性，表明 SelfCodeAlign 优于 GPT-4o 的直接提炼和领先的基于 GPT-3.5 的提炼方法，例如 OSS-Instruct 和 Evol-Instruct。SelfCodeAlign 还促成了 StarCoder2-Instruct 的创建，这是第一个完全透明、许可宽松且自对齐的代码 LLM，可实现最先进的编码性能。</li>
</ul>

<h3>Title: Length-Induced Embedding Collapse in Transformer-based Models</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24200">https://arxiv.org/abs/2410.24200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24200">https://arxiv.org/pdf/2410.24200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24200]] Length-Induced Embedding Collapse in Transformer-based Models(https://arxiv.org/abs/2410.24200)</code><input type="text"></li>
<li><strong>Keywords: </strong>long context</a></li>
<li><strong>Abstract: </strong>Text embeddings enable various applications, but their performance deteriorates on longer texts. In this paper, we find that the performance degradation is due to a phenomenon called Length Collapse, where longer text embeddings collapse into a narrow space. This collapse results in a distributional inconsistency between embeddings of different text lengths, ultimately hurting the performance of downstream tasks. Theoretically, by considering the self-attention mechanism inherently functions as a low-pass filter, we prove that long sequences increase the attenuation rate of the low-pass filter effect of the self-attention mechanism. With layers going deeper, excessive low-pass filtering causes the token signals to retain only their Direct-Current (DC) component, which means the input token feature maps will collapse into a narrow space, especially in long texts. Based on the above analysis, we propose to mitigate the undesirable length collapse limitation by introducing a temperature in softmax(), which achieves a higher low-filter attenuation rate. The tuning-free method, called TempScale, can be plugged into multiple transformer-based embedding models. Empirically, we demonstrate that TempScale can improve existing embedding models, especially on long text inputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text Embedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from LongEmbed, which specifically focuses on long context retrieval.</li>
<li><strong>摘要：</strong>文本嵌入可用于各种应用，但在较长的文本中性能会下降。在本文中，我们发现性能下降是由于一种称为长度崩溃的现象，其中较长的文本嵌入会崩溃到狭窄的空间中。这种崩溃导致不同文本长度的嵌入之间的分布不一致，最终损害下游任务的性能。理论上，通过将自注意力机制固有地用作低通滤波器，我们证明长序列会增加自注意力机制的低通滤波效果的衰减率。随着层的加深，过多的低通滤波会导致 token 信号仅保留其直流 (DC) 分量，这意味着输入 token 特征图将崩溃到狭窄的空间中，尤其是在长文本中。基于以上分析，我们提出通过在 softmax() 中引入温度来缓解不良的长度崩溃限制，从而实现更高的低滤波器衰减率。这种无需调优的方法称为 TempScale，可以插入多个基于 Transformer 的嵌入模型中。从经验上讲，我们证明 TempScale 可以改进现有的嵌入模型，尤其是在长文本输入方面，在 Massive Text Embedding Benchmark (MTEB) 的 40 个数据集上性能提升高达 0.53%，在 LongEmbed 的 4 个数据集上性能提升高达 0.82%，LongEmbed 专注于长上下文检索。</li>
</ul>

<h3>Title: P-Masking: Power Law Masking Improves Multi-attribute Controlled Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Elgaar, Hadi Amiri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24201">https://arxiv.org/abs/2410.24201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24201">https://arxiv.org/pdf/2410.24201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24201]] P-Masking: Power Law Masking Improves Multi-attribute Controlled Generation(https://arxiv.org/abs/2410.24201)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce LingGen, a novel approach for controlled text generation that offers precise control over a wide array of linguistic attributes, even as the number of attributes varies. LingGen employs a dynamic P-MASKING strategy, which samples masking rates from a power law distribution during training. This innovative approach enables the model to develop robust representations and adapt its attribute control capabilities across a variable number of attributes, from a single attribute to multiple complex configurations. The P-MASKING technique enhances LingGen's ability to manage different levels of attribute visibility, resulting in superior performance in multi-attribute generation tasks. Our experiments demonstrate that LingGen surpasses current state-of-the-art models in both attribute control accuracy and text fluency, particularly excelling in scenarios with varying attribute demands. Additionally, our ablation studies highlight the effectiveness of P-MASKING and the influence of different base language models on performance. These findings demonstrate LingGen's potential for applications requiring precise and adaptable control over multiple linguistic attributes in text generation.</li>
<li><strong>摘要：</strong>我们推出了 LingGen，这是一种用于受控文本生成的新方法，即使属性数量发生变化，它也能对各种语言属性进行精确控制。LingGen 采用动态 P-MASKING 策略，在训练期间从幂律分布中采样掩蔽率。这种创新方法使模型能够开发强大的表示，并使其属性控制功能适应可变数量的属性，从单个属性到多个复杂配置。P-MASKING 技术增强了 LingGen 管理不同级别属性可见性的能力，从而在多属性生成任务中表现出色。我们的实验表明，LingGen 在属性控制准确性和文本流畅性方面都超越了当前最先进的模型，尤其是在属性需求变化的场景中表现出色。此外，我们的消融研究强调了 P-MASKING 的有效性以及不同基础语言模型对性能的影响。这些发现证明了 LingGen 在需要对文本生成中的多个语言属性进行精确和适应性控制的应用中的潜力。</li>
</ul>

<h3>Title: Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Xi, Yinong He, Jianing Yang, Yinpei Dai, Joyce Chai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.24218">https://arxiv.org/abs/2410.24218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.24218">https://arxiv.org/pdf/2410.24218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.24218]] Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use(https://arxiv.org/abs/2410.24218)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. It's not clear how to incorporate rich language use to facilitate task learning. To address this question, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness (i.e., feedback on past behaviors and future guidance) and diversity (i.e., variation of language expressions) impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world. Project website: this https URL</li>
<li><strong>摘要：</strong>在现实世界中，具身代理希望能够利用人类语言来获得显性或隐性知识，以完成学习任务。尽管最近取得了进展，但大多数先前的方法都采用简单的低级指令作为语言输入，这可能无法反映自然的人类交流。目前尚不清楚如何结合丰富的语言使用来促进任务学习。为了解决这个问题，本文研究了不同类型的语言输入在促进强化学习 (RL) 具身代理方面的作用。更具体地说，我们研究了不同级别的语言信息量（即对过去行为和未来指导的反馈）和多样性（即语言表达的变化）如何影响代理学习和推理。我们基于四个 RL 基准的实证结果表明，使用多样化和信息丰富的语言反馈训练的代理可以实现增强的泛化和快速适应新任务。这些发现强调了语言使用在开放世界中教授具身代理新任务的关键作用。项目网站：此 https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
