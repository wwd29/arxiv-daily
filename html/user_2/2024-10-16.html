<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-16</h1>
<h3>Title: Crafting Narrative Closures: Zero-Shot Learning with SSM Mamba for Short Story Ending Generation</h3>
<ul>
<li><strong>Authors: </strong>Divyam Sharma, Divya Santhanam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10848">https://arxiv.org/abs/2410.10848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10848">https://arxiv.org/pdf/2410.10848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10848]] Crafting Narrative Closures: Zero-Shot Learning with SSM Mamba for Short Story Ending Generation(https://arxiv.org/abs/2410.10848)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>Writing stories is an engaging yet challenging endeavor. Often, authors encounter moments of creative block, where the path forward in their narrative becomes obscured. This paper is designed to address such moments by providing an innovative solution: A tool that completes stories based on given prompts. By inputting a short story prompt, users can receive a conclusion to their story, articulated in one sentence or more, thereby enhancing the storytelling process with AI-driven creativity. This tool aims not only to assist authors in navigating writer's block but also to offer a fun and interactive way for anyone to expand on story ideas spontaneously. Through this paper, we explore the intersection of artificial intelligence and creative writing, pushing the boundaries of how stories can be crafted and concluded. To create our final text-generation models, we used a pre-trained GPT-3.5 model and a newly created finetuned SSM-Mamba model, both of which perform well on a comprehensive list of metrics including BERT score, METEOR, BLEU, ROUGE, and Perplexity. The SSM model has also been made public for the NLP community on HuggingFace models as an open source contribution, which for the timebeing is a first of its kind state-space model for story-generation task on HuggingFace.</li>
<li><strong>摘要：</strong>写故事是一项有趣而又充满挑战性的工作。作者经常会遇到创作瓶颈，故事的前进道路变得模糊不清。本文旨在通过提供一种创新解决方案来解决此类问题：一种根据给定提示完成故事的工具。通过输入一个简短的故事提示，用户可以收到他们的故事的结论，用一句话或多句话表达出来，从而通过人工智能驱动的创造力增强讲故事的过程。这个工具不仅旨在帮助作者克服写作瓶颈，还为任何人提供一种有趣且互动的方式，让他们自发地扩展故事创意。通过这篇论文，我们探索了人工智能与创意写作的交集，突破了故事创作和结局的界限。为了创建最终的文本生成模型，我们使用了预先训练的 GPT-3.5 模型和新创建的微调 SSM-Mamba 模型，这两个模型在包括 BERT 分数、METEOR、BLEU、ROUGE 和困惑度在内的一系列指标上都表现良好。 SSM 模型也已作为开源贡献在 HuggingFace 模型上向 NLP 社区公开，这是目前 HuggingFace 上第一个用于故事生成任务的状态空间模型。</li>
</ul>

<h3>Title: On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts</h3>
<ul>
<li><strong>Authors: </strong>Toluwani Aremu, Oluwakemi Akinwehinmi, Chukwuemeka Nwagu, Syed Ishtiaque Ahmed, Rita Orji, Pedro Arnau Del Amo, Abdulmotaleb El Saddik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10850">https://arxiv.org/abs/2410.10850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10850">https://arxiv.org/pdf/2410.10850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10850]] On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts(https://arxiv.org/abs/2410.10850)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>We investigate and observe the behaviour and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution.</li>
<li><strong>摘要：</strong>我们调查并观察了大型语言模型 (LLM) 支持的聊天机器人在气候变化和心理健康领域中处理带有人口统计信息的错误提示和问题时的行为和性能。通过定量和定性方法的结合，我们评估了聊天机器人辨别陈述真实性、对事实的遵守以及其回答中是否存在偏见或错误信息的能力。我们使用判断题进行的定量分析表明，这些聊天机器人可以可靠地对这些封闭式问题给出正确的答案。然而，从领域专家那里收集到的定性见解表明，人们仍然担心隐私、道德影响以及聊天机器人将用户引导到专业服务的必要性。我们得出的结论是，虽然这些聊天机器人前景广阔，但它们在敏感领域的部署需要仔细考虑、道德监督和严格改进，以确保它们成为人类专业知识的有益补充，而不是自主解决方案。</li>
</ul>

<h3>Title: SafeLLM: Domain-Specific Safety Monitoring for Large Language Models: A Case Study of Offshore Wind Maintenance</h3>
<ul>
<li><strong>Authors: </strong>Connor Walker, Callum Rothon, Koorosh Aslansefat, Yiannis Papadopoulos, Nina Dethlefs</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10852">https://arxiv.org/abs/2410.10852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10852">https://arxiv.org/pdf/2410.10852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10852]] SafeLLM: Domain-Specific Safety Monitoring for Large Language Models: A Case Study of Offshore Wind Maintenance(https://arxiv.org/abs/2410.10852)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat, agent</a></li>
<li><strong>Abstract: </strong>The Offshore Wind (OSW) industry is experiencing significant expansion, resulting in increased Operations \& Maintenance (O\&M) costs. Intelligent alarm systems offer the prospect of swift detection of component failures and process anomalies, enabling timely and precise interventions that could yield reductions in resource expenditure, as well as scheduled and unscheduled downtime. This paper introduces an innovative approach to tackle this challenge by capitalising on Large Language Models (LLMs). We present a specialised conversational agent that incorporates statistical techniques to calculate distances between sentences for the detection and filtering of hallucinations and unsafe output. This potentially enables improved interpretation of alarm sequences and the generation of safer repair action recommendations by the agent. Preliminary findings are presented with the approach applied to ChatGPT-4 generated test sentences. The limitation of using ChatGPT-4 and the potential for enhancement of this agent through re-training with specialised OSW datasets are discussed.</li>
<li><strong>摘要：</strong>海上风电 (OSW) 行业正在经历显著扩张，导致运营和维护 (O\&M) 成本增加。智能报警系统有望快速检测组件故障和过程异常，从而实现及时和精确的干预，从而减少资源支出以及计划内和计划外停机时间。本文介绍了一种利用大型语言模型 (LLM) 应对这一挑战的创新方法。我们提出了一个专门的对话代理，它结合统计技术来计算句子之间的距离，以检测和过滤幻觉和不安全输出。这可能会使代理能够更好地解释警报序列并生成更安全的修复操作建议。本文介绍了将该方法应用于 ChatGPT-4 生成的测试句子的初步结果。讨论了使用 ChatGPT-4 的局限性以及通过使用专门的 OSW 数据集重新训练来增强此代理的潜力。</li>
</ul>

<h3>Title: Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support</h3>
<ul>
<li><strong>Authors: </strong>Abdul Muqtadir, Hafiz Syed Muhammad Bilal, Ayesha Yousaf, Hafiz Farooq Ahmed, Jamil Hussain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10853">https://arxiv.org/abs/2410.10853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10853">https://arxiv.org/pdf/2410.10853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10853]] Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support(https://arxiv.org/abs/2410.10853)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>This research work delves into the manifestation of hallucination within Large Language Models (LLMs) and its consequential impacts on applications within the domain of mental health. The primary objective is to discern effective strategies for curtailing hallucinatory occurrences, thereby bolstering the dependability and security of LLMs in facilitating mental health interventions such as therapy, counseling, and the dissemination of pertinent information. Through rigorous investigation and analysis, this study seeks to elucidate the underlying mechanisms precipitating hallucinations in LLMs and subsequently propose targeted interventions to alleviate their occurrence. By addressing this critical issue, the research endeavors to foster a more robust framework for the utilization of LLMs within mental health contexts, ensuring their efficacy and reliability in aiding therapeutic processes and delivering accurate information to individuals seeking mental health support.</li>
<li><strong>摘要：</strong>本研究深入探讨了大型语言模型 (LLM) 中幻觉的表现及其对心理健康领域应用的影响。主要目标是找出减少幻觉发生的有效策略，从而增强 LLM 在促进心理健康干预（如治疗、咨询和相关信息传播）方面的可靠性和安全性。通过严格的调查和分析，本研究旨在阐明 LLM 中幻觉的潜在机制，并随后提出有针对性的干预措施来缓解幻觉的发生。通过解决这一关键问题，本研究致力于为 LLM 在心理健康环境中的使用建立一个更强大的框架，确保其在辅助治疗过程方面的有效性和可靠性，并为寻求心理健康支持的个人提供准确的信息。</li>
</ul>

<h3>Title: Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shramay Palta, Nishant Balepur, Peter Rankel, Sarah Wiegreffe, Marine Carpuat, Rachel Rudinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10854">https://arxiv.org/abs/2410.10854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10854">https://arxiv.org/pdf/2410.10854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10854]] Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning(https://arxiv.org/abs/2410.10854)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Questions involving commonsense reasoning about everyday situations often admit many $\textit{possible}$ or $\textit{plausible}$ answers. In contrast, multiple-choice question (MCQ) benchmarks for commonsense reasoning require a hard selection of a single correct answer, which, in principle, should represent the $\textit{most}$ plausible answer choice. On $250$ MCQ items sampled from two commonsense reasoning benchmarks, we collect $5,000$ independent plausibility judgments on answer choices. We find that for over 20% of the sampled MCQs, the answer choice rated most plausible does not match the benchmark gold answers; upon manual inspection, we confirm that this subset exhibits higher rates of problems like ambiguity or semantic mismatch between question and answer choices. Experiments with LLMs reveal low accuracy and high variation in performance on the subset, suggesting our plausibility criterion may be helpful in identifying more reliable benchmark items for commonsense evaluation.</li>
<li><strong>摘要：</strong>涉及日常情况的常识推理问题通常会允许许多“可能”或“可信”的答案。相比之下，常识推理的多项选择题 (MCQ) 基准要求从单个正确答案中艰难地选择一个，原则上，该答案应该代表“最可信”的答案选择。在从两个常识推理基准中抽样的 $250$ 个 MCQ 项目中，我们收集了 $5,000$ 个独立的答案选择可信度判断。我们发现，对于超过 20% 的抽样 MCQ，被评为最可信的答案选择与基准黄金答案不匹配；经过人工检查，我们确认该子集表现出更高的问题发生率，例如问题和答案选项之间的歧义或语义不匹配。使用 LLM 的实验表明，该子集的准确率较低且性能差异很大，这表明我们的可信度标准可能有助于确定更可靠的常识评估基准项目。</li>
</ul>

<h3>Title: CogDevelop2K: Reversed Cognitive Development in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yijiang Li, Qingying Gao, Haoran Sun, Haiyun Lyu, Dezhi Luo, Hokin Deng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10855">https://arxiv.org/abs/2410.10855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10855">https://arxiv.org/pdf/2410.10855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10855]] CogDevelop2K: Reversed Cognitive Development in Multimodal Large Language Models(https://arxiv.org/abs/2410.10855)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Are Multi-modal Large Language Models (MLLMs) stochastic parrots? Do they genuinely understand and are capable of performing the tasks they excel at? This paper aims to explore the fundamental basis of MLLMs, i.e. core cognitive abilities that human intelligence builds upon to perceive, comprehend, and reason. To this end, we propose CogDevelop2K, a comprehensive benchmark that spans 12 sub-concepts from fundamental knowledge like object permanence and boundary to advanced reasoning like intentionality understanding, structured via the developmental trajectory of a human mind. We evaluate 46 MLLMs on our benchmarks. Comprehensively, we further evaluate the influence of evaluation strategies and prompting techniques. Surprisingly, we observe a reversed cognitive developmental trajectory compared to humans.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 是随机鹦鹉吗？它们真的理解并能够执行它们擅长的任务吗？本文旨在探索 MLLM 的基本基础，即人类智能建立在感知、理解和推理之上的核心认知能力。为此，我们提出了 CogDevelop2K，这是一个全面的基准，涵盖了 12 个子概念，从对象永久性和边界等基础知识到意向性理解等高级推理，通过人类思维的发展轨迹构建而成。我们在基准上评估了 46 个 MLLM。综合而言，我们进一步评估了评估策略和提示技术的影响。令人惊讶的是，与人类相比，我们观察到了相反的认知发展轨迹。</li>
</ul>

<h3>Title: Mirror-Consistency: Harnessing Inconsistency in Majority Voting</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10857">https://arxiv.org/abs/2410.10857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10857">https://arxiv.org/pdf/2410.10857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10857]] Mirror-Consistency: Harnessing Inconsistency in Majority Voting(https://arxiv.org/abs/2410.10857)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Self-Consistency, a widely-used decoding strategy, significantly boosts the reasoning capabilities of Large Language Models (LLMs). However, it depends on the plurality voting rule, which focuses on the most frequent answer while overlooking all other minority responses. These inconsistent minority views often illuminate areas of uncertainty within the model's generation process. To address this limitation, we present Mirror-Consistency, an enhancement of the standard Self-Consistency approach. Our method incorporates a 'reflective mirror' into the self-ensemble decoding process and enables LLMs to critically examine inconsistencies among multiple generations. Additionally, just as humans use the mirror to better understand themselves, we propose using Mirror-Consistency to enhance the sample-based confidence calibration methods, which helps to mitigate issues of overconfidence. Our experimental results demonstrate that Mirror-Consistency yields superior performance in both reasoning accuracy and confidence calibration compared to Self-Consistency.</li>
<li><strong>摘要：</strong>自一致性是一种广泛使用的解码策略，它显著提升了大型语言模型 (LLM) 的推理能力。然而，它依赖于多数投票规则，该规则关注最常见的答案，而忽略所有其他少数人的回答。这些不一致的少数人观点通常会揭示模型生成过程中的不确定领域。为了解决这一限制，我们提出了镜像一致性，这是标准自一致性方法的增强。我们的方法将“反射镜”纳入自集成解码过程，使 LLM 能够批判性地检查多个代之间的不一致性。此外，正如人类使用镜子来更好地了解自己一样，我们建议使用镜像一致性来增强基于样本的置信度校准方法，这有助于缓解过度自信的问题。我们的实验结果表明，与自一致性相比，镜像一致性在推理准确性和置信度校准方面都表现出色。</li>
</ul>

<h3>Title: Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths</h3>
<ul>
<li><strong>Authors: </strong>Yew Ken Chia, Guizhen Chen, Weiwen Xu, Luu Anh Tuan, Soujanya Poria, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10858">https://arxiv.org/abs/2410.10858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10858">https://arxiv.org/pdf/2410.10858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10858]] Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths(https://arxiv.org/abs/2410.10858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Advanced models such as OpenAI o1 exhibit impressive problem-solving capabilities through step-by-step reasoning. However, they may still falter on more complex problems, making errors that disrupt their reasoning paths. We attribute this to the expansive solution space, where each step has the risk of diverging into mistakes. To enhance language model reasoning, we introduce a specialized training framework called Reasoning Paths Optimization (RPO), which enables learning to reason and explore from diverse paths. Our approach encourages favorable branches at each reasoning step while penalizing unfavorable ones, enhancing the model's overall problem-solving performance. Reasoning Paths Optimization does not rely on large-scale human-annotated rationales or outputs from closed-source models, making it scalable and data-efficient. We focus on multi-step reasoning tasks, such as math word problems and science-based exam questions. The experiments demonstrate that our framework significantly enhances the reasoning performance of large language models, with up to 3.1% and 4.3% improvement on GSM8K and MMLU (STEM) respectively. Our data and code can be found at this https URL.</li>
<li><strong>摘要：</strong>OpenAI o1 等高级模型通过逐步推理展现出令人印象深刻的解决问题能力。然而，它们在更复杂的问题上仍可能失败，犯下错误，从而破坏其推理路径。我们将其归因于广阔的解决方案空间，其中每一步都有可能出现错误。为了增强语言模型推理能力，我们引入了一个称为推理路径优化 (RPO) 的专门训练框架，该框架可以学习从不同的路径进行推理和探索。我们的方法鼓励每个推理步骤中的有利分支，同时惩罚不利分支，从而提高模型的整体解决问题性能。推理路径优化不依赖于大规模人工注释的理由或闭源模型的输出，使其具有可扩展性和数据效率。我们专注于多步骤推理任务，例如数学应用题和基于科学的考试问题。实验表明，我们的框架显著提高了大型语言模型的推理性能，GSM8K 和 MMLU (STEM) 的改进率分别高达 3.1% 和 4.3%。我们的数据和代码可以在这个 https URL 找到。</li>
</ul>

<h3>Title: FAME: Towards Factual Multi-Task Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Li Zeng, Yingyu Shan, Zeming Liu, Jiashu Yao, Yuhang Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10859">https://arxiv.org/abs/2410.10859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10859">https://arxiv.org/pdf/2410.10859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10859]] FAME: Towards Factual Multi-Task Model Editing(https://arxiv.org/abs/2410.10859)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) embed extensive knowledge and utilize it to perform exceptionally well across various tasks. Nevertheless, outdated knowledge or factual errors within LLMs can lead to misleading or incorrect responses, causing significant issues in practical applications. To rectify the fatal flaw without the necessity for costly model retraining, various model editing approaches have been proposed to correct inaccurate knowledge within LLMs in a cost-efficient way. To evaluate these model editing methods, previous work introduced a series of datasets. However, most of the previous datasets only contain fabricated data in a single format, which diverges from real-world model editing scenarios, raising doubts about their usability in practice. To facilitate the application of model editing in real-world scenarios, we propose the challenge of practicality. To resolve such challenges and effectively enhance the capabilities of LLMs, we present FAME, an factual, comprehensive, and multi-task dataset, which is designed to enhance the practicality of model editing. We then propose SKEME, a model editing method that uses a novel caching mechanism to ensure synchronization with the real world. The experiments demonstrate that SKEME performs excellently across various tasks and scenarios, confirming its practicality.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 嵌入了大量知识，并利用这些知识在各种任务中表现出色。然而，LLM 中的过时知识或事实错误可能会导致误导或不正确的响应，从而在实际应用中造成重大问题。为了在无需昂贵的模型重新训练的情况下纠正致命缺陷，已经提出了各种模型编辑方法，以经济高效的方式纠正 LLM 中的不准确知识。为了评估这些模型编辑方法，以前的工作引入了一系列数据集。然而，大多数以前的数据集只包含单一格式的虚构数据，这与现实世界的模型编辑场景不同，引起了人们对它们在实践中的可用性的怀疑。为了促进模型编辑在现实世界场景中的应用，我们提出了实用性的挑战。为了解决这些挑战并有效增强 LLM 的功能，我们提出了 FAME，这是一个事实、全面和多任务的数据集，旨在提高模型编辑的实用性。然后，我们提出了一种模型编辑方法 SKEME，它使用一种新颖的缓存机制来确保与现实世界的同步。实验表明，SKEME 在各种任务和场景中均表现优异，证实了它的实用性。</li>
</ul>

<h3>Title: A Recipe For Building a Compliant Real Estate Chatbot</h3>
<ul>
<li><strong>Authors: </strong>Navid Madani, Anusha Bagalkotkar, Supriya Anand, Gabriel Arnson, Rohini Srihari, Kenneth Joseph</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10860">https://arxiv.org/abs/2410.10860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10860">https://arxiv.org/pdf/2410.10860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10860]] A Recipe For Building a Compliant Real Estate Chatbot(https://arxiv.org/abs/2410.10860)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>In recent years, there has been significant effort to align large language models with human preferences. This work focuses on developing a chatbot specialized in the real estate domain, with an emphasis on incorporating compliant behavior to ensure it can be used without perpetuating discriminatory practices like steering and redlining, which have historically plagued the real estate industry in the United States. Building on prior work, we present a method for generating a synthetic general instruction-following dataset, along with safety data. Through extensive evaluations and benchmarks, we fine-tuned a llama-3-8B-instruct model and demonstrated that we can enhance it's performance significantly to match huge closed-source models like GPT-4o while making it safer and more compliant. We open-source the model, data and code to support further development and research in the community.</li>
<li><strong>摘要：</strong>近年来，人们付出了巨大的努力来使大型语言模型与人类偏好保持一致。这项工作的重点是开发专门用于房地产领域的聊天机器人，重点是纳入合规行为，以确保它可以在使用时不会延续诸如操纵和红线之类的歧视性做法，这些做法在历史上一直困扰着美国的房地产行业。在前人工作的基础上，我们提出了一种生成合成通用指令遵循数据集以及安全数据的方法。通过广泛的评估和基准测试，我们对 llama-3-8B-instruct 模型进行了微调，并证明我们可以显著提高其性能，以匹配 GPT-4o 等大型闭源模型，同时使其更安全、更合规。我们开源模型、数据和代码，以支持社区的进一步开发和研究。</li>
</ul>

<h3>Title: Superficial Safety Alignment Hypothesis</h3>
<ul>
<li><strong>Authors: </strong>Jianwei Li, Jung-Eun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10862">https://arxiv.org/abs/2410.10862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10862">https://arxiv.org/pdf/2410.10862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10862]] Superficial Safety Alignment Hypothesis(https://arxiv.org/abs/2410.10862)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 越来越多地集成到各种应用程序中，确保它们生成安全且一致的响应已成为当务之急。之前对对齐的研究主要集中在一般的指令遵循上，但往往忽略了安全对齐的独特属性和挑战，例如安全机制的脆弱性。为了弥补这一差距，我们提出了表面安全对齐假设 (SSAH)，该假设认为安全对齐应该教会原本不安全的模型选择正确的推理方向——解释为专门的二元分类任务——并结合拒绝机制和多个​​保留的后备选项。此外，通过 SSAH，我们假设 LLM 中的安全护栏仅由少数基本组件即可建立。为了验证这一点，我们进行了一项消融研究，并成功识别了安全对齐 LLM 中的四种属性关键组件：专用安全单元 (ESU)、专用实用单元 (EUU)、复杂单元 (CU) 和冗余单元 (RU)。我们的研究结果表明，在微调过程中冻结某些安全关键组件 7.5% 可使模型在适应新任务的同时保留其安全属性。此外，我们表明，利用预训练模型中 20% 的冗余单元作为“对齐预算”可以有效地最大限度地减少对齐税，同时实现对齐目标。综合考虑，本文得出结论，LLM 中安全的原子功能单元处于神经元级别，并强调安全对齐不应该很复杂。我们相信这项工作有助于为未来的 LLM 奠定高效且可扩展的安全对齐基础。</li>
</ul>

<h3>Title: What makes your model a low-empathy or warmth person: Exploring the Origins of Personality in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shu Yang, Shenzhe Zhu, Ruoxuan Bao, Liang Liu, Yu Cheng, Lijie Hu, Mengdi Li, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10863">https://arxiv.org/abs/2410.10863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10863">https://arxiv.org/pdf/2410.10863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10863]] What makes your model a low-empathy or warmth person: Exploring the Origins of Personality in LLMs(https://arxiv.org/abs/2410.10863)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like text and exhibiting personality traits similar to those in humans. However, the mechanisms by which LLMs encode and express traits such as agreeableness and impulsiveness remain poorly understood. Drawing on the theory of social determinism, we investigate how long-term background factors, such as family environment and cultural norms, interact with short-term pressures like external instructions, shaping and influencing LLMs' personality traits. By steering the output of LLMs through the utilization of interpretable features within the model, we explore how these background and pressure factors lead to changes in the model's traits without the need for further fine-tuning. Additionally, we suggest the potential impact of these factors on model safety from the perspective of personality.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出生成类似人类的文本和表现出与人类相似的性格特征的卓越能力。然而，LLM 编码和表达诸如亲和性和冲动性等特征的机制仍然不太清楚。借鉴社会决定论，我们研究长期背景因素（如家庭环境和文化规范）如何与短期压力（如外部指示）相互作用，从而塑造和影响 LLM 的性格特征。通过利用模型中的可解释特征来控制 LLM 的输出，我们探索了这些背景和压力因素如何导致模型特征的变化而无需进一步微调。此外，我们从个性的角度提出了这些因素对模型安全性的潜在影响。</li>
</ul>

<h3>Title: Fill In The Gaps: Model Calibration and Generalization with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Yang Ba, Michelle V. Mancenido, Rong Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10864">https://arxiv.org/abs/2410.10864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10864">https://arxiv.org/pdf/2410.10864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10864]] Fill In The Gaps: Model Calibration and Generalization with Synthetic Data(https://arxiv.org/abs/2410.10864)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As machine learning models continue to swiftly advance, calibrating their performance has become a major concern prior to practical and widespread implementation. Most existing calibration methods often negatively impact model accuracy due to the lack of diversity of validation data, resulting in reduced generalizability. To address this, we propose a calibration method that incorporates synthetic data without compromising accuracy. We derive the expected calibration error (ECE) bound using the Probably Approximately Correct (PAC) learning framework. Large language models (LLMs), known for their ability to mimic real data and generate text with mixed class labels, are utilized as a synthetic data generation strategy to lower the ECE bound and improve model accuracy on real test data. Additionally, we propose data generation mechanisms for efficient calibration. Testing our method on four different natural language processing tasks, we observed an average up to 34\% increase in accuracy and 33\% decrease in ECE.</li>
<li><strong>摘要：</strong>随着机器学习模型的快速发展，在实际和广泛实施之前，校准其性能已成为主要关注点。由于验证数据缺乏多样性，大多数现有的校准方法通常会对模型准确性产生负面影响，从而降低通用性。为了解决这个问题，我们提出了一种在不影响准确性的情况下结合合成数据的校准方法。我们使用可能近似正确 (PAC) 学习框架推导出预期校准误差 (ECE) 界限。大型语言模型 (LLM) 以其模拟真实数据和生成具有混合类标签的文本的能力而闻名，它被用作合成数据生成策略来降低 ECE 界限并提高真实测试数据的模型准确性。此外，我们提出了有效校准的数据生成机制。在四种不同的自然语言处理任务上测试我们的方法后，我们观察到准确率平均提高了 34%，ECE 平均降低了 33%。</li>
</ul>

<h3>Title: Generating Synthetic Datasets for Few-shot Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xu Guo, Zilin Du, Boyang Li, Chunyan Miao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10865">https://arxiv.org/abs/2410.10865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10865">https://arxiv.org/pdf/2410.10865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10865]] Generating Synthetic Datasets for Few-shot Prompt Tuning(https://arxiv.org/abs/2410.10865)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful LLMs to synthesize task-specific labeled data for training the soft prompts. We first introduce a distribution-aligned weighted generator tuning (DawGen) method to encourage generating in-distribution data that aligns with the few-shot real data. Then, we train soft prompts on both synthetic and real datasets using a gradient surgery approach, which eliminates the conflicting gradients from different data sources. Experiments on seven sentence-pair classification datasets demonstrate the effectiveness of our proposed method for boosting prompt tuning in few-shot learning settings. Results on QQP, MRPC, and SICK datasets are even comparable to the performance of transfer learning from large real-world datasets, showing the promise of synthetic data as an alternative for enhancing soft prompt tuning.</li>
<li><strong>摘要：</strong>提示调优的一个主要限制是它依赖于大量标记的训练数据集。在少样本学习设置下，提示调优远远落后于全模型微调，限制了其应用范围。在本文中，我们利用强大的 LLM 来合成特定于任务的标记数据以训练软提示。我们首先引入一种分布对齐的加权生成器调优 (DawGen) 方法来鼓励生成与少样本真实数据对齐的分布内数据。然后，我们使用梯度手术方法在合成和真实数据集上训练软提示，从而消除来自不同数据源的冲突梯度。在七个句子对分类数据集上的实验证明了我们提出的方法在少样本学习设置中提升提示调优的有效性。QQP、MRPC 和 SICK 数据集上的结果甚至可以与来自大型真实数据集的迁移学习的性能相媲美，这表明合成数据作为增强软提示调优的替代方案的前景。</li>
</ul>

<h3>Title: CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept</h3>
<ul>
<li><strong>Authors: </strong>YuXuan Wu, Bonaventure F. P. Dossou, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10866">https://arxiv.org/abs/2410.10866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10866">https://arxiv.org/pdf/2410.10866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10866]] CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept(https://arxiv.org/abs/2410.10866)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 提供了各个领域的广泛知识，但它们可能会无意中记住敏感、未经授权或恶意的数据，例如医疗和金融领域的个人信息。机器反学习方法旨在在训练后从模型中删除特定信息以解决此问题。然而，由于 LLM 的复杂、密集和连续性质，当前的方法需要额外的模型训练或难以有效地清除特定数据点及其相关上下文。在本研究中，我们提出了一种使用码本特征和稀疏自动编码器 (SAE) 的新型摊销反学习方法。通过利用瓶颈来分解激活空间并调节信息流，我们的方法可以有效地反学习目标信息，同时保留模型在无关数据上的性能。据我们所知，这是第一项成功实现在 LLM 中反学习具有上下文相关性的特定主题的工作，标志着机器反学习向现实世界应用迈出了重要一步。</li>
</ul>

<h3>Title: Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging</h3>
<ul>
<li><strong>Authors: </strong>Ryota Tozuka, Hisashi Johno, Akitomo Amakawa, Junichi Sato, Mizuki Muto, Shoichiro Seki, Atsushi Komaba, Hiroshi Onishi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10869">https://arxiv.org/abs/2410.10869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10869">https://arxiv.org/pdf/2410.10869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10869]] Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging(https://arxiv.org/abs/2410.10869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Purpose: In radiology, large language models (LLMs), including ChatGPT, have recently gained attention, and their utility is being rapidly evaluated. However, concerns have emerged regarding their reliability in clinical applications due to limitations such as hallucinations and insufficient referencing. To address these issues, we focus on the latest technology, retrieval-augmented generation (RAG), which enables LLMs to reference reliable external knowledge (REK). Specifically, this study examines the utility and reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for staging lung cancer. Materials and methods: We summarized the current lung cancer staging guideline in Japan and provided this as REK to NotebookLM. We then tasked NotebookLM with staging 100 fictional lung cancer cases based on CT findings and evaluated its accuracy. For comparison, we performed the same task using a gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK. Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in searching reference locations within the REK. Conclusion: NotebookLM successfully performed lung cancer staging by utilizing the REK, demonstrating superior performance compared to GPT-4o. Additionally, it provided highly accurate reference locations within the REK, allowing radiologists to efficiently evaluate the reliability of NotebookLM's responses and detect possible hallucinations. Overall, this study highlights the potential of NotebookLM, a RAG-LLM, in image diagnosis.</li>
<li><strong>摘要：</strong>目的：在放射学领域，大型语言模型 (LLM)（包括 ChatGPT）最近引起了人们的关注，它们的实用性正在迅速得到评估。然而，由于幻觉和引用不足等限制，人们对它们在临床应用中的可靠性产生了担忧。为了解决这些问题，我们专注于最新技术——检索增强生成 (RAG)，它使 LLM 能够引用可靠的外部知识 (REK)。具体来说，本研究考察了最近发布的配备 RAG 的 LLM (RAG-LLM) NotebookLM 在肺癌分期中的实用性和可靠性。材料和方法：我们总结了日本目前的肺癌分期指南，并将其作为 REK 提供给 NotebookLM。然后，我们要求 NotebookLM 根据 CT 结果对 100 例虚构的肺癌病例进行分期，并评估其准确性。为了进行比较，我们使用金标准 LLM GPT-4 Omni (GPT-4o) 执行了相同的任务，包括带 REK 和不带 REK 两种情况。结果：NotebookLM 在肺癌分期实验中实现了 86% 的诊断准确率，优于 GPT-4o，后者在使用 REK 时准确率为 39%，不使用 REK 时准确率为 25%。此外，NotebookLM 在 REK 内搜索参考位置的准确率高达 95%。结论：NotebookLM 利用 REK 成功进行了肺癌分期，与 GPT-4o 相比，其性能更出色。此外，它在 REK 内提供了高度准确的参考位置，使放射科医生能够有效评估 NotebookLM 响应的可靠性并检测可能的幻觉。总体而言，这项研究凸显了 RAG-LLM NotebookLM 在图像诊断方面的潜力。</li>
</ul>

<h3>Title: PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches</h3>
<ul>
<li><strong>Authors: </strong>Rana Muhammad Shahroz Khan, Pingzhi Li, Sukwon Yun, Zhenyu Wang, Shahriar Nirjon, Chau-Wai Wong, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10870">https://arxiv.org/abs/2410.10870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10870">https://arxiv.org/pdf/2410.10870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10870]] PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches(https://arxiv.org/abs/2410.10870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM era for achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as ChatGPT are periodically evolved, i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application. Even though fine-tuning costs have nowadays been reduced thanks to the innovations of parameter-efficient fine-tuning such as LoRA, not all downstream users have adequate computing for frequent personalization. Moreover, access to fine-tuning datasets, particularly in sensitive domains such as healthcare, could be time-restrictive, making it crucial to retain the knowledge encoded in earlier fine-tuned rounds for future adaptation. In this paper, we present PortLLM, a training-free framework that (i) creates an initial lightweight model update patch to capture domain-specific knowledge, and (ii) allows a subsequent seamless plugging for the continual personalization of evolved LLM at minimal cost. Our extensive experiments cover seven representative datasets, from easier question-answering tasks {BoolQ, SST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including {Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our designed model patches and showcasing the effectiveness of our proposed framework. For instance, PortLLM achieves comparable performance to LoRA fine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, we provide theoretical justifications to understand the portability of our model update patches, which offers new insights into the theoretical dimension of LLMs' personalization.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 日益影响着 AI 格局，与 LLM 时代之前相比，微调预训练模型已变得比 LLM 时代更受欢迎，以便在特定领域任务中实现最佳性能。然而，预训练的 LLM（例如 ChatGPT）会定期演进（即模型参数会频繁更新），这使得资源有限的下游用户难以跟上针对其领域应用微调最新 LLM 的步伐。尽管如今由于 LoRA 等参数高效微调的创新，微调成本已经降低，但并非所有下游用户都拥有足够的计算能力来进行频繁的个性化。此外，访问微调数据集（尤其是在医疗保健等敏感领域）可能会受到时间限制，因此保留早期微调轮次中编码的知识以供将来调整至关重要。在本文中，我们介绍了 PortLLM，这是一个无需训练的框架，它 (i) 创建初始轻量级模型更新补丁来捕获特定领域的知识，以及 (ii) 允许随后无缝插入，以最低成本持续个性化演进的 LLM。我们的大量实验涵盖了七个代表性数据集，从较简单的问答任务 {BoolQ、SST2} 到较难的推理任务 {WinoGrande、GSM8K}，以及包括 {Mistral-7B、Llama2、Llama3.1 和 Gemma2} 在内的模型，验证了我们设计的模型补丁的可移植性并展示了我们提出的框架的有效性。例如，PortLLM 实现了与 LoRA 微调相当的性能，GPU 内存使用量减少了高达 12.2 倍。最后，我们提供了理论依据来理解我们的模型更新补丁的可移植性，这为 LLM 个性化的理论维度提供了新的见解。</li>
</ul>

<h3>Title: Applying Refusal-Vector Ablation to Llama 3.1 70B Agents</h3>
<ul>
<li><strong>Authors: </strong>Simon Lermen, Mateusz Dziemian, Govind Pimpale</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10871">https://arxiv.org/abs/2410.10871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10871">https://arxiv.org/pdf/2410.10871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10871]] Applying Refusal-Vector Ablation to Llama 3.1 70B Agents(https://arxiv.org/abs/2410.10871)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat, agent</a></li>
<li><strong>Abstract: </strong>Recently, language models like Llama 3.1 Instruct have become increasingly capable of agentic behavior, enabling them to perform tasks requiring short-term planning and tool use. In this study, we apply refusal-vector ablation to Llama 3.1 70B and implement a simple agent scaffolding to create an unrestricted agent. Our findings imply that these refusal-vector ablated models can successfully complete harmful tasks, such as bribing officials or crafting phishing attacks, revealing significant vulnerabilities in current safety mechanisms. To further explore this, we introduce a small Safe Agent Benchmark, designed to test both harmful and benign tasks in agentic scenarios. Our results imply that safety fine-tuning in chat models does not generalize well to agentic behavior, as we find that Llama 3.1 Instruct models are willing to perform most harmful tasks without modifications. At the same time, these models will refuse to give advice on how to perform the same tasks when asked for a chat completion. This highlights the growing risk of misuse as models become more capable, underscoring the need for improved safety frameworks for language model agents.</li>
<li><strong>摘要：</strong>最近，像 Llama 3.1 Instruct 这样的语言模型越来越能够执行代理行为，使它们能够执行需要短期规划和使用工具的任务。在本研究中，我们将拒绝向量消融应用于 Llama 3.1 70B，并实施一个简单的代理支架来创建不受限制的代理。我们的研究结果表明，这些拒绝向量消融模型可以成功完成有害任务，例如贿赂官员或策划网络钓鱼攻击，从而揭示了当前安全机制中的重大漏洞。为了进一步探索这一点，我们引入了一个小型安全代理基准，旨在测试代理场景中的有害和良性任务。我们的结果表明，聊天模型中的安全微调不能很好地推广到代理行为，因为我们发现 Llama 3.1 Instruct 模型愿意在未经修改的情况下执行大多数有害任务。同时，当被要求完成聊天时，这些模型将拒绝提供有关如何执行相同任务的建议。这凸显了随着模型功能越来越强大，滥用的风险也越来越大，强调了改进语言模型代理的安全框架的必要性。</li>
</ul>

<h3>Title: ToolBridge: An Open-Source Dataset to Equip LLMs with External Tool Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Zhenchao Jin, Mengchen Liu, Dongdong Chen, Lingting Zhu, Yunsheng Li, Lequan Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10872">https://arxiv.org/abs/2410.10872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10872">https://arxiv.org/pdf/2410.10872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10872]] ToolBridge: An Open-Source Dataset to Equip LLMs with External Tool Capabilities(https://arxiv.org/abs/2410.10872)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Through the integration of external tools, large language models (LLMs) such as GPT-4o and Llama 3.1 significantly expand their functional capabilities, evolving from elementary conversational agents to general-purpose assistants. We argue that the primary drivers of these advancements are the quality and diversity of the training data. However, the existing LLMs with external tool integration provide only limited transparency regarding their datasets and data collection methods, which has led to the initiation of this research. Specifically, in this paper, our objective is to elucidate the detailed process involved in constructing datasets that empower LLMs to effectively learn how to utilize external tools and make this information available to the public through the introduction of ToolBridge. ToolBridge proposes to employ a collection of general open-access datasets as its raw dataset pool and applies a series of strategies to identify appropriate data entries from the pool for external tool API insertions. By supervised fine-tuning on these curated data entries, LLMs can invoke external tools in appropriate contexts to boost their predictive accuracy, particularly for basic functions including data processing, numerical computation, and factual retrieval. Our experiments rigorously isolates model architectures and training configurations, focusing exclusively on the role of data. The experimental results indicate that LLMs trained on ToolBridge demonstrate consistent performance improvements on both standard benchmarks and custom evaluation datasets. All the associated code and data will be open-source at this https URL, promoting transparency and facilitating the broader community to explore approaches for equipping LLMs with external tools capabilities.</li>
<li><strong>摘要：</strong>通过集成外部工具，GPT-4o 和 Llama 3.1 等大型语言模型 (LLM) 显著扩展了其功能能力，从基本的对话代理发展为通用助手。我们认为，这些进步的主要驱动力是训练数据的质量和多样性。然而，现有的集成外部工具的 LLM 仅提供有限的数据集和数据收集方法透明度，这导致了这项研究的启动。具体来说，在本文中，我们的目标是阐明构建数据集的详细过程，使 LLM 能够有效地学习如何使用外部工具，并通过引入 ToolBridge 将这些信息提供给公众。ToolBridge 建议使用一组通用的开放访问数据集作为其原始数据集池，并应用一系列策略从池中识别适合插入外部工具 API 的数据条目。通过对这些精选数据条目进行监督微调，LLM 可以在适当的环境中调用外部工具来提高其预测准确性，特别是对于包括数据处理、数值计算和事实检索在内的基本功能。我们的实验严格隔离模型架构和训练配置，专注于数据的作用。实验结果表明，在 ToolBridge 上训练的 LLM 在标准基准和自定义评估数据集上都表现出一致的性能改进。所有相关代码和数据都将在此 https URL 上开源，以促进透明度并促进更广泛的社区探索为 LLM 配备外部工具功能的方法。</li>
</ul>

<h3>Title: AuditWen:An Open-Source Large Language Model for Audit</h3>
<ul>
<li><strong>Authors: </strong>Jiajia Huang, Haoran Zhu, Chao Xu, Tianming Zhan, Qianqian Xie, Jimin Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10873">https://arxiv.org/abs/2410.10873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10873">https://arxiv.org/pdf/2410.10873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10873]] AuditWen:An Open-Source Large Language Model for Audit(https://arxiv.org/abs/2410.10873)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Intelligent auditing represents a crucial advancement in modern audit practices, enhancing both the quality and efficiency of audits within the realm of artificial intelligence. With the rise of large language model (LLM), there is enormous potential for intelligent models to contribute to audit domain. However, general LLMs applied in audit domain face the challenges of lacking specialized knowledge and the presence of data biases. To overcome these challenges, this study introduces AuditWen, an open-source audit LLM by fine-tuning Qwen with constructing instruction data from audit domain. We first outline the application scenarios for LLMs in the audit and extract requirements that shape the development of LLMs tailored for audit purposes. We then propose an audit LLM, called AuditWen, by fine-tuning Qwen with constructing 28k instruction dataset from 15 audit tasks and 3 layers. In evaluation stage, we proposed a benchmark with 3k instructions that covers a set of critical audit tasks derived from the application scenarios. With the benchmark, we compare AuditWen with other existing LLMs from information extraction, question answering and document generation. The experimental results demonstrate superior performance of AuditWen both in question understanding and answer generation, making it an immediately valuable tool for audit.</li>
<li><strong>摘要：</strong>智能审计是现代审计实践的一项重要进步，在人工智能领域提高了审计的质量和效率。随着大型语言模型 (LLM) 的兴起，智能模型在审计领域的贡献潜力巨大。然而，应用于审计领域的一般 LLM 面临着缺乏专业知识和数据偏差的挑战。为了克服这些挑战，本研究引入了 AuditWen，这是一个开源审计 LLM，它通过从审计领域构建指令数据对 Qwen 进行微调。我们首先概述了 LLM 在审计中的应用场景，并提取了影响为审计目的而定制的 LLM 开发的需求。然后，我们提出了一种名为 AuditWen 的审计 LLM，它通过从 15 个审计任务和 3 个层构建 28k 指令数据集对 Qwen 进行微调。在评估阶段，我们提出了一个包含 3k 指令的基准，该基准涵盖了从应用场景中得出的一组关键审计任务。通过基准，我们将 AuditWen 与其他现有 LLM 从信息提取、问答和文档生成方面进行了比较。实验结果表明，AuditWen 在问题理解和答案生成方面均表现出色，使其成为一种立即有价值的审计工具。</li>
</ul>

<h3>Title: FreqMark: Frequency-Based Watermark for Sentence-Level Detection of LLM-Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Xu, Kun Zhang, Victor S. Sheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10876">https://arxiv.org/abs/2410.10876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10876">https://arxiv.org/pdf/2410.10876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10876]] FreqMark: Frequency-Based Watermark for Sentence-Level Detection of LLM-Generated Text(https://arxiv.org/abs/2410.10876)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The increasing use of Large Language Models (LLMs) for generating highly coherent and contextually relevant text introduces new risks, including misuse for unethical purposes such as disinformation or academic dishonesty. To address these challenges, we propose FreqMark, a novel watermarking technique that embeds detectable frequency-based watermarks in LLM-generated text during the token sampling process. The method leverages periodic signals to guide token selection, creating a watermark that can be detected with Short-Time Fourier Transform (STFT) analysis. This approach enables accurate identification of LLM-generated content, even in mixed-text scenarios with both human-authored and LLM-generated segments. Our experiments demonstrate the robustness and precision of FreqMark, showing strong detection capabilities against various attack scenarios such as paraphrasing and token substitution. Results show that FreqMark achieves an AUC improvement of up to 0.98, significantly outperforming existing detection methods.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于生成高度连贯和上下文相关的文本，这带来了新的风险，包括滥用于不道德的目的，例如虚假信息或学术欺诈。为了应对这些挑战，我们提出了 FreqMark，这是一种新颖的水印技术，可在标记采样过程中将可检测的基于频率的水印嵌入 LLM 生成的文本中。该方法利用周期性信号来指导标记选择，从而创建可通过短时傅里叶变换 (STFT) 分析检测到的水印。这种方法能够准确识别 LLM 生成的内容，即使在同时包含人工编写和 LLM 生成的片段的混合文本场景中也是如此。我们的实验证明了 FreqMark 的稳健性和精确性，显示出对各种攻击场景（例如释义和标记替换）的强大检测能力。结果表明，FreqMark 实现了高达 0.98 的 AUC 改进，明显优于现有的检测方法。</li>
</ul>

<h3>Title: Improving Data Efficiency via Curating LLM-Driven Rating Systems</h3>
<ul>
<li><strong>Authors: </strong>Jinlong Pang, Jiaheng Wei, Ankit Parag Shah, Zhaowei Zhu, Yaxuan Wang, Chen Qian, Yang Liu, Yujia Bao, Wei Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10877">https://arxiv.org/abs/2410.10877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10877">https://arxiv.org/pdf/2410.10877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10877]] Improving Data Efficiency via Curating LLM-Driven Rating Systems(https://arxiv.org/abs/2410.10877)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and biases, even in powerful models like GPT-4. In this work, we introduce DS2, a Diversity-aware Score curation method for Data Selection. By systematically modeling error patterns through a score transition matrix, DS2 corrects LLM-based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3% of the original dataset) outperforms full-scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sample size (1k samples). These findings challenge conventional data scaling assumptions, highlighting that redundant, low-quality samples can degrade performance and reaffirming that "more can be less."</li>
<li><strong>摘要：</strong>指令调整对于将大型语言模型 (LLM) 适应下游任务至关重要，最近的研究表明，少量人工整理的数据可以胜过较大的数据集，这对传统的数据缩放定律提出了挑战。虽然基于 LLM 的数据质量评级系统提供了一种经济高效的人工注释替代方案，但它们往往存在不准确性和偏差，即使在 GPT-4 等强大的模型中也是如此。在这项工作中，我们引入了 DS2，一种用于数据选择的多样性感知分数整理方法。通过分数转换矩阵系统地建模错误模式，DS2 可以纠正基于 LLM 的分数并促进所选数据样本的多样性。我们的方法表明，整理后的子集（仅占原始数据集的 3.3%）在各种机器对齐基准上的表现优于全尺寸数据集（300k 个样本），并且与具有相同样本量（1k 个样本）的人工对齐数据集（如 LIMA）相当或超过后者。这些发现挑战了传统的数据扩展假设，强调冗余、低质量的样本会降低性能，并重申“多即是少”。</li>
</ul>

<h3>Title: Herald: A Natural Language Annotated Lean 4 Dataset</h3>
<ul>
<li><strong>Authors: </strong>Guoxiong Gao, Yutong Wang, Jiedong Jiang, Qi Gao, Zihan Qin, Tianyi Xu, Bin Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10878">https://arxiv.org/abs/2410.10878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10878">https://arxiv.org/pdf/2410.10878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10878]] Herald: A Natural Language Annotated Lean 4 Dataset(https://arxiv.org/abs/2410.10878)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Verifiable formal languages like Lean have profoundly impacted mathematical reasoning, particularly through the use of large language models (LLMs) for automated reasoning. A significant challenge in training LLMs for these formal languages is the lack of parallel datasets that align natural language with formal language proofs. To address this challenge, this paper introduces a novel framework for translating the Mathlib4 corpus (a unified library of mathematics in formal language Lean 4) into natural language. Building upon this, we employ a dual augmentation strategy that combines tactic-based and informal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer. We present the results of this pipeline on Mathlib4 as Herald (Hierarchy and Retrieval-based Translated Lean Dataset). We also propose the Herald Translator, which is fine-tuned on Herald. Herald translator achieves a 93.2% accuracy (Pass@128) on formalizing statements in the miniF2F-test and a 22.5% accuracy on our internal graduate-level textbook dataset, outperforming InternLM2-Math-Plus-7B (74.0% and 7.5%) and TheoremLlama (50.1% and 4.0%). Furthermore, we propose a section-level translation framework for real-world applications. As a direct application of Herald translator, we have successfully translated a template section in the Stack project, marking a notable progress in the automatic formalization of graduate-level mathematical literature. Our model, along with the datasets, will be open-sourced to the public soon.</li>
<li><strong>摘要：</strong>可验证的形式语言（如 Lean）对数学推理产生了深远的影响，尤其是通过使用大型语言模型 (LLM) 进行自动推理。在训练这些形式语言的 LLM 时面临的一个重大挑战是缺乏将自然语言与形式语言证明对齐的并行数据集。为了应对这一挑战，本文介绍了一种将 Mathlib4 语料库（形式语言 Lean 4 的统一数学库）翻译成自然语言的新框架。在此基础上，我们采用了一种结合基于策略和基于非正式方法的双重增强策略，利用 Lean-jixia 系统（一种 Lean 4 分析器）。我们将该流程在 Mathlib4 上的结果展示为 Herald（基于层次结构和检索的翻译精益数据集）。我们还提出了 Herald 翻译器，它在 Herald 上进行了微调。 Herald 翻译器在 miniF2F 测试中的形式化语句准确率为 93.2%（Pass@128），在我们内部研究生级教科书数据集上的准确率为 22.5%，优于 InternLM2-Math-Plus-7B（74.0% 和 7.5%）和 TheoremLlama（50.1% 和 4.0%）。此外，我们提出了一个适用于实际应用的章节级翻译框架。作为 Herald 翻译器的直接应用，我们已成功翻译了 Stack 项目中的模板章节，标志着研究生级数学文献自动形式化方面取得了显著进展。我们的模型以及数据集将很快向公众开源。</li>
</ul>

<h3>Title: Fine-tuning can Help Detect Pretraining Data from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hengxiang Zhang, Songxin Zhang, Bingyi Jing, Hongxin Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10880">https://arxiv.org/abs/2410.10880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10880">https://arxiv.org/pdf/2410.10880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10880]] Fine-tuning can Help Detect Pretraining Data from Large Language Models(https://arxiv.org/abs/2410.10880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs perform differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation (FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 时代，出于对公平评估和道德风险的担忧，检测预训练数据变得越来越重要。当前的方法通过设计评分函数（如困惑度和 Min-k%）来区分成员和非成员。然而，训练数据的多样性和复杂性增加了区分的难度，导致检测预训练数据的性能不佳。在本文中，我们首先探讨了未见数据的好处，这些数据可以在 LLM 发布后轻松收集。我们发现，在使用少量以前未见的数据进行微调后，LLM 对成员和非成员的困惑度表现不同。鉴于此，我们引入了一种新颖有效的方法，称为微调分数偏差 (FSD)，它提高了当前评分函数用于预训练数据检测的性能。具体而言，我们建议在同一领域内对少量未见数据进行微调后测量当前分数的偏差距离。实际上，使用少量未见数据会大幅降低所有非会员的得分，导致偏差距离大于会员。大量实验证明了我们方法的有效性，显著提高了各种模型在常见基准数据集上的 AUC 得分。</li>
</ul>

<h3>Title: Performance in a dialectal profiling task of LLMs for varieties of Brazilian Portuguese</h3>
<ul>
<li><strong>Authors: </strong>Raquel Meister Ko Freitag, Túlio Sousa de Gois</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10991">https://arxiv.org/abs/2410.10991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10991">https://arxiv.org/pdf/2410.10991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10991]] Performance in a dialectal profiling task of LLMs for varieties of Brazilian Portuguese(https://arxiv.org/abs/2410.10991)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Different of biases are reproduced in LLM-generated responses, including dialectal biases. A study based on prompt engineering was carried out to uncover how LLMs discriminate varieties of Brazilian Portuguese, specifically if sociolinguistic rules are taken into account in four LLMs: GPT 3.5, GPT-4o, Gemini, and Sabi.-2. The results offer sociolinguistic contributions for an equity fluent NLP technology.</li>
<li><strong>摘要：</strong>在 LLM 生成的响应中重现了各种偏见，包括方言偏见。进行了一项基于即时工程的研究，以揭示 LLM 如何区分巴西葡萄牙语的变体，特别是如果在四个 LLM 中考虑社会语言学规则：GPT 3.5、GPT-4o、Gemini 和 Sabi.-2。结果为公平流畅的 NLP 技术提供了社会语言学贡献。</li>
</ul>

<h3>Title: Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs</h3>
<ul>
<li><strong>Authors: </strong>Haozhen Zhang, Tao Feng, Jiaxuan You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11001">https://arxiv.org/abs/2410.11001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11001">https://arxiv.org/pdf/2410.11001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11001]] Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs(https://arxiv.org/abs/2410.11001)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. Compared with long-context LLMs, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose \textit{graph of records} (\textbf{GoR}), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the \textit{retrieve-then-generate} paradigm of RAG, we construct a graph by establishing an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the intricate correlations between them, GoR further features a \textit{graph neural network} and an elaborately designed \textit{BERTScore}-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines across four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance e.g., 15\%, 8\%, and 19\% improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP dataset). Extensive experiments further demonstrate the effectiveness of GoR. Code is available at this https URL</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过注入非参数事实知识使大型语言模型 (LLM) 焕发了新的活力。与长上下文 LLM 相比，RAG 被认为是一种更简洁、更轻量级的有效摘要工具，它可以使用不同的查询与 LLM 进行多次交互以获得全面的响应。然而，现有方法在很大程度上忽略和丢弃了包含潜在有见地的信息的 LLM 生成的历史响应，导致结果不理想。在本文中，我们提出了 \textit{记录图} (\textbf{GoR})，它利用 LLM 生成的历史响应来增强 RAG 以进行长上下文全局摘要。受 RAG 的 \textit{检索-然后-生成} 范式的启发，我们通过在检索到的文本块和相应的 LLM 生成的响应之间建立边来构建图。为了进一步揭示它们之间的复杂关联，GoR 还采用了 \textit{图神经网络} 和精心设计的基于 \textit{BERTScore} 的自监督模型训练目标，从而实现参考摘要和节点嵌入之间的无缝监督信号反向传播。我们将 GoR 与四个长上下文摘要数据集中的 12 个基线进行了全面比较，结果表明，我们提出的方法达到了最佳性能，例如，在 WCEP 数据集上，与 Rouge-L、Rouge-1 和 Rouge-2 检索器相比，性能提高了 15\%、8\% 和 19\%。大量实验进一步证明了 GoR 的有效性。代码可从此 https URL 获取</li>
</ul>

<h3>Title: One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, Adrian de Wynter, Jing Yao, Si-Qing Chen, Michael Wooldridge, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11005">https://arxiv.org/abs/2410.11005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11005">https://arxiv.org/pdf/2410.11005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11005]] One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks(https://arxiv.org/abs/2410.11005)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Language is not monolithic. While many benchmarks are used as proxies to systematically estimate Large Language Models' (LLM) performance in real-life tasks, they tend to ignore the nuances of within-language variation and thus fail to model the experience of speakers of minority dialects. Focusing on African American Vernacular English (AAVE), we present the first study on LLMs' fairness and robustness to a dialect in canonical reasoning tasks (algorithm, math, logic, and comprehensive reasoning). We hire AAVE speakers, including experts with computer science backgrounds, to rewrite seven popular benchmarks, such as HumanEval and GSM8K. The result of this effort is ReDial, a dialectal benchmark comprising $1.2K+$ parallel query pairs in Standardized English and AAVE. We use ReDial to evaluate state-of-the-art LLMs, including GPT-4o/4/3.5-turbo, LLaMA-3.1/3, Mistral, and Phi-3. We find that, compared to Standardized English, almost all of these widely used models show significant brittleness and unfairness to queries in AAVE. Furthermore, AAVE queries can degrade performance more substantially than misspelled texts in Standardized English, even when LLMs are more familiar with the AAVE queries. Finally, asking models to rephrase questions in Standardized English does not close the performance gap but generally introduces higher costs. Overall, our findings indicate that LLMs provide unfair service to dialect users in complex reasoning tasks. Code can be found at this https URL.</li>
<li><strong>摘要：</strong>语言不是一成不变的。虽然许多基准测试被用作系统地评估大型语言模型 (LLM) 在实际任务中的表现的代理，但它们往往忽略了语言内差异的细微差别，因此无法模拟少数民族方言使用者的体验。我们专注于非裔美国人白话英语 (AAVE)，首次研究了 LLM 在规范推理任务（算法、数学、逻辑和综合推理）中对方言的公平性和鲁棒性。我们聘请了 AAVE 发言人，包括具有计算机科学背景的专家，来重写七个流行的基准测试，例如 HumanEval 和 GSM8K。这项工作的成果是 ReDial，这是一个方言基准测试，包含标准英语和 AAVE 中的 $1.2K+$ 个并行查询对。我们使用 ReDial 来评估最先进的 LLM，包括 GPT-4o/4/3.5-turbo、LLaMA-3.1/3、Mistral 和 Phi-3。我们发现，与标准英语相比，几乎所有这些广泛使用的模型都表现出对 AAVE 查询的严重脆弱性和不公平性。此外，即使 LLM 更熟悉 AAVE 查询，AAVE 查询也会比标准英语中拼写错误的文本更严重地降低性能。最后，要求模型用标准英语重新表述问题并不能缩小性能差距，但通常会带来更高的成本。总体而言，我们的研究结果表明，LLM 在复杂的推理任务中为方言用户提供了不公平的服务。代码可以在这个 https URL 中找到。</li>
</ul>

<h3>Title: Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Abdellah El Mekki, Muhammad Abdul-Mageed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11006">https://arxiv.org/abs/2410.11006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11006">https://arxiv.org/pdf/2410.11006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11006]] Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with LLMs(https://arxiv.org/abs/2410.11006)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performance on a wide range of natural language processing (NLP) tasks, primarily through in-context learning (ICL). In ICL, the LLM is provided with examples that represent a given task such that it learns to generate answers for test inputs. However, access to these in-context examples is not guaranteed especially for low-resource or massively multilingual tasks. In this work, we propose an unsupervised approach to mine in-context examples for machine translation (MT), enabling unsupervised MT (UMT) across different languages. Our approach begins with word-level mining to acquire word translations that are then used to perform sentence-level mining. As the quality of mined parallel pairs may not be optimal due to noise or mistakes, we introduce a filtering criterion to select the optimal in-context examples from a pool of unsupervised parallel sentences. We evaluate our approach using two multilingual LLMs on 288 directions from the FLORES-200 dataset and analyze the impact of various linguistic features on performance. Our findings demonstrate the effectiveness of our unsupervised approach in mining in-context examples for MT, leading to better or comparable translation performance as translation with regular in-context samples (extracted from human-annotated data), while also outperforming the other state-of-the-art UMT methods by an average of $7$ BLEU points.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在广泛的自然语言处理 (NLP) 任务中表现出色，主要是通过上下文学习 (ICL)。在 ICL 中，LLM 会获得代表给定任务的示例，以便它学习生成测试输入的答案。但是，无法保证能够访问这些上下文示例，尤其是对于资源较少或大量多语言的任务。在这项工作中，我们提出了一种无监督方法来挖掘机器翻译 (MT) 的上下文示例，从而实现跨不同语言的无监督 MT (UMT)。我们的方法从单词级挖掘开始，以获取单词翻译，然后将其用于执行句子级挖掘。由于挖掘出的平行对的质量可能由于噪音或错误而不是最佳的，我们引入了一个过滤标准，以从无监督的平行句子池中选择最佳的上下文示例。我们使用来自 FLORES-200 数据集的 288 个方向上的两个多语言 LLM 来评估我们的方法，并分析各种语言特征对性能的影响。我们的研究结果证明了我们的无监督方法在挖掘机器翻译的上下文示例方面的有效性，其翻译性能比使用常规上下文样本（从人工注释数据中提取）的翻译更好或相当，同时也比其他最先进的 UMT 方法平均高出 $7$ BLEU 点。</li>
</ul>

<h3>Title: Assessing the Human Likeness of AI-Generated Counterspeech</h3>
<ul>
<li><strong>Authors: </strong>Xiaoying Song, Sujana Mamidisetty, Eduardo Blanco, Lingzi Hong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11007">https://arxiv.org/abs/2410.11007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11007">https://arxiv.org/pdf/2410.11007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11007]] Assessing the Human Likeness of AI-Generated Counterspeech(https://arxiv.org/abs/2410.11007)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Counterspeech is a targeted response to counteract and challenge abusive or hateful content. It can effectively curb the spread of hatred and foster constructive online communication. Previous studies have proposed different strategies for automatically generated counterspeech. Evaluations, however, focus on the relevance, surface form, and other shallow linguistic characteristics. In this paper, we investigate the human likeness of AI-generated counterspeech, a critical factor influencing effectiveness. We implement and evaluate several LLM-based generation strategies, and discover that AI-generated and human-written counterspeech can be easily distinguished by both simple classifiers and humans. Further, we reveal differences in linguistic characteristics, politeness, and specificity.</li>
<li><strong>摘要：</strong>反驳是一种针对性的反应，旨在抵制和挑战辱骂性或仇恨性内容。它可以有效地遏制仇恨的蔓延，促进建设性的在线交流。先前的研究提出了不同的自动生成反驳策略。然而，评估侧重于相关性、表面形式和其他浅显的语言特征。在本文中，我们研究了人工智能生成的反驳与人类的相似性，这是影响有效性的一个关键因素。我们实施并评估了几种基于 LLM 的生成策略，发现人工智能生成的反驳和人类编写的反驳很容易被简单分类器和人类区分。此外，我们揭示了语言特征、礼貌性和特异性方面的差异。</li>
</ul>

<h3>Title: Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Bokai Hu, Sai Ashish Somayajula, Xin Pan, Zihan Huang, Pengtao Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11020">https://arxiv.org/abs/2410.11020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11020">https://arxiv.org/pdf/2410.11020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11020]] Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning(https://arxiv.org/abs/2410.11020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), built on decoder-only transformers, excel in natural language generation and adapt to diverse tasks using zero-shot and few-shot prompting. However, these prompting methods often struggle on natural language understanding (NLU) tasks, where encoder-only models like BERT-base outperform LLMs on benchmarks like GLUE and SuperGLUE. This paper explores two approaches-supervised fine-tuning (SFT) and proximal policy optimization (PPO)-to enhance LLMs' NLU abilities. To reduce the cost of full-model fine-tuning, we integrate low-rank adaptation (LoRA) layers, limiting updates to these layers during both SFT and PPO. In SFT, task-specific prompts are concatenated with input queries and ground-truth labels, optimizing with next-token prediction. Despite this, LLMs still underperform compared to models like BERT-base on several NLU tasks. To close this gap, we apply PPO, a reinforcement learning technique that treats each token generation as an action and uses a reward function based on alignment with ground-truth answers. PPO then updates the model to maximize these rewards, aligning outputs with correct labels. Our experiments with LLAMA2-7B show that PPO improves performance, with a 6.3-point gain over SFT on GLUE. PPO exceeds zero-shot by 38.7 points and few-shot by 26.1 points on GLUE, while surpassing these by 28.8 and 28.5 points on SuperGLUE. Additionally, PPO outperforms BERT-large by 2.7 points on GLUE and 9.3 points on SuperGLUE. The improvements are consistent across models like Qwen2.5-7B and MPT-7B, highlighting PPO's robustness in enhancing LLMs' NLU capabilities.</li>
<li><strong>摘要：</strong>基于仅解码器转换器构建的大型语言模型 (LLM) 在自然语言生成方面表现出色，并使用零样本和少样本提示适应各种任务。然而，这些提示方法通常在自然语言理解 (NLU) 任务上表现不佳，其中仅编码器模型（如 BERT-base）在 GLUE 和 SuperGLUE 等基准测试中的表现优于 LLM。本文探讨了两种方法——监督微调 (SFT) 和近端策略优化 (PPO)——以增强 LLM 的 NLU 能力。为了降低全模型微调的成本，我们集成了低秩自适应 (LoRA) 层，在 SFT 和 PPO 期间限制对这些层的更新。在 SFT 中，特定于任务的提示与输入查询和基本事实标签连接在一起，并通过下一个标记预测进行优化。尽管如此，与 BERT-base 等模型相比，LLM 在几个 NLU 任务上的表现仍然不佳。为了缩小这一差距，我们应用了 PPO，这是一种强化学习技术，它将每个 token 生成视为一个动作，并使用基于与真实答案对齐的奖励函数。然后，PPO 更新模型以最大化这些奖励，将输出与正确的标签对齐。我们对 LLAMA2-7B 的实验表明，PPO 提高了性能，在 GLUE 上比 SFT 高出 6.3 个百分点。PPO 在 GLUE 上超过零样本 38.7 分，在少样本 26.1 分，而在 SuperGLUE 上分别超过零样本 28.8 分和 28.5 分。此外，PPO 在 GLUE 上比 BERT-large 高出 2.7 分，在 SuperGLUE 上比 BERT-large 高出 9.3 分。这些改进在 Qwen2.5-7B 和 MPT-7B 等模型中是一致的，凸显了 PPO 在增强 LLM 的 NLU 功能方面的稳健性。</li>
</ul>

<h3>Title: Persistent Topological Features in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuri Gardinazzi, Giada Panerai, Karthik Viswanathan, Alessio Ansuini, Alberto Cazzaniga, Matteo Biagetti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CG, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11042">https://arxiv.org/abs/2410.11042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11042">https://arxiv.org/pdf/2410.11042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11042]] Persistent Topological Features in Large Language Models(https://arxiv.org/abs/2410.11042)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Understanding the decision-making processes of large language models (LLMs) is critical given their widespread applications. Towards this goal, describing the topological and geometrical properties of internal representations has recently provided valuable insights. For a more comprehensive characterization of these inherently complex spaces, we present a novel framework based on zigzag persistence, a method in topological data analysis (TDA) well-suited for describing data undergoing dynamic transformations across layers. Within this framework, we introduce persistence similarity, a new metric that quantifies the persistence and transformation of topological features such as $p$-cycles throughout the model layers. Unlike traditional similarity measures, our approach captures the entire evolutionary trajectory of these features, providing deeper insights into the internal workings of LLMs. As a practical application, we leverage persistence similarity to identify and prune redundant layers, demonstrating comparable performance to state-of-the-art methods across several benchmark datasets. Additionally, our analysis reveals consistent topological behaviors across various models and hyperparameter settings, suggesting a universal structure in LLM internal representations.</li>
<li><strong>摘要：</strong>鉴于大型语言模型 (LLM) 的广泛应用，了解其决策过程至关重要。为了实现这一目标，描述内部表示的拓扑和几何特性最近提供了宝贵的见解。为了更全面地描述这些固有的复杂空间，我们提出了一个基于锯齿形持久性的新框架，这是一种拓扑数据分析 (TDA) 方法，非常适合描述跨层进行动态转换的数据。在这个框架内，我们引入了持久性相似性，这是一种新的度量标准，可以量化整个模型层中拓扑特征（例如 $p$ 循环）的持久性和转换。与传统的相似性度量不同，我们的方法可以捕获这些特征的整个进化轨迹，从而更深入地了解 LLM 的内部工作原理。作为实际应用，我们利用持久性相似性来识别和修剪冗余层，在多个基准数据集上表现出与最先进方法相当的性能。此外，我们的分析揭示了跨各种模型和超参数设置的一致拓扑行为，表明 LLM 内部表示具有通用结构。</li>
</ul>

<h3>Title: Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only</h3>
<ul>
<li><strong>Authors: </strong>Jihan Yao, Wenxuan Ding, Shangbin Feng, Lucy Lu Wang, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11055">https://arxiv.org/abs/2410.11055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11055">https://arxiv.org/pdf/2410.11055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11055]] Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only(https://arxiv.org/abs/2410.11055)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the absence of abundant reliable annotations for challenging tasks and contexts, how can we expand the frontier of LLM capabilities with potentially wrong answers? We focus on two research questions: (1) Can LLMs generate reliable preferences among wrong options? And if so, (2) Would alignment with such wrong-over-wrong preferences be helpful? We employ methods based on self-consistency, token probabilities, and LLM-as-a-judge to elicit wrong-over-wrong preferences, and fine-tune language models with preference optimization approaches using these synthesized preferences. Extensive experiments with seven LLMs and eight datasets demonstrate that (1) LLMs do have preliminary capability in distinguishing various shades of wrong, achieving up to 20.9% higher performance than random guess; (2) Alignment with wrong-over-wrong preferences helps LLMs to produce less wrong and sometimes even outright correct answers, while overall improving model calibration.</li>
<li><strong>摘要：</strong>在缺乏针对具有挑战性的任务和上下文的大量可靠注释的情况下，我们如何扩展 LLM 在潜在错误答案方面的能力边界？我们关注两个研究问题：（1）LLM 能否在错误选项中生成可靠的偏好？如果可以，（2）与这种“错对错”偏好保持一致是否有帮助？我们采用基于自洽、标记概率和 LLM-as-a-judge 的方法来引出“错对错”偏好，并使用这些合成的偏好通过偏好优化方法对语言模型进行微调。对七个 LLM 和八个数据集进行的大量实验表明：（1）LLM 确实具有区分各种错误程度的初步能力，其性能比随机猜测高出 20.9%；（2）与“错对错”偏好保持一致有助于 LLM 产生更少的错误答案，有时甚至是完全正确的答案，同时整体上改善了模型校准。</li>
</ul>

<h3>Title: Assessing Bias in Metric Models for LLM Open-Ended Generation Bias Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Nathaniel Demchak, Xin Guan, Zekun Wu, Ziyi Xu, Adriano Koshiyama, Emre Kazim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11059">https://arxiv.org/abs/2410.11059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11059">https://arxiv.org/pdf/2410.11059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11059]] Assessing Bias in Metric Models for LLM Open-Ended Generation Bias Benchmarks(https://arxiv.org/abs/2410.11059)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Open-generation bias benchmarks evaluate social biases in Large Language Models (LLMs) by analyzing their outputs. However, the classifiers used in analysis often have inherent biases, leading to unfair conclusions. This study examines such biases in open-generation benchmarks like BOLD and SAGED. Using the MGSD dataset, we conduct two experiments. The first uses counterfactuals to measure prediction variations across demographic groups by altering stereotype-related prefixes. The second applies explainability tools (SHAP) to validate that the observed biases stem from these counterfactuals. Results reveal unequal treatment of demographic descriptors, calling for more robust bias metric models.</li>
<li><strong>摘要：</strong>开放代偏见基准通过分析大型语言模型 (LLM) 的输出来评估其中的社会偏见。然而，分析中使用的分类器通常具有固有偏见，从而导致不公平的结论。本研究考察了 BOLD 和 SAGED 等开放代基准中的此类偏见。使用 MGSD 数据集，我们进行了两项实验。第一个实验使用反事实通过改变刻板印象相关的前缀来衡量不同人口群体之间的预测变化。第二个实验应用可解释性工具 (SHAP) 来验证观察到的偏见是否源于这些反事实。结果显示对人口描述符的处理不平等，需要更强大的偏见度量模型。</li>
</ul>

<h3>Title: PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries</h3>
<ul>
<li><strong>Authors: </strong>Mingwen Dong, Nischal Ashok Kumar, Yiqun Hu, Anuj Chauhan, Chung-Wei Hang, Shuaichen Chang, Lin Pan, Wuwei Lan, Henghui Zhu, Jiarong Jiang, Patrick Ng, Zhiguo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11076">https://arxiv.org/abs/2410.11076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11076">https://arxiv.org/pdf/2410.11076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11076]] PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries(https://arxiv.org/abs/2410.11076)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seeking clarification, the user's clarification, and the assistant's clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We will release our code for data generation and experiments on GitHub.</li>
<li><strong>摘要：</strong>以前的文本到 SQL 数据集和系统主要关注具有明确意图且可以回答的用户问题。然而，由于缺乏相关数据，真正的用户问题往往是模棱两可的，有多种解释，或者无法回答。在这项工作中，我们构建了一个实用的对话式文本到 SQL 数据集 PRACTIQ，其中包含受现实世界用户问题启发的模棱两可和无法回答的问题。我们首先通过研究现有的文本到 SQL 数据集确定了四类模棱两可的问题和四类无法回答的问题。然后，我们生成四个回合的对话：初始用户问题、寻求澄清的助手回答、用户的澄清以及助手的澄清 SQL 回答以及对执行结果的自然语言解释。对于一些模棱两可的查询，我们还直接生成有用的 SQL 响应，这些响应考虑了模棱两可的多个方面，而不是要求用户澄清。为了对模棱两可、无法回答和可回答的问题的性能进行基准测试，我们使用各种 LLM 实现了基于大型语言模型 (LLM) 的基线。我们的方法包括两个步骤：问题类别分类和澄清 SQL 预测。我们的实验表明，最先进的系统很难有效地处理模棱两可和无法回答的问题。我们将在 GitHub 上发布用于数据生成和实验的代码。</li>
</ul>

<h3>Title: Code-Mixer Ya Nahi: Novel Approaches to Measuring Multilingual LLMs' Code-Mixing Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Ayushman Gupta, Akhil Bhogal, Kripabandhu Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11079">https://arxiv.org/abs/2410.11079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11079">https://arxiv.org/pdf/2410.11079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11079]] Code-Mixer Ya Nahi: Novel Approaches to Measuring Multilingual LLMs' Code-Mixing Capabilities(https://arxiv.org/abs/2410.11079)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Multilingual Large Language Models (LLMs) have demonstrated exceptional performance in Machine Translation (MT) tasks. However, their MT abilities in the context of code-switching (the practice of mixing two or more languages in an utterance) remain under-explored. In this paper, we introduce Rule-Based Prompting, a novel prompting technique to generate code-mixed sentences. We measure and compare the code-mixed MT abilities of 3 popular multilingual LLMs: GPT-3.5-turbo, GPT-4, and Gemini Pro across five language pairs: English-{Hindi, Bengali, Gujarati, French, Spanish} using $k$-shot prompting ($k\in\{0, 1, 10, 20\}$) and Rule-Based Prompting. Our findings suggest that though $k$-shot prompting often leads to the best results, Rule-Based prompting shows promise in generating unique code-mixed sentences that vary in their style of code-mixing. We also use $k$-shot prompting to gauge the code-mixed to English translation abilities of multilingual LLMs. For this purpose, we create a gold-standard code-mixed dataset spanning five language pairs: English-{Hindi, Bengali, Gujarati, French, Spanish}. As a real-world application of our work, we create a code-mixed chatbot.</li>
<li><strong>摘要：</strong>多语言大型语言模型 (LLM) 在机器翻译 (MT) 任务中表现出色。然而，它们在代码转换（在一句话中混合两种或多种语言的做法）背景下的机器翻译能力仍未得到充分探索。在本文中，我们介绍了一种基于规则的提示，这是一种生成代码混合句子的新型提示技术。我们使用 $k$ 次提示 ($k\in\{0, 1, 10, 20\}$) 和基于规则的提示，测量并比较了 3 种流行的多语言 LLM：GPT-3.5-turbo、GPT-4 和 Gemini Pro 在五种语言对（英语-{印地语、孟加拉语、古吉拉特语、法语、西班牙语}）中的代码混合机器翻译能力。我们的研究结果表明，尽管 $k$ 次提示通常会产生最佳结果，但基于规则的提示在生成代码混合风格各异的独特代码混合句子方面显示出良好的前景。我们还使用 $k$-shot 提示来衡量多语言 LLM 的代码混合到英语翻译能力。为此，我们创建了一个涵盖五种语言对的黄金标准代码混合数据集：英语-{印地语、孟加拉语、古吉拉特语、法语、西班牙语}。作为我们工作的实际应用，我们创建了一个代码混合聊天机器人。</li>
</ul>

<h3>Title: Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts</h3>
<ul>
<li><strong>Authors: </strong>Sharon Levy, William D. Adler, Tahilin Sanchez Karver, Mark Dredze, Michelle R. Kaufman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11084">https://arxiv.org/abs/2410.11084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11084">https://arxiv.org/pdf/2410.11084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11084]] Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts(https://arxiv.org/abs/2410.11084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) acquire beliefs about gender from training data and can therefore generate text with stereotypical gender attitudes. Prior studies have demonstrated model generations favor one gender or exhibit stereotypes about gender, but have not investigated the complex dynamics that can influence model reasoning and decision-making involving gender. We study gender equity within LLMs through a decision-making lens with a new dataset, DeMET Prompts, containing scenarios related to intimate, romantic relationships. We explore nine relationship configurations through name pairs across three name lists (men, women, neutral). We investigate equity in the context of gender roles through numerous lenses: typical and gender-neutral names, with and without model safety enhancements, same and mixed-gender relationships, and egalitarian versus traditional scenarios across various topics. While all models exhibit the same biases (women favored, then those with gender-neutral names, and lastly men), safety guardrails reduce bias. In addition, models tend to circumvent traditional male dominance stereotypes and side with 'traditionally female' individuals more often, suggesting relationships are viewed as a female domain by the models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 从训练数据中获取关于性别的信念，因此可以生成具有刻板性别态度的文本。先前的研究表明，模型生成偏向一种性别或表现出对性别的刻板印象，但尚未研究可能影响涉及性别的模型推理和决策的复杂动态。我们通过决策视角研究 LLM 中的性别平等，使用新数据集 DeMET Prompts，其中包含与亲密、浪漫关系相关的场景。我们通过三个姓名列表（男性、女性、中性）中的名字对探索九种关系配置。我们从多个视角研究性别角色背景下的平等：典型和性别中性的名字、有和没有模型安全增强、相同和混合性别关系，以及各个主题中的平等与传统场景。虽然所有模型都表现出相同的偏见（女性受青睐，然后是那些具有性别中性名字的人，最后是男性），但安全护栏可以减少偏见。此外，模特倾向于规避传统的男性主导刻板印象，而更多地站在“传统女性”一边，这表明模特将恋爱关系视为女性的领域。</li>
</ul>

<h3>Title: Active Learning for Robust and Representative LLM Generation in Safety-Critical Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Sabit Hassan, Anthony Sicilia, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11114">https://arxiv.org/abs/2410.11114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11114">https://arxiv.org/pdf/2410.11114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11114]] Active Learning for Robust and Representative LLM Generation in Safety-Critical Scenarios(https://arxiv.org/abs/2410.11114)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Ensuring robust safety measures across a wide range of scenarios is crucial for user-facing systems. While Large Language Models (LLMs) can generate valuable data for safety measures, they often exhibit distributional biases, focusing on common scenarios and neglecting rare but critical cases. This can undermine the effectiveness of safety protocols developed using such data. To address this, we propose a novel framework that integrates active learning with clustering to guide LLM generation, enhancing their representativeness and robustness in safety scenarios. We demonstrate the effectiveness of our approach by constructing a dataset of 5.4K potential safety violations through an iterative process involving LLM generation and an active learner model's feedback. Our results show that the proposed framework produces a more representative set of safety scenarios without requiring prior knowledge of the underlying data distribution. Additionally, data acquired through our method improves the accuracy and F1 score of both the active learner model as well models outside the scope of active learning process, highlighting its broad applicability.</li>
<li><strong>摘要：</strong>对于面向用户的系统来说，确保在各种场景中采取稳健的安全措施至关重要。虽然大型语言模型 (LLM) 可以为安全措施生成有价值的数据，但它们通常表现出分布偏差，侧重于常见场景而忽略罕见但关键的情况。这可能会破坏使用此类数据开发的安全协议的有效性。为了解决这个问题，我们提出了一个新颖的框架，将主动学习与聚类相结合以指导 LLM 生成，增强其在安全场景中的代表性和稳健性。我们通过涉及 LLM 生成和主动学习者模型反馈的迭代过程构建了一个包含 5.4K 个潜在安全违规行为的数据集，证明了我们方法的有效性。我们的结果表明，所提出的框架可以生成一组更具代表性的安全场景，而无需事先了解底层数据分布。此外，通过我们的方法获取的数据提高了主动学习者模型以及主动学习过程范围之外的模型的准确性和 F1 分数，凸显了其广泛的适用性。</li>
</ul>

<h3>Title: A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education</h3>
<ul>
<li><strong>Authors: </strong>Eason Chen, Danyang Wang, Luyi Xu, Chen Cao, Xiao Fang, Jionghao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11123">https://arxiv.org/abs/2410.11123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11123">https://arxiv.org/pdf/2410.11123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11123]] A Systematic Review on Prompt Engineering in Large Language Models for K-12 STEM Education(https://arxiv.org/abs/2410.11123)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have the potential to enhance K-12 STEM education by improving both teaching and learning processes. While previous studies have shown promising results, there is still a lack of comprehensive understanding regarding how LLMs are effectively applied, specifically through prompt engineering-the process of designing prompts to generate desired outputs. To address this gap, our study investigates empirical research published between 2021 and 2024 that explores the use of LLMs combined with prompt engineering in K-12 STEM education. Following the PRISMA protocol, we screened 2,654 papers and selected 30 studies for analysis. Our review identifies the prompting strategies employed, the types of LLMs used, methods of evaluating effectiveness, and limitations in prior work. Results indicate that while simple and zero-shot prompting are commonly used, more advanced techniques like few-shot and chain-of-thought prompting have demonstrated positive outcomes for various educational tasks. GPT-series models are predominantly used, but smaller and fine-tuned models (e.g., Blender 7B) paired with effective prompt engineering outperform prompting larger models (e.g., GPT-3) in specific contexts. Evaluation methods vary significantly, with limited empirical validation in real-world settings.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 有可能通过改进教学和学习过程来增强 K-12 STEM 教育。虽然先前的研究已经显示出令人鼓舞的结果，但对于如何有效应用 LLM，特别是通过提示工程（即设计提示以产生所需输出的过程），仍然缺乏全面的了解。为了解决这一差距，我们的研究调查了 2021 年至 2024 年期间发表的实证研究，这些研究探讨了 LLM 与提示工程在 K-12 STEM 教育中的结合使用。按照 PRISMA 协议，我们筛选了 2,654 篇论文并选择了 30 项研究进行分析。我们的审查确定了所采用的提示策略、使用的 LLM 类型、评估有效性的方法以及先前工作的局限性。结果表明，虽然简单和零样本提示通常被使用，但更先进的技术（如少样本和思路链提示）已在各种教育任务中显示出积极的结果。 GPT 系列模型被广泛使用，但较小且经过微调的模型（例如 Blender 7B）与有效的提示工程相结合，在特定情况下的表现优于提示较大的模型（例如 GPT-3）。评估方法差异很大，在现实环境中的经验验证有限。</li>
</ul>

<h3>Title: LLM Unlearning via Loss Adjustment with Only Forget Data</h3>
<ul>
<li><strong>Authors: </strong>Yaxuan Wang, Jiaheng Wei, Chris Yuhao Liu, Jinlong Pang, Quan Liu, Ankit Parag Shah, Yujia Bao, Yang Liu, Wei Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11143">https://arxiv.org/abs/2410.11143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11143">https://arxiv.org/pdf/2410.11143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11143]] LLM Unlearning via Loss Adjustment with Only Forget Data(https://arxiv.org/abs/2410.11143)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Unlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately balance unlearning performance with overall model utility. This challenge arises because leveraging explicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model tends to blur the boundaries between the forgotten and retain data, as different queries often elicit similar responses. In this work, we propose eliminating the need to retain data or the reference LLM for response calibration in LLM unlearning. Recognizing that directly applying gradient ascent on the forget data often leads to optimization instability and poor performance, our method guides the LLM on what not to respond to, and importantly, how to respond, based on the forget data. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a "flat" loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model's retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的反学习对于确保合乎道德和负责任的 AI 使用至关重要，尤其是在解决隐私泄露、偏见、安全和不断变化的法规方面。现有的 LLM 反学习方法通​​常依赖于保留数据或参考 LLM，但它们难以充分平衡反学习性能和整体模型效用。这一挑战之所以出现，是因为利用参考 LLM 中的显式保留数据或保留数据的隐性知识来微调模型往往会模糊遗忘数据和保留数据之间的界限，因为不同的查询通常会引发类似的响应。在这项工作中，我们建议在 LLM 反学习中消除对保留数据或参考 LLM 进行响应校准的需要。认识到直接对遗忘数据应用梯度上升通常会导致优化不稳定和性能不佳，我们的方法根据遗忘数据指导 LLM 不要响应什么，以及重要的是，如何响应。因此，我们引入了仅遗忘数据损失调整 (FLAT)，这是一种“平坦”损失调整方法，通过最大化可用模板答案和仅相对于遗忘数据的遗忘答案之间的 f 散度来解决这些问题。定义的 f 散度的变分形式在理论上提供了一种损失调整方法，即为相对于模板响应的学习和遗忘要进行反学习的响应分配不同的重要性权重。实证结果表明，与现有方法相比，我们的方法不仅实现了卓越的反学习性能，而且还最大限度地减少了对模型保留能力的影响，确保在不同任务中的高实用性，包括哈利波特数据集和 MUSE Benchmark 上的版权内容反学习，以及 TOFU 数据集上的实体反学习。</li>
</ul>

<h3>Title: Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Shangbin Feng, Zifeng Wang, Yike Wang, Sayna Ebrahimi, Hamid Palangi, Lesly Miculicich, Achin Kulshrestha, Nathalie Rauschmayr, Yejin Choi, Yulia Tsvetkov, Chen-Yu Lee, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11163">https://arxiv.org/abs/2410.11163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11163">https://arxiv.org/pdf/2410.11163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11163]] Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence(https://arxiv.org/abs/2410.11163)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We propose Model Swarms, a collaborative search algorithm to adapt LLMs via swarm intelligence, the collective behavior guiding individual systems. Specifically, Model Swarms starts with a pool of LLM experts and a utility function. Guided by the best-found checkpoints across models, diverse LLM experts collaboratively move in the weight space and optimize a utility function representing model adaptation objectives. Compared to existing model composition approaches, Model Swarms offers tuning-free model adaptation, works in low-data regimes with as few as 200 examples, and does not require assumptions about specific experts in the swarm or how they should be composed. Extensive experiments demonstrate that Model Swarms could flexibly adapt LLM experts to a single task, multi-task domains, reward models, as well as diverse human interests, improving over 12 model composition baselines by up to 21.0% across tasks and contexts. Further analysis reveals that LLM experts discover previously unseen capabilities in initial checkpoints and that Model Swarms enable the weak-to-strong transition of experts through the collaborative search process.</li>
<li><strong>摘要：</strong>我们提出了 Model Swarms，这是一种协作搜索算法，可通过群体智能（指导单个系统的集体行为）来调整 LLM。具体来说，Model Swarms 从一组 LLM 专家和一个效用函数开始。在模型中最佳检查点的指导下，不同的 LLM 专家在权重空间中协作移动并优化代表模型适应目标的效用函数。与现有的模型组合方法相比，Model Swarms 提供无需调整的模型适应，在仅需 200 个示例的低数据环境下工作，并且不需要对群体中的特定专家或他们应该如何组合做出假设。大量实验表明，Model Swarms 可以灵活地让 LLM 专家适应单一任务、多任务领域、奖励模型以及不同的人类兴趣，在任务和环境中将超过 12 个模型组合基线提高高达 21.0%。进一步分析表明，LLM 专家在初始检查点发现了以前未见过的能力，并且 Model Swarms 通过协作搜索过程实现了专家从弱到强的转变。</li>
</ul>

<h3>Title: Athena: Retrieval-augmented Legal Judgment Prediction with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Peng, Liang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11195">https://arxiv.org/abs/2410.11195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11195">https://arxiv.org/pdf/2410.11195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11195]] Athena: Retrieval-augmented Legal Judgment Prediction with Large Language Models(https://arxiv.org/abs/2410.11195)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, retrieval-augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) like ChatGPT, LLaMA, and Claude have prevailed in countless domains, including legal scenarios. With LLMs' rapid technological progress, the development of prompt engineering (PE) as an interface between the LLMs and real-world applications has drawn the attention of all developers. Various PE methods have been proposed to overcome real-world challenges, such as few-shot prompting, chain-of-thought, and retrieval-augmented generation (RAG). However, RAG for legal judgment prediction (LJP) is still underexplored. To address this, we propose "Athena", a novel framework cultivating RAG as a core preprocess component to enhance LLMs' performance on specialized tasks. Athena constructs a knowledge base for accusations, attached with a semantic retrieval mechanism through vectorization. Our experiments show that Athena's overall performance has improved significantly, achieving state-of-the-art results on the CAIL2018 dataset. Our ablation study on the in-context window size parameter further reproduces LLMs' "lost-in-the-middle" phenomenon with a relative positional variation. And with moderate hyper-parameter-tuning, we can achieve at most 95% of accuracy accordingly. We also study the impact of query rewriting and data distribution, providing possible directions for future research based on former analyses.</li>
<li><strong>摘要：</strong>最近，ChatGPT、LLaMA 和 Claude 等大型语言模型 (LLM) 在无数领域盛行，包括法律场景。随着 LLM 技术的快速进步，作为 LLM 与实际应用之间接口的提示工程 (PE) 的发展引起了所有开发人员的关注。已经提出了各种 PE 方法来克服现实世界的挑战，例如少样本提示、思路链和检索增强生成 (RAG)。然而，用于法律判断预测 (LJP) 的 RAG 仍未得到充分探索。为了解决这个问题，我们提出了“Athena”，这是一个新颖的框架，将 RAG 作为核心预处理组件来提高 LLM 在专门任务上的性能。Athena 构建了一个指控知识库，并通过矢量化附加了语义检索机制。我们的实验表明，Athena 的整体性能得到了显着提高，在 CAIL2018 数据集上取得了最先进的结果。我们对上下文窗口大小参数的消融研究进一步重现了 LLM 具有相对位置变化的“中间迷失”现象。通过适度的超参数调整，我们可以相应地实现最多 95% 的准确率。我们还研究了查询重写和数据分布的影响，为基于先前分析的未来研究提供了可能的方向。</li>
</ul>

<h3>Title: On the Capacity of Citation Generation by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haosheng Qian, Yixing Fan, Ruqing Zhang, Jiafeng Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11217">https://arxiv.org/abs/2410.11217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11217">https://arxiv.org/pdf/2410.11217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11217]] On the Capacity of Citation Generation by Large Language Models(https://arxiv.org/abs/2410.11217)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) appears as a promising method to alleviate the "hallucination" problem in large language models (LLMs), since it can incorporate external traceable resources for response generation. The essence of RAG in combating the hallucination issue lies in accurately attributing claims in responses to the corresponding retrieved documents. However, most of existing works focus on improving the quality of generated responses from the LLM, while largely overlooked its ability to attribute sources accurately. In this study, we conduct a systematic analysis about the capabilities of LLMs in generating citations within response generation, and further introduce a novel method to enhance their citation generation abilities. Specifically, we evaluate both the correctness and citation quality for seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce new citation evaluation metrics to eliminate the over-penalization of unnecessary and excessive citations in existing metrics. Furthermore, we propose a Generate-then-Refine method that completes relevant citations and removes irrelevant ones without altering the response text. The results on WebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves the quality of citations in responses generated by LLMs.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 似乎是一种有前途的方法，可以缓解大型语言模型 (LLM) 中的“幻觉”问题，因为它可以结合外部可追溯资源来生成响应。RAG 在解决幻觉问题方面的本质在于准确地将响应中的主张归因于相应的检索到的文档。然而，现有的大多数工作都侧重于提高 LLM 生成的响应的质量，而在很大程度上忽视了其准确归因来源的能力。在本研究中，我们对 LLM 在响应生成中生成引文的能力进行了系统分析，并进一步介绍了一种增强其引文生成能力的新方法。具体来说，我们在两个基准数据集上评估了七个广泛使用的 LLM 的正确性和引文质量。同时，我们引入了新的引文评估指标，以消除现有指标中对不必要和过度引文的过度惩罚。此外，我们提出了一种生成然后细化的方法，该方法在不改变响应文本的情况下完成相关引文并删除不相关的引文。 WebGLM-QA、ASQA 和 ELI5 数据集上的结果表明，我们的方法显著提高了 LLM 生成的响应中的引用质量。</li>
</ul>

<h3>Title: "Is Hate Lost in Translation?": Evaluation of Multilingual LGBTQIA+ Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Fai Leui Chan, Duke Nguyen, Aditya Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11230">https://arxiv.org/abs/2410.11230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11230">https://arxiv.org/pdf/2410.11230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11230]] "Is Hate Lost in Translation?": Evaluation of Multilingual LGBTQIA+ Hate Speech Detection(https://arxiv.org/abs/2410.11230)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This paper explores the challenges of detecting LGBTQIA+ hate speech of large language models across multiple languages, including English, Italian, Chinese and (code-switched) English-Tamil, examining the impact of machine translation and whether the nuances of hate speech are preserved across translation. We examine the hate speech detection ability of zero-shot and fine-tuned GPT. Our findings indicate that: (1) English has the highest performance and the code-switching scenario of English-Tamil being the lowest, (2) fine-tuning improves performance consistently across languages whilst translation yields mixed results. Through simple experimentation with original text and machine-translated text for hate speech detection along with a qualitative error analysis, this paper sheds light on the socio-cultural nuances and complexities of languages that may not be captured by automatic translation.</li>
<li><strong>摘要：</strong>本文探讨了大型语言模型在多种语言（包括英语、意大利语、中文和（代码转换的）英语-泰米尔语）中检测 LGBTQIA+ 仇恨言论所面临的挑战，研究了机器翻译的影响以及仇恨言论的细微差别是否在翻译过程中得以保留。我们研究了零样本和微调 GPT 的仇恨言论检测能力。我们的研究结果表明：（1）英语的性能最高，英语-泰米尔语的代码转换场景的性能最低；（2）微调可以持续提高不同语言的性能，而翻译的结果则好坏参半。通过对原始文本和机器翻译文本进行简单的仇恨言论检测实验以及定性错误分析，本文揭示了自动翻译可能无法捕捉到的语言的社会文化细微差别和复杂性。</li>
</ul>

<h3>Title: Unleashing the Power of LLMs as Multi-Modal Encoders for Text and Graph-Structured Data</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Lin, Kun Qian, Haoyu Han, Nurendra Choudhary, Tianxin Wei, Zhongruo Wang, Sahika Genc, Edward W Huang, Sheng Wang, Karthik Subbian, Danai Koutra, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11235">https://arxiv.org/abs/2410.11235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11235">https://arxiv.org/pdf/2410.11235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11235]] Unleashing the Power of LLMs as Multi-Modal Encoders for Text and Graph-Structured Data(https://arxiv.org/abs/2410.11235)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Graph-structured information offers rich contextual information that can enhance language models by providing structured relationships and hierarchies, leading to more expressive embeddings for various applications such as retrieval, question answering, and classification. However, existing methods for integrating graph and text embeddings, often based on Multi-layer Perceptrons (MLPs) or shallow transformers, are limited in their ability to fully exploit the heterogeneous nature of these modalities. To overcome this, we propose Janus, a simple yet effective framework that leverages Large Language Models (LLMs) to jointly encode text and graph data. Specifically, Janus employs an MLP adapter to project graph embeddings into the same space as text embeddings, allowing the LLM to process both modalities jointly. Unlike prior work, we also introduce contrastive learning to align the graph and text spaces more effectively, thereby improving the quality of learned joint embeddings. Empirical results across six datasets spanning three tasks, knowledge graph-contextualized question answering, graph-text pair classification, and retrieval, demonstrate that Janus consistently outperforms existing baselines, achieving significant improvements across multiple datasets, with gains of up to 11.4% in QA tasks. These results highlight Janus's effectiveness in integrating graph and text data. Ablation studies further validate the effectiveness of our method.</li>
<li><strong>摘要：</strong>图结构信息提供了丰富的上下文信息，可以通过提供结构化关系和层次结构来增强语言模型，从而为各种应用（如检索、问答和分类）提供更具表现力的嵌入。然而，现有的集成图和文本嵌入的方法通常基于多层感知器 (MLP) 或浅层转换器，在充分利用这些模态的异构性方面能力有限。为了解决这个问题，我们提出了 Janus，这是一个简单而有效的框架，它利用大型语言模型 (LLM) 来联合编码文本和图数据。具体来说，Janus 使用 MLP 适配器将图嵌入投影到与文本嵌入相同的空间中，从而允许 LLM 联合处理这两种模态。与之前的工作不同，我们还引入了对比学习来更有效地对齐图和文本空间，从而提高学习到的联合嵌入的质量。涵盖知识图谱语境化问答、图文对分类和检索三项任务的六个数据集的实证结果表明，Janus 的表现始终优于现有基准，在多个数据集上取得了显著的改进，问答任务的增益高达 11.4%。这些结果凸显了 Janus 在整合图文数据方面的有效性。消融研究进一步验证了我们方法的有效性。</li>
</ul>

<h3>Title: HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications</h3>
<ul>
<li><strong>Authors: </strong>Weijie Xu, Jay Desai, Fanyou Wu, Josef Valvoda, Srinivasan H. Sengamedu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11239">https://arxiv.org/abs/2410.11239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11239">https://arxiv.org/pdf/2410.11239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11239]] HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications(https://arxiv.org/abs/2410.11239)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent LLM (Large Language Models) advancements benefit many fields such as education and finance, but HR has hundreds of repetitive processes, such as access requests, medical claim filing and time-off submissions, which are unaddressed. We relate these tasks to the LLM agent, which has addressed tasks such as writing assisting and customer support. We present HR-Agent, an efficient, confidential, and HR-specific LLM-based task-oriented dialogue system tailored for automating repetitive HR processes such as medical claims and access requests. Since conversation data is not sent to an LLM during inference, it preserves confidentiality required in HR-related tasks.</li>
<li><strong>摘要：</strong>最近的 LLM（大型语言模型）进步使教育和金融等许多领域受益，但人力资源有数百个重复的流程，例如访问请求、医疗索赔申请和休假提交，这些流程尚未得到解决。我们将这些任务与 LLM 代理联系起来，该代理已解决写作协助和客户支持等任务。我们提出了 HR-Agent，这是一种高效、保密且特定于人力资源的基于 LLM 的任务导向型对话系统，专门用于自动化重复的人力资源流程，例如医疗索赔和访问请求。由于对话数据在推理期间不会发送到 LLM，因此它可以保留人力资源相关任务所需的机密性。</li>
</ul>

<h3>Title: In-Context Learning for Long-Context Sentiment Analysis on Infrastructure Project Opinions</h3>
<ul>
<li><strong>Authors: </strong>Alireza Shamshiri, Kyeong Rok Ryu, June Young Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11265">https://arxiv.org/abs/2410.11265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11265">https://arxiv.org/pdf/2410.11265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11265]] In-Context Learning for Long-Context Sentiment Analysis on Infrastructure Project Opinions(https://arxiv.org/abs/2410.11265)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive results across various tasks. However, they still struggle with long-context documents. This study evaluates the performance of three leading LLMs: GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro on lengthy, complex, and opinion-varying documents concerning infrastructure projects, under both zero-shot and few-shot scenarios. Our results indicate that GPT-4o excels in zero-shot scenarios for simpler, shorter documents, while Claude 3.5 Sonnet surpasses GPT-4o in handling more complex, sentiment-fluctuating opinions. In few-shot scenarios, Claude 3.5 Sonnet outperforms overall, while GPT-4o shows greater stability as the number of demonstrations increases.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中都取得了令人瞩目的成果。然而，它们在处理长上下文文档方面仍然举步维艰。这项研究评估了三种领先的 LLM：GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5 Pro 在零样本和少样本场景下对有关基础设施项目的冗长、复杂且意见各异的文档的表现。我们的结果表明，GPT-4o 在更简单、更短的文档的零样本场景中表现出色，而 Claude 3.5 Sonnet 在处理更复杂、情绪波动的意见方面则优于 GPT-4o。在少样本场景中，Claude 3.5 Sonnet 总体表现优异，而 GPT-4o 随着演示次数的增加表现出更高的稳定性。</li>
</ul>

<h3>Title: Cognitive Overload Attack:Prompt Injection for Long Context</h3>
<ul>
<li><strong>Authors: </strong>Bibek Upadhayay, Vahid Behzadan, Amin Karbasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11272">https://arxiv.org/abs/2410.11272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11272">https://arxiv.org/pdf/2410.11272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11272]] Cognitive Overload Attack:Prompt Injection for Long Context(https://arxiv.org/abs/2410.11272)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in performing tasks across various domains without needing explicit retraining. This capability, known as In-Context Learning (ICL), while impressive, exposes LLMs to a variety of adversarial prompts and jailbreaks that manipulate safety-trained LLMs into generating undesired or harmful output. In this paper, we propose a novel interpretation of ICL in LLMs through the lens of cognitive neuroscience, by drawing parallels between learning in human cognition with ICL. We applied the principles of Cognitive Load Theory in LLMs and empirically validate that similar to human cognition, LLMs also suffer from cognitive overload a state where the demand on cognitive processing exceeds the available capacity of the model, leading to potential errors. Furthermore, we demonstrated how an attacker can exploit ICL to jailbreak LLMs through deliberately designed prompts that induce cognitive overload on LLMs, thereby compromising the safety mechanisms of LLMs. We empirically validate this threat model by crafting various cognitive overload prompts and show that advanced models such as GPT-4, Claude-3.5 Sonnet, Claude-3 OPUS, Llama-3-70B-Instruct, Gemini-1.0-Pro, and Gemini-1.5-Pro can be successfully jailbroken, with attack success rates of up to 99.99%. Our findings highlight critical vulnerabilities in LLMs and underscore the urgency of developing robust safeguards. We propose integrating insights from cognitive load theory into the design and evaluation of LLMs to better anticipate and mitigate the risks of adversarial attacks. By expanding our experiments to encompass a broader range of models and by highlighting vulnerabilities in LLMs' ICL, we aim to ensure the development of safer and more reliable AI systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出在各个领域执行任务的卓越能力，无需进行明确的再训练。这种能力被称为情境学习 (ICL)，虽然令人印象深刻，但它将 LLM 暴露于各种对抗性提示和越狱，这些提示和越狱会操纵经过安全训练的 LLM 产生不受欢迎或有害的输出。在本文中，我们从认知神经科学的角度提出了对 LLM 中 ICL 的新解释，将人类认知中的学习与 ICL 进行比较。我们在 LLM 中应用了认知负荷理论的原理，并通过经验验证了与人类认知类似，LLM 也会遭受认知过载，即对认知处理的需求超出模型的可用容量，从而导致潜在错误。此外，我们还演示了攻击者如何利用 ICL 通过精心设计的提示来越狱 LLM，这些提示会导致 LLM 认知过载，从而损害 LLM 的安全机制。我们通过设计各种认知过载提示对这一威胁模型进行了实证验证，并表明 GPT-4、Claude-3.5 Sonnet、Claude-3 OPUS、Llama-3-70B-Instruct、Gemini-1.0-Pro 和 Gemini-1.5-Pro 等高级模型可以成功越狱，攻击成功率高达 99.99%。我们的研究结果凸显了 LLM 中的关键漏洞，并强调了开发强大保障措施的紧迫性。我们建议将认知负荷理论的见解整合到 LLM 的设计和评估中，以更好地预测和减轻对抗性攻击的风险。通过扩大我们的实验以涵盖更广泛的模型并强调 LLM 的 ICL 中的漏洞，我们旨在确保开发更安全、更可靠的人工智能系统。</li>
</ul>

<h3>Title: Process Reward Model with Q-Value Rankings</h3>
<ul>
<li><strong>Authors: </strong>Wendi Li, Yixuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11287">https://arxiv.org/abs/2410.11287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11287">https://arxiv.org/pdf/2410.11287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11287]] Process Reward Model with Q-Value Rankings(https://arxiv.org/abs/2410.11287)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Process Reward Modeling (PRM) is critical for complex reasoning and decision-making tasks where the accuracy of intermediate steps significantly influences the overall outcome. Existing PRM approaches, primarily framed as classification problems, employ cross-entropy loss to independently evaluate each step's correctness. This method can lead to suboptimal reward distribution and does not adequately address the interdependencies among steps. To address these limitations, we introduce the Process Q-value Model (PQM), a novel framework that redefines PRM in the context of a Markov Decision Process. PQM optimizes Q-value rankings based on a novel comparative loss function, enhancing the model's ability to capture the intricate dynamics among sequential decisions. This approach provides a more granular and theoretically grounded methodology for process rewards. Our extensive empirical evaluations across various sampling policies, language model backbones, and multi-step reasoning benchmarks show that PQM outperforms classification-based PRMs. The effectiveness of the comparative loss function is highlighted in our comprehensive ablation studies, confirming PQM's practical efficacy and theoretical advantage.</li>
<li><strong>摘要：</strong>过程奖励模型 (PRM) 对于复杂的推理和决策任务至关重要，因为中间步骤的准确性会显著影响整体结果。现有的 PRM 方法主要被定义为分类问题，采用交叉熵损失来独立评估每个步骤的正确性。这种方法可能会导致奖励分布不理想，并且不能充分解决步骤之间的相互依赖关系。为了解决这些限制，我们引入了过程 Q 值模型 (PQM)，这是一个在马尔可夫决策过程的背景下重新定义 PRM 的新框架。PQM 根据一种新的比较损失函数优化 Q 值排名，增强了模型捕捉连续决策之间复杂动态的能力。这种方法为过程奖励提供了一种更细致、更有理论依据的方法。我们对各种采样策略、语言模型主干和多步骤推理基准进行的广泛实证评估表明，PQM 优于基于分类的 PRM。我们全面的消融研究强调了比较损失函数的有效性，证实了 PQM 的实际功效和理论优势。</li>
</ul>

<h3>Title: Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset Repository</h3>
<ul>
<li><strong>Authors: </strong>S. Tamang, D. J. Bora</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11291">https://arxiv.org/abs/2410.11291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11291">https://arxiv.org/pdf/2410.11291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11291]] Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset Repository(https://arxiv.org/abs/2410.11291)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, chat</a></li>
<li><strong>Abstract: </strong>This paper introduces a centralized, open-source dataset repository designed to advance NLP and NMT for Assamese, a low-resource language. The repository supports various tasks like sentiment analysis, named entity recognition, and machine translation by providing both pre-training and fine-tuning corpora. We review existing datasets, highlighting the need for standardized resources in Assamese NLP, and discuss potential applications in AI-driven research, such as LLMs, OCR, and chatbots. While promising, challenges like data scarcity and linguistic diversity remain. The repository aims to foster collaboration and innovation, promoting Assamese language research in the digital age.</li>
<li><strong>摘要：</strong>本文介绍了一个集中式开源数据集存储库，旨在推进资源匮乏的语言阿萨姆语的 NLP 和 NMT。该存储库通过提供预训练和微调语料库，支持情绪分析、命名实体识别和机器翻译等各种任务。我们回顾了现有数据集，强调了阿萨姆语 NLP 对标准化资源的需求，并讨论了 AI 驱动研究（如 LLM、OCR 和聊天机器人）中的潜在应用。虽然前景光明，但数据稀缺和语言多样性等挑战仍然存在。该存储库旨在促进协作和创新，促进数字时代的阿萨姆语研究。</li>
</ul>

<h3>Title: SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11315">https://arxiv.org/abs/2410.11315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11315">https://arxiv.org/pdf/2410.11315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11315]] SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation(https://arxiv.org/abs/2410.11315)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Recent studies in Retrieval-Augmented Generation (RAG) have investigated extracting evidence from retrieved passages to reduce computational costs and enhance the final RAG performance, yet it remains challenging. Existing methods heavily rely on heuristic-based augmentation, encountering several issues: (1) Poor generalization due to hand-crafted context filtering; (2) Semantics deficiency due to rule-based context chunking; (3) Skewed length due to sentence-wise filter learning. To address these issues, we propose a model-based evidence extraction learning framework, SEER, optimizing a vanilla model as an evidence extractor with desired properties through self-aligned learning. Extensive experiments show that our method largely improves the final RAG performance, enhances the faithfulness, helpfulness, and conciseness of the extracted evidence, and reduces the evidence length by 9.25 times. The code will be available at this https URL.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 的最新研究已经研究了从检索到的段落中提取证据以降低计算成本并提高最终的 RAG 性能，但这仍然具有挑战性。现有方法严重依赖基于启发式的增强，遇到了几个问题：（1）由于手工制作的上下文过滤导致泛化能力差；（2）由于基于规则的上下文分块导致语义缺陷；（3）由于逐句过滤学习导致长度偏差。为了解决这些问题，我们提出了一个基于模型的证据提取学习框架 SEER，通过自对齐学习优化原始模型作为具有所需属性的证据提取器。大量实验表明，我们的方法大大提高了最终的 RAG 性能，提高了提取证据的真实性、有用性和简洁性，并将证据长度缩短了 9.25 倍。代码将在此 https URL 上提供。</li>
</ul>

<h3>Title: Self-adaptive Multimodal Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenjia Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11321">https://arxiv.org/abs/2410.11321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11321">https://arxiv.org/pdf/2410.11321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11321]] Self-adaptive Multimodal Retrieval-Augmented Generation(https://arxiv.org/abs/2410.11321)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Traditional Retrieval-Augmented Generation (RAG) methods are limited by their reliance on a fixed number of retrieved documents, often resulting in incomplete or noisy information that undermines task performance. Although recent adaptive approaches alleviated these problems, their application in intricate and real-world multimodal tasks remains limited. To address these, we propose a new approach called Self-adaptive Multimodal Retrieval-Augmented Generation (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG not only dynamically filters relevant documents based on the input query, including image captions when needed, but also verifies the quality of both the retrieved documents and the output. Extensive experimental results show that SAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy and response generation. By further ablation experiments and effectiveness analysis, SAM-RAG maintains high recall quality while improving overall task performance in multimodal RAG task. Our codes are available at this https URL.</li>
<li><strong>摘要：</strong>传统的检索增强生成 (RAG) 方法受限于对固定数量的检索文档的依赖，这通常会导致信息不完整或嘈杂，从而影响任务性能。尽管最近的自适应方法缓解了这些问题，但它们在复杂的现实世界多模态任务中的应用仍然有限。为了解决这些问题，我们提出了一种称为自适应多模态检索增强生成 (SAM-RAG) 的新方法，专门针对多模态环境量身定制。SAM-RAG 不仅根据输入查询动态过滤相关文档（包括需要时的图像标题），而且还验证检索到的文档和输出的质量。大量实验结果表明，SAM-RAG 在检索准确性和响应生成方面都超越了现有的最先进方法。通过进一步的消融实验和有效性分析，SAM-RAG 在多模态 RAG 任务中保持了较高的召回质量，同时提高了整体任务性能。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</h3>
<ul>
<li><strong>Authors: </strong>Syed Abdul Gaffar Shakhadri, Kruthika KR, Rakshit Aralimatti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11331">https://arxiv.org/abs/2410.11331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11331">https://arxiv.org/pdf/2410.11331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11331]] SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments(https://arxiv.org/abs/2410.11331)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI.</li>
<li><strong>摘要：</strong>我们推出了 Shakti，这是一个 25 亿参数语言模型，专门针对资源受限的环境（例如智能手机、可穿戴设备和物联网系统等边缘设备）进行了优化。Shakti 将高性能 NLP 与优化的效率和精度相结合，使其成为计算资源和内存有限的实时 AI 应用的理想选择。凭借对本土语言和领域特定任务的支持，Shakti 在医疗保健、金融和客户服务等行业中表现出色。基准评估表明，Shakti 在保持低延迟和设备效率的同时，与更大的模型相比具有竞争力，使其成为边缘 AI 的领先解决方案。</li>
</ul>

<h3>Title: RATE: Score Reward Models with Imperfect Rewrites of Rewrites</h3>
<ul>
<li><strong>Authors: </strong>David Reber, Sean Richardson, Todd Nief, Cristina Garbacea, Victor Veitch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11348">https://arxiv.org/abs/2410.11348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11348">https://arxiv.org/pdf/2410.11348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11348]] RATE: Score Reward Models with Imperfect Rewrites of Rewrites(https://arxiv.org/abs/2410.11348)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This paper concerns the evaluation of reward models used in language modeling. A reward model is a function that takes a prompt and a response and assigns a score indicating how good that response is for the prompt. A key challenge is that reward models are usually imperfect proxies for actual preferences. For example, we may worry that a model trained to reward helpfulness learns to instead prefer longer responses. In this paper, we develop an evaluation method, RATE (Rewrite-based Attribute Treatment Estimators), that allows us to measure the causal effect of a given attribute of a response (e.g., length) on the reward assigned to that response. The core idea is to use large language models to rewrite responses to produce imperfect counterfactuals, and to adjust for rewriting error by rewriting twice. We show that the RATE estimator is consistent under reasonable assumptions. We demonstrate the effectiveness of RATE on synthetic and real-world data, showing that it can accurately estimate the effect of a given attribute on the reward model.</li>
<li><strong>摘要：</strong>本文涉及语言建模中使用的奖励模型的评估。奖励模型是一种函数，它接受提示和响应并分配一个分数，以表明该响应对于提示有多好。一个关键的挑战是，奖励模型通常不是实际偏好的完美代理。例如，我们可能会担心，一个被训练为奖励乐于助人的模型会学会更喜欢较长的响应。在本文中，我们开发了一种评估方法，RATE（基于重写的属性处理估计器），它使我们能够测量响应的给定属性（例如长度）对分配给该响应的奖励的因果影响。核心思想是使用大型语言模型重写响应以产生不完美的反事实，并通过重写两次来调整重写错误。我们表明，在合理的假设下，RATE 估计器是一致的。我们证明了 RATE 对合成数据和现实世界数据的有效性，表明它可以准确估计给定属性对奖励模型的影响。</li>
</ul>

<h3>Title: LargePiG: Your Large Language Model is Secretly a Pointer Generator</h3>
<ul>
<li><strong>Authors: </strong>Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11366">https://arxiv.org/abs/2410.11366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11366">https://arxiv.org/pdf/2410.11366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11366]] LargePiG: Your Large Language Model is Secretly a Pointer Generator(https://arxiv.org/abs/2410.11366)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recent research on query generation has focused on using Large Language Models (LLMs), which despite bringing state-of-the-art performance, also introduce issues with hallucinations in the generated queries. In this work, we introduce relevance hallucination and factuality hallucination as a new typology for hallucination problems brought by query generation based on LLMs. We propose an effective way to separate content from form in LLM-generated queries, which preserves the factual knowledge extracted and integrated from the inputs and compiles the syntactic structure, including function words, using the powerful linguistic capabilities of the LLM. Specifically, we introduce a model-agnostic and training-free method that turns the Large Language Model into a Pointer-Generator (LargePiG), where the pointer attention distribution leverages the LLM's inherent attention weights, and the copy probability is derived from the difference between the vocabulary distribution of the model's high layers and the last layer. To validate the effectiveness of LargePiG, we constructed two datasets for assessing the hallucination problems in query generation, covering both document and video scenarios. Empirical studies on various LLMs demonstrated the superiority of LargePiG on both datasets. Additional experiments also verified that LargePiG could reduce hallucination in large vision language models and improve the accuracy of document-based question-answering and factuality evaluation tasks.</li>
<li><strong>摘要：</strong>查询生成的最新研究集中于使用大型语言模型 (LLM)，尽管它带来了最先进的性能，但也在生成的查询中引入了幻觉问题。在这项工作中，我们引入了相关性幻觉和事实性幻觉作为基于 LLM 的查询生成所带来的幻觉问题的新类型。我们提出了一种有效的方法来将 LLM 生成的查询中的内容与形式分离，该方法保留了从输入中提取和集成的事实知识，并使用 LLM 强大的语言功能编译包括功能词在内的句法结构。具体而言，我们引入了一种与模型无关且无需训练的方法，将大型语言模型转变为指针生成器 (LargePiG)，其中指针注意力分布利用了 LLM 固有的注意力权重，而复制概率则来自模型高层和最后一层的词汇分布之间的差异。为了验证 LargePiG 的有效性，我们构建了两个数据集来评估查询生成中的幻觉问题，涵盖文档和视频场景。对各种 LLM 的实证研究表明 LargePiG 在这两个数据集上都表现出色。额外的实验也验证了 LargePiG 可以减少大型视觉语言模型中的幻觉，并提高基于文档的问答和事实性评估任务的准确性。</li>
</ul>

<h3>Title: Enhance Graph Alignment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, Yujun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11370">https://arxiv.org/abs/2410.11370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11370">https://arxiv.org/pdf/2410.11370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11370]] Enhance Graph Alignment for Large Language Models(https://arxiv.org/abs/2410.11370)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Graph-structured data is prevalent in the real world. Recently, due to the powerful emergent capabilities, Large Language Models (LLMs) have shown promising performance in modeling graphs. The key to effectively applying LLMs on graphs is converting graph data into a format LLMs can comprehend. Graph-to-token approaches are popular in enabling LLMs to process graph information. They transform graphs into sequences of tokens and align them with text tokens through instruction tuning, where self-supervised instruction tuning helps LLMs acquire general knowledge about graphs, and supervised fine-tuning specializes LLMs for the downstream tasks on graphs. Despite their initial success, we find that existing methods have a misalignment between self-supervised tasks and supervised downstream tasks, resulting in negative transfer from self-supervised fine-tuning to downstream tasks. To address these issues, we propose Graph Alignment Large Language Models (GALLM) to benefit from aligned task templates. In the self-supervised tuning stage, we introduce a novel text matching task using templates aligned with downstream tasks. In the task-specific tuning stage, we propose two category prompt methods that learn supervision information from additional explanation with further aligned templates. Experimental evaluations on four datasets demonstrate substantial improvements in supervised learning, multi-dataset generalizability, and particularly in zero-shot capability, highlighting the model's potential as a graph foundation model.</li>
<li><strong>摘要：</strong>图结构数据在现实世界中非常普遍。最近，由于强大的新兴能力，大型语言模型 (LLM) 在图形建模方面表现出色。有效地将 LLM 应用于图形的关键是将图形数据转换为 LLM 可以理解的格式。图到标记方法在使 LLM 能够处理图形信息方面非常流行。它们将图形转换为标记序列，并通过指令调整将它们与文本标记对齐，其中自监督指令调整有助于 LLM 获取有关图形的一般知识，而监督微调使 LLM 专门用于图形上的下游任务。尽管它们最初取得了成功，但我们发现现有方法在自监督任务和监督下游任务之间存在不一致，导致从自监督微调到下游任务的负迁移。为了解决这些问题，我们提出了图对齐大型语言模型 (GALLM)，以从对齐的任务模板中受益。在自监督调整阶段，我们引入了一种新颖的文本匹配任务，使用与下游任务对齐的模板。在特定于任务的调整阶段，我们提出了两种类别提示方法，它们通过进一步对齐的模板从附加解释中学习监督信息。在四个数据集上的实验评估表明，监督学习、多数据集通用性以及零样本能力都有了显着的改进，凸显了该模型作为图基础模型的潜力。</li>
</ul>

<h3>Title: Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Qihuang Zhong, Kunfeng Chen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11371">https://arxiv.org/abs/2410.11371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11371">https://arxiv.org/pdf/2410.11371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11371]] Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL(https://arxiv.org/abs/2410.11371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown promising performance in text-to-SQL, which involves translating natural language questions into SQL queries. However, current text-to-SQL LLMs are computationally expensive and challenging to deploy in real-world applications, highlighting the importance of compressing them. To achieve this goal, knowledge distillation (KD) is a common approach, which aims to distill the larger teacher model into a smaller student model. While numerous KD methods for autoregressive LLMs have emerged recently, it is still under-explored whether they work well in complex text-to-SQL scenarios. To this end, we conduct a series of analyses and reveal that these KD methods generally fall short in balancing performance and efficiency. In response to this problem, we propose to improve the KD with Imperfect Data, namely KID, which effectively boosts the performance without introducing much training budget. The core of KID is to efficiently mitigate the training-inference mismatch by simulating the cascading effect of inference in the imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks show that, KID can not only achieve consistent and significant performance gains (up to +5.83% average score) across all model types and sizes, but also effectively improve the training efficiency.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在文本到 SQL 转换（将自然语言问题转换为 SQL 查询）方面表现出色。然而，当前的文本到 SQL LLM 在计算上非常昂贵，难以在实际应用中部署，因此压缩它们的重要性凸显出来。为了实现这一目标，知识蒸馏 (KD) 是一种常用方法，旨在将较大的教师模型蒸馏为较小的学生模型。虽然最近出现了许多用于自回归 LLM 的 KD 方法，但它们在复杂的文本到 SQL 场景中是否能很好地工作仍未得到充分探索。为此，我们进行了一系列分析，并发现这些 KD 方法在平衡性能和效率方面普遍存在不足。针对这一问题，我们提出改进具有不完美数据的 KD，即 KID，它可以在不引入太多训练预算的情况下有效提升性能。KID 的核心是通过模拟不完美训练数据中推理的级联效应来有效缓解训练-推理不匹配的问题。在 5 个文本到 SQL 基准测试上的大量实验表明，KID 不仅可以在所有模型类型和大小上实现一致且显著的性能提升（平均得分高达 +5.83%），而且还有效提高了训练效率。</li>
</ul>

<h3>Title: Do LLMs Have the Generalization Ability in Conducting Causal Inference?</h3>
<ul>
<li><strong>Authors: </strong>Chen Wang, Dongming Zhao, Bo Wang, Ruifang He, Yuexian Hou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11385">https://arxiv.org/abs/2410.11385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11385">https://arxiv.org/pdf/2410.11385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11385]] Do LLMs Have the Generalization Ability in Conducting Causal Inference?(https://arxiv.org/abs/2410.11385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In causal inference, generalization capability refers to the ability to conduct causal inference methods on new data to estimate the causal-effect between unknown phenomenon, which is crucial for expanding the boundaries of knowledge. Studies have evaluated the causal inference capabilities of Large Language Models (LLMs) concerning known phenomena, yet the generalization capabilities of LLMs concerning unseen phenomena remain unexplored. In this paper, we selected four tasks: Causal Path Discovery (CP), Backdoor Adjustment (BA), Factual Inference (FI), and Counterfactual Inference (CI) as representatives of causal inference tasks. To generate evaluation questions about previously unseen phenomena in new data on the four tasks, we propose a benchmark generation framework, which employs randomly generated graphs and node names to formulate questions within hypothetical new causal scenarios. Based on this framework, we compile a benchmark dataset of varying levels of question complexity. We extensively tested the generalization capabilities of five leading LLMs across four tasks. Experiment results reveal that while LLMs exhibit good generalization performance in solving simple CP, FI, and complex CI questions, they encounter difficulties when tackling BA questions and face obvious performance fluctuations as the problem complexity changes. Furthermore, when the names of phenomena incorporate existing terms, even if these names are entirely novel, their generalization performance can still be hindered by interference from familiar terms.</li>
<li><strong>摘要：</strong>在因果推理中，泛化能力是指使用因果推理方法对新数据估计未知现象间因果关系的能力，这对于拓展知识的边界至关重要。已有研究评估了大型语言模型（LLM）对已知现象的因果推理能力，而其对未知现象的泛化能力尚待探索。本文选取因果路径发现（CP）、后门调整（BA）、事实推理（FI）和反事实推理（CI）四个任务作为因果推理任务的代表。为了在这四个任务中生成关于新数据中未知现象的评估问题，我们提出了一个基准生成框架，该框架使用随机生成的图和节点名在假设的新因果场景中提出问题。基于该框架，我们编制了一个具有不同问题复杂度级别的基准数据集。我们在四个任务中对五款领先的LLM的泛化能力进行了广泛测试。实验结果表明，LLM 在解决简单的 CP、FI 和复杂的 CI 问题时表现出良好的泛化性能，但在解决 BA 问题时会遇到困难，并且随着问题复杂度的变化，性能波动明显。此外，当现象名称中引入现有术语时，即使这些名称是全新的，其泛化性能仍会受到熟悉术语干扰的影响。</li>
</ul>

<h3>Title: PMMT: Preference Alignment in Multilingual Machine Translation via LLM Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shuqiao Sun, Yutong Yao, Peiwen Wu, Feijun Jiang, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11410">https://arxiv.org/abs/2410.11410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11410">https://arxiv.org/pdf/2410.11410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11410]] PMMT: Preference Alignment in Multilingual Machine Translation via LLM Distillation(https://arxiv.org/abs/2410.11410)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Translation is important for cross-language communication, and many efforts have been made to improve its accuracy. However, less investment is conducted in aligning translations with human preferences, such as translation tones or styles. In this paper, a new method is proposed to effectively generate large-scale multilingual parallel corpora with specific translation preferences using Large Language Models (LLMs). Meanwhile, an automatic pipeline is designed to distill human preferences into smaller Machine Translation (MT) models for efficiently and economically supporting large-scale calls in online services. Experiments indicate that the proposed method takes the lead in translation tasks with aligned human preferences by a large margin. Meanwhile, on popular public benchmarks like WMT and Flores, on which our models were not trained, the proposed method also shows a competitive performance compared to SOTA works.</li>
<li><strong>摘要：</strong>翻译是跨语言交流的重要内容，人们为提高翻译准确性做出了很多努力，然而在使翻译与人类偏好（如翻译语气或风格）保持一致方面投入的精力较少。本文提出了一种新方法，利用大型语言模型 (LLM) 有效地生成具有特定翻译偏好的大规模多语言平行语料库。同时，设计了一种自动化流程将人类偏好提炼到更小的机器翻译 (MT) 模型中，以高效、经济的方式支持在线服务中的大规模调用。实验表明，所提出的方法在具有一致人类偏好的翻译任务中遥遥领先。同时，在我们的模型未在其上训练的流行公共基准测试（如 WMT 和 Flores）上，所提出的方法也表现出与 SOTA 作品相比具有竞争力的性能。</li>
</ul>

<h3>Title: ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11414">https://arxiv.org/abs/2410.11414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11414">https://arxiv.org/pdf/2410.11414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11414]] ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability(https://arxiv.org/abs/2410.11414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) utilize external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual stream, while Copying Heads fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose ReDeEP, a novel method that detects hallucinations by decoupling LLM's utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 模型旨在整合外部知识，减少因参数（内部）知识不足而导致的幻觉。然而，即使检索到准确且相关的内容，RAG 模型仍然会通过生成与检索到的信息相冲突的输出来产生幻觉。检测此类幻觉需要理清大型语言模型 (LLM) 如何利用外部和参数知识。当前的检测方法通常只关注其中一种机制，或者不解开它们相互交织的影响，这使得准确检测变得困难。在本文中，我们研究了 RAG 场景中幻觉背后的内部机制。我们发现，当 LLM 中的知识 FFN 过分强调残差流中的参数知识，而复制头无法有效地保留或整合来自检索到的内容的外部知识时，就会出现幻觉。基于这些发现，我们提出了 ReDeEP，这是一种通过解开 LLM 对外部上下文和参数知识的利用来检测幻觉的新方法。我们的实验表明，ReDeEP 显著提高了 RAG 幻觉检测准确率。此外，我们引入了 AARF，它通过调节知识 FFN 和复制头的贡献来减轻幻觉。</li>
</ul>

<h3>Title: Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sihang Zhao, Youliang Yuan, Xiaoying Tang, Pinjia He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11437">https://arxiv.org/abs/2410.11437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11437">https://arxiv.org/pdf/2410.11437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11437]] Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs(https://arxiv.org/abs/2410.11437)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) demonstrate a strong understanding of the real world and can even handle complex tasks. However, they still fail on some straightforward visual question-answering (VQA) problems. This paper dives deeper into this issue, revealing that models tend to err when answering easy questions (e.g. Yes/No questions) about an image, even though they can correctly describe it. We refer to this model behavior discrepancy between difficult and simple questions as model laziness. To systematically investigate model laziness, we manually construct LazyBench, a benchmark that includes Yes/No, multiple choice, short answer questions, and image description tasks that are related to the same subjects in the images. Based on LazyBench, we observe that laziness widely exists in current advanced MLLMs (e.g. GPT-4o, Gemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced on stronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and find that about half of its failure cases are caused by model laziness, which further highlights the importance of ensuring that the model fully utilizes its capability. To this end, we conduct preliminary exploration on how to mitigate laziness and find that chain of thought (CoT) can effectively address this issue.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 展现出对现实世界的强大理解，甚至可以处理复杂的任务。然而，它们在一些简单的视觉问答 (VQA) 问题上仍然失败。本文深入探讨了这个问题，揭示了模型在回答有关图像的简单问题（例如，是/否问题）时往往会出错，即使它们可以正确描述图像。我们将这种困难问题和简单问题之间的模型行为差异称为模型惰性。为了系统地研究模型惰性，我们手动构建了 LazyBench，这是一个基准，其中包括与图像中相同主题相关的是/否、多项选择题、简答题和图像描述任务。基于 LazyBench，我们观察到惰性在当前先进的 MLLM（例如 GPT-4o、Gemini-1.5-pro、Claude 3 和 LLaVA-v1.5-13B）中广泛存在，并且在更强大的模型上更为明显。我们还分析了 VQA v2 (LLaVA-v1.5-13B) 基准，发现其失败案例中约有一半是由于模型惰性导致的，这进一步凸显了确保模型充分发挥其能力的重要性。为此，我们对如何缓解惰性进行了初步探索，发现思路链 (CoT) 可以有效解决这个问题。</li>
</ul>

<h3>Title: AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a simple RAG task</h3>
<ul>
<li><strong>Authors: </strong>Herbert Ullrich, Tomáš Mlynář, Jan Drchal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11446">https://arxiv.org/abs/2410.11446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11446">https://arxiv.org/pdf/2410.11446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11446]] AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a simple RAG task(https://arxiv.org/abs/2410.11446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This paper describes our $3^{rd}$ place submission in the AVeriTeC shared task in which we attempted to address the challenge of fact-checking with evidence retrieved in the wild using a simple scheme of Retrieval-Augmented Generation (RAG) designed for the task, leveraging the predictive power of Large Language Models. We release our codebase and explain its two modules - the Retriever and the Evidence & Label generator - in detail, justifying their features such as MMR-reranking and Likert-scale confidence estimation. We evaluate our solution on AVeriTeC dev and test set and interpret the results, picking the GPT-4o as the most appropriate model for our pipeline at the time of our publication, with Llama 3.1 70B being a promising open-source alternative. We perform an empirical error analysis to see that faults in our predictions often coincide with noise in the data or ambiguous fact-checks, provoking further research and data augmentation.</li>
<li><strong>摘要：</strong>本文介绍了我们在 AVeriTeC 共享任务中获得的第三名的论文，在该论文中，我们尝试使用为该任务设计的简单检索增强生成 (RAG) 方案，利用大型语言模型的预测能力，解决使用在野外检索到的证据进行事实核查的挑战。我们发布了代码库，并详细解释了它的两个模块——检索器和证据与标签生成器，证明了它们的功能，例如 MMR 重新排序和 Likert 量表置信度估计。我们在 AVeriTeC 开发和测试集上评估了我们的解决方案并解释了结果，选择 GPT-4o 作为我们发布时最适合我们管道的模型，而 Llama 3.1 70B 是一个很有前途的开源替代方案。我们进行了实证误差分析，发现我们的预测中的错误通常与数据中的噪声或模糊的事实核查相吻合，从而引发了进一步的研究和数据增强。</li>
</ul>

<h3>Title: A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies</h3>
<ul>
<li><strong>Authors: </strong>Yen-Hsiang Wang, Feng-Dian Su, Tzu-Yu Yeh, Yao-Chung Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11450">https://arxiv.org/abs/2410.11450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11450">https://arxiv.org/pdf/2410.11450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11450]] A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies(https://arxiv.org/abs/2410.11450)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>This paper introduces a cross-lingual statutory article retrieval (SAR) dataset designed to enhance legal information retrieval in multilingual settings. Our dataset features spoken-language-style legal inquiries in English, paired with corresponding Chinese versions and relevant statutes, covering all Taiwanese civil, criminal, and administrative laws. This dataset aims to improve access to legal information for non-native speakers, particularly for foreign nationals in Taiwan. We propose several LLM-based methods as baselines for evaluating retrieval effectiveness, focusing on mitigating translation errors and improving cross-lingual retrieval performance. Our work provides a valuable resource for developing inclusive legal information retrieval systems.</li>
<li><strong>摘要：</strong>本文介绍了一种跨语言法定文章检索 (SAR) 数据集，旨在增强多语言环境下的法律信息检索。我们的数据集以英语口语式法律查询为特色，并配有相应的中文版本和相关法规，涵盖所有台湾民事、刑事和行政法律。该数据集旨在改善非母语人士（尤其是台湾的外国人）获取法律信息的途径。我们提出了几种基于 LLM 的方法作为评估检索效果的基准，重点是减少翻译错误并提高跨语言检索性能。我们的工作为开发包容性法律信息检索系统提供了宝贵的资源。</li>
</ul>

<h3>Title: Tending Towards Stability: Convergence Challenges in Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Richard Diehl Martinez, Pietro Lesci, Paula Buttery</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11451">https://arxiv.org/abs/2410.11451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11451">https://arxiv.org/pdf/2410.11451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11451]] Tending Towards Stability: Convergence Challenges in Small Language Models(https://arxiv.org/abs/2410.11451)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.</li>
<li><strong>摘要：</strong>增加语言模型中的参数数量是提高其性能的常用策略。然而，较小的语言模型由于其较低的运营成本而仍然很有价值。尽管它们具有优势，但即使提供同等的数据和计算资源，较小的模型也经常表现不佳。具体来说，它们的性能往往会在后期预训练阶段下降。据传闻，这归因于它们的表征能力下降。然而，这种性能下降的确切原因仍不清楚。我们使用 Pythia 模型套件来分析这种现象背后的训练动态。在不同的模型大小中，我们研究了注意力和 MLP 激活到其最终状态的收敛，并研究了它们参数的有效等级如何影响这一过程。我们发现，大型模型中的几乎所有层都在训练早期稳定下来 - 在前 20% 内 - 而小型模型中的层表现出较慢且不太稳定的收敛，尤其是当它们的参数具有较低有效等级时。通过将层激活的收敛与其参数的有效等级联系起来，我们的分析可以指导未来的工作，以解决小型模型学习动态中的低效率问题。</li>
</ul>

<h3>Title: Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11459">https://arxiv.org/abs/2410.11459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11459">https://arxiv.org/pdf/2410.11459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11459]] Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models(https://arxiv.org/abs/2410.11459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited outstanding performance in engaging with humans and addressing complex questions by leveraging their vast implicit knowledge and robust reasoning capabilities. However, such models are vulnerable to jailbreak attacks, leading to the generation of harmful responses. Despite recent research on single-turn jailbreak strategies to facilitate the development of defence mechanisms, the challenge of revealing vulnerabilities under multi-turn setting remains relatively under-explored. In this work, we propose Jigsaw Puzzles (JSP), a straightforward yet effective multi-turn jailbreak strategy against the advanced LLMs. JSP splits questions into harmless fractions as the input of each turn, and requests LLMs to reconstruct and respond to questions under multi-turn interaction. Our experimental results demonstrate that the proposed JSP jailbreak bypasses original safeguards against explicitly harmful content, achieving an average attack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs (Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini). Moreover, JSP achieves a state-of-the-art attack success rate of 92% on GPT-4 on the harmful query benchmark, and exhibits strong resistant to defence strategies. Warning: this paper contains offensive examples.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 利用其丰富的隐性知识和强大的推理能力，在与人类互动和解决复杂问题方面表现出色。然而，这类模型容易受到越狱攻击，从而产生有害的反应。尽管最近有关于单轮越狱策略的研究，以促进防御机制的发展，但在多轮设置下揭示漏洞的挑战仍然相对较少。在这项工作中，我们提出了拼图 (JSP)，这是一种针对高级 LLM 的简单而有效的多轮越狱策略。JSP 将问题拆分为无害的部分作为每轮的输入，并要求 LLM 在多轮交互下重建和回答问题。我们的实验结果表明，所提出的 JSP 越狱绕过了针对明确有害内容的原始保护措施，在 5 个高级 LLM（Gemini-1.5-Pro、Llama-3.1-70B、GPT-4、GPT-4o、GPT-4o-mini）中对 189 个有害查询的平均攻击成功率为 93.76%。此外，JSP 在有害查询基准上对 GPT-4 的攻击成功率达到了 92%，并且表现出对防御策略的强大抵抗力。警告：本文包含攻击性示例。</li>
</ul>

<h3>Title: Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Richard Diehl Martinez, Zebulon Goriely, Andrew Caines, Paula Buttery, Lisa Beinborn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11462">https://arxiv.org/abs/2410.11462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11462">https://arxiv.org/pdf/2410.11462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11462]] Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing(https://arxiv.org/abs/2410.11462)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models strongly rely on frequency information because they maximize the likelihood of tokens during pre-training. As a consequence, language models tend to not generalize well to tokens that are seldom seen during training. Moreover, maximum likelihood training has been discovered to give rise to anisotropy: representations of tokens in a model tend to cluster tightly in a high-dimensional cone, rather than spreading out over their representational capacity. Our work introduces a method for quantifying the frequency bias of a language model by assessing sentence-level perplexity with respect to token-level frequency. We then present a method for reducing the frequency bias of a language model by inducing a syntactic prior over token representations during pre-training. Our Syntactic Smoothing method adjusts the maximum likelihood objective function to distribute the learning signal to syntactically similar tokens. This approach results in better performance on infrequent English tokens and a decrease in anisotropy. We empirically show that the degree of anisotropy in a model correlates with its frequency bias.</li>
<li><strong>摘要：</strong>语言模型强烈依赖频率信息，因为它们在预训练期间最大化了标记的似然性。因此，语言模型往往不能很好地推广到训练期间很少见到的标记。此外，已发现最大似然训练会导致各向异性：模型中的标记表示往往紧密聚集在高维锥体中，而不是分散在其表示容量上。我们的工作引入了一种通过评估句子级困惑度相对于标记级频率来量化语言模型频率偏差的方法。然后，我们提出了一种通过在预训练期间引入标记表示的句法先验来减少语言模型频率偏差的方法。我们的句法平滑方法调整最大似然目标函数以将学习信号分配给句法相似的标记。这种方法可以在不常见的英语标记上获得更好的性能，并降低各向异性。我们通过经验表明，模型中的各向异性程度与其频率偏差相关。</li>
</ul>

<h3>Title: O-Edit: Orthogonal Subspace Editing for Language Model Sequential Editing</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Cai, Ding Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11469">https://arxiv.org/abs/2410.11469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11469">https://arxiv.org/pdf/2410.11469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11469]] O-Edit: Orthogonal Subspace Editing for Language Model Sequential Editing(https://arxiv.org/abs/2410.11469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) acquire knowledge during pre-training, but over time, this knowledge may become incorrect or outdated, necessitating updates after training. Knowledge editing techniques address this issue without the need for costly re-training. However, most existing methods are designed for single edits, and as the number of edits increases, they often cause a decline in the model's overall performance, posing significant challenges for sequential editing. To overcome this, we propose Orthogonal Subspace Editing, O-Edit. This algorithm orthogonalizes the direction of each knowledge update, minimizing interference between successive updates and reducing the impact of new updates on unrelated knowledge. Our approach does not require replaying previously edited data and processes each edit knowledge on time. It can perform thousands of edits on mainstream LLMs, achieving an average performance improvement that is 4.2 times better than existing methods while effectively preserving the model's performance on downstream tasks, all with minimal additional parameter overhead.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在预训练期间获取知识，但随着时间的推移，这些知识可能会变得不正确或过时，因此需要在训练后进行更新。知识编辑技术解决了这个问题，而无需昂贵的重新训练。然而，大多数现有方法都是为单次编辑而设计的，随着编辑次数的增加，它们往往会导致模型整体性能下降，这对顺序编辑提出了重大挑战。为了解决这个问题，我们提出了正交子空间编辑 (O-Edit)。该算法将每次知识更新的方向正交化，最大限度地减少连续更新之间的干扰，并减少新更新对不相关知识的影响。我们的方法不需要重放以前编辑的数据，并按时处理每个编辑知识。它可以在主流 LLM 上执行数千次编辑，实现比现有方法好 4.2 倍的平均性能提升，同时有效地保留模型在下游任务上的性能，所有这些都将额外的参数开销降到最低。</li>
</ul>

<h3>Title: DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG</h3>
<ul>
<li><strong>Authors: </strong>Jinyoung Kim, Dayoon Ko, Gunhee Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11494">https://arxiv.org/abs/2410.11494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11494">https://arxiv.org/pdf/2410.11494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11494]] DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG(https://arxiv.org/abs/2410.11494)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of language, resolving new linguistic expressions in continuously updating knowledge bases remains a formidable challenge. This challenge becomes critical in retrieval-augmented generation (RAG) with knowledge bases, as emerging expressions hinder the retrieval of relevant documents, leading to generator hallucinations. To address this issue, we introduce a novel task aimed at resolving emerging mentions to dynamic entities and present DynamicER benchmark. Our benchmark includes dynamic entity mention resolution and entity-centric knowledge-intensive QA task, evaluating entity linking and RAG model's adaptability to new expressions, respectively. We discovered that current entity linking models struggle to link these new expressions to entities. Therefore, we propose a temporal segmented clustering method with continual adaptation, effectively managing the temporal dynamics of evolving entities and emerging mentions. Extensive experiments demonstrate that our method outperforms existing baselines, enhancing RAG model performance on QA task with resolved mentions.</li>
<li><strong>摘要：</strong>在快速发展的语言领域中，在不断更新的知识库中解析新的语言表达仍然是一项艰巨的挑战。这一挑战在具有知识库的检索增强生成 (RAG) 中变得至关重要，因为新出现的表达会阻碍相关文档的检索，从而导致生成器幻觉。为了解决这个问题，我们引入了一项旨在将新出现的提及解析为动态实体的新任务，并提出了 DynamicER 基准。我们的基准包括动态实体提及解析和以实体为中心的知识密集型 QA 任务，分别评估实体链接和 RAG 模型对新表达的适应性。我们发现当前的实体链接模型很难将这些新表达链接到实体。因此，我们提出了一种具有持续适应性的时间分段聚类方法，有效地管理不断发展的实体和新出现的提及的时间动态。大量实验表明，我们的方法优于现有基线，提高了 RAG 模型在具有已解析提及的 QA 任务上的性能。</li>
</ul>

<h3>Title: TopoLM: brain-like spatio-functional organization in a topographic language model</h3>
<ul>
<li><strong>Authors: </strong>Neil Rathi, Johannes Mehrer, Badr AlKhamissi, Taha Binhuraib, Nicholas M. Blauch, Martin Schrimpf</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11516">https://arxiv.org/abs/2410.11516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11516">https://arxiv.org/pdf/2410.11516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11516]] TopoLM: brain-like spatio-functional organization in a topographic language model(https://arxiv.org/abs/2410.11516)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.</li>
<li><strong>摘要：</strong>大脑中的神经元在空间上是有序的，因此组织上的相邻神经元通常表现出相似的反应特征。在人类语言系统中，实验研究已经观察到句法和语义类别的聚类，但这种功能组织背后的机制仍不清楚。在此，基于视觉文献的工作，我们开发了 TopoLM，这是一种具有模型单元显式二维空间表示的变换语言模型。通过将下一个标记预测目标与空间平滑度损失相结合，此模型中的表示会组装成与语义上可解释的文本分组相对应的聚类，并与大脑语言系统中的功能组织紧密匹配。TopoLM 成功预测了皮层语言系统的空间功能组织的出现，以及针对人类皮层中经验观察到的细粒度语言特征选择性的功能簇的组织。我们的结果表明，人类语言系统的功能组织由统一的空间目标驱动，并提供了功能和空间一致的大脑语言处理模型。</li>
</ul>

<h3>Title: Multi-round jailbreak attack on large language models</h3>
<ul>
<li><strong>Authors: </strong>Yihua Zhou, Xiaochuan Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11533">https://arxiv.org/abs/2410.11533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11533">https://arxiv.org/pdf/2410.11533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11533]] Multi-round jailbreak attack on large language models(https://arxiv.org/abs/2410.11533)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Ensuring the safety and alignment of large language models (LLMs) with human values is crucial for generating responses that are beneficial to humanity. While LLMs have the capability to identify and avoid harmful queries, they remain vulnerable to "jailbreak" attacks, where carefully crafted prompts can induce the generation of toxic content. Traditional single-round jailbreak attacks, such as GCG and AutoDAN, do not alter the sensitive words in the dangerous prompts. Although they can temporarily bypass the model's safeguards through prompt engineering, their success rate drops significantly as the LLM is further fine-tuned, and they cannot effectively circumvent static rule-based filters that remove the hazardous vocabulary. In this study, to better understand jailbreak attacks, we introduce a multi-round jailbreak approach. This method can rewrite the dangerous prompts, decomposing them into a series of less harmful sub-questions to bypass the LLM's safety checks. We first use the LLM to perform a decomposition task, breaking down a set of natural language questions into a sequence of progressive sub-questions, which are then used to fine-tune the Llama3-8B model, enabling it to decompose hazardous prompts. The fine-tuned model is then used to break down the problematic prompt, and the resulting sub-questions are sequentially asked to the victim model. If the victim model rejects a sub-question, a new decomposition is generated, and the process is repeated until the final objective is achieved. Our experimental results show a 94\% success rate on the llama2-7B and demonstrate the effectiveness of this approach in circumventing static rule-based filters.</li>
<li><strong>摘要：</strong>确保大型语言模型 (LLM) 的安全性和与人类价值观的一致性对于生成对人类有益的响应至关重要。虽然 LLM 能够识别和避免有害查询，但它们仍然容易受到“越狱”攻击，精心设计的提示可能会诱导有毒内容的生成。传统的单轮越狱攻击（例如 GCG 和 AutoDAN）不会改变危险提示中的敏感词。虽然它们可以通过提示工程暂时绕过模型的保护措施，但随着 LLM 的进一步微调，它们的成功率会显著下降，并且它们无法有效绕过删除危险词汇的基于规则的静态过滤器。在本研究中，为了更好地理解越狱攻击，我们引入了一种多轮越狱方法。这种方法可以重写危险提示，将其分解为一系列危害较小的子问题，以绕过 LLM 的安全检查。我们首先使用 LLM 执行分解任务，将一组自然语言问题分解为一系列渐进的子问题，然后使用它们对 Llama3-8B 模型进行微调，使其能够分解危险提示。然后使用微调后的模型分解有问题的提示，并将得到的子问题按顺序询问受害者模型。如果受害者模型拒绝某个子问题，则生成新的分解，并重复该过程，直到实现最终目标。我们的实验结果显示，在 llama2-7B 上的成功率为 94%，并证明了该方法在绕过基于静态规则的过滤器方面的有效性。</li>
</ul>

<h3>Title: Causal Reasoning in Large Language Models: A Knowledge Graph Approach</h3>
<ul>
<li><strong>Authors: </strong>Yejin Kim, Eojin Kang, Juae Kim, H. Howie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11588">https://arxiv.org/abs/2410.11588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11588">https://arxiv.org/pdf/2410.11588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11588]] Causal Reasoning in Large Language Models: A Knowledge Graph Approach(https://arxiv.org/abs/2410.11588)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) typically improve performance by either retrieving semantically similar information, or enhancing reasoning abilities through structured prompts like chain-of-thought. While both strategies are considered crucial, it remains unclear which has a greater impact on model performance or whether a combination of both is necessary. This paper answers this question by proposing a knowledge graph (KG)-based random-walk reasoning approach that leverages causal relationships. We conduct experiments on the commonsense question answering task that is based on a KG. The KG inherently provides both relevant information, such as related entity keywords, and a reasoning structure through the connections between nodes. Experimental results show that the proposed KG-based random-walk reasoning method improves the reasoning ability and performance of LLMs. Interestingly, incorporating three seemingly irrelevant sentences into the query using KG-based random-walk reasoning enhances LLM performance, contrary to conventional wisdom. These findings suggest that integrating causal structures into prompts can significantly improve reasoning capabilities, providing new insights into the role of causality in optimizing LLM performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常通过检索语义相似的信息或通过思路链等结构化提示增强推理能力来提高性能。虽然这两种策略都被认为很重要，但目前尚不清楚哪种策略对模型性能的影响更大，或者是否需要两者结合。本文通过提出一种利用因果关系的基于知识图谱 (KG) 的随机游走推理方法来回答这个问题。我们对基于 KG 的常识性问答任务进行了实验。KG 本质上既提供相关信息，例如相关实体关键字，也通过节点之间的连接提供推理结构。实验结果表明，所提出的基于 KG 的随机游走推理方法提高了 LLM 的推理能力和性能。有趣的是，与传统观点相反，使用基于 KG 的随机游走推理将三个看似不相关的句子合并到查询中可以提高 LLM 的性能。这些发现表明，将因果结构整合到提示中可以显著提高推理能力，为因果关系在优化 LLM 性能中的作用提供新的见解。</li>
</ul>

<h3>Title: Findings of the WMT 2024 Shared Task on Chat Translation</h3>
<ul>
<li><strong>Authors: </strong>Wafaa Mohammed, Sweta Agrawal, M. Amin Farajian, Vera Cabarrão, Bryan Eikema, Ana C. Farinha, José G. C. de Souza</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11624">https://arxiv.org/abs/2410.11624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11624">https://arxiv.org/pdf/2410.11624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11624]] Findings of the WMT 2024 Shared Task on Chat Translation(https://arxiv.org/abs/2410.11624)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, agent</a></li>
<li><strong>Abstract: </strong>This paper presents the findings from the third edition of the Chat Translation Shared Task. As with previous editions, the task involved translating bilingual customer support conversations, specifically focusing on the impact of conversation context in translation quality and evaluation. We also include two new language pairs: English-Korean and English-Dutch, in addition to the set of language pairs from previous editions: English-German, English-French, and English-Brazilian Portuguese. We received 22 primary submissions and 32 contrastive submissions from eight teams, with each language pair having participation from at least three teams. We evaluated the systems comprehensively using both automatic metrics and human judgments via a direct assessment framework. The official rankings for each language pair were determined based on human evaluation scores, considering performance in both translation directions--agent and customer. Our analysis shows that while the systems excelled at translating individual turns, there is room for improvement in overall conversation-level translation quality.</li>
<li><strong>摘要：</strong>本文介绍了第三版聊天翻译共享任务的调查结果。与之前的版本一样，该任务涉及翻译双语客户支持对话，特别关注对话上下文对翻译质量和评估的影响。除了之前版本的语言对：英语-德语、英语-法语和英语-巴西葡萄牙语之外，我们还包括两个新的语言对：英语-韩语和英语-荷兰语。我们收到了来自 8 个团队的 22 份主要提交和 32 份对比提交，每个语言对都有至少 3 个团队的参与。我们通过直接评估框架，使用自动指标和人工判断对系统进行了全面评估。每个语言对的官方排名都是根据人工评估分数确定的，考虑了两个翻译方向（代理和客户）的表现。我们的分析表明，虽然系统在翻译个别轮次方面表现出色，但整体对话级别的翻译质量仍有改进空间。</li>
</ul>

<h3>Title: Tokenization and Morphology in Multilingual Language Models: A~Comparative Analysis of mT5 and ByT5</h3>
<ul>
<li><strong>Authors: </strong>Thao Anh Dang, Limor Raviv, Lukas Galke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11627">https://arxiv.org/abs/2410.11627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11627">https://arxiv.org/pdf/2410.11627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11627]] Tokenization and Morphology in Multilingual Language Models: A~Comparative Analysis of mT5 and ByT5(https://arxiv.org/abs/2410.11627)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Morphology is a crucial factor for multilingual language modeling as it poses direct challenges for tokenization. Here, we seek to understand how tokenization influences the morphological knowledge encoded in multilingual language models. Specifically, we capture the impact of tokenization by contrasting two multilingual language models: mT5 and ByT5. The two models share the same architecture, training objective, and training data and only differ in their tokenization strategies: subword tokenization vs. character-level tokenization. Probing the morphological knowledge encoded in these models on four tasks and 17 languages, our analyses show that multilingual language models learn the morphological systems of some languages better than others despite similar average performance and that morphological information is encoded in the middle and late layers, where characted-based models need a few more layers to yield commensurate probing accuracy. Finally, we show that languages with more irregularities benefit more from having a higher share of the pre-training data.</li>
<li><strong>摘要：</strong>形态学是多语言语言建模的关键因素，因为它对标记化提出了直接挑战。在这里，我们试图了解标记化如何影响多语言语言模型中编码的形态学知识。具体来说，我们通过对比两个多语言语言模型 mT5 和 ByT5 来捕捉标记化的影响。这两个模型共享相同的架构、训练目标和训练数据，仅在标记化策略上有所不同：子词标记化与字符级标记化。通过在四个任务和 17 种语言上探索这些模型中编码的形态学知识，我们的分析表明，尽管平均性能相似，但多语言语言模型对某些语言的形态系统的学习效果优于其他语言，并且形态学信息编码在中间层和后层，而基于字符的模型需要更多层才能获得相应的探测精度。最后，我们表明，不规则性较多的语言从拥有更高份额的预训练数据中受益更多。</li>
</ul>

<h3>Title: Measuring Spiritual Values and Bias of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Songyuan Liu, Ziyang Zhang, Runze Yan, Wei Wu, Carl Yang, Jiaying Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11647">https://arxiv.org/abs/2410.11647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11647">https://arxiv.org/pdf/2410.11647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11647]] Measuring Spiritual Values and Bias of Large Language Models(https://arxiv.org/abs/2410.11647)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become integral tool for users from various backgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural nuances embedded in their pre-training data. However, the values and perspectives inherent in this data can influence the behavior of LLMs, leading to potential biases. As a result, the use of LLMs in contexts involving spiritual or moral values necessitates careful consideration of these underlying biases. Our work starts with verification of our hypothesis by testing the spiritual values of popular LLMs. Experimental results show that LLMs' spiritual values are quite diverse, as opposed to the stereotype of atheists or secularists. We then investigate how different spiritual values affect LLMs in social-fairness scenarios e.g., hate speech identification). Our findings reveal that different spiritual values indeed lead to different sensitivity to different hate target groups. Furthermore, we propose to continue pre-training LLMs on spiritual texts, and empirical results demonstrate the effectiveness of this approach in mitigating spiritual bias.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为来自不同背景的用户不可或缺的工具。在大量语料库上训练的 LLM 反映了其预训练数据中嵌入的语言和文化细微差别。然而，这些数据中固有的价值观和观点会影响 LLM 的行为，从而导致潜在的偏见。因此，在涉及精神或道德价值观的背景下使用 LLM 需要仔细考虑这些潜在的偏见。我们的工作首先通过测试流行 LLM 的精神价值观来验证我们的假设。实验结果表明，LLM 的精神价值观非常多样化，与无神论者或世俗主义者的刻板印象相反。然后，我们研究不同的精神价值观如何影响社会公平场景中的 LLM（例如仇恨言论识别）。我们的研究结果表明，不同的精神价值观确实会导致对不同仇恨目标群体的不同敏感度。此外，我们建议继续对精神文本的 LLM 进行预训练，实证结果证明了这种方法在减轻精神偏见方面的有效性。</li>
</ul>

<h3>Title: Transformer Layer Injection: A Novel Approach for Efficient Upscaling of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Vo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11654">https://arxiv.org/abs/2410.11654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11654">https://arxiv.org/pdf/2410.11654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11654]] Transformer Layer Injection: A Novel Approach for Efficient Upscaling of Large Language Models(https://arxiv.org/abs/2410.11654)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we propose Transformer Layer Injection (TLI), a novel method for efficiently upscaling large language models (LLMs) while minimizing computational costs and maintaining model performance. Model scale is a key factor in enhancing the quality of machine learning models, and TLI addresses the challenge of scaling by reducing initial loss, minimizing fine-tuning requirements, and preserving model complexity. Our approach improves upon the conventional Depth Up-Scaling (DUS) technique by injecting new layers into every set of K layers, enabling hidden representations to pass through transformer blocks with minimal disruption. We compare TLI with existing approaches, including Mixture of Experts (MoE) and DUS, and validate its efficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results show that TLI achieves better initialization, requires fewer training steps, and delivers superior accuracy on tasks such as KoBEST and KMCQA, with models performing effectively even without additional training. TLI is demonstrated to be both data-efficient and cost-effective, significantly outperforming existing methods. Its scalability and simplicity make it a promising solution for upscaling transformer-based models, with potential applications in scaling models from 10B to 405B parameters.</li>
<li><strong>摘要：</strong>在本文中，我们提出了 Transformer Layer Injection (TLI)，这是一种高效升级大型语言模型 (LLM) 的新方法，同时最大限度地降低计算成本并保持模型性能。模型规模是提高机器学习模型质量的关键因素，TLI 通过减少初始损失、最大限度地降低微调要求和保持模型复杂性来解决扩展挑战。我们的方法改进了传统的深度升级 (DUS) 技术，将新层注入每组 K 层，使隐藏表示能够以最小的干扰通过 Transformer 块。我们将 TLI 与现有方法（包括混合专家 (MoE) 和 DUS）进行了比较，并通过在小型 LLM（LLama3 1B、3B 和 8B）上的实验验证了其效率。结果表明，TLI 实现了更好的初始化，需要更少的训练步骤，并在 KoBEST 和 KMCQA 等任务上提供了卓越的准确性，即使没有额外的训练，模型也能有效地执行。 TLI 被证明既节省数据又节省成本，其表现远超现有方法。它的可扩展性和简单性使其成为升级基于 Transformer 的模型的有前途的解决方案，在将模型从 10B 扩展到 405B 参数方面具有潜在应用。</li>
</ul>

<h3>Title: Retrieval Augmented Spelling Correction for E-Commerce Applications</h3>
<ul>
<li><strong>Authors: </strong>Xuan Guo, Rohit Patki, Dante Everaert, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11655">https://arxiv.org/abs/2410.11655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11655">https://arxiv.org/pdf/2410.11655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11655]] Retrieval Augmented Spelling Correction for E-Commerce Applications(https://arxiv.org/abs/2410.11655)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>The rapid introduction of new brand names into everyday language poses a unique challenge for e-commerce spelling correction services, which must distinguish genuine misspellings from novel brand names that use unconventional spelling. We seek to address this challenge via Retrieval Augmented Generation (RAG). On this approach, product names are retrieved from a catalog and incorporated into the context used by a large language model (LLM) that has been fine-tuned to do contextual spelling correction. Through quantitative evaluation and qualitative error analyses, we find improvements in spelling correction utilizing the RAG framework beyond a stand-alone LLM. We also demonstrate the value of additional finetuning of the LLM to incorporate retrieved context.</li>
<li><strong>摘要：</strong>新品牌名称迅速融入日常语言，这对电子商务拼写更正服务提出了独特的挑战，因为该服务必须区分真正的拼写错误和使用非常规拼写的新品牌名称。我们试图通过检索增强生成 (RAG) 来解决这一挑战。在这种方法中，产品名称从目录中检索出来，并合并到大型语言模型 (LLM) 使用的上下文中，该模型已微调以进行上下文拼写更正。通过定量评估和定性错误分析，我们发现使用 RAG 框架的拼写更正比独立的 LLM 有所改进。我们还展示了对 LLM 进行额外微调以纳入检索到的上下文的价值。</li>
</ul>

<h3>Title: Eliciting Textual Descriptions from Representations of Continuous Prompts</h3>
<ul>
<li><strong>Authors: </strong>Dana Ramati, Daniela Gottesman, Mor Geva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11660">https://arxiv.org/abs/2410.11660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11660">https://arxiv.org/pdf/2410.11660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11660]] Eliciting Textual Descriptions from Representations of Continuous Prompts(https://arxiv.org/abs/2410.11660)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Continuous prompts, or "soft prompts", are a widely-adopted parameter-efficient tuning strategy for large language models, but are often less favorable due to their opaque nature. Prior attempts to interpret continuous prompts relied on projecting individual prompt tokens onto the vocabulary space. However, this approach is problematic as performant prompts can yield arbitrary or contradictory text, and it interprets prompt tokens individually. In this work, we propose a new approach to interpret continuous prompts that elicits textual descriptions from their representations during model inference. Using a Patchscopes variant (Ghandeharioun et al., 2024) called InSPEcT over various tasks, we show our method often yields accurate task descriptions which become more faithful as task performance increases. Moreover, an elaborated version of InSPEcT reveals biased features in continuous prompts, whose presence correlates with biased model predictions. Providing an effective interpretability solution, InSPEcT can be leveraged to debug unwanted properties in continuous prompts and inform developers on ways to mitigate them.</li>
<li><strong>摘要：</strong>连续提示或“软提示”是一种广泛采用的参数高效调整策略，适用于大型语言模型，但由于其不透明性，因此通常不太受欢迎。之前解释连续提示的尝试依赖于将单个提示标记投射到词汇空间上。然而，这种方法是有问题的，因为高性能提示可能会产生任意或矛盾的文本，并且它会单独解释提示标记。在这项工作中，我们提出了一种解释连续提示的新方法，该方法在模型推理过程中从其表示中引出文本描述。使用 Patchscopes 变体（Ghandeharioun 等人，2024 年）InSPEcT 执行各种任务，我们表明我们的方法通常可以产生准确的任务描述，并且随着任务性能的提高，这些描述会变得更加忠实。此外，InSPEcT 的详细版本揭示了连续提示中的有偏特征，其存在与有偏模型预测相关。通过提供有效的可解释性解决方案，InSPEcT 可用于调试连续提示中不需要的属性，并告知开发人员如何缓解这些属性。</li>
</ul>

<h3>Title: Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Pacchiardi, Marko Tesic, Lucy G. Cheke, José Hernández-Orallo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11672">https://arxiv.org/abs/2410.11672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11672">https://arxiv.org/pdf/2410.11672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11672]] Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers(https://arxiv.org/abs/2410.11672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The integrity of AI benchmarks is fundamental to accurately assess the capabilities of AI systems. The internal validity of these benchmarks - i.e., making sure they are free from confounding factors - is crucial for ensuring that they are measuring what they are designed to measure. In this paper, we explore a key issue related to internal validity: the possibility that AI systems can solve benchmarks in unintended ways, bypassing the capability being tested. This phenomenon, widely known in human and animal experiments, is often referred to as the 'Clever Hans' effect, where tasks are solved using spurious cues, often involving much simpler processes than those putatively assessed. Previous research suggests that language models can exhibit this behaviour as well. In several older Natural Language Processing (NLP) benchmarks, individual $n$-grams like "not" have been found to be highly predictive of the correct labels, and supervised NLP models have been shown to exploit these patterns. In this work, we investigate the extent to which simple $n$-grams extracted from benchmark instances can be combined to predict labels in modern multiple-choice benchmarks designed for LLMs, and whether LLMs might be using such $n$-gram patterns to solve these benchmarks. We show how simple classifiers trained on these $n$-grams can achieve high scores on several benchmarks, despite lacking the capabilities being tested. Additionally, we provide evidence that modern LLMs might be using these superficial patterns to solve benchmarks. This suggests that the internal validity of these benchmarks may be compromised and caution should be exercised when interpreting LLM performance results on them.</li>
<li><strong>摘要：</strong>人工智能基准的完整性对于准确评估人工智能系统的能力至关重要。这些基准的内部有效性（即确保它们不受混杂因素的影响）对于确保它们测量的是它们设计用来测量的内容至关重要。在本文中，我们探讨了与内部有效性相关的一个关键问题：人工智能系统可能以意想不到的方式解决基准问题，从而绕过正在测试的能力。这种现象在人类和动物实验中广为人知，通常被称为“聪明的汉斯”效应，即使用虚假线索解决任务，通常涉及比假定评估的过程简单得多的过程。先前的研究表明，语言模型也可以表现出这种行为。在几个较旧的自然语言处理 (NLP) 基准中，已经发现像“not”这样的单个 $n$-gram 对正确标签具有很高的预测性，并且监督式 NLP 模型已被证明可以利用这些模式。在这项工作中，我们研究了从基准实例中提取的简单 $n$-grams 可以在多大程度上组合起来预测为 LLM 设计的现代多项选择基准中的标签，以及 LLM 是否可能使用此类 $n$-gram 模式来解决这些基准。我们展示了在这些 $n$-grams 上训练的简单分类器如何在缺乏所测试的能力的情况下在多个基准上取得高分。此外，我们还提供了证据表明现代 LLM 可能正在使用这些表面模式来解决基准。这表明这些基准的内部有效性可能会受到影响，在解释 LLM 在这些基准上的表现结果时应谨慎行事。</li>
</ul>

<h3>Title: Understanding Likelihood Over-optimisation in Direct Alignment Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Zhengyan Shi, Sander Land, Acyr Locatelli, Matthieu Geist, Max Bartolo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11677">https://arxiv.org/abs/2410.11677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11677">https://arxiv.org/pdf/2410.11677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11677]] Understanding Likelihood Over-optimisation in Direct Alignment Algorithms(https://arxiv.org/abs/2410.11677)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation (DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as Proximal Policy Optimisation (PPO) for aligning language models to human preferences, without the need for explicit reward modelling. These methods generally aim to increase the likelihood of generating better (preferred) completions while discouraging worse (non-preferred) ones, while staying close to the original model's behaviour. In this work, we explore the relationship between completion likelihood and model performance in state-of-the-art DAAs, and identify a critical issue of likelihood over-optimisation. Contrary to expectations, we find that higher likelihood of better completions and larger margins between better and worse completion likelihoods do not necessarily lead to better performance, and may even degrade it. Our analysis reveals that while higher likelihood correlates with better memorisation of factual knowledge patterns, a slightly lower completion likelihood tends to improve output diversity, thus leading to better generalisation to unseen scenarios. Moreover, we identify two key indicators that signal when over-optimised output diversity begins to harm performance: Decreasing Entropy over Top-k Tokens and Diminishing Top-k Probability Mass. Our experimental results validate that these indicators are reliable signs of declining performance under different regularisations, helping prevent over-optimisation and improve alignment with human preferences.</li>
<li><strong>摘要：</strong>直接对齐算法 (DAA)，例如直接偏好优化 (DPO) 和身份偏好优化 (IPO)，已成为在线强化学习人类反馈 (RLHF) 算法（例如近端策略优化 (PPO)）的替代方案，用于将语言模型与人类偏好对齐，而无需显式奖励建模。这些方法通常旨在增加生成更好（首选）完成的可能性，同时阻止更差（非首选）完成，同时保持接近原始模型的行为。在这项工作中，我们探索了完成可能性与最先进 DAA 中的模型性能之间的关系，并确定了可能性过度优化的关键问题。与预期相反，我们发现更好的完成可能性更高以及更好和更差的完成可能性之间的更大差距并不一定会带来更好的性能，甚至可能会降低它。我们的分析表明，虽然较高的完成可能性与更好地记忆事实知识模式相关，但略低的完成可能性往往会提高输出多样性，从而更好地推广到未见过的场景。此外，我们确定了两个关键指标，它们表明过度优化的输出多样性何时开始损害性能：Top-k 标记的熵降低和 Top-k 概率质量的减小。我们的实验结果证实，这些指标是不同正则化下性能下降的可靠迹象，有助于防止过度优化并提高与人类偏好的一致性。</li>
</ul>

<h3>Title: IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with Sentence Interpolation and Gradual MT</h3>
<ul>
<li><strong>Authors: </strong>Seung-Woo Choi, Ga-Hyun Yoo, Jay-Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11693">https://arxiv.org/abs/2410.11693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11693">https://arxiv.org/pdf/2410.11693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11693]] IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with Sentence Interpolation and Gradual MT(https://arxiv.org/abs/2410.11693)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent Large Language Models (LLMs) have demonstrated strong performance in translation without needing to be finetuned on additional parallel corpora. However, they still underperform for low-resource language pairs. Previous works have focused on mitigating this issue by leveraging relevant few-shot examples or external resources such as dictionaries or grammar books, making models heavily reliant on these nonparametric sources of information. In this paper, we propose a novel method named IntGrad MT that focuses on fully exploiting an LLM's inherent translation capability. IntGrad MT achieves this by constructing a chain of few-shot examples, each consisting of a source sentence and the model's own translation, that rise incrementally in difficulty. IntGrad MT employs two techniques: Sentence Interpolation, which generates a sequence of sentences that gradually change from an easy sentence to translate to a difficult one, and Gradual MT, which sequentially translates this chain using translations of earlier sentences as few-shot examples for the translation of subsequent ones. With this approach, we observe a substantial enhancement in the xCOMET scores of various LLMs for multiple languages, especially in low-resource languages such as Hindi(8.26), Swahili(7.10), Bengali(6.97) and Marathi(13.03). Our approach presents a practical way of enhancing LLMs' performance without extra training.</li>
<li><strong>摘要：</strong>最近的大型语言模型 (LLM) 在翻译方面表现出色，无需在其他平行语料库上进行微调。然而，它们在资源较少的语言对上仍然表现不佳。以前的研究主要通过利用相关的少量样本或外部资源（如字典或语法书）来缓解这一问题，这使得模型严重依赖这些非参数信息源。在本文中，我们提出了一种名为 IntGrad MT 的新方法，该方法专注于充分利用 LLM 固有的翻译能力。IntGrad MT 通过构建一系列少量样本示例来实现这一点，每个样本示例都由一个源句子和模型自己的翻译组成，难度逐渐增加。IntGrad MT 采用了两种技术：句子插值，它生成一系列句子，这些句子逐渐从易翻译的句子变为难翻译的句子；渐进式 MT，它使用前面句子的翻译作为后续句子翻译的少量样本示例来按顺序翻译这个链。通过这种方法，我们观察到多种语言的各种 LLM 的 xCOMET 分数都有显著提高，尤其是在资源匮乏的语言中，例如印地语（8.26）、斯瓦希里语（7.10）、孟加拉语（6.97）和马拉地语（13.03）。我们的方法提供了一种无需额外训练即可提高 LLM 性能的实用方法。</li>
</ul>

<h3>Title: Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Fu, Ruobing Xie, Jiazhen Liu, Bangxiang Lan, Xingwu Sun, Zhanhui Kang, Xirong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11701">https://arxiv.org/abs/2410.11701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11701">https://arxiv.org/pdf/2410.11701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11701]] Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions(https://arxiv.org/abs/2410.11701)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Hallucinations in multimodal large language models (MLLMs) hinder their practical applications. To address this, we propose a Magnifier Prompt (MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs via extremely simple instructions. MagPrompt is based on the following two key principles, which guide the design of various effective prompts, demonstrating robustness: (1) MLLMs should focus more on the image. (2) When there are conflicts between the image and the model's inner knowledge, MLLMs should prioritize the image. MagPrompt is training-free and can be applied to open-source and closed-source models, such as GPT-4o and Gemini-pro. It performs well across many datasets and its effectiveness is comparable or even better than more complex methods like VCD. Furthermore, our prompt design principles and experimental analyses provide valuable insights into multimodal hallucination.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 中的幻觉阻碍了它们的实际应用。为了解决这个问题，我们提出了一种放大镜提示 (MagPrompt)，这是一种简单而有效的方法，通过极其简单的指令来解决 MLLM 中的幻觉问题。MagPrompt 基于以下两个关键原则，指导各种有效提示的设计，并表现出鲁棒性：(1) MLLM 应该更多地关注图像。(2) 当图像与模型的内部知识存在冲突时，MLLM 应该优先考虑图像。MagPrompt 无需训练，可应用于开源和闭源模型，例如 GPT-4o 和 Gemini-pro。它在许多数据集上表现良好，其有效性与 VCD 等更复杂的方法相当甚至更好。此外，我们的提示设计原则和实验分析为多模态幻觉提供了宝贵的见解。</li>
</ul>

<h3>Title: MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pei Wang, Yanan Wu, Zekun Wang, Jiaheng Liu, Xiaoshuai Song, Zhongyuan Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, Ge Zhang, Hangyu Guo, Zhaoxiang Zhang, Wenbo Su, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11710">https://arxiv.org/abs/2410.11710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11710">https://arxiv.org/pdf/2410.11710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11710]] MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models(https://arxiv.org/abs/2410.11710)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the "multi-granularity" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTUthis http URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在推理和决策能力方面表现出了巨大的进步，并且可以与用户进行自然的对话。最近，已经提出了许多工具使用基准数据集。然而，现有的数据集有以下局限性：(1)。评估场景不足（例如，仅涵盖有限的工具使用场景）。(2)。评估成本高昂（例如，GPT API 成本）。为了解决这些限制，在这项工作中，我们提出了一种用于大型语言模型的多粒度工具使用基准，称为 MTU-Bench。对于“多粒度”属性，我们的 MTU-Bench 涵盖了五种工具使用场景（即单转单工具、单转多工具、多转单工具、多转多工具以及分布外任务）。此外，我们的 MTU-Bench 的所有评估指标均基于预测结果和基本事实，而不使用任何 GPT 或人工评估指标。此外，我们的 MTU-Bench 是通过转换现有的高质量数据集来模拟真实世界的工具使用场景而收集的，我们还提出了一个名为 MTU-Instruct 数据的指令数据集来增强现有 LLM 的工具使用能力。全面的实验结果证明了我们的 MTU-Bench 的有效性。代码和数据将在 https://github.com/MTU-Bench-Team/MTU 这个 http URL 上发布。</li>
</ul>

<h3>Title: Converging to a Lingua Franca: Evolution of Linguistic Regions and Semantics Alignment in Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongchuan Zeng, Senyu Han, Lu Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11718">https://arxiv.org/abs/2410.11718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11718">https://arxiv.org/pdf/2410.11718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11718]] Converging to a Lingua Franca: Evolution of Linguistic Regions and Semantics Alignment in Multilingual Large Language Models(https://arxiv.org/abs/2410.11718)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable performance, particularly in multilingual contexts. While recent studies suggest that LLMs can transfer skills learned in one language to others, the internal mechanisms behind this ability remain unclear. We observed that the neuron activation patterns of LLMs exhibit similarities when processing the same language, revealing the existence and location of key linguistic regions. Additionally, we found that neuron activation patterns are similar when processing sentences with the same semantic meaning in different languages. This indicates that LLMs map semantically identical inputs from different languages into a "Lingua Franca", a common semantic latent space that allows for consistent processing across languages. This semantic alignment becomes more pronounced with training and increased model size, resulting in a more language-agnostic activation pattern. Moreover, we found that key linguistic neurons are concentrated in the first and last layers of LLMs, becoming denser in the first layers as training progresses. Experiments on BLOOM and LLaMA2 support these findings, highlighting the structural evolution of multilingual LLMs during training and scaling up. This paper provides insights into the internal workings of LLMs, offering a foundation for future improvements in their cross-lingual capabilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出色，尤其是在多语言环境中。虽然最近的研究表明 LLM 可以将一种语言中学到的技能转移到其他语言中，但这种能力背后的内部机制仍不清楚。我们观察到，LLM 的神经元激活模式在处理同一种语言时表现出相似性，揭示了关键语言区域的存在和位置。此外，我们发现在处理不同语言中具有相同语义含义的句子时，神经元激活模式相似。这表明 LLM 将来自不同语言的语义相同的输入映射到“Lingua Franca”，这是一个通用的语义潜在空间，允许跨语言进行一致的处理。这种语义对齐随着训练和模型大小的增加而变得更加明显，从而产生与语言无关的激活模式。此外，我们发现关键语言神经元集中在 LLM 的第一层和最后一层，随着训练的进行，第一层的神经元变得更密集。BLOOM 和 LLaMA2 上的实验支持这些发现，突出了多语言 LLM 在训练和扩展过程中的结构演变。本文深入探讨了法学硕士 (LLM) 的内部运作，为未来提高跨语言能力奠定了基础。</li>
</ul>

<h3>Title: Personas with Attitudes: Controlling LLMs for Diverse Data Annotation</h3>
<ul>
<li><strong>Authors: </strong>Leon Fröhling, Gianluca Demartini, Dennis Assenmacher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11745">https://arxiv.org/abs/2410.11745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11745">https://arxiv.org/pdf/2410.11745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11745]] Personas with Attitudes: Controlling LLMs for Diverse Data Annotation(https://arxiv.org/abs/2410.11745)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We present a novel approach for enhancing diversity and control in data annotation tasks by personalizing large language models (LLMs). We investigate the impact of injecting diverse persona descriptions into LLM prompts across two studies, exploring whether personas increase annotation diversity and whether the impacts of individual personas on the resulting annotations are consistent and controllable. Our results show that persona-prompted LLMs produce more diverse annotations than LLMs prompted without personas and that these effects are both controllable and repeatable, making our approach a suitable tool for improving data annotation in subjective NLP tasks like toxicity detection.</li>
<li><strong>摘要：</strong>我们提出了一种通过个性化大型语言模型 (LLM) 来增强数据注释任务多样性和控制力的新方法。我们在两项研究中调查了将不同的人物角色描述注入 LLM 提示的影响，探索人物角色是否会增加注释多样性，以及个人人物角色对最终注释的影响是否一致且可控。我们的结果表明，人物角色提示的 LLM 比没有人物角色提示的 LLM 产生更多样化的注释，并且这些效果既可控又可重复，使我们的方法成为改进主观 NLP 任务（如毒性检测）中数据注释的合适工具。</li>
</ul>

<h3>Title: Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kai Yao, Penlei Gao, Lichun Li, Yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11772">https://arxiv.org/abs/2410.11772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11772">https://arxiv.org/pdf/2410.11772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11772]] Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models(https://arxiv.org/abs/2410.11772)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant popularity for adapting pre-trained Large Language Models (LLMs) to downstream tasks, primarily due to their potential to significantly reduce memory and computational overheads. However, a common limitation in most PEFT approaches is their application of a uniform architectural design across all layers. This uniformity involves identical trainable modules and ignores the varying importance of each layer, leading to sub-optimal fine-tuning results. To overcome the above limitation and obtain better performance, we develop a novel approach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent sparsity and select the most important subset of full layers with effective layer-wise importance scoring. The proposed IST is a versatile and plug-and-play technique compatible with various PEFT methods that operate on a per-layer basis. By leveraging the estimated importance scores, IST dynamically updates these selected layers in PEFT modules, leading to reduced memory demands. We further provide theoretical proof of convergence and empirical evidence of superior performance to demonstrate the advantages of IST over uniform updating strategies. Extensive experiments on a range of LLMs, PEFTs, and downstream tasks substantiate the effectiveness of our proposed method, showcasing IST's capacity to enhance existing layer-based PEFT methods. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>参数高效微调 (PEFT) 方法因将预训练的大型语言模型 (LLM) 应用于下游任务而广受欢迎，这主要是因为它们有可能显著减少内存和计算开销。然而，大多数 PEFT 方法的一个共同限制是它们在所有层上应用统一的架构设计。这种统一性涉及相同的可训练模块，并忽略了每层的不同重要性，导致微调结果不理想。为了克服上述限制并获得更好的性能，我们开发了一种新方法，即重要性感知稀疏调整 (IST)，以充分利用固有的稀疏性，并通过有效的逐层重要性评分选择最重要的全层子集。所提出的 IST 是一种多功能且即插即用的技术，与基于每层运行的各种 PEFT 方法兼容。通过利用估计的重要性分数，IST 动态更新 PEFT 模块中这些选定的层，从而减少内存需求。我们进一步提供了收敛的理论证明和卓越性能的经验证据，以证明 IST 优于统一更新策略。在一系列 LLM、PEFT 和下游任务上进行的大量实验证实了我们提出的方法的有效性，展示了 IST 增强现有基于层的 PEFT 方法的能力。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11779">https://arxiv.org/abs/2410.11779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11779">https://arxiv.org/pdf/2410.11779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11779]] MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation(https://arxiv.org/abs/2410.11779)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at this https URL.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 经常出现幻觉现象，但其根本原因仍不清楚。在本文中，我们进行了实证分析，发现尽管 MLLM 错误地生成了最终输出中的对象，但它们实际上能够识别前几层中的视觉对象。我们推测这可能是由于语言模型的强大知识先验抑制了视觉信息，从而导致幻觉。受此启发，我们提出了一种新颖的 MLLM 动态校正解码方法 (DeCo)，该方法自适应地选择适当的前几层并按比例将知识整合到最后一层以调整输出逻辑。请注意，DeCo 与模型无关，可以与各种经典解码策略无缝结合并应用于不同的 MLLM。我们在广泛使用的基准上对 DeCo 进行了评估，表明与基线相比，它可以大幅降低幻觉率，凸显了其缓解幻觉的潜力。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability</h3>
<ul>
<li><strong>Authors: </strong>Tsz Ting Chung, Leyang Cui, Lemao Liu, Xinting Huang, Shuming Shi, Dit-Yan Yeung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11786">https://arxiv.org/abs/2410.11786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11786">https://arxiv.org/pdf/2410.11786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11786]] Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability(https://arxiv.org/abs/2410.11786)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities in a wide range of natural language processing tasks when leveraging in-context learning. To mitigate the additional computational and financial costs associated with in-context learning, several prompt compression methods have been proposed to compress the in-context learning prompts. Despite their success, these methods face challenges with transferability due to model-specific compression, or rely on external training data, such as GPT-4. In this paper, we investigate the ability of LLMs to develop a unified compression method that discretizes uninformative tokens, utilizing a self-supervised pre-training technique. By introducing a small number of parameters during the continual pre-training, the proposed Selection-p produces a probability for each input token, indicating whether to preserve or discard it. Experiments show Selection-p achieves state-of-the-art performance across numerous classification tasks, achieving compression rates of up to 10 times while experiencing only a marginal 0.8% decrease in performance. Moreover, it exhibits superior transferability to different models compared to prior work. Additionally, we further analyze how Selection-p helps maintain performance on in-context learning with long contexts.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在利用上下文学习时，已在广泛的自然语言处理任务中展现出令人印象深刻的能力。为了减轻与上下文学习相关的额外计算和财务成本，已经提出了几种提示压缩方法来压缩上下文学习提示。尽管这些方法取得了成功，但由于模型特定的压缩，这些方法面临着可移植性的挑战，或者依赖于外部训练数据，例如 GPT-4。在本文中，我们研究了 LLM 利用自监督预训练技术开发一种统一的压缩方法的能力，该方法可以离散化无信息标记。通过在持续预训练期间引入少量参数，提出的 Selection-p 为每个输入标记生成一个概率，指示是保留还是丢弃它。实验表明，Selection-p 在众多分类任务中实现了最先进的性能，实现了高达 10 倍的压缩率，而性能仅下降了 0.8%。此外，与以前的工作相比，它表现出对不同模型的卓越可移植性。此外，我们进一步分析了 Selection-p 如何帮助维持长上下文学习中的表现。</li>
</ul>

<h3>Title: NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, Wenliang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.11805">https://arxiv.org/abs/2410.11805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.11805">https://arxiv.org/pdf/2410.11805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.11805]] NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models(https://arxiv.org/abs/2410.11805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 与工具学习相结合在实际应用中取得了令人瞩目的成果。在工具学习过程中，LLM 可能会以嵌套顺序调用多个工具，其中后一个工具调用可能会将前一个工具调用的响应作为其输入参数。然而，由于现有基准缺乏相关数据实例，目前对嵌套工具学习能力的研究仍未得到充分探索。为了解决这个问题，我们引入了 NesTools 来弥补目前在全面嵌套工具学习评估方面的差距。NesTools 包含一种新颖的自动数据生成方法，用于构建具有不同嵌套结构的大规模嵌套工具调用。通过人工审查和改进，数据集质量很高，并且与实际场景紧密相关。因此，NesTools 可以作为评估 LLM 嵌套工具学习能力的新基准。我们对 22 个 LLM 进行了广泛的实验，并使用 NesTools 进行了深入分析，结果表明当前的 LLM 仍然难以完成复杂的嵌套工具学习任务。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
