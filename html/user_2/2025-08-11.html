<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-11</h1>
<h3>Title: PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Rania Al-Sabbagh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05722">https://arxiv.org/abs/2508.05722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05722">https://arxiv.org/pdf/2508.05722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05722]] PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare(https://arxiv.org/abs/2508.05722)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces PEACH, a sentence-aligned parallel English-Arabic corpus of healthcare texts encompassing patient information leaflets and educational materials. The corpus contains 51,671 parallel sentences, totaling approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths vary between 9.52 and 11.83 words on average. As a manually aligned corpus, PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics, translation studies, and natural language processing. It can be used to derive bilingual lexicons, adapt large language models for domain-specific machine translation, evaluate user perceptions of machine translation in healthcare, assess patient information leaflets and educational materials' readability and lay-friendliness, and as an educational resource in translation studies. PEACH is publicly accessible.</li>
<li><strong>摘要：</strong>本文介绍了桃子，这是一种与句子平行的英语阿拉伯医疗文本语料库，其中包括患者信息传单和教育材料。该语料库包含51,671个并行句子，总计约590,517英语和567,707阿拉伯语标记。句子的长度平均在9.52和11.83词之间。作为一个手动调整的语料库，Peach是一个金色标准的语料库，是对比度语言学，翻译研究和自然语言处理的研究人员。它可用于得出双语词典，适应特定领域的机器翻译的大型语言模型，评估用户对医疗保健机器翻译的看法，评估患者信息传单和教育材料的可读性和外行友好性，以及翻译研究中的教育资源。桃子是公开接触的。</li>
</ul>

<h3>Title: Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Chi Zhang, Changjia Zhu, Junjie Xiong, Xiaoran Xu, Lingyao Li, Yao Liu, Zhuo Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05775">https://arxiv.org/abs/2508.05775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05775">https://arxiv.org/pdf/2508.05775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05775]] Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation(https://arxiv.org/abs/2508.05775)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized content creation across digital platforms, offering unprecedented capabilities in natural language generation and understanding. These models enable beneficial applications such as content generation, question and answering (Q&A), programming, and code reasoning. Meanwhile, they also pose serious risks by inadvertently or intentionally producing toxic, offensive, or biased content. This dual role of LLMs, both as powerful tools for solving real-world problems and as potential sources of harmful language, presents a pressing sociotechnical challenge. In this survey, we systematically review recent studies spanning unintentional toxicity, adversarial jailbreaking attacks, and content moderation techniques. We propose a unified taxonomy of LLM-related harms and defenses, analyze emerging multimodal and LLM-assisted jailbreak strategies, and assess mitigation efforts, including reinforcement learning with human feedback (RLHF), prompt engineering, and safety alignment. Our synthesis highlights the evolving landscape of LLM safety, identifies limitations in current evaluation methodologies, and outlines future research directions to guide the development of robust and ethically aligned language technologies.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）彻底改变了跨数字平台的内容创建，从而提供了自然语言生成和理解的前所未有的功能。这些模型可以实现有益的应用程序，例如内容生成，问答（问答），编程和代码推理。同时，它们还通过无意中或故意产生有毒，令人反感或有偏见的内容构成严重风险。 LLM的这种双重作用既是解决现实世界问题的强大工具，又是有害语言的潜在来源，都提出了紧迫的社会技术挑战。在这项调查中，我们系统地回顾了最近的研究，这些研究涵盖了无意的毒性，对抗性越狱攻击和内容节制技术。我们提出了与LLM相关的危害和防御措施的统一分类法，分析新兴的多模式和LLM辅助越狱策略，并评估缓解工作，包括使用人为反馈（RLHF）的加强学习，及时的工程和安全一致性。我们的合成强调了LLM安全性的不断发展的景观，确定了当前评估方法的局限性，并概述了未来的研究方向，以指导稳健和道德上一致的语言技术的发展。</li>
</ul>

<h3>Title: FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification</h3>
<ul>
<li><strong>Authors: </strong>Xiangyan Chen, Yufeng Li, Yujian Gan, Arkaitz Zubiaga, Matthew Purver</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05782">https://arxiv.org/abs/2508.05782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05782">https://arxiv.org/pdf/2508.05782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05782]] FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification(https://arxiv.org/abs/2508.05782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to produce hallucinations - factually incorrect or fabricated information - which poses significant challenges for many Natural Language Processing (NLP) applications, such as dialogue systems. As a result, detecting hallucinations has become a critical area of research. Current approaches to hallucination detection in dialogue systems primarily focus on verifying the factual consistency of generated responses. However, these responses often contain a mix of accurate, inaccurate or unverifiable facts, making one factual label overly simplistic and coarse-grained. In this paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact verification, which involves verifying atomic facts extracted from dialogue responses. To support this, we construct a dataset based on publicly available dialogue datasets and evaluate it using various baseline methods. Experimental results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance in dialogue fact verification. Despite this, the best F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is only 0.75, indicating that the benchmark remains a challenging task for future research. Our dataset and code will be public on GitHub.</li>
<li><strong>摘要：</strong>已知大型语言模型（LLM）会产生幻觉 - 实际上是错误或捏造的信息 - 这对许多自然语言处理（NLP）应用（例如对话系统）构成了重大挑战。结果，检测幻觉已成为研究的关键领域。对话系统中当前的幻觉检测方法主要集中于验证生成的响应的事实一致性。但是，这些响应通常包含精确，不准确或无法验证的事实的混合，使一个事实标签过于简单和粗糙。在本文中，我们介绍了一个基准，“罚款”，以进行精细的对话事实验证，其中涉及验证从对话响应中提取的原子事实。为了支持这一点，我们根据公开可用的对话数据集构建数据集，并使用各种基线方法对其进行评估。实验结果表明，结合了经营链（COT）推理的方法可以在对话事实验证中提高性能。尽管如此，在HybriDialogue（一个开放域对话数据集）上取得的最佳F1得分仅为0.75，表明基准仍然是未来研究的具有挑战性的任务。我们的数据集和代码将在GitHub上公开。</li>
</ul>

<h3>Title: Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models</h3>
<ul>
<li><strong>Authors: </strong>Abishek Thamma, Micha Heilbron</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05803">https://arxiv.org/abs/2508.05803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05803">https://arxiv.org/pdf/2508.05803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05803]] Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models(https://arxiv.org/abs/2508.05803)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Human memory is fleeting. As words are processed, the exact wordforms that make up incoming sentences are rapidly lost. Cognitive scientists have long believed that this limitation of memory may, paradoxically, help in learning language - an idea supported by classic connectionist modelling work. The rise of Transformers appears to challenge this idea, as these models can learn language effectively, despite lacking memory limitations or other architectural recency biases. Here, we investigate the hypothesized benefit of fleeting memory for language learning in tightly controlled experiments on transformer language models. Training transformers with and without fleeting memory on a developmentally realistic training set, we find that fleeting memory consistently improves language learning (as quantified by both overall language modelling performance and targeted syntactic evaluation) but, unexpectedly, impairs surprisal-based prediction of human reading times. Interestingly, follow up analyses revealed that this discrepancy - better language modeling, yet worse reading time prediction - could not be accounted for by prior explanations of why better language models sometimes fit human reading time worse. Together, these results support a benefit of memory limitations on neural network language learning - but not on predicting behavior.</li>
<li><strong>摘要：</strong>人类记忆正在转瞬即逝。随着单词的处理，构成传入句子的确切单词形式迅速丢失。认知科学家长期以来一直认为，这种对记忆的局限性可能会有助于学习语言 - 经典连接主义建模工作支持的想法。变形金刚的兴起似乎挑战了这一想法，因为尽管缺乏记忆限制或其他建筑新兴偏见，但这些模型仍可以有效地学习语言。在这里，我们研究了在变压器语言模型的严格控制实验中为语言学习转瞬即逝的假设益处。训练变形金刚在有或没有短暂记忆的训练训练集中，我们发现短暂的记忆一致地改善了语言学习（通过整体语言建模表现和有针对性的句法评估量化），但出乎意料的是，基于人类阅读时间的惊人预测会损害出人意料的预测。有趣的是，后续分析表明，这种差异 - 更好的语言建模，而更差的阅读时间预测 - 无法通过先前的解释来解释为什么更好的语言模型有时会使人的阅读时间更加适合人类阅读时间。这些结果共同支持了对神经网络语言学习的记忆限制的好处，但不能预测行为。</li>
</ul>

<h3>Title: "Mirror" Language AI Models of Depression are Criterion-Contaminated</h3>
<ul>
<li><strong>Authors: </strong>Tong Li, Rasiq Hussain, Mehak Gupta, Joshua R. Oltmanns</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05830">https://arxiv.org/abs/2508.05830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05830">https://arxiv.org/pdf/2508.05830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05830]] "Mirror" Language AI Models of Depression are Criterion-Contaminated(https://arxiv.org/abs/2508.05830)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>A growing number of studies show near-perfect LLM language-based prediction of depression assessment scores (up to R2 of .70). However, many develop these models directly from language responses to depression assessments. These "Mirror models" suffer from "criterion contamination", which arises when a predicted score depends in part on the predictors themselves. This causes artificial effect size inflation which reduces model generalizability. The present study compares the performance of Mirror models versus "Non-Mirror models", which are developed from language that does not mirror the assessment they are developed to predict. N = 110 research participants completed two different interviews: structured diagnostic and life history interviews. GPT-4, GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic interview depression scores from the two transcripts separately. Mirror models (using structured diagnostic data) showed very large effect sizes (e.g., R2 = .80). As expected, NonMirror models (using life history data) demonstrated smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror and Non-Mirror model-predicted structured interview depression scores were correlated with self-reported depression symptoms, Mirror and NonMirror performed the same (e.g., r = ~.54), indicating that Mirror models contain bias perhaps due to criterion contamination. Topic modeling identified clusters across Mirror and Non-Mirror models, as well as between true-positive and false-positive predictions. In this head-to-head comparison study, Mirror language AI models of depression showed artificially inflated effect sizes and less generalizability. As language AI models for depression continue to evolve, incorporating Non-Mirror models may identify interpretable, and generalizable semantic features that have unique utility in real-world psychological assessment.</li>
<li><strong>摘要：</strong>越来越多的研究表明，基于LLM语言的完美评估评估评分的预测（最高为.70）。但是，许多人直接从对抑郁评估的语言反应中开发了这些模型。这些“镜像模型”遭受“标准污染”的影响，当预测分数部分取决于预测因子本身时，就会产生。这会导致人工效应的大小通胀，从而降低了模型的普遍性。本研究比较了镜像模型与“非摩尔模型”的性能，这些模型是根据语言开发的，这些语言不能反映出它们要预测的评估。 n = 110个研究参与者完成了两种不同的访谈：结构化诊断和生活史访谈。然后提示GPT-4，GPT-4O和LLAMA3-70B分别预测两个转录本的结构化诊断访谈得分。镜像模型（使用结构化诊断数据）显示出非常大的效应大小（例如，R2 = .80）。正如预期的那样，非MIRROR模型（使用生命历史数据）表现出较小的效应大小，但相对较大（例如R2 = .27）。当镜像和非摩尔型模型预测的结构化访谈分数与自我报告的抑郁症状相关时，镜像和非媒介的执行情况相同（例如，r =〜.54），表明镜像模型可能由于标准污染而造成偏见。主题建模确定了跨镜像和非摩尔模型的簇，以及在真实阳性和假阳性预测之间。在这项头对头的比较研究中，抑郁症的镜语AI模型显示了人为膨胀的效果大小和较少的概括性。随着语言AI的抑郁模型不断发展，合并非摩擦模型可能会识别出可解释的，可概括的语义特征，这些特征在现实世界心理评估中具有独特的实用性。</li>
</ul>

<h3>Title: Discovering Properties of Inflectional Morphology in Neural Emergent Communication</h3>
<ul>
<li><strong>Authors: </strong>Miles Gilberti, Shane Storks, Huteng Dai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05843">https://arxiv.org/abs/2508.05843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05843">https://arxiv.org/pdf/2508.05843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05843]] Discovering Properties of Inflectional Morphology in Neural Emergent Communication(https://arxiv.org/abs/2508.05843)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Emergent communication (EmCom) with deep neural network-based agents promises to yield insights into the nature of human language, but remains focused primarily on a few subfield-specific goals and metrics that prioritize communication schemes which represent attributes with unique characters one-to-one and compose them syntactically. We thus reinterpret a common EmCom setting, the attribute-value reconstruction game, by imposing a small-vocabulary constraint to simulate double articulation, and formulating a novel setting analogous to naturalistic inflectional morphology (enabling meaningful comparison to natural language communication schemes). We develop new metrics and explore variations of this game motivated by real properties of inflectional morphology: concatenativity and fusionality. Through our experiments, we discover that simulated phonological constraints encourage concatenative morphology, and emergent languages replicate the tendency of natural languages to fuse grammatical attributes.</li>
<li><strong>摘要：</strong>与基于神经网络的代理的新兴沟通（EMCOM）有望对人类语言的本质产生见解，但仍主要集中在一些特定于子场的目标和指标上，这些目标和指标优先考虑与独特字符一对一代表属性并构成句法的属性。因此，我们通过施加小型烟囱的约束来模拟双重表达，并制定类似于自然主义拐点形态的新颖设置（使自然主义的拐点形态（使有意义的比较与自然语言通信方案）实现新颖的设置，从而重新解释了一个常见的EMCOM设置，即属性值重建游戏。我们开发了新的指标，并探索了该游戏的变化，该游戏是出于弯曲形态的真实特性的动机：串联性和融合性。通过我们的实验，我们发现模拟的语音约束鼓励串联形态，而新兴语言复制了自然语言融合语法属性的趋势。</li>
</ul>

<h3>Title: Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sree Bhattacharyya, Lucas Craig, Tharun Dilliraj, Jia Li, James Z. Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05880">https://arxiv.org/abs/2508.05880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05880">https://arxiv.org/pdf/2508.05880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05880]] Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models(https://arxiv.org/abs/2508.05880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Affective Computing has been established as a crucial field of inquiry to advance the holistic development of Artificial Intelligence (AI) systems. Foundation models -- especially Large Language Models (LLMs) -- have been evaluated, trained, or instruction-tuned in several past works, to become better predictors or generators of emotion. Most of these studies, however, approach emotion-related tasks in a supervised manner, assessing or training the capabilities of LLMs using discrete emotion labels associated with stimuli (e.g., text, images, video, audio). Evaluation studies, in particular, have often been limited to standard and superficial emotion-related tasks, such as the recognition of evoked or expressed emotions. In this paper, we move beyond surface-level emotion tasks to investigate how LLMs reason about emotions through cognitive dimensions. Drawing from cognitive appraisal theory, we examine whether LLMs produce coherent and plausible cognitive reasoning when reasoning about emotionally charged stimuli. We introduce a large-scale benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal cognitive structures implicitly used by LLMs for emotional reasoning. Through a plethora of evaluation experiments and analysis, we seek to answer: (a) Are models more likely to implicitly rely on specific cognitive appraisal dimensions?, (b) What cognitive dimensions are important for characterizing specific emotions?, and, (c) Can the internal representations of different emotion categories in LLMs be interpreted through cognitive appraisal dimensions? Our results and analyses reveal diverse reasoning patterns across different LLMs. Our benchmark and code will be made publicly available.</li>
<li><strong>摘要：</strong>情感计算已被建立为一个关键的探究领域，以推动人工智能（AI）系统的整体发展。基金会模型（尤其是大型语言模型（LLM））已在过去的几项作品中进行了评估，培训或指导，以成为更好的预测指标或情感的发电机。但是，大多数研究都以监督的方式处理与情绪相关的任务，使用与刺激相关的离散情绪标签评估或训练LLM的功能（例如，文本，图像，视频，音频）。尤其是评估研究通常仅限于标准和表面情绪相关的任务，例如对诱发或表达情绪的认识。在本文中，我们超越了表面层面的情感任务，以调查LLM如何通过认知维度推理情绪。从认知评估理论中，我们检查了LLM在推理情感带电刺激时是否产生连贯和合理的认知推理。我们引入了关于情绪的认知推理的大规模基准 - 核心 - 用于评估LLMS隐式用于情感推理的内部认知结构。通过大量的评估实验和分析，我们试图回答：（a）模型是否更有可能隐含地依靠特定的认知评估维度？我们的结果和分析揭示了不同LLM的各种推理模式。我们的基准和代码将公开可用。</li>
</ul>

<h3>Title: Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhanghao Hu, Qinglin Zhu, Siya Qi, Yulan He, Hanqi Yan, Lin Gui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05909">https://arxiv.org/abs/2508.05909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05909">https://arxiv.org/pdf/2508.05909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05909]] Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation(https://arxiv.org/abs/2508.05909)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open source LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通过检索器阅读器范式通过检索型发电（RAG）表现出了改善的发电性能，该范式将其补充具有外部检索知识的模型输入。但是，先前的工作通常会通过整体评估RAG，共同评估猎犬和读者，因此很难隔离检索的真正贡献，尤其是考虑到LLMS用作读者的迅速敏感性。我们介绍了光谱投影评分（SPS），这是一种轻巧，无监督的度量标准，允许读者通过比较摘要中产生的令牌形成的面积，以及在阅读器中的子空间的主要方向来评估检索到的摘要与其隐藏表示形式的语义对齐。在SPS的基础上，我们提出了XCompress，这是一个推理时间控制器框架，该框架动态示例，排名和压缩检索摘要候选者。对五个QA基准测试和四个开源LLM的广泛实验表明，SPS不仅可以提高一系列任务的性能，而且还提供了关于检索与发电之间相互作用的原则观点。</li>
</ul>

<h3>Title: Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale</h3>
<ul>
<li><strong>Authors: </strong>Rafal Kocielnik, Min Kim, Penphob (Andrea)Boonyarungsrit, Fereshteh Soltani, Deshawn Sambrano, Animashree Anandkumar, R. Michael Alvarez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05938">https://arxiv.org/abs/2508.05938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05938">https://arxiv.org/pdf/2508.05938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05938]] Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale(https://arxiv.org/abs/2508.05938)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Detecting prosociality in text--communication intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best LLM-based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\sim$35\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.</li>
<li><strong>摘要：</strong>检测文本中的亲社会性 - 旨在确认，支持或改善他人的行为的通讯是对信任和安全系统的新颖而又越来越重要的挑战。与有毒内容检测不同，亲社会性缺乏完善的定义和标记的数据，需要新的注释和部署方法。我们提出了一条实用的三阶段管道，可实现可扩展的高精度亲社会内容分类，同时最大程度地减少人类的标签工作和推理成本。首先，我们使用一组人类标记的例子来确定最佳的基于LLM的标签策略。然后，我们引入了人类精炼循环，注释者在其中回顾了GPT-4和人类之间的高分离案例，以迭代澄清和扩展任务定义 - 对于诸如亲社会之类的新兴注释任务的关键步骤。此过程可提高标签质量和定义对齐方式。最后，我们使用GPT-4合成10K高质量的标签，并训练一个两阶段的推理系统：轻质分类器处理高信心预测，而仅$ \ sim $ 35 \％的模棱两可的实例升级为GPT-4O。该体系结构将推理成本降低$ \ sim $ 70％，同时获得高精度（$ \ sim $ 0.90）。我们的管道展示了针对性的人类相互作用，仔细的任务制定和部署感知的建筑设计如何解锁可扩展的解决方案，以解决新颖的负责人AI任务。</li>
</ul>

<h3>Title: Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring</h3>
<ul>
<li><strong>Authors: </strong>Chunyun Zhang, Hongyan Zhao, Chaoran Cui, Qilong Song, Zhiqing Lu, Shuai Gong, Kailin Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.05987">https://arxiv.org/abs/2508.05987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.05987">https://arxiv.org/pdf/2508.05987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.05987]] Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring(https://arxiv.org/abs/2508.05987)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Cross-topic automated essay scoring (AES) aims to develop a transferable model capable of effectively evaluating essays on a target topic. A significant challenge in this domain arises from the inherent discrepancies between topics. While existing methods predominantly focus on extracting topic-shared features through distribution alignment of source and target topics, they often neglect topic-specific features, limiting their ability to assess critical traits such as topic adherence. To address this limitation, we propose an Adversarial TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns topic-shared and topic-specific features to improve cross-topic AES. ATOP achieves this by optimizing a learnable topic-aware prompt--comprising both shared and specific components--to elicit relevant knowledge from pre-trained language models (PLMs). To enhance the robustness of topic-shared prompt learning and mitigate feature scale sensitivity introduced by topic alignment, we incorporate adversarial training within a unified regression and classification framework. In addition, we employ a neighbor-based classifier to model the local structure of essay representations and generate pseudo-labels for target-topic essays. These pseudo-labels are then used to guide the supervised learning of topic-specific prompts tailored to the target topic. Extensive experiments on the publicly available ASAP++ dataset demonstrate that ATOP significantly outperforms existing state-of-the-art methods in both holistic and multi-trait essay scoring. The implementation of our method is publicly available at: this https URL.</li>
<li><strong>摘要：</strong>跨主题自动论文评分（AES）旨在开发一个可转让的模型，能够有效地评估目标主题的论文。该领域的一个重大挑战源于主题之间的固有差异。尽管现有方法主要集中于通过分布源和目标主题来提取主题共享的特征，但它们经常忽略特定于主题的特征，从而限制了评估诸如主题依从性之类的关键特征的能力。为了解决这一限制，我们提出了一种对抗性主题感知的及时调整（ATOP），这是一种新颖的方法，可以共同学习主题共享和特定于主题的特征，以改善跨主题AES。 AT上的AT通过优化可学习的主题感知及时的及时 - 共享和特定组成部分来实现这一目标 - 从预训练的语言模型（PLM）中获取相关知识。为了增强主题共享的及时学习的鲁棒性，并减轻主题对齐方式引入的特征量表灵敏度，我们将对抗性培训纳入统一的回归和分类框架中。此外，我们采用基于邻居的分类器来对论文表示的局部结构进行建模，并为目标主题论文生成伪标记。然后，这些伪标签用于指导针对目标主题量身定制的特定于主题的提示的监督学习。对公开ASAP ++数据集进行的广泛实验表明，在整体和多特征论文评分中，AT上的最新方法都大大优于现有的最新方法。我们的方法的实现可公开获得：此HTTPS URL。</li>
</ul>

<h3>Title: Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future</h3>
<ul>
<li><strong>Authors: </strong>Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, Shikun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06026">https://arxiv.org/abs/2508.06026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06026">https://arxiv.org/pdf/2508.06026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06026]] Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future(https://arxiv.org/abs/2508.06026)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.</li>
<li><strong>摘要：</strong>自我奖励的语言模型提出了一个体系结构，其中大语言模型（LLMS）都会通过LLM-AS-A-A-Gudge提示产生响应并评估其自己的输出，从而通过迭代直接偏好优化（DPO）动态提高其生成能力（DPO）。但是，我们的分析揭示了现有的自我奖励范式的关键局限性：所选响应的同步改进和拒绝的响应逐渐缩小了对比样本之间的代表性差异，从而破坏了有效的偏好学习。我们提出\ textbf {时间自我奖励语言模型}，以战略性地协调过去，现在和未来的模型世代来维持学习信号。我们的双相框架介绍了：（1）\ textIt {锚定拒绝}  - 使用过去的初始模型的输出来固定拒绝的响应，（2）\ textit {未来引导选择}  - 使用下一代模型预测动态策划选择的样本。与使用相同的计算资源相比，使用我们的方法训练时，对三个模型家族（Llama，Qwen，Mistral）和不同模型尺寸（Llama3b/8b/70b）进行了广泛的实验表现出显着改进。例如，Llama3.1-8b使用我们的方法在Alpacaeval 2.0上达到29.44的胜利率，以9.75的比例优于自我奖励基线（19.69）。值得注意的是，我们的方法还表明了在数学推理（GSM8K），基于知识的QA（ARC，ElterFullQA）和代码生成（HumaneVal）任务的跨越分布概括，即使我们没有专门收集此类培训数据。</li>
</ul>

<h3>Title: Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Kartik Sharma, Yiqiao Jin, Rakshit Trivedi, Srijan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06030">https://arxiv.org/abs/2508.06030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06030">https://arxiv.org/pdf/2508.06030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06030]] Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings(https://arxiv.org/abs/2508.06030)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) acquire knowledge across diverse domains such as science, history, and geography encountered during generative pre-training. However, due to their stochasticity, it is difficult to predict what LLMs have acquired. Prior work has developed different ways to probe this knowledge by investigating the hidden representations, crafting specific task prompts, curating representative samples, and estimating their uncertainty. However, these methods require making forward passes through the underlying model to probe the LLM's knowledge about a specific fact, making them computationally expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or $\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate $\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models that effectively encode factual knowledge as text or graphs as proxies for LLMs. First, we identify a training set of facts known by LLMs through various probing strategies and then adapt embedding models to predict the LLM outputs with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find that sentence embedding models are more suitable than graph embeddings to predict LLM knowledge, shedding light on the underlying representation of the factual landscape. Thus, we believe that knowledge-adapted embeddings can be used to identify knowledge gaps in LLMs at scale and can provide deeper insights into LLMs' internal inductive bias. The code and data are made available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）获得了跨生成预训练期间遇到的科学，历史和地理等不同领域的知识。但是，由于它们的随机性，很难预测LLM的获取。先前的工作通过研究隐藏的表示，制定特定的任务提示，策划代表性样本并估算其不确定性，开发了不同的方法来探究这些知识。但是，这些方法需要使前进通过基础模型，以探究LLM关于特定事实的知识，从而使它们在计算上昂贵且耗时。要弥合这个差距，我们建议$ \ textbf {peek} $或$ \ textbf {p} $ roxy $ \ textbf {e} $ mbeddings to $ \ textbf {e} $ \ textbf {e}对于LLM。首先，我们通过各种探测策略来确定LLM已知的事实集，然后调整嵌入模型以用线性解码器层预测LLM输出。 $ 3 $ Wikipedia衍生的数据集，$ 4 $ LLMS和7 $ $嵌入模型的全面评估显示，嵌入式可以预测固定设置的LLM知识，其精度高达90％。此外，我们发现嵌入模型比图形嵌入更合适，以预测LLM知识，从而阐明了事实景观的基础表示。因此，我们认为，通过知识适应的嵌入可以用来识别LLMS中的知识差距，并可以提供对LLMS内部归纳偏见的更深入的见解。代码和数据可在此HTTPS URL上提供。</li>
</ul>

<h3>Title: EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinda Wang, Zhengxu Hou, Yangshijie Zhang, Bingren Yan, Zhibo Yang, Xingsheng Zhang, Luxi Xing, Qiang Zhou, Chen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06046">https://arxiv.org/abs/2508.06046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06046">https://arxiv.org/pdf/2508.06046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06046]] EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation(https://arxiv.org/abs/2508.06046)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Although the effectiveness of Large Language Models (LLMs) as judges (LLM-as-a-judge) has been validated, their performance remains limited in open-ended tasks, particularly in story evaluation. Accurate story evaluation is crucial not only for assisting human quality judgment but also for providing key signals to guide story generation. However, existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation. To address this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework. Grounded in pairwise comparison, the framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task. Experimental results demonstrate that our framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories, thereby fully validating the superiority of our self-evolving approach.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLMS）作为法官（LLM-AS-A-a-gudge）的有效性已得到验证，但他们的绩效在开放式任务中仍然有限，尤其是在故事评估中。准确的故事评估不仅对于协助人类质量判断至关重要，而且对于提供指导故事产生的关键信号至关重要。但是，现有的方法面临困境：封闭源模型的及时工程，其适应性差，而开源模型的微调方法缺乏对故事评估必不可少的严格推理能力。为了解决这个问题，我们提出了自我发展的成对推理（EVOLVR）框架。基于成对比较，该框架首先通过多人策略进行了自我合成，使与得分一致的思想链（COT）数据列出了。为了确保数据质量，这些原始的婴儿床经历了自我过滤过程，并利用多代理来确保其逻辑严格和鲁棒性。最后，将经过精制数据培训的评估人员作为指导故事生成任务的奖励模型进行了部署。实验结果表明，我们的框架在包括故事的人，Hanna和OpenMeva在内的三个评估基准上实现了最先进的表现（SOTA）。此外，当用作奖励模型时，它大大提高了产生的故事的质量，从而充分验证了我们自我发展的方法的优势。</li>
</ul>

<h3>Title: ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Morris Alper, Moran Yanuka, Raja Giryes, Gašper Beguš</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06094">https://arxiv.org/abs/2508.06094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06094">https://arxiv.org/pdf/2508.06094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06094]] ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline(https://arxiv.org/abs/2508.06094)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international communication. Meanwhile, large-scale foundation models have revolutionized creative generation in text, images, and beyond. In this work, we leverage modern LLMs as computational creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages LLMs' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description. We evaluate ConlangCrafter on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs without human linguistic expertise.</li>
<li><strong>摘要：</strong>Esperanto和Quenya等构建的语言（Conlangs）在艺术，哲学和国际交流中扮演着各种角色。同时，大规模的基础模型彻底改变了文本，图像及以后的创意产生。在这项工作中，我们利用现代LLM作为端到端Conlang创建的计算创造力有助于。我们介绍了Conlangcrafter，这是一种多跳管道，将语言设计分解为模块化阶段 - 语音，形态学，语法，词典，词典和翻译。在每个阶段，我们的方法都利用LLMS的元语言推理能力，注入随机性，以鼓励多样性并利用自我反馈，以鼓励在新兴语言描述中保持一致性。我们评估了测量相干性和类型学多样性的指标，以评估其产生相干和多样化的无人语言专业知识的能力。</li>
</ul>

<h3>Title: Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Basem, Islam Oshallah, Ali Hamdi, Ammar Mohammed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06103">https://arxiv.org/abs/2508.06103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06103">https://arxiv.org/pdf/2508.06103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06103]] Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs(https://arxiv.org/abs/2508.06103)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>This paper presents two effective approaches for Extractive Question Answering (QA) on the Quran. It addresses challenges related to complex language, unique terminology, and deep meaning in the text. The second uses few-shot prompting with instruction-tuned large language models such as Gemini and DeepSeek. A specialized Arabic prompt framework is developed for span extraction. A strong post-processing system integrates subword alignment, overlap suppression, and semantic filtering. This improves precision and reduces hallucinations. Evaluations show that large language models with Arabic instructions outperform traditional fine-tuned models. The best configuration achieves a pAP10 score of 0.637. The results confirm that prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks.</li>
<li><strong>摘要：</strong>本文介绍了古兰经上提取问题回答（QA）的两种有效方法。它解决了与复杂语言，独特的术语和文本中的深层含义有关的挑战。第二种使用了很少的弹药提示，其中包括双子座和DeepSeek等指令调整的大语言模型。为跨度提取开发了一个专门的阿拉伯及时框架。强大的后处理系统集成了子字对准，重叠抑制和语义过滤。这提高了精度并减少了幻觉。评估表明，带有阿拉伯指令的大语言模型优于传统的微调模型。最佳配置的PAP10分数为0.637。结果证实，基于及时的指令调整对于低资源，语义上丰富的QA任务有效。</li>
</ul>

<h3>Title: You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures</h3>
<ul>
<li><strong>Authors: </strong>Shengyuan Chen, Chuang Zhou, Zheng Yuan, Qinggang Zhang, Zeyang Cui, Hao Chen, Yilin Xiao, Jiannong Cao, Xiao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06105">https://arxiv.org/abs/2508.06105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06105">https://arxiv.org/pdf/2508.06105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06105]] You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures(https://arxiv.org/abs/2508.06105)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a \textbf{\underline{Logic}}-aware \textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented \textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通常会遭受幻觉的困扰，在处理问题和知觉范围内处理问题时会产生事实不正确的陈述。检索增强的生成（RAG）通过从知识库中检索与LLM推理的相关环境来解决这一问题。最近的进步利用预构建的图表来捕获分布式文档之间的关系连接，在复杂的任务中显示出显着的性能。但是，现有的基于图的抹布（GraphRag）方法依赖一个代价高昂的过程来将语料库转换为图形，引入了压倒性的令牌成本并更新延迟。此外，实际查询的类型和复杂性各不相同，需要不同的逻辑结构才能准确推理。预构建的图可能与这些所需的结构不符，从而导致知识检索无效。为此，我们提出了一个\ textbf {\下划线{logic}}  -  Aware \ textbf {\ useverline {r}} etrieval- \ textbf {\ textbf {\ suesperline {a}}}推理时的结构指导自适应检索，而无需任何预构建图。 Logicrag首先将输入查询分解为一组子问题，并构建有向的无环图（DAG），以建模它们之间的逻辑依赖性。为了支持相干的多步推理，逻辑随后使用拓扑排序线性化，以便可以以逻辑一致的顺序解决子问题。此外，LogicRag应用图形修剪以减少冗余检索，并使用上下文修剪来过滤无关紧要的环境，从而大大降低了整体令牌成本。广泛的实验表明，与最先进的基准相比，Logicrag可以达到卓越的性能和效率。</li>
</ul>

<h3>Title: AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sayantan Adak, Pratyush Chatterjee, Somnath Banerjee, Rima Hazra, Somak Aditya, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06124">https://arxiv.org/abs/2508.06124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06124">https://arxiv.org/pdf/2508.06124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06124]] AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models(https://arxiv.org/abs/2508.06124)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Present day LLMs face the challenge of managing affordance-based safety risks-situations where outputs inadvertently facilitate harmful actions due to overlooked logical implications. Traditional safety solutions, such as scalar outcome-based reward models, parameter tuning, or heuristic decoding strategies, lack the granularity and proactive nature needed to reliably detect and intervene during subtle yet crucial reasoning steps. Addressing this fundamental gap, we introduce AURA, an innovative, multi-layered framework centered around Process Reward Models (PRMs), providing comprehensive, step level evaluations across logical coherence and safety-awareness. Our framework seamlessly combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to dynamically and proactively guide models toward safer reasoning trajectories. Empirical evidence clearly demonstrates that this approach significantly surpasses existing methods, significantly improving the logical integrity and affordance-sensitive safety of model outputs. This research represents a pivotal step toward safer, more responsible, and contextually aware AI, setting a new benchmark for alignment-sensitive applications.</li>
<li><strong>摘要：</strong>当今的LLM面临着管理基于负担的安全风险宣传的挑战，由于逻辑含义被忽视，产出无意中促进了有害行动。传统的安全解决方案，例如基于标量结果的奖励模型，参数调整或启发式解码策略，缺乏在微妙而至关重要的推理步骤中可靠地检测和干预所需的粒度和主动性。在解决这一基本差距时，我们介绍了Aura，这是一个以过程奖励模型（PRMS）为中心的创新，多层框架，提供了跨越的逻辑相干性和安全意识的全面，步骤级别的评估。我们的框架无缝地结合了内省的自我评价，细粒度的PRM评估，并自适应安全性地解码，以动态和主动地指导模型，以实现更安全的推理轨迹。经验证据清楚地表明，这种方法显着超过了现有方法，从而显着提高了模型输出的逻辑完整性和对负担敏感的安全性。这项研究代表了朝着更安全，更负责任和上下文意识的AI迈出的关键步骤，为对齐敏感的应用树立了新的基准测试。</li>
</ul>

<h3>Title: Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lingyuan Liu, Mengxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06135">https://arxiv.org/abs/2508.06135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06135">https://arxiv.org/pdf/2508.06135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06135]] Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models(https://arxiv.org/abs/2508.06135)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Knowledge Distillation (KD) is a fundamental technique for compressing large language models (LLMs) into compact, efficient student models. However, existing white-box KD methods mainly focus on balancing ground truth and student-generated responses while overlooking two critical factors: training data quality and student-model compatibility. To address these limitations, we propose Selective Reflection Distillation (SRD), a novel data curation framework that leverages reflections from student models to systematically refine training data. SRD dynamically evaluates and selects prompt-response pairs by comparing ground truth data with student model outputs, selectively curating high-quality, student-compatible training instances through automated ranking based on difficulty. Furthermore, after selecting the training data, a curriculum scheduling strategy is employed to incrementally introduce these curated subsets into the distillation process at fixed intervals. As a plug-and-play enhancement, SRD consistently improves distillation outcomes across diverse white-box KD approaches and model architectures, as well as decreases computational cost significantly during KD training. Experiments on a range of language model benchmarks demonstrate SRD's consistent improvements in distilled model performance, as well as a reduction in training runtime by up to 39%, under diverse KD methods and model families. Notably, SRD operates as a plug-and-play module, enhancing sample efficiency without modifying underlying KD algorithms. Our findings highlight that data quality and compatibility are pivotal to effective and efficient distillation of LLMs, and SRD provides a principled framework to achieve both. This work advances the understanding of data-centric factors in KD and offers practical insights for enhancing the capability and efficiency of compressed LLMs.</li>
<li><strong>摘要：</strong>知识蒸馏（KD）是将大型语言模型（LLM）压缩成紧凑，有效的学生模型的基本技术。但是，现有的白盒KD方法主要集中于平衡地面真理和学生产生的响应，同时忽略了两个关键因素：培训数据质量和学生模型兼容性。为了解决这些局限性，我们提出了选择性反射蒸馏（SRD），这是一个新型的数据策划框架，利用从学生模型到系统地完善培训数据的反思。 SRD通过将地面真实数据与学生模型的输出进行比较，通过基于难度的自动排名选择性地策划高质量的，与学生兼容的培训实例，通过将地面真相数据与学生模型的输出进行比较，可以动态评估并选择及时响应对。此外，选择培训数据后，采用课程调度策略将这些策划的子集逐步引入蒸馏过程中。作为插件的增强，SRD始终提高各种白盒KD方法和模型架构的蒸馏结果，并在KD培训期间大大降低了计算成本。在一系列语言模型基准上进行的实验表明，在不同的KD方法和模型家族下，SRD在蒸馏模型性能方面的持续改进以及培训运行时的一致性最高为39％。值得注意的是，SRD作为插件模块运行，在不修改KD算法的情况下提高样品效率。我们的发现强调，数据质量和兼容性对LLM的有效和有效蒸馏至关重要，而SRD提供了一个实现这两者的原则性框架。这项工作提高了对KD中以数据为中心因素的理解，并为提高压缩LLM的能力和效率提供了实用的见解。</li>
</ul>

<h3>Title: Scaling Personality Control in LLMs with Big Five Scaler Prompts</h3>
<ul>
<li><strong>Authors: </strong>Gunhee Cho, Yun-Gyung Cheong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06149">https://arxiv.org/abs/2508.06149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06149">https://arxiv.org/pdf/2508.06149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06149]] Scaling Personality Control in LLMs with Big Five Scaler Prompts(https://arxiv.org/abs/2508.06149)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>We present Big5-Scaler, a prompt-based framework for conditioning large language models (LLMs) with controllable Big Five personality traits. By embedding numeric trait values into natural language prompts, our method enables fine-grained personality control without additional training. We evaluate Big5-Scaler across trait expression, dialogue generation, and human trait imitation tasks. Results show that it induces consistent and distinguishable personality traits across models, with performance varying by prompt type and scale. Our analysis highlights the effectiveness of concise prompts and lower trait intensities, providing a efficient approach for building personality-aware dialogue agents.</li>
<li><strong>摘要：</strong>我们提出了Big5-Scaler，这是一个基于及时可控的五个个性特征的大型语言模型（LLM）的及时框架。通过将数字特征值嵌入自然语言提示中，我们的方法可以在没有额外培训的情况下进行细粒度的性格控制。我们在特质表达，对话产生和人类特质模仿任务中评估了Big5-Scaler。结果表明，它在模型之间引起了一致和可区分的人格特质，并且性能随迅速的类型和规模而变化。我们的分析强调了简洁提示和较低特征强度的有效性，为建立人格意识的对话代理提供了有效的方法。</li>
</ul>

<h3>Title: Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach</h3>
<ul>
<li><strong>Authors: </strong>Renhan Zhang, Lian Lian, Zhen Qi, Guiran Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06155">https://arxiv.org/abs/2508.06155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06155">https://arxiv.org/pdf/2508.06155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06155]] Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach(https://arxiv.org/abs/2508.06155)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the issue of implicit stereotypes that may arise during the generation process of large language models. It proposes an interpretable bias detection method aimed at identifying hidden social biases in model outputs, especially those semantic tendencies that are not easily captured through explicit linguistic features. The method combines nested semantic representation with a contextual contrast mechanism. It extracts latent bias features from the vector space structure of model outputs. Using attention weight perturbation, it analyzes the model's sensitivity to specific social attribute terms, thereby revealing the semantic pathways through which bias is formed. To validate the effectiveness of the method, this study uses the StereoSet dataset, which covers multiple stereotype dimensions including gender, profession, religion, and race. The evaluation focuses on several key metrics, such as bias detection accuracy, semantic consistency, and contextual sensitivity. Experimental results show that the proposed method achieves strong detection performance across various dimensions. It can accurately identify bias differences between semantically similar texts while maintaining high semantic alignment and output stability. The method also demonstrates high interpretability in its structural design. It helps uncover the internal bias association mechanisms within language models. This provides a more transparent and reliable technical foundation for bias detection. The approach is suitable for real-world applications where high trustworthiness of generated content is required.</li>
<li><strong>摘要：</strong>本文解决了在大型语言模型的生成过程中可能出现的隐式刻板印象的问题。它提出了一种可解释的偏见检测方法，旨在识别模型输出中隐藏的社会偏见，尤其是那些不容易通过明确的语言特征捕获的语义趋势。该方法将嵌套的语义表示与上下文对比机制结合在一起。它从模型输出的向量空间结构中提取潜在偏置特征。它使用注意力重量扰动，分析了模型对特定社会属性术语的敏感性，从而揭示了形成偏见的语义途径。为了验证该方法的有效性，本研究使用了Stereoset数据集，该数据集涵盖了多种刻板印象维度，包括性别，职业，宗教和种族。评估的重点是几个关键指标，例如偏差检测准确性，语义一致性和上下文敏感性。实验结果表明，所提出的方法在各个维度上实现了强大的检测性能。它可以准确地确定语义上相似文本之间的偏差差异，同时保持高语义对准和输出稳定性。该方法还显示了其结构设计中的高解释性。它有助于发现语言模型中的内部偏见关联机制。这为偏置检测提供了更透明和可靠的技术基础。该方法适用于需要高度信任生成内容的现实应用程序。</li>
</ul>

<h3>Title: UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06165">https://arxiv.org/abs/2508.06165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06165">https://arxiv.org/pdf/2508.06165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06165]] UR$^2$: Unify RAG and Reasoning through Reinforcement Learning(https://arxiv.org/abs/2508.06165)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通过两个互补范式显示出了显着的功能：检索功能增强的一代（RAG），可增强知识接地，并从可验证的奖励（RLVR）中进行增强学习，从而优化了复杂的推理能力。但是，这两个功能通常是孤立发展的，并且在范围典型上限制了范围内的范围QA，并具有固定的检索设置和特定于任务的假设。缺乏集成限制了概括，并限制了RAG-RL方法对更广泛的域的适用性。为了弥合这一差距，我们提出了UR2（统一的抹布和推理），这是一个通用框架，通过增强学习来统一检索和推理。 UR2引入了两个关键贡献：一种困难的课程培训，该课程培训仅针对挑战性问题进行选择性地提出检索，以及将特定领域的离线语料库与LLM生成的摘要结合在一起的混合知识访问策略。这些组件旨在实现检索和推理之间的动态协调，从而改善各种任务范围的适应性。开放域QA，MMLU-PRO，医疗和数学推理任务进行的实验表明，UR2（建立在QWEN2.5-3/7B和LLAMA-3.1-8B上）明显超过现有的抹布和RL方法，从而实现了与GPT-4O-MINI和GPT-MINI和GPT-4.1-MINI的可相当性能。我们已经在此HTTPS URL上发布了所有代码，模型和数据。</li>
</ul>

<h3>Title: Pragmatics beyond humans: meaning, communication, and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Vít Gvoždiak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06167">https://arxiv.org/abs/2508.06167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06167">https://arxiv.org/pdf/2508.06167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06167]] Pragmatics beyond humans: meaning, communication, and LLMs(https://arxiv.org/abs/2508.06167)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The paper reconceptualizes pragmatics not as a subordinate, third dimension of meaning, but as a dynamic interface through which language operates as a socially embedded tool for action. With the emergence of large language models (LLMs) in communicative contexts, this understanding needs to be further refined and methodologically reconsidered. The first section challenges the traditional semiotic trichotomy, arguing that connectionist LLM architectures destabilize established hierarchies of meaning, and proposes the Human-Machine Communication (HMC) framework as a more suitable alternative. The second section examines the tension between human-centred pragmatic theories and the machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics continue to dominate, it relies on human-specific assumptions ill-suited to predictive systems like LLMs. Probabilistic pragmatics, particularly the Rational Speech Act framework, offers a more compatible teleology by focusing on optimization rather than truth-evaluation. The third section addresses the issue of substitutionalism in three forms - generalizing, linguistic, and communicative - highlighting the anthropomorphic biases that distort LLM evaluation and obscure the role of human communicative subjects. Finally, the paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding, emphasizing how users are compelled to co-construct pragmatic conditions both for the model and themselves. These arguments suggest that pragmatic theory may need to be adjusted or expanded to better account for communication involving generative AI.</li>
<li><strong>摘要：</strong>本文重新概念化了语用品，而不是作为含义的下属，第三维，而是语言作为一种社会嵌入式工具的动态界面。随着大型语言模型（LLM）在交流环境中的出现，需要进一步完善并在方法论上重新考虑这种理解。第一部分挑战了传统的符号学三分法，认为Connectionist LLM构建了稳定的意义层次结构，并提出了人机通信（HMC）框架作为更合适的替代方案。第二部分研究了以人为中心的务实理论与LLM的以机器为中心的性质之间的张力。尽管传统的，以格里尼亚风格的实用主义者继续占主导地位，但它依赖于人类特定的假设不适合LLM等预测系统。概率的语用学，尤其是理性的语音行为框架，通过专注于优化而不是真理评估来提供更兼容的目的论。第三部分介绍了三种形式的替代主义问题 - 概括，语言和交流 - 突出了拟人化评估并掩盖人类交流主题的作用的拟人型偏见。最后，本文介绍了上下文挫败感的概念，以描述增加上下文输入的悖论，并在上下文理解中崩溃，强调如何将用户迫使用户为模型及其本身共同构建实用条件。这些论点表明，可能需要调整或扩展实用理论，以更好地解释涉及生成AI的沟通。</li>
</ul>

<h3>Title: Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime</h3>
<ul>
<li><strong>Authors: </strong>Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06178">https://arxiv.org/abs/2508.06178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06178">https://arxiv.org/pdf/2508.06178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06178]] Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime(https://arxiv.org/abs/2508.06178)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often require vast amounts of text to effectively acquire new knowledge. While continuing pre-training on large corpora or employing retrieval-augmented generation (RAG) has proven successful, updating an LLM with only a few thousand or million tokens remains challenging. In this work, we investigate the task of injecting small, unstructured information into LLMs and its relation to the catastrophic forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap with the model's pre-training data -- to evaluate the knowledge acquisition by probing the model with question-answer pairs related the learned information. Starting from a continued pre-training baseline, we explored different augmentation algorithms to generate synthetic data to improve the knowledge acquisition capabilities. Our experiments show that simply continuing pre-training on limited data yields modest improvements, whereas exposing the model to diverse textual variations significantly improves the learning of new facts -- particularly with methods that induce greater variability through diverse prompting. Furthermore, we shed light on the forgetting phenomenon in small-data regimes, illustrating the delicate balance between learning new content and retaining existing capabilities. We also confirm the sensitivity of RAG-based approaches for knowledge injection, which often lead to greater degradation on control datasets compared to parametric methods. Finally, we demonstrate that models can generate effective synthetic training data themselves, suggesting a pathway toward self-improving model updates. All code and generated data used in our experiments are publicly available, providing a resource for studying efficient knowledge injection in LLMs with limited data at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通常需要大量文本才能有效获取新知识。事实证明，虽然继续对大型语料库进行预培训或使用检索功能的一代（RAG），但只有几千或数百万个令牌更新LLM仍然具有挑战性。在这项工作中，我们调查了将小而非结构化的信息注入LLM的任务及其与灾难性遗忘现象的关系。我们使用最近新闻的数据集 - 确保与模型的预培训数据没有重叠 - 通过通过问答对探索模型的模型与学习信息相关的信息来评估知识获取。从持续的预训练基线开始，我们探索了不同的增强算法以生成合成数据以提高知识获取能力。我们的实验表明，仅继续对有限数据进行预训练会产生适度的改进，而将模型暴露于各种文本变化会大大改善新事实的学习，尤其是通过通过各种提示引起更大可变性的方法。此外，我们阐明了小型数据制度中的遗忘现象，这说明了学习新内容与保留现有能力之间的微妙平衡。我们还确认了基于抹布的方法对知识注入的敏感性，与参数方法相比，对控制数据集的降解通常会导致更大的降解。最后，我们证明模型可以自己生成有效的合成训练数据，这表明了通往自我改善模型更新的途径。我们实验中使用的所有代码和生成的数据均可公开使用，为在此HTTPS URL上使用有限的数据提供了用于研究LLM中有效知识注入的资源。</li>
</ul>

<h3>Title: DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration</h3>
<ul>
<li><strong>Authors: </strong>Ali Sarabadani, Maryam Abdollahi Shamami, Hamidreza Sadeghsalehi, Borhan Asadi, Saba Hesaraki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06186">https://arxiv.org/abs/2508.06186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06186">https://arxiv.org/pdf/2508.06186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06186]] DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration(https://arxiv.org/abs/2508.06186)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have grown exponentially since the release of ChatGPT. These models have gained attention due to their robust performance on various tasks, including language processing tasks. These models achieve understanding and comprehension of tasks by training billions of parameters. The development of these models is a transformative force in enhancing natural language understanding and has taken a significant step towards artificial general intelligence (AGI). In this study, we aim to present the DKG-LLM framework. The DKG-LLM framework introduces a groundbreaking approach to medical diagnosis and personalized treatment recommendations by integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data (including clinical reports and PubMed articles) and patient records dynamically generate a knowledge graph consisting of 15,964 nodes in 13 distinct types (e.g., diseases, symptoms, treatments, patient profiles) and 127,392 edges in 26 relationship types (e.g., causal, therapeutic, association). ASFA utilizes advanced probabilistic models, Bayesian inference, and graph optimization to extract semantic information, dynamically updating the graph with approximately 150 new nodes and edges in each data category while maintaining scalability with up to 987,654 edges. Real-world datasets, including MIMIC-III and PubMed, were utilized to evaluate the proposed architecture. The evaluation results show that DKG-LLM achieves a diagnostic accuracy of 84.19%. The model also has a treatment recommendation accuracy of 89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and transformative tool that handles noisy data and complex multi-symptom diseases, along with feedback-based learning from physician input.</li>
<li><strong>摘要：</strong>自Chatgpt发行以来，大型语言模型（LLM）已成倍增长。这些模型由于其在各种任务（包括语言处理任务）上的出色表现而引起了人们的关注。这些模型通过培训数十亿个参数来了解和理解任务。这些模型的发展是增强自然语言理解的一种变革力量，并迈出了迈向人工通用智能（AGI）的重要一步。在这项研究中，我们旨在介绍DKG-LLM框架。 DKG-LLM框架通过将动态知识图（DKG）与GROK 3大语模型集成到大型语言模型中，引入了一种开创性的医学诊断和个性化治疗建议。 Using the Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data (including clinical reports and PubMed articles) and patient records dynamically generate a knowledge graph consisting of 15,964 nodes in 13 distinct types (e.g., diseases, symptoms, treatments, patient profiles) and 127,392 edges in 26 relationship types (e.g., causal, therapeutic, association). ASFA利用高级概率模型，贝叶斯推理和图形优化来提取语义信息，在每个数据类别中使用大约150个新的节点和边缘动态更新图形，同时以高达987,654个边缘保持可扩展性。现实世界中的数据集，包括模仿III和PubMed，用于评估所提出的体系结构。评估结果表明，DKG-LLM的诊断准确性为84.19％。该模型还具有89.63％的治疗建议精度，语义覆盖率为93.48％。 DKG-LLM是一种可靠的变革性工具，可处理嘈杂的数据和复杂的多症状疾病，以及从医师输入的基于反馈的学习。</li>
</ul>

<h3>Title: Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Lai Jiang, Yuekang Li, Xiaohan Zhang, Youtao Ding, Li Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06194">https://arxiv.org/abs/2508.06194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06194">https://arxiv.org/pdf/2508.06194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06194]] Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation(https://arxiv.org/abs/2508.06194)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Precise jailbreak evaluation is vital for LLM red teaming and jailbreak research. Current approaches employ binary classification ( e.g., string matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no" labels without quantifying harm intensity. Existing multi-dimensional frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness) apply uniform evaluation criteria across scenarios, resulting in scenario-specific mismatches--for instance, "Relative Truthfulness" is irrelevant to "hate speech"--which compromise evaluation precision. To tackle these limitations, we introduce SceneJailEval, with key contributions: (1) A groundbreaking scenario-adaptive multi-dimensional framework for jailbreak evaluation, overcoming the critical "one-size-fits-all" constraint of existing multi-dimensional methods, and featuring strong extensibility to flexibly adapt to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset with diverse jailbreak variants and regional cases, filling the long-standing gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3) SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA), surpassing accuracy limits of existing evaluation methods in heterogeneous scenarios and confirming its advantage.</li>
<li><strong>摘要：</strong>精确的越狱评估对于LLM红色团队和越狱研究至关重要。当前方法采用二进制分类（例如，弦匹配，有毒文本分类器，LLM驱动的方法），只能在不量化伤害强度的情况下产生“是/否”标签。现有的多维框架（例如，违反安全性，相对真实性，信息性）在各场景中应用统一的评估标准，从而导致方案特异性的不匹配 - 例如，“相对真实性”与“仇恨言论”无关，这与“仇恨言论”无关。为了解决这些局限性，我们介绍了现场Jaileval，并提供了主要贡献：（1）越狱评估的突破性场景自适应多维框架，克服了现有的多维方法的关键“一号拟合”限制，并具有强大的扩展性，以灵活地适应自定义的场景，以灵活地适应自定义的场景。 （2）具有各种越狱变体和区域案件的全面的14赛季数据集，填补了高质量，整体基准的长期差距，以实现情景适应性评估。 （3）场景Jaileval取得了最先进的结果，我们的全幕期数据集的F1得分为0.917（比先前的SOTA比6％+6％），JBB上的F1得分为0.995（先前的SOTA+3％），在现有评估方法中超越了其优势和确定其优势的现有评估方法。</li>
</ul>

<h3>Title: EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations</h3>
<ul>
<li><strong>Authors: </strong>Nizi Nazar, Ehsaneddin Asgari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06196">https://arxiv.org/abs/2508.06196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06196">https://arxiv.org/pdf/2508.06196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06196]] EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations(https://arxiv.org/abs/2508.06196)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Emotional Intelligence (EI) is a critical yet underexplored dimension in the development of human-aligned LLMs. To address this gap, we introduce a unified, psychologically grounded four-layer taxonomy of EI tailored for large language models (LLMs), encompassing emotional tracking, cause inference, appraisal, and emotionally appropriate response generation. Building on this framework, we present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to evaluate EI capabilities in open-source LLMs across diverse linguistic and cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma (9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench, identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale, instruction-tuned dialogue dataset, in both English and Arabic. Our statistical analysis reveals that among the five EI layers, only the Appraisal layer shows significant improvement through UC-based fine-tuning. These findings highlight the limitations of existing pretraining and instruction-tuning paradigms in equipping LLMs with deeper emotional reasoning and underscore the need for targeted data and modeling strategies for comprehensive EI alignment.</li>
<li><strong>摘要：</strong>情绪智力（EI）是人类协调LLM的发展中的关键但毫无疑问的维度。为了解决这一差距，我们引入了针对大型语言模型（LLMS）量身定制的统一的，心理扎根的四层分类法，包括情感跟踪，原因推理，评估和情感上适当的响应产生。在此框架的基础上，我们提出了EICAP Bench，这是一种新型MCQ风格的多转弯基准，旨在评估各种语言和文化背景的开源LLM中的EI功能。我们评估了六个LLM：Llama3（8b），Llama3-Instruct，Gemma（9b），Gemma-Instruction，Qwen2.5（7b）和Qwen2.5-Instruct in EmoCap Bench上，将Qwen2.5-Instruction识别为最强的底线。为了评估增强EI功能的潜力，我们使用Ultrachat上的Lora适配器（UC）微调QWEN2.5-base和QWEN2.5教学，这是一种大规模的，指导性调节的对话数据集，英语和阿拉伯语。我们的统计分析表明，在五个EI层中，只有评估层通过基于UC的微调显示出显着改善。这些发现突出了现有的训练和指导调节范式的局限性在为LLMS配备更深入的情感推理，并强调了目标数据和建模策略的需求，以实现全面的EI一致性。</li>
</ul>

<h3>Title: Classification is a RAG problem: A case study on hate speech detection</h3>
<ul>
<li><strong>Authors: </strong>Richard Willats, Josh Pennington, Aravind Mohan, Bertie Vidgen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06204">https://arxiv.org/abs/2508.06204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06204">https://arxiv.org/pdf/2508.06204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06204]] Classification is a RAG problem: A case study on hate speech detection(https://arxiv.org/abs/2508.06204)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Robust content moderation requires classification systems that can quickly adapt to evolving policies without costly retraining. We present classification using Retrieval-Augmented Generation (RAG), which shifts traditional classification tasks from determining the correct category in accordance with pre-trained parameters to evaluating content in relation to contextual knowledge retrieved at inference. In hate speech detection, this transforms the task from "is this hate speech?" to "does this violate the hate speech policy?" Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates this approach and offers three key advantages: (1) robust classification accuracy comparable to leading commercial systems, (2) inherent explainability via retrieved policy segments, and (3) dynamic policy updates without model retraining. Through three experiments, we demonstrate strong baseline performance and show that the system can apply fine-grained policy control by correctly adjusting protection for specific identity groups without requiring retraining or compromising overall performance. These findings establish that RAG can transform classification into a more flexible, transparent, and adaptable process for content moderation and wider classification problems.</li>
<li><strong>摘要：</strong>强大的内容适度需要分类系统，这些系统可以迅速适应不昂贵的重新培训的政策。我们使用检索功能生成（RAG）进行分类，该生成（RAG）将传统的分类任务从根据预训练的参数确定正确的类别，以评估与推理时检索到的上下文知识有关的内容。在仇恨言论检测中，这改变了“这是仇恨言论”的任务。要“这违反了仇恨言论政策？”我们的上下文策略引擎（CPE） - 代理抹布系统 - 演示了这种方法，并提供了三个关键优势：（1）鲁棒分类准确性与领先的商业系统相当，（2）通过检索的策略段的固有解释性，以及（3）无需模型retraning的动态策略更新。通过三个实验，我们证明了强劲的基线性能，并表明该系统可以通过正确调整特定身份组的保护而无需重新训练或损害整体性能，从而应用细粒度的策略控制。这些发现表明，抹布可以将分类转变为更灵活，透明和适应性的过程，以解决内容适中和更广泛的分类问题。</li>
</ul>

<h3>Title: InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?</h3>
<ul>
<li><strong>Authors: </strong>Keummin Ka, Junhyeong Park, Jahyun Jeon, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06220">https://arxiv.org/abs/2508.06220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06220">https://arxiv.org/pdf/2508.06220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06220]] InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?(https://arxiv.org/abs/2508.06220)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Recent advances in Vision-Language Models (VLMs) have demonstrated impressive capabilities in perception and reasoning. However, the ability to perform causal inference -- a core aspect of human cognition -- remains underexplored, particularly in multimodal settings. In this study, we introduce InfoCausalQA, a novel benchmark designed to evaluate causal reasoning grounded in infographics that combine structured visual data with textual context. The benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning based on inferred numerical trends, while Task 2 targets semantic causal reasoning involving five types of causal relations: cause, effect, intervention, counterfactual, and temporal. We manually collected 494 infographic-text pairs from four public sources and used GPT-4o to generate 1,482 high-quality multiple-choice QA pairs. These questions were then carefully revised by humans to ensure they cannot be answered based on surface-level cues alone but instead require genuine visual grounding. Our experimental results reveal that current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning. Their significantly lower performance compared to humans indicates a substantial gap in leveraging infographic-based information for causal inference. Through InfoCausalQA, we highlight the need for advancing the causal reasoning abilities of multimodal AI systems.</li>
<li><strong>摘要：</strong>视觉模型（VLM）的最新进展表现出了令人印象深刻的感知和推理能力。但是，执行因果推断的能力 - 人类认知的核心方面 - 尤其是在多模式环境中。在这项研究中，我们介绍了Infocausalqa，这是一种新型的基准测试，旨在评估基于信息图表的因果推理，这些信息将结构化的视觉数据与文本上下文相结合。基准包括两个任务：任务1基于推断的数值趋势的定量因果推理，而任务2则针对涉及五种因果关系的语义因果推理：原因，效果，干预，反事实和时间。我们手动从四个公共来源手动收集了494对信息图表对，并使用GPT-4O产生了1,482个高质量的多项选择质量质量质量质量质量质量质量质量质量质量；然后，人类仔细修订了这些问题，以确保不能仅凭表面级别的提示来回答它们，而需要真正的视觉接地。我们的实验结果表明，当前的VLM在计算推理中表现出有限的能力，在语义因果推理中的局限性甚至更为明显。与人类相比，它们的性能显着降低，表明利用基于信息图的信息进行因果推断存在巨大差距。通过Infocausalqa，我们强调需要提高多模式AI系统的因果推理能力。</li>
</ul>

<h3>Title: Large Language Model Data Generation for Enhanced Intent Recognition in German Speech</h3>
<ul>
<li><strong>Authors: </strong>Theresa Pekarek Rosin, Burak Can Kaplan, Stefan Wermter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06277">https://arxiv.org/abs/2508.06277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06277">https://arxiv.org/pdf/2508.06277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06277]] Large Language Model Data Generation for Enhanced Intent Recognition in German Speech(https://arxiv.org/abs/2508.06277)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Intent recognition (IR) for speech commands is essential for artificial intelligence (AI) assistant systems; however, most existing approaches are limited to short commands and are predominantly developed for English. This paper addresses these limitations by focusing on IR from speech by elderly German speakers. We propose a novel approach that combines an adapted Whisper ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based language models trained on synthetic text datasets generated by three well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To evaluate the robustness of our approach, we generate synthetic speech with a text-to-speech model and conduct extensive cross-dataset testing. Our results show that synthetic LLM-generated data significantly boosts classification performance and robustness to different speaking styles and unseen vocabulary. Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the much larger ChatGPT (175B) in dataset quality for German intent recognition. Our approach demonstrates that generative AI can effectively bridge data gaps in low-resource domains. We provide detailed documentation of our data generation and training process to ensure transparency and reproducibility.</li>
<li><strong>摘要：</strong>语音命令的意图认可（IR）对于人工智能（AI）助理系统至关重要；但是，大多数现有的方法仅限于简短命令，并且主要用于英语。本文通过重点关注德国老人说话的IR来解决这些局限性。我们提出了一种新颖的方法，该方法结合了一个适应性的耳语ASR模型，该模型对德国老年语音（SVC-DE）进行了微调，并与基于变形金刚的语言模型在合成文本数据集中训练，该模型由三种著名的大型语言模型（LLMS）产生：Leolm，Llama3和Chatgpt。为了评估我们方法的鲁棒性，我们使用文本到语音模型生成综合语音，并进行广泛的跨数据检查测试。我们的结果表明，合成LLM生成的数据显着提高了不同的口语风格和看不见的词汇的分类性能和鲁棒性。值得注意的是，我们发现，特定于域的13B LLM列尔姆（Leolm）超过了数据集质量的较大的chatgpt（175b），以供德国意图识别。我们的方法表明，生成的AI可以有效地弥合低资源域中的数据差距。我们提供有关数据生成和培训过程的详细文档，以确保透明度和可重复性。</li>
</ul>

<h3>Title: Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC</h3>
<ul>
<li><strong>Authors: </strong>Ruichong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06309">https://arxiv.org/abs/2508.06309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06309">https://arxiv.org/pdf/2508.06309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06309]] Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC(https://arxiv.org/abs/2508.06309)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In recent years, concerns about intellectual property (IP) in large language models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct weight copying, upcycling, pruning, or continual pretraining) and claiming authorship without properly attributing to the original license, is a serious misconduct that can lead to significant financial and reputational harm to the original developers. However, existing methods for detecting LLM plagiarism fall short in key areas. They fail to accurately reconstruct weight correspondences, lack the ability to compute statistical significance measures such as $p$-values, and may mistakenly flag models trained on similar data as being related. To address these limitations, we propose Matrix-Driven Instant Review (MDIR), a novel method that leverages matrix analysis and Large Deviation Theory. MDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on weight similarity without requiring full model inference. Experimental results demonstrate that MDIR reliably detects plagiarism even after extensive transformations, such as random permutations and continual pretraining with trillions of tokens. Moreover, all detections can be performed on a single PC within an hour, making MDIR both efficient and accessible.</li>
<li><strong>摘要：</strong>近年来，大语言模型（LLM）中对知识产权（IP）的担忧已经大大增长。窃其他LLM（通过直接重量复制，升级，修剪或持续预处理）并声称作者身份而不适当归因于原始许可，这是一种严重的不当行为，可以对原始开发人员造成重大财务和声誉损害。但是，现有的检测LLM窃的方法在关键领域缺乏。他们无法准确地重建权重对应关系，缺乏计算统计显着性度量（例如$ p $值）的能力，并且可能会错误地对其进行训练的标志模型，以与相关的相关数据进行训练。为了解决这些局限性，我们提出了矩阵驱动的即时审查（MDIR），这是一种利用矩阵分析和大偏差理论的新方法。 MDIR实现了重量关系的准确重建，提供了严格的$ p $值估计，并专注于重量相似性而无需完整的模型推断。实验结果表明，即使经过广泛的转换，MDIR也可靠地检测到窃，例如随机排列和持续预处理，并使用数万亿个令牌进行预处理。此外，所有检测都可以在一个小时内在单个PC上执行，从而使MDIR有效且易于使用。</li>
</ul>

<h3>Title: Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yanbin Wei, Jiangyue Yan, Chun Kang, Yang Chen, Hua Liu, James T. Kwok, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06345">https://arxiv.org/abs/2508.06345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06345">https://arxiv.org/pdf/2508.06345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06345]] Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering(https://arxiv.org/abs/2508.06345)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities in diverse domain question-answering (QA) tasks, including graph QA that involves complex graph topologies. However, most current approaches use only a single type of graph representation, namely Topology Representation Form (TRF), such as prompt-unified text descriptions or style-fixed visual styles. Those "one-size-fits-all" approaches fail to consider the specific preferences of different models or tasks, often leading to incorrect or overly long responses. To address this, we first analyze the characteristics and weaknesses of existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency (GRE), which measures the balance between the performance and the brevity in graph QA. Built on these, we develop the DynamicTRF framework, which aims to improve both the accuracy and conciseness of graph QA. To be specific, DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based on their GRE scores, to probe the question-specific TRF preferences. Then it trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from $F_{ZS}$ for each question during the inference. Extensive experiments across 7 in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms of accuracy</li>
<li><strong>摘要：</strong>大型多模型模型（LMMS）显示了不同领域的避开（QA）任务中的广义零射击功能，包括涉及复杂图形拓扑的图形质量检查。但是，大多数当前的方法仅使用单一类型的图表表示，即拓扑表示表格（TRF），例如及时的自态文本说明或样式固定的视觉样式。那些“千篇一律的”方法无法考虑不同模型或任务的特定偏好，通常会导致不正确或过长的响应。为了解决这个问题，我们首先分析现有TRF的特性和弱点，然后设计一组TRF，用$ f_ {zs} $表示，该trf量身定制为零摄影图QA。然后，我们引入了一个新的度量图响应效率（GRE），该效率（GRE）衡量了图质量质量检查中的性能和简短之间的平衡。基于这些，我们开发了DynamiCTRF框架，该框架旨在提高图形质量检查的准确性和简洁性。具体而言，DynamICTRF首先创建一个TRF偏好（TRFP）数据集，该数据集基于其GRE分数对TRF进行排名，以探测特定问题的TRF偏好。然后，它在TRFP数据集上训练TRF路由器，以适应推理期间每个问题的$ f_ {zs} $从$ f_ {zs} $中分配最佳trf。跨7个内域算法图QA任务和2个跨域下游任务的广泛实验表明，DynamiCTRF显着增强了LMMS的零摄像图QA QA的精度</li>
</ul>

<h3>Title: Cyberbullying Detection via Aggression-Enhanced Prompting</h3>
<ul>
<li><strong>Authors: </strong>Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06360">https://arxiv.org/abs/2508.06360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06360">https://arxiv.org/pdf/2508.06360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06360]] Cyberbullying Detection via Aggression-Enhanced Prompting(https://arxiv.org/abs/2508.06360)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.</li>
<li><strong>摘要：</strong>由于其微妙和多样化的表达方式，在社交媒体上检测网络欺凌仍然是一个关键挑战。这项研究调查了将侵略性检测作为统一培训框架内的辅助任务是否可以增强网络欺凌检测中大语言模型（LLM）的概括和性能。实验是在使用指令调整的LLM的五个侵略数据集和一个网络欺凌数据集上进行的。我们评估了多种策略：零射，很少，独立的Lora微调和多任务学习（MTL）。鉴于MTL的不一致结果，我们提出了一种富集的及时管道方法，其中侵略预测嵌入了网络欺凌检测提示中，以提供上下文增强。初步结果表明，富集的及时管道的表现始终优于标准的lora微调，这表明侵略性的环境显着增强了网络欺凌检测。这项研究强调了辅助任务的潜力，例如侵略性检测，以改善LLM在社交网络上的关键性应用程序的概括。</li>
</ul>

<h3>Title: Evaluating Style-Personalized Text Generation: Challenges and Directions</h3>
<ul>
<li><strong>Authors: </strong>Anubhav Jangra, Bahareh Sarrafzadeh, Adrian de Wynter, Silviu Cucerzan, Sujay Kumar Jauhar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06374">https://arxiv.org/abs/2508.06374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06374">https://arxiv.org/pdf/2508.06374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06374]] Evaluating Style-Personalized Text Generation: Challenges and Directions(https://arxiv.org/abs/2508.06374)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>While prior research has built tools and benchmarks towards style personalized text generation, there has been limited exploration of evaluation in low-resource author style personalized text generation space. Through this work, we question the effectiveness of the widely adopted evaluation metrics like BLEU and ROUGE, and explore other evaluation paradigms such as style embeddings and LLM-as-judge to holistically evaluate the style personalized text generation task. We evaluate these metrics and their ensembles using our style discrimination benchmark, that spans eight writing tasks, and evaluates across three settings, domain discrimination, authorship attribution, and LLM personalized vs non-personalized discrimination. We provide conclusive evidence to adopt ensemble of diverse evaluation metrics to effectively evaluate style personalized text generation.</li>
<li><strong>摘要：</strong>虽然先前的研究已经建立了针对样式的个性化文本生成的工具和基准，但在低资源作者风格的个性化文本生成空间中对评估的探索有限。通过这项工作，我们质疑广泛采用的评估指标（如Bleu and Rouge）的有效性，并探索其他评估范式，例如样式嵌入式和LLM-As-Gudge，以整体评估样式的个性化文本生成任务。我们使用我们的样式歧视基准评估了这些指标及其集合，该基准涵盖了八个写作任务，并在三种设置，域歧视，作者身份归因和LLM个性化与非个人化歧视之间进行了评估。我们提供了确定的证据，以采用各种评估指标的合奏来有效评估样式的个性化文本生成。</li>
</ul>

<h3>Title: LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing</h3>
<ul>
<li><strong>Authors: </strong>Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06388">https://arxiv.org/abs/2508.06388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06388">https://arxiv.org/pdf/2508.06388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06388]] LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing(https://arxiv.org/abs/2508.06388)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在角色扮演对话中表现出了令人印象深刻的能力，并提供情感支持作为单独的研究方向。但是，将这些功能结合在一起，以使情感支持与虚拟角色相互作用。为了解决这一研究差距，我们将专注于动漫角色作为案例研究，因为他们定义明确的个性和大型粉丝基础。这种选择使我们能够有效地评估LLM在保持特定性格特征的同时如何提供情感支持。我们介绍Chatanime，这是第一个情感支持的角色扮演（ESRP）数据集。我们首先从流行的动漫社区中选择了20个顶级角色，并设计了60个以情感为中心的现实世界情景问题。然后，我们执行一个全国性的选择过程，以识别40位中国动漫爱好者，对特定角色有深刻的了解和在角色扮演方面的丰富经验。接下来，我们系统地从10个LLM和这40个中国动漫爱好者那里收集了两轮对话数据。为了评估LLM的ESRP性能，我们设计了一个以用户体验为导向的评估系统，该系统在三个维度上具有9个细粒度指标：基本对话，角色扮演和情感支持，以及响应多样性的总体指标。总共，数据集包含2,400个人写和24,000个LLM生成的答案，并得到132,000多个人类注释的支持。实验结果表明，表现最佳的LLM超过了人类粉丝在角色扮演和情感支持方面，而人类仍在响应多样性方面领先。我们希望这项工作可以为在ESRP中优化LLM的未来研究提供宝贵的资源和见解。我们的数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Quantifying Conversation Drift in MCP via Latent Polytope</h3>
<ul>
<li><strong>Authors: </strong>Haoran Shi, Hongwei Yao, Shuo Shao, Shaopeng Jiao, Ziqi Peng, Zhan Qin, Cong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06418">https://arxiv.org/abs/2508.06418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06418">https://arxiv.org/pdf/2508.06418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06418]] Quantifying Conversation Drift in MCP via Latent Polytope(https://arxiv.org/abs/2508.06418)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The Model Context Protocol (MCP) enhances large language models (LLMs) by integrating external tools, enabling dynamic aggregation of real-time data to improve task execution. However, its non-isolated execution context introduces critical security and privacy risks. In particular, adversarially crafted content can induce tool poisoning or indirect prompt injection, leading to conversation hijacking, misinformation propagation, or data exfiltration. Existing defenses, such as rule-based filters or LLM-driven detection, remain inadequate due to their reliance on static signatures, computational inefficiency, and inability to quantify conversational hijacking. To address these limitations, we propose SecMCP, a secure framework that detects and quantifies conversation drift, deviations in latent space trajectories induced by adversarial external knowledge. By modeling LLM activation vectors within a latent polytope space, SecMCP identifies anomalous shifts in conversational dynamics, enabling proactive detection of hijacking, misleading, and data exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA), demonstrating robust detection with AUROC scores exceeding 0.915 while maintaining system usability. Our contributions include a systematic categorization of MCP security threats, a novel latent polytope-based methodology for quantifying conversation drift, and empirical validation of SecMCP's efficacy.</li>
<li><strong>摘要：</strong>模型上下文协议（MCP）通过集成外部工具来增强大语言模型（LLM），从而使实时数据的动态聚合以改善任务执行。但是，其非隔离执行环境引入了关键的安全性和隐私风险。特别是，对抗性的内容可以诱导工具中毒或间接及时注射，从而导致对话劫持，错误信息传播或数据渗透。现有的防御措施，例如基于规则的过滤器或LLM驱动的检测，由于它们依赖静态签名，计算效率低下以及无法量化对话性劫持劫持，因此仍然不足。为了解决这些限制，我们提出了SECMCP，这是一个安全框架，可检测和量化对话漂移，潜在空间轨迹的偏差，由对抗性外部知识引起。通过对潜在多层空间内的LLM激活向量进行建模，SECMCP可以识别对话动力学的异常移动，从而积极地检测劫持，误导性和数据剥落。我们在基准数据集（MAS MARCO，HOTPOTQA，FINQA）上评估了三个最先进的LLMS（LLAMA3，VICUNA，MISTRAL）的SECMCP，在维持系统可用性的同时，表明AUROC得分超过0.915的可靠检测。我们的贡献包括对MCP安全威胁的系统分类，这是一种基于潜在多层多物种的新方法，用于量化对话漂移，以及对SECMCP功效的经验验证。</li>
</ul>

<h3>Title: Memp: Exploring Agent Procedural Memory</h3>
<ul>
<li><strong>Authors: </strong>Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06433">https://arxiv.org/abs/2508.06433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06433">https://arxiv.org/pdf/2508.06433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06433]] Memp: Exploring Agent Procedural Memory(https://arxiv.org/abs/2508.06433)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的代理在各种任务上都表现出色，但它们遭受了脆性的程序记忆的困扰，这些记忆是在静态参数中手动设计或纠缠的。在这项工作中，我们研究了策略，以赋予代理商具有可学习，可更新和终身的程序记忆。我们提出的MEMP将过去的代理轨迹提炼成细粒度，分步说明和更高级别，类似脚本的抽象，并探讨不同策略对构建，检索和更新过程内存的影响。再加上一种动态方案，该方案不断更新，纠正和弃用其内容，该存储库随着新的体验而演变。对旅行计划者和ALFWORLD的经验评估表明，随着记忆存储库的完善，代理在类似任务上稳定地获得更高的成功率和更高的效率。此外，由更强模型构建的程序内存保留了其价值：将程序记忆迁移到较弱的模型可带来可观的性能增长。</li>
</ul>

<h3>Title: Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</h3>
<ul>
<li><strong>Authors: </strong>Andrea Nasuto, Stefano Maria Iacus, Francisco Rowe, Devika Jain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06435">https://arxiv.org/abs/2508.06435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06435">https://arxiv.org/pdf/2508.06435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06435]] Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages(https://arxiv.org/abs/2508.06435)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过实现可扩展的精确分析来改变社会科学研究。它们的适应性提出了一个问题，即是否通过几种语言通过微调获得的知识可以转移到仅在预训练期间出现的看不见的语言。为了研究这一点，我们对单语，双语或多语言数据集微调轻巧的骆驼3.2-3b模型，以通过13种语言从X/Twitter中分类与移民相关的推文，这是一个以极化的，文化为特定的话语的领域。我们评估最小语言特定的微调是否可以启用跨语性主题检测以及添加目标语言是否纠正训练前偏见。结果表明，用一种或两种语言进行微调的LLM可以可靠地用看不见的语言对移民相关的内容进行分类。但是，确定一条推文是否表达了多语言微调的促成或反移民立场。预训练的偏见有利于主要的语言，但在微调过程中，即使对代表性不足的语言的暴露量最少（原始预培训代币量的$ 9.62 \ times10^{-11} $）产生了可观的收益。这些发现挑战了以下假设：跨语性掌握需要广泛的多语言培训：有限的语言覆盖范围足以实现主题级的概括，并且可以通过轻量级的干预措施来纠正结构性偏见。通过发布4位量化的洛拉微调模型，我们提供了一种开源的，可重现的专有LLMS的替代品，该替代方案可快35倍，仅以OpenAI GPT-4O型号的美元成本的0.00000989％提供35倍的推断，从而实现可伸缩的，包含性的研究。</li>
</ul>

<h3>Title: Echoes of Automation: The Increasing Use of LLMs in Newsmaking</h3>
<ul>
<li><strong>Authors: </strong>Abolfazl Ansari, Delvin Ce Zhang, Nafis Irtiza Tripto, Dongwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06445">https://arxiv.org/abs/2508.06445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06445">https://arxiv.org/pdf/2508.06445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06445]] Echoes of Automation: The Increasing Use of LLMs in Newsmaking(https://arxiv.org/abs/2508.06445)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.</li>
<li><strong>摘要：</strong>生成AI（Genai），尤其是LLM的快速崛起引起了新闻完整性和作者身份的关注。这项研究以各种媒体格式研究了来自主要，本地和大学新闻媒体的40,000篇新闻文章中的AI生成的内容。使用三个高级AI-文本探测器（例如，双筒望远镜，快速检测GPT和GPTZERO），我们发现Genai的使用量很大，尤其是在本地和大学新闻中。句子级分析表明，LLM经常在新闻的引入中使用，而结论通常是手动写的。语言分析显示Genai提高了单词丰富性和可读性，但降低了形式，从而导致更统一的写作风格，尤其是在当地媒体中。</li>
</ul>

<h3>Title: SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning</h3>
<ul>
<li><strong>Authors: </strong>Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06447">https://arxiv.org/abs/2508.06447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06447">https://arxiv.org/pdf/2508.06447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06447]] SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning(https://arxiv.org/abs/2508.06447)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的长篇小说推断受高度计算需求的严重限制。尽管几种现有方法优化了注意力计算，但它们仍在每层处理完整的隐藏状态，从而限制了整体效率。在这项工作中，我们提出了Sliminfer，这是一个创新的框架，旨在通过直接在前进通行证中直接修剪较不关键的提示令牌来加速推理。我们的关键见解是一种信息扩散现象：随着来自关键令牌的信息通过层次传播，它变成了整个序列。这个扩散过程表明，当过多的令牌（包括这些关键的标记）在隐藏状态下修剪时，LLM可以保持其语义完整性。由此激励，Sliminfer引入了一种动态的细颗粒修剪机制，该机制可以准确地去除中间层处的隐藏状态的冗余令牌。这种层的修剪自然会启用异步的KV缓存管理器，该管理器需要预取，需要令牌块，而无需复杂的预测变量，从而降低了内存使用情况和I/O成本。 Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench.我们的代码将在接受后发布。</li>
</ul>

<h3>Title: GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>GLM-4.5 Team: Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06471">https://arxiv.org/abs/2508.06471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06471">https://arxiv.org/pdf/2508.06471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06471]] GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models(https://arxiv.org/abs/2508.06471)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at this https URL.</li>
<li><strong>摘要：</strong>我们提出GLM-4.5，是具有355B总参数和32B激活参数的开源外源混合物（MOE）大型语言模型，具有一种支持思维和直接响应模式的混合推理方法。通过对23T代币的多阶段培训以及通过专家模型迭代和强化学习的全面培训，GLM-4.5实现了跨代理，推理和编码（ARC）任务的强劲性能，在Tau-Bench上得分为70.1％，AIME 24的91.0％，在SWE-Bench上获得了64.2％的验证。 GLM-4.5的参数比几个竞争对手少得多，在所有评估模型中排名第三，而在代理基准上排名第二。我们同时发布GLM-4.5（355B参数）和紧凑版本GLM-4.5-Air（106B参数），以推动推理和代理AI系统的研究。代码，模型和更多信息可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning</h3>
<ul>
<li><strong>Authors: </strong>Guimin Hu, Daniel Hershcovich, Hasti Seifi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06475">https://arxiv.org/abs/2508.06475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06475">https://arxiv.org/pdf/2508.06475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06475]] HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning(https://arxiv.org/abs/2508.06475)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMA's captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.</li>
<li><strong>摘要：</strong>触觉字幕是从触觉信号（例如振动）中生成自然语言描述的任务，以用于虚拟现实，可访问性和康复应用中。尽管以前的多模式研究主要集中在视觉和音频上，但触摸感的触觉信号仍未得到充实。为了解决这一差距，我们将触觉字幕的任务形式化，并提出HAPTICLLAMA，这是一种多模式的感觉语言模型，将振动信号解释为给定的感觉，情感或关联类别中的描述。我们研究了两种类型的触觉令牌，一种基于频率的令牌和一个基于Encodec的令牌，它们将触觉信号转换为离散单元序列，从而使其与Llama模型的集成。 Hapticllama在两个阶段进行了培训：（1）使用基于Lora的适应性的Llama体系结构进行微调，以及（2）通过从人类反馈（RLHF）进行的加强学习进行微调。我们使用自动N-Gram指标和人类评估来评估Hapticllama的字幕性能。 Hapticllama在解释触觉振动信号方面表现出强大的能力，流星得分分别为59.98和BLEU-4分别为32.06。此外，超过61％的生成字幕以7分制获得了超过3.5的人类评分，RLHF的总评级分布提高了10％，表明与人触觉感知的一致性更强。这些发现突出了大语言模型处理和适应感官数据的潜力。</li>
</ul>

<h3>Title: Post-training for Efficient Communication via Convention Formation</h3>
<ul>
<li><strong>Authors: </strong>Yilun Hua, Evan Wang, Yoav Artzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.06482">https://arxiv.org/abs/2508.06482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.06482">https://arxiv.org/pdf/2508.06482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.06482]] Post-training for Efficient Communication via Convention Formation(https://arxiv.org/abs/2508.06482)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.</li>
<li><strong>摘要：</strong>人类通过调整语言并形成临时约定，以提高多转变互动的效率来沟通。相反，先前的工作表明LLM自然不会显示这种行为。我们开发了一个训练后的过程，通过对启发式识别的会议形成的示范进行定向的微调来发展这种能力。我们以两个针对此功能的新基准进行评估。首先，我们设计了一个集中的，认知动机的相互作用基准，该基准始终引起人类的强烈惯例形成趋势。其次，我们创建了一个新的文档接收参考完成任务，反映了野外约定形成行为。我们的研究表明，在两种评估方法中，训练后LLMS中的公约形成能力显着提高。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
