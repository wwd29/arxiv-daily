<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-16</h1>
<h3>Title: Integrating Large Language Models with Graph-based Reasoning for Conversational Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Parag Jain, Mirella Lapata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09506">https://arxiv.org/abs/2407.09506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09506">https://arxiv.org/pdf/2407.09506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09506]] Integrating Large Language Models with Graph-based Reasoning for Conversational Question Answering(https://arxiv.org/abs/2407.09506)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We focus on a conversational question answering task which combines the challenges of understanding questions in context and reasoning over evidence gathered from heterogeneous sources like text, knowledge graphs, tables, and infoboxes. Our method utilizes a graph structured representation to aggregate information about a question and its context (i.e., the conversation so far and evidence retrieved to find an answer), while also harnessing the reasoning and text generation capabilities of large language models (LLMs). Graph embeddings are directly injected into the LLM, bypassing the token embedding layers, and learned end-to-end by minimizing cross-entropy. Our model maintains a memory module to track and update past evidence, thus influencing the graph's structure, as the conversation evolves. Experimental results on the ConvMix benchmark(Christmann et al., 2022a) show that graph embeddings enhance the LLM's ability to reason, while the memory module provides robustness against noise and retrieval errors.</li>
<li><strong>摘要：</strong>我们专注于对话式问答任务，该任务结合了理解上下文中的问题和推理从文本、知识图谱、表格和信息框等异构来源收集的证据的挑战。我们的方法利用图形结构化表示来聚合有关问题及其上下文的信息（即迄今为止的对话和为找到答案而检索的证据），同时还利用大型语言模型 (LLM) 的推理和文本生成功能。图形嵌入直接注入 LLM，绕过标记嵌入层，并通过最小化交叉熵进行端到端学习。我们的模型维护一个记忆模块来跟踪和更新过去的证据，从而随着对话的发展影响图形的结构。ConvMix 基准测试（Christmann 等人，2022a）上的实验结果表明，图形嵌入增强了 LLM 的推理能力，而记忆模块则提供了对噪声和检索错误的鲁棒性。</li>
</ul>

<h3>Title: MATE: Meet At The Embedding -- Connecting Images with Long Texts</h3>
<ul>
<li><strong>Authors: </strong>Young Kyun Jang, Junmo Kang, Yong Jae Lee, Donghyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] MATE: Meet At The Embedding -- Connecting Images with Long Texts(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While advancements in Vision Language Models (VLMs) have significantly improved the alignment of visual and textual data, these models primarily focus on aligning images with short descriptive captions. This focus limits their ability to handle complex text interactions, particularly with longer texts such as lengthy captions or documents, which have not been extensively explored yet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach that combines the capabilities of VLMs with Large Language Models (LLMs) to overcome this challenge without the need for additional image-long text pairs. Specifically, we replace the text encoder of the VLM with a pretrained LLM-based encoder that excels in understanding long texts. To bridge the gap between VLM and LLM, MATE incorporates a projection module that is trained in a multi-stage manner. It starts by aligning the embeddings from the VLM text encoder with those from the LLM using extensive text pairs. This module is then employed to seamlessly align image embeddings closely with LLM embeddings. We propose two new cross-modal retrieval benchmarks to assess the task of connecting images with long texts (lengthy captions / documents). Extensive experimental results demonstrate that MATE effectively connects images with long texts, uncovering diverse semantic relationships.</li>
<li><strong>摘要：</strong>虽然视觉语言模型 (VLM) 的进步显著改善了视觉和文本数据的对齐，但这些模型主要侧重于对齐带有简短描述性标题的图像。这种侧重限制了它们处理复杂文本交互的能力，尤其是较长的文本，例如长标题或文档，这些文本尚未得到广泛探索。在本文中，我们介绍了 Meet At The Embedding (MATE)，这是一种新颖的方法，它将 VLM 的功能与大型语言模型 (LLM) 相结合，以克服这一挑战，而无需额外的图像长文本对。具体来说，我们用预训练的基于 LLM 的编码器替换 VLM 的文本编码器，该编码器擅长理解长文本。为了弥合 VLM 和 LLM 之间的差距，MATE 采用了以多阶段方式训练的投影模块。它首先使用大量文本对将来自 VLM 文本编码器的嵌入与来自 LLM 的嵌入对齐。然后使用此模块将图像嵌入与 LLM 嵌入无缝对齐。我们提出了两个新的跨模态检索基准来评估将图像与长文本（长标题/文档）连接起来的任务。大量实验结果表明，MATE 可以有效地将图像与长文本连接起来，揭示出各种语义关系。</li>
</ul>

<h3>Title: Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Zeliang Zhang, Xiaodong Liu, Hao Cheng, Chenliang Xu, Jianfeng Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09590">https://arxiv.org/abs/2407.09590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09590">https://arxiv.org/pdf/2407.09590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09590]] Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts(https://arxiv.org/abs/2407.09590)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>By increasing model parameters but activating them sparsely when performing a task, the use of Mixture-of-Experts (MoE) architecture significantly improves the performance of Large Language Models (LLMs) without increasing the inference cost. However, the memory consumption due to the growing number of experts presents a challenge to the deployment of these models in many real world settings. Our empirical study reveals that some experts encode redundant knowledge during pre-training. We thus propose a method of grouping and pruning similar experts to improve model's parameter efficiency. We validate the effectiveness of our method by pruning two state-of-the-art MoE models, Mixtral-8x7B and Mixtral-8x22B. Evaluation shows that our method outperforms other model pruning methods on a range of natural language tasks. To facilitate future research, we will release our code and the pruned MoE models.</li>
<li><strong>摘要：</strong>通过增加模型参数但在执行任务时稀疏地激活它们，混合专家 (MoE) 架构的使用显著提高了大型语言模型 (LLM) 的性能，而不会增加推理成本。然而，由于专家数量的增加而导致的内存消耗对在许多现实世界环境中部署这些模型提出了挑战。我们的实证研究表明，一些专家在预训练期间编码了冗余知识。因此，我们提出了一种对相似专家进行分组和修剪的方法，以提高模型的参数效率。我们通过修剪两个最先进的 MoE 模型 Mixtral-8x7B 和 Mixtral-8x22B 来验证我们方法的有效性。评估表明，我们的方法在一系列自然语言任务上优于其他模型修剪方法。为了促进未来的研究，我们将发布我们的代码和修剪后的 MoE 模型。</li>
</ul>

<h3>Title: How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs</h3>
<ul>
<li><strong>Authors: </strong>Andrea W Wen-Yi, Unso Eun Seo Jo, Lu Jia Lin, David Mimno</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09652">https://arxiv.org/abs/2407.09652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09652">https://arxiv.org/pdf/2407.09652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09652]] How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs(https://arxiv.org/abs/2407.09652)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Contemporary language models are increasingly multilingual, but Chinese LLM developers must navigate complex political and business considerations of language diversity. Language policy in China aims at influencing the public discourse and governing a multi-ethnic society, and has gradually transitioned from a pluralist to a more assimilationist approach since 1949. We explore the impact of these influences on current language technology. We evaluate six open-source multilingual LLMs pre-trained by Chinese companies on 18 languages, spanning a wide range of Chinese, Asian, and Anglo-European languages. Our experiments show Chinese LLMs performance on diverse languages is indistinguishable from international LLMs. Similarly, the models' technical reports also show lack of consideration for pretraining data language coverage except for English and Mandarin Chinese. Examining Chinese AI policy, model experiments, and technical reports, we find no sign of any consistent policy, either for or against, language diversity in China's LLM development. This leaves a puzzling fact that while China regulates both the languages people use daily as well as language model development, they do not seem to have any policy on the languages in language models.</li>
<li><strong>摘要：</strong>当代语言模型日益多语言化，但中国法学硕士开发人员必须应对语言多样性的复杂政治和商业考虑。中国的语言政策旨在影响公共话语和治理多民族社会，自 1949 年以来已逐渐从多元化转向更具同化主义的方法。我们探讨了这些影响对当前语言技术的影响。我们评估了由中国公司对 18 种语言进行预训练的六种开源多语言法学硕士，涵盖了广泛的中文、亚洲和英欧语言。我们的实验表明，中国法学硕士在不同语言上的表现与国际法学硕士没有区别。同样，这些模型的技术报告也显示，除了英语和普通话外，没有考虑预训练数据的语言覆盖范围。通过研究中国的人工智能政策、模型实验和技术报告，我们发现没有迹象表明中国法学硕士发展中存在任何一致的政策，无论是支持还是反对语言多样性。这留下了一个令人费解的事实：虽然中国既规范人们日常使用的语言，也规范语言模型开发，但他们似乎没有对语言模型中的语言制定任何政策。</li>
</ul>

<h3>Title: Bridging the Gap Between Information Seeking and Product Search Systems: Q&A Recommendation for E-commerce</h3>
<ul>
<li><strong>Authors: </strong>Saar Kuzi, Shervin Malmasi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09653">https://arxiv.org/abs/2407.09653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09653">https://arxiv.org/pdf/2407.09653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09653]] Bridging the Gap Between Information Seeking and Product Search Systems: Q&A Recommendation for E-commerce(https://arxiv.org/abs/2407.09653)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Consumers on a shopping mission often leverage both product search and information seeking systems, such as web search engines and Question Answering (QA) systems, in an iterative process to improve their understanding of available products and reach a purchase decision. While product search is useful for shoppers to find the actual products meeting their requirements in the catalog, information seeking systems can be utilized to answer any questions they may have to refine those requirements. The recent success of Large Language Models (LLMs) has opened up an opportunity to bridge the gap between the two tasks to help customers achieve their goals quickly and effectively by integrating conversational QA within product search. In this paper, we propose to recommend users Question-Answer (Q&A) pairs that are relevant to their product search and can help them make a purchase decision. We discuss the different aspects of the problem including the requirements and characteristics of the Q&A pairs, their generation, and the optimization of the Q&A recommendation task. We highlight the challenges, open problems, and suggested solutions to encourage future research in this emerging area.</li>
<li><strong>摘要：</strong>消费者在购物时通常会在迭代过程中利用产品搜索和信息搜索系统（如网络搜索引擎和问答系统）来提高对可用产品的了解并做出购买决定。虽然产品搜索有助于购物者在目录中找到符合其要求的实际产品，但信息搜索系统可用于回答他们可能提出的任何问题以完善这些要求。大型语言模型 (LLM) 的最新成功为弥合这两项任务之间的差距提供了机会，通过在产品搜索中集成对话式问答，帮助客户快速有效地实现目标。在本文中，我们建议向用户推荐与他们的产品搜索相关的问答 (Q&A) 对，并帮助他们做出购买决定。我们讨论了问题的不同方面，包括问答对的要求和特征、问答对的生成以及问答推荐任务的优化。我们强调了挑战、未解决的问题和建议的解决方案，以鼓励未来在这一新兴领域的研究。</li>
</ul>

<h3>Title: Large Language Models for Integrating Social Determinant of Health Data: A Case Study on Heart Failure 30-Day Readmission Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chase Fensore, Rodrigo M. Carrillo-Larco, Shivani A. Patel, Alanna A. Morris, Joyce C. Ho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09688">https://arxiv.org/abs/2407.09688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09688">https://arxiv.org/pdf/2407.09688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09688]] Large Language Models for Integrating Social Determinant of Health Data: A Case Study on Heart Failure 30-Day Readmission Prediction(https://arxiv.org/abs/2407.09688)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Social determinants of health (SDOH) $-$ the myriad of circumstances in which people live, grow, and age $-$ play an important role in health outcomes. However, existing outcome prediction models often only use proxies of SDOH as features. Recent open data initiatives present an opportunity to construct a more comprehensive view of SDOH, but manually integrating the most relevant data for individual patients becomes increasingly challenging as the volume and diversity of public SDOH data grows. Large language models (LLMs) have shown promise at automatically annotating structured data. Here, we conduct an end-to-end case study evaluating the feasibility of using LLMs to integrate SDOH data, and the utility of these SDOH features for clinical prediction. We first manually label 700+ variables from two publicly-accessible SDOH data sources to one of five semantic SDOH categories. Then, we benchmark performance of 9 open-source LLMs on this classification task. Finally, we train ML models to predict 30-day hospital readmission among 39k heart failure (HF) patients, and we compare the prediction performance of the categorized SDOH variables with standard clinical variables. Additionally, we investigate the impact of few-shot LLM prompting on LLM annotation performance, and perform a metadata ablation study on prompts to evaluate which information helps LLMs accurately annotate these variables. We find that some open-source LLMs can effectively, accurately annotate SDOH variables with zero-shot prompting without the need for fine-tuning. Crucially, when combined with standard clinical features, the LLM-annotated Neighborhood and Built Environment subset of the SDOH variables shows the best performance predicting 30-day readmission of HF patients.</li>
<li><strong>摘要：</strong>健康的社会决定因素 (SDOH) $-$ 人们生活、成长和衰老的各种情况 $-$ 在健康结果中发挥着重要作用。然而，现有的结果预测模型通常仅使用 SDOH 的代理作为特征。最近的开放数据计划提供了构建更全面的 SDOH 视图的机会，但随着公共 SDOH 数据的数量和多样性的增长，手动整合与个体患者最相关的数据变得越来越具有挑战性。大型语言模型 (LLM) 在自动注释结构化数据方面显示出良好的前景。在这里，我们进行了一项端到端案例研究，评估使用 LLM 集成 SDOH 数据的可行性，以及这些 SDOH 特征对临床预测的实用性。我们首先将来自两个可公开访问的 SDOH 数据源的 700 多个变量手动标记为五个语义 SDOH 类别之一。然后，我们在此分类任务上对 9 个开源 LLM 的性能进行基准测试。最后，我们训练 ML 模型来预测 39,000 名心力衰竭 (HF) 患者的 30 天再入院率，并将分类后的 SDOH 变量的预测性能与标准临床变量进行比较。此外，我们研究了少量 LLM 提示对 LLM 注释性能的影响，并对提示进行了元数据消融研究，以评估哪些信息有助于 LLM 准确注释这些变量。我们发现一些开源 LLM 可以使用零样本提示有效、准确地注释 SDOH 变量，而无需进行微调。至关重要的是，当与标准临床特征相结合时，LLM 注释的 SDOH 变量的邻里和建筑环境子集在预测 HF 患者 30 天再入院率方面表现出最佳性能。</li>
</ul>

<h3>Title: What an Elegant Bridge: Multilingual LLMs are Biased Similarly in Different Languages</h3>
<ul>
<li><strong>Authors: </strong>Viktor Mihaylov, Aleksandar Shtedritski</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09704">https://arxiv.org/abs/2407.09704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09704">https://arxiv.org/pdf/2407.09704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09704]] What an Elegant Bridge: Multilingual LLMs are Biased Similarly in Different Languages(https://arxiv.org/abs/2407.09704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper investigates biases of Large Language Models (LLMs) through the lens of grammatical gender. Drawing inspiration from seminal works in psycholinguistics, particularly the study of gender's influence on language perception, we leverage multilingual LLMs to revisit and expand upon the foundational experiments of Boroditsky (2003). Employing LLMs as a novel method for examining psycholinguistic biases related to grammatical gender, we prompt a model to describe nouns with adjectives in various languages, focusing specifically on languages with grammatical gender. In particular, we look at adjective co-occurrences across gender and languages, and train a binary classifier to predict grammatical gender given adjectives an LLM uses to describe a noun. Surprisingly, we find that a simple classifier can not only predict noun gender above chance but also exhibit cross-language transferability. We show that while LLMs may describe words differently in different languages, they are biased similarly.</li>
<li><strong>摘要：</strong>本文从语法性别的角度研究大型语言模型 (LLM) 的偏见。从心理语言学的开创性著作中汲取灵感，特别是性别对语言感知影响的研究，我们利用多语言 LLM 重新审视和扩展 Boroditsky (2003) 的基础实验。使用 LLM 作为研究与语法性别相关的心理语言学偏见的新方法，我们促使一个模型用各种语言中的形容词描述名词，特别关注具有语法性别的语言。具体来说，我们研究了跨性别和语言的形容词共现，并训练二元分类器来预测 LLM 用来描述名词的形容词的语法性别。令人惊讶的是，我们发现一个简单的分类器不仅可以预测名词性别，而且还具有跨语言可迁移性。我们表明，虽然 LLM 可能以不同的方式描述不同语言中的单词，但它们的偏见是相似的。</li>
</ul>

<h3>Title: Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09722">https://arxiv.org/abs/2407.09722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09722">https://arxiv.org/pdf/2407.09722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09722]] Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference(https://arxiv.org/abs/2407.09722)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Transformer-based Large language models (LLMs) have demonstrated their power in various tasks, but their inference incurs significant time and energy costs. To accelerate LLM inference, speculative decoding uses a smaller model to propose one sequence of tokens, which are subsequently validated in batch by the target large model. Compared with autoregressive decoding, speculative decoding generates the same number of tokens with fewer runs of the large model, hence accelerating the overall inference by $1$-$2\times$. However, greedy decoding is not the optimal decoding algorithm in terms of output perplexity, which is a direct measurement of the effectiveness of a decoding algorithm. An algorithm that has better output perplexity and even better efficiency than speculative decoding can be more useful in practice. To achieve this seemingly contradictory goal, we first introduce multi-token joint greedy decoding (MJGD), which greedily generates multiple tokens at each step based on their joint perplexity. We show that it leads to better perplexity for the whole output. But the computation cost of MJGD is infeasible in practice. So we further propose multi-token joint speculative decoding (MJSD), which approximates and accelerates the MJGD from two aspects: it approximates the joint distribution of the large model with that of a small model, and uses a verification step to guarantee the accuracy of approximation; then it uses beam decoding to accelerate the sequence generation from the joint distribution. Compared with vanilla speculative decoding, MJSD has two advantages: (1) it is an approximation of MJGD, thus achieving better output perplexity; (2) verification with joint likelihood allows it to accept the longest prefix sub-sequence of the draft tokens with valid perplexity, leading to better efficiency...</li>
<li><strong>摘要：</strong>基于 Transformer 的大型语言模型 (LLM) 已在各种任务中展示了其强大功能，但其推理需要耗费大量时间和能源。为了加速 LLM 推理，推测解码使用较小的模型来提出一个 token 序列，随后由目标大型模型批量验证这些 token。与自回归解码相比，推测解码使用较少的大型模型运行生成相同数量的 token，因此将整体推理速度加快了 $1$-$2\times$。然而，贪婪解码在输出困惑度方面并不是最佳解码算法，而输出困惑度是解码算法有效性的直接衡量标准。与推测解码相比，具有更好输出困惑度甚至更好效率的算法在实践中会更有用。为了实现这个看似矛盾的目标，我们首先引入了多 token 联合贪婪解码 (MJGD)，它根据它们的联合困惑度在每个步骤中贪婪地生成多个 token。我们表明，它可以为整个输出带来更好的困惑度。但 MJGD 的计算成本在实践中是不可行的。因此，我们进一步提出了多令牌联合推测解码（MJSD），它从两个方面近似并加速 MJGD：它用小模型的联合分布近似大模型的联合分布，并使用验证步骤来保证近似的准确性；然后它使用波束解码来加速从联合分布生成序列。与原始推测解码相比，MJSD 有两个优点：（1）它是 MJGD 的近似，从而实现更好的输出困惑度；（2）使用联合似然的验证使其能够接受具有有效困惑度的草稿令牌的最长前缀子序列，从而获得更好的效率......</li>
</ul>

<h3>Title: LLM-Collaboration on Automatic Science Journalism for the General Audience</h3>
<ul>
<li><strong>Authors: </strong>Gongyao Jiang, Xinran Shi, Qiong Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09756">https://arxiv.org/abs/2407.09756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09756">https://arxiv.org/pdf/2407.09756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09756]] LLM-Collaboration on Automatic Science Journalism for the General Audience(https://arxiv.org/abs/2407.09756)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Science journalism reports current scientific discoveries to non-specialists, aiming to enable public comprehension of the state of the art. However, this task can be challenging as the audience often lacks specific knowledge about the presented research. To address this challenge, we propose a framework that integrates three LLMs mimicking the real-world writing-reading-feedback-revision workflow, with one LLM acting as the journalist, a smaller LLM as the general public reader, and the third LLM as an editor. The journalist's writing is iteratively refined by feedback from the reader and suggestions from the editor. Our experiments demonstrate that by leveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can generate articles that are more accessible than those generated by existing methods, including advanced models such as GPT-4.</li>
<li><strong>摘要：</strong>科学新闻报道当前的科学发现给非专业人士，旨在让公众了解最新技术。然而，这项任务可能具有挑战性，因为观众往往缺乏关于所呈现研究的具体知识。为了应对这一挑战，我们提出了一个框架，该框架集成了三个 LLM，模拟现实世界中的写作-阅读-反馈-修改工作流程，其中一个 LLM 充当记者，一个较小的 LLM 充当普通读者，第三个 LLM 充当编辑。记者的写作通过读者的反馈和编辑的建议不断完善。我们的实验表明，通过利用两个 7B 和一个 1.8B 开源 LLM 的协作，我们可以生成比现有方法（包括 GPT-4 等高级模型）生成的文章更容易理解的文章。</li>
</ul>

<h3>Title: AraFinNLP 2024: The First Arabic Financial NLP Shared Task</h3>
<ul>
<li><strong>Authors: </strong>Sanad Malaysha, Mo El-Haj, Saad Ezzini, Mohammed Khalilia, Mustafa Jarrar, Sultan Almujaiwel, Ismail Berrada, Houda Bouamor</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09818">https://arxiv.org/abs/2407.09818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09818">https://arxiv.org/pdf/2407.09818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09818]] AraFinNLP 2024: The First Arabic Financial NLP Shared Task(https://arxiv.org/abs/2407.09818)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>The expanding financial markets of the Arab world require sophisticated Arabic NLP tools. To address this need within the banking domain, the Arabic Financial NLP (AraFinNLP) shared task proposes two subtasks: (i) Multi-dialect Intent Detection and (ii) Cross-dialect Translation and Intent Preservation. This shared task uses the updated ArBanking77 dataset, which includes about 39k parallel queries in MSA and four dialects. Each query is labeled with one or more of a common 77 intents in the banking domain. These resources aim to foster the development of robust financial Arabic NLP, particularly in the areas of machine translation and banking chat-bots. A total of 45 unique teams registered for this shared task, with 11 of them actively participated in the test phase. Specifically, 11 teams participated in Subtask 1, while only 1 team participated in Subtask 2. The winning team of Subtask 1 achieved F1 score of 0.8773, and the only team submitted in Subtask 2 achieved a 1.667 BLEU score.</li>
<li><strong>摘要：</strong>阿拉伯世界不断扩大的金融市场需要复杂的阿拉伯语 NLP 工具。为了满足银行业领域的这一需求，阿拉伯语金融 NLP (AraFinNLP) 共享任务提出了两个子任务：(i) 多方言意图检测和 (ii) 跨方言翻译和意图保留。此共享任务使用更新的 ArBanking77 数据集，其中包括 MSA 和四种方言中的约 39k 个并行查询。每个查询都标有银行领域中常见的 77 种意图中的一个或多个。这些资源旨在促进强大的金融阿拉伯语 NLP 的发展，特别是在机器翻译和银行聊天机器人领域。共有 45 个独特的团队注册了此共享任务，其中 11 个团队积极参与了测试阶段。具体来说，子任务 1 有 11 支队伍参与，而子任务 2 只有 1 支队伍参与。子任务 1 的获胜队伍取得了 F1 分数 0.8773，子任务 2 中唯一提交论文的队伍取得了 1.667 的 BLEU 分数。</li>
</ul>

<h3>Title: NativQA: Multilingual Culturally-Aligned Natural Query for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Md. Arid Hasan, Maram Hasanain, Fatema Ahmad, Sahinur Rahman Laskar, Sunaya Upadhyay, Vrunda N Sukhadia, Mucahid Kutlu, Shammur Absar Chowdhury, Firoj Alam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09823">https://arxiv.org/abs/2407.09823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09823">https://arxiv.org/pdf/2407.09823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09823]] NativQA: Multilingual Culturally-Aligned Natural Query for LLMs(https://arxiv.org/abs/2407.09823)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Natural Question Answering (QA) datasets play a crucial role in developing and evaluating the capabilities of large language models (LLMs), ensuring their effective usage in real-world applications. Despite the numerous QA datasets that have been developed, there is a notable lack of region-specific datasets generated by native users in their own languages. This gap hinders the effective benchmarking of LLMs for regional and cultural specificities. In this study, we propose a scalable framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. Moreover, to demonstrate the efficacy of the proposed framework, we designed a multilingual natural QA dataset, MultiNativQA, consisting of ~72K QA pairs in seven languages, ranging from high to extremely low resource, based on queries from native speakers covering 18 topics. We benchmark the MultiNativQA dataset with open- and closed-source LLMs. We made both the framework NativQA and MultiNativQA dataset publicly available for the community. (this https URL)</li>
<li><strong>摘要：</strong>自然问答 (QA) 数据集在开发和评估大型语言模型 (LLM) 的功能方面发挥着至关重要的作用，可确保其在实际应用中的有效使用。尽管已经开发了大量 QA 数据集，但由母语用户用自己的语言生成的特定区域数据集明显不足。这一差距阻碍了对 LLM 进行有效的区域和文化特异性基准测试。在本研究中，我们提出了一个可扩展的框架 NativQA，以无缝构建母语中文化和区域一致的 QA 数据集，用于 LLM 评估和调整。此外，为了证明所提框架的有效性，我们设计了一个多语言自然 QA 数据集 MultiNativQA，它包含七种语言的约 72K 个 QA 对，资源从高到极低不等，基于母语人士的查询，涵盖 18 个主题。我们使用开源和闭源 LLM 对 MultiNativQA 数据集进行基准测试。我们向社区公开了框架 NativQA 和 MultiNativQA 数据集。（此 https URL）</li>
</ul>

<h3>Title: Investigating Low-Rank Training in Transformer Language Models: Efficiency and Scaling Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09835">https://arxiv.org/abs/2407.09835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09835">https://arxiv.org/pdf/2407.09835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09835]] Investigating Low-Rank Training in Transformer Language Models: Efficiency and Scaling Analysis(https://arxiv.org/abs/2407.09835)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>State-of-the-art LLMs often rely on scale with high computational costs, which has sparked a research agenda to reduce parameter counts and costs without significantly impacting performance. Our study focuses on Transformer-based LLMs, specifically applying low-rank parametrization to the computationally intensive feedforward networks (FFNs), which are less studied than attention blocks. In contrast to previous works, (i) we explore low-rank parametrization at scale, up to 1.3B parameters; (ii) within Transformer language models rather than convolutional architectures; and (iii) starting from training from scratch. Experiments on the large RefinedWeb dataset show that low-rank parametrization is both efficient (e.g., 2.6$\times$ FFN speed-up with 32\% parameters) and effective during training. Interestingly, these structured FFNs exhibit steeper scaling curves than the original models. Motivated by this finding, we develop the wide and structured networks surpassing the current medium-sized and large-sized Transformer in perplexity and throughput performance.</li>
<li><strong>摘要：</strong>最先进的 LLM 通常依赖于计算成本高的规模，这引发了一项研究议程，以减少参数数量和成本而不显着影响性能。我们的研究重点是基于 Transformer 的 LLM，特别是将低秩参数化应用于计算密集型的前馈网络 (FFN)，这些网络的研究较少，而注意力模块则较少。与之前的研究相比，(i) 我们探索大规模低秩参数化，最多 13 亿个参数；(ii) 在 Transformer 语言模型中而不是卷积架构中；(iii) 从头开始​​训练。在大型 RefinedWeb 数据集上进行的实验表明，低秩参数化既高效（例如，2.6$\times$ FFN 速度提高 32\% 参数），又在训练期间有效。有趣的是，这些结构化的 FFN 表现出比原始模型更陡峭的缩放曲线。受此发现的启发，我们开发了广泛而结构化的网络，其困惑度和吞吐量性能超越了当前的中型和大型 Transformer。</li>
</ul>

<h3>Title: Text-Based Detection of On-Hold Scripts in Contact Center Calls</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Galimzianov, Viacheslav Vyshegorodtsev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09849">https://arxiv.org/abs/2407.09849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09849">https://arxiv.org/pdf/2407.09849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09849]] Text-Based Detection of On-Hold Scripts in Contact Center Calls(https://arxiv.org/abs/2407.09849)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Average hold time is a concern for call centers because it affects customer satisfaction. Contact centers should instruct their agents to use special on-hold scripts to maintain positive interactions with clients. This study presents a natural language processing model that detects on-hold phrases in customer service calls transcribed by automatic speech recognition technology. The task of finding hold scripts in dialogue was formulated as a multiclass text classification problem with three mutually exclusive classes: scripts for putting a client on hold, scripts for returning to a client, and phrases irrelevant to on-hold scripts. We collected an in-house dataset of calls and labeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by exploring various hyperparameter sets and achieved high model performance. The developed model can help agent monitoring by providing a way to check whether an agent follows predefined on-hold scripts.</li>
<li><strong>摘要：</strong>平均等待时间是呼叫中心关注的问题，因为它会影响客户满意度。联络中心应指示其代理使用特殊的等待脚本来与客户保持积极的互动。这项研究提出了一种自然语言处理模型，该模型可检测自动语音识别技术转录的客户服务呼叫中的等待短语。在对话中查找等待脚本的任务被表述为一个多类文本分类问题，其中包含三个互斥类：用于让客户等待的脚本、用于返回客户的脚本以及与等待脚本无关的短语。我们收集了一个内部通话数据集，并标记了每个通话中的每个对话轮次。我们通过探索各种超参数集在数据集上对 RuBERT 进行了微调，并实现了较高的模型性能。开发的模型可以通过提供一种方法来检查代理是否遵循预定义的等待脚本，从而帮助代理监控。</li>
</ul>

<h3>Title: Building pre-train LLM Dataset for the INDIC Languages: a case study on Hindi</h3>
<ul>
<li><strong>Authors: </strong>Shantipriya Parida, Shakshi Panwar, Kusum Lata, Sanskruti Mishra, Sambit Sekhar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09855">https://arxiv.org/abs/2407.09855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09855">https://arxiv.org/pdf/2407.09855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09855]] Building pre-train LLM Dataset for the INDIC Languages: a case study on Hindi(https://arxiv.org/abs/2407.09855)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrated transformative capabilities in many applications that require automatically generating responses based on human instruction. However, the major challenge for building LLMs, particularly in Indic languages, is the availability of high-quality data for building foundation LLMs. In this paper, we are proposing a large pre-train dataset in Hindi useful for the Indic language Hindi. We have collected the data span across several domains including major dialects in Hindi. The dataset contains 1.28 billion Hindi tokens. We have explained our pipeline including data collection, pre-processing, and availability for LLM pre-training. The proposed approach can be easily extended to other Indic and low-resource languages and will be available freely for LLM pre-training and LLM research purposes.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在许多需要根据人工指令自动生成响应的应用中展示了转换能力。然而，构建 LLM（尤其是印度语 LLM）的主要挑战在于，用于构建基础 LLM 的高质量数据的可用性。在本文中，我们提出了一个大型印地语预训练数据集，可用于印度语印地语。我们收集了多个领域的数据，包括印地语的主要方言。该数据集包含 12.8 亿个印地语标记。我们已经解释了我们的流程，包括数据收集、预处理和 LLM 预训练的可用性。所提出的方法可以轻松扩展到其他印度语和低资源语言，并且可以免费用于 LLM 预训练和 LLM 研究目的。</li>
</ul>

<h3>Title: sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Tejas Indulal Dhamecha, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09879">https://arxiv.org/abs/2407.09879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09879">https://arxiv.org/pdf/2407.09879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09879]] sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting(https://arxiv.org/abs/2407.09879)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of LLMs in English, there is a significant gap in performance in non-English languages. In order to address this, we introduce a novel recipe for creating a multilingual synthetic instruction tuning dataset, sPhinX, which is created by selectively translating instruction response pairs from English into 50 languages. We test the effectiveness of sPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small and Mistral-7B and then evaluating them across a comprehensive suite of multilingual benchmarks that test reasoning, question answering, and reading comprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned with sPhinX perform better on an average by 4.2%pt and 5%pt respectively as compared to the baselines. We also devise a strategy to incorporate N-shot examples in each fine-tuning sample which further boosts the performance of these models by 3%pt and 10%pt respectively. Additionally, sPhinX also outperforms other multilingual instruction tuning datasets on the same benchmarks along with being sample efficient and diverse, thereby reducing dataset creation costs. Additionally, instruction tuning with sPhinX does not lead to regression on most English benchmarks.</li>
<li><strong>摘要：</strong>尽管 LLM 在英语领域取得了显著成功，但在非英语语言领域，其表现仍存在显著差距。为了解决这个问题，我们引入了一种创建多语言合成指令调整数据集 sPhinX 的新方法，该数据集通过有选择地将英语中的指令响应对翻译成 50 种语言而创建。我们通过使用 sPhinX 对两个最先进的模型 Phi-3-small 和 Mistral-7B 进行微调来测试其有效性，然后在一套全面的多语言基准测试中对它们进行评估，这些基准测试包括测试推理、问答和阅读理解。我们的结果表明，与基线相比，使用 sPhinX 进行微调的 Phi-3-small 和 Mistral-7B 的平均表现分别提高了 4.2%pt 和 5%pt。我们还设计了一种策略，在每个微调样本中加入 N-shot 示例，从而进一步将这些模型的性能分别提高 3%pt 和 10%pt。此外，sPhinX 在相同基准上的表现也优于其他多语言指令调优数据集，同时具有样本效率高、多样性强等特点，从而降低了数据集创建成本。此外，使用 sPhinX 进行指令调优不会导致大多数英语基准上的回归。</li>
</ul>

<h3>Title: Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks</h3>
<ul>
<li><strong>Authors: </strong>Shengbin Yue, Siyuan Wang, Wei Chen, Xuanjing Huang, Zhongyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09893">https://arxiv.org/abs/2407.09893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09893">https://arxiv.org/pdf/2407.09893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09893]] Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks(https://arxiv.org/abs/2407.09893)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have led to significant breakthroughs in various natural language processing tasks. However, generating factually consistent responses in knowledge-intensive scenarios remains a challenge due to issues such as hallucination, difficulty in acquiring long-tailed knowledge, and limited memory expansion. This paper introduces SMART, a novel multi-agent framework that leverages external knowledge to enhance the interpretability and factual consistency of LLM-generated responses. SMART comprises four specialized agents, each performing a specific sub-trajectory action to navigate complex knowledge-intensive tasks. We propose a multi-agent co-training paradigm, Long- and Short-Trajectory Learning, which ensures synergistic collaboration among agents while maintaining fine-grained execution by each agent. Extensive experiments on 5 tasks demonstrate SMART's superior performance compared to previous widely adopted methods.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展已导致各种自然语言处理任务取得重大突破。然而，由于幻觉、获取长尾知识的困难以及内存扩展有限等问题，在知识密集型场景中生成事实一致的响应仍然是一项挑战。本文介绍了 SMART，这是一种新颖的多智能体框架，它利用外部知识来增强 LLM 生成的响应的可解释性和事实一致性。SMART 由四个专门的智能体组成，每个智能体执行特定的子轨迹动作来处理复杂的知识密集型任务。我们提出了一种多智能体协同训练范式，即长轨迹和短轨迹学习，它确保智能体之间的协同合作，同时保持每个智能体的细粒度执行。对 5 个任务的大量实验表明，与以前广泛采用的方法相比，SMART 的性能更优越。</li>
</ul>

<h3>Title: Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated Dialogues</h3>
<ul>
<li><strong>Authors: </strong>KuanChao Chu, Yi-Pei Chen, Hideki Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09897">https://arxiv.org/abs/2407.09897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09897">https://arxiv.org/pdf/2407.09897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09897]] Cohesive Conversations: Enhancing Authenticity in Multi-Agent Simulated Dialogues(https://arxiv.org/abs/2407.09897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>This paper investigates the quality of multi-agent dialogues in simulations powered by Large Language Models (LLMs), focusing on a case study from Park et al. (2023), where 25 agents engage in day-long simulations of life, showcasing complex behaviors and interactions. Analyzing dialogues and memory over multiple sessions revealed significant issues such as repetition, inconsistency, and hallucination, exacerbated by the propagation of erroneous information. To combat these challenges, we propose a novel Screening, Diagnosis, and Regeneration (SDR) framework that detects and corrects utterance errors through a comprehensive process involving immediate issue identification, evidence gathering from past dialogues, and LLM analysis for utterance revision. The effectiveness of the SDR framework is validated through GPT-4 assessments and human evaluations, demonstrating marked improvements in dialogue consistency, diversity, and the reduction of false information. This work presents a pioneering approach to enhancing dialogue quality in multi-agent simulations, establishing a new standard for future research in the field.</li>
<li><strong>摘要：</strong>本文研究了由大型语言模型 (LLM) 驱动的模拟中多智能体对话的质量，重点关注 Park 等人 (2023) 的案例研究，其中 25 个智能体参与了一整天的生活模拟，展示了复杂的行为和交互。通过分析多个会话中的对话和记忆，发现了诸如重复、不一致和幻觉等重大问题，而错误信息的传播加剧了这些问题。为了应对这些挑战，我们提出了一种新颖的筛选、诊断和再生 (SDR) 框架，该框架通过一个全面的过程来检测和纠正话语错误，包括立即识别问题、从过去对话中收集证据以及用于话语修订的 LLM 分析。SDR 框架的有效性通过 GPT-4 评估和人工评估得到验证，表明对话一致性、多样性和虚假信息减少方面有显着改善。这项工作提出了一种提高多智能体模拟对话质量的开创性方法，为该领域的未来研究建立了新的标准。</li>
</ul>

<h3>Title: Minimizing PLM-Based Few-Shot Intent Detectors</h3>
<ul>
<li><strong>Authors: </strong>Haode Zhang, Xiao-Ming Wu, Albert Y.S. Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.09943">https://arxiv.org/abs/2407.09943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.09943">https://arxiv.org/pdf/2407.09943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.09943]] Minimizing PLM-Based Few-Shot Intent Detectors(https://arxiv.org/abs/2407.09943)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent research has demonstrated the feasibility of training efficient intent detectors based on pre-trained language model~(PLM) with limited labeled data. However, deploying these detectors in resource-constrained environments such as mobile devices poses challenges due to their large sizes. In this work, we aim to address this issue by exploring techniques to minimize the size of PLM-based intent detectors trained with few-shot data. Specifically, we utilize large language models (LLMs) for data augmentation, employ a cutting-edge model compression method for knowledge distillation, and devise a vocabulary pruning mechanism called V-Prune. Through these approaches, we successfully achieve a compression ratio of 21 in model memory usage, including both Transformer and the vocabulary, while maintaining almost identical performance levels on four real-world benchmarks.</li>
<li><strong>摘要：</strong>最近的研究表明，使用有限的标记数据，基于预训练语言模型 (PLM) 训练高效意图检测器是可行的。然而，由于这些检测器规模庞大，在资源受限的移动设备等环境中部署它们会面临挑战。在这项工作中，我们旨在通过探索最小化使用少量数据训练的基于 PLM 的意图检测器规模的技术来解决这一问题。具体来说，我们利用大型语言模型 (LLM) 进行数据增强，采用尖端的模型压缩方法进行知识提炼，并设计一种称为 V-Prune 的词汇表修剪机制。通过这些方法，我们成功地实现了模型内存使用率 21 的压缩比，包括 Transformer 和词汇表，同时在四个实际基准测试中保持几乎相同的性能水平。</li>
</ul>

<h3>Title: Causality extraction from medical text using Large Language Models (LLMs)</h3>
<ul>
<li><strong>Authors: </strong>Seethalakshmi Gopalakrishnan, Luciana Garbayo, Wlodek Zadrozny</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10020">https://arxiv.org/abs/2407.10020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10020">https://arxiv.org/pdf/2407.10020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10020]] Causality extraction from medical text using Large Language Models (LLMs)(https://arxiv.org/abs/2407.10020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This study explores the potential of natural language models, including large language models, to extract causal relations from medical texts, specifically from Clinical Practice Guidelines (CPGs). The outcomes causality extraction from Clinical Practice Guidelines for gestational diabetes are presented, marking a first in the field. We report on a set of experiments using variants of BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs), namely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better than other models, including the Large Language Models, with an average F1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less consistency. We also release the code and an annotated a corpus of causal statements within the Clinical Practice Guidelines for gestational diabetes.</li>
<li><strong>摘要：</strong>本研究探索了自然语言模型（包括大型语言模型）从医学文本（特别是临床实践指南 (CPG)）中提取因果关系的潜力。本文介绍了妊娠糖尿病临床实践指南中因果关系提取的结果，这是该领域的首次。我们报告了使用 BERT 变体（BioBERT、DistilBERT 和 BERT）和大型语言模型 (LLM)（即 GPT-4 和 LLAMA2）进行的一组实验。我们的实验表明，BioBERT 的表现优于其他模型（包括大型语言模型），平均 F1 分数为 0.72。GPT-4 和 LLAMA2 结果表现出相似的性能，但一致性较差。我们还发布了妊娠糖尿病临床实践指南中的因果陈述代码和带注释的语料库。</li>
</ul>

<h3>Title: Document-level Clinical Entity and Relation Extraction via Knowledge Base-Guided Generation</h3>
<ul>
<li><strong>Authors: </strong>Kriti Bhattarai, Inez Y. Oh, Zachary B. Abrams, Albert M. Lai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10021">https://arxiv.org/abs/2407.10021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10021">https://arxiv.org/pdf/2407.10021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10021]] Document-level Clinical Entity and Relation Extraction via Knowledge Base-Guided Generation(https://arxiv.org/abs/2407.10021)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Generative pre-trained transformer (GPT) models have shown promise in clinical entity and relation extraction tasks because of their precise extraction and contextual understanding capability. In this work, we further leverage the Unified Medical Language System (UMLS) knowledge base to accurately identify medical concepts and improve clinical entity and relation extraction at the document level. Our framework selects UMLS concepts relevant to the text and combines them with prompts to guide language models in extracting entities. Our experiments demonstrate that this initial concept mapping and the inclusion of these mapped concepts in the prompts improves extraction results compared to few-shot extraction tasks on generic language models that do not leverage UMLS. Further, our results show that this approach is more effective than the standard Retrieval Augmented Generation (RAG) technique, where retrieved data is compared with prompt embeddings to generate results. Overall, we find that integrating UMLS concepts with GPT models significantly improves entity and relation identification, outperforming the baseline and RAG models. By combining the precise concept mapping capability of knowledge-based approaches like UMLS with the contextual understanding capability of GPT, our method highlights the potential of these approaches in specialized domains like healthcare.</li>
<li><strong>摘要：</strong>生成式预训练转换器 (GPT) 模型因其精确的提取和上下文理解能力而在临床实体和关系提取任务中显示出良好的前景。在这项工作中，我们进一步利用统一医学语言系统 (UMLS) 知识库来准确识别医学概念，并在文档级别改进临床实体和关系提取。我们的框架选择与文本相关的 UMLS 概念，并将它们与提示相结合，以指导语言模型提取实体。我们的实验表明，与不利用 UMLS 的通用语言模型上的少量提取任务相比，这种初始概念映射和将这些映射概念纳入提示中可以改善提取结果。此外，我们的结果表明，这种方法比标准的检索增强生成 (RAG) 技术更有效，在标准检索增强生成 (RAG) 技术中，将检索到的数据与提示嵌入进行比较以生成结果。总体而言，我们发现将 UMLS 概念与 GPT 模型相结合可以显着改善实体和关系识别，优于基线和 RAG 模型。通过将 UMLS 等基于知识的方法的精确概念映射能力与 GPT 的上下文理解能力相结合，我们的方法凸显了这些方法在医疗保健等专业领域的潜力。</li>
</ul>

<h3>Title: AutoGRAMS: Autonomous Graphical Agent Modeling Software</h3>
<ul>
<li><strong>Authors: </strong>Ben Krause, Lucia Chen, Emmanuel Kahembwe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10049">https://arxiv.org/abs/2407.10049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10049">https://arxiv.org/pdf/2407.10049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10049]] AutoGRAMS: Autonomous Graphical Agent Modeling Software(https://arxiv.org/abs/2407.10049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>We introduce the AutoGRAMS framework for programming multi-step interactions with language models. AutoGRAMS represents AI agents as a graph, where each node can execute either a language modeling instruction or traditional code. Likewise, transitions in the graph can be governed by either language modeling decisions or traditional branch logic. AutoGRAMS supports using variables as memory and allows nodes to call other AutoGRAMS graphs as functions. We show how AutoGRAMS can be used to design highly sophisticated agents, including self-referential agents that can modify their own graph. AutoGRAMS's graph-centric approach aids interpretability, controllability, and safety during the design, development, and deployment of AI agents. We provide our framework as open source at this https URL .</li>
<li><strong>摘要：</strong>我们引入了 AutoGRAMS 框架，用于使用语言模型对多步骤交互进行编程。AutoGRAMS 将 AI 代理表示为一个图，其中每个节点都可以执行语言建模指令或传统代码。同样，图中的转换可以由语言建模决策或传统分支逻辑控制。AutoGRAMS 支持使用变量作为内存，并允许节点将其他 AutoGRAMS 图作为函数调用。我们展示了如何使用 AutoGRAMS 设计高度复杂的代理，包括可以修改自己图的自引用代理。AutoGRAMS 以图为中心的方法有助于在 AI 代理的设计、开发和部署过程中实现可解释性、可控制性和安全性。我们在此 https URL 上以开源形式提供我们的框架。</li>
</ul>

<h3>Title: Learning to Refuse: Towards Mitigating Privacy Risks in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Wenliang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10058">https://arxiv.org/abs/2407.10058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10058">https://arxiv.org/pdf/2407.10058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10058]] Learning to Refuse: Towards Mitigating Privacy Risks in LLMs(https://arxiv.org/abs/2407.10058)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable capabilities in understanding and generating natural language. However, these models can inadvertently memorize private information, posing significant privacy risks. This study addresses the challenge of enabling LLMs to protect specific individuals' private data without the need for complete retraining. We propose \return, a Real-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from Wikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods for protecting personal data in a realistic scenario. Additionally, we introduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection, which enables the model to learn which individuals' information should be protected without affecting its ability to answer questions related to other unrelated individuals. Our extensive experiments demonstrate that NAUF achieves a state-of-the-art average unlearning score, surpassing the best baseline method by 5.65 points, effectively protecting target individuals' personal data while maintaining the model's general capabilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在理解和生成自然语言方面表现出卓越的能力。然而，这些模型可能会无意中记住私人信息，带来重大的隐私风险。这项研究解决了使 LLM 能够保护特定个人的私人数据而无需完全重新训练的挑战。我们提出了 \return，这是一个真实世界的个人数据学习数据集，包含来自维基百科的 2,492 名个人及其相关的 QA 对，以评估在现实场景中保护个人数据的机器学习 (MU) 方法。此外，我们引入了用于隐私保护的名称感知学习框架 (NAUF)，这使模型能够学习哪些个人的信息应该受到保护，而不会影响其回答与其他无关个人相关的问题的能力。我们广泛的实验表明，NAUF 实现了最先进的平均学习分数，超过最佳基线方法 5.65 分，有效保护目标个人的个人数据，同时保持模型的一般能力。</li>
</ul>

<h3>Title: Multi-Granularity Semantic Revision for Large Language Model Distillation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Liu, Yun Zhang, Wei Li, Simiao Li, Xudong Huang, Hanting Chen, Yehui Tang, Jie Hu, Zhiwei Xiong, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10068">https://arxiv.org/abs/2407.10068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10068">https://arxiv.org/pdf/2407.10068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10068]] Multi-Granularity Semantic Revision for Large Language Model Distillation(https://arxiv.org/abs/2407.10068)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge distillation plays a key role in compressing the Large Language Models (LLMs), which boosts a small-size student model under large teacher models' guidance. However, existing LLM distillation methods overly rely on student-generated outputs, which may introduce generation errors and misguide the distillation process. Moreover, the distillation loss functions introduced in previous art struggle to align the most informative part due to the complex distribution of LLMs' outputs. To address these problems, we propose a multi-granularity semantic revision method for LLM distillation. At the sequence level, we propose a sequence correction and re-generation (SCRG) strategy. SCRG first calculates the semantic cognitive difference between the teacher and student to detect the error token, then corrects it with the teacher-generated one, and re-generates the sequence to reduce generation errors and enhance generation diversity. At the token level, we design a distribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the distillation objective function. DAC-KL loss exploits a learnable sub-network to adaptively extract semantically dense areas from the teacher's output, avoiding the interference of redundant information in the distillation process. Finally, at the span level, we leverage the span priors of a sequence to compute the probability correlations within spans, and constrain the teacher and student's probability correlations to be consistent, further enhancing the transfer of semantic information. Extensive experiments across different model families with parameters ranging from 0.1B to 13B demonstrate the superiority of our method compared to existing methods.</li>
<li><strong>摘要：</strong>知识蒸馏在压缩大型语言模型（LLM）中起着关键作用，它可以在大型教师模型的指导下提升小型学生模型。然而，现有的LLM蒸馏方法过度依赖学生生成的输出，这可能会引入生成错误并误导蒸馏过程。此外，由于LLM输出的分布复杂，现有技术中引入的蒸馏损失函数难以对齐最有用的部分。针对这些问题，我们提出了一种用于LLM蒸馏的多粒度语义修订方法。在序列级别，我们提出了一种序列校正和重新生成（SCRG）策略。SCRG首先计算老师和学生之间的语义认知差异来检测错误标记，然后用老师生成的标记进行校正，并重新生成序列以减少生成错误并增强生成多样性。在标记级别，我们设计了一个分布自适应裁剪Kullback-Leibler（DAC-KL）损失作为蒸馏目标函数。 DAC-KL 损失利用可学习的子网络自适应地从教师输出中提取语义密集区域，避免蒸馏过程中冗余信息的干扰。最后，在跨度级别，我们利用序列的跨度先验来计算跨度内的概率相关性，并约束教师和学生的概率相关性保持一致，进一步增强语义信息的传递。在不同模型系列（参数范围从 0.1B 到 13B）上进行的大量实验证明了我们的方法与现有方法相比的优越性。</li>
</ul>

<h3>Title: Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine</h3>
<ul>
<li><strong>Authors: </strong>Omid Rohanian, Mohammadmahdi Nouriborji, Olena Seminog, Rodrigo Furst, Thomas Mendy, Shanthi Levanita, Zaharat Kadri-Alab, Nusrat Jabin, Daniela Toale, Georgina Humphreys, Emilia Antonio, Adrian Bucher, Alice Norton, David A. Clifton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10086">https://arxiv.org/abs/2407.10086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10086">https://arxiv.org/pdf/2407.10086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10086]] Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine(https://arxiv.org/abs/2407.10086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces the Pandemic PACT Advanced Categorisation Engine (PPACE) along with its associated dataset. PPACE is a fine-tuned model developed to automatically classify research abstracts from funded biomedical projects according to WHO-aligned research priorities. This task is crucial for monitoring research trends and identifying gaps in global health preparedness and response. Our approach builds on human-annotated projects, which are allocated one or more categories from a predefined list. A large language model is then used to generate `rationales' explaining the reasoning behind these annotations. This augmented data, comprising expert annotations and rationales, is subsequently used to fine-tune a smaller, more efficient model. Developed as part of the Pandemic PACT project, which aims to track and analyse research funding and clinical evidence for a wide range of diseases with outbreak potential, PPACE supports informed decision-making by research funders, policymakers, and independent researchers. We introduce and release both the trained model and the instruction-based dataset used for its training. Our evaluation shows that PPACE significantly outperforms its baselines. The release of PPACE and its associated dataset offers valuable resources for researchers in multilabel biomedical document classification and supports advancements in aligning biomedical research with key global health priorities.</li>
<li><strong>摘要：</strong>本文介绍了 Pandemic PACT 高级分类引擎 (PPACE) 及其相关数据集。PPACE 是一种经过微调的模型，旨在根据与 WHO 一致的研究重点自动对资助的生物医学项目的研究摘要进行分类。这项任务对于监测研究趋势和确定全球卫生准备和响应方面的差距至关重要。我们的方法建立在人工注释的项目之上，这些项目从预定义列表中分配一个或多个类别。然后使用大型语言模型生成“原理”来解释这些注释背后的原因。随后使用这些增强数据（包括专家注释和原理）来微调更小、更高效的模型。PPACE 是作为 Pandemic PACT 项目的一部分开发的，该项目旨在跟踪和分析具有爆发潜力的各种疾病的研究资金和临床证据，它支持研究资助者、政策制定者和独立研究人员做出明智的决策。我们介绍并发布了经过训练的模型和用于训练的基于指令的数据集。我们的评估表明，PPACE 的表现明显优于其基线。 PPACE 及其相关数据集的发布为多标签生物医学文档分类的研究人员提供了宝贵的资源，并支持将生物医学研究与全球关键卫生重点相结合。</li>
</ul>

<h3>Title: Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT and Seq2Seq Models for Free-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Ge Gao, Jongin Kim, Sejin Paik, Ekaterina Novozhilova, Yi Liu, Sarah T. Bonna, Margrit Betke, Derry Tanti Wijaya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10091">https://arxiv.org/abs/2407.10091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10091">https://arxiv.org/pdf/2407.10091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10091]] Enhancing Emotion Prediction in News Headlines: Insights from ChatGPT and Seq2Seq Models for Free-Text Generation(https://arxiv.org/abs/2407.10091)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Predicting emotions elicited by news headlines can be challenging as the task is largely influenced by the varying nature of people's interpretations and backgrounds. Previous works have explored classifying discrete emotions directly from news headlines. We provide a different approach to tackling this problem by utilizing people's explanations of their emotion, written in free-text, on how they feel after reading a news headline. Using the dataset BU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the free-text explanations have a strong correlation with the dominant emotion elicited by the headlines. The free-text explanations also contain more sentimental context than the news headlines alone and can serve as a better input to emotion classification models. Therefore, in this work we explored generating emotion explanations from headlines by training a sequence-to-sequence transformer model and by using pretrained large language model, ChatGPT (GPT-4). We then used the generated emotion explanations for emotion classification. In addition, we also experimented with training the pretrained T5 model for the intermediate task of explanation generation before fine-tuning it for emotion classification. Using McNemar's significance test, methods that incorporate GPT-generated free-text emotion explanations demonstrated significant improvement (P-value < 0.05) in emotion classification from headlines, compared to methods that only use headlines. This underscores the value of using intermediate free-text explanations for emotion prediction tasks with headlines.</li>
<li><strong>摘要：</strong>预测新闻标题引发的情绪可能具有挑战性，因为这项任务在很大程度上受到人们的解释和背景的不同性质的影响。先前的研究已经探索了直接从新闻标题中对离散情绪进行分类。我们提供了一种不同的方法来解决这个问题，即利用人们对阅读新闻标题后情绪的自由文本解释。使用数据集 BU-NEmo+ (Gao et al., 2022)，我们发现对于情绪分类，自由文本解释与标题引发的主导情绪具有很强的相关性。自由文本解释还包含比单独的新闻标题更多的情感背景，可以作为情绪分类模型的更好输入。因此，在这项工作中，我们探索了通过训练序列到序列转换器模型和使用预训练的大型语言模型 ChatGPT (GPT-4) 从标题生成情绪解释。然后，我们使用生成的情绪解释进行情绪分类。此外，我们还尝试对预训练的 T5 模型进行训练，以完成解释生成的中间任务，然后再对其进行微调以进行情绪分类。使用 McNemar 显著性检验，与仅使用标题的方法相比，结合 GPT 生成的自由文本情绪解释的方法在标题情绪分类方面表现出显著的改进（P 值 < 0.05）。这凸显了使用中间自由文本解释进行带有标题的情绪预测任务的价值。</li>
</ul>

<h3>Title: TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation</h3>
<ul>
<li><strong>Authors: </strong>Roni Goldshmidt, Miriam Horovicz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10114">https://arxiv.org/abs/2407.10114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10114">https://arxiv.org/pdf/2407.10114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10114]] TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation(https://arxiv.org/abs/2407.10114)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly prevalent in critical applications, the need for interpretable AI has grown. We introduce TokenSHAP, a novel method for interpreting LLMs by attributing importance to individual tokens or substrings within input prompts. This approach adapts Shapley values from cooperative game theory to natural language processing, offering a rigorous framework for understanding how different parts of an input contribute to a model's response. TokenSHAP leverages Monte Carlo sampling for computational efficiency, providing interpretable, quantitative measures of token importance. We demonstrate its efficacy across diverse prompts and LLM architectures, showing consistent improvements over existing baselines in alignment with human judgments, faithfulness to model behavior, and consistency. Our method's ability to capture nuanced interactions between tokens provides valuable insights into LLM behavior, enhancing model transparency, improving prompt engineering, and aiding in the development of more reliable AI systems. TokenSHAP represents a significant step towards the necessary interpretability for responsible AI deployment, contributing to the broader goal of creating more transparent, accountable, and trustworthy AI systems.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在关键应用中越来越普遍，对可解释 AI 的需求也随之增长。我们引入了 TokenSHAP，这是一种通过赋予输入提示中的各个标记或子字符串重要性来解释 LLM 的新方法。这种方法将 Shapley 值从合作博弈论调整到自然语言处理，提供了一个严格的框架来理解输入的不同部分如何影响模型的响应。TokenSHAP 利用蒙特卡洛抽样来提高计算效率，提供可解释的定量标记重要性度量。我们展示了它在各种提示和 LLM 架构中的有效性，显示出与现有基线相比的持续改进，符合人类判断、忠实于模型行为和一致性。我们的方法能够捕捉标记之间的细微交互，为 LLM 行为提供了宝贵的见解，增强了模型透明度，改进了提示工程，并有助于开发更可靠的 AI 系统。TokenSHAP 代表着朝着负责任的 AI 部署所需的可解释性迈出了重要一步，有助于实现创建更透明、更负责和更值得信赖的 AI 系统的更广泛目标。</li>
</ul>

<h3>Title: Look Within, Why LLMs Hallucinate: A Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>He Li, Haoang Chi, Mingyu Liu, Wenjing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10153">https://arxiv.org/abs/2407.10153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10153">https://arxiv.org/pdf/2407.10153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10153]] Look Within, Why LLMs Hallucinate: A Causal Perspective(https://arxiv.org/abs/2407.10153)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) is a milestone in generative artificial intelligence, achieving significant success in text comprehension and generation tasks. Despite the tremendous success of LLMs in many downstream tasks, they suffer from severe hallucination problems, posing significant challenges to the practical applications of LLMs. Most of the works about LLMs' hallucinations focus on data quality. Self-attention is a core module in transformer-based LLMs, while its potential relationship with LLMs' hallucination has been hardly investigated. To fill this gap, we study this problem from a causal perspective. We propose a method to intervene in LLMs' self-attention layers and maintain their structures and sizes intact. Specifically, we disable different self-attention layers in several popular open-source LLMs and then compare their degrees of hallucination with the original ones. We evaluate the intervened LLMs on hallucination assessment benchmarks and conclude that disabling some specific self-attention layers in the front or tail of the LLMs can alleviate hallucination issues. The study paves a new way for understanding and mitigating LLMs' hallucinations.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的出现是生成式人工智能的一个里程碑，在文本理解和生成任务中取得了重大成功。尽管LLM在许多下游任务中取得了巨大的成功，但它们存在严重的幻觉问题，对LLM的实际应用构成了重大挑战。大多数关于LLM幻觉的研究都集中在数据质量上。自注意力是基于Transformer的LLM的核心模块，但它与LLM幻觉的潜在关系却很少被研究。为了填补这一空白，我们从因果的角度研究了这个问题。我们提出了一种干预LLM自注意力层的方法，并保持其结构和大小不变。具体来说，我们在几个流行的开源LLM中禁用了不同的自注意力层，然后将它们的幻觉程度与原始层进行比较。我们在幻觉评估基准上评估了干预后的LLM，并得出结论，禁用LLM前端或尾部的一些特定自注意力层可以缓解幻觉问题。这项研究为理解和缓解法学硕士的幻觉开辟了一条新途径。</li>
</ul>

<h3>Title: Key-Point-Driven Mathematical Reasoning Distillation of Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10167">https://arxiv.org/abs/2407.10167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10167">https://arxiv.org/pdf/2407.10167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10167]] Key-Point-Driven Mathematical Reasoning Distillation of Large Language Model(https://arxiv.org/abs/2407.10167)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional proficiency in mathematical reasoning tasks due to their extensive parameter counts and training on vast datasets. Despite these capabilities, deploying LLMs is hindered by their computational demands. Distilling LLM mathematical reasoning into Smaller Language Models (SLMs) has emerged as a solution to this challenge, although these smaller models often suffer from errors in calculation and semantic understanding. Prior work has proposed Program-of-Thought Distillation (PoTD) to avoid calculation error. To further address semantic understanding errors, we propose Key-Point-Driven Mathematical Reasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs by breaking down the problem-solving process into three stages: Core Question Extraction, Problem-Solving Information Extraction, and Step-by-Step Solution. This method is further divided into KPDD-CoT, which generates Chain-of-Thought rationales, and KPDD-PoT, which creates Program-of-Thought rationales. The experiment results show that KPDD-CoT significantly improves reasoning abilities, while KPDD-PoT achieves state-of-the-art performance in mathematical reasoning tasks. Our approach effectively mitigates misunderstanding errors, advancing the deployment of efficient and capable SLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其广泛的参数数量和对海量数据集的训练而表现出在数学推理任务中的卓越能力。尽管具有这些功能，但部署 LLM 仍受到其计算需求的阻碍。将 LLM 数学推理提炼为较小的语言模型 (SLM) 已成为解决这一挑战的解决方案，尽管这些较小的模型经常会出现计算和语义理解错误。先前的工作提出了思维程序提炼 (PoTD) 以避免计算错误。为了进一步解决语义理解错误，我们提出了关键点驱动的数学推理提炼 (KPDD)。KPDD 通过将问题解决过程分解为三个阶段来增强 SLM 的推理性能：核心问题提取、问题解决信息提取和分步解决。该方法进一步分为生成思维链原理的 KPDD-CoT 和创建思维程序原理的 KPDD-PoT。实验结果表明，KPDD-CoT 显著提高了推理能力，而 KPDD-PoT 在数学推理任务中取得了最佳表现。我们的方法有效地减少了误解错误，促进了高效且功能强大的 SLM 的部署。</li>
</ul>

<h3>Title: BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhiting Fan, Ruizhe Chen, Ruiling Xu, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10241">https://arxiv.org/abs/2407.10241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10241">https://arxiv.org/pdf/2407.10241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10241]] BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs(https://arxiv.org/abs/2407.10241)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Evaluating the bias in Large Language Models (LLMs) becomes increasingly crucial with their rapid development. However, existing evaluation methods rely on fixed-form outputs and cannot adapt to the flexible open-text generation scenarios of LLMs (e.g., sentence completion and question answering). To address this, we introduce BiasAlert, a plug-and-play tool designed to detect social bias in open-text generations of LLMs. BiasAlert integrates external human knowledge with inherent reasoning capabilities to detect bias reliably. Extensive experiments demonstrate that BiasAlert significantly outperforms existing state-of-the-art methods like GPT4-as-A-Judge in detecting bias. Furthermore, through application studies, we demonstrate the utility of BiasAlert in reliable LLM bias evaluation and bias mitigation across various scenarios. Model and code will be publicly released.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的快速发展，评估其中的偏见变得越来越重要。然而，现有的评估方法依赖于固定形式的输出，无法适应 LLM 灵活的开放文本生成场景（例如，句子完成和问答）。为了解决这个问题，我们推出了 BiasAlert，这是一种即插即用的工具，旨在检测 LLM 开放文本生成中的社会偏见。BiasAlert 将外部人类知识与固有推理能力相结合，以可靠地检测偏见。大量实验表明，BiasAlert 在检测偏见方面明显优于现有的最先进方法，如 GPT4-as-A-Judge。此外，通过应用研究，我们展示了 BiasAlert 在各种场景中可靠的 LLM 偏见评估和偏见缓解中的实用性。模型和代码将公开发布。</li>
</ul>

<h3>Title: GenSco: Can Question Decomposition based Passage Alignment improve Question Answering?</h3>
<ul>
<li><strong>Authors: </strong>Barah Fazili, Koustava Goswami, Natwar Modani, Inderjeet Nair</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10245">https://arxiv.org/abs/2407.10245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10245">https://arxiv.org/pdf/2407.10245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10245]] GenSco: Can Question Decomposition based Passage Alignment improve Question Answering?(https://arxiv.org/abs/2407.10245)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval augmented generation (RAG) with large language models (LLMs) for Question Answering (QA) entails furnishing relevant context within the prompt to facilitate the LLM in answer generation. During the generation, inaccuracies or hallucinations frequently occur due to two primary factors: inadequate or distracting context in the prompts, and the inability of LLMs to effectively reason through the facts. In this paper, we investigate whether providing aligned context via a carefully selected passage sequence leads to better answer generation by the LLM for multi-hop QA. We introduce, "GenSco", a novel approach of selecting passages based on the predicted decomposition of the multi-hop questions}. The framework consists of two distinct LLMs: (i) Generator LLM, which is used for question decomposition and final answer generation; (ii) an auxiliary open-sourced LLM, used as the scorer, to semantically guide the Generator for passage selection. The generator is invoked only once for the answer generation, resulting in a cost-effective and efficient approach. We evaluate on three broadly established multi-hop question answering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve an absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect to the best performing baselines over MuSiQue and 2WikiMultiHop respectively.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 进行问答 (QA) 的检索增强生成 (RAG) 需要在提示中提供相关上下文，以方便 LLM 生成答案。在生成过程中，由于两个主要因素，经常会出现不准确或幻觉：提示中的上下文不足或分散注意力，以及 LLM 无法有效地推理事实。在本文中，我们研究通过精心选择的段落序列提供对齐的上下文是否会导致 LLM 为多跳 QA 生成更好的答案。我们引入了“GenSco”，这是一种基于多跳问题的预测分解来选择段落的新方法。该框架由两个不同的 LLM 组成：(i) 生成器 LLM，用于问题分解和最终答案生成；(ii) 辅助开源 LLM，用作评分器，在语义上指导生成器进行段落选择。生成器仅调用一次以生成答案，从而产生一种经济高效的方法。我们对三个广泛建立的多跳问答数据集进行了评估：2WikiMultiHop、Adversarial HotPotQA 和 MuSiQue，并分别相对于 MuSiQue 和 2WikiMultiHop 上表现最佳的基线，在精确匹配得分方面获得了 $15.1$ 和 $5.9$ 分的绝对增益。</li>
</ul>

<h3>Title: Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a Simple Contrastive Learning based Approach</h3>
<ul>
<li><strong>Authors: </strong>Aditi Khandelwal, Harman Singh, Hengrui Gu, Tianlong Chen, Kaixiong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Cross-Lingual Multi-Hop Knowledge Editing -- Benchmarks, Analysis and a Simple Contrastive Learning based Approach(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models are often expected to constantly adapt to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual knowledge editing in English, even though new information can emerge in any language from any part of the world. We propose the Cross-Lingual Multi-Hop Knowledge Editing paradigm, for measuring and analyzing the performance of various SoTA knowledge editing techniques in a cross-lingual setup. Specifically, we create a parallel cross-lingual benchmark, CROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive analysis over various knowledge editing techniques uncover significant gaps in performance between the cross-lingual and English-centric setting. Following this, we propose a significantly improved system for cross-lingual multi-hop knowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and generate knowledge editing framework, where a retriever is formulated to recall edited facts and support an LLM to adhere to knowledge edits. We develop language-aware and hard-negative based contrastive objectives for improving the cross-lingual and fine-grained fact retrieval and verification process used in this framework. Extensive experiments on three LLMs, eight languages, and two datasets show CLEVER-CKE's significant gains of up to 30% over prior methods.</li>
<li><strong>摘要：</strong>大型语言模型通常需要不断适应新的知识来源，而知识编辑技术旨在以最少的修改有效地修补过时的模型知识。尽管新信息可能以任何语言出现在世界任何地方，但大多数先前的研究都侧重于英语的单语知识编辑。我们提出了跨语言多跳知识编辑范式，用于测量和分析跨语言设置中各种 SoTA 知识编辑技术的性能。具体来说，我们创建了一个并行跨语言基准 CROLIN-MQUAKE 来测量知识编辑能力。我们对各种知识编辑技术的广泛分析发现，跨语言和以英语为中心的设置之间存在显著的性能差距。在此之后，我们提出了一个显著改进的跨语言多跳知识编辑系统 CLEVER-CKE。CLEVER-CKE 基于检索、验证和生成知识编辑框架，其中检索器被设计为回忆已编辑的事实并支持 LLM 遵守知识编辑。我们开发了语言感知和基于硬否定的对比目标，以改进此框架中使用的跨语言和细粒度事实检索和验证过程。对三个 LLM、八种语言和两个数据集进行的大量实验表明，CLEVER-CKE 比以前的方法提高了高达 30% 的显著收益。</li>
</ul>

<h3>Title: Comparing Complex Concepts with Transformers: Matching Patent Claims Against Natural Language Text</h3>
<ul>
<li><strong>Authors: </strong>Matthias Blume, Ghobad Heidari, Christoph Hewel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10351">https://arxiv.org/abs/2407.10351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10351">https://arxiv.org/pdf/2407.10351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10351]] Comparing Complex Concepts with Transformers: Matching Patent Claims Against Natural Language Text(https://arxiv.org/abs/2407.10351)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>A key capability in managing patent applications or a patent portfolio is comparing claims to other text, e.g. a patent specification. Because the language of claims is different from language used elsewhere in the patent application or in non-patent text, this has been challenging for computer based natural language processing. We test two new LLM-based approaches and find that both provide substantially better performance than previously published values. The ability to match dense information from one domain against much more distributed information expressed in a different vocabulary may also be useful beyond the intellectual property space.</li>
<li><strong>摘要：</strong>管理专利申请或专利组合的一项关键能力是将权利要求与其他文本（例如专利说明书）进行比较。由于权利要求的语言与专利申请或非专利文本中使用的语言不同，因此这对于基于计算机的自然语言处理来说是一个挑战。我们测试了两种新的基于 LLM 的方法，发现它们的性能都比之前发布的值好得多。将一个领域的密集信息与用不同词汇表达的更分散的信息进行匹配的能力在知识产权领域之外也可能很有用。</li>
</ul>

<h3>Title: By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting</h3>
<ul>
<li><strong>Authors: </strong>Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10385">https://arxiv.org/abs/2407.10385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10385">https://arxiv.org/pdf/2407.10385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10385]] By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting(https://arxiv.org/abs/2407.10385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. We propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). We design a visual prompt that directs MLLMs to utilize visualized sensor data alongside the target sensory task descriptions. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy than text-based prompts and reducing token costs by 15.8x. Our findings highlight the effectiveness and cost-efficiency of visual prompts with MLLMs for various sensory tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已在各个领域展现出卓越的能力。然而，将 LLM 用于无处不在的传感应用仍然具有挑战性，因为现有的文本提示方法在处理长传感器数据序列时会显著降低性能。我们提出了一种使用多模态 LLM (MLLM) 的传感器数据视觉提示方法。我们设计了一个视觉提示，指导 MLLM 利用可视化的传感器数据以及目标传感任务描述。此外，我们引入了一个可视化生成器，它可以自动创建针对给定传感任务的最佳可视化，从而无需事先了解特定于任务的知识。我们在涉及四种传感模态的九个传感任务上评估了我们的方法，平均比基于文本的提示高出 10% 的准确率，并将令牌成本降低了 15.8 倍。我们的研究结果强调了使用 MLLM 进行视觉提示对各种传感任务的有效性和成本效益。</li>
</ul>

<h3>Title: Enhancing Medication Recommendation with LLM Text Representation</h3>
<ul>
<li><strong>Authors: </strong>Yu-Tzu Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10453">https://arxiv.org/abs/2407.10453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10453">https://arxiv.org/pdf/2407.10453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10453]] Enhancing Medication Recommendation with LLM Text Representation(https://arxiv.org/abs/2407.10453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Most of the existing medication recommendation models are predicted with only structured data such as medical codes, with the remaining other large amount of unstructured or semi-structured data underutilization. To increase the utilization effectively, we proposed a method of enhancing medication recommendation with Large Language Model (LLM) text representation. LLM harnesses powerful language understanding and generation capabilities, enabling the extraction of information from complex and lengthy unstructured data such as clinical notes which contain complex terminology. This method can be applied to several existing base models we selected and improve medication recommendation performance with the combination representation of text and medical codes experiments on two different datasets. LLM text representation alone can even demonstrate a comparable ability to the medical code representation alone. Overall, this is a general method that can be applied to other models for improved recommendations.</li>
<li><strong>摘要：</strong>现有的药物推荐模型大多仅使用医疗代码等结构化数据进行预测，其余大量非结构化或半结构化数据未得到充分利用。为了有效提高利用率，我们提出了一种利用大型语言模型 (LLM) 文本表示增强药物推荐的方法。LLM 利用强大的语言理解和生成能力，能够从复杂且冗长的非结构化数据（例如包含复杂术语的临床笔记）中提取信息。该方法可应用于我们选择的几个现有基础模型，并通过在两个不同数据集上进行文本和医疗代码组合表示实验来提高药物推荐性能。单独的 LLM 文本表示甚至可以表现出与单独的医疗代码表示相当的能力。总的来说，这是一种可以应用于其他模型以改进推荐的通用方法。</li>
</ul>

<h3>Title: The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism</h3>
<ul>
<li><strong>Authors: </strong>Yifan Song, Guoyin Wang, Sujian Li, Bill Yuchen Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10457">https://arxiv.org/abs/2407.10457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10457">https://arxiv.org/pdf/2407.10457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10457]] The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism(https://arxiv.org/abs/2407.10457)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks' consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance. Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation.</li>
<li><strong>摘要：</strong>当前对大型语言模型 (LLM) 的评估往往忽视了非确定性，通常关注每个示例的单个输出。这限制了我们对实际应用中 LLM 性能变化的理解。我们的研究通过探索贪婪解码和采样之间的性能差异的关键问题、确定基准在非确定性方面的一致性以及检查独特的模型行为来解决此问题。通过大量实验，我们观察到贪婪解码通常优于大多数评估任务的采样方法。我们还观察到不同 LLM 大小和对齐方法之间的一致性能，并注意到对齐可以减少采样方差。此外，我们的最佳 N 采样方法表明较小的 LLM 可以匹配或超越 GPT-4-Turbo 等较大的模型，凸显了较小 LLM 尚未开发的潜力。这项研究表明了在 LLM 评估中考虑非确定性的重要性，并为未来的 LLM 开发和评估提供了见解。</li>
</ul>

<h3>Title: How and where does CLIP process negation?</h3>
<ul>
<li><strong>Authors: </strong>Vincent Quantmeyer, Pablo Mosteiro, Albert Gatt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10488">https://arxiv.org/abs/2407.10488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10488">https://arxiv.org/pdf/2407.10488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10488]] How and where does CLIP process negation?(https://arxiv.org/abs/2407.10488)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Various benchmarks have been proposed to test linguistic understanding in pre-trained vision \& language (VL) models. Here we build on the existence task from the VALSE benchmark (Parcalabescu et al, 2022) which we use to test models' understanding of negation, a particularly interesting issue for multimodal models. However, while such VL benchmarks are useful for measuring model performance, they do not reveal anything about the internal processes through which these models arrive at their outputs in such visio-linguistic tasks. We take inspiration from the growing literature on model interpretability to explain the behaviour of VL models on the understanding of negation. Specifically, we approach these questions through an in-depth analysis of the text encoder in CLIP (Radford et al, 2021), a highly influential VL model. We localise parts of the encoder that process negation and analyse the role of attention heads in this task. Our contributions are threefold. We demonstrate how methods from the language model interpretability literature (such as causal tracing) can be translated to multimodal models and tasks; we provide concrete insights into how CLIP processes negation on the VALSE existence task; and we highlight inherent limitations in the VALSE dataset as a benchmark for linguistic understanding.</li>
<li><strong>摘要：</strong>已经提出了各种基准来测试预训练视觉和语言 (VL) 模型中的语言理解能力。在这里，我们以 VALSE 基准 (Parcalabescu et al, 2022) 中的存在任务为基础，我们用它来测试模型对否定的理解，这对于多模态模型来说是一个特别有趣的问题。然而，虽然这样的 VL 基准对于衡量模型性能很有用，但它们并没有揭示这些模型在这种视觉语言任务中得出输出的内部过程。我们从越来越多的模型可解释性文献中汲取灵感，以解释 VL 模型在否定理解方面的行为。具体来说，我们通过深入分析 CLIP (Radford et al, 2021) 中的文本编码器来探讨这些问题，CLIP 是一个极具影响力的 VL 模型。我们定位了编码器中处理否定的部分，并分析了注意力头在此任务中的作用。我们的贡献有三方面。我们展示了如何将语言模型可解释性文献中的方法（例如因果追踪）转化为多模态模型和任务；我们对 CLIP 如何处理 VALSE 存在任务中的否定提供了具体的见解；并且我们强调了 VALSE 数据集作为语言理解基准的固有局限性。</li>
</ul>

<h3>Title: CIBench: Evaluating Your LLMs with a Code Interpreter Plugin</h3>
<ul>
<li><strong>Authors: </strong>Songyang Zhang, Chuyu Zhang, Yingfan Hu, Haowen Shen, Kuikun Liu, Zerun Ma, Fengzhe Zhou, Wenwei Zhang, Xuming He, Dahua Lin, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10499">https://arxiv.org/abs/2407.10499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10499">https://arxiv.org/pdf/2407.10499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10499]] CIBench: Evaluating Your LLMs with a Code Interpreter Plugin(https://arxiv.org/abs/2407.10499)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>While LLM-Based agents, which use external tools to solve complex problems, have made significant progress, benchmarking their ability is challenging, thereby hindering a clear understanding of their limitations. In this paper, we propose an interactive evaluation framework, named CIBench, to comprehensively assess LLMs' ability to utilize code interpreters for data science tasks. Our evaluation framework includes an evaluation dataset and two evaluation modes. The evaluation dataset is constructed using an LLM-human cooperative approach and simulates an authentic workflow by leveraging consecutive and interactive IPython sessions. The two evaluation modes assess LLMs' ability with and without human assistance. We conduct extensive experiments to analyze the ability of 24 LLMs on CIBench and provide valuable insights for future LLMs in code interpreter utilization.</li>
<li><strong>摘要：</strong>虽然使用外部工具解决复杂问题的基于 LLM 的代理已经取得了重大进展，但对其能力进行基准测试却具有挑战性，从而阻碍了对它们的局限性的清晰认识。在本文中，我们提出了一个交互式评估框架 CIBench，以全面评估 LLM 利用代码解释器执行数据科学任务的能力。我们的评估框架包括一个评估数据集和两种评估模式。评估数据集采用 LLM 与人类合作的方法构建，并通过利用连续和交互式 IPython 会话来模拟真实的工作流程。两种评估模式分别评估 LLM 在有无人工协助下的能力。我们进行了广泛的实验，以分析 24 个 LLM 在 CIBench 上的能力，并为未来 LLM 在代码解释器利用方面的发展提供宝贵的见解。</li>
</ul>

<h3>Title: TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xingzhi Zhou, Xin Dong, Chunhao Li, Yuning Bai, Yulong Xu, Ka Chun Cheung, Simon See, Xinpeng Song, Runshun Zhang, Xuezhong Zhou, Nevin L. Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10510">https://arxiv.org/abs/2407.10510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10510">https://arxiv.org/pdf/2407.10510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10510]] TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction(https://arxiv.org/abs/2407.10510)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Traditional Chinese medicine (TCM) relies on specific combinations of herbs in prescriptions to treat symptoms and signs, a practice that spans thousands of years. Predicting TCM prescriptions presents a fascinating technical challenge with practical implications. However, this task faces limitations due to the scarcity of high-quality clinical datasets and the intricate relationship between symptoms and herbs. To address these issues, we introduce DigestDS, a new dataset containing practical medical records from experienced experts in digestive system diseases. We also propose a method, TCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs) through supervised fine-tuning on DigestDS. Additionally, we enhance computational efficiency using a low-rank adaptation technique. TCM-FTP also incorporates data augmentation by permuting herbs within prescriptions, capitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves an F1-score of 0.8031, surpassing previous methods significantly. Furthermore, it demonstrates remarkable accuracy in dosage prediction, achieving a normalized mean square error of 0.0604. In contrast, LLMs without fine-tuning perform poorly. Although LLMs have shown capabilities on a wide range of tasks, this work illustrates the importance of fine-tuning for TCM prescription prediction, and we have proposed an effective way to do that.</li>
<li><strong>摘要：</strong>传统中医 (TCM) 依靠处方中的特定草药组合来治疗症状和体征，这种做法已有数千年历史。预测中医处方是一项极具挑战性的技术挑战，具有实际意义。然而，由于高质量临床数据集的稀缺以及症状和草药之间的复杂关系，这项任务面临限制。为了解决这些问题，我们引入了 DigestDS，这是一个新的数据集，其中包含来自消化系统疾病经验丰富的专家的实际医疗记录。我们还提出了一种方法，TCM-FTP（TCM 微调预训练），通过对 DigestDS 进行监督微调来利用预训练的大型语言模型 (LLM)。此外，我们使用低秩自适应技术提高了计算效率。TCM-FTP 还通过排列处方中的草药来整合数据增强，利用其顺序不可知的特性。令人印象深刻的是，TCM-FTP 的 F1 得分达到 0.8031，大大超过了以前的方法。此外，它在剂量预测方面表现出了惊人的准确性，实现了 0.0604 的归一化均方误差。相比之下，没有微调的 LLM 表现不佳。尽管 LLM 已在广泛的任务上表现出色，但这项工作说明了微调对于中药处方预测的重要性，我们提出了一种有效的方法。</li>
</ul>

<h3>Title: Beyond Generative Artificial Intelligence: Roadmap for Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>María Miró Maestre, Iván Martínez-Murillo, Tania J. Martin, Borja Navarro-Colorado, Antonio Ferrández, Armando Suárez Cueto, Elena Lloret</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10554">https://arxiv.org/abs/2407.10554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10554">https://arxiv.org/pdf/2407.10554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10554]] Beyond Generative Artificial Intelligence: Roadmap for Natural Language Generation(https://arxiv.org/abs/2407.10554)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence has grown exponentially as a result of Large Language Models (LLMs). This has been possible because of the impressive performance of deep learning methods created within the field of Natural Language Processing (NLP) and its subfield Natural Language Generation (NLG), which is the focus of this paper. Within the growing LLM family are the popular GPT-4, Bard and more specifically, tools such as ChatGPT have become a benchmark for other LLMs when solving most of the tasks involved in NLG research. This scenario poses new questions about the next steps for NLG and how the field can adapt and evolve to deal with new challenges in the era of LLMs. To address this, the present paper conducts a review of a representative sample of surveys recently published in NLG. By doing so, we aim to provide the scientific community with a research roadmap to identify which NLG aspects are still not suitably addressed by LLMs, as well as suggest future lines of research that should be addressed going forward.</li>
<li><strong>摘要：</strong>由于大型语言模型 (LLM) 的出现，生成式人工智能呈指数级增长。这得益于自然语言处理 (NLP) 领域及其子领域自然语言生成 (NLG) 中深度学习方法的出色表现，而自然语言生成 (NLG) 正是本文的重点。在日益壮大的 LLM 家族中，流行的 GPT-4、Bard 以及 ChatGPT 等工具已成为其他 LLM 在解决 NLG 研究所涉及的大多数任务时的基准。这种情况提出了关于 NLG 下一步发展的新问题，以及该领域如何适应和发展以应对 LLM 时代的新挑战。为了解决这个问题，本文对最近在 NLG 上发表的代表性调查样本进行了回顾。通过这样做，我们旨在为科学界提供一份研究路线图，以确定哪些 NLG 方面仍不适合由 LLM 解决，并提出未来应解决的研究方向。</li>
</ul>

<h3>Title: Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection</h3>
<ul>
<li><strong>Authors: </strong>Barah Fazili, Ashish Sunil Agrawal, Preethi Jyothi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10582">https://arxiv.org/abs/2407.10582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10582">https://arxiv.org/pdf/2407.10582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10582]] Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection(https://arxiv.org/abs/2407.10582)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are very proficient text generators. We leverage this capability of LLMs to generate task-specific data via zero-shot prompting and promote cross-lingual transfer for low-resource target languages. Given task-specific data in a source language and a teacher model trained on this data, we propose using this teacher to label LLM generations and employ a set of simple data selection strategies that use the teacher's label probabilities. Our data selection strategies help us identify a representative subset of diverse generations that help boost zero-shot accuracies while being efficient, in comparison to using all the LLM generations (without any subset selection). We also highlight other important design choices that affect cross-lingual performance such as the use of translations of source data and what labels are best to use for the LLM generations. We observe significant performance gains across sentiment analysis and natural language inference tasks (of up to a maximum of 7.13 absolute points and 1.5 absolute points on average) across a number of target languages (Hindi, Marathi, Urdu, Swahili) and domains.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是非常熟练的文本生成器。我们利用 LLM 的这种能力通过零样本提示生成特定于任务的数据，并促进低资源目标语言的跨语言迁移。给定源语言中的特定于任务的数据和基于此数据训练的教师模型，我们建议使用此教师来标记 LLM 代，并采用一组使用教师标签概率的简单数据选择策略。与使用所有 LLM 代（没有任何子集选择）相比，我们的数据选择策略可帮助我们识别不同代的代表性子集，这有助于提高零样本准确率，同时又不失效率。我们还强调了影响跨语言性能的其他重要设计选择，例如源数据的翻译的使用以及最适合 LLM 代使用的标签。我们观察到情绪分析和自然语言推理任务的性能显著提升（最高 7.13 个绝对点，平均 1.5 个绝对点），涉及多种目标语言（印地语、马拉地语、乌尔都语、斯瓦希里语）和领域。</li>
</ul>

<h3>Title: NoviCode: Generating Programs from Natural Language Utterances by Novices</h3>
<ul>
<li><strong>Authors: </strong>Asaf Achi Mordechai, Yoav Goldberg, Reut Tsarfaty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10626">https://arxiv.org/abs/2407.10626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10626">https://arxiv.org/pdf/2407.10626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10626]] NoviCode: Generating Programs from Natural Language Utterances by Novices(https://arxiv.org/abs/2407.10626)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Current Text-to-Code models demonstrate impressive capabilities in generating executable code from natural language snippets. However, current studies focus on technical instructions and programmer-oriented language, and it is an open question whether these models can effectively translate natural language descriptions given by non-technical users and express complex goals, to an executable program that contains an intricate flow - composed of API access and control structures as loops, conditions, and sequences. To unlock the challenge of generating a complete program from a plain non-technical description we present NoviCode, a novel NL Programming task, which takes as input an API and a natural language description by a novice non-programmer and provides an executable program as output. To assess the efficacy of models on this task, we provide a novel benchmark accompanied by test suites wherein the generated program code is assessed not according to their form, but according to their functional execution. Our experiments show that, first, NoviCode is indeed a challenging task in the code synthesis domain, and that generating complex code from non-technical instructions goes beyond the current Text-to-Code paradigm. Second, we show that a novel approach wherein we align the NL utterances with the compositional hierarchical structure of the code, greatly enhances the performance of LLMs on this task, compared with the end-to-end Text-to-Code counterparts.</li>
<li><strong>摘要：</strong>当前的文本到代码模型在从自然语言片段生成可执行代码方面表现出了令人印象深刻的能力。然而，当前的研究主要集中在技术指令和面向程序员的语言上，这些模型是否能够有效地将非技术用户给出的自然语言描述和表达复杂目标转化为包含复杂流程的可执行程序仍是一个悬而未决的问题——由 API 访问和控制结构组成，如循环、条件和序列。为了解决从简单的非技术描述生成完整程序的挑战，我们提出了 NoviCode，这是一种新颖的 NL 编程任务，它将 API 和新手非程序员的自然语言描述作为输入，并提供可执行程序作为输出。为了评估模型在此任务上的有效性，我们提供了一个新颖的基准测试和测试套件，其中生成的程序代码不是根据其形式进行评估，而是根据其功能执行情况进行评估。我们的实验表明，首先，NoviCode 确实是代码合成领域的一项具有挑战性的任务，并且从非技术指令生成复杂代码超越了当前的文本到代码范式。其次，我们展示了一种新方法，即将 NL 话语与代码的组合层次结构对齐，与端到端的文本到代码相比，这种方法大大提高了 LLM 在该任务上的性能。</li>
</ul>

<h3>Title: Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena</h3>
<ul>
<li><strong>Authors: </strong>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, Weizhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10627">https://arxiv.org/abs/2407.10627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10627">https://arxiv.org/pdf/2407.10627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10627]] Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena(https://arxiv.org/abs/2407.10627)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Assessing the effectiveness of large language models (LLMs) presents substantial challenges. The method of conducting human-annotated battles in an online Chatbot Arena is a highly effective evaluative technique. However, this approach is limited by the costs and time required for human annotation. In this paper, we introduce Arena Learning, an innovative offline strategy designed to simulate these arena battles using AI-driven annotations to evaluate battle outcomes, thus facilitating the continuous improvement of the target model through both supervised fine-tuning and reinforcement learning. Arena Learning comprises two key elements. First, it ensures precise evaluations and maintains consistency between offline simulations and online competitions via WizardArena, a pipeline developed to accurately predict the Elo rankings of various models using a meticulously designed offline test set. Our results demonstrate that WizardArena's predictions closely align with those from the online Arena. Second, it involves the continuous improvement of training data based on the battle results and the refined model. We establish a data flywheel to iteratively update the training data by highlighting the weaknesses of the target model based on its battle results, enabling it to learn from the strengths of multiple different models. We apply Arena Learning to train our target model, WizardLM-$\beta$, and demonstrate significant performance enhancements across various metrics. This fully automated training and evaluation pipeline sets the stage for continuous advancements in various LLMs via post-training. Notably, Arena Learning plays a pivotal role in the success of WizardLM-2, and this paper serves both as an exploration of its efficacy and a foundational study for future discussions related to WizardLM-2 and its derivatives.</li>
<li><strong>摘要：</strong>评估大型语言模型 (LLM) 的有效性面临巨大挑战。在在线聊天机器人竞技场中进行人工注释战斗的方法是一种非常有效的评估技术。然而，这种方法受到人工注释所需的成本和时间的限制。在本文中，我们介绍了竞技场学习，这是一种创新的离线策略，旨在使用人工智能驱动的注释来模拟这些竞技场战斗以评估战斗结果，从而通过监督微调和强化学习促进目标模型的持续改进。竞技场学习包含两个关键要素。首先，它通过 WizardArena 确保精确评估并保持离线模拟和在线比赛之间的一致性，WizardArena 是一种使用精心设计的离线测试集准确预测各种模型的 Elo 排名的管道。我们的结果表明，WizardArena 的预测与在线竞技场的预测非常吻合。其次，它涉及根据战斗结果和改进的模型不断改进训练数据。我们建立了一个数据飞轮，通过根据目标模型的战斗结果突出其弱点来迭代更新训练数据，使其能够从多个不同模型的优势中学习。我们应用竞技场学习来训练我们的目标模型 WizardLM-$\beta$，并在各种指标上展示出显著的性能提升。这种完全自动化的训练和评估流程为通过后期训练不断改进各种 LLM 奠定了基础。值得注意的是，竞技场学习在 WizardLM-2 的成功中起着关键作用，本文既是对其功效的探索，也是未来与 WizardLM-2 及其衍生产品相关的讨论的基础研究。</li>
</ul>

<h3>Title: Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Louis Abraham, Charles Arnal, Antoine Marie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10645">https://arxiv.org/abs/2407.10645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10645">https://arxiv.org/pdf/2407.10645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10645]] Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models(https://arxiv.org/abs/2407.10645)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models have recently been applied to text annotation tasks from social sciences, equalling or surpassing the performance of human workers at a fraction of the cost. However, no inquiry has yet been made on the impact of prompt selection on labelling accuracy. In this study, we show that performance greatly varies between prompts, and we apply the method of automatic prompt optimization to systematically craft high quality prompts. We also provide the community with a simple, browser-based implementation of the method at this https URL .</li>
<li><strong>摘要：</strong>大型语言模型最近已应用于社会科学领域的文本注释任务，其性能与人类工作者相当甚至超过人类工作者，而成本仅为人类工作者的一小部分。然而，目前尚未对提示选择对标记准确性的影响进行调查。在本研究中，我们表明不同提示之间的性能差异很大，并且我们应用自动提示优化方法来系统地制作高质量的提示。我们还通过此 https URL 为社区提供了该方法的简单、基于浏览器的实现。</li>
</ul>

<h3>Title: An Empirical Study of Validating Synthetic Data for Formula Generation</h3>
<ul>
<li><strong>Authors: </strong>Usneek Singh, José Cambronero, Sumit Gulwani, Aditya Kanade, Anirudh Khatry, Vu Le, Mukul Singh, Gust Verbruggen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10657">https://arxiv.org/abs/2407.10657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10657">https://arxiv.org/pdf/2407.10657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10657]] An Empirical Study of Validating Synthetic Data for Formula Generation(https://arxiv.org/abs/2407.10657)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可用于帮助在电子表格中编写公式，但这些公式的资源稀缺，这既影响了预训练模型的基本性能，也限制了对其进行微调的能力。给定一个公式语料库，我们可以使用另一个模型来生成合成的自然语言语句以进行微调。但是，验证 LLM 生成的 NL 是否确实准确以利于微调非常重要。在本文中，我们提供了使用替代目标验证这些合成训练示例的影响的实证结果，这些替代目标评估合成注释的准确性。我们证明验证可以提高四个模型（2 个开放权重和 2 个封闭权重）对原始数据的性能。有趣的是，我们表明，尽管验证倾向于修剪更具挑战性的示例，但它增加了模型在经过验证的数据上进行微调后可以解决的问题的复杂性。</li>
</ul>

<h3>Title: Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems</h3>
<ul>
<li><strong>Authors: </strong>Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, Min Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10670">https://arxiv.org/abs/2407.10670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10670">https://arxiv.org/pdf/2407.10670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10670]] Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems(https://arxiv.org/abs/2407.10670)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) techniques leverage the in-context learning capabilities of large language models (LLMs) to produce more accurate and relevant responses. Originating from the simple 'retrieve-then-read' approach, the RAG framework has evolved into a highly flexible and modular paradigm. A critical component, the Query Rewriter module, enhances knowledge retrieval by generating a search-friendly query. This method aligns input questions more closely with the knowledge base. Our research identifies opportunities to enhance the Query Rewriter module to Query Rewriter+ by generating multiple queries to overcome the Information Plateaus associated with a single query and by rewriting questions to eliminate Ambiguity, thereby clarifying the underlying intent. We also find that current RAG systems exhibit issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge Filter. These two modules are both based on the instruction-tuned Gemma-2B model, which together enhance response quality. The final identified issue is Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the Retriever Trigger to solve this. The former supports the dynamic expansion of the RAG system's knowledge base in a parameter-free manner, while the latter optimizes the cost for accessing external knowledge, thereby improving resource utilization and response efficiency. These four RAG modules synergistically improve the response quality and efficiency of the RAG system. The effectiveness of these modules has been validated through experiments and ablation studies across six common QA datasets. The source code can be accessed at this https URL.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 技术利用大型语言模型 (LLM) 的上下文学习能力来产生更准确和更相关的响应。RAG 框架起源于简单的“先检索后阅读”方法，现已发展成为一种高度灵活和模块化的范例。关键组件——查询重写器模块通过生成搜索友好的查询来增强知识检索。此方法将输入问题与知识库更紧密地结合在一起。我们的研究发现了将查询重写器模块增强为查询重写器+的机会，方法是生成多个查询以克服与单个查询相关的信息高原，并通过重写问题来消除歧义，从而澄清基本意图。我们还发现当前的 RAG 系统存在不相关知识的问题；为了解决这个问题，我们提出了知识过滤器。这两个模块都基于指令调整的 Gemma-2B 模型，它们共同提高了响应质量。最后确定的问题是冗余检索；为了解决这个问题，我们引入了记忆知识库和检索触发器。前者以无参数的方式支持 RAG 系统知识库的动态扩展，而后者优化了访问外部知识的成本，从而提高了资源利用率和响应效率。这四个 RAG 模块协同提高了 RAG 系统的响应质量和效率。这些模块的有效性已通过六个常见 QA 数据集上的实验和消融研究得到验证。源代码可以通过此 https URL 访问。</li>
</ul>

<h3>Title: Qwen2 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, Zhihao Fan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10671">https://arxiv.org/abs/2407.10671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10671">https://arxiv.org/pdf/2407.10671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10671]] Qwen2 Technical Report(https://arxiv.org/abs/2407.10671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.</li>
<li><strong>摘要：</strong>本报告介绍了 Qwen2 系列，这是我们大型语言模型和大型多模态模型的最新成员。我们发布了一套全面的基础和指令调整语言模型，涵盖从 0.5 到 720 亿的参数范围，包括密集模型和混合专家模型。Qwen2 超越了大多数之前的开放权重模型，包括其前身 Qwen1.5，并且在语言理解、生成、多语言能力、编码、数学和推理等各种基准测试中，与专有模型相比表现出了极具竞争力的性能。旗舰模型 Qwen2-72B 表现出色：在 MMLU 上为 84.2，在 GPQA 上为 37.9，在 HumanEval 上为 64.6，在 GSM8K 上为 89.5，作为基础语言模型，在 BBH 上为 82.4。经过指令调优的 Qwen2-72B-Instruct 在 MT-Bench 上达到 9.1，在 Arena-Hard 上达到 48.1，在 LiveCodeBench 上达到 35.7。此外，Qwen2 还展示了强大的多语言能力，精通大约 30 种语言，涵盖英语、中文、西班牙语、法语、德语、阿拉伯语、俄语、韩语、日语、泰语、越南语等，彰显了其多功能性和全球影响力。为了促进社区创新和可访问性，我们在 Hugging Face1 和 ModelScope2 上公开了 Qwen2 模型权重，并在 GitHub3 上公开了包括示例代码在内的补充材料。这些平台还包括用于量化、微调和部署的资源，为广泛的应用和研究工作提供了便利。</li>
</ul>

<h3>Title: DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems</h3>
<ul>
<li><strong>Authors: </strong>Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10701">https://arxiv.org/abs/2407.10701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10701">https://arxiv.org/pdf/2407.10701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10701]] DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems(https://arxiv.org/abs/2407.10701)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, there has been a growing interest among large language model (LLM) developers in LLM-based document reading systems, which enable users to upload their own documents and pose questions related to the document contents, going beyond simple reading comprehension tasks. Consequently, these systems have been carefully designed to tackle challenges such as file parsing, metadata extraction, multi-modal information understanding and long-context reading. However, no current benchmark exists to evaluate their performance in such scenarios, where a raw file and questions are provided as input, and a corresponding response is expected as output. In this paper, we introduce DocBench, a new benchmark designed to evaluate LLM-based document reading systems. Our benchmark involves a meticulously crafted process, including the recruitment of human annotators and the generation of synthetic questions. It includes 229 real documents and 1,102 questions, spanning across five different domains and four major types of questions. We evaluate both proprietary LLM-based systems accessible via web interfaces or APIs, and a parse-then-read pipeline employing open-source LLMs. Our evaluations reveal noticeable gaps between existing LLM-based document reading systems and human performance, underscoring the challenges of developing proficient systems. To summarize, DocBench aims to establish a standardized benchmark for evaluating LLM-based document reading systems under diverse real-world scenarios, thereby guiding future advancements in this research area.</li>
<li><strong>摘要：</strong>最近，大型语言模型 (LLM) 开发人员对基于 LLM 的文档阅读系统的兴趣日益浓厚，该系统允许用户上传自己的文档并提出与文档内容相关的问题，而不仅仅是简单的阅读理解任务。因此，这些系统经过精心设计，可以应对文件解析、元数据提取、多模态信息理解和长上下文阅读等挑战。然而，目前还没有基准来评估它们在这些场景中的表现，在这些场景中，输入原始文件和问题，并期望相应的响应作为输出。在本文中，我们介绍了 DocBench，这是一个旨在评估基于 LLM 的文档阅读系统的新基准。我们的基准涉及一个精心设计的过程，包括招募人工注释者和生成合成问题。它包括 229 份真实文档和 1,102 个问题，涵盖五个不同的领域和四种主要类型的问题。我们评估了可通过 Web 界面或 API 访问的专有 LLM 系统，以及采用开源 LLM 的解析然后读取管道。我们的评估表明，现有基于 LLM 的文档阅读系统与人类的表现之间存在明显差距，这凸显了开发熟练系统的挑战。总而言之，DocBench 旨在建立一个标准化的基准，用于在各种现实场景下评估基于 LLM 的文档阅读系统，从而指导该研究领域的未来发展。</li>
</ul>

<h3>Title: CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses</h3>
<ul>
<li><strong>Authors: </strong>Jing Yao, Xiaoyuan Yi, Xing Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10725">https://arxiv.org/abs/2407.10725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10725">https://arxiv.org/pdf/2407.10725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10725]] CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses(https://arxiv.org/abs/2407.10725)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing LLMs' values can help expose their misalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or close-source ones like GPT-4, to identify values reflected in generated responses. Nevertheless, these evaluators face two challenges in open-ended value evaluation: they should align with changing human value definitions with minimal annotation, against their own bias (adaptability), and detect varying value expressions and scenarios robustly (generalizability). To handle these challenges, we introduce CLAVE, a novel framework which integrates two complementary LLMs, a large one to extract high-level value concepts from a few human labels, leveraging its extensive knowledge and generalizability, and a smaller one fine-tuned on such concepts to better align with human value understanding. This dual-model approach enables calibration with any value systems using <100 human-labeled samples per value type. Then we present ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the capabilities of 12+ popular LLM evaluators and analyze their strengths and weaknesses. Our findings reveal that combining fine-tuned small models and prompt-based large ones serves as a superior balance in value evaluation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展带来了生成不道德内容等潜在风险。评估 LLM 的价值观有助于揭示其错位，但依赖于无参考评估器（例如经过微调的 LLM 或 GPT-4 等闭源评估器）来识别生成的响应中反映的价值观。然而，这些评估器在开放式价值观评估中面临两个挑战：它们应该以最少的注释与不断变化的人类价值观定义保持一致，以克服自己的偏见（适应性），并稳健地检测不同的价值观表达和场景（普遍性）。为了应对这些挑战，我们引入了 CLAVE，这是一个集成了两个互补 LLM 的新框架，一个大型 LLM 利用其广泛的知识和普遍性从少数人类标签中提取高级价值概念，另一个小型 LLM 针对这些概念进行了微调，以更好地与人类价值观理解保持一致。这种双模型方法可以使用每个价值类型使用不到 100 个人工标记的样本来校准任何价值体系。然后，我们介绍了 ValEval，这是一个全面的数据集，包含 13k+ 个（文本、值、标签）元组，涵盖了三个主要的价值体系。我们对 12 多个流行的 LLM 评估器的功能进行了基准测试，并分析了它们的优势和劣势。我们的研究结果表明，将微调的小模型和基于提示的大模型结合起来，可以在价值评估中实现更好的平衡。</li>
</ul>

<h3>Title: Codebook LLMs: Adapting Political Science Codebooks for LLM Use and Adapting LLMs to Follow Codebooks</h3>
<ul>
<li><strong>Authors: </strong>Andrew Halterman, Katherine A. Keith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10747">https://arxiv.org/abs/2407.10747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10747">https://arxiv.org/pdf/2407.10747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10747]] Codebook LLMs: Adapting Political Science Codebooks for LLM Use and Adapting LLMs to Follow Codebooks(https://arxiv.org/abs/2407.10747)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Codebooks -- documents that operationalize constructs and outline annotation procedures -- are used almost universally by social scientists when coding unstructured political texts. Recently, to reduce manual annotation costs, political scientists have looked to generative large language models (LLMs) to label and analyze text data. However, previous work using LLMs for classification has implicitly relied on the universal label assumption -- correct classification of documents is possible using only a class label or minimal definition and the information that the LLM inductively learns during its pre-training. In contrast, we argue that political scientists who care about valid measurement should instead make a codebook-construct label assumption -- an LLM should follow the definition and exclusion criteria of a construct/label provided in a codebook. In this work, we collect and curate three political science datasets and their original codebooks and conduct a set of experiments to understand whether LLMs comply with codebook instructions, whether rewriting codebooks improves performance, and whether instruction-tuning LLMs on codebook-document-label tuples improves performance over zero-shot classification. Using Mistral 7B Instruct as our LLM, we find re-structuring the original codebooks gives modest gains in zero-shot performance but the model still struggles to comply with the constraints of the codebooks. Optimistically, instruction-tuning Mistral on one of our datasets gives significant gains over zero-shot inference (0.76 versus 0.53 micro F1). We hope our conceptualization of the codebook-specific task, assumptions, and instruction-tuning pipeline as well our semi-structured LLM codebook format will help political scientists readily adapt to the LLM era.</li>
<li><strong>摘要：</strong>代码本（一种使结构可操作并概述注释程序的文档）几乎被社会科学家普遍用于编码非结构化政治文本。最近，为了降低手动注释成本，政治科学家开始寻求生成式大型语言模型 (LLM) 来标记和分析文本数据。然而，以前使用 LLM 进行分类的工作隐含地依赖于通用标签假设——仅使用类标签或最小定义以及 LLM 在预训练期间归纳学习的信息就可以正确分类文档。相反，我们认为关心有效测量的政治科学家应该做出代码本构造标签假设——LLM 应该遵循代码本中提供的构造/标签的定义和排除标准。在这项工作中，我们收集并整理了三个政治学数据集及其原始代码本，并进行了一系列实验，以了解 LLM 是否符合代码本指令，重写代码本是否可以提高性能，以及在代码本-文档-标签元组上调整 LLM 的指令是否可以提高零样本分类的性能。使用 Mistral 7B Instruct 作为我们的 LLM，我们发现重新构建原始代码本可以适度提高零样本性能，但模型仍然难以遵守代码本的限制。乐观地说，在我们的一个数据集上对 Mistral 进行指令调整比零样本推理有显著的提升（0.76 对 0.53 微 F1）。我们希望我们对代码本特定任务、假设和指令调整管道的概念化以及我们的半结构化 LLM 代码本格式将帮助政治科学家轻松适应 LLM 时代。</li>
</ul>

<h3>Title: GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Hannah Sansford, Nicholas Richardson, Hermina Petric Maretic, Juba Nait Saada</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10793">https://arxiv.org/abs/2407.10793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10793">https://arxiv.org/pdf/2407.10793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10793]] GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework(https://arxiv.org/abs/2407.10793)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Methods to evaluate Large Language Model (LLM) responses and detect inconsistencies, also known as hallucinations, with respect to the provided knowledge, are becoming increasingly important for LLM applications. Current metrics fall short in their ability to provide explainable decisions, systematically check all pieces of information in the response, and are often too computationally expensive to be used in practice. We present GraphEval: a hallucination evaluation framework based on representing information in Knowledge Graph (KG) structures. Our method identifies the specific triples in the KG that are prone to hallucinations and hence provides more insight into where in the response a hallucination has occurred, if at all, than previous methods. Furthermore, using our approach in conjunction with state-of-the-art natural language inference (NLI) models leads to an improvement in balanced accuracy on various hallucination benchmarks, compared to using the raw NLI models. Lastly, we explore the use of GraphEval for hallucination correction by leveraging the structure of the KG, a method we name GraphCorrect, and demonstrate that the majority of hallucinations can indeed be rectified.</li>
<li><strong>摘要：</strong>对于 LLM 应用来说，评估大型语言模型 (LLM) 响应并检测与所提供知识不一致（也称为幻觉）的方法正变得越来越重要。当前的指标在提供可解释的决策、系统地检查响应中的所有信息方面存在不足，而且在实践中计算成本通常太高。我们提出了 GraphEval：一个基于在知识图谱 (KG) 结构中表示信息的幻觉评估框架。我们的方法可以识别 KG 中容易出现幻觉的特定三元组，因此与以前的方法相比，可以更深入地了解响应中出现幻觉的位置（如果有的话）。此外，与使用原始 NLI 模型相比，将我们的方法与最先进的自然语言推理 (NLI) 模型结合使用可以提高各种幻觉基准的平衡准确度。最后，我们探索利用 KG 的结构使用 GraphEval 来纠正幻觉，我们将这种方法命名为 GraphCorrect，并证明大多数幻觉确实可以得到纠正。</li>
</ul>

<h3>Title: Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10794">https://arxiv.org/abs/2407.10794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10794">https://arxiv.org/pdf/2407.10794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10794]] Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education(https://arxiv.org/abs/2407.10794)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) are crucial in the field of artificial intelligence and are widely applied in downstream tasks, such as enhancing Question Answering (QA) systems. The construction of KGs typically requires significant effort from domain experts. Recently, Large Language Models (LLMs) have been used for knowledge graph construction (KGC), however, most existing approaches focus on a local perspective, extracting knowledge triplets from individual sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC framework from free text. The core fusion module provides a global view of triplets, incorporating entity merging, conflict resolution, and novel triplet discovery. We showcase how Graphusion could be applied to the natural language processing (NLP) domain and validate it in the educational scenario. Specifically, we introduce TutorQA, a new expert-verified benchmark for graph reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our evaluation demonstrates that Graphusion surpasses supervised baselines by up to 10% in accuracy on link prediction. Additionally, it achieves average scores of 2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and relation recognition, respectively.</li>
<li><strong>摘要：</strong>知识图谱 (KG) 在人工智能领域至关重要，广泛应用于下游任务，例如增强问答 (QA) 系统。构建 KG 通常需要领域专家付出大量努力。最近，大型语言模型 (LLM) 已用于知识图谱构建 (KGC)，然而，大多数现有方法都侧重于局部视角，从单个句子或文档中提取知识三元组。在这项工作中，我们引入了 Graphusion，这是一个来自自由文本的零样本 KGC 框架。核心融合模块提供了三元组的全局视图，结合了实体合并、冲突解决和新的三元组发现。我们展示了如何将 Graphusion 应用于自然语言处理 (NLP) 领域并在教育场景中对其进行验证。具体来说，我们引入了 TutorQA，这是一个新的专家验证的图推理和 QA 基准，包含六个任务和总共 1,200 个 QA 对。我们的评估表明，Graphusion 在链接预测方面的准确率比监督基线高出 10%。此外，在概念实体提取和关系识别的人工评估中，它分别获得了 3 分中的 2.92 分和 2.37 分的平均分数。</li>
</ul>

<h3>Title: Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Zhu, Sizhe Liu, Shujian Huang, Shuaijie She, Chris Wendler, Jiajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10795">https://arxiv.org/abs/2407.10795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10795">https://arxiv.org/pdf/2407.10795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10795]] Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping(https://arxiv.org/abs/2407.10795)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Decoding by contrasting layers (DoLa), is designed to improve the generation quality of large language models (LLMs) by contrasting the prediction probabilities between an early exit output (amateur logits) and the final output (expert logits). However, we find that this approach does not work well on non-English tasks. Inspired by previous interpretability work on language transition during the model's forward pass, we discover that this issue arises from a language mismatch between early exit output and final output. In this work, we propose an improved contrastive decoding algorithm that is effective for diverse languages beyond English. To obtain more helpful amateur logits, we devise two strategies to skip a set of bottom, language-agnostic layers based on our preliminary analysis. Experimental results on multilingual reasoning benchmarks demonstrate that our proposed method outperforms previous contrastive decoding baselines and substantially improves LLM's chain-of-thought reasoning accuracy across 11 languages. The project will be available at: this https URL.</li>
<li><strong>摘要：</strong>通过对比层解码 (DoLa) 旨在通过对比早期退出输出 (业余 logits) 和最终输出 (专家 logits) 之间的预测概率来提高大型语言模型 (LLM) 的生成质量。然而，我们发现这种方法在非英语任务上效果不佳。受到之前关于模型前向传递过程中语言转换的可解释性工作的启发，我们发现这个问题源于早期退出输出和最终输出之间的语言不匹配。在这项工作中，我们提出了一种改进的对比解码算法，该算法对英语以外的多种语言都有效。为了获得更多有用的业余 logits，我们根据初步分析设计了两种策略来跳过一组底层的、与语言无关的层。多语言推理基准上的实验结果表明，我们提出的方法优于之前的对比解码基线，并显著提高了 LLM 在 11 种语言中的思路推理准确性。该项目将在以下网址提供：此 https URL。</li>
</ul>

<h3>Title: Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10804">https://arxiv.org/abs/2407.10804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10804">https://arxiv.org/pdf/2407.10804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10804]] Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment(https://arxiv.org/abs/2407.10804)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Adapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. This adaptation typically requires continual pre-training on massive domain-specific corpora to facilitate knowledge memorization, followed by training to apply this knowledge following human instructions and preferences. However, this method may result in inefficient knowledge memorization due to a lack of awareness of knowledge utilization and imposes substantial demands on LLMs to simultaneously learn knowledge utilization and format alignment with limited training samples. To facilitate the domain adaptation of LLM, we revise this process and propose a new domain adaptation framework including domain knowledge learning and general format alignment, called Mix-CPT. Specifically, we first conduct a knowledge mixture continual pre-training that concurrently focuses on knowledge memorization and utilization, allowing for mutual reinforcement. To avoid catastrophic forgetting during the continual pre-training process, we further incorporate a logit swap self-distillation constraint. Subsequently, leveraging the knowledge and capabilities acquired during continual pre-training, we efficiently perform instruction tuning and alignment with a few general training samples to achieve format alignment. Extensive experiments demonstrate that our proposed Mix-CPT framework can simultaneously improve the task-solving capabilities of LLMs on the target and general domains compared to the traditional adaptation methods.</li>
<li><strong>摘要：</strong>由于数据分布各异，将通用大型语言模型 (LLM) 适配到专业领域面临着巨大的挑战。这种适配通常需要在大量领域特定语料上进行持续的预训练以促进知识记忆，然后根据人类的指示和偏好进行训练以应用这些知识。然而，这种方法可能会因为缺乏对知识利用的认识而导致知识记忆效率低下，并且对 LLM 提出了巨大的要求，使其在有限的训练样本下同时学习知识利用和格式对齐。为了促进 LLM 的领域适应，我们修改了此过程并提出了一个包括领域知识学习和通用格式对齐的新领域适应框架，称为 Mix-CPT。具体而言，我们首先进行知识混合持续预训练，同时关注知识记忆和利用，以实现相互强化。为了避免在持续预训练过程中出现灾难性遗忘，我们进一步加入了 logit 交换自我提炼约束。随后，利用在持续预训练过程中获得的知识和能力，我们利用少量通用训练样本高效地执行指令调整和对齐，以实现格式对齐。大量实验表明，与传统的自适应方法相比，我们提出的 Mix-CPT 框架可以同时提高 LLM 在目标领域和通用领域的任务解决能力。</li>
</ul>

<h3>Title: Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10805">https://arxiv.org/abs/2407.10805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10805">https://arxiv.org/pdf/2407.10805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10805]] Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval(https://arxiv.org/abs/2407.10805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has significantly advanced large language models (LLMs) by enabling dynamic information retrieval to mitigate knowledge gaps and hallucinations in generated content. However, these systems often falter with complex reasoning and consistency across diverse queries. In this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns questions with the knowledge graph and uses it as a navigational tool, which deepens and refines the RAG paradigm for information collection and integration. The KG-guided navigation fosters deep and long-range associations to uphold logical consistency and optimize the scope of retrieval for precision and interoperability. In conjunction, factual consistency can be better ensured through semantic similarity guided by precise directives. ToG${2.0}$ not only improves the accuracy and reliability of LLMs' responses but also demonstrates the potential of hybrid structured knowledge systems to significantly advance LLM reasoning, aligning it closer to human-like performance. We conducted extensive experiments on four public datasets to demonstrate the advantages of our method compared to the baseline.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过实现动态信息检索来缓解生成内容中的知识差距和幻觉，从而显著推进了大型语言模型 (LLM)。然而，这些系统在复杂推理和跨不同查询的一致性方面往往表现不佳。在这项工作中，我们提出了 Think-on-Graph 2.0，这是一个增强的 RAG 框架，它将问题与知识图谱对齐并将其用作导航工具，从而深化和完善了 RAG 信息收集和集成范式。KG 引导的导航促进了深度和长距离关联，以维护逻辑一致性并优化检索范围以实现精确性和互操作性。同时，通过精确指令引导的语义相似性可以更好地确保事实一致性。ToG${2.0}$ 不仅提高了 LLM 响应的准确性和可靠性，而且还展示了混合结构化知识系统显着推进 LLM 推理的潜力，使其更接近人类的表现。我们对四个公共数据集进行了大量实验，以证明我们的方法相对于基线的优势。</li>
</ul>

<h3>Title: Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10817">https://arxiv.org/abs/2407.10817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10817">https://arxiv.org/pdf/2407.10817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10817]] Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation(https://arxiv.org/abs/2407.10817)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on our large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25x less training datapoints. Overall, our FLAMe variants outperform all popular proprietary LLM-as-a-Judge models we consider across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的发展，由于人工评估成本高昂，可靠地评估其输出变得越来越具有挑战性。为了改进 LLM 自动评分器，我们推出了 FLAMe，这是基础大型自动评分器模型系列。FLAMe 在我们庞大而多样化的 100 多个质量评估任务集合上进行训练，这些任务包含 500 多万个人工判断，并使用先前研究中公开发布的人工评估进行整理和标准化。FLAMe 显著提高了对各种保留任务的泛化能力，在许多任务上的表现优于在 GPT-4 和 Claude-3 等专有数据上训练的 LLM。我们表明，FLAMe 也可以作为进一步下游微调的强大起点，以奖励模型评估为案例研究 (FLAMe-RM)。值得注意的是，在 RewardBench 上，我们的 FLAMe-RM-24B 模型（准确率为 87.8%）是性能最佳的生成模型，该模型仅使用经过许可的数据进行训练，其性能优于 GPT-4-0125（85.9%）和 GPT-4o（84.7%）。此外，我们还探索了一种计算效率更高的方法，使用一种新颖的尾部补丁微调策略来优化我们的 FLAMe 多任务混合奖励模型评估（FLAMe-Opt-RM），提供具有竞争力的 RewardBench 性能，同时所需的训练数据点数量大约减少了 25 倍。总体而言，我们的 FLAMe 变体在 12 个自动评分器评估基准中的 8 个中均优于我们考虑的所有流行的专有 LLM-as-a-Judge 模型，涵盖 53 个质量评估任务，包括 RewardBench 和 LLM-AggreFact。最后，我们的分析表明，在 CoBBLEr 自动评分偏差基准上，FLAMe 的偏差明显小于这些 LLM-as-a-Judge 模型，同时能够有效识别用于代码生成的高质量响应。</li>
</ul>

<h3>Title: BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy</h3>
<ul>
<li><strong>Authors: </strong>Tim Menzner, Jochen L. Leidner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10829">https://arxiv.org/abs/2407.10829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10829">https://arxiv.org/pdf/2407.10829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10829]] BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy(https://arxiv.org/abs/2407.10829)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The increasing consumption of news online in the 21st century coincided with increased publication of disinformation, biased reporting, hate speech and other unwanted Web content. We describe BiasScanner, an application that aims to strengthen democracy by supporting news consumers with scrutinizing news articles they are reading online. BiasScanner contains a server-side pre-trained large language model to identify biased sentences of news articles and a front-end Web browser plug-in. At the time of writing, BiasScanner can identify and classify more than two dozen types of media bias at the sentence level, making it the most fine-grained model and only deployed application (automatic system in use) of its kind. It was implemented in a light-weight and privacy-respecting manner, and in addition to highlighting likely biased sentence it also provides explanations for each classification decision as well as a summary analysis for each news article. While prior research has addressed news bias detection, we are not aware of any work that resulted in a deployed browser plug-in (c.f. also this http URL for a Web demo).</li>
<li><strong>摘要：</strong>21 世纪，随着网络新闻消费的增加，虚假信息、偏见报道、仇恨言论和其他不受欢迎的网络内容的发布也随之增加。我们描述了 BiasScanner，这是一款旨在通过支持新闻消费者仔细审查他们在线阅读的新闻文章来加强民主的应用程序。BiasScanner 包含一个服务器端预训练的大型语言模型，用于识别新闻文章的偏见句子和一个前端 Web 浏览器插件。在撰写本文时，BiasScanner 可以在句子级别识别和分类二十多种媒体偏见，使其成为同类中最细粒度的模型和唯一部署的应用程序（正在使用的自动系统）。它以轻量级和尊重隐私的方式实现，除了突出显示可能有偏见的句子外，它还为每个分类决策提供解释以及每篇新闻文章的摘要分析。虽然先前的研究已经解决了新闻偏见检测问题，但我们不知道有任何工作导致部署了浏览器插件（参见此 http URL 的 Web 演示）。</li>
</ul>

<h3>Title: An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases</h3>
<ul>
<li><strong>Authors: </strong>Dylan Bouchard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10853">https://arxiv.org/abs/2407.10853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10853">https://arxiv.org/pdf/2407.10853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10853]] An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases(https://arxiv.org/abs/2407.10853)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can exhibit bias in a variety of ways. Such biases can create or exacerbate unfair outcomes for certain groups within a protected attribute, including, but not limited to sex, race, sexual orientation, or age. This paper aims to provide a technical guide for practitioners to assess bias and fairness risks in LLM use cases. The main contribution of this work is a decision framework that allows practitioners to determine which metrics to use for a specific LLM use case. To achieve this, this study categorizes LLM bias and fairness risks, maps those risks to a taxonomy of LLM use cases, and then formally defines various metrics to assess each type of risk. As part of this work, several new bias and fairness metrics are introduced, including innovative counterfactual metrics as well as metrics based on stereotype classifiers. Instead of focusing solely on the model itself, the sensitivity of both prompt-risk and model-risk are taken into account by defining evaluations at the level of an LLM use case, characterized by a model and a population of prompts. Furthermore, because all of the evaluation metrics are calculated solely using the LLM output, the proposed framework is highly practical and easily actionable for practitioners.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可以以多种方式表现出偏见。此类偏见可能会对受保护属性中的某些群体造成或加剧不公平结果，包括但不限于性别、种族、性取向或年龄。本文旨在为从业者提供技术指南，以评估 LLM 用例中的偏见和公平风险。这项工作的主要贡献是一个决策框架，允许从业者确定针对特定 LLM 用例使用哪些指标。为此，本研究对 LLM 偏见和公平风险进行分类，将这些风险映射到 LLM 用例的分类法中，然后正式定义各种指标来评估每种风险。作为这项工作的一部分，引入了几个新的偏见和公平指标，包括创新的反事实指标以及基于刻板印象分类器的指标。通过在 LLM 用例级别定义评估（以模型和提示群体为特征），不再仅仅关注模型本身，而是考虑提示风险和模型风险的敏感性。此外，由于所有评估指标都是仅使用 LLM 输出计算的，因此提出的框架对于从业者来说非常实用且易于操作。</li>
</ul>

<h3>Title: Weighted Grouped Query Attention in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Sai Sena Chinnakonduru, Astarag Mohapatra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10855">https://arxiv.org/abs/2407.10855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10855">https://arxiv.org/pdf/2407.10855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10855]] Weighted Grouped Query Attention in Transformers(https://arxiv.org/abs/2407.10855)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.</li>
<li><strong>摘要：</strong>注意力机制构成了 Transformer 语言模型的基础模块。最近的方法表明，扩展模型可以实现人类水平的性能。然而，随着对扩展的需求和硬件内存的限制不断增加，这些模型的推理成本仍然很高。为了减少推理时间，（Shazeer，2019）和（Ainslieet al.，2023）分别提出了多查询注意力（MQA）和分组查询注意力（GQA）。在本文中，我们提出了分组查询注意力的变体，称为加权分组查询注意力（WGQA）。我们在 T5 解码器注意力块中为每个键和值头引入了新的可学习参数，使模型能够在微调期间取加权平均值。我们的模型比 GQA 平均提高了 0.53%，性能收敛到传统的多头注意力（MHA），并且在推理过程中没有额外的开销。我们评估了这些参数的引入，随后的微调使模型了解训练期间的分组机制，从而提高性能。此外，我们通过比较 T5-small 和 T5-base 架构之间的结果来展示我们分析中的缩放规律。</li>
</ul>

<h3>Title: Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together</h3>
<ul>
<li><strong>Authors: </strong>Dilara Soylu, Christopher Potts, Omar Khattab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10930">https://arxiv.org/abs/2407.10930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10930">https://arxiv.org/pdf/2407.10930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10930]] Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together(https://arxiv.org/abs/2407.10930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Natural Language Processing (NLP) systems are increasingly taking the form of multi-stage pipelines involving multiple distinct language models (LMs) and prompting strategies. Here we address the question of how to fine-tune such systems to improve their performance. We cast this as a problem of optimizing the underlying LM weights and the prompting strategies together, and consider a challenging but highly realistic scenario in which we have no gold labels for any intermediate stages in the pipeline. To address this challenge, we evaluate approximate optimization strategies in which we bootstrap training labels for all pipeline stages and use these to optimize the pipeline's prompts and fine-tune its weights alternatingly. In experiments with multi-hop QA, mathematical reasoning, and feature-based classification, we find that simple approaches for optimizing the prompts and weights together outperform directly optimizing weights alone and prompts alone by up to 65% and 5%, respectively, on average across LMs and tasks. We will release our new optimizers in DSPy at this http URL</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 系统越来越多地采用多阶段管道的形式，涉及多个不同的语言模型 (LM) 和提示策略。在这里，我们讨论如何微调此类系统以提高其性能的问题。我们将其视为优化底层 LM 权重和提示策略的问题，并考虑一个具有挑战性但非常现实的场景，其中我们没有管道中任何中间阶段的黄金标签。为了应对这一挑战，我们评估了近似优化策略，其中我们为所有管道阶段引导训练标签，并使用这些标签交替优化管道的提示和微调其权重。在多跳 QA、数学推理和基于特征的分类实验中，我们发现优化提示和权重的简单方法比直接优化权重和提示更有效，在 LM 和任务中分别平均高出 65% 和 5%。我们将在 DSPy 中发布我们的新优化器，网址为 http URL</li>
</ul>

<h3>Title: Learning from Naturally Occurring Feedback</h3>
<ul>
<li><strong>Authors: </strong>Shachar Don-Yehiya, Leshem Choshen, Omri Abend</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10944">https://arxiv.org/abs/2407.10944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10944">https://arxiv.org/pdf/2407.10944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10944]] Learning from Naturally Occurring Feedback(https://arxiv.org/abs/2407.10944)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Human feedback data is a critical component in developing language models. However, collecting this feedback is costly and ultimately not scalable. We propose a scalable method for extracting feedback that users naturally include when interacting with chat models, and leveraging it for model training. We are further motivated by previous work that showed there are also qualitative advantages to using naturalistic (rather than auto-generated) feedback, such as less hallucinations and biases. We manually annotated conversation data to confirm the presence of naturally occurring feedback in a standard corpus, finding that as much as 30% of the chats include explicit feedback. We apply our method to over 1M conversations to obtain hundreds of thousands of feedback samples. Training with the extracted feedback shows significant performance improvements over baseline models, demonstrating the efficacy of our approach in enhancing model alignment to human preferences.</li>
<li><strong>摘要：</strong>人类反馈数据是开发语言模型的关键组成部分。然而，收集这些反馈成本高昂，而且最终不可扩展。我们提出了一种可扩展的方法来提取用户在与聊天模型交互时自然包含的反馈，并利用它进行模型训练。我们进一步受到之前研究的激励，这些研究表明，使用自然反馈（而不是自动生成的反馈）也有定性优势，例如幻觉和偏见更少。我们手动注释了对话数据以确认标准语料库中存在自然发生的反馈，发现多达 30% 的聊天包含明确反馈。我们将我们的方法应用于超过 100 万次对话，以获得数十万个反馈样本。使用提取的反馈进行训练显示出比基线模型显著的性能改进，证明了我们的方法在增强模型与人类偏好的一致性方面的有效性。</li>
</ul>

<h3>Title: Representing Rule-based Chatbots with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Dan Friedman, Abhishek Panigrahi, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10949">https://arxiv.org/abs/2407.10949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10949">https://arxiv.org/pdf/2407.10949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10949]] Representing Rule-based Chatbots with Transformers(https://arxiv.org/abs/2407.10949)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, agent</a></li>
<li><strong>Abstract: </strong>Transformer-based chatbots can conduct fluent, natural-sounding conversations, but we have limited understanding of the mechanisms underlying their behavior. Prior work has taken a bottom-up approach to understanding Transformers by constructing Transformers for various synthetic and formal language tasks, such as regular expressions and Dyck languages. However, it is not obvious how to extend this approach to understand more naturalistic conversational agents. In this work, we take a step in this direction by constructing a Transformer that implements the ELIZA program, a classic, rule-based chatbot. ELIZA illustrates some of the distinctive challenges of the conversational setting, including both local pattern matching and long-term dialog state tracking. We build on constructions from prior work -- in particular, for simulating finite-state automata -- showing how simpler constructions can be composed and extended to give rise to more sophisticated behavior. Next, we train Transformers on a dataset of synthetically generated ELIZA conversations and investigate the mechanisms the models learn. Our analysis illustrates the kinds of mechanisms these models tend to prefer -- for example, models favor an induction head mechanism over a more precise, position based copying mechanism; and using intermediate generations to simulate recurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results offer a new setting for mechanistic analysis of conversational agents.</li>
<li><strong>摘要：</strong>基于 Transformer 的聊天机器人可以进行流畅、自然的对话，但我们对其行为背后的机制的理解有限。先前的研究采用自下而上的方法来理解 Transformer，即为各种合成和形式语言任务（例如正则表达式和 Dyck 语言）构建 Transformer。然而，如何扩展这种方法来理解更自然的对话代理并不明显。在这项工作中，我们朝这个方向迈出了一步，构建了一个实现 ELIZA 程序（一个经典的基于规则的聊天机器人）的 Transformer。ELIZA 说明了对话环境中的一些独特挑战，包括局部模式匹配和长期对话状态跟踪。我们以先前研究的构造为基础（特别是用于模拟有限状态自动机的构造），展示了如何组合和扩展更简单的构造以产生更复杂的行为。接下来，我们将在合成生成的 ELIZA 对话数据集上训练 Transformer，并研究模型学习的机制。我们的分析表明这些模型倾向于采用哪些类型的机制 - 例如，模型倾向于采用感应头机制，而不是更精确的基于位置的复制机制；并使用中间代来模拟循环数据结构，如 ELIZA 的记忆机制。总体而言，通过在神经聊天机器人和可解释的符号机制之间建立明确的联系，我们的结果为对话代理的机械分析提供了一个新的环境。</li>
</ul>

<h3>Title: MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chengguang Gan, Qingyu Yin, Xinyang He, Hanjun Wei, Yunhao Liang, Younghun Lim, Shijian Wang, Hexiang Huang, Qinghao Zhang, Shiwen Ni, Tatsunori Mori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10953">https://arxiv.org/abs/2407.10953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10953">https://arxiv.org/pdf/2407.10953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10953]] MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models(https://arxiv.org/abs/2407.10953)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The Mutual Reinforcement Effect (MRE) represents a promising avenue in information extraction and multitasking research. Nevertheless, its applicability has been constrained due to the exclusive availability of MRE mix datasets in Japanese, thereby limiting comprehensive exploration by the global research community. To address this limitation, we introduce a Multilingual MRE mix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and Chinese. In this paper, we also propose a method for dataset translation assisted by Large Language Models (LLMs), which significantly reduces the manual annotation time required for dataset construction by leveraging LLMs to translate the original Japanese datasets. Additionally, we have enriched the dataset by incorporating open-domain Named Entity Recognition (NER) and sentence classification tasks. Utilizing this expanded dataset, we developed a unified input-output framework to train an Open-domain Information Extraction Large Language Model (OIELLM). The OIELLM model demonstrates the capability to effectively process novel MMM datasets, exhibiting significant improvements in performance.</li>
<li><strong>摘要：</strong>相互强化效应 (MRE) 代表了信息提取和多任务研究的一个有前途的途径。然而，由于 MRE 混合数据集仅限于日语，其适用性受到限制，从而限制了全球研究界的全面探索。为了解决这一限制，我们引入了一个多语言 MRE 混合数据集 (MMM)，其中包含英语、日语和中文的 21 个子数据集。在本文中，我们还提出了一种由大型语言模型 (LLM) 辅助的数据集翻译方法，该方法通过利用 LLM 翻译原始日语数据集，大大减少了数据集构建所需的手动注释时间。此外，我们通过结合开放域命名实体识别 (NER) 和句子分类任务丰富了数据集。利用这个扩展的数据集，我们开发了一个统一的输入输出框架来训练开放域信息提取大型语言模型 (OIELLM)。OIELLM 模型展示了有效处理新 MMM 数据集的能力，性能显著提高。</li>
</ul>

<h3>Title: Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Wang, Shuming Ma, Ruiping Wang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10969">https://arxiv.org/abs/2407.10969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10969">https://arxiv.org/pdf/2407.10969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10969]] Q-Sparse: All Large Language Models can be Fully Sparsely-Activated(https://arxiv.org/abs/2407.10969)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs.</li>
<li><strong>摘要：</strong>我们引入了 Q-Sparse，一种简单而有效的方法来训练稀疏激活大型语言模型 (LLM)。Q-Sparse 可实现 LLM 中激活的完全稀疏性，从而显著提高推理效率。这是通过对激活应用 top-K 稀疏化和对训练应用直通式估计器来实现的。这项工作的主要成果是：(1) Q-Sparse 可以实现与基线 LLM 相当的结果，同时在推理时间上效率更高；(2) 我们为稀疏激活的 LLM 提出了一种推理最佳缩放律；(3) Q-Sparse 在不同设置下都有效，包括从头开始训练、继续训练现成的 LLM 和微调；(4) Q-Sparse 适用于全精度和 1 位 LLM（例如 BitNet b1.58）。特别是BitNet b1.58和Q-Sparse（可配备MoE）的协同作用为彻底改变未来LLM的效率（包括成本和能源消耗）提供了基石和明确的道路。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
