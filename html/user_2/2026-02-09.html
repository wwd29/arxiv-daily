<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-09</h1>
<h3>Title: Recontextualizing Famous Quotes for Brand Slogan Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziao Yang, Zizhang Chen, Lei Zhang, Hongfu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06049">https://arxiv.org/abs/2602.06049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06049">https://arxiv.org/pdf/2602.06049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06049]] Recontextualizing Famous Quotes for Brand Slogan Generation(https://arxiv.org/abs/2602.06049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.</li>
<li><strong>摘要：</strong>口号是简洁易记的标语，通过传达品牌形象和塑造公众认知，在广告中发挥着至关重要的作用。然而，广告疲劳降低了重复口号的有效性，从而对新颖、有创意和有洞察力的口号产生日益增长的需求。虽然最近的工作利用大型语言模型 (LLM) 来完成此任务，但现有方法通常会产生风格冗余的输出，这些输出缺乏清晰的品牌角色，并且明显是机器生成的。我们认为，有效的口号应该平衡新颖性和熟悉度，并提出一种新的范式，将与人物角色相关的名言重新语境化，以生成口号。众所周知的名言自然地与口号长度的文本相一致，采用丰富的修辞手段，并提供深度和洞察力，使它们成为创造性生成的强大资源。从技术上讲，我们引入了一个模块化框架，将口号生成分解为可解释的子任务，包括引用匹配、结构分解、词汇替换和混音生成。广泛的自动和人工评估表明，与三个最先进的法学硕士基线相比，在多样性、新颖性、情感影响和人类偏好方面略有改善。</li>
</ul>

<h3>Title: Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jongha Kim, Byungoh Ko, Jeehye Na, Jinsung Yoon, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06050">https://arxiv.org/abs/2602.06050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06050">https://arxiv.org/pdf/2602.06050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06050]] Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering(https://arxiv.org/abs/2602.06050)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at this https URL.</li>
<li><strong>摘要：</strong>尽管大视觉语言模型（LVLM）具有非凡的能力，但它们仍然缺乏对特定实体的详细了解。检索增强生成 (RAG) 是一种广泛采用的解决方案，它通过提供来自外部知识库的附加上下文来增强 LVLM。然而，我们观察到，之前的 RAG 解码方法并不是最优的，因为它们无法充分利用多个相关上下文并抑制不相关上下文的负面影响。为此，我们提出了相关性感知多上下文对比解码（RMCD），这是一种新颖的 RAG 解码方法。 RMCD 通过组合与每个上下文预测的输出来输出最终预测，其中每个输出根据其与问题的相关性进行加权。通过这样做，RMCD 有效地聚合了来自多个相关上下文的有用信息，同时还抵消了不相关上下文的负面影响。实验表明，RMCD 在多个 LVLM 中始终优于其他解码方法，在三个知识密集型视觉问答基准上实现了最佳性能。此外，RMCD可以通过替换LVLM的解码方法来简单地应用，而无需额外的训练。分析还表明，RMCD 对检索结果具有鲁棒性，在从最弱到最强的检索结果中始终表现最佳。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: CAST: Character-and-Scene Episodic Memory for Agents</h3>
<ul>
<li><strong>Authors: </strong>Kexin Ma, Bojun Li, Yuhua Tang, Ruochun Jin, Liting Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06051">https://arxiv.org/abs/2602.06051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06051">https://arxiv.org/pdf/2602.06051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06051]] CAST: Character-and-Scene Episodic Memory for Agents(https://arxiv.org/abs/2602.06051)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.</li>
<li><strong>摘要：</strong>情景记忆是人类记忆的核心组成部分，指的是回忆基于人物、时间和地点的连贯事件的能力。然而，大多数智能体记忆系统只强调语义回忆，并将经验视为键值、向量或图等结构，这使得它们很难表示和检索连贯的事件。为了应对这一挑战，我们受戏剧理论的启发，提出了一种基于角色和场景的内存架构（CAST）。具体来说，CAST 构建 3D 场景（时间/地点/主题）并将其组织成角色档案，总结角色的事件以表示情景记忆。此外，CAST 通过基于图形的语义记忆补充了这种情景记忆，从而产生了强大的双记忆设计。实验表明，CAST 在各种数据集上比基线平均提高了 8.11% F1 和 10.21% J（LLM-as-a-Judge），特别是在开放和时间敏感的对话问题上。</li>
</ul>

<h3>Title: Rethinking Memory Mechanisms of Foundation Agents in the Second Half</h3>
<ul>
<li><strong>Authors: </strong>Wei-Chieh Huang, Weizhi Zhang, Yueqing Liang, Yuanchen Bei, Yankai Chen, Tao Feng, Xinyu Pan, Zhen Tan, Yu Wang, Tianxin Wei, Shanglin Wu, Ruiyao Xu, Liangwei Yang, Rui Yang, Wooseong Yang, Chin-Yuan Yeh, Hanrong Zhang, Haozhen Zhang, Siqi Zhu, Henry Peng Zou, Wanjia Zhao, Song Wang, Wujiang Xu, Zixuan Ke, Zheng Hui, Dawei Li, Yaozu Wu, Langzhou He, Chen Wang, Xiongxiao Xu, Baixiang Huang, Juntao Tan, Shelby Heinecke, Huan Wang, Caiming Xiong, Ahmed A. Metwally, Jun Yan, Chen-Yu Lee, Hanqing Zeng, Yinglong Xia, Xiaokai Wei, Ali Payani, Yu Wang, Haitong Ma, Wenya Wang, Chengguang Wang, Yu Zhang, Xin Wang, Yongfeng Zhang, Jiaxuan You, Hanghang Tong, Xiao Luo, Yizhou Sun, Wei Wang, Julian McAuley, James Zou, Jiawei Han, Philip S. Yu, Kai Shu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06052">https://arxiv.org/abs/2602.06052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06052">https://arxiv.org/pdf/2602.06052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06052]] Rethinking Memory Mechanisms of Foundation Agents in the Second Half(https://arxiv.org/abs/2602.06052)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.</li>
<li><strong>摘要：</strong>人工智能的研究正在经历范式转变，从优先考虑模型创新而不是基准分数，转向强调问题定义和严格的现实世界评估。随着该领域进入“下半场”，核心挑战成为长期、动态和依赖用户的环境中的真正实用性，在这种环境中，代理面临上下文爆炸，必须在扩展的交互中不断积累、管理和有选择地重用大量信息。因此，今年发布了数百篇论文，内存成为填补实用空白的关键解决方案。在本次调查中，我们从三个维度提供了基础代理记忆的统一视图：记忆基础（内部和外部）、认知机制（情景、语义、感觉、工作和程序）和记忆主体（以代理和用户为中心）。然后，我们分析内存在不同代理拓扑下如何实例化和操作，并重点介绍内存操作的学习策略。最后，我们回顾了用于评估内存效用的评估基准和指标，并概述了各种开放挑战和未来方向。</li>
</ul>

<h3>Title: PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models</h3>
<ul>
<li><strong>Authors: </strong>Rajarshi Roy, Jonathan Raiman, Sang-gil Lee, Teodor-Dumitru Ene, Robert Kirby, Sungwon Kim, Jaehyeon Kim, Bryan Catanzaro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06053">https://arxiv.org/abs/2602.06053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06053">https://arxiv.org/pdf/2602.06053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06053]] PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models(https://arxiv.org/abs/2602.06053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Recent advances in duplex speech models have enabled natural, low-latency speech-to-speech interactions. However, existing models are restricted to a fixed role and voice, limiting their ability to support structured, role-driven real-world applications and personalized interactions. In this work, we introduce PersonaPlex, a duplex conversational speech model that incorporates hybrid system prompts, combining role conditioning with text prompts and voice cloning with speech samples. PersonaPlex is trained on a large-scale synthetic dataset of paired prompts and user-agent conversations, generated with open-source large language models (LLM) and text-to-speech (TTS) models. To evaluate role conditioning in real-world settings, we extend the Full-Duplex-Bench benchmark beyond a single assistant role to multi-role customer service scenarios. Experiments show that PersonaPlex achieves strong role-conditioned behavior, voice-conditioned speech, and natural conversational responsiveness, surpassing state-of-the-art duplex speech models and hybrid large language model-based speech systems in role adherence, speaker similarity, latency, and naturalness.</li>
<li><strong>摘要：</strong>双工语音模型的最新进展实现了自然、低延迟的语音交互。然而，现有模型仅限于固定的角色和声音，限制了它们支持结构化、角色驱动的现实世界应用程序和个性化交互的能力。在这项工作中，我们介绍了 PersonaPlex，这是一种双工会话语音模型，它结合了混合系统提示，将角色调节与文本提示以及语音克隆与语音样本相结合。 PersonaPlex 在配对提示和用户代理对话的大规模合成数据集上进行训练，这些数据集是使用开源大型语言模型 (LLM) 和文本转语音 (TTS) 模型生成的。为了评估现实环境中的角色调节，我们将 Full-Duplex-Bench 基准测试从单一助理角色扩展到多角色客户服务场景。实验表明，PersonaPlex 实现了强大的角色条件行为、语音条件语音和自然会话响应能力，在角色依从性、说话者相似性、延迟和自然度方面超越了最先进的双工语音模型和基于混合大语言模型的语音系统。</li>
</ul>

<h3>Title: What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Abeer Mostafa, Thi Huyen Nguyen, Zahra Ahmadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06054">https://arxiv.org/abs/2602.06054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06054">https://arxiv.org/pdf/2602.06054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06054]] What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation(https://arxiv.org/abs/2602.06054)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.</li>
<li><strong>摘要：</strong>评估研究新颖性是同行评审的一个核心但高度主观的方面，通常基于隐性判断和与先前工作的不完整比较。我们引入了一种具有文献意识的新颖性评估框架，该框架明确地学习人类如何从同行评审报告中判断新颖性，并将这些判断建立在与现有研究的结构化比较的基础上。利用来自顶级 AI 会议的近 80K 条新颖性注释评论，我们对大型语言模型进行了微调，以捕获与审稿人一致的新颖性评估行为。对于给定的手稿，系统提取其想法、方法和权利要求的结构化表示，检索语义相关的论文，并构建一个相似性图，以便与先前的工作进行细粒度的概念级比较。以这种结构化证据为条件，该模型产生校准的新颖性分数和类人的解释性评估，减少高估并提高相对于现有方法的一致性。</li>
</ul>

<h3>Title: Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yanzheng Xiang, Lan Wei, Yizhen Yao, Qinglin Zhu, Hanqi Yan, Chen Jin, Philip Alexander Teare, Dandan Zhang, Lin Gui, Amrutha Saseendran, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06161">https://arxiv.org/abs/2602.06161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06161">https://arxiv.org/pdf/2602.06161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06161]] Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding(https://arxiv.org/abs/2602.06161)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.</li>
<li><strong>摘要：</strong>并行扩散解码可以通过每步揭露多个标记来加速扩散语言模型推理，但激进的并行性通常会损害质量。可撤销解码通过重新检查早期令牌来缓解这种情况，但我们观察到现有的验证方案经常触发触发器振荡，其中令牌被重新屏蔽，然后恢复不变。这种行为以两种方式减慢了推理速度：重新屏蔽已验证的位置会削弱并行起草的条件环境，并且重复的重新屏蔽周期会消耗修订预算，而净进展却很少。我们提出了 COVER（高效修订的缓存覆盖验证），它在一次前向传递中执行留一验证和稳定的起草。 COVER 通过 KV 缓存覆盖构建两个注意力视图：选定的种子被屏蔽以进行验证，而其缓存的键值状态被注入用于所有其他查询以保留上下文信息，并使用封闭形式的对角线校正来防止种子位置的自泄漏。 COVER 使用平衡不确定性、下游影响和缓存漂移的稳定性感知分数进一步优先考虑种子，并调整每步验证种子的数量。在各个基准测试中，COVER 显着减少了不必要的修改，并在保持输出质量的同时实现更快的解码。</li>
</ul>

<h3>Title: Uncertainty Drives Social Bias Changes in Quantized Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Stanley Z. Hua, Sanae Lotfi, Irene Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06181">https://arxiv.org/abs/2602.06181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06181">https://arxiv.org/pdf/2602.06181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06181]] Uncertainty Drives Social Bias Changes in Quantized Large Language Models(https://arxiv.org/abs/2602.06181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.</li>
<li><strong>摘要：</strong>训练后量化降低了大型语言模型的计算成本，但从根本上改变了它们的社会偏见，这是聚合指标无法捕获的。我们首次对在 PostTrainingBiasBench 上评估的 50 个量化模型进行了大规模研究，PostTrainingBiasBench 是 13 个封闭式和开放式偏差数据集的统一基准。我们发现了一种现象，我们称之为量化引起的屏蔽偏差翻转，其中高达 21% 的响应在量化后在有偏差和无偏差状态之间翻转，尽管总体偏差分数没有变化。这些翻转是由模型不确定性强烈驱动的，其中高度不确定性的响应发生变化的可能性是自信响应的 3-11 倍。量化强度放大了这种效应，4 位量化模型表现出的行为变化是 8 位量化模型的 4-6 倍。至关重要的是，这些变化对不同人口群体产生了不对称的影响，其中某些群体的偏见可能恶化高达 18.6%，而另一些群体的偏见则改善 14.1%，从而产生具有误导性的中性总体结果。较大的模型没有表现出一致的鲁棒性优势，并且特定组的变化在模型系列之间存在不可预测的变化。我们的研究结果表明，压缩从根本上改变了偏差模式，需要关键的量化后评估和干预措施以确保实践中的可靠性。</li>
</ul>

<h3>Title: BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Nishant Balepur, Bhavya Rajasekaran, Jane Oh, Michael Xie, Atrey Desai, Vipul Gupta, Steven James Moore, Eunsol Choi, Rachel Rudinger, Jordan Lee Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06221">https://arxiv.org/abs/2602.06221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06221">https://arxiv.org/pdf/2602.06221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06221]] BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks(https://arxiv.org/abs/2602.06221)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.</li>
<li><strong>摘要：</strong>多项选择题回答（MCQA）是 NLP 的标准，但基准测试缺乏严格的质量控制。我们推出 BenchMarker，这是一个受教育启发的工具包，使用 LLM 法官来标记三个常见的 MCQ 缺陷：1）污染 - 项目完全出现在网上； 2) 捷径——选项中的提示，可以进行猜测； 3) 写作错误 - 基于 19 条教育规则的结构/语法问题。我们使用人工注释来验证 BenchMarker，然后运行该工具来审核 12 个基准测试，结果表明：2）受污染的 MCQ 往往会提高准确性，而书写错误往往会降低准确性并改变排名，使其超出随机范围； 3）之前的基准测试修复解决了他们的目标问题（即，用LLM编写的干扰因素降低了准确性），但无意中添加了新的缺陷（即令人难以置信的干扰因素，许多正确答案）。总体而言，MCQ 的缺陷会降低 NLP 评估的质量，但教育研究提供了一条前进的道路。我们发布 BenchMarker 来弥合各个领域并改进 MCQA 基准设计。</li>
</ul>

<h3>Title: Can One-sided Arguments Lead to Response Change in Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Pedro Cisneros-Velarde</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06260">https://arxiv.org/abs/2602.06260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06260">https://arxiv.org/pdf/2602.06260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06260]] Can One-sided Arguments Lead to Response Change in Large Language Models?(https://arxiv.org/abs/2602.06260)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.</li>
<li><strong>摘要：</strong>争论性问题需要多个观点来表达平衡的答案。大型语言模型（LLM）可以提供平衡的答案，但也可以采取单一一致的观点或拒绝回答。在本文中，我们研究是否可以通过一种简单直观的方式将这种初始反应引导至特定观点：仅提供支持该观点的片面论据。我们的系统研究有三个维度：(i) 法学硕士的回答中归纳出哪种立场，(ii) 如何提出辩论问题，(iii) 如何展示论点。我们构建了一个小型数据集，并发现对于不同的模型、参数数量和主题，意见引导发生在 (i)-(iii) 之间。转向其他论点会持续减少舆论引导。</li>
</ul>

<h3>Title: MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Junhyeok Lee, Han Jang, Kyu Sung Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06268">https://arxiv.org/abs/2602.06268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06268">https://arxiv.org/pdf/2602.06268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06268]] MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs(https://arxiv.org/abs/2602.06268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 和检索增强生成 (RAG) 系统越来越多地集成到临床工作流程中；然而，即时注入攻击可能会引导这些系统产生临床上不安全或误导性的输出。我们介绍了医疗即时注射基准 (MPIB)，这是一个数据集和基准套件，用于评估跨临床基础任务的直接即时注射和 RAG 介导的间接注射下的临床安全性。 MPIB 通过临床伤害事件率 (CHER) 强调结果级别的风险，该指标根据临床分类法衡量高严重性的临床伤害事件，并报告 CHER 和攻击成功率 (ASR)，以将指令合规性与下游患者风险分开。该基准包括通过多阶段质量门控和临床安全检查构建的 9,697 个精选实例。通过评估一组不同的基线 LLM 和防御配置的 MPIB，我们发现 ASR 和 CHER 可能存在很大差异，并且鲁棒性主要取决于对抗性指令是否出现在用户查询中或检索到的上下文中。我们发布了包含评估代码、对抗性基线和综合文档的 MPIB，以支持临床即时注射的可重复和系统性研究。代码和数据可在 GitHub（代码）和 Hugging Face（数据）上获取。</li>
</ul>

<h3>Title: VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Yancheng Wang, Osama Hanna, Ruiming Xie, Xianfeng Rui, Maohao Shen, Xuedong Zhang, Christian Fuegen, Jilong Wu, Debjyoti Paul, Arthur Guo, Zhihong Lei, Ozlem Kalinli, Qing He, Yingzhen Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06270">https://arxiv.org/abs/2602.06270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06270">https://arxiv.org/pdf/2602.06270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06270]] VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation(https://arxiv.org/abs/2602.06270)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Emotion recognition in speech presents a complex multimodal challenge, requiring comprehension of both linguistic content and vocal expressivity, particularly prosodic features such as fundamental frequency, intensity, and temporal dynamics. Although large language models (LLMs) have shown promise in reasoning over textual transcriptions for emotion recognition, they typically neglect fine-grained prosodic information, limiting their effectiveness and interpretability. In this work, we propose VowelPrompt, a linguistically grounded framework that augments LLM-based emotion recognition with interpretable, fine-grained vowel-level prosodic cues. Drawing on phonetic evidence that vowels serve as primary carriers of affective prosody, VowelPrompt extracts pitch-, energy-, and duration-based descriptors from time-aligned vowel segments, and converts these features into natural language descriptions for better interpretability. Such a design enables LLMs to jointly reason over lexical semantics and fine-grained prosodic variation. Moreover, we adopt a two-stage adaptation procedure comprising supervised fine-tuning (SFT) followed by Reinforcement Learning with Verifiable Reward (RLVR), implemented via Group Relative Policy Optimization (GRPO), to enhance reasoning capability, enforce structured output adherence, and improve generalization across domains and speaker variations. Extensive evaluations across diverse benchmark datasets demonstrate that VowelPrompt consistently outperforms state-of-the-art emotion recognition methods under zero-shot, fine-tuned, cross-domain, and cross-linguistic conditions, while enabling the generation of interpretable explanations that are jointly grounded in contextual semantics and fine-grained prosodic structure.</li>
<li><strong>摘要：</strong>语音中的情感识别提出了复杂的多模态挑战，需要理解语言内容和声音表达力，特别是基频、强度和时间动态等韵律特征。尽管大型语言模型（LLM）在情感识别的文本转录推理方面表现出了良好的前景，但它们通常会忽略细粒度的韵律信息，从而限制了其有效性和可解释性。在这项工作中，我们提出了 VowelPrompt，一个以语言为基础的框架，通过可解释的、细粒度的元音级韵律线索增强基于法学硕士的情感识别。 VowelPrompt 利用元音作为情感韵律的主要载体的语音证据，从时间对齐的元音片段中提取基于音调、能量和持续时间的描述符，并将这些特征转换为自然语言描述，以获得更好的可解释性。这样的设计使法学硕士能够联合推理词汇语义和细粒度的韵律变化。此外，我们采用了两阶段适应程序，包括监督微调（SFT）和可验证奖励强化学习（RLVR），通过组相对策略优化（GRPO）实现，以增强推理能力，强制结构化输出遵守，并提高跨领域和说话者变化的泛化能力。对不同基准数据集的广泛评估表明，VowelPrompt 在零样本、微调、跨域和跨语言条件下始终优于最先进的情感识别方法，同时能够生成基于上下文语义和细粒度韵律结构的可解释解释。</li>
</ul>

<h3>Title: RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution</h3>
<ul>
<li><strong>Authors: </strong>Isaac Picov, Ritesh Goru</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06275">https://arxiv.org/abs/2602.06275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06275">https://arxiv.org/pdf/2602.06275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06275]] RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution(https://arxiv.org/abs/2602.06275)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.</li>
<li><strong>摘要：</strong>解释闭源 LLM 输出具有挑战性，因为 API 访问会阻止基于梯度的归因，而扰动方法在依赖于重新生成的文本时成本高昂且噪音较大。我们引入了 RoPE-LIME，它是 gSMILE 的开源扩展，它将推理与解释分离：给定封闭模型的固定输出，较小的开源代理在输入扰动下根据基于概率的目标（负对数似然和散度目标）计算令牌级归因。 RoPE-LIME 结合了 (i) 基于在 RoPE 嵌入空间中计算的 Relaxed Word Mover's Distance 的局部性内核，以实现掩码下的稳定相似性，以及 (ii) Sparse-K 采样，这是一种有效的扰动策略，可在有限的预算下提高交互覆盖范围。 HotpotQA（句子特征）和手工标记的 MMLU 子集（单词特征）的实验表明，RoPE-LIME 比留一采样产生更多信息属性，并且比 gSMILE 有所改进，同时大大减少了封闭模型 API 调用。</li>
</ul>

<h3>Title: Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math</h3>
<ul>
<li><strong>Authors: </strong>Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Hyunwoo Ko, Amit Agarwal, Sunghee Ahn, Kyong-Ha Lee, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06291">https://arxiv.org/abs/2602.06291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06291">https://arxiv.org/pdf/2602.06291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06291]] Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math(https://arxiv.org/abs/2602.06291)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.</li>
<li><strong>摘要：</strong>推理模型的最新进展表明，为研究级数学生成合理的尝试可能是可以实现的，但验证仍然是一个瓶颈，消耗了稀缺的专家时间。我们假设一个有意义的解决方案应该包含足够的方法级信息，当应用于相关问题的邻域时，它应该比不正确的解决方案产生更好的下游性能。基于这个想法，我们提出了 \textbf{Consequence-Based Utility}，这是一个无预言机的评估器，通过测试每个候选者作为解决相关但可验证问题的上下文样本的价值来对每个候选者进行评分。我们的方法是根据一组原始的研究级数学问题进行评估的，每个问题都配有一个专家编写的解决方案和九个法学硕士生成的解决方案。值得注意的是，基于结果的效用在排名质量方面始终优于奖励模型、生成奖励模型和法学硕士评委。具体来说，对于 GPT-OSS-120B，它将 Acc@1 从 67.2 提高到 76.3，AUC 从 71.4 提高到 79.6，GPT-OSS-20B 的 AUC 也有类似的大幅提升（69.0 到 79.2）。此外，与 LLM-Judges 相比，它还表现出更大的求解器-评估器差距，即使在底层求解器经常无法求解的情况下，也能保持更强的正确与错误分离。</li>
</ul>

<h3>Title: Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions</h3>
<ul>
<li><strong>Authors: </strong>Nemika Tyagi, Holly Hendrix, Nelvin Licona-Guevara, Justin Mackie, Phanos Kareen, Muhammad Imran, Megan Michelle Smith, Tatiana Gallego Hernande, Chitta Baral, Olga Kellert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06307">https://arxiv.org/abs/2602.06307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06307">https://arxiv.org/pdf/2602.06307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06307]] Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions(https://arxiv.org/abs/2602.06307)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Spoken code-switching (CSW) challenges syntactic parsing in ways not observed in written text. Disfluencies, repetition, ellipsis, and discourse-driven structure routinely violate standard Universal Dependencies (UD) assumptions, causing parsers and large language models (LLMs) to fail despite strong performance on written data. These failures are compounded by rigid evaluation metrics that conflate genuine structural errors with acceptable variation. In this work, we present a systems-oriented approach to spoken CSW parsing. We introduce a linguistically grounded taxonomy of spoken CSW phenomena and SpokeBench, an expert-annotated gold benchmark designed to test spoken-language structure beyond standard UD assumptions. We further propose FLEX-UD, an ambiguity-aware evaluation metric, which reveals that existing parsing techniques perform poorly on spoken CSW by penalizing linguistically plausible analyses as errors. We then propose DECAP, a decoupled agentic parsing framework that isolates spoken-phenomena handling from core syntactic analysis. Experiments show that DECAP produces more robust and interpretable parses without retraining and achieves up to 52.6% improvements over existing parsing techniques. FLEX-UD evaluations further reveal qualitative improvements that are masked by standard metrics.</li>
<li><strong>摘要：</strong>口语语码转换（CSW）以书面文本中未观察到的方式挑战句法解析。不流畅、重复、省略和话语驱动的结构经常违反标准的通用依赖（UD）假设，导致解析器和大型语言模型（LLM）失败，尽管书面数据的性能很强。这些失败因严格的评估指标而变得更加复杂，这些指标将真正的结构错误与可接受的变化混为一谈。在这项工作中，我们提出了一种面向系统的口语 CSW 解析方法。我们引入了基于语言的 CSW 口语现象分类法和 SpokeBench，这是一个专家注释的黄金基准，旨在测试超出标准 UD 假设的口语结构。我们进一步提出了 FLEX-UD，一种歧义感知评估指标，它通过将语言上合理的分析惩罚为错误，揭示了现有的解析技术在口语 CSW 上表现不佳。然后，我们提出 DECAP，一个解耦的代理解析框架，它将口语现象处理与核心句法分析分开。实验表明，DECAP 无需重新训练即可产生更强大且可解释的解析，并且比现有解析技术实现高达 52.6% 的改进。 FLEX-UD 评估进一步揭示了标准指标掩盖的质量改进。</li>
</ul>

<h3>Title: Can Post-Training Transform LLMs into Causal Reasoners?</h3>
<ul>
<li><strong>Authors: </strong>Junqi Chen, Sirui Chen, Chaochao Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06337">https://arxiv.org/abs/2602.06337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06337">https://arxiv.org/pdf/2602.06337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06337]] Can Post-Training Transform LLMs into Causal Reasoners?(https://arxiv.org/abs/2602.06337)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at this https URL.</li>
<li><strong>摘要：</strong>因果推理对于决策至关重要，但对于非专家来说仍然具有挑战性。虽然大型语言模型（LLM）在该领域显示出前景，但其精确的因果估计能力仍然有限，并且训练后对这些能力的影响尚未得到充分探索。本文探讨了培训后可以在多大程度上增强法学硕士的因果推理能力。我们引入了 CauGym，这是一个综合数据集，包含七个核心因果训练任务和五个不同的测试集。使用该数据集，我们系统地评估了五种训练后方法：SFT、DPO、KTO、PPO 和 GRPO。在五个领域内基准和四个现有基准中，我们的实验表明，适当的后训练使较小的法学硕士能够有竞争力地执行因果推理，通常超越更大的模型。我们的 14B 参数模型在 CaLM 基准上的准确率达到 93.5%，而 OpenAI o3 的准确率仅为 55.4%。此外，经过训练的法学硕士在分布变化和噪声数据等现实条件下表现出很强的泛化性和鲁棒性。总的来说，这些发现提供了第一个系统证据，表明有针对性的后培训可以产生可靠且稳健的基于法学硕士的因果推理。我们的数据和 GRPO 模型可从此 https URL 获取。</li>
</ul>

<h3>Title: SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass</h3>
<ul>
<li><strong>Authors: </strong>Yewei Liu, Xiyuan Wang, Yansheng Mao, Yoav Gelbery, Haggai Maron, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06358">https://arxiv.org/abs/2602.06358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06358">https://arxiv.org/pdf/2602.06358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06358]] SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass(https://arxiv.org/abs/2602.06358)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at this https URL</li>
<li><strong>摘要：</strong>我们提出了 SHINE（可扩展的超上下文网络），这是一种可扩展的超网络，可以将不同的有意义的上下文映射到大型语言模型（LLM）的高质量 LoRA 适配器中。通过在上下文超网络设计中重用冻结的LLM自身参数并引入架构创新，SHINE克服了先前超网络的关键限制，并以相对较少的参数实现了强大的表达能力。我们引入了预训练和指令微调管道，并训练我们的超网络在一次前向传递中从不同的有意义的上下文中生成高质量的 LoRA 适配器。它无需任何微调即可更新LLM参数，无需直接访问上下文即可立即启用与上下文相关的复杂问答任务，有效地将上下文知识一次性转换为参数内知识。我们的工作在各种任务上取得了出色的结果，与基于 SFT 的 LLM 适配相比，大大节省了时间、计算和内存成本，并显示出巨大的扩展潜力。我们的代码可在此 https URL 获取</li>
</ul>

<h3>Title: Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production</h3>
<ul>
<li><strong>Authors: </strong>Alberto Andres Valdes Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06370">https://arxiv.org/abs/2602.06370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06370">https://arxiv.org/pdf/2602.06370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06370]] Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production(https://arxiv.org/abs/2602.06370)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems. In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost. We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting. Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.</li>
<li><strong>摘要：</strong>GPT-4o 和 Claude Sonnet 4.5 等大型语言模型 (LLM) 在开放式推理和生成语言任务中表现出了强大的能力，从而在广泛的 NLP 应用中得到广泛采用。然而，对于具有固定标签空间的结构化文本分类问题，模型选择通常仅由预测性能驱动，而忽略了生产系统中遇到的操作约束。在这项工作中，我们对文本分类的两种对比范例进行了系统比较：基于零样本和少样本提示的大型语言模型，以及完全微调的仅编码器架构。我们通过四个规范基准（IMDB、SST-2、AG News 和 DBPedia）评估这些方法，测量预测质量（宏观 F1）、推理延迟和货币成本。我们将模型评估构建为多目标决策问题，并使用帕累托前沿预测和反映不同部署机制的参数化效用函数来分析权衡。我们的结果表明，与零样本和少样本 LLM 提示相比，BERT 系列中基于编码器的微调模型可实现具有竞争力且通常更优越的分类性能，同时运行成本和延迟要低一到两个数量级。总的来说，我们的研究结果表明，不加区别地使用大型语言模型来执行标准文本分类工作负载可能会导致系统级结果不理想。相反，经过微调的编码器成为结构化 NLP 管道的强大且高效的组件，而 LLM 则更适合作为混合架构中的补充元素。我们发布了所有代码、数据集和评估协议，以支持可重复性和具有成本意识的 NLP 系统设计。</li>
</ul>

<h3>Title: ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tianqiang Yan, Sihan Shang, Yuheng Li, Song Qiu, Hao Peng, Wenjian Luo, Jue Xie, Lizhen Qu, Yuan Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06373">https://arxiv.org/abs/2602.06373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06373">https://arxiv.org/pdf/2602.06373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06373]] ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis(https://arxiv.org/abs/2602.06373)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While self-reflection can enhance language model reliability, its underlying mechanisms remain opaque, with existing analyses often yielding correlation-based insights that fail to generalize. To address this, we introduce \textbf{\texttt{ReBeCA}} (self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}}havior explained through \textbf{\texttt{C}}ausal \textbf{\texttt{A}}nalysis), a framework that unveils the interpretable behavioral hierarchy governing the self-reflection outcome. By modeling self-reflection trajectories as causal graphs, ReBeCA isolates genuine determinants of performance through a three-stage Invariant Causal Prediction (ICP) pipeline. We establish three critical findings: (1) \textbf{Behavioral hierarchy:} Semantic behaviors of the model influence final self-reflection results hierarchically: directly or indirectly; (2) \textbf{Causation matters:} Generalizability in self-reflection effects is limited to just a few semantic behaviors; (3) \textbf{More $\mathbf{\neq}$ better:} The confluence of seemingly positive semantic behaviors, even among direct causal factors, can impair the efficacy of self-reflection. ICP-based verification identifies sparse causal parents achieving up to $49.6\%$ structural likelihood gains, stable across tasks where correlation-based patterns fail. Intervention studies on novel datasets confirm these causal relationships hold out-of-distribution ($p = .013, \eta^2_\mathrm{p} = .071$). ReBeCA thus provides a rigorous methodology for disentangling genuine causal mechanisms from spurious associations in self-reflection dynamics.</li>
<li><strong>摘要：</strong>虽然自我反思可以增强语言模型的可靠性，但其底层机制仍然不透明，现有的分析通常会产生基于相关性的见解，但无法概括。为了解决这个问题，我们引入了 \textbf{\texttt{ReBeCA}} （通过 \textbf{\texttt{C}}ausal \textbf{\texttt{A}} 分析解释的 self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}} 行为），一个框架，揭示了控制自我反思结果的可解释行为层次结构。通过将自我反思轨迹建模为因果图，ReBeCA 通过三阶段不变因果预测 (ICP) 管道隔离了性能的真正决定因素。我们建立了三个关键发现：（1）\textbf{行为层次结构：}模型的语义行为按层次结构影响最终的自我反思结果：直接或间接； (2) \textbf{因果关系很重要：}自我反思效应的普遍性仅限于少数语义行为； (3) \textbf{更多$\mathbf{\neq}$更好：}看似积极的语义行为的融合，即使是直接因果因素，也会损害自我反思的功效。基于 ICP 的验证识别出稀疏因果父母，实现了高达 $49.6\%$ 的结构似然增益，在基于相关性的模式失败的任务中保持稳定。对新数据集的干预研究证实这些因果关系保持分布外（$p = .013，\eta^2_\mathrm{p} = .071$）。因此，ReBeCA 提供了一种严格的方法论，用于将自我反思动态中的真实因果机制与虚假关联区分开来。</li>
</ul>

<h3>Title: FMBench: Adaptive Large Language Model Output Formatting</h3>
<ul>
<li><strong>Authors: </strong>Yaoting Wang, Yun Zhou, Henghui Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06384">https://arxiv.org/abs/2602.06384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06384">https://arxiv.org/pdf/2602.06384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06384]] FMBench: Adaptive Large Language Model Output Formatting(https://arxiv.org/abs/2602.06384)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Producing outputs that satisfy both semantic intent and format constraints is essential for deploying large language models in user-facing and system-integrated workflows. In this work, we focus on Markdown formatting, which is ubiquitous in assistants, documentation, and tool-augmented pipelines but still prone to subtle, hard-to-detect errors (e.g., broken lists, malformed tables, inconsistent headings, and invalid code blocks) that can significantly degrade downstream usability. We present FMBench, a benchmark for adaptive Markdown output formatting that evaluates models under a wide range of instruction-following scenarios with diverse structural requirements. FMBench emphasizes real-world formatting behaviors such as multi-level organization, mixed content (natural language interleaved with lists/tables/code), and strict adherence to user-specified layout constraints. To improve Markdown compliance without relying on hard decoding constraints, we propose a lightweight alignment pipeline that combines supervised fine-tuning (SFT) with reinforcement learning fine-tuning. Starting from a base model, we first perform SFT on instruction-response pairs, and then optimize a composite objective that balances semantic fidelity with structural correctness. Experiments on two model families (OpenPangu and Qwen) show that SFT consistently improves semantic alignment, while reinforcement learning provides additional gains in robustness to challenging Markdown instructions when initialized from a strong SFT policy. Our results also reveal an inherent trade-off between semantic and structural objectives, highlighting the importance of carefully designed rewards for reliable formatted generation. Code is available at: this https URL.</li>
<li><strong>摘要：</strong>生成满足语义意图和格式约束的输出对于在面向用户和系统集成的工作流程中部署大型语言模型至关重要。在这项工作中，我们重点关注 Markdown 格式，它在助手、文档和工具增强管道中无处不在，但仍然容易出现微妙的、难以检测的错误（例如，损坏的列表、格式错误的表格、不一致的标题和无效的代码块），这些错误可能会显着降低下游可用性。我们推出了 FMBench，它是自适应 Markdown 输出格式的基准，可在具有不同结构要求的各种指令跟踪场景下评估模型。 FMBench 强调现实世界的格式化行为，例如多级组织、混合内容（自然语言与列表/表格/代码交错）以及严格遵守用户指定的布局约束。为了在不依赖硬解码约束的情况下提高 Markdown 合规性，我们提出了一种轻量级对齐管道，它将监督微调（SFT）与强化学习微调相结合。从基本模型开始，我们首先对指令-响应对执行 SFT，然后优化平衡语义保真度与结构正确性的复合目标。对两个模型系列（OpenPangu 和 Qwen）的实验表明，SFT 持续改进了语义对齐，而强化学习在从强大的 SFT 策略初始化时，为挑战 Markdown 指令提供了额外的鲁棒性增益。我们的结果还揭示了语义和结构目标之间固有的权衡，强调了精心设计的奖励对于可靠的格式化生成的重要性。代码可在以下位置获取：此 https URL。</li>
</ul>

<h3>Title: Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding</h3>
<ul>
<li><strong>Authors: </strong>Daisuke Oba, Danushka Bollegala, Masahiro Kaneko, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06412">https://arxiv.org/abs/2602.06412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06412">https://arxiv.org/pdf/2602.06412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06412]] Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding(https://arxiv.org/abs/2602.06412)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at this https URL .</li>
<li><strong>摘要：</strong>屏蔽扩散语言模型通过迭代采样生成序列，逐步揭开令牌的屏蔽。然而，他们仍然在每一步重新计算每个令牌位置的注意力和前馈块——即使许多未屏蔽的令牌基本上是固定的，从而导致计算上的大量浪费。我们提出 SureLock：当未屏蔽位置的后验在步骤中稳定下来（我们的确定条件）时，我们锁定该位置 - 此后跳过其查询投影和前馈子层 - 同时缓存其注意力键和值，以便其他位置可以继续关注它。这将主要的每次迭代计算成本从 $O(N^2d)$ 降低到 $O(MNd)$，其中 $N$ 是序列长度，$M$ 是解锁令牌位置的数量，$d$ 是模型维度。实际上，随着迭代的进行，$M$ 会减少，从而节省大量成本。在 LLaDA-8B 上，相对于没有锁定的相同采样器，SureLock 将算法 FLOP 降低了 30--50%，同时保持可比的生成质量。我们还提供了理论分析来证明 SureLock 的设计原理：仅在锁定步骤监控局部 KL 就足以限制最终令牌概率的偏差。我们的代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Shang, Yuxi Sun, Jing Ma, Xin Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06423">https://arxiv.org/abs/2602.06423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06423">https://arxiv.org/pdf/2602.06423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06423]] On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation(https://arxiv.org/abs/2602.06423)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.</li>
<li><strong>摘要：</strong>幽默是日常生活中常用且复杂的人类语言。幽默生成，尤其是在多模态场景中，对于大型语言模型（LLM）来说是一项具有挑战性的任务，它通常是图像的有趣标题生成，需要视觉理解、幽默推理、创造性想象力等。现有的基于法学硕士的方法依赖于推理链或自我完善，其创造力和可解释性有限。为了解决这些瓶颈，我们基于基本幽默理论 GTVH 开发了一种新颖的基于 LLM 的幽默生成机制。为了制作有趣且与脚本相反的字幕，我们引入了一个幽默理论驱动的多角色 LLM 协作框架，并增强了幽默检索 (HOMER)。该框架由三个基于法学硕士的角色组成：（1）冲突脚本提取器，将幽默融入关键脚本对立中，形成字幕生成的基础； （2）检索增强的分层想象器，识别关键的幽默目标，并通过想象树结构的不同关联来扩展它们的创造性空间； （3）字幕生成器，根据所获得的知识生成有趣且多样化的字幕。对两个《纽约客卡通》基准数据集进行的广泛实验表明，HOMER 在多模式幽默字幕方面优于最先进的基线和强大的 LLM 推理策略。</li>
</ul>

<h3>Title: TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking</h3>
<ul>
<li><strong>Authors: </strong>Sung-Hoon Yoon, Ruizhi Qian, Minda Zhao, Weiyue Li, Mengyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06440">https://arxiv.org/abs/2602.06440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06440">https://arxiv.org/pdf/2602.06440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06440]] TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking(https://arxiv.org/abs/2602.06440)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为许多领域不可或缺的一部分，因此其安全性成为重中之重。之前的越狱研究已经探索了多种方法，包括即时优化、自动红队、混淆和基于强化学习 (RL) 的方法。然而，大多数现有技术无法有效利用早期交互回合中暴露的漏洞，导致攻击效率低下且不稳定。由于越狱涉及顺序交互，其中每个响应都会影响未来的行动，因此强化学习为这个问题提供了一个自然的框架。受此启发，我们提出了一个基于强化学习的历史感知越狱框架，该框架可以分析和重新衡量先前步骤中的漏洞信号，以指导未来的决策。我们表明，仅合并历史信息就可以提高越狱成功率。基于这一见解，我们引入了一种基于注意力的重新加权机制，该机制突出显示交互历史记录中的关键漏洞，从而以更少的查询实现更有效的探索。 AdvBench 和 HarmBench 上的大量实验表明，我们的方法实现了最先进的越狱性能，同时显着提高了查询效率。这些结果强调了历史漏洞信号在强化学习驱动的越狱策略中的重要性，并为推进法学硕士保障措施的对抗性研究提供了原则性途径。</li>
</ul>

<h3>Title: CORE: Comprehensive Ontological Relation Evaluation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Satyam Dwivedi, Sanjukta Ghosh, Shivam Dwivedi, Nishi Kumari, Anil Thakur, Anurag Purushottam, Deepak Alok, Praveen Gatla, Manjuprasad B, Bipasha Patgiri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06446">https://arxiv.org/abs/2602.06446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06446">https://arxiv.org/pdf/2602.06446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06446]] CORE: Comprehensive Ontological Relation Evaluation for Large Language Models(https://arxiv.org/abs/2602.06446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在许多推理基准上表现良好，但现有的评估很少评估它们区分有意义的语义关系和真正的无关性的能力。我们引入了 CORE（综合本体关系评估），这是一个涵盖 74 个学科的 225K 多项选择题的数据集，以及包含 203 个经过严格验证的问题的通用领域开源基准（Cohen's Kappa = 1.0），涵盖 24 种语义关系类型，并且对不相关的对进行平等表示。来自 1,000 多名参与者的人类基线达到了 92.6% 的准确率（对于不相关的配对，准确率为 95.1%）。相比之下，29 个最先进的法学硕士的总体准确率达到了 48.25-70.9%，尽管分配了相似的置信度 (92-94%)，但在相关对 (86.5-100%) 上的表现接近上限，但在不相关对 (0-41.35%) 上却严重退化。不相关对的预期校准误差增加 2-4 倍，平均语义崩溃率为 37.6%，表明系统生成了虚假关系。在 CORE 225K MCQ 数据集上，准确率进一步下降至约 2%，凸显了特定领域语义推理中的巨大挑战。我们认为无关性推理是法学硕士评估和安全性的一个关键的、被低估的前沿领域。</li>
</ul>

<h3>Title: Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xinxin Lin, Guangxin Dai, Yi Zhong, Xiang Li, Xue Xiao, Yixin Zhang, Zhengdong Wu, Yongbo Zheng, Runchuan Zhu, Ming Zhao, Huizi Yu, Shuo Wu, Jun Zhao, Lingming Hu, Yumei Wang, Ping Yin, Joey W.Y. Chan, Ngan Yin Chan, Sijing Chen, Yun Kwok Wing, Lin Lu, Xin Ma, Lizhou Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06449">https://arxiv.org/abs/2602.06449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06449">https://arxiv.org/pdf/2602.06449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06449]] Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning(https://arxiv.org/abs/2602.06449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.</li>
<li><strong>摘要：</strong>大语言模型（LLM）在医疗决策支持方面具有变革潜力，但其在精神病学中的应用仍然受到幻觉和肤浅推理的限制。这种限制在轻参数法学硕士中尤其严重，这对于隐私保护和高效的临床部署至关重要。现有的培训范式优先考虑语言的流畅性而不是结构化的临床逻辑，并导致与专业诊断认知的根本失调。在这里，我们介绍 ClinMPO，这是一个强化学习框架，旨在将法学硕士的内部推理与专业精神病学实践相结合。该框架采用了专门的奖励模型，该模型在源自 4,474 篇精神病学期刊文章的数据集上独立训练，并根据循证医学原则构建。我们在基准的一个看不见的子集上评估了 ClinMPO，该子集旨在将推理能力与死记硬背分开。该测试集包含领先的大参数法学硕士始终失败的项目。我们将符合 ClinMPO 的轻型 LLM 表现与 300 名医学生的队列进行了比较。经过 ClinMPO 调整的 Qwen3-8B 模型在这些复杂病例上的诊断准确率达到了 31.4%，超过了人类 30.8% 的基准。这些结果表明，医学证据引导的优化使轻参数法学硕士能够掌握复杂的推理任务。我们的研究结果表明，明确的认知一致性为可靠且安全的精神病学决策支持提供了一条可扩展的途径。</li>
</ul>

<h3>Title: Diffusion-State Policy Optimization for Masked Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daisuke Oba, Hiroki Furuta, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06462">https://arxiv.org/abs/2602.06462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06462">https://arxiv.org/pdf/2602.06462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06462]] Diffusion-State Policy Optimization for Masked Diffusion Language Models(https://arxiv.org/abs/2602.06462)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at this https URL .</li>
<li><strong>摘要：</strong>屏蔽扩散语言模型是通过在多个去噪步骤中迭代填充屏蔽标记来生成的，因此仅从最终完成时的最终奖励中学习会产生比中间决策更粗略的信用分配。我们提出了 DiSPO（扩散状态策略优化），这是一个插件信用分配层，可以直接优化中间填充决策。在选定的中间屏蔽状态下，DiSPO 通过从 rollout 缓存的 logits 对当前屏蔽位置的填充进行重新采样来进行分支，对结果完成进行评分，并仅更新新填充的标记 - 无需额外的多步扩散 rollout。我们形式化了分支完成的固定状态目标，并得出了一个策略梯度估计器，可以使用相同的推出与终端反馈策略优化相结合。在 LLaDA-8B-Instruct 上，DiSPO 在匹配的推出计算和优化器步骤下持续改进数学和规划基准的终端反馈 diffu-GRPO 基线。我们的代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: Improve Large Language Model Systems with User Logs</h3>
<ul>
<li><strong>Authors: </strong>Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06470">https://arxiv.org/abs/2602.06470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06470">https://arxiv.org/pdf/2602.06470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06470]] Improve Large Language Model Systems with User Logs(https://arxiv.org/abs/2602.06470)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at this https URL .</li>
<li><strong>摘要：</strong>长期以来，扩展训练数据和模型参数一直推动着大型语言模型 (LLM) 的进步，但这种范式越来越受到高质量数据稀缺和计算成本上升带来的回报递减的限制。因此，最近的工作越来越关注从现实世界的部署中持续学习，其中用户交互日志提供了真实的人类反馈和程序知识的丰富来源。然而，由于用户日志的非结构化和嘈杂性质，从用户日志中学习具有挑战性。普通的 LLM 系统通常很难区分有用的反馈信号和嘈杂的用户行为，并且用户日志收集和模型优化（例如，离策略优化问题）之间的差异进一步加剧了这个问题。为此，我们提出UNO（用户日志驱动优化），这是一个利用用户日志改进LLM系统（LLMsys）的统一框架。 UNO首先将日志提炼成半结构化规则和偏好对，然后采用查询和反馈驱动的聚类来管理数据异构性，最后量化模型先验知识和日志数据之间的认知差距。该评估指导 LLMsys 自适应地过滤掉噪音反馈，并为从用户日志中提取的主要和反思体验构建不同的模块，从而改善未来的响应。大量实验表明，UNO 实现了最先进的有效性和效率，显着优于检索增强生成 (RAG) 和基于内存的基线。我们已在此 https URL 开源我们的代码。</li>
</ul>

<h3>Title: Revisiting the Shape Convention of Transformer Language Models</h3>
<ul>
<li><strong>Authors: </strong>Feng-Ting Liao, Meng-Hsi Chen, Guan-Ting Yi, Da-shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06471">https://arxiv.org/abs/2602.06471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06471">https://arxiv.org/pdf/2602.06471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06471]] Revisiting the Shape Convention of Transformer Language Models(https://arxiv.org/abs/2602.06471)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.</li>
<li><strong>摘要：</strong>Dense Transformer 语言模型在很大程度上遵循一个一致的架构形状：每一层都由一个注意力模块组成，后跟一个具有窄-宽-窄 MLP 的前馈网络 (FFN)，以 2 到 4 之间的扩展比率将大多数参数分配给 MLP。受最近结果的启发，残差宽-窄-宽（沙漏）MLP 提供卓越的函数逼近能力，我们重新审视 Transformer 中长期存在的 MLP 形状约定，挑战了窄-宽-窄设计。为了研究这一点，我们开发了一种 Transformer 变体，用更深的沙漏形 FFN 代替传统的 FFN，其中包含一堆通过残余路径连接的沙漏子 MLP。我们认为更深但更轻的沙漏 FFN 可以作为传统 FFN 的竞争替代品，并且可以更有效地利用使用更轻的沙漏 FFN 节省的参数，例如通过在固定预算下扩大模型隐藏维度。我们通过跨模型尺度的实证验证证实了这些：沙漏 FFN 在高达 400M 的范围内优于传统 FFN，并在更大尺度上达到与 1B 参数相当的性能；具有减少的 FFN 和增加的注意力参数的沙漏 FFN 变体在匹配的预算下显示出比传统配置的持续改进。总之，这些发现为最近的工作提供了新的思路，并促使人们重新思考窄-宽-窄 MLP 约定以及注意力和 FFN 之间的平衡，以实现高效和富有表现力的现代语言模型。</li>
</ul>

<h3>Title: Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Minjeong Ban, Jeonghwan Choi, Hyangsuk Min, Nicole Hee-Yeon Kim, Minseok Kim, Jae-Gil Lee, Hwanjun Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06526">https://arxiv.org/abs/2602.06526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06526">https://arxiv.org/pdf/2602.06526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06526]] Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks(https://arxiv.org/abs/2602.06526)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at this https URL.</li>
<li><strong>摘要：</strong>由于包含未标记的相关块的 IR 基准数据集不完整，信息检索 (IR) 评估仍然具有挑战性。虽然法学硕士和法学硕士-人类混合策略减少了昂贵的人力成本，但它们仍然容易出现法学硕士过度自信和无效的人工智能到人类的升级。为了解决这个问题，我们提出了 DREAM，这是一个与法学硕士代理人合作的基于多轮辩论的相关性评估框架，建立在反对的初始立场和迭代的相互批评的基础上。通过我们基于协议的辩论，它可以为某些情况提供更准确的标记，并为不确定的情况提供更可靠的人工智能到人类的升级，仅需要 3.5% 的人类参与即可实现 95.2% 的标记准确率。使用 DREAM，我们构建了 BRIDGE，这是一个改进的基准，通过发现 29,824 个缺失的相关块来减轻评估偏差并实现更公平的检索器比较。然后，我们重新对 IR 系统进行基准测试并将评估扩展到 RAG，结果表明未解决的漏洞不仅会扭曲检索器排名，还会导致检索生成错位。相关性评估框架可在 https://github.com/DISL-Lab/DREAM-ICLR-26 获取； BRIDGE 数据集可从此 https URL 获取。</li>
</ul>

<h3>Title: MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew</h3>
<ul>
<li><strong>Authors: </strong>Andy Rosenbaum, Assaf Siani, Ilan Kernerman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06546">https://arxiv.org/abs/2602.06546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06546">https://arxiv.org/pdf/2602.06546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06546]] MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew(https://arxiv.org/abs/2602.06546)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>We release this http URL-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. this http URL-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. this http URL-he and our experimental results enable future research on this under-resourced language pair.</li>
<li><strong>摘要：</strong>我们发布了此 http URL-he：据我们所知，这是第一个公开可用的机器翻译质量评估英语-希伯来语基准。这个http URL - 他包含来自WMT24++的959个英语片段，每个片段都配有希伯来语的机器翻译，以及由三位人类专家注释的翻译质量的直接评估分数。我们对 ChatGPT 提示、TransQuest 和 CometKiwi 进行了基准测试，结果表明，这三个模型的集成比最佳单一模型 (CometKiwi) 的性能高出 Pearson 6.4 个百分点和 Spearman 5.6 个百分点。 TransQuest 和 CometKiwi 的微调实验表明，全模型更新对过度拟合和分布崩溃很敏感，但参数高效的方法（LoRA、BitFit 和 FHead，即仅微调分类头）可以稳定训练并产生 2-3 个百分点的改进。这个http URL - 他和我们的实验结果使得未来对这个资源贫乏的语言对的研究成为可能。</li>
</ul>

<h3>Title: Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Baichuan-M3 Team: Chengfeng Dou, Fan Yang, Fei Li, Jiyuan Jia, Qiang Ju, Shuai Wang, Tianpeng Li, Xiangrong Zeng, Yijie Zhou, Hongda Zhang, Jinyang Tai, Linzhuang Sun, Peidong Guo, Yichuan Mo, Xiaochuan Wang, Hengfu Cui, Zhishou Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06570">https://arxiv.org/abs/2602.06570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06570">https://arxiv.org/pdf/2602.06570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06570]] Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making(https://arxiv.org/abs/2602.06570)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination</a></li>
<li><strong>Abstract: </strong>We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at this https URL.</li>
<li><strong>摘要：</strong>我们推出了 Baichuan-M3，这是一种医学增强型大型语言模型，旨在将被动问答模式转变为主动的临床级决策支持。为了解决现有系统在开放式咨询中的局限性，百川-M3利用专门的培训管道来模拟医生的系统工作流程。主要能力包括：(i) 主动获取信息以解决歧义； (ii) 将分散的证据统一为连贯的诊断的长期推理； (iii) 自适应幻觉抑制以确保事实可靠性。实证评估表明，百川-M3在HealthBench、新推出的HealthBench-Hallu和ScanBench上取得了最先进的结果，在临床询问、咨询和安全性方面显着优于GPT-5.2。这些模型可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Deqian Kong, Minglu Zhao, Aoyang Qin, Bo Pang, Chenxin Tao, David Hartmann, Edouardo Honig, Dehong Xu, Amit Kumar, Matt Sarte, Chuan Li, Jianwen Xie, Ying Nian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06584">https://arxiv.org/abs/2602.06584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06584">https://arxiv.org/pdf/2602.06584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06584]] Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning(https://arxiv.org/abs/2602.06584)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought</a></li>
<li><strong>Abstract: </strong>Standard chain-of-thought reasoning generates a solution in a single forward pass, committing irrevocably to each token and lacking a mechanism to recover from early errors. We introduce Inference-Time Rethinking, a generative framework that enables iterative self-correction by decoupling declarative latent thought vectors from procedural generation. We factorize reasoning into a continuous latent thought vector (what to reason about) and a decoder that verbalizes the trace conditioned on this vector (how to reason). Beyond serving as a declarative buffer, latent thought vectors compress the reasoning structure into a continuous representation that abstracts away surface-level token variability, making gradient-based optimization over reasoning strategies well-posed. Our prior model maps unstructured noise to a learned manifold of valid reasoning patterns, and at test time we employ a Gibbs-style procedure that alternates between generating a candidate trace and optimizing the latent vector to better explain that trace, effectively navigating the latent manifold to refine the reasoning strategy. Training a 0.2B-parameter model from scratch on GSM8K, our method with 30 rethinking iterations surpasses baselines with 10 to 15 times more parameters, including a 3B counterpart. This result demonstrates that effective mathematical reasoning can emerge from sophisticated inference-time computation rather than solely from massive parameter counts.</li>
<li><strong>摘要：</strong>标准的思想链推理在一次前向传递中生成一个解决方案，不可撤销地承诺每个令牌，并且缺乏从早期错误中恢复的机制。我们引入了推理时间重新思考，这是一种生成框架，可以通过将声明性潜在思维向量与程序生成分离来实现迭代自我校正。我们将推理分解为一个连续的潜在思维向量（推理什么）和一个解码器，该解码器用语言表达以该向量为条件的轨迹（如何推理）。除了充当声明性缓冲区之外，潜在思维向量还将推理结构压缩为连续表示，从而抽象出表面级标记的可变性，从而使基于梯度的推理策略优化变得合理。我们先前的模型将非结构化噪声映射到有效推理模式的学习流形，并且在测试时，我们采用吉布斯式程序，在生成候选轨迹和优化潜在向量之间交替，以更好地解释该轨迹，有效地导航潜在流形以完善推理策略。在 GSM8K 上从头开始训练 0.2B 参数模型，我们的方法经过 30 次重新思考迭代，参数超出了基线 10 到 15 倍，其中包括 3B 对应模型。这一结果表明，有效的数学推理可以从复杂的推理时间计算中产生，而不仅仅是从大量的参数计数中产生。</li>
</ul>

<h3>Title: Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyuan Hao, Zhuo Li, Wu Li, Fangming Liu, Min Zhang, Jing Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06600">https://arxiv.org/abs/2602.06600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06600">https://arxiv.org/pdf/2602.06600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06600]] Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning(https://arxiv.org/abs/2602.06600)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} $\Delta\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at this https URL.</li>
<li><strong>摘要：</strong>大型推理模型 (LRM) 中的测试时计算分配被广泛使用，并在数学问题解决、代码综合和规划中得到应用。最近的工作通过扩展自我一致性和并行思维、添加通用的“思维标记”并提示模型在回答之前重新阅读问题来解决这个问题。不幸的是，这些方法要么注入与任务无关的令牌，要么强制执行启发式算法，这些启发式算法无法解释（并且常常忽略）许多 LRM 在其内部链头部表现出的\emph{自发}重复。相比之下，我们分析并利用模型重述问题的倾向，我们将其称为 \emph{Echo of Prompt (EOP)}，作为一种前置的计算整形机制。我们通过将回声消除作为基于拒绝的调节并将 \emph{Echo Likelihood Gap} $\Delta\mathcal{L}$ 定义为可计算代理来形式化其概率成本。这提供了缺失的理论联系，将早期重复与似然增益和下游准确性联系起来。然而，它本身并没有指定如何利用 EOP。因此，我们开发了 \emph{Echo-Distilled SFT (ED-SFT)} 来通过监督微调灌输“echo-then-reason”模式，并开发 \emph{Echoic Prompting (EP)} 来重新调整模型中间轨迹而无需训练。虽然前景广阔，但量化冗长的好处并非易事。因此，我们将长度和后缀控制的似然分析与分层注意力研究一起进行，表明 EOP 增加了中间层对答案前缀注意力的答案，这与 \emph{注意力重新聚焦} 机制一致。我们在相同的解码设置和预算下对 GSM8K、MathQA、Hendrycks-MATH、AIME24 和 MATH-500 进行评估，并发现与基线相比的一致增益。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Singh, Ziwei Xu, A. V. Subramanyam, Mohan Kankanhalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06623">https://arxiv.org/abs/2602.06623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06623">https://arxiv.org/pdf/2602.06623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06623]] Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention(https://arxiv.org/abs/2602.06623)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是功能强大的文本生成器，但即使给出看似无害的提示，它们也可能产生有毒或有害的内容。这提出了严重的安全挑战，并可能造成现实世界的伤害。毒性通常是微妙的并且依赖于上下文，因此很难在标记级别或通过粗略的句子级别信号进行检测。此外，减轻毒性的努力常常面临安全性和生成文本的连贯性或流畅性之间的权衡。在这项工作中，我们提出了一种有针对性的子空间干预策略，用于从底层模型表示中识别和抑制隐藏的有毒模式，同时保留生成安全流畅内容的整体能力。在 RealToxicityPrompts 上，与现有基线相比，我们的方法实现了强大的缓解性能，并且对推理复杂性的影响最小。在多个法学硕士中，我们的方法将最先进的解毒系统的毒性降低了 8-20%，同时保持相当的流畅性。通过广泛的定量和定性分析，我们表明我们的方法在不损害生成性能的情况下实现了有效的毒性降低，始终优于现有基线。</li>
</ul>

<h3>Title: FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Bo Yang, Lanfei Feng, Yunkui Chen, Yu Zhang, Xiao Xu, Shijian Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06625">https://arxiv.org/abs/2602.06625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06625">https://arxiv.org/pdf/2602.06625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06625]] FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge(https://arxiv.org/abs/2602.06625)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.</li>
<li><strong>摘要：</strong>现有的法学硕士作为法官系统受到三个基本限制：对特定于任务和领域的评估标准的适应性有限，由位置、长度、格式和模型来源等非语义线索驱动的系统偏差，以及导致不同评估模式（例如，逐点与成对）的矛盾判断的评估不一致。为了解决这些问题，我们提出了 FairJudge，一个适应性强、去偏见且一致的法学硕士法官。与之前将法官视为静态评估者的方法不同，FairJudge 将法官行为本身建模为可学习且规范化的策略。从以数据为中心的角度来看，我们构建了一个高信息密度的判断数据集，明确地注入与评估行为一致的监督信号。在此数据集的基础上，我们采用了课程式的 SFT-DPO-GRPO 训练范式，逐步调整规则遵守、偏见缓解和跨模式一致性，同时避免灾难性遗忘。多个内部和公共基准的实验结果表明，FairJudge 持续改进一致性和 F1，减少非语义偏差，并且性能明显优于较大的指令调整 LLM。所有资源将在接受后公开发布，以方便未来的研究。</li>
</ul>

<h3>Title: Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Jianfeng Si, Lin Sun, Weihong Lin, Xiangzheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06650">https://arxiv.org/abs/2602.06650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06650">https://arxiv.org/pdf/2602.06650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06650]] Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought(https://arxiv.org/abs/2602.06650)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent. Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.</li>
<li><strong>摘要：</strong>由于缺乏运行时可控性的静态、一刀切的安全策略，大型语言模型 (LLM) 面临着基本的安全性与有用性的权衡，因此很难根据不同的应用程序需求定制响应。因此，模型可能会过度拒绝良性请求或限制有害请求。我们提出了 \textbf{PACT} （通过思想链进行即时配置的行动），这是一个通过明确的、风险意识推理进行动态安全控制的框架。 PACT 在分层策略架构下运行：不可覆盖的全局安全策略为关键风险（例如儿童安全、暴力极端主义）建立了不可变的边界，而用户定义的策略可以引入特定于域的（非全局）风险类别并指定标签到操作行为，以提高实际部署设置中的实用性。该框架将安全决策分解为结构化的 Classify$\rightarrow$Act 路径，将查询路由到适当的操作（遵守、指导或拒绝）并使决策过程透明。大量的实验表明，PACT 在全局政策评估下实现了接近最先进的安全性能，同时在特定于用户的政策评估下获得了最佳的可控性，有效地减轻了安全性与有用性的权衡。我们将发布 PACT 模型套件、训练数据和评估协议，以促进可控安全对准的可重复研究。</li>
</ul>

<h3>Title: Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity</h3>
<ul>
<li><strong>Authors: </strong>Bowen Zhang, Meiyi Wang, Harold Soh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06665">https://arxiv.org/abs/2602.06665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06665">https://arxiv.org/pdf/2602.06665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06665]] Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity(https://arxiv.org/abs/2602.06665)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we hypothesize that mode collapse can be localized to specific layers and that restoring a carefully chosen range of layers to their pre-trained weights can recover diversity while maintaining high output quality. To validate this hypothesis and decide which layers to restore, we design a proxy task -- Constrained Random Character(CRC) -- with an explicit validity set and a natural diversity objective. Results on CRC reveal a clear diversity-validity trade-off across restoration ranges and identify configurations that increase diversity with minimal quality loss. Based on these findings, we propose Selective Layer Restoration (SLR), a training-free method that restores selected layers in a post-trained model to their pre-trained weights, yielding a hybrid model with the same architecture and parameter count, incurring no additional inference cost. Across three different tasks (creative writing, open-ended question answering, and multi-step reasoning) and three different model families (Llama, Qwen, and Gemma), we find SLR can consistently and substantially improve output diversity while maintaining high output quality.</li>
<li><strong>摘要：</strong>训练后可以改善大型语言模型 (LLM) 的指令跟踪和有用性，但通常会降低生成多样性，从而导致开放式设置中的重复输出，这种现象称为模式崩溃。受 LLM 层发挥不同功能作用的证据的启发，我们假设模式崩溃可以局限于特定层，并且将精心选择的层范围恢复为其预训练的权重可以恢复多样性，同时保持高输出质量。为了验证这个假设并决定恢复哪些层，我们设计了一个代理任务——约束随机字符（CRC）——具有明确的有效性集和自然多样性目标。 CRC 的结果揭示了恢复范围内明显的多样性与有效性权衡，并确定了以最小的质量损失增加多样性的配置。基于这些发现，我们提出了选择性层恢复（SLR），这是一种免训练方法，可将训练后模型中的选定层恢复为其预训练的权重，从而生成具有相同架构和参数数量的混合模型，并且不会产生额外的推理成本。在三种不同的任务（创意写作、开放式问答和多步推理）和三种不同的模型系列（Llama、Qwen 和 Gemma）中，我们发现 SLR 可以持续且大幅提高输出多样性，同时保持高输出质量。</li>
</ul>

<h3>Title: compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data</h3>
<ul>
<li><strong>Authors: </strong>Lucie Termignon, Simonas Zilinskas, Hadrien Pélissier, Aurélien Barrot, Nicolas Chesnais, Elie Gavoty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06669">https://arxiv.org/abs/2602.06669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06669">https://arxiv.org/pdf/2602.06669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06669]] compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data(https://arxiv.org/abs/2602.06669)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在非英语语言中通常表现出性能、文化一致性和安全鲁棒性下降，部分原因是英语在预训练数据和人类偏好一致性数据集中占主导地位。人类反馈强化学习 (RLHF) 和直​​接偏好优化 (DPO) 等训练方法需要人类偏好数据，而对于英语以外的许多语言来说，这些数据仍然稀缺且基本上是非公开的。为了解决这一差距，我们引入了 compar:IA，这是法国政府内部开发的一项开源数字公共服务，旨在从主要讲法语的普通受众中收集大规模人类偏好数据。该平台使用盲目的成对比较界面来捕获跨多种语言模型的不受约束的真实世界提示和用户判断，同时保持低参与摩擦和保护隐私的自动过滤。截至 2026 年 2 月 7 日，compar:IA 已收集了超过 600,000 个自由格式提示和 250,000 个偏好投票，其中约 89% 的数据为法语。我们在开放许可下发布了三个补充数据集——对话、投票和反应，并提供初步分析，包括法语模型排行榜和用户交互模式。除了法国背景之外，compar:IA 正在向国际数字公共产品发展，为多语言模型训练、评估和人机交互研究提供可重用的基础设施。</li>
</ul>

<h3>Title: Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>Kerstin Sahler, Sophie Jentzsch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06692">https://arxiv.org/abs/2602.06692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06692">https://arxiv.org/pdf/2602.06692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06692]] Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts(https://arxiv.org/abs/2602.06692)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The groundbreaking capabilities of Large Language Models (LLMs) offer new opportunities for enhancing human-computer interaction through emotion-adaptive Artificial Intelligence (AI). However, deliberately controlling the sentiment in these systems remains challenging. The present study investigates the potential of prompt engineering for controlling sentiment in LLM-generated text, providing a resource-sensitive and accessible alternative to existing methods. Using Ekman's six basic emotions (e.g., joy, disgust), we examine various prompting techniques, including Zero-Shot and Chain-of-Thought prompting using gpt-3.5-turbo, and compare it to fine-tuning. Our results indicate that prompt engineering effectively steers emotions in AI-generated texts, offering a practical and cost-effective alternative to fine-tuning, especially in data-constrained settings. In this regard, Few-Shot prompting with human-written examples was the most effective among other techniques, likely due to the additional task-specific guidance. The findings contribute valuable insights towards developing emotion-adaptive AI systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的突破性功能为通过情感自适应人工智能 (AI) 增强人机交互提供了新的机会。然而，有意控制这些系统中的情绪仍然具有挑战性。本研究调查了即时工程控制法学硕士生成文本中的情绪的潜力，为现有方法提供了一种资源敏感且易于访问的替代方案。使用 Ekman 的六种基本情绪（例如快乐、厌恶），我们研究了各种提示技术，包括使用 gpt-3.5-turbo 的零射击和思维链提示，并将其与微调进行比较。我们的结果表明，即时工程可以有效地引导人工智能生成的文本中的情绪，为微调提供了一种实用且经济高效的替代方案，尤其是在数据受限的环境中。在这方面，使用人工编写的示例进行的少样本提示是其他技术中最有效的，这可能是由于额外的特定于任务的指导。这些发现为开发情绪自适应人工智能系统提供了宝贵的见解。</li>
</ul>

<h3>Title: Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion</h3>
<ul>
<li><strong>Authors: </strong>Tian Lan, Felix Henry, Bin Zhu, Qianghuai Jia, Junyang Ren, Qihang Pu, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06724">https://arxiv.org/abs/2602.06724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06724">https://arxiv.org/pdf/2602.06724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06724]] Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion(https://arxiv.org/abs/2602.06724)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at this https URL.</li>
<li><strong>摘要：</strong>当前的信息搜索（InfoSeeking）智能体在长期探索过程中很难保持专注和连贯性，因为在一个纯文本上下文中跟踪搜索状态（包括规划过程和大量搜索结果）本质上是脆弱的。为了解决这个问题，我们引入了 \textbf{Table-as-Search (TaS)}，这是一个结构化的规划框架，它将 InfoSeeking 任务重新表述为 Table Completion 任务。 TaS 将每个查询映射到外部数据库中维护的结构化表模式，其中行表示搜索候选，列表示约束或所需信息。该表精确地管理搜索状态：填充的单元格严格记录历史和搜索结果，而空的单元格则作为明确的搜索计划。至关重要的是，TaS 统一了三种不同的 InfoSeeking 任务：深度搜索、广泛搜索和具有挑战性的 DeepWide 搜索。大量实验表明，TaS 在三种基准测试中显着优于众多最先进的基准，包括多代理框架和商业系统。此外，我们的分析验证了 TaS 在长期 InfoSeeking 中的卓越稳健性以及效率、可扩展性和灵活性。代码和数据集在此 https URL 公开发布。</li>
</ul>

<h3>Title: R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging</h3>
<ul>
<li><strong>Authors: </strong>Yanlin Lai, Mitt Huang, Hangyu Guo, Xiangfeng Wang, Haodong Li, Shaoxiong Zhan, Liang Zhao, Chengyuan Yao, Yinmin Zhang, Qi Han, Chun Yuan, Zheng Ge, Xiangyu Zhang, Daxin Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06763">https://arxiv.org/abs/2602.06763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06763">https://arxiv.org/pdf/2602.06763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06763]] R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging(https://arxiv.org/abs/2602.06763)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.</li>
<li><strong>摘要：</strong>来自人类反馈的强化学习 (RLHF) 对于在主观领域中调整大型语言模型 (LLM) 仍然是不可或缺的。为了增强稳健性，最近的工作转向生成奖励模型（GenRM），该模型在预测偏好之前生成理由。然而，在 GenRM 训练和评估中，实践仍然只贴结果标签，推理质量不受检查。我们表明，推理保真度（GenRM 的偏好决策和参考决策原理之间的一致性）可以高度预测下游 RLHF 结果，超出标准标签准确性。具体来说，我们重新利用现有的奖励模型基准来计算虚假正确性（S-Corr）——标签正确决策的比例与黄金判断不一致。我们的实证评估显示，即使对于有竞争力的 GenRM，S-Corr 也相当大，并且较高的 S-Corr 与优化下的策略退化相关。为了提高保真度，我们提出了以基本原理为中心的对齐方式（Rationale-Centric Align），即 R-Align，它通过黄金判断来增强训练，并明确监督基本原理对齐。 R-Align 减少了 RM 基准上的 S-Corr，并在 STEM、编码、指令遵循和一般任务方面的参与者表现上产生了一致的增益。</li>
</ul>

<h3>Title: Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Kate Sanders, Nathaniel Weir, Sapana Chaudhary, Kaj Bostrom, Huzefa Rangwala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06795">https://arxiv.org/abs/2602.06795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06795">https://arxiv.org/pdf/2602.06795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06795]] Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling(https://arxiv.org/abs/2602.06795)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 进行推理输出验证的一个障碍是，LLM 很难可靠地识别思维轨迹中的错误，特别是在长输出、需要专业知识的领域以及没有可验证奖励的问题中。我们提出了一种数据驱动的方法来自动构建高度精细的推理错误分类法，以增强对看不见的推理轨迹的 LLM 驱动的错误检测。我们的研究结果表明，与编码、数学和化学工程等技术领域的基线方法相比，利用这些错误分类法或“量规”的分类方法表现出强大的错误识别能力。这些规则可用于构建更强大的法学硕士作为法官奖励函数，用于通过强化学习进行推理模型训练。实验结果表明，与由普通法学硕士作为评委训练的模型相比，这些奖励有可能将模型在困难领域的任务准确性提高 45%，并且接近由可验证奖励训练的模型的性能，同时使用的金标签数量少至 20%。通过我们的方法，我们将奖励标准的使用范围从评估定性模型行为扩展到评估通常通过 RLVR 奖励学习的任务的定量模型正确性。这一扩展为教学模型打开了大门，可以在没有完整的黄金标签数据集的情况下解决复杂的技术问题，而黄金标签的采购成本通常很高。</li>
</ul>

<h3>Title: Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations</h3>
<ul>
<li><strong>Authors: </strong>Shamik Bhattacharya, Daniel Perkins, Yaren Dogan, Vineeth Konjeti, Sudarshan Srinivasan, Edmon Begoli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06799">https://arxiv.org/abs/2602.06799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06799">https://arxiv.org/pdf/2602.06799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06799]] Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations(https://arxiv.org/abs/2602.06799)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.</li>
<li><strong>摘要：</strong>歧义性给大型语言模型 (LLM) 的自然语言理解带来了持续的挑战。为了更好地理解如何通过视觉领域解决词汇歧义，我们开发了一个可解释的视觉词义消歧（VWSD）框架。该模型利用 CLIP 将模糊的语言和候选图像投影到共享的多模态空间中。我们使用语义和基于照片的提示的双通道组合以及 WordNet 同义词来丰富文本嵌入，同时通过强大的测试时间增强来完善图像嵌入。然后，我们使用余弦相似度来确定与模糊文本最匹配的图像。在 SemEval-2023 VWSD 数据集上进行评估时，丰富嵌入可将 MRR 从 0.7227 提高到 0.7590，将命中率从 0.5810 提高到 0.6220。消融研究表明，双通道提示可提供强大、低延迟的性能，而激进的图像增强只能产生边际收益。对 WordNet 定义和多语言提示集合的其他实验进一步表明，嘈杂的外部信号往往会削弱语义特异性，从而增强了精确的、与 CLIP 对齐的提示在视觉词义消歧方面的有效性。</li>
</ul>

<h3>Title: The Representational Geometry of Number</h3>
<ul>
<li><strong>Authors: </strong>Zhimin Hu, Lanhao Niu, Sashank Varma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06843">https://arxiv.org/abs/2602.06843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06843">https://arxiv.org/pdf/2602.06843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06843]] The Representational Geometry of Number(https://arxiv.org/abs/2602.06843)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.</li>
<li><strong>摘要：</strong>认知科学的一个核心问题是概念表征是否会收敛到共享流形上以支持泛化，或者是否会发散到正交子空间以最小化任务干扰。虽然之前的工作已经发现了这两者的证据，但对这些属性如何在任务中共存和转换的机械解释仍然难以捉摸。我们认为表征共享不在于概念本身，而在于它们之间的几何关系。使用数字概念作为测试平台和语言模型作为高维计算基础，我们表明数字表示在任务之间保持稳定的关系结构。特定于任务的表示嵌入在不同的子空间中，具有沿可分离线性方向编码的幅度和奇偶校验等低级特征。至关重要的是，我们发现这些子空间在很大程度上可以通过线性映射相互转换，这表明尽管位于不同的子空间中，表示仍共享关系结构。总之，这些结果提供了语言模型如何平衡数字表示的共享结构与功能灵活性的机械视角。它表明，当特定于任务的转换应用于概念表示的共享底层关系结构时，就会产生理解。</li>
</ul>

<h3>Title: SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Mingqian Feng, Xiaodong Liu, Weiwei Yang, Jialin Song, Xuekai Zhu, Chenliang Xu, Jianfeng Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06854">https://arxiv.org/abs/2602.06854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06854">https://arxiv.org/pdf/2602.06854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06854]] SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks(https://arxiv.org/abs/2602.06854)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: this https URL.</li>
<li><strong>摘要：</strong>多轮越狱捕获了安全聊天机器人的真实威胁模型，其中单轮攻击只是一个特例。然而，现有的方法因探索复杂性和意图漂移而崩溃。我们提出了 SEMA，这是一个简单而有效的框架，可以在不依赖任何现有策略或外部数据的情况下训练多轮攻击者。 SEMA 分为两个阶段。预填充自调整通过对以最小前缀自行生成的非拒绝、结构良好、多轮对抗性提示进行微调来实现可用的推出，从而稳定后续学习。具有意图漂移感知奖励的强化学习训练攻击者引出有效的多轮对抗性提示，同时保持相同的有害目标。我们通过意图漂移感知奖励将有害意图锚定在多轮越狱中，该奖励结合了意图一致性、合规风险和详细程度。我们的开环攻击机制避免了对受害者反馈的依赖，统一了单轮和多轮设置，并降低了探索复杂性。在多个数据集、受害者模型和越狱法官中，我们的方法实现了最先进的 (SOTA) 攻击成功率 (ASR)，优于所有单轮基线、手动脚本和模板驱动的多轮基线，以及我们的 SFT（监督微调）和 DPO（直接偏好优化）变体。例如，SEMA 在 AdvBench 上的三个闭源和开源受害者模型中平均执行 $80.1\%$ ASR@1，比 SOTA 高出 33.9%。该方法紧凑、可重复且可跨目标传输，为大语言模型 (LLM) 安全性提供更强大、更现实的压力测试，并支持自动红队来暴露和本地化故障模式。我们的代码位于：此 https URL。</li>
</ul>

<h3>Title: Uncovering Cross-Objective Interference in Multi-Objective Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yining Lu, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06869">https://arxiv.org/abs/2602.06869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06869">https://arxiv.org/pdf/2602.06869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06869]] Uncovering Cross-Objective Interference in Multi-Objective Alignment(https://arxiv.org/abs/2602.06869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence. To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.</li>
<li><strong>摘要：</strong>我们研究了大型语言模型（LLM）多目标对齐中的持久失败模式：训练仅提高了目标子集的性能，同时导致其他目标性能下降。我们将这种现象形式化为跨目标干扰，并对经典标量化算法进行了首次系统研究，表明干扰是普遍存在的，并且表现出很强的模型依赖性。为了解释这种现象，我们推导了一个局部协方差定律，表明当目标的奖励与标量化分数呈现正协方差时，目标会在一阶上得到改善。我们将这种分析扩展到现代比对中使用的剪切替代目标，证明尽管进行了剪切，协方差律在温和条件下仍然有效。在此分析的基础上，我们提出了协方差目标权重适应（CTWA），这是一种即插即用的方法，可以保持目标奖励和训练信号之间的正协方差，以有效减轻跨目标干扰。最后，我们通过 Polyak--Łojasiewicz 条件下的全局收敛分析来补充这些局部改进条件，确定非凸标量优化何时实现全局收敛以及跨目标干扰如何依赖于特定模型的几何属性。</li>
</ul>

<h3>Title: Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Samir Abdaljalil, Parichit Sharma, Erchin Serpedin, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06920">https://arxiv.org/abs/2602.06920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06920">https://arxiv.org/pdf/2602.06920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06920]] Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs(https://arxiv.org/abs/2602.06920)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{this https URL}.</li>
<li><strong>摘要：</strong>大语言模型中的幻觉仍然是一个持续存在的挑战，特别是在难以维持事实一致性的多语言和生成环境中。虽然最近的模型在以英语为中心的基准测试中表现出强劲的性能，但它们在语言、任务和幻觉类型上的行为尚未得到很好的理解。在这项工作中，我们引入了 Halluverse-M^3，这是一个旨在对跨多种语言、多种生成任务和多种幻觉类别的幻觉进行系统分析的数据集。 Halluverse-M^3涵盖英语、阿拉伯语、印地语、土耳其语四种语言，支持问答和对话摘要两种生成任务。该数据集明确区分了实体级、关系级和句子级幻觉。幻觉输出是通过受控编辑过程构建的，并由人类注释者验证，确保原始内容和幻觉生成之间的清晰一致性。使用这个数据集，我们评估了一组不同的当代开源和专有语言模型的细粒度幻觉检测。我们的结果表明，问题回答始终比对话摘要更容易，而即使对于最强大的模型来说，句子级幻觉仍然具有挑战性。英语的性能最高，但资源较低的语言的性能较差，印地语的检测精度最低。总体而言，Halluverse-M^3 为研究多语言、多任务环境中的幻觉提供了一个现实且具有挑战性的基准。我们发布该数据集是为了支持未来关于幻觉检测和缓解的研究\footnote{this https URL}。</li>
</ul>

<h3>Title: Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay</h3>
<ul>
<li><strong>Authors: </strong>Duygu Altinok</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06942">https://arxiv.org/abs/2602.06942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06942">https://arxiv.org/pdf/2602.06942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06942]] Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay(https://arxiv.org/abs/2602.06942)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.</li>
<li><strong>摘要：</strong>标记化是土耳其语等形态丰富的语言 (MRL) 中神经语言建模的关键设计选择，其中有效的凝集对词汇效率和形态保真度提出了挑战。先前的研究已经探索了分词器家族和词汇量大小，但通常（i）在没有系统地控制分词器的训练语料库的情况下改变词汇量，（ii）提供有限的内在诊断，以及（iii）评估一小部分下游任务。我们提出了第一个关于土耳其语子词标记化的全面、原则性的研究； “子词清单”，联合改变词汇量和分词器训练语料库大小（数据和词汇耦合），在匹配的参数预算（WordPiece、形态水平和字符基线）下比较多个分词器系列，并跨语义（NLI、STS、情感分析、NER）、句法（POS、依存分析）和形态敏感探针进行评估。为了解释标记器成功或失败的原因，我们引入了形态感知诊断工具包，该工具包超越了粗聚合到边界级微观/宏观 F1、解耦引理原子性与表面边界命中、过度/欠分割索引、字符/单词编辑距离 (CER/WER)、连续率以及词缀类型覆盖和标记级原子性。我们的贡献有四个方面：（i）对词汇-语料库-成功三元组的系统研究； (ii) 一个统一的、形态感知的评估框架，将内在诊断与外在结果联系起来； (iii) 受控比较，确定字符级和形态级标记化何时获得回报； (iv) 评估代码、分词器管道和模型的开源版本。作为此类的第一个工作，这个“子词清单”为在 MRL 中构建有效的标记器提供了可行的指导，并为未来的研究奠定了可重复的基础。</li>
</ul>

<h3>Title: DAWN: Dependency-Aware Fast Inference for Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lizhuo Luo, Zhuoran Shi, Jiajun Luo, Zhi Wang, Shen Ren, Wenya Wang, Tianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06953">https://arxiv.org/abs/2602.06953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06953">https://arxiv.org/pdf/2602.06953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06953]] DAWN: Dependency-Aware Fast Inference for Diffusion LLMs(https://arxiv.org/abs/2602.06953)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at this https URL.</li>
<li><strong>摘要：</strong>扩散大语言模型 (dLLM) 在文本生成方面显示出优势，特别是由于其固有的并行解码能力。然而，受质量与速度权衡的限制，现有的推理解决方案采用保守的并行策略，从而导致巨大的效率潜力尚未得到充分开发。一个核心挑战是并行解码假设每个位置都可以独立填充，但令牌通常在语义上是耦合的。因此，一个位置的正确选择会限制其他位置的有效选择。如果不对这些令牌间依赖关系进行建模，并行策略会产生恶化的输出。受这一见解的启发，我们提出了 DAWN，一种用于快速 dLLM 推理的免训练、依赖性感知解码方法。 DAWN 提取代币依赖性并利用两个关键动机：（1）依赖于未屏蔽的某些位置的位置变得更加可靠，（2）同时揭露强耦合的不确定位置会导致错误。鉴于这些发现，DAWN 利用依赖图在每次迭代中选择更可靠的揭露位置，实现高度并行性，而生成质量的损失可以忽略不计。跨多个模型和数据集的大量实验表明，DAWN 的推理速度比基线提高了 1.80-8.06 倍，同时保持了生成质量。代码在此 https URL 发布。</li>
</ul>

<h3>Title: InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06960">https://arxiv.org/abs/2602.06960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06960">https://arxiv.org/pdf/2602.06960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06960]] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning(https://arxiv.org/abs/2602.06960)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.</li>
<li><strong>摘要：</strong>大型推理模型通过扩展推理时间思想链来实现强大的性能，但这种范式遭受二次成本、上下文长度限制以及由于中间丢失效应而导致推理能力下降。迭代推理通过定期总结中间思想来缓解这些问题，但现有方法依赖于监督学习或固定启发法，无法优化何时总结、保留什么以及如何恢复推理。我们提出了 InftyThink+，这是一种端到端强化学习框架，它基于模型控制的迭代边界和显式总结来优化整个迭代推理轨迹。 InftyThink+采用有监督冷启动和轨迹级强化学习的两阶段训练方案，使模型能够学习策略总结和持续决策。 DeepSeek-R1-Distill-Qwen-1.5B 上的实验表明，InftyThink+ 在 AIME24 上的准确率提高了 21%，明显优于传统的长链强化学习，同时也能更好地泛化到分布外基准。此外，InftyThink+显着降低了推理延迟并加速了强化学习训练，展示了推理效率的提高和性能的增强。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
