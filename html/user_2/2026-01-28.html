<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-28</h1>
<h3>Title: Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Zhang, Ravi Shekhar, Haralambos Mouratidis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18899">https://arxiv.org/abs/2601.18899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18899">https://arxiv.org/pdf/2601.18899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18899]] Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries(https://arxiv.org/abs/2601.18899)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose an efficient and novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.</li>
<li><strong>摘要：</strong>由大型语言模型 (LLM) 驱动的自动语音识别 (ASR) 系统通过轻量级连接器将冻结语音编码器链接到预训练的 LLM，以有限的资源实现强大的性能。之前的工作为每种语言训练了一个单独的连接器，忽略了语言相关性。我们提出了一种基于语言家庭成员资格的高效且新颖的连接器共享策略，为每个家庭启用一个连接器，并通过经验验证其在两个多语言法学硕士和两个跨越策划和众包演讲的现实世界语料库中的有效性。我们的结果表明，基于系列的连接器减少了参数数量，同时提高了跨领域的泛化性，为多语言 ASR 部署提供了实用且可扩展的策略。</li>
</ul>

<h3>Title: Self-Aware Knowledge Probing: Evaluating Language Models' Relational Knowledge through Confidence Calibration</h3>
<ul>
<li><strong>Authors: </strong>Christopher Kissling, Elena Merdjanovska, Alan Akbik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18901">https://arxiv.org/abs/2601.18901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18901">https://arxiv.org/pdf/2601.18901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18901]] Self-Aware Knowledge Probing: Evaluating Language Models' Relational Knowledge through Confidence Calibration(https://arxiv.org/abs/2601.18901)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge probing quantifies how much relational knowledge a language model (LM) has acquired during pre-training. Existing knowledge probes evaluate model capabilities through metrics like prediction accuracy and precision. Such evaluations fail to account for the model's reliability, reflected in the calibration of its confidence scores. In this paper, we propose a novel calibration probing framework for relational knowledge, covering three modalities of model confidence: (1) intrinsic confidence, (2) structural consistency and (3) semantic grounding. Our extensive analysis of ten causal and six masked language models reveals that most models, especially those pre-trained with the masking objective, are overconfident. The best-calibrated scores come from confidence estimates that account for inconsistencies due to statement rephrasing. Moreover, even the largest pre-trained models fail to encode the semantics of linguistic confidence expressions accurately.</li>
<li><strong>摘要：</strong>知识探索量化了语言模型（LM）在预训练期间获得了多少关系知识。现有的知识探针通过预测准确性和精度等指标来评估模型能力。此类评估未能考虑模型的可靠性，这反映在其置信度分数的校准中。在本文中，我们提出了一种新颖的关系知识校准探测框架，涵盖模型置信度的三种模式：（1）内在置信度，（2）结构一致性和（3）语义基础。我们对十种因果语言模型和六种屏蔽语言模型的广泛分析表明，大多数模型，尤其是那些经过屏蔽目标预先训练的模型，都过于自信。最佳校准分数来自置信度估计，该估计考虑了由于陈述改写而导致的不一致。此外，即使是最大的预训练模型也无法准确编码语言置信度表达的语义。</li>
</ul>

<h3>Title: Flatter Tokens are More Valuable for Speculative Draft Model Training</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Fan, Daming Cao, Xiangzhong Luo, Jiale Fu, Chonghan Liu, Xu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18902">https://arxiv.org/abs/2601.18902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18902">https://arxiv.org/pdf/2601.18902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18902]] Flatter Tokens are More Valuable for Speculative Draft Model Training(https://arxiv.org/abs/2601.18902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD acceptance rate. Specifically, our theoretical analysis and empirical validation reveals that tokens inducing flatter predictive distributions from the target model are more valuable than those yielding sharply peaked distributions. Based on this insight, we propose flatness, a new metric to quantify this property, and develop the Sample-level-flatness-based Dataset Distillation (SFDD) approach, which filters the training data to retain only the most valuable samples. Experiments on the EAGLE framework demonstrate that SFDD can achieve over 2$\times$ training speedup using only 50% of the data, while keeping the final model's inference speedup within 4% of the full-dataset baseline. This work introduces an effective, data-centric approach that substantially improves the training efficiency for Speculative Decoding. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>推测解码 (SD) 是加速大型语言模型 (LLM) 推理的关键技术，但它通常需要在大型数据集上训练草稿模型。我们从以数据为中心的角度来解决这个问题，发现并非所有训练样本对 SD 接受率的贡献都相同。具体来说，我们的理论分析和实证验证表明，从目标模型中诱导出更平坦的预测分布的代币比那些产生尖锐峰值分布的代币更有价值。基于这一见解，我们提出了平坦度（一种量化此属性的新指标），并开发了基于样本级别平坦度的数据集蒸馏（SFDD）方法，该方法过滤训练数据以仅保留最有价值的样本。 EAGLE 框架上的实验表明，SFDD 仅使用 50% 的数据即可实现超过 2 美元\倍的训练加速，同时将最终模型的推理加速保持在全数据集基线的 4% 以内。这项工作引入了一种有效的、以数据为中心的方法，可以大大提高推测解码的训练效率。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: BabyReasoningBench: Generating Developmentally-Inspired Reasoning Tasks for Evaluating Baby Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaustubh D. Dhole</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18933">https://arxiv.org/abs/2601.18933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18933">https://arxiv.org/pdf/2601.18933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18933]] BabyReasoningBench: Generating Developmentally-Inspired Reasoning Tasks for Evaluating Baby Language Models(https://arxiv.org/abs/2601.18933)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Traditional evaluations of reasoning capabilities of language models are dominated by adult-centric benchmarks that presuppose broad world knowledge, complex instruction following, and mature pragmatic competence. These assumptions are mismatched to baby language models trained on developmentally plausible input such as child-directed speech and early-childhood narratives, and they obscure which reasoning abilities (if any) emerge under such constraints. We introduce BabyReasoningBench, a GPT-5.2 generated benchmark of 19 reasoning tasks grounded in classic paradigms from developmental psychology, spanning theory of mind, analogical and relational reasoning, causal inference and intervention selection, and core reasoning primitives that are known to be confounded by memory and pragmatics. We find that two GPT-2 based baby language models (pretrained on 10M and 100M of child-directed speech text) show overall low but uneven performance, with dissociations across task families: scaling improves several causal and physical reasoning tasks, while belief attribution and pragmatics-sensitive tasks remain challenging. BabyReasoningBench provides a developmentally grounded lens for analyzing what kinds of reasoning are supported by child-like training distributions, and for testing mechanistic hypotheses about how such abilities emerge.</li>
<li><strong>摘要：</strong>对语言模型推理能力的传统评估以成人为中心的基准主导，这些基准预设了广泛的世界知识、复杂的指令遵循和成熟的语用能力。这些假设与接受发育合理输入（例如儿童定向言语和幼儿叙述）训练的婴儿语言模型不匹配，并且它们掩盖了在这种限制下出现的推理能力（如果有的话）。我们介绍 BabyReasoningBench，它是 GPT-5.2 生成的 19 项推理任务的基准，这些任务基于发展心理学的经典范式，涵盖心理理论、类比和关系推理、因果推理和干预选择，以及已知会被记忆和语用学混淆的核心推理原语。我们发现，两种基于 GPT-2 的婴儿语言模型（在 10M 和 100M 面向儿童的语音文本上进行预训练）总体表现较低但参差不齐，任务系列之间存在分离：扩展改进了多个因果和物理推理任务，而信念归因和语用敏感任务仍然具有挑战性。 BabyReasoningBench 提供了一个基于发展的视角，用于分析类似儿童的训练分布支持哪些类型的推理，并测试有关此类能力如何出现的机械假设。</li>
</ul>

<h3>Title: LLMs versus the Halting Problem: Revisiting Program Termination Prediction</h3>
<ul>
<li><strong>Authors: </strong>Oren Sultan, Jordi Armengol-Estape, Pascal Kesseli, Julien Vanegue, Dafna Shahaf, Yossi Adi, Peter O'Hearn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18987">https://arxiv.org/abs/2601.18987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18987">https://arxiv.org/pdf/2601.18987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18987]] LLMs versus the Halting Problem: Revisiting Program Termination Prediction(https://arxiv.org/abs/2601.18987)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.</li>
<li><strong>摘要：</strong>确定程序是否终止是计算机科学的一个中心问题。图灵的基本结果将停止问题确定为不可判定的，这表明没有算法可以普遍确定所有程序和输入的终止。因此，自动验证工具接近终止，有时无法证明或反驳；这些工具依赖于特定于问题的架构和抽象，并且通常与特定的编程语言相关联。最近大型语言模型 (LLM) 的成功和进展提出了以下问题：LLM 能否可靠地预测程序终止？在这项工作中，我们对 2025 年国际软件验证竞赛 (SV-Comp) 终止类别中的各种 C 程序的法学硕士进行了评估。我们的结果表明，法学硕士在预测程序终止方面表现非常出色，其中 GPT-5 和 Claude Sonnet-4.5 紧随排名第一的工具（使用测试时间缩放），而代码世界模型 (CWM) 紧随排名第二的工具。虽然法学硕士可以有效地预测项目终止，但他们往往无法提供有效的证人作为证据。此外，法学硕士的成绩随着课程长度的增加而下降。我们希望这些见解能够激发对项目终止的进一步研究，以及法学硕士在推理不可判定问题方面的更广泛潜力。</li>
</ul>

<h3>Title: Malicious Repurposing of Open Science Artefacts by Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zahra Hashemi, Zhiqiang Zhong, Jun Pang, Wei Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18998">https://arxiv.org/abs/2601.18998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18998">https://arxiv.org/pdf/2601.18998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18998]] Malicious Repurposing of Open Science Artefacts by Using Large Language Models(https://arxiv.org/abs/2601.18998)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The rapid evolution of large language models (LLMs) has fuelled enthusiasm about their role in advancing scientific discovery, with studies exploring LLMs that autonomously generate and evaluate novel research ideas. However, little attention has been given to the possibility that such models could be exploited to produce harmful research by repurposing open science artefacts for malicious ends. We fill the gap by introducing an end-to-end pipeline that first bypasses LLM safeguards through persuasion-based jailbreaking, then reinterprets NLP papers to identify and repurpose their artefacts (datasets, methods, and tools) by exploiting their vulnerabilities, and finally assesses the safety of these proposals using our evaluation framework across three dimensions: harmfulness, feasibility of misuse, and soundness of technicality. Overall, our findings demonstrate that LLMs can generate harmful proposals by repurposing ethically designed open artefacts; however, we find that LLMs acting as evaluators strongly disagree with one another on evaluation outcomes: GPT-4.1 assigns higher scores (indicating greater potential harms, higher soundness and feasibility of misuse), Gemini-2.5-pro is markedly stricter, and Grok-3 falls between these extremes. This indicates that LLMs cannot yet serve as reliable judges in a malicious evaluation setup, making human evaluation essential for credible dual-use risk assessment.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的快速发展激发了人们对其在推进科学发现方面的作用的热情，研究探索了能够自主生成和评估新颖研究想法的 LLM。然而，很少有人关注这种模型可能被利用来进行有害研究的可能性，即通过重新利用开放科学制品来达到恶意目的。我们通过引入端到端管道来填补这一空白，该管道首先通过基于说服的越狱绕过 LLM 保护措施，然后重新解释 NLP 论文，通过利用其漏洞来识别和重新利用其人工制品（数据集、方法和工具），最后使用我们的评估框架从三个维度评估这些提案的安全性：有害性、滥用的可行性和技术性的健全性。总的来说，我们的研究结果表明，法学硕士可以通过重新利用符合道德的开放人工制品来产生有害的建议；然而，我们发现作为评估者的法学硕士在评估结果上彼此存在强烈分歧：GPT-4.1 给出了更高的分数（表明更大的潜在危害、更高的稳健性和滥用的可能性），Gemini-2.5-pro 明显更严格，而 Grok-3 则介于这两个极端之间。这表明法学硕士还不能在恶意评估设置中充当可靠的法官，因此人工评估对于可信的双重用途风险评估至关重要。</li>
</ul>

<h3>Title: FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haozheng Luo, Zhuolin Jiang, Md Zahid Hasan, Yan Chen, Soumalya Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19001">https://arxiv.org/abs/2601.19001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19001">https://arxiv.org/pdf/2601.19001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19001]] FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning(https://arxiv.org/abs/2601.19001)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at this https URL</li>
<li><strong>摘要：</strong>我们提出了 FROST，一种用于高效推理的注意力感知方法。与传统方法不同，FROST 利用注意力权重来修剪不加批判的推理路径，产生更短、更可靠的推理轨迹。在方法上，我们引入了推理异常值的概念，并设计了一种基于注意力的机制来消除它们。理论上，FROST 保留并增强了模型的推理能力，同时消除了句子级别的异常值。根据经验，我们使用两个强大的推理模型（Phi-4-Reasoning 和 GPT-OSS-20B）在四个基准上验证 FROST，其性能优于 TALE 和 ThinkLess 等最先进的方法。值得注意的是，与基本模型相比，FROST 的代币使用量平均减少了 69.68%，准确性提高了 26.70%。此外，在注意力异常值指标的评估中，与基础模型相比，FROST 将最大无穷范数降低了 15.97%，将平均峰度降低了 91.09%。代码可在此 https URL 获取</li>
</ul>

<h3>Title: Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback</h3>
<ul>
<li><strong>Authors: </strong>Siddhant Arora, Jinchuan Tian, Jiatong Shi, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19063">https://arxiv.org/abs/2601.19063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19063">https://arxiv.org/pdf/2601.19063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19063]] Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback(https://arxiv.org/abs/2601.19063)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human or AI feedback (RLHF/RLAIF) for speech-in/speech-out dialogue systems (SDS) remains underexplored, with prior work largely limited to single semantic rewards applied at the utterance level. Such setups overlook the multi-dimensional and multi-modal nature of conversational quality, which encompasses semantic coherence, audio naturalness, speaker consistency, emotion alignment, and turn-taking behavior. Moreover, they are fundamentally mismatched with duplex spoken dialogue systems that generate responses incrementally, where agents must make decisions based on partial utterances. We address these limitations with the first multi-reward RLAIF framework for SDS, combining semantic, audio-quality, and emotion-consistency rewards. To align utterance-level preferences with incremental, blockwise decoding in duplex models, we apply turn-level preference sampling and aggregate per-block log-probabilities within a single DPO objective. We present the first systematic study of preference learning for improving SDS quality in both multi-turn Chain-of-Thought and blockwise duplex models, and release a multi-reward DPO dataset to support reproducible research. Experiments show that single-reward RLAIF selectively improves its targeted metric, while joint multi-reward training yields consistent gains across semantic quality and audio naturalness. These results highlight the importance of holistic, multi-reward alignment for practical conversational SDS.</li>
<li><strong>摘要：</strong>用于语音输入/语音输出对话系统 (SDS) 的人类或人工智能反馈 (RLHF/RLAIF) 的强化学习仍未得到充分探索，之前的工作主要限于在话语级别应用的单一语义奖励。这种设置忽视了会话质量的多维和多模态本质，其中包括语义连贯性、音频自然度、说话者一致性、情绪一致性和轮流行为。此外，它们从根本上与增量生成响应的双向语音对话系统不匹配，在这种系统中，代理必须根据部分话语做出决策。我们通过第一个针对 SDS 的多奖励 RLAIF 框架解决了这些限制，结合了语义、音频质量和情感一致性奖励。为了使双工模型中的话语级偏好与增量、按块解码保持一致，我们在单个 DPO 目标中应用轮级偏好采样并聚合每个块的对数概率。我们首次提出了偏好学习的系统研究，以提高多轮思维链和块式双工模型中的 SDS 质量，并发布了多奖励 DPO 数据集以支持可重复的研究。实验表明，单奖励 RLAIF 有选择地提高其目标指标，而联合多奖励训练在语义质量和音频自然度方面产生一致的增益。这些结果凸显了整体、多奖励调整对于实际会话 SDS 的重要性。</li>
</ul>

<h3>Title: PsyProbe: Proactive and Interpretable Dialogue through User State Modeling for Exploratory Counseling</h3>
<ul>
<li><strong>Authors: </strong>Sohhyung Park, Hyunji Kang, Sungzoon Cho, Dongil Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19096">https://arxiv.org/abs/2601.19096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19096">https://arxiv.org/pdf/2601.19096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19096]] PsyProbe: Proactive and Interpretable Dialogue through User State Modeling for Exploratory Counseling(https://arxiv.org/abs/2601.19096)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models have enabled mental health dialogue systems, yet existing approaches remain predominantly reactive, lacking systematic user state modeling for proactive therapeutic exploration. We introduce PsyProbe, a dialogue system designed for the exploration phase of counseling that systematically tracks user psychological states through the PPPPPI framework (Presenting, Predisposing, Precipitating, Perpetuating, Protective, Impact) augmented with cognitive error detection. PsyProbe combines State Builder for extracting structured psychological profiles, Memory Construction for tracking information gaps, Strategy Planner for Motivational Interviewing behavioral codes, and Response Generator with Question Ideation and Critic/Revision modules to generate contextually appropriate, proactive questions. We evaluate PsyProbe with 27 participants in real-world Korean counseling scenarios, including automatic evaluation across ablation modes, user evaluation, and expert evaluation by a certified counselor. The full PsyProbe model consistently outperforms baseline and ablation modes in automatic evaluation. User evaluation demonstrates significantly increased engagement intention and improved naturalness compared to baseline. Expert evaluation shows that PsyProbe substantially improves core issue understanding and achieves question rates comparable to professional counselors, validating the effectiveness of systematic state modeling and proactive questioning for therapeutic exploration.</li>
<li><strong>摘要：</strong>大语言模型的最新进展使心理健康对话系统成为可能，但现有方法仍然主要是反应性的，缺乏用于主动治疗探索的系统用户状态建模。我们推出了 PsyProbe，这是一个专为咨询探索阶段设计的对话系统，通过 PPPPPI 框架（呈现、预处置、沉淀、永久、保护、影响）系统地跟踪用户的心理状态，并增强了认知错误检测。 PsyProbe 结合了用于提取结构化心理特征的状态构建器、用于跟踪信息差距的记忆构建器、用于动机访谈行为代码的策略规划器，以及带有问题构思和批评/修订模块的响应生成器，以生成上下文适当的主动问题。我们与 27 名参与者在真实的韩国咨询场景中对 PsyProbe 进行了评估，包括跨消融模式的自动评估、用户评估以及由认证咨询师进行的专家评估。完整的 PsyProbe 模型在自动评估中始终优于基线和消融模式。用户评估表明，与基线相比，参与意愿显着增加，自然度也有所提高。专家评估表明，PsyProbe 极大地提高了核心问题的理解，并达到了与专业咨询师相当的提问率，验证了系统状态建模和主动提问对治疗探索的有效性。</li>
</ul>

<h3>Title: Transparency-First Medical Language Models: Datasheets, Model Cards, and End-to-End Data Provenance for Clinical NLP</h3>
<ul>
<li><strong>Authors: </strong>Olaf Yunus Laitinen Imanov, Taner Yilmaz, Ayse Tuba Tugrul, Melike Nesrin Zaman, Ozkan Gunalp, Duygu Erisken, Sila Burde Dulger, Rana Irem Turhan, Izzet Ozdemir, Derya Umut Kulali, Ozan Akbulut, Harun Demircioglu, Hasan Basri Kara, Berfin Tavan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19191">https://arxiv.org/abs/2601.19191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19191">https://arxiv.org/pdf/2601.19191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19191]] Transparency-First Medical Language Models: Datasheets, Model Cards, and End-to-End Data Provenance for Clinical NLP(https://arxiv.org/abs/2601.19191)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce TeMLM, a set of transparency-first release artifacts for clinical language models. TeMLM unifies provenance, data transparency, modeling transparency, and governance into a single, machine-checkable release bundle. We define an artifact suite (TeMLM-Card, TeMLM-Datasheet, TeMLM-Provenance) and a lightweight conformance checklist for repeatable auditing. We instantiate the artifacts on Technetium-I, a large-scale synthetic clinical NLP dataset with 498,000 notes, 7.74M PHI entity annotations across 10 types, and ICD-9-CM diagnosis labels, and report reference results for ProtactiniumBERT (about 100 million parameters) on PHI de-identification (token classification) and top-50 ICD-9 code extraction (multi-label classification). We emphasize that synthetic benchmarks are valuable for tooling and process validation, but models should be validated on real clinical data prior to deployment.</li>
<li><strong>摘要：</strong>我们推出 TeMLM，这是一组用于临床语言模型的透明度优先的发布工件。 TeMLM 将来源、数据透明度、建模透明度和治理统一到一个可机器检查的发布包中。我们定义了一个工件套件（TeMLM-Card、TeMLM-Datasheet、TeMLM-Provenance）和一个用于可重复审核的轻量级一致性检查表。我们在 Technetium-I 上实例化了工件，这是一个大规模合成临床 NLP 数据集，包含 498,000 个注释、10 种类型的 7.74M PHI 实体注释和 ICD-9-CM 诊断标签，并报告 ProtactiniumBERT（约 1 亿个参数）在 PHI 去标识（令牌分类）和前 50 个 ICD-9 代码提取（多标签分类）上的参考结果。我们强调，综合基准对于工具和流程验证很有价值，但模型应在部署之前根据真实的临床数据进行验证。</li>
</ul>

<h3>Title: Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs</h3>
<ul>
<li><strong>Authors: </strong>Chi Zhang, Wenxuan Ding, Jiale Liu, Mingrui Wu, Qingyun Wu, Ray Mooney</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19202">https://arxiv.org/abs/2601.19202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19202">https://arxiv.org/pdf/2601.19202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19202]] Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs(https://arxiv.org/abs/2601.19202)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have shown strong multimodal reasoning capabilities on Visual-Question-Answering (VQA) benchmarks. However, their robustness against textual misinformation remains under-explored. While existing research has studied the effect of misinformation in text-only domains, it is not clear how VLMs arbitrate between contradictory information from different modalities. To bridge the gap, we first propose the CONTEXT-VQA (i.e., Conflicting Text) dataset, consisting of image-question pairs together with systematically generated persuasive prompts that deliberately conflict with visual evidence. Then, a thorough evaluation framework is designed and executed to benchmark the susceptibility of various models to these conflicting multimodal inputs. Comprehensive experiments over 11 state-of-the-art VLMs reveal that these models are indeed vulnerable to misleading textual prompts, often overriding clear visual evidence in favor of the conflicting text, and show an average performance drop of over 48.2% after only one round of persuasive conversation. Our findings highlight a critical limitation in current VLMs and underscore the need for improved robustness against textual manipulation.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 在视觉问答 (VQA) 基准上显示出强大的多模态推理能力。然而，它们对文本错误信息的鲁棒性仍有待探索。虽然现有研究已经研究了纯文本领域中错误信息的影响，但尚不清楚 VLM 如何在来自不同模式的矛盾信息之间进行仲裁。为了弥补这一差距，我们首先提出了 CONTEXT-VQA（即冲突文本）数据集，该数据集由图像问题对以及系统生成的故意与视觉证据相冲突的有说服力的提示组成。然后，设计并执行一个全面的评估框架，以衡量各种模型对这些相互冲突的多模式输入的敏感性。对 11 个最先进的 VLM 进行的综合实验表明，这些模型确实容易受到误导性文本提示的影响，常常会推翻清晰的视觉证据，转而支持相互冲突的文本，并且在仅一轮说服性对话后，平均性能下降超过 48.2%。我们的研究结果强调了当前 VLM 的一个关键限制，并强调需要提高针对文本操作的鲁棒性。</li>
</ul>

<h3>Title: How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Shawn Im, Changdae Oh, Zhen Fang, Sharon Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19208">https://arxiv.org/abs/2601.19208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19208">https://arxiv.org/pdf/2601.19208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19208]] How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability(https://arxiv.org/abs/2601.19208)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Semantic associations such as the link between "bird" and "flew" are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data in attention-based language models through the lens of training dynamics. By leveraging a leading-term approximation of the gradients, we develop closed-form expressions for the weights at early stages of training that explain how semantic associations first take shape. Through our analysis, we reveal that each set of weights of the transformer has closed-form expressions as simple compositions of three basis functions (bigram, token-interchangeability, and context mappings), reflecting the statistics of the text corpus and uncovering how each component of the transformer captures semantic associations based on these compositions. Experiments on real-world LLMs demonstrate that our theoretical weight characterizations closely match the learned weights, and qualitative analyses further show how our theorem shines light on interpreting the learned associations in transformers.</li>
<li><strong>摘要：</strong>诸如“鸟”和“飞”之间的联系之类的语义关联是语言建模的基础，因为它们使模型能够超越记忆，而是概括并生成连贯的文本。了解这些关联是如何在语言模型中学习和表示的，对于将深度学习与语言理论联系起来并为大型语言模型奠定机制基础至关重要。在这项工作中，我们从训练动态的角度分析了这些关联如何从基于注意力的语言模型中的自然语言数据中出现。通过利用梯度的前导项近似，我们在训练的早期阶段开发了权重的封闭式表达式，以解释语义关联如何首先形成。通过我们的分析，我们揭示了转换器的每组权重都具有封闭式表达式，作为三个基本函数（二元组、标记互换性和上下文映射）的简单组合，反映了文本语料库的统计数据，并揭示了转换器的每个组件如何基于这些组合捕获语义关联。对现实世界法学硕士的实验表明，我们的理论权重特征与学习到的权重紧密匹配，定性分析进一步表明我们的定理如何在解释 Transformer 中学习到的关联方面发挥作用。</li>
</ul>

<h3>Title: A Hybrid Supervised-LLM Pipeline for Actionable Suggestion Mining in Unstructured Customer Reviews</h3>
<ul>
<li><strong>Authors: </strong>Aakash Trivedi, Aniket Upadhyay, Pratik Narang, Dhruv Kumar, Praveen Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19214">https://arxiv.org/abs/2601.19214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19214">https://arxiv.org/pdf/2601.19214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19214]] A Hybrid Supervised-LLM Pipeline for Actionable Suggestion Mining in Unstructured Customer Reviews(https://arxiv.org/abs/2601.19214)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Extracting actionable suggestions from customer reviews is essential for operational decision-making, yet these directives are often embedded within mixed-intent, unstructured text. Existing approaches either classify suggestion-bearing sentences or generate high-level summaries, but rarely isolate the precise improvement instructions businesses need. We evaluate a hybrid pipeline combining a high-recall RoBERTa classifier trained with a precision-recall surrogate to reduce unrecoverable false negatives with a controlled, instruction-tuned LLM for suggestion extraction, categorization, clustering, and summarization. Across real-world hospitality and food datasets, the hybrid system outperforms prompt-only, rule-based, and classifier-only baselines in extraction accuracy and cluster coherence. Human evaluations further confirm that the resulting suggestions and summaries are clear, faithful, and interpretable. Overall, our results show that hybrid reasoning architectures achieve meaningful improvements fine-grained actionable suggestion mining while highlighting challenges in domain adaptation and efficient local deployment.</li>
<li><strong>摘要：</strong>从客户评论中提取可行的建议对于运营决策至关重要，但这些指令通常嵌入在混合意图的非结构化文本中。现有的方法要么对带有建议的句子进行分类，要么生成高级摘要，但很少分离出企业所需的精确改进指令。我们评估了一个混合管道，该管道结合了经过精确召回代理训练的高召回率 RoBERTa 分类器，以减少不可恢复的假阴性，并使用受控的、指令调整的 LLM 来进行建议提取、分类、聚类和摘要。在现实世界的酒店和食品数据集中，混合系统在提取准确性和聚类一致性方面优于仅提示、基于规则和仅分类器的基线。人工评估进一步证实，由此产生的建议和摘要是清晰、忠实且可解释的。总体而言，我们的结果表明，混合推理架构实现了细粒度可操作建议挖掘的有意义的改进，同时强调了领域适应和高效本地部署方面的挑战。</li>
</ul>

<h3>Title: DREAMSTATE: Diffusing States and Parameters for Recurrent Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liu Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19221">https://arxiv.org/abs/2601.19221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19221">https://arxiv.org/pdf/2601.19221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19221]] DREAMSTATE: Diffusing States and Parameters for Recurrent Large Language Models(https://arxiv.org/abs/2601.19221)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Modern Recurrent Neural Networks (RNNs), such as RWKV, are distinguished by their powerful short-range modeling capabilities and efficient fixed-size states, which constitute a core advantage over standard Transformers. However, there is a significant lack of research into their internal state as an editable knowledge representation. To fill this gap, we first explore the representational properties of the RWKV state by proposing the DREAMSTATE framework. This framework utilizes a conditional Diffusion Transformer (DiT) to directly model the probability manifold of the state, enabling its generation and editing. The structural nature of this representation is validated through t-SNE visualizations and controlled generation experiments. After successfully uncovering and modeling the state's representational potential, we further propose a novel hybrid architecture that combines the local advantages of RNNs with global context adaptability. This architecture features a parallel DiT that processes a variable-length global context to dynamically generate and adjust the core recurrent module's WKV parameters, transforming the fixed recurrence mechanism into a context-aware dynamic function. Experiments demonstrate that this hybrid model can be trained stably via a multi-objective loss, validating its design feasibility. Our work not only opens a new research direction for RNN state representation but also provides a concrete architectural reference for future model design. The code is publicly available at: this https URL.</li>
<li><strong>摘要：</strong>现代循环神经网络（RNN），例如 RWKV，以其强大的短程建模能力和高效的固定大小状态而著称，这是相对于标准 Transformer 的核心优势。然而，对其内部状态作为可编辑知识表示的研究却严重缺乏。为了填补这一空白，我们首先通过提出 DREAMSTATE 框架来探索 RWKV 状态的表征属性。该框架利用条件扩散变压器（DiT）直接对状态的概率流形进行建模，从而实现其生成和编辑。这种表示的结构性质通过 t-SNE 可视化和受控生成实验得到验证。在成功揭示和建模状态的表征潜力后，我们进一步提出了一种新颖的混合架构，它将 RNN 的局部优势与全局上下文适应性相结合。该架构采用并行 DiT，可处理可变长度的全局上下文，以动态生成和调整核心循环模块的 WKV 参数，将固定循环机制转变为上下文感知的动态函数。实验表明，该混合模型可以通过多目标损失进行稳定训练，验证了其设计可行性。我们的工作不仅为 RNN 状态表示开辟了新的研究方向，而且为未来的模型设计提供了具体的架构参考。该代码可在以下位置公开获取：此 https URL。</li>
</ul>

<h3>Title: RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Kaehyun Um, KyuHwan Yeom, Haerim Yang, Minyoung Choi, Hyeongjun Yang, Kyong-Ho Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19225">https://arxiv.org/abs/2601.19225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19225">https://arxiv.org/pdf/2601.19225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19225]] RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering(https://arxiv.org/abs/2601.19225)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）最近表现出了卓越的推理能力，但对知识密集型任务却产生了幻觉。检索增强生成（RAG）通过将答案基于外部来源（例如知识图（KG））来缓解这个问题。然而，现有的基于 KG 的 RAG 方法依赖于语义未知的路径采样，并且与 KG 推理目标的一致性较弱，这限制了进一步的准确性提升。他们还将检索到的路径直接输入推理器，而不将它们组织成以答案为中心的推理路径，从而阻碍了小型法学硕士利用检索到的知识的能力。此外，先前的工作主要依赖于大型 LLM（例如 ChatGPT/GPT-4）或假设骨干网参数高于 7B，而低于 7B 的模型尚未得到充分探索。据我们所知，我们通过 RPO-RAG 解决了这一差距，这是第一个专门为小型 LLM 设计的基于 KG 的 RAG 框架。 RPO-RAG 引入了三个关键创新：（1）提供信息丰富的监督信号的查询路径语义采样策略； (2) 关系感知偏好优化，将训练与中间 KG 推理信号（例如关系）结合起来； (3)以答案为中心的提示设计，以可解释的格式组织实体和推理路径。在两个基准知识图问答 (KGQA) 数据集 WebQSP 和 CWQ 上进行的大量实验表明，RPO-RAG 有效地弥合了小型和大型语言模型之间的性能差距。在 WebQSP 上，它将 F1 提高了高达 8.8%，反映出答案精度的提高，而在 CWQ 上，它在 Hit 和 F1 中的 8B 参数下的模型中实现了新的最先进结果。总体而言，RPO-RAG 极大地提高了小型 LLM 的推理能力，即使在 3B 参数下也是如此 - 突出了它们在资源高效且实用的设备上 KGQA 应用程序方面的潜力。</li>
</ul>

<h3>Title: DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinlong Chen, Weihong Lin, Jingyun Hua, Linli Yao, Yue Ding, Bozhou Li, Bohan Zeng, Yang Shi, Qiang Liu, Yuanxing Zhang, Pengfei Wan, Liang Wang, Tieniu Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19267">https://arxiv.org/abs/2601.19267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19267">https://arxiv.org/pdf/2601.19267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19267]] DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models(https://arxiv.org/abs/2601.19267)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Accurate dialogue description in audiovisual video captioning is crucial for downstream understanding and generation tasks. However, existing models generally struggle to produce faithful dialogue descriptions within audiovisual captions. To mitigate this limitation, we propose DiaDem, a powerful audiovisual video captioning model capable of generating captions with more precise dialogue descriptions while maintaining strong overall performance. We first synthesize a high-quality dataset for SFT, then employ a difficulty-partitioned two-stage GRPO strategy to further enhance dialogue descriptions. To enable systematic evaluation of dialogue description capabilities, we introduce DiaDemBench, a comprehensive benchmark designed to evaluate models across diverse dialogue scenarios, emphasizing both speaker attribution accuracy and utterance transcription fidelity in audiovisual captions. Extensive experiments on DiaDemBench reveal even commercial models still exhibit substantial room for improvement in dialogue-aware captioning. Notably, DiaDem not only outperforms the Gemini series in dialogue description accuracy but also achieves competitive performance on general audiovisual captioning benchmarks, demonstrating its overall effectiveness.</li>
<li><strong>摘要：</strong>视听视频字幕中准确的对话描述对于下游理解和生成任务至关重要。然而，现有模型通常难以在视听字幕中产生忠实的对话描述。为了缓解这一限制，我们提出了 DiaDem，这是一种强大的视听视频字幕模型，能够生成具有更精确对话描述的字幕，同时保持强大的整体性能。我们首先为 SFT 合成高质量的数据集，然后采用难度划分的两阶段 GRPO 策略来进一步增强对话描述。为了能够系统地评估对话描述能力，我们引入了 DiaDemBench，这是一个综合基准测试，旨在评估不同对话场景的模型，强调说话人归因的准确性和视听字幕中的语音转录保真度。 DiaDemBench 上的大量实验表明，即使是商业模型，在对话感知字幕方面仍然有很大的改进空间。值得注意的是，DiaDem不仅在对话描述准确性方面优于Gemini系列，而且在通用视听字幕基准上也取得了具有竞争力的表现，展示了其整体有效性。</li>
</ul>

<h3>Title: Riddle Quest : The Enigma of Words</h3>
<ul>
<li><strong>Authors: </strong>Niharika Sri Parasa, Chaitali Diwan, Srinath Srinivasa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19273">https://arxiv.org/abs/2601.19273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19273">https://arxiv.org/pdf/2601.19273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19273]] Riddle Quest : The Enigma of Words(https://arxiv.org/abs/2601.19273)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Riddles are concise linguistic puzzles that describe an object or idea through indirect, figurative, or playful clues. They are a longstanding form of creative expression, requiring the solver to interpret hints, recognize patterns, and draw inferences to identify the answers. In this work, we introduce a simple pipeline for creating and evaluating analogy-based riddles. The system includes a triples creator that builds structured facts about a concept, a semantic mapper that selects attributes useful for analogy, a stylized generator that turns them into riddle clues, and a validator that collects all possible answers the riddle could point to. We use this validator to study whether large language models can recover the full answer set for different riddle types. Our case study shows that while models often guess the main intended answer, they frequently miss other valid interpretations. This highlights the value of riddles as a lightweight tool for examining reasoning coverage and ambiguity handling in language models.</li>
<li><strong>摘要：</strong>谜语是简洁的语言谜题，通过间接、比喻或有趣的线索来描述物体或想法。它们是一种长期存在的创造性表达形式，要求求解者解释提示、识别模式并进行推论来确定答案。在这项工作中，我们介绍了一个用于创建和评估基于类比的谜语的简单管道。该系统包括一个三元组创建器，用于构建有关概念的结构化事实，一个语义映射器，用于选择对类比有用的属性，一个将它们转化为谜语线索的程式化生成器，以及一个收集谜语可能指向的所有可能答案的验证器。我们使用这个验证器来研究大型语言模型是否可以恢复不同谜语类型的完整答案集。我们的案例研究表明，虽然模型经常猜测主要的预期答案，但它们经常会错过其他有效的解释。这凸显了谜语作为检查语言模型中的推理覆盖率和歧义处理的轻量级工具的价值。</li>
</ul>

<h3>Title: DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Fuliang Liu, Xue Li, Ketai Zhao, Yinxi Gao, Ziyan Zhou, Zhonghui Zhang, Zhibin Wang, Wanchun Dou, Sheng Zhong, Chen Tian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19278">https://arxiv.org/abs/2601.19278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19278">https://arxiv.org/pdf/2601.19278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19278]] DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference(https://arxiv.org/abs/2601.19278)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative decoding is an effective and lossless approach for accelerating LLM inference. However, existing widely adopted model-based draft designs, such as EAGLE3, improve accuracy at the cost of multi-step autoregressive inference, resulting in high drafting latency and ultimately rendering the drafting stage itself a performance bottleneck. Inspired by diffusion-based large language models (dLLMs), we propose DART, which leverages parallel generation to reduce drafting latency. DART predicts logits for multiple future masked positions in parallel within a single forward pass based on hidden states of the target model, thereby eliminating autoregressive rollouts in the draft model while preserving a lightweight design. Based on these parallel logit predictions, we further introduce an efficient tree pruning algorithm that constructs high-quality draft token trees with N-gram-enforced semantic continuity. DART substantially reduces draft-stage overhead while preserving high draft accuracy, leading to significantly improved end-to-end decoding speed. Experimental results demonstrate that DART achieves a 2.03x--3.44x wall-clock time speedup across multiple datasets, surpassing EAGLE3 by 30% on average and offering a practical speculative decoding framework. Code is released at this https URL.</li>
<li><strong>摘要：</strong>推测性解码是加速 LLM 推理的有效且无损的方法。然而，现有广泛采用的基于模型的草图设计（例如 EAGLE3）以多步自回归推理为代价提高了准确性，导致草图延迟较高，最终使草图阶段本身成为性能瓶颈。受基于扩散的大型语言模型 (dLLM) 的启发，我们提出了 DART，它利用并行生成来减少起草延迟。 DART 根据目标模型的隐藏状态在一次前向传递中并行预测多个未来屏蔽位置的逻辑，从而消除草稿模型中的自回归，同时保留轻量级设计。基于这些并行 Logit 预测，我们进一步引入了一种高效的树剪枝算法，该算法可构建具有 N-gram 强制语义连续性的高质量草稿令牌树。 DART 大幅减少了草稿阶段的开销，同时保持了较高的草稿精度，从而显着提高了端到端解码速度。实验结果表明，DART 在多个数据集上实现了 2.03 倍--3.44 倍的挂钟时间加速，平均超过 EAGLE3 30%，并提供了实用的推测解码框架。代码在此 https URL 发布。</li>
</ul>

<h3>Title: ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jesus Lovon-Melgarejo (IRIT), Jose G. Moreno (IRIT-IRIS), Christine Damase-Michel, Lynda Tamine (IRIT-IRIS)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19286">https://arxiv.org/abs/2601.19286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19286">https://arxiv.org/pdf/2601.19286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19286]] ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction(https://arxiv.org/abs/2601.19286)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHRs) provide crucial information for clinical decision-making. However, their high-dimensionality, heterogeneity, and sparsity make clinical prediction challenging. Large Language Models (LLMs) allowed progress towards addressing this challenge by leveraging parametric medical knowledge to enhance EHR data for clinical prediction tasks. Despite the significant achievements made so far, most of the existing approaches are fundamentally task-agnostic in the sense that they deploy LLMs as EHR encoders or EHR completion modules without fully integrating signals from the prediction tasks. This naturally hinders task performance accuracy. In this work, we propose Rewrite-To-Predict (ReToP), an LLM-based framework that addresses this limitation through an end-to-end training of an EHR rewriter and a clinical predictor. To cope with the lack of EHR rewrite training data, we generate synthetic pseudo-labels using clinical-driven feature selection strategies to create diverse patient rewrites for fine-tuning the EHR rewriter. ReToP aligns the rewriter with prediction objectives using a novel Classifier Supervised Contribution (CSC) score that enables the EHR rewriter to generate clinically relevant rewrites that directly enhance prediction. Our ReToP framework surpasses strong baseline models across three clinical tasks on MIMIC-IV. Moreover, the analysis of ReToP shows its generalizability to unseen datasets and tasks with minimal fine-tuning while preserving faithful rewrites and emphasizing task-relevant predictive features.</li>
<li><strong>摘要：</strong>电子健康记录 (EHR) 为临床决策提供重要信息。然而，它们的高维性、异质性和稀疏性使得临床预测具有挑战性。大型语言模型 (LLM) 通过利用参数化医学知识来增强临床预测任务的 EHR 数据，从而在应对这一挑战方面取得进展。尽管迄今为止取得了重大成就，但大多数现有方法基本上与任务无关，因为它们将 LLM 部署为 EHR 编码器或 EHR 完成模块，而没有完全集成来自预测任务的信号。这自然会阻碍任务执行的准确性。在这项工作中，我们提出了重写预测（ReToP），这是一种基于法学硕士的框架，通过对 EHR 重写器和临床预测器进行端到端训练来解决这一限制。为了应对 EHR 重写训练数据的缺乏，我们使用临床驱动的特征选择策略生成合成伪标签，以创建不同的患者重写，以微调 EHR 重写器。 ReToP 使用新颖的分类器监督贡献 (CSC) 评分使重写器与预测目标保持一致，使 EHR 重写器能够生成可直接增强预测的临床相关重写。我们的 ReToP 框架在 MIMIC-IV 上的三项临床任务中超越了强大的基线模型。此外，ReToP 的分析显示了其对未见过的数据集和任务的通用性，只需最少的微调，同时保留忠实的重写并强调与任务相关的预测特征。</li>
</ul>

<h3>Title: MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yimeng Wang, Jiaxing Zhao, Hongbin Xie, Hexing Ma, Yuzhen Lei, Shuangxue Liu, Xuan Song, Zichen Zhang, Haoran Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19290">https://arxiv.org/abs/2601.19290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19290">https://arxiv.org/pdf/2601.19290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19290]] MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning(https://arxiv.org/abs/2601.19290)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地部署为多代理系统，其中专门的角色通过结构化交互进行通信和协作，以解决通常超出单个代理能力的复杂任务。然而，大多数现有系统仍然依赖于固定的角色库和执行冻结的交互拓扑，这种严格的设计选择经常导致任务不匹配，无法在推理过程中出现新证据时及时适应，并进一步增加推理成本。我们引入了 MetaGen，这是一个免训练框架，可以在推理时调整角色空间和协作拓扑，而无需更新基础模型权重。 MetaGen 生成并重写查询条件角色规范以维护可控的动态角色池，然后围绕最小骨干实例化受约束的执行图。在执行过程中，它会迭代更新角色提示并使用轻量级反馈信号调整结构决策。代码生成和多步推理基准测试表明，MetaGen 比强大的多智能体基准提高了准确性和成本权衡。</li>
</ul>

<h3>Title: Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics</h3>
<ul>
<li><strong>Authors: </strong>Natapong Nitarach, Pittawat Taveekitworachai, Kunat Pipatanakul</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19302">https://arxiv.org/abs/2601.19302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19302">https://arxiv.org/pdf/2601.19302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19302]] Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics(https://arxiv.org/abs/2601.19302)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Prompting techniques such as Chain-of-Thought (CoT) and Program-of-Thought (PoT) improve LLM mathematical reasoning by structuring intermediate steps in natural language or code. However, applied mathematics problems in domains like finance, physics, and cryptography often require recalling or deriving governing equations, a step that current approaches do not explicitly leverage. We propose Formula-One Prompting (F-1), a two-phase approach that uses mathematical equations as an intermediate representation before adaptive solving. F-1 first formulates governing equations from problem descriptions, then selects a solving strategy among CoT, PoT, or direct computation based on the generated equations, all within a single LLM call. Results across five models and four benchmarks show F-1 outperforms CoT by +5.76% and PoT by +8.42% on average. Crucially, gains are largest in applied domains: +13.30% on FinanceMath over CoT, and within OlympiadBench, larger gains on physics (+2.55%) than pure math (+0.44%). This demonstrates that F-1 is more effective than CoT in applied mathematics problems.</li>
<li><strong>摘要：</strong>思维链 (CoT) 和思维程序 (PoT) 等提示技术通过用自然语言或代码构建中间步骤来改进 LLM 数学推理。然而，金融、物理和密码学等领域的应用数学问题通常需要回忆或推导控制方程，而当前的方法并未明确利用这一步骤。我们提出一级方程式提示（F-1），这是一种两阶段方法，在自适应求解之前使用数学方程作为中间表示。 F-1 首先根据问题描述制定控制方程，然后根据生成的方程在 CoT、PoT 或直接计算中选择求解策略，所有这些都在单个 LLM 调用中完成。五个模型和四个基准的结果显示，F-1 的平均性能优于 CoT +5.76%，平均优于 PoT +8.42%。至关重要的是，应用领域的收益最大：FinanceMath 比 CoT +13.30%，而在 OlympiadBench 中，物理 (+2.55%) 的收益比纯数学 (+0.44%) 更大。这表明 F-1 在应用数学问题上比 CoT 更有效。</li>
</ul>

<h3>Title: When Benchmarks Leak: Inference-Time Decontamination for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jianzhe Chai, Yu Zhe, Jun Sakuma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19334">https://arxiv.org/abs/2601.19334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19334">https://arxiv.org/pdf/2601.19334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19334]] When Benchmarks Leak: Inference-Time Decontamination for LLMs(https://arxiv.org/abs/2601.19334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Benchmark-based evaluation is the de facto standard for comparing large language models (LLMs). However, its reliability is increasingly threatened by test set contamination, where test samples or their close variants leak into training data and artificially inflate reported performance. To address this issue, prior work has explored two main lines of mitigation. One line attempts to identify and remove contaminated benchmark items before evaluation, but this inevitably alters the evaluation set itself and becomes unreliable when contamination is moderate or severe. The other line preserves the benchmark and instead suppresses contaminated behavior at evaluation time; however, such interventions often interfere with normal inference and lead to noticeable performance degradation on clean inputs. We propose DeconIEP, a decontamination framework that operates entirely during evaluation by applying small, bounded perturbations in the input embedding space. Guided by a relatively less-contaminated reference model, DeconIEP learns an instance-adaptive perturbation generator that steers the evaluated model away from memorization-driven shortcut pathways. Across multiple open-weight LLMs and benchmarks, extensive empirical results show that DeconIEP achieves strong decontamination effectiveness while incurring only minimal degradation in benign utility.</li>
<li><strong>摘要：</strong>基于基准的评估是比较大型语言模型 (LLM) 的事实上的标准。然而，其可靠性越来越受到测试集污染的威胁，测试样本或其近似变体泄漏到训练数据中并人为地夸大了报告的性能。为了解决这个问题，之前的工作探索了两条主要的缓解措施。一条生产线试图在评估之前识别并删除受污染的基准项目，但这不可避免地会改变评估集本身，并且当污染程度为中度或严重时变得不可靠。另一条线保留基准，并在评估时抑制污染行为；然而，此类干预通常会干扰正常的推理，并导致干净输入的性能明显下降。我们提出了 DeconIEP，一种去污框架，它完全在评估过程中通过在输入嵌入空间中应用小的、有界的扰动来运行。在污染相对较少的参考模型的指导下，DeconIEP 学习了一种实例自适应扰动生成器，可以引导评估的模型远离记忆驱动的捷径。在多个开放权重法学硕士和基准中，广泛的实证结果表明，DeconIEP 实现了强大的净化效果，同时仅导致良性效用的最小化退化。</li>
</ul>

<h3>Title: Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Diego Rossini, Lonneke van der Plas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19360">https://arxiv.org/abs/2601.19360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19360">https://arxiv.org/pdf/2601.19360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19360]] Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement(https://arxiv.org/abs/2601.19360)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We present a comprehensive approach for multiword expression (MWE) identification that combines binary token-level classification, linguistic feature integration, and data augmentation. Our DeBERTa-v3-large model achieves 69.8% F1 on the CoAM dataset, surpassing the best results (Qwen-72B, 57.8% F1) on this dataset by 12 points while using 165x fewer parameters. We achieve this performance by (1) reformulating detection as binary token-level START/END/INSIDE classification rather than span-based prediction, (2) incorporating NP chunking and dependency features that help discontinuous and NOUN-type MWEs identification, and (3) applying oversampling that addresses severe class imbalance in the training data. We confirm the generalization of our method on the STREUSLE dataset, achieving 78.9% F1. These results demonstrate that carefully designed smaller models can substantially outperform LLMs on structured NLP tasks, with important implications for resource-constrained deployments.</li>
<li><strong>摘要：</strong>我们提出了一种综合的多词表达（MWE）识别方法，该方法结合了二进制标记级分类、语言特征集成和数据增强。我们的 DeBERTa-v3-large 模型在 CoAM 数据集上实现了 69.8% F1，比该数据集上的最佳结果（Qwen-72B，57.8% F1）高出 12 个百分点，同时使用的参数减少了 165 倍。我们通过以下方式实现这一性能：(1) 将检测重新表述为二进制标记级 START/END/INSIDE 分类，而不是基于跨度的预测；(2) 结合 NP 分块和依赖性特征，帮助识别不连续和名词型 MWE；(3) 应用过采样来解决训练数据中严重的类别不平衡问题。我们确认了我们的方法在 STREUSLE 数据集上的泛化，实现了 78.9% F1。这些结果表明，精心设计的较小模型在结构化 NLP 任务上可以大大优于法学硕士，这对资源受限的部署具有重要意义。</li>
</ul>

<h3>Title: Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?</h3>
<ul>
<li><strong>Authors: </strong>Ahrii Kim, Seong-heum Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19410">https://arxiv.org/abs/2601.19410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19410">https://arxiv.org/pdf/2601.19410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19410]] Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?(https://arxiv.org/abs/2601.19410)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Automatic post-editing (APE) aims to refine machine translations by correcting residual errors. Although recent large language models (LLMs) demonstrate strong translation capabilities, their effectiveness for APE--especially under document-level context--remains insufficiently understood. We present a systematic comparison of proprietary and open-weight LLMs under a naive document-level prompting setup, analyzing APE quality, contextual behavior, robustness, and efficiency. Our results show that proprietary LLMs achieve near human-level APE quality even with simple one-shot prompting, regardless of whether document context is provided. While these models exhibit higher robustness to data poisoning attacks than open-weight counterparts, this robustness also reveals a limitation: they largely fail to exploit document-level context for contextual error correction. Furthermore, standard automatic metrics do not reliably reflect these qualitative improvements, highlighting the continued necessity of human evaluation. Despite their strong performance, the substantial cost and latency overheads of proprietary LLMs render them impractical for real-world APE deployment. Overall, our findings elucidate both the promise and current limitations of LLM-based document-aware APE, and point toward the need for more efficient long-context modeling approaches for translation refinement.</li>
<li><strong>摘要：</strong>自动译后编辑 (APE) 旨在通过纠正残留错误来完善机器翻译。尽管最近的大型语言模型 (LLM) 表现出了强大的翻译能力，但它们对 APE 的有效性（尤其是在文档级上下文下）仍然没有得到充分理解。我们在简单的文档级提示设置下对专有法学硕士和开放权重法学硕士进行了系统比较，分析了 APE 质量、上下文行为、稳健性和效率。我们的结果表明，即使使用简单的一次性提示，专有法学硕士也能达到接近人类水平的 APE 质量，无论是否提供文档上下文。虽然这些模型比开放权重模型对数据中毒攻击表现出更高的鲁棒性，但这种鲁棒性也暴露了一个局限性：它们在很大程度上无法利用文档级上下文来进行上下文错误纠正。此外，标准自动指标并不能可靠地反映这些定性改进，这凸显了人类评估的持续必要性。尽管性能强劲，但专有 LLM 的巨大成本和延迟开销使其对于现实世界的 APE 部署来说不切实际。总体而言，我们的研究结果阐明了基于 LLM 的文档感知 APE 的前景和当前局限性，并指出需要更有效的长上下文建模方法来进行翻译细化。</li>
</ul>

<h3>Title: KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking</h3>
<ul>
<li><strong>Authors: </strong>Vítor N. Lourenço, Aline Paes, Tillman Weyde, Audrey Depeige, Mohnish Dubey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19447">https://arxiv.org/abs/2601.19447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19447">https://arxiv.org/pdf/2601.19447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19447]] KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking(https://arxiv.org/abs/2601.19447)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Claim verification is a core component of automated fact-checking systems, aimed at determining the truthfulness of a statement by assessing it against reliable evidence sources such as documents or knowledge bases. This work presents KG-CRAFT, a method that improves automatic claim verification by leveraging large language models (LLMs) augmented with contrastive questions grounded in a knowledge graph. KG-CRAFT first constructs a knowledge graph from claims and associated reports, then formulates contextually relevant contrastive questions based on the knowledge graph structure. These questions guide the distillation of evidence-based reports, which are synthesised into a concise summary that is used for veracity assessment by LLMs. Extensive evaluations on two real-world datasets (LIAR-RAW and RAWFC) demonstrate that our method achieves a new state-of-the-art in predictive performance. Comprehensive analyses validate in detail the effectiveness of our knowledge graph-based contrastive reasoning approach in improving LLMs' fact-checking capabilities.</li>
<li><strong>摘要：</strong>声明验证是自动事实检查系统的核心组成部分，旨在通过根据可靠的证据来源（例如文档或知识库）进行评估来确定声明的真实性。这项工作提出了 KG-CRAFT，这是一种通过利用大型语言模型 (LLM) 来改进自动索赔验证的方法，该模型通过基于知识图谱的对比问题进行了增强。 KG-CRAFT 首先根据权利要求和相关报告构建知识图，然后根据知识图结构制定上下文相关的对比问题。这些问题指导着基于证据的报告的提炼，这些报告被合成为简明的摘要，用于法学硕士的准确性评估。对两个现实世界数据集（LIAR-RAW 和 RAWFC）的广泛评估表明，我们的方法在预测性能方面达到了新的最先进水平。全面的分析详细验证了我们基于知识图谱的对比推理方法在提高法学硕士事实核查能力方面的有效性。</li>
</ul>

<h3>Title: Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Isha Pandey, Ashish Mittal, Vartul Bahuguna, Ganesh Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19451">https://arxiv.org/abs/2601.19451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19451">https://arxiv.org/pdf/2601.19451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19451]] Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition(https://arxiv.org/abs/2601.19451)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.</li>
<li><strong>摘要：</strong>基于 LLM 的 ASR 的最新进展通过轻量级投影仪将冻结的语音编码器与大型语言模型 (LLM) 连接起来。虽然在单语言环境中有效，但单个投影仪很难捕获多语言 ASR 所需的各种声学到语义映射。为了解决这个问题，我们提出了 SMEAR-MoE，这是一种稳定的专家混合投影仪，可确保所有专家的密集梯度流，防止专家崩溃，同时实现跨语言共享。我们系统地比较了四种印度语言（印地语、马拉地语、泰米尔语、泰卢固语）的单片、静态多投影仪和动态 MoE 设计。我们的 SMEAR-MoE 实现了强大的性能，与单投影机基准相比，WER 相对降低了 7.6%，同时保持了可比的运行时效率。对专家路由的分析进一步显示了具有语言意义的专业化，相关语言共享专家。这些结果表明，稳定的多专家投影仪是可扩展且强大的多语言 ASR 的关键。</li>
</ul>

<h3>Title: GradPruner: Gradient-Guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Anda Cheng, Yinggui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19503">https://arxiv.org/abs/2601.19503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19503">https://arxiv.org/pdf/2601.19503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19503]] GradPruner: Gradient-Guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs(https://arxiv.org/abs/2601.19503)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight downstream datasets. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is publicly available.</li>
<li><strong>摘要：</strong>使用下游数据微调大型语言模型 (LLM) 通常被认为既耗时又昂贵。结构化剪枝方法主要用于提高预训练模型的推理效率。同时，它们通常需要额外的时间和内存来进行训练、知识蒸馏、结构搜索和其他策略，使得高效的模型微调难以实现。为了同时提高下游任务微调的训练和推理效率，我们引入了GradPruner，它可以在微调的早期阶段修剪由梯度引导的LLM层。 GradPruner 在微调的初始阶段使用每个参数的累积梯度来计算初始梯度信息累积矩阵（IGIA-Matrix）来评估层的重要性并进行剪枝。我们基于 IGIA 矩阵对修剪后的层进行稀疏化，并将它们与剩余层合并。仅合并具有相同符号的元素，以减少符号变化的干扰。我们对跨八个下游数据集的两个法学硕士进行了广泛的实验。包括医疗、财务和一般基准任务。结果表明，GradPruner 实现了参数减少 40%，而准确率仅下降 0.99%。我们的代码是公开的。</li>
</ul>

<h3>Title: Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiangyang Zhu, Yuan Tian, Zicheng Zhang, Qi Jia, Chunyi Li, Renrui Zhang, Heng Li, Zongrui Wang, Wei Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19507">https://arxiv.org/abs/2601.19507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19507">https://arxiv.org/pdf/2601.19507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19507]] Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs(https://arxiv.org/abs/2601.19507)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) exhibit remarkable capabilities in cross-modal tasks but face significant safety challenges, which undermine their reliability in real-world applications. Efforts have been made to build LVLM safety evaluation benchmarks to uncover their vulnerability. However, existing benchmarks are hindered by their labor-intensive construction process, static complexity, and limited discriminative power. Thus, they may fail to keep pace with rapidly evolving models and emerging risks. To address these limitations, we propose VLSafetyBencher, the first automated system for LVLM safety benchmarking. VLSafetyBencher introduces four collaborative agents: Data Preprocessing, Generation, Augmentation, and Selection agents to construct and select high-quality samples. Experiments validates that VLSafetyBencher can construct high-quality safety benchmarks within one week at a minimal cost. The generated benchmark effectively distinguish safety, with a safety rate disparity of 70% between the most and least safe models.</li>
<li><strong>摘要：</strong>大型视觉语言模型（LVLM）在跨模式任务中表现出卓越的能力，但面临着重大的安全挑战，这削弱了它们在实际应用中的可靠性。人们正在努力建立LVLM安全评估基准来揭示其脆弱性。然而，现有的基准因其劳动密集型的构建过程、静态复杂性和有限的判别力而受到阻碍。因此，它们可能无法跟上快速发展的模式和新出现的风险的步伐。为了解决这些限制，我们提出了 VLSafetyBencher，这是第一个用于 LVLM 安全基准测试的自动化系统。 VLSafetyBencher 引入了四种协作代理：数据预处理、生成、增强和选择代理来构建和选择高质量样本。实验验证VLSafetyBencher可以在一周内以最小的成本构建高质量的安全基准。生成的基准有效地区分了安全性，最安全模型和最不安全模型之间的安全率差距达 70%。</li>
</ul>

<h3>Title: Yunque DeepResearch Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Cai, Xinyi Lai, Peng Yuan, Weiting Liu, Huajian Li, Mingda Li, Xinghua Wang, Shengxie Zheng, Yanchao Hao, Yuyang Yin, Zheng Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19578">https://arxiv.org/abs/2601.19578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19578">https://arxiv.org/pdf/2601.19578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19578]] Yunque DeepResearch Technical Report(https://arxiv.org/abs/2601.19578)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Deep research has emerged as a transformative capability for autonomous agents, empowering Large Language Models to navigate complex, open-ended tasks. However, realizing its full potential is hindered by critical limitations, including escalating contextual noise in long-horizon tasks, fragility leading to cascading errors, and a lack of modular extensibility. To address these challenges, we introduce Yunque DeepResearch, a hierarchical, modular, and robust framework. The architecture is characterized by three key components: (1) a centralized Multi-Agent Orchestration System that routes subtasks to an Atomic Capability Pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that structures completed sub-goals into semantic summaries to mitigate information overload; and (3) a proactive Supervisor Module that ensures resilience through active anomaly detection and context pruning. Yunque DeepResearch achieves state-of-the-art performance across a range of agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam. We open-source the framework, reproducible implementations, and application cases to empower the community.</li>
<li><strong>摘要：</strong>深度研究已成为自主代理的变革能力，使大型语言模型能够导航复杂的、开放式的任务。然而，实现其全部潜力受到关键限制的阻碍，包括长期任务中不断升级的上下文噪音、导致级联错误的脆弱性以及缺乏模块化可扩展性。为了应对这些挑战，我们引入了云雀深度研究，这是一个分层、模块化、健壮的框架。该架构的特点是三个关键组件：（1）集中式多代理编排系统，将子任务路由到工具和专用子代理的原子能力池； (2)动态上下文管理机制，将已完成的子目标构建为语义摘要，以减轻信息过载； (3) 主动监控模块，通过主动异常检测和上下文修剪确保恢复能力。云雀 DeepResearch 在一系列代理深度研究基准测试中实现了最先进的性能，包括 GAIA、BrowseComp、BrowseComp-ZH 和 Humanity's Last Exam。我们开源框架、可复制的实现和应用案例，为社区提供支持。</li>
</ul>

<h3>Title: Decompose-and-Formalise: Recursively Verifiable Natural Language Inference</h3>
<ul>
<li><strong>Authors: </strong>Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19605">https://arxiv.org/abs/2601.19605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19605">https://arxiv.org/pdf/2601.19605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19605]] Decompose-and-Formalise: Recursively Verifiable Natural Language Inference(https://arxiv.org/abs/2601.19605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent work has shown that integrating large language models (LLMs) with theorem provers (TPs) in neuro-symbolic pipelines helps with entailment verification and proof-guided refinement of explanations for natural language inference (NLI). However, scaling such refinement to naturalistic NLI remains difficult: long, syntactically rich inputs and deep multi-step arguments amplify autoformalisation errors, where a single local mismatch can invalidate the proof. Moreover, current methods often handle failures via costly global regeneration due to the difficulty of localising the responsible span or step from prover diagnostics. Aiming to address these problems, we propose a decompose-and-formalise framework that (i) decomposes premise-hypothesis pairs into an entailment tree of atomic steps, (ii) verifies the tree bottom-up to isolate failures to specific nodes, and (iii) performs local diagnostic-guided refinement instead of regenerating the whole explanation. Moreover, to improve faithfulness of autoformalisation, we introduce $\theta$-substitution in an event-based logical form to enforce consistent argument-role bindings. Across a range of reasoning tasks using five LLM backbones, our method achieves the highest explanation verification rates, improving over the state-of-the-art by 26.2%, 21.7%, 21.6% and 48.9%, while reducing refinement iterations and runtime and preserving strong NLI accuracy.</li>
<li><strong>摘要：</strong>最近的工作表明，将大型语言模型 (LLM) 与神经符号管道中的定理证明器 (TP) 集成有助于自然语言推理 (NLI) 的蕴涵验证和证明引导的解释细化。然而，将这种细化扩展到自然主义 NLI 仍然很困难：长、语法丰富的输入和深度的多步骤参数会放大自动形式化错误，其中单个局部不匹配可能会使证明无效。此外，由于难以从证明者诊断中定位负责的跨度或步骤，当前的方法通常通过昂贵的全局再生来处理故障。为了解决这些问题，我们提出了一个分解和形式化框架，该框架（i）将前提假设对分解为原子步骤的蕴涵树，（ii）自下而上验证树以隔离特定节点的故障，以及（iii）执行局部诊断引导的细化而不是重新生成整个解释。此外，为了提高自动形式化的可信度，我们在基于事件的逻辑形式中引入 $\theta$ 替换，以强制一致的参数角色绑定。在使用五个 LLM 主干的一系列推理任务中，我们的方法实现了最高的解释验证率，比最先进的方法提高了 26.2%、21.7%、21.6% 和 48.9%，同时减少了细化迭代和运行时间，并保持了强大的 NLI 准确性。</li>
</ul>

<h3>Title: Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Xinzhong Wang, Ya Guo, Jing Li, Huan Chen, Yi Tu, Yijie Hong, Gongshen Liu, Huijia Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19613">https://arxiv.org/abs/2601.19613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19613">https://arxiv.org/pdf/2601.19613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19613]] Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs(https://arxiv.org/abs/2601.19613)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Key Information Extraction (KIE) from visually-rich documents (VrDs) is a critical task, for which recent Large Language Models (LLMs) and Multi-Modal Large Language Models (MLLMs) have demonstrated strong potential. However, their reliance on autoregressive inference, which generates outputs sequentially, creates a significant efficiency bottleneck, especially as KIE tasks often involve extracting multiple, semantically independent fields. To overcome this limitation, we introduce PIP: a Parallel Inference Paradigm for KIE. Our approach reformulates the problem by using "[mask]" tokens as placeholders for all target values, enabling their simultaneous generation in a single forward pass. To facilitate this paradigm, we develop a tailored mask pre-training strategy and construct large-scale supervised datasets. Experimental results show that our PIP-models achieve a 5-36x inference speedup with negligible performance degradation compared to traditional autoregressive base models. By substantially improving efficiency while maintaining high accuracy, PIP paves the way for scalable and practical real-world KIE solutions.</li>
<li><strong>摘要：</strong>从视觉丰富的文档 (VrD) 中提取关键信息 (KIE) 是一项关键任务，最近的大型语言模型 (LLM) 和多模态大型语言模型 (MLLM) 已显示出强大的潜力。然而，它们对按顺序生成输出的自回归推理的依赖造成了显着的效率瓶颈，特别是当 KIE 任务通常涉及提取多个语义独立的字段时。为了克服这个限制，我们引入了 PIP：KIE 的并行推理范式。我们的方法通过使用“[mask]”标记作为所有目标值的占位符来重新表述问题，从而能够在单个前向传递中同时生成它们。为了促进这种范例，我们开发了定制的掩模预训练策略并构建了大规模的监督数据集。实验结果表明，与传统的自回归基本模型相比，我们的 PIP 模型实现了 5-36 倍的推理加速，并且性能下降可以忽略不计。通过在保持高精度的同时大幅提高效率，PIP 为可扩展且实用的现实世界 KIE 解决方案铺平了道路。</li>
</ul>

<h3>Title: RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems</h3>
<ul>
<li><strong>Authors: </strong>Weicong Liu, Zixuan Yang, Yibo Zhao, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19637">https://arxiv.org/abs/2601.19637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19637">https://arxiv.org/pdf/2601.19637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19637]] RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems(https://arxiv.org/abs/2601.19637)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Reviewer assignment is increasingly critical yet challenging in the LLM era, where rapid topic shifts render many pre-2023 benchmarks outdated and where proxy signals poorly reflect true reviewer familiarity. We address this evaluation bottleneck by introducing LR-bench, a high-fidelity, up-to-date benchmark curated from 2024-2025 AI/NLP manuscripts with five-level self-assessed familiarity ratings collected via a large-scale email survey, yielding 1055 expert-annotated paper-reviewer-score annotations. We further propose RATE, a reviewer-centric ranking framework that distills each reviewer's recent publications into compact keyword-based profiles and fine-tunes an embedding model with weak preference supervision constructed from heuristic retrieval signals, enabling matching each manuscript against a reviewer profile directly. Across LR-bench and the CMU gold-standard dataset, our approach consistently achieves state-of-the-art performance, outperforming strong embedding baselines by a clear margin. We release LR-bench at this https URL, and a GitHub repository at this https URL.</li>
<li><strong>摘要：</strong>在 LLM 时代，审稿人分配变得越来越重要但也充满挑战，主题的快速转变使得许多 2023 年之前的基准已经过时，而且代理信号很难反映审稿人的真实熟悉程度。我们通过引入 LR-bench 来解决这一评估瓶颈，这是一个高保真、最新的基准，根据 2024-2025 年 AI/NLP 手稿策划，具有通过大规模电子邮件调查收集的五级自我评估熟悉度评级，产生 1055 个专家注释的论文审稿人评分注释。我们进一步提出了 RATE，一个以审稿人为中心的排名框架，它将每个审稿人最近的出版物提炼成紧凑的基于关键词的配置文件，并微调由启发式检索信号构建的弱偏好监督的嵌入模型，从而能够将每个手稿直接与审稿人配置文件进行匹配。在 LR-bench 和 CMU 黄金标准数据集上，我们的方法始终实现了最先进的性能，明显优于强大的嵌入基线。我们在此 https URL 发布 LR-bench，并在此 https URL 发布 GitHub 存储库。</li>
</ul>

<h3>Title: One Token Is Enough: Improving Diffusion Language Models with a Sink Token</h3>
<ul>
<li><strong>Authors: </strong>Zihou Zhang, Zheyong Xie, Li Zhong, Haifeng Liu, Shaosheng Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19657">https://arxiv.org/abs/2601.19657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19657">https://arxiv.org/pdf/2601.19657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19657]] One Token Is Enough: Improving Diffusion Language Models with a Sink Token(https://arxiv.org/abs/2601.19657)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.</li>
<li><strong>摘要：</strong>扩散语言模型 (DLM) 已成为自回归方法的一种引人注目的替代方案，能够实现具有竞争性能的并行文本生成。尽管有这些优点，DLM 仍然存在一个严重的不稳定性：移动汇现象。我们的分析表明，Sink 代币在 Transformer 的价值空间中表现出低范数表示，并且移动 Sink 现象可作为 DLM 中的保护机制，以防止过度信息混合。然而，它们在扩散步骤中的不可预测的位置破坏了推理的稳健性。为了解决这个问题，我们提出了一个简单但有效的额外接收器令牌，通过修改后的注意力掩码实现。具体来说，我们引入了一种特殊的令牌，该令牌仅限于自身，同时对所有其他令牌保持全局可见。实验结果表明，引入单个额外令牌可以稳定注意力池，从而显着提高模型性能。至关重要的是，进一步的分析证实，该令牌的有效性与其位置无关，并且其语义内容可以忽略不计，从而验证了其作为强大且专用的结构汇的作用。</li>
</ul>

<h3>Title: SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking</h3>
<ul>
<li><strong>Authors: </strong>Adam Remaki, Christel Gérardin, Eulàlia Farré-Maduell, Martin Krallinger, Xavier Tannier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19667">https://arxiv.org/abs/2601.19667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19667">https://arxiv.org/pdf/2601.19667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19667]] SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking(https://arxiv.org/abs/2601.19667)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present SynCABEL (Synthetic Contextualized Augmentation for Biomedical Entity Linking), a framework that addresses a central bottleneck in supervised biomedical entity linking (BEL): the scarcity of expert-annotated training data. SynCABEL leverages large language models to generate context-rich synthetic training examples for all candidate concepts in a target knowledge base, providing broad supervision without manual annotation. We demonstrate that SynCABEL, when combined with decoder-only models and guided inference establish new state-of-the-art results across three widely used multilingual benchmarks: MedMentions for English, QUAERO for French, and SPACCC for Spanish. Evaluating data efficiency, we show that SynCABEL reaches the performance of full human supervision using up to 60% less annotated data, substantially reducing reliance on labor-intensive and costly expert labeling. Finally, acknowledging that standard evaluation based on exact code matching often underestimates clinically valid predictions due to ontology redundancy, we introduce an LLM-as-a-judge protocol. This analysis reveals that SynCABEL significantly improves the rate of clinically valid predictions. Our synthetic datasets, models, and code are released to support reproducibility and future research.</li>
<li><strong>摘要：</strong>我们提出了 SynCABEL（生物医学实体链接的综合上下文增强），该框架解决了监督生物医学实体链接 (BEL) 的中心瓶颈：专家注释的训练数据的稀缺。 SynCABEL 利用大型语言模型为目标知识库中的所有候选概念生成上下文丰富的综合训练示例，无需手动注释即可提供广泛的监督。我们证明，SynCABEL 与仅解码器模型和引导推理相结合，在三个广泛使用的多语言基准上建立了新的最先进的结果：英语的 MedMentions、法语的 QUAERO 和西班牙语的 SPACCC。在评估数据效率时，我们表明 SynCABEL 使用减少多达 60% 的注释数据达到了完全人工监督的性能，从而大大减少了对劳动密集型且成本高昂的专家标记的依赖。最后，我们认识到基于精确代码匹配的标准评估往往会由于本体冗余而低估临床有效的预测，因此我们引入了法学硕士作为法官协议。该分析表明，SynCABEL 显着提高了临床有效预测率。我们发布的合成数据集、模型和代码是为了支持可重复性和未来的研究。</li>
</ul>

<h3>Title: Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye, Jixing Li, Chengqing Zong, Shaonan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19723">https://arxiv.org/abs/2601.19723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19723">https://arxiv.org/pdf/2601.19723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19723]] Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes(https://arxiv.org/abs/2601.19723)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地表现出类似人类的语言行为和内部表示，它们可以作为语言认知的计算模拟器。我们询问是否可以系统地操纵法学硕士来重现局灶性脑损伤后失语症的语言产生障碍特征。这些模型可以为测试康复假设提供可扩展的代理，并为探索语言的功能组织提供受控框架。我们引入了一种以临床为基础的组件级框架，通过选择性地干扰法学硕士中的功能组件来模拟失语症，并使用统一的干预接口将其应用于模块化专家混合模型和密集 Transformer。我们的流程（i）识别布罗卡失语症和韦尼克失语症的亚型相关成分，（ii）通过语言探测任务解释这些成分，以及（iii）通过逐步扰动前 k 亚型相关成分来诱导分级损伤，通过失语商（AQ）总结的西方失语症电池（WAB）子测试评估结果。在架构和损伤策略中，以亚型为目标的扰动比大小匹配的随机扰动产生更系统的、类似失语症的回归，并且 MoE 模块化支持更本地化和可解释的表型到组件映射。这些发现表明，模块化法学硕士与临床知情的成分扰动相结合，为模拟失语性语言产生和研究不同语言功能在有针对性的干扰下如何退化提供了一个有前景的平台。</li>
</ul>

<h3>Title: TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching</h3>
<ul>
<li><strong>Authors: </strong>Runjia Zeng, Qifan Wang, Qiang Guan, Ruixiang Tang, Lifu Huang, Zhenting Wang, Xueling Zhang, Cheng Han, Dongfang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19739">https://arxiv.org/abs/2601.19739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19739">https://arxiv.org/pdf/2601.19739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19739]] TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching(https://arxiv.org/abs/2601.19739)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency. Homepage: this https URL</li>
<li><strong>摘要：</strong>微调已被认为是使大型语言模型（LLM）适应下游任务的事实上的方法，但继承自LLM的高训练内存消耗使得该过程效率低下。在现有的内存高效方法中，与激活相关的优化已被证明特别有效，因为激活始终主导着整体内存消耗。尽管现有技术提供了各种激活优化策略，但它们与数据无关的性质最终导致无效且不稳定的微调。在本文中，我们提出了 TokenSeek，这是一种通用插件解决方案，通过实例感知的令牌搜索和丢弃，适用于各种基于 Transformer 的模型，从而实现显着的微调内存节省（例如，在 Llama3.2 1B 上仅需要 14.8% 的内存），并具有同等甚至更好的性能。此外，我们的可解释代币搜索过程揭示了其有效性的根本原因，为未来代币效率的研究提供了宝贵的见解。主页：这个 https URL</li>
</ul>

<h3>Title: Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Zhuohan Long, Zhijie Bao, Zhongyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19773">https://arxiv.org/abs/2601.19773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19773">https://arxiv.org/pdf/2601.19773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19773]] Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis(https://arxiv.org/abs/2601.19773)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at this https URL .</li>
<li><strong>摘要：</strong>交互式医疗咨询需要代理人在不确定的情况下主动找出缺失的临床证据。然而，现有的评估在很大程度上仍然是静态的或以结果为中心，忽视了证据收集过程。在这项工作中，我们提出了一个交互式评估框架，该框架使用模拟患者和基于原子证据的\rev{模拟记者}明确地模拟咨询过程。基于这种表示，我们引入信息覆盖率（ICR）来量化智能体在交互过程中发现必要证据的完全程度。为了支持系统研究，我们建立了 EviMed，这是一个基于证据的基准，涵盖从常见疾病到罕见疾病的各种情况，并评估了 10 个具有不同推理能力的模型。我们发现，强大的诊断推理并不能保证有效的信息收集，这种不足是限制交互设置中性能的主要瓶颈。为了解决这个问题，我们提出了 REFINE，一种利用诊断验证来指导智能体主动解决不确定性的策略。大量实验表明，REFINE 在不同数据集上的表现始终优于基线，并促进有效的模型协作，使较小的智能体能够在强大的推理监督下实现卓越的性能。我们的代码可以在此 https URL 中找到。</li>
</ul>

<h3>Title: LVLMs and Humans Ground Differently in Referential Communication</h3>
<ul>
<li><strong>Authors: </strong>Peter Zeng, Weiling Li, Amie Paige, Zhengxiang Wang, Panagiotis Kaliosis, Dimitris Samaras, Gregory Zelinsky, Susan Brennan, Owen Rambow</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19792">https://arxiv.org/abs/2601.19792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19792">https://arxiv.org/pdf/2601.19792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19792]] LVLMs and Humans Ground Differently in Referential Communication(https://arxiv.org/abs/2601.19792)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.</li>
<li><strong>摘要：</strong>对于生成式人工智能代理与人类用户有效合作来说，准确预测人类意图的能力至关重要。但这种协作能力仍然受到一个严重缺陷的限制：无法建立共同基础。在这里，我们提出了一个采用析因设计的参考通信实验，涉及导演-匹配对（人类-人类、人类-人工智能、人工智能-人类和人工智能-人工智能），它们在重复轮次中与多个轮次交互，以匹配与任何明显词汇化标签无关的对象图片。我们发布了用于数据收集的在线管道、用于准确性、效率和词汇重叠的工具和分析，以及由 356 个对话组成的语料库（每个对话 89 对，每轮 4 轮），这些对话揭示了 LVLM 在交互式解决指称表达方面的局限性，这是人类语言使用基础的一项关键技能。</li>
</ul>

<h3>Title: Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Aohua Li, Yuanshuo Zhang, Ge Gao, Bo Chen, Xiaobing Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19802">https://arxiv.org/abs/2601.19802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19802">https://arxiv.org/pdf/2601.19802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19802]] Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation(https://arxiv.org/abs/2601.19802)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Current stance detection research typically relies on predicting stance based on given targets and text. However, in real-world social media scenarios, targets are neither predefined nor static but rather complex and dynamic. To address this challenge, we propose a novel task: zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), which aims to automatically identify multiple target-stance pairs from text without prior target knowledge. We construct a Chinese social media stance detection dataset and design multi-dimensional evaluation metrics. We explore both integrated and two-stage fine-tuning strategies for large language models (LLMs) and evaluate various baseline models. Experimental results demonstrate that fine-tuned LLMs achieve superior performance on this task: the two-stage fine-tuned Qwen2.5-7B attains the highest comprehensive target recognition score of 66.99%, while the integrated fine-tuned DeepSeek-R1-Distill-Qwen-7B achieves a stance detection F1 score of 79.26%.</li>
<li><strong>摘要：</strong>当前的姿态检测研究通常依赖于基于给定目标和文本的姿态预测。然而，在现实世界的社交媒体场景中，目标既不是预定义的也不是静态的，而是复杂且动态的。为了应对这一挑战，我们提出了一项新任务：利用动态目标生成和多目标适应（DGTA）进行野外零镜头姿态检测，其目的是在没有先验目标知识的情况下从文本中自动识别多个目标姿态对。我们构建了中国社交媒体立场检测数据集并设计了多维评估指标。我们探索大型语言模型（LLM）的集成和两阶段微调策略，并评估各种基线模型。实验结果表明，微调的LLM在此任务上取得了优异的性能：两阶段微调的Qwen2.5-7B获得了最高的综合目标识别分数66.99%，而集成微调的DeepSeek-R1-Distill-Qwen-7B实现了姿态检测F1分数79.26%。</li>
</ul>

<h3>Title: When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Astaraki, Mohammad Arshi Saloot, Ali Shiraee Kasmaee, Hamidreza Mahyar, Soheila Samiee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19827">https://arxiv.org/abs/2601.19827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19827">https://arxiv.org/pdf/2601.19827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19827]] When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering(https://arxiv.org/abs/2601.19827)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 将大型语言模型 (LLM) 扩展到参数知识之外，但尚不清楚迭代检索推理循环何时能够明显优于静态 RAG，特别是在具有多跳推理、稀疏领域知识和异构证据的科学领域。我们提供了第一个受控的机制级诊断研究，研究同步迭代检索和推理是否可以超越理想化的静态上限（黄金上下文）RAG。我们在三种制度下对十一个最先进的法学硕士进行了基准测试：（i）无背景，衡量对参数记忆的依赖； (ii) Gold Context，一次性提供所有预言机证据； (iii) 迭代 RAG，一种无需训练的控制器，可交替检索、假设细化和证据感知停止。使用以化学为重点的 ChemKGMultiHopQA 数据集，我们隔离需要真正检索的问题，并通过跨越检索覆盖差距、锚进位下降、查询质量、组成保真度和控制校准的诊断来分析行为。在各个模型中，Iterative RAG 的表现始终优于 Gold Context，增幅高达 25.6 个百分点，特别是对于非推理微调模型。分阶段检索减少了后期跳跃故障，减轻了上下文过载，并能够动态校正早期假设漂移，但剩余的故障模式包括不完整的跳跃覆盖、干扰器闩锁轨迹、早期停止错误校准以及即使完美检索的高组合失败率。总体而言，分阶段检索通常比仅仅存在理想证据更具影响力；我们为在专业科学环境中部署和诊断 RAG 系统提供实用指导，并为更可靠、可控的迭代检索推理框架奠定基础。</li>
</ul>

<h3>Title: Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering</h3>
<ul>
<li><strong>Authors: </strong>Fangan Dong, Zuming Yan, Xuri Ge, Zhiwei Xu, Mengqi Zhang, Xuanang Chen, Ben He, Xin Xin, Zhumin Chen, Ying Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19847">https://arxiv.org/abs/2601.19847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19847">https://arxiv.org/pdf/2601.19847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19847]] Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering(https://arxiv.org/abs/2601.19847)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS identifies Reasoning-Critical Neurons (RCNs) via a polarity-aware mean-difference criterion and adaptively steers their activations during inference, enhancing incorrect reasoning traces while avoiding degradation on already-correct cases. Experiments on 10 mathematics and coding benchmarks demonstrate consistent improvements, including over 13% gains on AIME-24 and AIME-25. Moreover, AdaRAS exhibits strong transferability across datasets and scalability to stronger models, outperforming post-training methods without additional training or sampling cost.</li>
<li><strong>摘要：</strong>尽管最近的大型语言模型（LLM）具有强大的推理能力，但在具有挑战性的任务上实现可靠的性能通常需要后期训练或计算成本昂贵的采样策略，限制了它们的实际效率。在这项工作中，我们首先表明法学硕士中的一小部分神经元与推理正确性表现出很强的预测相关性。基于这一观察，我们提出了 AdaRAS（自适应推理激活引导），这是一种轻量级测试时框架，可通过选择性干预神经元激活来提高推理可靠性。 AdaRAS 通过极性感知均值差标准识别推理关键神经元 (RCN)，并在推理过程中自适应地控制它们的激活，增强不正确的推理痕迹，同时避免已经正确的情况退化。对 10 个数学和编码基准进行的实验显示出持续的改进，其中 AIME-24 和 AIME-25 的增益超过 13%。此外，AdaRAS 表现出强大的跨数据集可迁移性和对更强大模型的可扩展性，优于训练后方法，无需额外的训练或采样成本。</li>
</ul>

<h3>Title: Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19871">https://arxiv.org/abs/2601.19871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19871">https://arxiv.org/pdf/2601.19871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19871]] Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection(https://arxiv.org/abs/2601.19871)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Low-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation due to limited parallel data and linguistic resources. Recent advances in large language models suggest that self-reflection, prompting a model to critique and revise its own outputs, can improve reasoning quality and factual consistency. Building on this idea, this paper introduces Reflective Translation, a prompt-based framework in which a model generates an initial translation, produces a structured self-critique, and then uses this reflection to generate a refined translation. The approach is evaluated on English-isiZulu and English-isiXhosa translation using OPUS-100 and NTREX-African, across multiple prompting strategies and confidence thresholds. Results show consistent improvements in both BLEU and COMET scores between first- and second-pass translations, with average gains of up to +0.22 BLEU and +0.18 COMET. Statistical significance testing using paired nonparametric tests confirms that these improvements are robust. The proposed method is model-agnostic, requires no fine-tuning, and introduces a reflection-augmented dataset that can support future supervised or analysis-driven work. These findings demonstrate that structured self-reflection is a practical and effective mechanism for improving translation quality in low-resource settings.</li>
<li><strong>摘要：</strong>由于并行数据和语言资源有限，isiZulu 和 isiXhosa 等低资源语言在机器翻译方面面临着持续的挑战。大型语言模型的最新进展表明，自我反思，促使模型批评和修改自己的输出，可以提高推理质量和事实一致性。基于这个想法，本文介绍了反射翻译，这是一种基于提示的框架，其中模型生成初始翻译，产生结构化的自我批评，然后使用这种反射来生成精炼的翻译。该方法使用 OPUS-100 和 NTREX-African 在英语-isiZulu 和英语-isiXhosa 翻译上进行评估，跨越多种提示策略和置信度阈值。结果显示，第一次和第二次翻译之间的 BLEU 和 COMET 分数一致提高，平均增益高达 +0.22 BLEU 和 +0.18 COMET。使用配对非参数检验的统计显着性检验证实了这些改进是稳健的。所提出的方法与模型无关，不需要微调，并引入了反射增强数据集，可以支持未来的监督或分析驱动的工作。这些发现表明，结构化自我反思是在资源匮乏的环境中提高翻译质量的实用且有效的机制。</li>
</ul>

<h3>Title: Evaluation of Oncotimia: An LLM based system for supporting tumour boards</h3>
<ul>
<li><strong>Authors: </strong>Luis Lorenzo, Marcos Montana-Mendez, Sergio Figueiras, Miguel Boubeta, Cristobal Bernardo-Castineira</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19899">https://arxiv.org/abs/2601.19899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19899">https://arxiv.org/pdf/2601.19899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19899]] Evaluation of Oncotimia: An LLM based system for supporting tumour boards(https://arxiv.org/abs/2601.19899)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.</li>
<li><strong>摘要：</strong>多学科肿瘤委员会 (MDTB) 在肿瘤学决策中发挥着核心作用，但需要手动流程并构建大量异质临床信息，从而导致大量的文档负担。在这项工作中，我们推出了 ONCOTIMIA，这是一种模块化且安全的临床工具，旨在将生成人工智能 (GenAI) 集成到肿瘤学工作流程中，并评估其在使用大语言模型 (LLM) 自动完成肺癌肿瘤板表格中的应用。该系统结合了多层数据湖、混合关系和向量存储、检索增强生成（RAG）和规则驱动的自适应形式模型，将非结构化临床文档转换为结构化和标准化的肿瘤板记录。我们评估了通过 AWS Bedrock 部署的 6 位法学硕士在 10 个肺癌病例中的表现，测量了填写表格的准确性和端到端延迟。结果证明了跨模型的高性能，对于大多数法学硕士来说，性能最佳的配置可实现 80% 的正确现场完成率和临床可接受的响应时间。更大、更新的模型表现出最佳的精度，而不会产生令人望而却步的延迟。这些发现提供了经验证据，证明法学硕士辅助自动完成表格在多学科肺癌工作流程中在技术上和操作上都是可行的，并支持其在保持数据质量的同时显着减少文档负担的潜力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
