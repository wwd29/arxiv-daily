<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-30</h1>
<h3>Title: Can generative AI and ChatGPT outperform humans on cognitive-demanding  problem-solving tasks in science?</h3>
<ul>
<li><strong>Authors: </strong>Xiaoming Zhai, Matthew Nyaaba, Wenchao Ma</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15081">https://arxiv.org/abs/2401.15081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15081">https://arxiv.org/pdf/2401.15081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15081]] Can generative AI and ChatGPT outperform humans on cognitive-demanding  problem-solving tasks in science?(https://arxiv.org/abs/2401.15081)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, code, chat, rag</a></li>
<li><strong>Abstract: </strong>This study aimed to examine an assumption that generative artificial intelligence (GAI) tools can overcome the cognitive intensity that humans suffer when solving problems. We compared the performance of ChatGPT and GPT-4 on 2019 NAEP science assessments with students by cognitive demands of the items. Fifty-four tasks were coded by experts using a two-dimensional cognitive load framework, including task cognitive complexity and dimensionality. ChatGPT and GPT-4 responses were scored using the scoring keys of NAEP. The analysis of the available data was based on the average student ability scores for students who answered each item correctly and the percentage of students who responded to individual items. Results showed that both ChatGPT and GPT-4 consistently outperformed most students who answered the NAEP science assessments. As the cognitive demand for NAEP tasks increases, statistically higher average student ability scores are required to correctly address the questions. This pattern was observed for students in grades 4, 8, and 12, respectively. However, ChatGPT and GPT-4 were not statistically sensitive to the increase in cognitive demands of the tasks, except for Grade 4. As the first study focusing on comparing GAI and K-12 students in problem-solving in science, this finding implies the need for changes to educational objectives to prepare students with competence to work with GAI tools in the future. Education ought to emphasize the cultivation of advanced cognitive skills rather than depending solely on tasks that demand cognitive intensity. This approach would foster critical thinking, analytical skills, and the application of knowledge in novel contexts. Findings also suggest the need for innovative assessment practices by moving away from cognitive intensity tasks toward creativity and analytical skills to avoid the negative effects of GAI on testing more efficiently.</li>
<li><strong>摘要：</strong>本研究旨在检验生成人工智能（GAI）工具可以克服人类在解决问题时所遭受的认知强度的假设。我们根据项目的认知需求，将 ChatGPT 和 GPT-4 在 2019 年 NAEP 科学评估中的表现与学生进行了比较。专家使用二维认知负荷框架对 54 项任务进行编码，包括任务认知复杂性和维度。 ChatGPT 和 GPT-4 反应使用 NAEP 的评分标准进行评分。对可用数据的分析是基于正确回答每个项目的学生的平均学生能力分数以及对单个项目做出回答的学生的百分比。结果显示，ChatGPT 和 GPT-4 的表现始终优于大多数回答 NAEP 科学评估的学生。随着 NAEP 任务的认知需求增加，统计上需要更高的平均学生能力分数才能正确解决问题。分别在 4、8 和 12 年级的学生中观察到了这种模式。然而，除了 4 年级外，ChatGPT 和 GPT-4 对任务认知需求的增加在统计上不敏感。作为第一项专注于比较 GAI 和 K-12 学生解决科学问题的研究，这一发现意味着需要改变教育目标，让学生具备未来使用 GAI 工具的能力。教育应该强调高级认知技能的培养，而不是仅仅依赖于需要认知强度的任务。这种方法将培养批判性思维、分析能力以及在新环境中知识的应用。研究结果还表明，需要创新的评估实践，从认知强度任务转向创造力和分析技能，以避免 GAI 对更有效测试的负面影响。</li>
</ul>

<h3>Title: Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement  Learning</h3>
<ul>
<li><strong>Authors: </strong>Chaofan Pan, Xin Yang, Hao Wang, Wei Wei, Tianrui Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15098">https://arxiv.org/abs/2401.15098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15098">https://arxiv.org/pdf/2401.15098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15098]] Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement  Learning(https://arxiv.org/abs/2401.15098)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGrid have demonstrated the effectiveness of Hi-Core in handling diverse CRL tasks, outperforming popular baselines.</li>
<li><strong>摘要：</strong>持续强化学习 (CRL) 使 RL 代理能够从一系列任务中学习，保留以前的知识并利用它来促进未来的学习。然而，现有的方法往往侧重于在相似任务之间迁移低级知识，忽略了人类认知控制的层次结构，导致跨不同任务的知识迁移不足。为了增强高级知识迁移，我们提出了一种名为 Hi-Core（持续强化学习的分层知识迁移）的新颖框架，该框架分为两层：1）利用高级策略制定的强大推理能力大型语言模型（LLM）设定目标，2）通过以高层目标为导向的强化学习进行低层政策学习。此外，构建知识库（策略库）来存储可以检索以进行分层知识转移的策略。在 MiniGrid 中进行的实验证明了 Hi-Core 在处理各种 CRL 任务方面的有效性，优于流行的基线。</li>
</ul>

<h3>Title: Multi-agent Deep Reinforcement Learning for Dynamic Pricing by  Fast-charging Electric Vehicle Hubs in ccompetition</h3>
<ul>
<li><strong>Authors: </strong>Diwas Paudel, Tapas K. Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, econ.GN, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15108">https://arxiv.org/abs/2401.15108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15108">https://arxiv.org/pdf/2401.15108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15108]] Multi-agent Deep Reinforcement Learning for Dynamic Pricing by  Fast-charging Electric Vehicle Hubs in ccompetition(https://arxiv.org/abs/2401.15108)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitive Markov decision process (CMDP) and solving it using a multi-agent deep reinforcement learning (MADRL) approach. We develop a numerical case study for a pricing game between two charging hubs. We solve the case study with our methodology by using combinations of two different DRL algorithms, DQN and SAC, and two different neural networks (NN) architectures, a feed-forward (FF) neural network, and a multi-head attention (MHA) neural network. We construct a measure of collusion (index) using the hub profits. A value of zero for this index indicates no collusion (perfect competition) and a value of one indicates full collusion (monopolistic behavior). Our results show that the collusion index varies approximately between 0.14 and 0.45 depending on the combinations of the algorithms and the architectures chosen by the hubs.</li>
<li><strong>摘要：</strong>电动汽车快速充电中心将很快成为全球新建的交通电气化基础设施的一部分。这些充电中心预计将拥有许多直流快速充电站，并且仅允许电动汽车充电。与加油站一样，附近的快速充电中心将动态改变其价格，以争夺同一群电动汽车车主。这些中心将通过在日前（DA）电力市场中做出购买承诺来满足其电力需求的很大一部分，并满足与实时（RT）市场的差异，从而与电力网络进行互动。集线器可能有补充电池存储系统（BSS），它们将用于套利。在本文中，我们为价格竞争中的枢纽开发了一种两步数据驱动的动态定价方法。我们首先通过求解随机 DA 承诺模型来获得 DA 承诺。此后，我们通过将游戏建模为竞争性马尔可夫决策过程 (CMDP) 并使用多智能体深度强化学习 (MADRL) 方法求解来获得中心定价策略。我们为两个充电中心之间的定价游戏开发了一个数值案例研究。我们通过结合使用两种不同的 DRL 算法（DQN 和 SAC）以及两种不同的神经网络（NN）架构（前馈（FF）神经网络和多头注意力（MHA））的方法来解决案例研究神经网络。我们使用中心利润构建了共谋衡量标准（指数）。该指数的值为 0 表示没有共谋（完全竞争），值为 1 表示完全共谋（垄断行为）。我们的结果表明，共谋指数大约在 0.14 到 0.45 之间变化，具体取决于中心选择的算法和架构的组合。</li>
</ul>

<h3>Title: Interpreting Time Series Transformer Models and Sensitivity Analysis of  Population Age Groups to COVID-19 Infections</h3>
<ul>
<li><strong>Authors: </strong>Md Khairul Islam, Tyler Valentine, Timothy Joowon Sue, Ayush Karmacharya, Luke Neil Benham, Zhengguang Wang, Kingsley Kim, Judy Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.PE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15119">https://arxiv.org/abs/2401.15119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15119">https://arxiv.org/pdf/2401.15119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15119]] Interpreting Time Series Transformer Models and Sensitivity Analysis of  Population Age Groups to COVID-19 Infections(https://arxiv.org/abs/2401.15119)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Interpreting deep learning time series models is crucial in understanding the model's behavior and learning patterns from raw data for real-time decision-making. However, the complexity inherent in transformer-based time series models poses challenges in explaining the impact of individual features on predictions. In this study, we leverage recent local interpretation methods to interpret state-of-the-art time series models. To use real-world datasets, we collected three years of daily case data for 3,142 US counties. Firstly, we compare six transformer-based models and choose the best prediction model for COVID-19 infection. Using 13 input features from the last two weeks, we can predict the cases for the next two weeks. Secondly, we present an innovative way to evaluate the prediction sensitivity to 8 population age groups over highly dynamic multivariate infection data. Thirdly, we compare our proposed perturbation-based interpretation method with related work, including a total of eight local interpretation methods. Finally, we apply our framework to traffic and electricity datasets, demonstrating that our approach is generic and can be applied to other time-series domains.</li>
<li><strong>摘要：</strong>解释深度学习时间序列模型对于理解模型的行为和从原始数据中学习模式以进行实时决策至关重要。然而，基于变压器的时间序列模型固有的复杂性给解释单个特征对预测的影响带来了挑战。在本研究中，我们利用最新的局部解释方法来解释最先进的时间序列模型。为了使用真实世界的数据集，我们收集了美国 3,142 个县三年的每日病例数据。首先，我们比较了六种基于 Transformer 的模型，并选择了 COVID-19 感染的最佳预测模型。使用过去两周的 13 个输入特征，我们可以预测接下来两周的病例。其次，我们提出了一种创新的方法来评估高度动态的多变量感染数据对 8 个人口年龄组的预测敏感性。第三，我们将我们提出的基于扰动的解释方法与相关工作进行比较，包括总共八种局部解释方法。最后，我们将我们的框架应用于交通和电力数据集，证明我们的方法是通用的，可以应用于其他时间序列领域。</li>
</ul>

<h3>Title: Large Language Model Guided Knowledge Distillation for Time Series  Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, Wenchao Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15123">https://arxiv.org/abs/2401.15123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15123">https://arxiv.org/pdf/2401.15123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15123]] Large Language Model Guided Knowledge Distillation for Time Series  Anomaly Detection(https://arxiv.org/abs/2401.15123)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) We use synthetic anomalies to enlarge the representation gap between the two networks. AnomalyLLM demonstrates state-of-the-art performance on 15 datasets, improving accuracy by at least 14.5\% in the UCR dataset.</li>
<li><strong>摘要：</strong>由于可用注释的稀缺，自监督方法在时间序列异常检测中获得了突出的地位。然而，它们通常需要大量的训练数据来获取可概括的表示图，这与少数可用样本的场景相冲突，从而限制了它们的性能。为了克服这一限制，我们提出了 \textbf{AnomalyLLM}，一种基于知识蒸馏的时间序列异常检测方法，其中学生网络经过训练以模仿基于大型语言模型（LLM）的教师网络的特征，该教师网络是在大型语言模型上进行预训练的。 -规模数据集。在测试阶段，当教师和学生网络的特征差异较大时，就会检测到异常。为了避免学生网络学习教师网络的异常样本特征，我们设计了两个关键策略。 1）将原型信号合并到学生网络中，以巩固正常的特征提取。 2）我们使用合成异常来扩大两个网络之间的表示差距。 AnomalyLLM 在 15 个数据集上展示了最先进的性能，在 UCR 数据集中将准确性提高了至少 14.5%。</li>
</ul>

<h3>Title: Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning  Matches Human Performance in Some Hermeneutic Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zackary Okun Dunivin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15170">https://arxiv.org/abs/2401.15170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15170">https://arxiv.org/pdf/2401.15170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15170]] Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning  Matches Human Performance in Some Hermeneutic Tasks(https://arxiv.org/abs/2401.15170)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq 0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding fidelity improves considerably when the LLM is prompted to give rationale justifying its coding decisions (chain-of-thought reasoning). We present these and other findings along with a set of best practices for adapting traditional codebooks for LLMs. Our results indicate that for certain codebooks, state-of-the-art LLMs are already adept at large-scale content analysis. Furthermore, they suggest the next generation of models will likely render AI coding a viable option for a majority of codebooks.</li>
<li><strong>摘要：</strong>定性编码或内容分析从文本中提取含义，以识别文本语料库中的定量模式。最近，大型语言模型（LLM）解释能力的进步为自动化编码过程（将类别标签应用于文本）提供了潜力，从而使人类研究人员能够专注于更具创造性的研究方面，同时将这些解释任务委托给人工智能。我们的案例研究包括一组代表人文研究的密集、长段落的社会历史代码。我们证明 GPT-4 能够进行人类等效的解释，而 GPT-3.5 则不能。与我们的人类黄金标准相比，GPT-4 为 9 个代码中的 3 个提供了出色的内部编码器可靠性（Cohen 的 $\kappa \geq 0.79$），为 9 个代码中的 8 个提供了极高的可靠性（$\kappa \geq 0.6$）。相比之下，GPT-3.5 对于所有代码都表现不佳（$mean(\kappa) = 0.34$；$max(\kappa) = 0.55$）。重要的是，我们发现，当法学硕士被提示给出证明其编码决策合理性的基本原理（思想链推理）时，编码保真度会大大提高。我们展示了这些和其他发现以及一套针对法学硕士改编传统密码本的最佳实践。我们的结果表明，对于某些密码本，最先进的法学硕士已经擅长大规模内容分析。此外，他们认为下一代模型可能会使人工智能编码成为大多数密码本的可行选择。</li>
</ul>

<h3>Title: CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for  Mental Health</h3>
<ul>
<li><strong>Authors: </strong>Sheng Yu, Narjes Nourzad, Randye J. Semple, Yixue Zhao, Emily Zhou, Bhaskar Krishnamachari</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15188">https://arxiv.org/abs/2401.15188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15188">https://arxiv.org/pdf/2401.15188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15188]] CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for  Mental Health(https://arxiv.org/abs/2401.15188)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>The COVID-19 pandemic has intensified the urgency for effective and accessible mental health interventions in people's daily lives. Mobile Health (mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained traction as they expand beyond traditional clinical settings to support daily life. However, the effectiveness of current mHealth solutions is impeded by the lack of context-awareness, personalization, and modularity to foster their reusability. This paper introduces CAREForMe, a contextual multi-armed bandit (CMAB) recommendation framework for mental health. Designed with context-awareness, personalization, and modularity at its core, CAREForMe harnesses mobile sensing and integrates online learning algorithms with user clustering capability to deliver timely, personalized recommendations. With its modular design, CAREForMe serves as both a customizable recommendation framework to guide future research, and a collaborative platform to facilitate interdisciplinary contributions in mHealth research. We showcase CAREForMe's versatility through its implementation across various platforms (e.g., Discord, Telegram) and its customization to diverse recommendation features.</li>
<li><strong>摘要：</strong>COVID-19 大流行加剧了人们日常生活中有效且易于获得的心理健康干预措施的紧迫性。人工智能聊天机器人和正念应用程序等移动健康 (mHealth) 解决方案因超越传统临床环境以支持日常生活而受到关注。然而，当前移动医疗解决方案的有效性因缺乏情境感知、个性化和模块化来促进其可重用性而受到阻碍。本文介绍了 CAREForMe，一种针对心理健康的上下文多臂老虎机 (CMAB) 推荐框架。 CAREForMe 的设计以情境感知、个性化和模块化为核心，它利用移动传感，并将在线学习算法与用户聚类功能相集成，以提供及时、个性化的建议。凭借其模块化设计，CAREForMe 既可作为指导未来研究的可定制推荐框架，又可作为促进移动医疗研究中跨学科贡献的协作平台。我们通过跨各种平台（例如 Discord、Telegram）的实施以及针对不同推荐功能的定制来展示 CAREForMe 的多功能性。</li>
</ul>

<h3>Title: Regularized Q-Learning with Linear Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Xi, Alfredo Garcia, Petar Momcilovic</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15196">https://arxiv.org/abs/2401.15196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15196">https://arxiv.org/pdf/2401.15196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15196]] Regularized Q-Learning with Linear Function Approximation(https://arxiv.org/abs/2401.15196)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, lora</a></li>
<li><strong>Abstract: </strong>Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.</li>
<li><strong>摘要：</strong>一些成功的强化学习算法利用正则化来促进多模式策略，从而增强探索性和鲁棒性。对于函数逼近，其中一些算法（例如软 Q 学习）的收敛特性尚不清楚。在本文中，我们考虑一种单循环算法，用于在线性函数逼近的情况下最小化具有有限时间收敛保证的投影贝尔曼误差。该算法在两个尺度上运行：较慢的尺度用于更新状态动作值的目标网络，以及较快的尺度用于逼近基向量跨度子空间中的贝尔曼备份。我们表明，在某些假设下，所提出的算法在存在马尔可夫噪声的情况下收敛到驻点。此外，我们为从所提出的算法导出的策略提供了性能保证。</li>
</ul>

<h3>Title: FedGT: Federated Node Classification with Scalable Graph Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zaixi Zhang, Qingyong Hu, Yang Yu, Weibo Gao, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15203">https://arxiv.org/abs/2401.15203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15203">https://arxiv.org/pdf/2401.15203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15203]] FedGT: Federated Node Classification with Scalable Graph Transformer(https://arxiv.org/abs/2401.15203)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Graphs are widely used to model relational data. As graphs are getting larger and larger in real-world scenarios, there is a trend to store and compute subgraphs in multiple local systems. For example, recently proposed \emph{subgraph federated learning} methods train Graph Neural Networks (GNNs) distributively on local subgraphs and aggregate GNN parameters with a central server. However, existing methods have the following limitations: (1) The links between local subgraphs are missing in subgraph federated learning. This could severely damage the performance of GNNs that follow message-passing paradigms to update node/edge features. (2) Most existing methods overlook the subgraph heterogeneity issue, brought by subgraphs being from different parts of the whole graph. To address the aforementioned challenges, we propose a scalable \textbf{Fed}erated \textbf{G}raph \textbf{T}ransformer (\textbf{FedGT}) in the paper. Firstly, we design a hybrid attention scheme to reduce the complexity of the Graph Transformer to linear while ensuring a global receptive field with theoretical bounds. Specifically, each node attends to the sampled local neighbors and a set of curated global nodes to learn both local and global information and be robust to missing links. The global nodes are dynamically updated during training with an online clustering algorithm to capture the data distribution of the corresponding local subgraph. Secondly, FedGT computes clients' similarity based on the aligned global nodes with optimal transport. The similarity is then used to perform weighted averaging for personalized aggregation, which well addresses the data heterogeneity problem. Moreover, local differential privacy is applied to further protect the privacy of clients. Finally, extensive experimental results on 6 datasets and 2 subgraph settings demonstrate the superiority of FedGT.</li>
<li><strong>摘要：</strong>图广泛用于对关系数据进行建模。随着现实场景中图变得越来越大，出现了在多个本地系统中存储和计算子图的趋势。例如，最近提出的 \emph{子图联邦学习} 方法在局部子图上分布式训练图神经网络（GNN），并通过中央服务器聚合 GNN 参数。然而，现有方法存在以下局限性：（1）子图联邦学习中缺少局部子图之间的联系。这可能会严重损害遵循消息传递范例来更新节点/边缘特征的 GNN 的性能。 （2）大多数现有方法忽略了子图异质性问题，该问题是由子图来自整个图的不同部分带来的。为了解决上述挑战，我们在论文中提出了一个可扩展的 \textbf{Fed}erated \textbf{G}raph \textbf{T}transformer (\textbf{FedGT})。首先，我们设计了一种混合注意力方案，将图变换器的复杂度降低到线性，同时确保全局感受野具有理论界限。具体来说，每个节点都会关注采样的本地邻居和一组精选的全局节点，以学习本地和全局信息并对丢失的链接具有鲁棒性。全局节点在训练过程中通过在线聚类算法动态更新，以捕获相应局部子图的数据分布。其次，FedGT 基于具有最佳传输的对齐全球节点来计算客户端的相似度。然后利用相似度进行加权平均进行个性化聚合，很好地解决了数据异构性问题。此外，还采用本地差分隐私，进一步保护客户端的隐私。最后，6 个数据集和 2 个子图设置的广泛实验结果证明了 FedGT 的优越性。</li>
</ul>

<h3>Title: HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Liu, Yiqun Zhang, Qian Li, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15207">https://arxiv.org/abs/2401.15207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15207">https://arxiv.org/pdf/2401.15207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15207]] HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy(https://arxiv.org/abs/2401.15207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including AdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\% GPU memory compared with standard full-parameter fine-tuning for 7B model. (4) HiFT enables full-parameter fine-tuning of a 7B model on single 48G A6000 with a precision of 32 using the AdamW optimizer, without using any memory saving techniques.</li>
<li><strong>摘要：</strong>全参数微调因其优异的性能而成为语言模型（LM）适应下游任务的首选。随着 LM 尺寸的增大，微调 LM 的完整参数需要大量的 GPU 内存。现有方法利用零阶优化器来节省 GPU 内存，这可能会损害 LM 的性能，因为非零阶优化器往往更容易收敛于大多数下游任务。在本文中，我们提出了一种新颖的独立于优化器的端到端分层微调策略 HiFT，它仅在每个训练步骤更新参数的子集。 HiFT 可以显着减少同时驻留在 GPU 内存中的梯度和优化器状态参数的数量，从而减少 GPU 内存的使用。我们的结果表明：（1）HiFT 实现了与参数高效微调和标准全参数微调相当的性能。 (2) HiFT 支持各种优化器，包括 AdamW、AdaGrad、SGD 等。 (3) HiFT 与 7B 模型的标准全参数微调相比，可以节省 60\% 以上的 GPU 内存。 (4) HiFT 可以使用 AdamW 优化器在单个 48G A6000 上以 32 的精度对 7B 模型进行全参数微调，而无需使用任何内存节省技术。</li>
</ul>

<h3>Title: Deep Learning with Tabular Data: A Self-supervised Approach</h3>
<ul>
<li><strong>Authors: </strong>Tirth Kiranbhai Vyas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15238">https://arxiv.org/abs/2401.15238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15238">https://arxiv.org/pdf/2401.15238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15238]] Deep Learning with Tabular Data: A Self-supervised Approach(https://arxiv.org/abs/2401.15238)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to examine performance of the TabTransformer model against baseline models such as MLP and supervised TabTransformer. The research has presented with a novel approach by creating various variants of TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT which has helped to increase the effective capturing of the underlying relationship between various features of the tabular dataset by constructing optimal inputs. And further we have employed a self-supervised learning approach in the form of a masking-based unsupervised setting for tabular data. The findings shed light on the best way to represent categorical and numerical features, emphasizing the TabTransormer performance when compared to established machine learning models and other self-supervised learning methods.</li>
<li><strong>摘要：</strong>我们描述了一种使用 TabTransformer 模型和自监督学习来训练表格数据的新方法。表格数据的传统机器学习模型（例如 GBDT）正在被广泛使用，尽管我们的论文检查了 TabTransformer 的有效性，TabTransformer 是专门针对表格数据优化的基于 Transformer 的模型。 TabTransformer 通过利用 Transformer 的自注意力机制来捕获表格数据中特征之间复杂的关系和依赖关系。我们在本研究中使用了自我监督学习方法，其中 TabTransformer 通过创建代理监督任务来从未标记数据中学习，从而消除了对标记数据的需求。目的是找到分类和数值特征最有效的 TabTransformer 模型表示。解决在 Transformers 中构建各种输入设置过程中面临的挑战。此外，还进行了比较分析，以检查 TabTransformer 模型与 MLP 和监督 TabTransformer 等基线模型的性能。该研究提出了一种新颖的方法，通过创建 TabTransformer 模型的各种变体，即 Binned-TT、Vanilla-MLP-TT、MLP-based-TT，这有助于提高对表格各种特征之间潜在关系的有效捕获。通过构建最佳输入来构建数据集。此外，我们还采用了一种自我监督学习方法，其形式是针对表格数据的基于掩码的无监督设置。研究结果揭示了表示分类和数字特征的最佳方式，强调了 TabTransormer 与已建立的机器学习模型和其他自监督学习方法相比的性能。</li>
</ul>

<h3>Title: Unlearning Reveals the Influential Training Data of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Masaru Isonuma, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15241">https://arxiv.org/abs/2401.15241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15241">https://arxiv.org/pdf/2401.15241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15241]] Unlearning Reveals the Influential Training Data of Language Models(https://arxiv.org/abs/2401.15241)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.</li>
<li><strong>摘要：</strong>为了提高语言模型的性能，同时降低生成有害内容的风险，确定哪些训练数据集影响模型的输出至关重要。理想情况下，我们可以通过将每个数据集从训练中删除来衡量其影响；然而，多次重新训练模型的成本高昂。本文提出了 UnTrac，它通过从训练模型中忘却训练数据集的影响来估计训练数据集的影响。 UnTrac 非常简单；每个训练数据集都通过梯度上升来取消学习，并且我们评估取消学习后模型的预测发生了多少变化。我们凭经验检验我们的方法是否可以评估预训练数据集对生成有毒、有偏见和不真实内容的影响。实验结果表明，我们的方法比现有方法更准确地估计它们的影响，同时既不需要过多的内存空间，也不需要多个模型检查点。</li>
</ul>

<h3>Title: Better Representations via Adversarial Training in Pre-Training: A  Theoretical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yue Xing, Xiaofeng Lin, Qifan Song, Yi Xu, Belinda Zeng, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15248">https://arxiv.org/abs/2401.15248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15248">https://arxiv.org/pdf/2401.15248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15248]] Better Representations via Adversarial Training in Pre-Training: A  Theoretical Perspective(https://arxiv.org/abs/2401.15248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., \cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.</li>
<li><strong>摘要：</strong>众所周知，预训练可以为大规模深度学习（例如大型语言模型）中的下游任务生成通用表示。现有文献，例如 \cite{kim2020adversarial}，根据经验观察到下游任务可以继承预训练模型的对抗鲁棒性。我们为这种鲁棒性继承现象提供了理论依据。我们的理论结果表明，特征纯化在连接预训练模型的对抗鲁棒性和两层神经网络中的下游任务方面发挥着重要作用。具体来说，我们表明（i）通过对抗性训练，每个隐藏节点往往只选择一个（或几个）特征； (ii) 如果没有对抗性训练，隐藏节点可能容易受到攻击。这一观察结果对于监督预训练和对比学习都有效。事实证明，通过纯化的节点，干净的训练足以在下游任务中实现对抗鲁棒性。</li>
</ul>

<h3>Title: Towards Stable Preferences for Stakeholder-aligned Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Haleema Sheraz, Stefan C. Kremer, Joshua August Skorburg, Graham Taylor, Walter Sinnott-Armstrong, Kyle Boerstler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15268">https://arxiv.org/abs/2401.15268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15268">https://arxiv.org/pdf/2401.15268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15268]] Towards Stable Preferences for Stakeholder-aligned Machine Learning(https://arxiv.org/abs/2401.15268)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of organ transplantation, contributing to more transparent and equitable practices while promoting the integration of moral values into algorithmic decision-making.</li>
<li><strong>摘要：</strong>为了应对肾脏分配面临的紧迫挑战（其特点是对器官的需求不断增长），本研究着手开发一种数据驱动的解决方案来解决这一问题，其中也融入了利益相关者的价值观。这项研究的主要目的是创建一种方法来了解与肾脏分配有关的个人和群体层面的偏好。利用“配对肾脏患者在线调查”的数据。利用两个不同的数据集并在三个级别（个人、群体和稳定性）进行评估，我们采用通过多个​​指标进行评估的机器学习分类器。个人级别模型预测个人参与者的偏好，组级别模型汇总参与者的偏好，稳定性级别模型（组级别的扩展）评估这些偏好随时间的稳定性。通过将利益相关者的偏好纳入肾脏分配过程，我们渴望提高器官移植的伦理维度，促进更加透明和公平的实践，同时促进将道德价值观融入算法决策中。</li>
</ul>

<h3>Title: Improving Medical Reasoning through Retrieval and Self-Reflection with  Retrieval-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15269">https://arxiv.org/abs/2401.15269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15269">https://arxiv.org/pdf/2401.15269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15269]] Improving Medical Reasoning through Retrieval and Self-Reflection with  Retrieval-Augmented Large Language Models(https://arxiv.org/abs/2401.15269)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.</li>
<li><strong>摘要：</strong>最近的专有大语言模型 (LLM)，例如 GPT-4，在应对生物医学领域的各种挑战（从多项选择题到长格式生成）方面取得了里程碑式的进展。为了解决法学硕士的编码知识仍然无法处理的挑战，人们开发了各种检索增强生成（RAG）方法，通过从知识库中搜索文档并将其无条件或有选择地附加到法学硕士的输入中进行生成。然而，当将现有方法应用于不同领域的特定问题时，泛化性较差，导致获取错误的文档或做出不准确的判断。在本文中，我们介绍了 Self-BioRAG，这是一个可靠的生物医学文本框架，专门用于生成解释、检索特定领域的文档以及自我反思生成的响应。我们利用 84k 过滤的生物医学指令集来训练 Self-BioRAG，它可以使用定制的反射令牌评估其生成的解释。我们的工作证明，特定领域的组件（例如检索器、领域相关文档语料库和指令集）对于遵守领域相关指令是必要的。使用三个主要的医学问答基准数据集，Self-BioRAG 的实验结果表明，与参数大小为 7B 或更小的最先进的开放式基础模型相比，平均绝对提升了 7.2%，显着提高了性能。总的来说，我们分析 Self-BioRAG 找到问题中的线索，在需要时检索相关文档，并像医学专家一样理解如何利用检索到的文档和编码知识中的信息进行回答。我们发布了用于训练框架组件和模型权重（7B 和 13B）的数据和代码，以增强生物医学和临床领域的能力。</li>
</ul>

<h3>Title: Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement  Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Zhang, Han Wang, Aritra Mitra, James Anderson</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15273">https://arxiv.org/abs/2401.15273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15273">https://arxiv.org/pdf/2401.15273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15273]] Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement  Learning(https://arxiv.org/abs/2401.15273)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-time error analysis. Notably, we establish that FedSARSA converges to a policy that is near-optimal for all agents, with the extent of near-optimality proportional to the level of heterogeneity. Furthermore, we prove that FedSARSA leverages agent collaboration to enable linear speedups as the number of agents increases, which holds for both fixed and adaptive step-size configurations.</li>
<li><strong>摘要：</strong>联合强化学习（FRL）已成为一种有前途的范例，通过利用来自不同代理的信息来降低强化学习任务的样本复杂性。然而，当每个智能体与潜在不同的环境交互时，理论上对 FRL 算法的非渐近性能知之甚少。缺乏这样的结果可以归因于各种技术挑战及其复杂的相互作用：马尔可夫采样、线性函数逼近、用于保存通信的多次本地更新、奖励函数的异质性和代理 MDP 的转换内核以及连续的状态动作空间。此外，在策略设置中，行为策略随时间变化，这使得分析变得更加复杂。为此，我们引入了 FedSARSA，这是一种新颖的联合策略强化学习方案，配备线性函数逼近，以应对这些挑战并提供全面的有限时间误差分析。值得注意的是，我们确定 FedSARSA 收敛于对所有主体而言接近最优的策略，接近最优的程度与异质性水平成正比。此外，我们证明 FedSARSA 利用代理协作来随着代理数量的增加实现线性加速，这对于固定和自适应步长配置都适用。</li>
</ul>

<h3>Title: Multi-Trigger Backdoor Attacks: More Triggers, More Threats</h3>
<ul>
<li><strong>Authors: </strong>Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15295">https://arxiv.org/abs/2401.15295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15295">https://arxiv.org/pdf/2401.15295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15295]] Multi-Trigger Backdoor Attacks: More Triggers, More Threats(https://arxiv.org/abs/2401.15295)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Backdoor attacks have emerged as a primary threat to (pre-)training and deployment of deep neural networks (DNNs). While backdoor attacks have been extensively studied in a body of works, most of them were focused on single-trigger attacks that poison a dataset using a single type of trigger. Arguably, real-world backdoor attacks can be much more complex, e.g., the existence of multiple adversaries for the same dataset if it is of high value. In this work, we investigate the practical threat of backdoor attacks under the setting of \textbf{multi-trigger attacks} where multiple adversaries leverage different types of triggers to poison the same dataset. By proposing and investigating three types of multi-trigger attacks, including parallel, sequential, and hybrid attacks, we provide a set of important understandings of the coexisting, overwriting, and cross-activating effects between different triggers on the same dataset. Moreover, we show that single-trigger attacks tend to cause overly optimistic views of the security of current defense techniques, as all examined defense methods struggle to defend against multi-trigger attacks. Finally, we create a multi-trigger backdoor poisoning dataset to help future evaluation of backdoor attacks and defenses. Although our work is purely empirical, we hope it can help steer backdoor research toward more realistic settings.</li>
<li><strong>摘要：</strong>后门攻击已成为深度神经网络 (DNN)（预）训练和部署的主要威胁。虽然后门攻击已在大量工作中得到广泛研究，但其中大多数都集中于使用单一类型触发器毒害数据集的单触发攻击。可以说，现实世界的后门攻击可能要复杂得多，例如，如果同一数据集具有高价值，则存在多个对手。在这项工作中，我们研究了 \textbf{多触发攻击} 设置下后门攻击的实际威胁，其中多个对手利用不同类型的触发器来毒害同一数据集。通过提出和研究三种类型的多触发攻击，包括并行攻击、顺序攻击和混合攻击，我们对同一数据集上不同触发器之间的共存、覆盖和交叉激活效果提供了一系列重要的理解。此外，我们表明单触发攻击往往会导致对当前防御技术的安全性过于乐观，因为所有检查的防御方法都难以防御多触发攻击。最后，我们创建了一个多触发后门中毒数据集，以帮助将来评估后门攻击和防御。尽管我们的工作纯粹是实证性的，但我们希望它能够帮助引导后门研究走向更现实的环境。</li>
</ul>

<h3>Title: UNSEE: Unsupervised Non-contrastive Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Ömer Veysel Çağatan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15316">https://arxiv.org/abs/2401.15316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15316">https://arxiv.org/pdf/2401.15316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15316]] UNSEE: Unsupervised Non-contrastive Sentence Embeddings(https://arxiv.org/abs/2401.15316)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our exploration begins by addressing the challenge of representation collapse, a phenomenon observed when contrastive objectives in SimCSE are replaced with non-contrastive objectives. To counter this issue, we propose a straightforward solution known as the target network, effectively mitigating representation collapse. The introduction of the target network allows us to leverage non-contrastive objectives, maintaining training stability while achieving performance improvements comparable to contrastive objectives. Our method has achieved peak performance in non-contrastive sentence embeddings through meticulous fine-tuning and optimization. This comprehensive effort has yielded superior sentence representation models, showcasing the effectiveness of our approach.</li>
<li><strong>摘要：</strong>我们提出了 UNSEE：无监督非对比句子嵌入，这是一种在大规模文本嵌入基准测试中优于 SimCSE 的新颖方法。我们的探索从解决表征崩溃的挑战开始，这是当 SimCSE 中的对比目标被非对比目标替换时观察到的现象。为了解决这个问题，我们提出了一种称为目标网络的简单解决方案，可以有效缓解表示崩溃。目标网络的引入使我们能够利用非对比目标，保持训练稳定性，同时实现与对比目标相当的性能改进。通过细致的微调和优化，我们的方法在非对比句子嵌入方面取得了最佳性能。这种全面的努力产生了卓越的句子表示模型，展示了我们方法的有效性。</li>
</ul>

<h3>Title: Equipping Language Models with Tool Use Capability for Tabular Data  Analysis in Finance</h3>
<ul>
<li><strong>Authors: </strong>Adrian Theuma, Ehsan Shareghi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15328">https://arxiv.org/abs/2401.15328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15328">https://arxiv.org/pdf/2401.15328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15328]] Equipping Language Models with Tool Use Capability for Tabular Data  Analysis in Finance(https://arxiv.org/abs/2401.15328)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM's inherent abilities. More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and 'task solver'. The 'task router' dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with strong GPT-3.5 results. To the best of our knowledge, our work is the first that investigates tool augmentation of language models for the finance domain.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）展现了一系列推理能力，但面临着错误传播和幻觉等挑战，特别是在金融等数据异构且精度至关重要的专业领域。我们探索使用外部工具增强语言模型的潜力，以减轻这些限制，并将某些推理步骤转移到更适合任务的外部工具，而不是仅仅依赖于法学硕士的固有能力。更具体地说，使用金融领域问答数据集，我们对 LLaMA-2 13B 聊天模型应用监督微调，以充当“任务路由器”和“任务求解器”。 “任务路由器”动态地引导一个问题，要么由法学硕士内部回答，要么通过工具集中的正确工具从外部回答。我们配备工具的 SFT 模型 Raven 比基本模型和仅 SFT 基线分别提高了 35.2% 和 5.06%，并且与强大的 GPT-3.5 结果相比具有很强的竞争力。据我们所知，我们的工作是第一个研究金融领域语言模型工具增强的工作。</li>
</ul>

<h3>Title: Deep Learning with Information Fusion and Model Interpretation for  Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart  Rate Monitoring Data</h3>
<ul>
<li><strong>Authors: </strong>Zenghui Lin, Xintong Liu, Nan Wang, Ruichen Li, Qingao Liu, Jingying Ma, Liwei Wang, Yan Wang, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15337">https://arxiv.org/abs/2401.15337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15337">https://arxiv.org/pdf/2401.15337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15337]] Deep Learning with Information Fusion and Model Interpretation for  Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart  Rate Monitoring Data(https://arxiv.org/abs/2401.15337)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Long-term fetal heart rate (FHR) monitoring during the antepartum period, increasingly popularized by electronic FHR monitoring, represents a growing approach in FHR monitoring. This kind of continuous monitoring, in contrast to the short-term one, collects an extended period of fetal heart data. This offers a more comprehensive understanding of fetus's conditions. However, the interpretation of long-term antenatal fetal heart monitoring is still in its early stages, lacking corresponding clinical standards. Furthermore, the substantial amount of data generated by continuous monitoring imposes a significant burden on clinical work when analyzed manually. To address above challenges, this study develops an automatic analysis system named LARA (Long-term Antepartum Risk Analysis system) for continuous FHR monitoring, combining deep learning and information fusion methods. LARA's core is a well-established convolutional neural network (CNN) model. It processes long-term FHR data as input and generates a Risk Distribution Map (RDM) and Risk Index (RI) as the analysis results. We evaluate LARA on inner test dataset, the performance metrics are as follows: AUC 0.872, accuracy 0.816, specificity 0.811, sensitivity 0.806, precision 0.271, and F1 score 0.415. In our study, we observe that long-term FHR monitoring data with higher RI is more likely to result in adverse outcomes (p=0.0021). In conclusion, this study introduces LARA, the first automated analysis system for long-term FHR monitoring, initiating the further explorations into its clinical value in the future.</li>
<li><strong>摘要：</strong>产前的长期胎心率 (FHR) 监测，随着电子胎心率监测的日益普及，代表了胎心率监测的一种不断发展的方法。与短期监测相比，这种连续监测可以收集较长时间的胎心数据。这可以更全面地了解胎儿的状况。然而，对长期产前胎心监测的解读仍处于早期阶段，缺乏相应的临床标准。此外，连续监测产生的大量数据在手动分析时给临床工作带来了巨大的负担。为了解决上述挑战，本研究结合深度学习和信息融合方法，开发了一种名为 LARA（长期产前风险分析系统）的连续胎心率监测自动分析系统。 LARA 的核心是一个完善的卷积神经网络（CNN）模型。它处理长期胎心率数据作为输入，并生成风险分布图（RDM）和风险指数（RI）作为分析结果。我们在内部测试数据集上评估 LARA，性能指标如下：AUC 0.872，准确度 0.816，特异性 0.811，灵敏度 0.806，精度 0.271，F1 得分 0.415。在我们的研究中，我们观察到，RI 较高的长期胎心率监测数据更有可能导致不良后果 (p=0.0021)。总之，本研究引入了LARA这一首个用于长期FHR监测的自动化分析系统，开启了未来对其临床价值的进一步探索。</li>
</ul>

<h3>Title: A Comprehensive Survey of Compression Algorithms for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seungcheol Park, Jaehyeon Choi, Sojin Lee, U Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15347">https://arxiv.org/abs/2401.15347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15347">https://arxiv.org/pdf/2401.15347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15347]] A Comprehensive Survey of Compression Algorithms for Language Models(https://arxiv.org/abs/2401.15347)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.</li>
<li><strong>摘要：</strong>我们如何在不牺牲准确性的情况下压缩语言模型？语言模型的压缩算法的数量正在快速增长，受益于最新语言模型的显着进步，并且不会因语言模型的巨大规模而产生副作用，例如碳排放增加和昂贵的维护费用。尽管许多压缩算法在压缩语言模型方面取得了显着进展，但具有讽刺意味的是，由于算法数量过多，捕捉新兴趋势并识别其背后的基本概念变得具有挑战性。在本文中，我们调查并总结了各种压缩算法，包括剪枝、量化、知识蒸馏、低秩近似、参数共享和高效架构设计。我们不仅总结了各种压缩算法的总体趋势，而且选择了具有代表性的算法并对其进行了深入分析。我们讨论了每一类压缩算法的价值，以及低成本压缩算法的所需属性，这些属性由于大型语言模型的出现而产生了重大影响。最后，我们根据我们的调查结果介绍了有前景的未来研究主题。</li>
</ul>

<h3>Title: A Statistical Framework for Measuring AI Reliance</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15356">https://arxiv.org/abs/2401.15356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15356">https://arxiv.org/pdf/2401.15356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15356]] A Statistical Framework for Measuring AI Reliance(https://arxiv.org/abs/2401.15356)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI's prediction from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies from literature, we demonstrate how our framework can be used to separate the loss due to mis-reliance from the loss due to not accurately differentiating the signals. We evaluate these losses by comparing to a baseline and a benchmark for complementary performance defined by the expected payoff achieved by a rational agent facing the same decision task as the behavioral agents.</li>
<li><strong>摘要：</strong>人类经常借助人工智能 (AI) 系统做出决策。人工智能的一个常见模式是向保留最终决策控制权的人类推荐一项行动。研究人员发现，确保人类对人工智能有适当的依赖是实现互补绩效的关键组成部分。我们认为，目前此类研究中使用的适当依赖的定义缺乏正式的统计基础，可能会导致矛盾。我们基于统计决策理论提出了信赖的正式定义，它将信赖的概念分开，即决策者遵循人工智能预测的概率与人类在区分信号和形成对情况的准确信念时可能面临的挑战。我们的定义提出了一个框架，可用于指导人类与人工智能的互补性和依赖性研究的设计和解释。利用最近文献中的人工智能建议决策研究，我们展示了如何使用我们的框架将由于错误依赖而导致的损失与由于无法准确地区分信号而导致的损失分开。我们通过与互补绩效的基线和基准进行比较来评估这些损失，该互补绩效是由理性代理人面临与行为代理人相同的决策任务所实现的预期回报所定义的。</li>
</ul>

<h3>Title: LegalDuet: Learning Effective Representations for Legal Judgment  Prediction through a Dual-View Legal Clue Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Pengjie Liu, Zhenghao Liu, Xiaoyuan Yi, Liner Yang, Shuo Wang, Yu Gu, Ge Yu, Xing Xie, Shuang-hua Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15371">https://arxiv.org/abs/2401.15371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15371">https://arxiv.org/pdf/2401.15371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15371]] LegalDuet: Learning Effective Representations for Legal Judgment  Prediction through a Dual-View Legal Clue Reasoning(https://arxiv.org/abs/2401.15371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag</a></li>
<li><strong>Abstract: </strong>Most existing Legal Judgment Prediction (LJP) models focus on discovering the legal triggers in the criminal fact description. However, in real-world scenarios, a professional judge not only needs to assimilate the law case experience that thrives on past sentenced legal judgments but also depends on the professional legal grounded reasoning that learned from professional legal knowledge. In this paper, we propose a LegalDuet model, which pretrains language models to learn a tailored embedding space for making legal judgments. It proposes a dual-view legal clue reasoning mechanism, which derives from two reasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments according to the judgment experiences learned from analogy/confusing legal cases; 2) Legal Ground Reasoning, which lies in matching the legal clues between criminal cases and legal decisions. Our experiments show that LegalDuet achieves state-of-the-art performance on the CAIL2018 dataset and outperforms baselines with about 4% improvements on average. Our dual-view reasoning based pretraining can capture critical legal clues to learn a tailored embedding space to distinguish criminal cases. It reduces LegalDuet's uncertainty during prediction and brings pretraining advances to the confusing/low frequent charges. All codes are available at https://github.com/NEUIR/LegalDuet.</li>
<li><strong>摘要：</strong>大多数现有的法律判决预测（LJP）模型侧重于发现犯罪事实描述中的法律触发因素。然而，在现实世界中，专业法官不仅需要吸收过去判决的法律判决中积累的案件经验，还依赖于从专业法律知识中学到的专业法律推理。在本文中，我们提出了一种 LegalDuet 模型，该模型对语言模型进行预训练，以学习用于做出法律判断的定制嵌入空间。提出了双视法律线索推理机制，源于法官的两条推理链条：1）法例推理，根据类比/混淆的法律案件中汲取的判断经验做出法律判断； 2）法律依据推理，即刑事案件与法律判决之间的法律线索的匹配。我们的实验表明，LegalDuet 在 CAIL2018 数据集上实现了最先进的性能，并且优于基线，平均提高了约 4%。我们基于双视图推理的预训练可以捕获关键的法律线索，以学习定制的嵌入空间来区分刑事案件。它减少了 LegalDuet 在预测过程中的不确定性，并为令人困惑/低频的费用带来了预训练方面的进步。所有代码均可在 https://github.com/NEUIR/LegalDuet 获取。</li>
</ul>

<h3>Title: A RAG-based Question Answering System Proposal for Understanding Islam:  MufassirQAS LLM</h3>
<ul>
<li><strong>Authors: </strong>Ahmet Yusuf Alan, Enis Karaarslan, Omer Aydin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15378">https://arxiv.org/abs/2401.15378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15378">https://arxiv.org/pdf/2401.15378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15378]] A RAG-based Question Answering System Proposal for Understanding Islam:  MufassirQAS LLM(https://arxiv.org/abs/2401.15378)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, hallucination, prompt, chat, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We created a vector database with several open-access books that include Turkish context. These are Turkish translations, and interpretations on Islam. We worked on creating system prompts with care, ensuring they provide instructions that prevent harmful, offensive, or disrespectful responses. We also tested the MufassirQAS and ChatGPT with sensitive questions. We got better performance with our system. Study and enhancements are still in progress. Results and future works are given.</li>
<li><strong>摘要：</strong>由于宗教教义和教义的复杂性和深度，学习和理解宗教存在挑战。聊天机器人作为问答系统可以帮助解决这些挑战。 LLM 聊天机器人使用 NLP 技术在主题之间建立联系并准确回答复杂的问题。这些功能使其非常适合作为问答聊天机器人用于宗教启蒙。然而，法学硕士也有产生虚假信息的倾向，称为幻觉。聊天机器人的响应可能包括侮辱个人宗教信仰、宗教间冲突以及有争议或敏感话题的内容。它需要避免此类案件，同时又不宣扬仇恨言论或冒犯某些群体或其信仰。本研究使用基于向量数据库的检索增强生成（RAG）方法来提高法学硕士的准确性和透明度。我们的问答系统称为“MufassirQAS”。我们创建了一个矢量数据库，其中包含几本包含土耳其语背景的开放获取书籍。这些是土耳其语翻译以及对伊斯兰教的解释。我们精心创建系统提示，确保它们提供的说明可以防止有害、冒犯或不尊重的反应。我们还使用敏感问题测试了 MufassirQAS 和 ChatGPT。我们的系统获得了更好的性能。研究和改进仍在进行中。给出了结果和未来的工作。</li>
</ul>

<h3>Title: Towards Event Extraction from Speech with Contextual Clues</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Kang, Tongtong Wu, Jinming Zhao, Guitao Wang, Guilin Qi, Yuan-Fang Li, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15385">https://arxiv.org/abs/2401.15385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15385">https://arxiv.org/pdf/2401.15385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15385]] Towards Event Extraction from Speech with Contextual Clues(https://arxiv.org/abs/2401.15385)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>While text-based event extraction has been an active research area and has seen successful application in many domains, extracting semantic events from speech directly is an under-explored problem. In this paper, we introduce the Speech Event Extraction (SpeechEE) task and construct three synthetic training sets and one human-spoken test set. Compared to event extraction from text, SpeechEE poses greater challenges mainly due to complex speech signals that are continuous and have no word boundaries. Additionally, unlike perceptible sound events, semantic events are more subtle and require a deeper understanding. To tackle these challenges, we introduce a sequence-to-structure generation paradigm that can produce events from speech signals in an end-to-end manner, together with a conditioned generation method that utilizes speech recognition transcripts as the contextual clue. We further propose to represent events with a flat format to make outputs more natural language-like. Our experimental results show that our method brings significant improvements on all datasets, achieving a maximum F1 gain of 10.7%. The code and datasets are released on https://github.com/jodie-kang/SpeechEE.</li>
<li><strong>摘要：</strong>虽然基于文本的事件提取一直是一个活跃的研究领域，并在许多领域取得了成功的应用，但直接从语音中提取语义事件是一个尚未充分研究的问题。在本文中，我们介绍了语音事件提取（SpeechEE）任务，并构建了三个综合训练集和一个人类口语测试集。与从文本中提取事件相比，SpeechEE 提出了更大的挑战，主要是因为复杂的语音信号是连续的且没有单词边界。此外，与可感知的声音事件不同，语义事件更加微妙，需要更深入的理解。为了应对这些挑战，我们引入了一种序列到结构的生成范例，该范例可以以端到端的方式从语音信号生成事件，以及利用语音识别转录本作为上下文线索的条件生成方法。我们进一步建议用平面格式表示事件，以使输出更加自然语言。我们的实验结果表明，我们的方法对所有数据集都带来了显着的改进，实现了最大 10.7% 的 F1 增益。代码和数据集发布在 https://github.com/jodie-kang/SpeechEE 上。</li>
</ul>

<h3>Title: MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop  Queries</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Tang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15391">https://arxiv.org/abs/2401.15391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15391">https://arxiv.org/pdf/2401.15391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15391]] MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop  Queries(https://arxiv.org/abs/2401.15391)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）通过检索相关知识来增强大型语言模型（LLM），在减轻 LLM 幻觉和提高响应质量方面显示出巨大的潜力，从而促进 LLM 在实践中的广泛采用。然而，我们发现现有的 RAG 系统不足以回答多跳查询，这需要对多个支持证据进行检索和推理。此外，据我们所知，现有的 RAG 基准测试数据集还没有关注多跳查询。在本文中，我们开发了一个新颖的数据集 MultiHop-RAG，它由知识库、大量多跳查询、其真实答案以及相关的支持证据组成。我们详细介绍了构建数据集的过程，利用英语新闻文章数据集作为底层 RAG 知识库。我们在两个实验中展示了 MultiHop-RAG 的基准测试实用性。第一个实验比较了用于检索多跳查询证据的不同嵌入模型。在第二个实验中，我们检查了各种最先进的 LLM（包括 GPT-4、PaLM 和 Llama2-70B）在给定证据的情况下推理和回答多跳查询的能力。这两个实验都表明，现有的 RAG 方法在检索和回答多跳查询方面的表现并不令人满意。我们希望 MultiHop-RAG 能够成为社区开发有效 RAG 系统的宝贵资源，从而促进法学硕士在实践中得到更多采用。 MultiHop-RAG 和实施的 RAG 系统可在 https://github.com/yixuantt/MultiHop-RAG/ 上公开获取。</li>
</ul>

<h3>Title: A Survey on Data Augmentation in Large Model Era</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15422">https://arxiv.org/abs/2401.15422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15422">https://arxiv.org/pdf/2401.15422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15422]] A Survey on Data Augmentation in Large Model Era(https://arxiv.org/abs/2401.15422)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, lora, rag</a></li>
<li><strong>Abstract: </strong>Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data augmentation. Our discussion then expands to encompass the array of applications for these data augmentation methods within natural language processing, computer vision, and audio signal processing. We proceed to evaluate the successes and limitations of large model-based data augmentation across different scenarios. Concluding our review, we highlight prospective challenges and avenues for future exploration in the field of data augmentation. Our objective is to furnish researchers with critical insights, ultimately contributing to the advancement of more sophisticated large models. We consistently maintain the related open-source materials at: https://github.com/MLGroup-JLU/LLM-data-aug-survey.</li>
<li><strong>摘要：</strong>包含大型语言和扩散模型的大型模型在接近人类水平的智能方面表现出了非凡的前景，引起了学术界和工业界的极大兴趣。然而，这些大型模型的训练需要大量的高质量数据，而随着这些模型的不断更新，现有的高质量数据库可能很快就会耗尽。这一挑战促进了数据增强方法研究的激增。利用大型模型，这些数据增强技术的性能优于传统方法。本文采用综合视角，对大型模型驱动的数据增强方法进行了详尽的回顾。我们首先将相关研究分为三个主要类别：图像增强、文本增强和配对数据增强。接下来，我们深入研究与基于大型模型的数据增强相关的各种数据后处理技术。然后，我们的讨论扩展到涵盖这些数据增强方法在自然语言处理、计算机视觉和音频信号处理中的一系列应用。我们继续评估不同场景中基于大型模型的数据增强的成功和局限性。在总结我们的评论时，我们强调了数据增强领域未来探索的潜在挑战和途径。我们的目标是为研究人员提供批判性见解，最终为更复杂的大型模型的进步做出贡献。我们始终在以下位置维护相关开源材料：https://github.com/MLGroup-JLU/LLM-data-aug-survey。</li>
</ul>

<h3>Title: Pre-training and Diagnosing Knowledge Base Completion Models</h3>
<ul>
<li><strong>Authors: </strong>Vid Kocijan, Myeongjun Erik Jang, Thomas Lukasiewicz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15439">https://arxiv.org/abs/2401.15439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15439">https://arxiv.org/pdf/2401.15439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15439]] Pre-training and Diagnosing Knowledge Base Completion Models(https://arxiv.org/abs/2401.15439)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this work, we introduce and analyze an approach to knowledge transfer from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. The main contribution is a method that can make use of large-scale pre-training on facts, which were collected from unstructured text, to improve predictions on structured data from a specific domain. The introduced method is most impactful on small datasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal rank and 65% relative decrease of mean rank over the previously best method was achieved, despite not relying on large pre-trained models like Bert. To understand the obtained pre-trained models better, we then introduce a novel dataset for the analysis of pre-trained models for Open Knowledge Base Completion, called Doge (Diagnostics of Open knowledge Graph Embeddings). It consists of 6 subsets and is designed to measure multiple properties of a pre-trained model: robustness against synonyms, ability to perform deductive reasoning, presence of gender stereotypes, consistency with reverse relations, and coverage of different areas of general knowledge. Using the introduced dataset, we show that the existing OKBC models lack consistency in the presence of synonyms and inverse relations and are unable to perform deductive reasoning. Moreover, their predictions often align with gender stereotypes, which persist even when presented with counterevidence. We additionally investigate the role of pre-trained word embeddings and demonstrate that avoiding biased word embeddings is not a sufficient measure to prevent biased behavior of OKBC models.</li>
<li><strong>摘要：</strong>在这项工作中，我们介绍并分析了一种从一组事实到另一组事实的知识转移方法，无需实体或关系匹配。该方法适用于规范化知识库和非规范化或开放知识库，即可能存在多个现实世界实体或关系副本的知识库。主要贡献是一种可以利用从非结构化文本收集的事实的大规模预训练来改进对特定领域的结构化数据的预测的方法。引入的方法对 ReVerb20k 等小型数据集影响最大，尽管不依赖于 Bert 等大型预训练模型，但与之前的最佳方法相比，平均倒数排名绝对增加了 6%，平均排名相对下降了 65% 。为了更好地理解所获得的预训练模型，我们引入了一个新的数据集，用于分析开放知识库补全的预训练模型，称为 Doge（开放知识图嵌入诊断）。它由 6 个子集组成，旨在衡量预训练模型的多个属性：对同义词的鲁棒性、执行演绎推理的能力、性别刻板印象的存在、反向关系的一致性以及对不同领域常识的覆盖。使用引入的数据集，我们表明现有的 OKBC 模型在同义词和逆关系的存在下缺乏一致性，并且无法执行演绎推理。此外，他们的预测通常与性别刻板印象相一致，即使有反证据，这种刻板印象也仍然存在。我们还研究了预训练词嵌入的作用，并证明避免有偏见的词嵌入并不足以防止 OKBC 模型的偏见行为。</li>
</ul>

<h3>Title: DiffuserLite: Towards Real-time Diffusion Planning</h3>
<ul>
<li><strong>Authors: </strong>Zibin Dong, Jianye Hao, Yifu Yuan, Fei Ni, Yitian Wang, Pengyi Li, Yan Zheng</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15443">https://arxiv.org/abs/2401.15443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15443">https://arxiv.org/pdf/2401.15443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15443]] DiffuserLite: Towards Real-time Diffusion Planning(https://arxiv.org/abs/2401.15443)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve as a flexible plugin to enhance decision frequency in other diffusion planning algorithms, providing a structural design reference for future works. More details and visualizations are available at [project website](https://diffuserlite.github.io/).</li>
<li><strong>摘要：</strong>扩散规划已被认为是各个领域的有效决策范例。长视距轨迹的高质量条件生成能力使其成为一个有前途的研究方向。然而，由于昂贵的迭代采样成本，现有的扩散规划方法决策频率较低。为了解决这个问题，我们引入了 DiffuserLite，一个快速且轻量级的扩散规划框架。 DiffuserLite 采用规划细化流程 (PRP) 来生成从粗到细粒度的轨迹，这显着减少了冗余信息的建模，并导致决策频率显着增加。我们的实验结果表明，与之前的框架相比，DiffuserLite 的运行时成本仅为 $0.88\%$，实现了 $122$Hz 的平均决策频率，并在 D4RL 基准测试中达到了最先进的性能。此外，我们干净的DiffuserLite框架可以作为灵活的插件来提高其他扩散规划算法的决策频率，为未来的工作提供结构设计参考。更多细节和可视化可在[项目网站](https://diffuserlite.github.io/)获得。</li>
</ul>

<h3>Title: Towards Causal Classification: A Comprehensive Study on Graph Neural  Networks</h3>
<ul>
<li><strong>Authors: </strong>Simi Job, Xiaohui Tao, Taotao Cai, Lin Li, Haoran Xie, Jianming Yong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15444">https://arxiv.org/abs/2401.15444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15444">https://arxiv.org/pdf/2401.15444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15444]] Towards Causal Classification: A Comprehensive Study on Graph Neural  Networks(https://arxiv.org/abs/2401.15444)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>The exploration of Graph Neural Networks (GNNs) for processing graph-structured data has expanded, particularly their potential for causal analysis due to their universal approximation capabilities. Anticipated to significantly enhance common graph-based tasks such as classification and prediction, the development of a causally enhanced GNN framework is yet to be thoroughly investigated. Addressing this shortfall, our study delves into nine benchmark graph classification models, testing their strength and versatility across seven datasets spanning three varied domains to discern the impact of causality on the predictive prowess of GNNs. This research offers a detailed assessment of these models, shedding light on their efficiency, and flexibility in different data environments, and highlighting areas needing advancement. Our findings are instrumental in furthering the understanding and practical application of GNNs in diverse datacentric fields</li>
<li><strong>摘要：</strong>图神经网络（GNN）用于处理图结构数据的探索已经扩大，特别是由于其通用逼近功能，它们在因果分析方面的潜力。因果增强 GNN 框架的开发预计将显着增强常见的基于图的任务，例如分类和预测，但仍有待彻底研究。为了解决这一不足，我们的研究深入研究了九个基准图分类模型，在跨越三个不同领域的七个数据集上测试它们的强度和多功能性，以辨别因果关系对 GNN 预测能力的影响。这项研究对这些模型进行了详细评估，揭示了它们在不同数据环境中的效率和灵活性，并强调了需要改进的领域。我们的发现有助于进一步理解 GNN 在不同数据中心领域的实际应用</li>
</ul>

<h3>Title: Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for  Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Liang, Zhuoyang Song, Hao Wang, Jiaxing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15449">https://arxiv.org/abs/2401.15449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15449">https://arxiv.org/pdf/2401.15449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15449]] Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for  Hallucination Mitigation(https://arxiv.org/abs/2401.15449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, rag</a></li>
<li><strong>Abstract: </strong>We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks.</li>
<li><strong>摘要：</strong>我们评估大型语言模型（LLM）辨别和表达其内部知识状态的能力，这是对抗事实幻觉和确保 LLM 可靠应用的关键因素。我们观察到法学硕士对内部知识状态有很强的自我意识，知识探索的准确率超过 85% 就证明了这一点。然而，法学硕士在生成过程中往往无法表达他们的内部知识，导致事实幻觉。我们开发了一种自动幻觉注释工具 Dreamcatcher，它融合了知识探索和一致性检查方法来对事实偏好数据进行排名。以知识偏好作为奖励，我们提出了知识反馈强化学习（RLKF）培训框架，利用强化学习来增强法学硕士的事实性和诚实性。我们对多个模型的实验表明，RLKF 训练有效地增强了模型利用其内部知识状态的能力，从而提高了各种基于知识和诚实相关任务的性能。</li>
</ul>

<h3>Title: DataFrame QA: A Universal LLM Framework on DataFrame Question Answering  Without Data Exposure</h3>
<ul>
<li><strong>Authors: </strong>Junyi Ye, Mengnan Du, Guiling Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15463">https://arxiv.org/abs/2401.15463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15463">https://arxiv.org/pdf/2401.15463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15463]] DataFrame QA: A Universal LLM Framework on DataFrame Question Answering  Without Data Exposure(https://arxiv.org/abs/2401.15463)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>This paper introduces DataFrame question answering (QA), a novel task that utilizes large language models (LLMs) to generate Pandas queries for information retrieval and data analysis on dataframes, emphasizing safe and non-revealing data handling. Our method, which solely relies on dataframe column names, not only ensures data privacy but also significantly reduces the context window in the prompt, streamlining information processing and addressing major challenges in LLM-based data analysis. We propose DataFrame QA as a comprehensive framework that includes safe Pandas query generation and code execution. Various LLMs, notably GPT-4, are evaluated using the pass@1 metric on the renowned WikiSQL and our newly developed 'UCI-DataFrameQA', tailored for complex data analysis queries. Our findings indicate that GPT-4 achieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA, underscoring its capability in securely retrieving and aggregating dataframe values and conducting sophisticated data analyses. This approach, deployable in a zero-shot manner without prior training or adjustments, proves to be highly adaptable and secure for diverse applications.</li>
<li><strong>摘要：</strong>本文介绍了 DataFrame 问答 (QA)，这是一项利用大型语言模型 (LLM) 生成 Pandas 查询以对数据框进行信息检索和数据分析的新颖任务，强调安全和非泄露的数据处理。我们的方法仅依赖于数据框列名称，不仅确保了数据隐私，而且还显着减少了提示中的上下文窗口，简化了信息处理并解决了基于 LLM 的数据分析中的主要挑战。我们建议 DataFrame QA 作为一个综合框架，其中包括安全的 Pandas 查询生成和代码执行。各种 LLM，特别是 GPT-4，使用著名的 WikiSQL 上的 pass@1 指标和我们新开发的“UCI-DataFrameQA”进行评估，专为复杂的数据分析查询而定制。我们的研究结果表明，GPT-4 在 WikiSQL 上的通过率达到 86%，在 UCI-DataFrameQA 上的通过率达到 97%，这突显了其安全检索和聚合数据帧值以及进行复杂数据分析的能力。这种方法可以以零次方式部署，无需事先培训或调整，事实证明对于不同的应用程序具有高度的适应性和安全性。</li>
</ul>

<h3>Title: Wind speed super-resolution and validation: from ERA5 to CERRA via  diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Fabio Merizzi, Andrea Asperti, Stefano Colamonaco</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15469">https://arxiv.org/abs/2401.15469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15469">https://arxiv.org/pdf/2401.15469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15469]] Wind speed super-resolution and validation: from ERA5 to CERRA via  diffusion models(https://arxiv.org/abs/2401.15469)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution regional reanalysis dataset for the European domain. In recent years it has shown significant utility across various climate-related tasks, ranging from forecasting and climate change research to renewable energy prediction, resource management, air quality risk assessment, and the forecasting of rare events, among others. Unfortunately, the availability of CERRA is lagging two years behind the current date, due to constraints in acquiring the requisite external data and the intensive computational demands inherent in its generation. As a solution, this paper introduces a novel method using diffusion models to approximate CERRA downscaling in a data-driven manner, without additional informations. By leveraging the lower resolution ERA5 dataset, which provides boundary conditions for CERRA, we approach this as a super-resolution task. Focusing on wind speed around Italy, our model, trained on existing CERRA data, shows promising results, closely mirroring original CERRA data. Validation with in-situ observations further confirms the model's accuracy in approximating ground measurements.</li>
<li><strong>摘要：</strong>哥白尼欧洲区域再分析 (CERRA) 是欧洲域的高分辨率区域再分析数据集。近年来，它在各种与气候相关的任务中显示出显着的实用性，从预测和气候变化研究到可再生能源预测、资源管理、空气质量风险评估和罕见事件的预测等。不幸的是，由于获取必要的外部数据的限制以及其生成过程中固有的密集计算需求，CERRA 的可用性比当前日期晚了两年。作为解决方案，本文介绍了一种使用扩散模型以数据驱动的方式近似 CERRA 降尺度的新方法，无需额外信息。通过利用为 CERRA 提供边界条件的较低分辨率 ERA5 数据集，我们将其视为超分辨率任务。我们的模型专注于意大利周围的风速，并根据现有 CERRA 数据进行训练，显示出可喜的结果，与原始 CERRA 数据密切相关。现场观测的验证进一步证实了该模型在近似地面测量方面的准确性。</li>
</ul>

<h3>Title: ConvoSense: Overcoming Monotonous Commonsense Inferences for  Conversational AI</h3>
<ul>
<li><strong>Authors: </strong>Sarah E. Finch, Jinho D. Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15471">https://arxiv.org/abs/2401.15471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15471">https://arxiv.org/pdf/2401.15471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15471]] ConvoSense: Overcoming Monotonous Commonsense Inferences for  Conversational AI(https://arxiv.org/abs/2401.15471)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Mastering commonsense understanding and reasoning is a pivotal skill essential for conducting engaging conversations. While there have been several attempts to create datasets that facilitate commonsense inferences in dialogue contexts, existing datasets tend to lack in-depth details, restate information already present in the conversation, and often fail to capture the multifaceted nature of commonsense reasoning. In response to these limitations, we compile a new synthetic dataset for commonsense reasoning in dialogue contexts using GPT, ConvoSense, that boasts greater contextual novelty, offers a higher volume of inferences per example, and substantially enriches the detail conveyed by the inferences. Our dataset contains over 500,000 inferences across 12,000 dialogues with 10 popular inference types, which empowers the training of generative commonsense models for dialogue that are superior in producing plausible inferences with high novelty when compared to models trained on the previous datasets. To the best of our knowledge, ConvoSense is the first of its kind to provide such a multitude of novel inferences at such a large scale.</li>
<li><strong>摘要：</strong>掌握常识性理解和推理是进行引人入胜的对话所必需的关键技能。尽管已经多次尝试创建有助于对话上下文中常识推理的数据集，但现有数据集往往缺乏深入的细节，重述了对话中已存在的信息，并且常常无法捕捉常识推理的多方面性质。为了应对这些限制，我们使用 GPT ConvoSense 编译了一个新的合成数据集，用于对话上下文中的常识推理，该数据集拥有更大的上下文新颖性，每个示例提供更多的推理量，并大大丰富了推理所传达的细节。我们的数据集包含 12,000 个对话中 10 种流行推理类型的超过 500,000 个推理，这使得能够训练对话生成常识模型，与在之前的数据集上训练的模型相比，这些模型在产生具有高度新颖性的合理推理方面表现出色。据我们所知，ConvoSense 是同类中第一个能够在如此大的范围内提供如此多的新颖推论的。</li>
</ul>

<h3>Title: To Burst or Not to Burst: Generating and Quantifying Improbable Text</h3>
<ul>
<li><strong>Authors: </strong>Kuleen Sasse, Samuel Barham, Efsun Sarioglu Kayi, Edward W. Staley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15476">https://arxiv.org/abs/2401.15476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15476">https://arxiv.org/pdf/2401.15476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15476]] To Burst or Not to Burst: Generating and Quantifying Improbable Text(https://arxiv.org/abs/2401.15476)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) are extremely capable at text generation, their outputs are still distinguishable from human-authored text. We explore this separation across many metrics over text, many sampling techniques, many types of text data, and across two popular LLMs, LLaMA and Vicuna. Along the way, we introduce a new metric, recoverability, to highlight differences between human and machine text; and we propose a new sampling technique, burst sampling, designed to close this gap. We find that LLaMA and Vicuna have distinct distributions under many of the metrics, and that this influences our results: Recoverability separates real from fake text better than any other metric when using LLaMA. When using Vicuna, burst sampling produces text which is distributionally closer to real text compared to other sampling techniques.</li>
<li><strong>摘要：</strong>虽然大型语言模型（LLM）在文本生成方面非常有能力，但它们的输出仍然与人类创作的文本有区别。我们跨多种文本指标、多种采样技术、多种类型的文本数据以及两种流行的法学硕士（LLaMA 和 Vicuna）探索这种分离。在此过程中，我们引入了一个新的指标：可恢复性，以突出人类文本和机器文本之间的差异；我们提出了一种新的采样技术，即突发采样，旨在缩小这一差距。我们发现 LLaMA 和 Vicuna 在许多指标下具有不同的分布，这会影响我们的结果：使用 LLaMA 时，可恢复性比任何其他指标都能更好地将真实文本与虚假文本区分开来。使用 Vicuna 时，与其他采样技术相比，突发采样生成的文本在分布上更接近真实文本。</li>
</ul>

<h3>Title: Social Interpretable Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Lucio Custode, Giovanni Iacca</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15480">https://arxiv.org/abs/2401.15480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15480">https://arxiv.org/pdf/2401.15480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15480]] Social Interpretable Reinforcement Learning(https://arxiv.org/abs/2401.15480)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) bears the promise of being an enabling technology for many applications. However, since most of the literature in the field is currently focused on opaque models, the use of RL in high-stakes scenarios, where interpretability is crucial, is still limited. Recently, some approaches to interpretable RL, e.g., based on Decision Trees, have been proposed, but one of the main limitations of these techniques is their training cost. To overcome this limitation, we propose a new population-based method, called Social Interpretable RL (SIRL), inspired by social learning principles, to improve learning efficiency. Our method mimics a social learning process, where each agent in a group learns to solve a given task based both on its own individual experience as well as the experience acquired together with its peers. Our approach is divided into two phases. In the \emph{collaborative phase}, all the agents in the population interact with a shared instance of the environment, where each agent observes the state and independently proposes an action. Then, voting is performed to choose the action that will actually be performed in the environment. In the \emph{individual phase}, each agent refines its individual performance by interacting with its own instance of the environment. This mechanism makes the agents experience a larger number of episodes while simultaneously reducing the computational cost of the process. Our results on six well-known benchmarks show that SIRL reaches state-of-the-art performance w.r.t. the alternative interpretable methods from the literature.</li>
<li><strong>摘要：</strong>强化学习 (RL) 有望成为许多应用的支持技术。然而，由于该领域的大多数文献目前都集中在不透明模型上，因此在可解释性至关重要的高风险场景中使用强化学习仍然受到限制。最近，人们提出了一些可解释强化学习的方法，例如基于决策树的方法，但这些技术的主要限制之一是它们的训练成本。为了克服这一限制，我们受到社会学习原理的启发，提出了一种基于人群的新方法，称为社交可解释强化学习（SIRL），以提高学习效率。我们的方法模仿社会学习过程，其中组中的每个智能体都根据自己的个人经验以及与同伴一起获得的经验来学习解决给定的任务。我们的方法分为两个阶段。在\emph{协作阶段}，群体中的所有智能体与环境的共享实例交互，其中每个智能体观察状态并独立地提出一个动作。然后，进行投票以选择将在环境中实际执行的操作。在\emph{个体阶段}，每个智能体通过与自己的环境实例交互来完善其个体性能。这种机制使代理经历更多的事件，同时降低过程的计算成本。我们在六个著名基准测试中的结果表明，SIRL 达到了最先进的性能。文献中的替代解释方法。</li>
</ul>

<h3>Title: Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue  Summarization</h3>
<ul>
<li><strong>Authors: </strong>Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, Yiyong Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15496">https://arxiv.org/abs/2401.15496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15496">https://arxiv.org/pdf/2401.15496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15496]] Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue  Summarization(https://arxiv.org/abs/2401.15496)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We release our model and related codes to facilitate future studies on dialogue summarization task.</li>
<li><strong>摘要：</strong>Llama、Baichuan 和 Bloom 模型等大型语言模型 (LLM) 在许多自然语言任务中显示出卓越的指令微调能力。然而，对于旨在为对话中不同角色生成摘要的对话摘要任务，大多数最先进的方法都是在小型模型上进行的（例如 Bart 和 Bert）。现有的方法尝试在小模型上添加任务指定的优化，例如向模型添加全局-局部中心性得分。在本文中，我们提出了一种指令微调模型：Baichuan2-Sum，用于面向角色的对话摘要。通过为不同的角色设置不同的指令，模型可以从对话交互中学习并输出预期的摘要。此外，我们应用 NEFTune 技术在训练过程中添加适当的噪声以改善结果。实验表明，所提出的模型在两个公共对话摘要数据集：CSDS 和 SAMSUM 上取得了最新的结果。我们发布我们的模型和相关代码，以方便未来对对话摘要任务的研究。</li>
</ul>

<h3>Title: Do We Need Language-Specific Fact-Checking Models? The Case of Chinese</h3>
<ul>
<li><strong>Authors: </strong>Caiqi Zhang, Zhijiang Guo, Andreas Vlachos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15498">https://arxiv.org/abs/2401.15498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15498">https://arxiv.org/pdf/2401.15498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15498]] Do We Need Language-Specific Fact-Checking Models? The Case of Chinese(https://arxiv.org/abs/2401.15498)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese. We demonstrate the limitations of methods such as translating Chinese claims and evidence into English or directly using multilingual large language models (e.g. GPT4), highlighting the need for language-specific systems. We further develop a state-of-the-art Chinese fact-checking system that, in contrast to previous approaches which treat evidence selection as a pairwise sentence classification task, considers the context of sentences. We also create an adversarial dataset to identify biases in our model, and while they are present as in English language datasets and models, they are often specific to the Chinese culture. Our study emphasizes the importance of language-specific fact-checking models to effectively combat misinformation.</li>
<li><strong>摘要：</strong>本文研究了特定语言的事实检查模型的潜在好处，重点关注中文的案例。我们论证了将中文主张和证据翻译成英文或直接使用多语言大语言模型（例如 GPT4）等方法的局限性，强调了对特定语言系统的需求。我们进一步开发了一种最先进的中文事实核查系统，与之前将证据选择视为成对句子分类任务的方法相比，该系统考虑了句子的上下文。我们还创建了一个对抗性数据集来识别模型中的偏差，虽然它们以英语数据集和模型的形式出现，但它们通常是针对中国文化的。我们的研究强调了特定语言的事实检查模型对于有效打击错误信息的重要性。</li>
</ul>

<h3>Title: Semantic Properties of cosine based bias scores for word embeddings</h3>
<ul>
<li><strong>Authors: </strong>Sarah Schröder, Alexander Schulz, Fabian Hinder, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15499">https://arxiv.org/abs/2401.15499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15499">https://arxiv.org/pdf/2401.15499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15499]] Semantic Properties of cosine based bias scores for word embeddings(https://arxiv.org/abs/2401.15499)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Plenty of works have brought social biases in language models to attention and proposed methods to detect such biases. As a result, the literature contains a great deal of different bias tests and scores, each introduced with the premise to uncover yet more biases that other scores fail to detect. What severely lacks in the literature, however, are comparative studies that analyse such bias scores and help researchers to understand the benefits or limitations of the existing methods. In this work, we aim to close this gap for cosine based bias scores. By building on a geometric definition of bias, we propose requirements for bias scores to be considered meaningful for quantifying biases. Furthermore, we formally analyze cosine based scores from the literature with regard to these requirements. We underline these findings with experiments to show that the bias scores' limitations have an impact in the application case.</li>
<li><strong>摘要：</strong>许多工作引起了人们对语言模型中社会偏见的关注，并提出了检测此类偏见的方法。因此，文献中包含了大量不同的偏见测试和分数，每一个引入的前提都是为了发现更多其他分数无法检测到的偏见。然而，文献中严重缺乏的是分析此类偏差分数并帮助研究人员了解现有方法的优点或局限性的比较研究。在这项工作中，我们的目标是缩小基于余弦偏差分数的差距。通过建立偏差的几何定义，我们提出了偏差分数的要求，以被认为对量化偏差有意义。此外，我们根据这些要求正式分析了文献中基于余弦的分数。我们通过实验强调这些发现，以表明偏差分数的局限性对应用案例有影响。</li>
</ul>

<h3>Title: Data-Driven Estimation of the False Positive Rate of the Bayes Binary  Classifier via Soft Labels</h3>
<ul>
<li><strong>Authors: </strong>Minoh Jeong, Martina Cardone, Alex Dytso</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15500">https://arxiv.org/abs/2401.15500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15500">https://arxiv.org/pdf/2401.15500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15500]] Data-Driven Estimation of the False Positive Rate of the Bayes Binary  Classifier via Soft Labels(https://arxiv.org/abs/2401.15500)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Classification is a fundamental task in many applications on which data-driven methods have shown outstanding performances. However, it is challenging to determine whether such methods have achieved the optimal performance. This is mainly because the best achievable performance is typically unknown and hence, effectively estimating it is of prime importance. In this paper, we consider binary classification problems and we propose an estimator for the false positive rate (FPR) of the Bayes classifier, that is, the optimal classifier with respect to accuracy, from a given dataset. Our method utilizes soft labels, or real-valued labels, which are gaining significant traction thanks to their properties. We thoroughly examine various theoretical properties of our estimator, including its consistency, unbiasedness, rate of convergence, and variance. To enhance the versatility of our estimator beyond soft labels, we also consider noisy labels, which encompass binary labels. For noisy labels, we develop effective FPR estimators by leveraging a denoising technique and the Nadaraya-Watson estimator. Due to the symmetry of the problem, our results can be readily applied to estimate the false negative rate of the Bayes classifier.</li>
<li><strong>摘要：</strong>分类是许多应用中的一项基本任务，数据驱动方法在这些应用中表现出了出色的性能。然而，确定这些方法是否达到了最佳性能是具有挑战性的。这主要是因为可实现的最佳性能通常是未知的，因此有效估计它至关重要。在本文中，我们考虑二元分类问题，并提出了贝叶斯分类器误报率（FPR）的估计器，即给定数据集的准确度最佳分类器。我们的方法利用软标签或实值标签，这些标签因其特性而获得了巨大的关注。我们彻底检查了估计器的各种理论属性，包括其一致性、无偏性、收敛率和方差。为了增强估计器超越软标签的多功能性，我们还考虑包含二进制标签的噪声标签。对于噪声标签，我们通过利用去噪技术和 Nadaraya-Watson 估计器来开发有效的 FPR 估计器。由于问题的对称性，我们的结果可以很容易地应用于估计贝叶斯分类器的假阴性率。</li>
</ul>

<h3>Title: Style-News: Incorporating Stylized News Generation and Adversarial  Verification for Neural Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Wei-Yao Wang, Yu-Chieh Chang, Wen-Chih Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15509">https://arxiv.org/abs/2401.15509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15509">https://arxiv.org/pdf/2401.15509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15509]] Style-News: Incorporating Stylized News Generation and Adversarial  Verification for Neural Fake News Detection(https://arxiv.org/abs/2401.15509)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>With the improvements in generative models, the issues of producing hallucinations in various domains (e.g., law, writing) have been brought to people's attention due to concerns about misinformation. In this paper, we focus on neural fake news, which refers to content generated by neural networks aiming to mimic the style of real news to deceive people. To prevent harmful disinformation spreading fallaciously from malicious social media (e.g., content farms), we propose a novel verification framework, Style-News, using publisher metadata to imply a publisher's template with the corresponding text types, political stance, and credibility. Based on threat modeling aspects, a style-aware neural news generator is introduced as an adversary for generating news content conditioning for a specific publisher, and style and source discriminators are trained to defend against this attack by identifying which publisher the style corresponds with, and discriminating whether the source of the given news is human-written or machine-generated. To evaluate the quality of the generated content, we integrate various dimensional metrics (language fluency, content preservation, and style adherence) and demonstrate that Style-News significantly outperforms the previous approaches by a margin of 0.35 for fluency, 15.24 for content, and 0.38 for style at most. Moreover, our discriminative model outperforms state-of-the-art baselines in terms of publisher prediction (up to 4.64%) and neural fake news detection (+6.94% $\sim$ 31.72%).</li>
<li><strong>摘要：</strong>随着生成模型的改进，由于对错误信息的担忧，在各个领域（例如法律、写作）产生幻觉的问题已经引起人们的注意。在本文中，我们关注的是神经假新闻，它是指由神经网络生成的内容，旨在模仿真实新闻的风格来欺骗人们。为了防止有害的虚假信息从恶意社交媒体（例如内容农场）错误地传播，我们提出了一种新颖的验证框架，Style-News，使用发布者元数据来暗示发布者的模板以及相应的文本类型、政治立场和可信度。基于威胁建模方面，引入了风格感知神经新闻生成器作为对手，用于为特定发布者生成新闻内容调节，并且训练风格和源鉴别器通过识别风格对应的发布者来防御这种攻击，并且区分给定新闻的来源是人写的还是机器生成的。为了评估生成内容的质量，我们整合了各种维度指标（语言流畅性、内容保留和风格遵循），并证明 Style-News 显着优于以前的方法，流畅度为 0.35，内容为 15.24，内容为 0.38最多是为了风格。此外，我们的判别模型在发布商预测（高达 4.64%）和神经假新闻检测（+6.94% $\sim$ 31.72%）方面优于最先进的基线。</li>
</ul>

<h3>Title: Quantifying Stereotypes in Language</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15535">https://arxiv.org/abs/2401.15535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15535">https://arxiv.org/pdf/2401.15535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15535]] Quantifying Stereotypes in Language(https://arxiv.org/abs/2401.15535)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>A stereotype is a generalized perception of a specific group of humans. It is often potentially encoded in human language, which is more common in texts on social issues. Previous works simply define a sentence as stereotypical and anti-stereotypical. However, the stereotype of a sentence may require fine-grained quantification. In this paper, to fill this gap, we quantify stereotypes in language by annotating a dataset. We use the pre-trained language models (PLMs) to learn this dataset to predict stereotypes of sentences. Then, we discuss stereotypes about common social issues such as hate speech, sexism, sentiments, and disadvantaged and advantaged groups. We demonstrate the connections and differences between stereotypes and common social issues, and all four studies validate the general findings of the current studies. In addition, our work suggests that fine-grained stereotype scores are a highly relevant and competitive dimension for research on social issues.</li>
<li><strong>摘要：</strong>刻板印象是对特定人群的普遍看法。它通常可能用人类语言进行编码，这在有关社会问题的文本中更为常见。以前的作品只是简单地将一个句子定义为刻板印象和反刻板印象。然而，句子的刻板印象可能需要细粒度的量化。在本文中，为了填补这一空白，我们通过注释数据集来量化语言中的刻板印象。我们使用预先训练的语言模型（PLM）来学习这个数据集来预测句子的刻板印象。然后，我们讨论关于仇恨言论、性别歧视、情绪以及弱势群体和优势群体等常见社会问题的刻板印象。我们展示了刻板印象与常见社会问题之间的联系和差异，所有四项研究都验证了当前研究的一般结果。此外，我们的工作表明，细粒度的刻板印象分数对于社会问题的研究来说是一个高度相关且具有竞争力的维度。</li>
</ul>

<h3>Title: Anomaly Detection of Particle Orbit in Accelerator using LSTM Deep  Learning Technology</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Chen, Wei Lu, Radhika Bhong, Yimin Hu, Brian Freeman, Adam Carpenter</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.acc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15543">https://arxiv.org/abs/2401.15543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15543">https://arxiv.org/pdf/2401.15543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15543]] Anomaly Detection of Particle Orbit in Accelerator using LSTM Deep  Learning Technology(https://arxiv.org/abs/2401.15543)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>A stable, reliable, and controllable orbit lock system is crucial to an electron (or ion) accelerator because the beam orbit and beam energy instability strongly affect the quality of the beam delivered to experimental halls. Currently, when the orbit lock system fails operators must manually intervene. This paper develops a Machine Learning based fault detection methodology to identify orbit lock anomalies and notify accelerator operations staff of the off-normal behavior. Our method is unsupervised, so it does not require labeled data. It uses Long-Short Memory Networks (LSTM) Auto Encoder to capture normal patterns and predict future values of monitoring sensors in the orbit lock system. Anomalies are detected when the prediction error exceeds a threshold. We conducted experiments using monitoring data from Jefferson Lab's Continuous Electron Beam Accelerator Facility (CEBAF). The results are promising: the percentage of real anomalies identified by our solution is 68.6%-89.3% using monitoring data of a single component in the orbit lock control system. The accuracy can be as high as 82%.</li>
<li><strong>摘要：</strong>稳定、可靠、可控的轨道锁定系统对于电子（或离子）加速器至关重要，因为束流轨道和束流能量的不稳定性强烈影响输送到实验大厅的束流质量。目前，当轨道锁定系统出现故障时，操作员必须手动干预。本文开发了一种基于机器学习的故障检测方法来识别轨道锁定异常并向加速器操作人员通知异常行为。我们的方法是无监督的，因此不需要标记数据。它使用长短记忆网络（LSTM）自动编码器来捕获正常模式并预测轨道锁定系统中监控传感器的未来值。当预测误差超过阈值时，就会检测到异常。我们使用杰斐逊实验室连续电子束加速器设施 (CEBAF) 的监测数据进行了实验。结果令人鼓舞：使用轨道锁控制系统中单个组件的监测数据，我们的解决方案识别出的真实异常百分比为 68.6%-89.3%。准确率可高达82%。</li>
</ul>

<h3>Title: Augment before You Try: Knowledge-Enhanced Table Question Answering via  Table Expansion</h3>
<ul>
<li><strong>Authors: </strong>Yujian Liu, Jiabao Ji, Tong Yu, Ryan Rossi, Sungchul Kim, Handong Zhao, Ritwik Sinha, Yang Zhang, Shiyu Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15555">https://arxiv.org/abs/2401.15555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15555">https://arxiv.org/pdf/2401.15555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15555]] Augment before You Try: Knowledge-Enhanced Table Question Answering via  Table Expansion(https://arxiv.org/abs/2401.15555)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Table question answering is a popular task that assesses a model's ability to understand and interact with structured data. However, the given table often does not contain sufficient information for answering the question, necessitating the integration of external knowledge. Existing methods either convert both the table and external knowledge into text, which neglects the structured nature of the table; or they embed queries for external sources in the interaction with the table, which complicates the process. In this paper, we propose a simple yet effective method to integrate external information in a given table. Our method first constructs an augmenting table containing the missing information and then generates a SQL query over the two tables to answer the question. Experiments show that our method outperforms strong baselines on three table QA benchmarks. Our code is publicly available at https://github.com/UCSB-NLP-Chang/Augment_tableQA.</li>
<li><strong>摘要：</strong>表问答是一项流行的任务，用于评估模型理解结构化数据并与之交互的能力。然而，给定的表格通常不包含足够的信息来回答问题，需要整合外部知识。现有方法要么将表格和外部知识都转换为文本，忽略了表格的结构化性质；或者它们在与表的交互中嵌入对外部源的查询，这使过程变得复杂。在本文中，我们提出了一种简单而有效的方法来将外部信息集成到给定的表中。我们的方法首先构建一个包含缺失信息的扩充表，然后对这两个表生成 SQL 查询来回答问题。实验表明，我们的方法在三个表 QA 基准上的性能优于强基线。我们的代码可在 https://github.com/UCSB-NLP-Chang/Au​​gment_tableQA 上公开获取。</li>
</ul>

<h3>Title: Efficient Tuning and Inference for Large Language Models on Textual  Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15569">https://arxiv.org/abs/2401.15569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15569">https://arxiv.org/pdf/2401.15569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15569]] Efficient Tuning and Inference for Large Language Models on Textual  Graphs(https://arxiv.org/abs/2401.15569)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Rich textual and topological information of textual graphs need to be modeled in real-world applications such as webpages, e-commerce, and academic articles. Practitioners have been long following the path of adopting a shallow text encoder and a subsequent graph neural network (GNN) to solve this problem. In light of recent advancements in large language models (LLMs), it is apparent that integrating LLMs for enhanced textual encoding can substantially improve the performance of textual graphs. Nevertheless, the efficiency of these methods poses a significant challenge. In this paper, we propose ENGINE, a parameter- and memory-efficient fine-tuning method for textual graphs with an LLM encoder. The key insight is to combine the LLMs and GNNs through a tunable side structure, which significantly reduces the training complexity without impairing the joint model's capacity. Extensive experiments on textual graphs demonstrate our method's effectiveness by achieving the best model performance, meanwhile having the lowest training cost compared to previous methods. Moreover, we introduce two variants with caching and dynamic early exit to further enhance training and inference speed. Specifically, caching accelerates ENGINE's training by 12x, and dynamic early exit achieves up to 5x faster inference with a negligible performance drop (at maximum 1.17% relevant drop across 7 datasets).</li>
<li><strong>摘要：</strong>文本图的丰富文本和拓扑信息需要在网页、电子商务和学术文章等现实应用中进行建模。从业者长期以来一直遵循采用浅层文本编码器和随后的图神经网络（GNN）来解决这个问题的道路。鉴于大型语言模型 (LLM) 的最新进展，显然，集成 LLM 来增强文本编码可以显着提高文本图的性能。然而，这些方法的效率提出了重大挑战。在本文中，我们提出了 ENGINE，一种使用 LLM 编码器对文本图进行参数和内存高效微调的方法。关键的见解是通过可调节的侧面结构将 LLM 和 GNN 结合起来，这在不损害联合模型容量的情况下显着降低了训练复杂性。对文本图的大量实验证明了我们的方法的有效性，实现了最佳模型性能，同时与以前的方法相比具有最低的训练成本。此外，我们引入了两种具有缓存和动态提前退出的变体，以进一步提高训练和推理速度。具体来说，缓存将 ENGINE 的训练速度提高了 12 倍，动态提前退出将推理速度提高了 5 倍，而性能下降可以忽略不计（7 个数据集的相关下降最多为 1.17%）。</li>
</ul>

<h3>Title: DGNN: Decoupled Graph Neural Networks with Structural Consistency  between Attribute and Graph Embedding Representations</h3>
<ul>
<li><strong>Authors: </strong>Jinlu Wang, Jipeng Guo, Yanfeng Sun, Junbin Gao, Shaofan Wang, Yachao Yang, Baocai Yin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15584">https://arxiv.org/abs/2401.15584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15584">https://arxiv.org/pdf/2401.15584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15584]] DGNN: Decoupled Graph Neural Networks with Structural Consistency  between Attribute and Graph Embedding Representations(https://arxiv.org/abs/2401.15584)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) demonstrate a robust capability for representation learning on graphs with complex structures, showcasing superior performance in various applications. The majority of existing GNNs employ a graph convolution operation by using both attribute and structure information through coupled learning. In essence, GNNs, from an optimization perspective, seek to learn a consensus and compromise embedding representation that balances attribute and graph information, selectively exploring and retaining valid information. To obtain a more comprehensive embedding representation of nodes, a novel GNNs framework, dubbed Decoupled Graph Neural Networks (DGNN), is introduced. DGNN explores distinctive embedding representations from the attribute and graph spaces by decoupled terms. Considering that semantic graph, constructed from attribute feature space, consists of different node connection information and provides enhancement for the topological graph, both topological and semantic graphs are combined for the embedding representation learning. Further, structural consistency among attribute embedding and graph embeddings is promoted to effectively remove redundant information and establish soft connection. This involves promoting factor sharing for adjacency reconstruction matrices, facilitating the exploration of a consensus and high-level correlation. Finally, a more powerful and complete representation is achieved through the concatenation of these embeddings. Experimental results conducted on several graph benchmark datasets verify its superiority in node classification task.</li>
<li><strong>摘要：</strong>图神经网络（GNN）展示了在具有复杂结构的图上进行表示学习的强大能力，在各种应用中展示了卓越的性能。大多数现有的 GNN 通过耦合学习使用属性和结构信息来采用图卷积运算。本质上，GNN 从优化的角度来看，寻求学习共识和折衷的嵌入表示，以平衡属性和图信息，有选择地探索和保留有效信息。为了获得更全面的节点嵌入表示，引入了一种新颖的 GNN 框架，称为解耦图神经网络（DGNN）。 DGNN 通过解耦项探索属性和图空间中独特的嵌入表示。考虑到语义图是从属性特征空间构建的，由不同的节点连接信息组成，并为拓扑图提供增强，因此将拓扑图和语义图结合起来进行嵌入表示学习。此外，促进属性嵌入和图嵌入之间的结构一致性，以有效去除冗余信息并建立软连接。这涉及促进邻接重建矩阵的因子共享，促进共识和高级相关性的探索。最后，通过这些嵌入的串联实现了更强大、更完整的表示。在多个图基准数据集上进行的实验结果验证了其在节点分类任务中的优越性。</li>
</ul>

<h3>Title: Evaluating Gender Bias in Large Language Models via Chain-of-Thought  Prompting</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15585">https://arxiv.org/abs/2401.15585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15585">https://arxiv.org/pdf/2401.15585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15585]] Evaluating Gender Bias in Large Language Models via Chain-of-Thought  Prompting(https://arxiv.org/abs/2401.15585)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>There exist both scalable tasks, like reading comprehension and fact-checking, where model performance improves with model size, and unscalable tasks, like arithmetic reasoning and symbolic reasoning, where model performance does not necessarily improve with model size. Large language models (LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate incremental predictions even on unscalable tasks. Unfortunately, despite their exceptional reasoning abilities, LLMs tend to internalize and reproduce discriminatory societal biases. Whether CoT can provide discriminatory or egalitarian rationalizations for the implicit information in unscalable tasks remains an open question. In this study, we examine the impact of LLMs' step-by-step predictions on gender bias in unscalable tasks. For this purpose, we construct a benchmark for an unscalable task where the LLM is given a list of words comprising feminine, masculine, and gendered occupational words, and is required to count the number of feminine and masculine words. In our CoT prompts, we require the LLM to explicitly indicate whether each word in the word list is a feminine or masculine before making the final predictions. With counting and handling the meaning of words, this benchmark has characteristics of both arithmetic reasoning and symbolic reasoning. Experimental results in English show that without step-by-step prediction, most LLMs make socially biased predictions, despite the task being as simple as counting words. Interestingly, CoT prompting reduces this unconscious social bias in LLMs and encourages fair predictions.</li>
<li><strong>摘要：</strong>既存在可扩展的任务，例如阅读理解和事实检查，其中模型性能随着模型大小而提高，也存在不可扩展的任务，例如算术推理和符号推理，其中模型性能不一定随着模型大小而提高。配备思想链 (CoT) 提示的大型语言模型 (LLM) 即使在不可扩展的任务上也能够做出准确的增量预测。不幸的是，尽管法学硕士具有非凡的推理能力，但他们往往会内化并再现歧视性的社会偏见。 CoT 是否可以为不可扩展任务中的隐含信息提供歧视性或平等主义的合理化仍然是一个悬而未决的问题。在这项研究中，我们研究了法学硕士的逐步预测对不可扩展任务中性别偏见的影响。为此，我们为不可扩展的任务构建了一个基准，其中法学硕士被给予包含女性、男性和性别职业单词的单词列表，并需要计算女性和男性单词的数量。在我们的 CoT 提示中，我们要求 LLM 在做出最终预测之前明确指出单词列表中的每个单词是阴性还是阳性。该基准通过计算和处理词义，兼具算术推理和符号推理的特点。英语实验结果表明，如果没有逐步预测，大多数法学硕士都会做出带有社会偏见的预测，尽管任务就像数单词一样简单。有趣的是，CoT 激励减少了法学硕士中这种无意识的社会偏见，并鼓励公平的预测。</li>
</ul>

<h3>Title: Neural Network-Based Score Estimation in Diffusion Models: Optimization  and Generalization</h3>
<ul>
<li><strong>Authors: </strong>Yinbin Han, Meisam Razaviyayn, Renyuan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15604">https://arxiv.org/abs/2401.15604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15604">https://arxiv.org/pdf/2401.15604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15604]] Neural Network-Based Score Estimation in Diffusion Models: Optimization  and Generalization(https://arxiv.org/abs/2401.15604)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing existing techniques from being applied directly. In this paper, we show that with a properly designed neural network architecture, the score function can be accurately approximated by a reproducing kernel Hilbert space induced by neural tangent kernels. Furthermore, by applying an early-stopping rule for gradient descent and leveraging certain coupling arguments between neural network training and kernel regression, we establish the first generalization error (sample complexity) bounds for learning the score function despite the presence of noise in the observations. Our analysis is grounded in a novel parametric form of the neural network and an innovative connection between score matching and regression analysis, facilitating the application of advanced statistical and optimization techniques.</li>
<li><strong>摘要：</strong>扩散模型已成为可与 GAN 相媲美的强大工具，可生成具有更高保真度、灵活性和鲁棒性的高质量样本。这些模型的关键组成部分是通过分数匹配来学习分数函数。尽管在各种任务上取得了经验上的成功，但目前尚不清楚基于梯度的算法是否能够以可证明的准确性学习得分函数。作为回答这个问题的第一步，本文建立了一个数学框架，用于使用梯度下降训练的神经网络来分析分数估计。我们的分析涵盖了学习过程的优化和泛化方面。特别是，我们提出了一种参数形式，将去噪分数匹配问题表示为带有噪声标签的回归。与标准的监督学习设置相比，分数匹配问题引入了明显的挑战，包括无界输入、向量值输出和额外的时间变量，从而阻碍了现有技术的直接应用。在本文中，我们表明，通过正确设计的神经网络架构，可以通过由神经切线核引起的再现核希尔伯特空间来准确地近似得分函数。此外，通过应用梯度下降的早期停止规则并利用神经网络训练和核回归之间的某些耦合参数，我们建立了用于学习得分函数的第一个泛化误差（样本复杂性）界限，尽管观察中存在噪声。我们的分析基于神经网络的新颖参数形式以及分数匹配和回归分析之间的创新连接，促进了先进统计和优化技术的应用。</li>
</ul>

<h3>Title: Addressing Noise and Efficiency Issues in Graph-Based Machine Learning  Models From the Perspective of Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Yongyu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15615">https://arxiv.org/abs/2401.15615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15615">https://arxiv.org/pdf/2401.15615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15615]] Addressing Noise and Efficiency Issues in Graph-Based Machine Learning  Models From the Perspective of Adversarial Attack(https://arxiv.org/abs/2401.15615)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Given that no existing graph construction method can generate a perfect graph for a given dataset, graph-based algorithms are invariably affected by the plethora of redundant and erroneous edges present within the constructed graphs. In this paper, we propose treating these noisy edges as adversarial attack and use a spectral adversarial robustness evaluation method to diminish the impact of noisy edges on the performance of graph algorithms. Our method identifies those points that are less vulnerable to noisy edges and leverages only these robust points to perform graph-based algorithms. Our experiments with spectral clustering, one of the most representative and widely utilized graph algorithms, reveal that our methodology not only substantially elevates the precision of the algorithm but also greatly accelerates its computational efficiency by leveraging only a select number of robust data points.</li>
<li><strong>摘要：</strong>鉴于现有的图构造方法无法为给定的数据集生成完美的图，基于图的算法总是受到构造图中存在的过多冗余和错误边的影响。在本文中，我们建议将这些噪声边缘视为对抗性攻击，并使用谱对抗鲁棒性评估方法来减少噪声边缘对图算法性能的影响。我们的方法识别那些不易受噪声边缘影响的点，并仅利用这些稳健的点来执行基于图的算法。我们对谱聚类（最具代表性和广泛使用的图算法之一）的实验表明，我们的方法不仅大大提高了算法的精度，而且通过仅利用选定数量的稳健数据点，大大提高了算法的计算效率。</li>
</ul>

<h3>Title: Diffusion-based graph generative methods</h3>
<ul>
<li><strong>Authors: </strong>Hongyang Chen, Can Xu, Lingyu Zheng, Qiang Zhang, Xuemin Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15617">https://arxiv.org/abs/2401.15617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15617">https://arxiv.org/pdf/2401.15617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15617]] Diffusion-based graph generative methods(https://arxiv.org/abs/2401.15617)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Being the most cutting-edge generative methods, diffusion methods have shown great advances in wide generation tasks. Among them, graph generation attracts significant research attention for its broad application in real life. In our survey, we systematically and comprehensively review on diffusion-based graph generative methods. We first make a review on three mainstream paradigms of diffusion methods, which are denoising diffusion probabilistic models, score-based genrative models, and stochastic differential equations. Then we further categorize and introduce the latest applications of diffusion models on graphs. In the end, we point out some limitations of current studies and future directions of future explorations. The summary of existing methods metioned in this survey is in https://github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.</li>
<li><strong>摘要：</strong>作为最前沿的生成方法，扩散方法在广泛的生成任务中显示出了巨大的进步。其中，图生成因其在现实生活中的广泛应用而引起了广泛的研究关注。在我们的调查中，我们系统、全面地回顾了基于扩散的图生成方法。我们首先回顾了扩散方法的三种主流范式，即去噪扩散概率模型、基于分数的生成模型和随机微分方程。然后我们进一步分类并介绍扩散模型在图上的最新应用。最后，我们指出了当前研究的一些局限性和未来探索的方向。本次调查中提到的现有方法的总结位于 https://github.com/zhejianzhuque/Diffusion-based-Graph-Generative-Methods。</li>
</ul>

<h3>Title: SNAP: Semantic Stories for Next Activity Prediction</h3>
<ul>
<li><strong>Authors: </strong>Alon Oved, Segev Shlomov, Sergey Zeltyn, Nir Mashkif, Avi Yaeli</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15621">https://arxiv.org/abs/2401.15621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15621">https://arxiv.org/pdf/2401.15621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15621]] SNAP: Semantic Stories for Next Activity Prediction(https://arxiv.org/abs/2401.15621)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Predicting the next activity in an ongoing process is one of the most common classification tasks in the business process management (BPM) domain. It allows businesses to optimize resource allocation, enhance operational efficiency, and aids in risk mitigation and strategic decision-making. This provides a competitive edge in the rapidly evolving confluence of BPM and AI. Existing state-of-the-art AI models for business process prediction do not fully capitalize on available semantic information within process event logs. As current advanced AI-BPM systems provide semantically-richer textual data, the need for novel adequate models grows. To address this gap, we propose the novel SNAP method that leverages language foundation models by constructing semantic contextual stories from the process historical event logs and using them for the next activity prediction. We compared the SNAP algorithm with nine state-of-the-art models on six benchmark datasets and show that SNAP significantly outperforms them, especially for datasets with high levels of semantic content.</li>
<li><strong>摘要：</strong>预测正在进行的流程中的下一个活动是业务流程管理 (BPM) 领域中最常见的分类任务之一。它使企业能够优化资源配置，提高运营效率，并有助于缓解风险和战略决策。这在 BPM 和 AI 快速发展的融合中提供了竞争优势。现有最先进的业务流程预测人工智能模型并未充分利用流程事件日志中的可用语义信息。随着当前先进的 AI-BPM 系统提供语义更丰富的文本数据，对新颖的适当模型的需求不断增长。为了解决这一差距，我们提出了新颖的 SNAP 方法，该方法利用语言基础模型，通过从流程历史事件日志构建语义上下文故事并将其用于下一个活动预测。我们将 SNAP 算法与六个基准数据集上的九个最先进模型进行了比较，结果表明 SNAP 明显优于它们，特别是对于具有高水平语义内容的数据集。</li>
</ul>

<h3>Title: TA&AT: Enhancing Task-Oriented Dialog with Turn-Level Auxiliary Tasks  and Action-Tree Based Scheduled Sampling</h3>
<ul>
<li><strong>Authors: </strong>Longxiang Liu, Xiuxing Li, Yang Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15626">https://arxiv.org/abs/2401.15626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15626">https://arxiv.org/pdf/2401.15626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15626]] TA&AT: Enhancing Task-Oriented Dialog with Turn-Level Auxiliary Tasks  and Action-Tree Based Scheduled Sampling(https://arxiv.org/abs/2401.15626)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Task-oriented dialog systems have witnessed substantial progress due to conversational pre-training techniques. Yet, two significant challenges persist. First, most systems primarily utilize the latest turn's state label for the generator. This practice overlooks the comprehensive value of state labels in boosting the model's understanding for future generations. Second, an overreliance on generated policy often leads to error accumulation, resulting in suboptimal responses when adhering to incorrect actions. To combat these challenges, we propose turn-level multi-task objectives for the encoder. With the guidance of essential information from labeled intermediate states, we establish a more robust representation for both understanding and generation. For the decoder, we introduce an action tree-based scheduled sampling technique. Specifically, we model the hierarchical policy as trees and utilize the similarity between trees to sample negative policy based on scheduled sampling, hoping the model to generate invariant responses under perturbations. This method simulates potential pitfalls by sampling similar negative policy, bridging the gap between task-oriented dialog training and inference. Among methods without continual pre-training, our approach achieved state-of-the-art (SOTA) performance on the MultiWOZ dataset series and was also competitive with pre-trained SOTA methods.</li>
<li><strong>摘要：</strong>由于对话预训练技术，面向任务的对话系统已经取得了实质性进展。然而，仍然存在两个重大挑战。首先，大多数系统主要利用发电机的最新回合的状态标签。这种做法忽视了国家标签在促进后代对模型的理解方面的综合价值。其次，过度依赖生成的策略通常会导致错误积累，从而在坚持不正确的操作时导致次优响应。为了应对这些挑战，我们为编码器提出了回合级多任务目标。在标记中间状态的基本信息的指导下，我们为理解和生成建立了更强大的表示。对于解码器，我们引入了基于动作树的调度采样技术。具体来说，我们将分层策略建模为树，并利用树之间的相似性根据计划采样对负策略进行采样，希望模型在扰动下产生不变的响应。该方法通过对类似的负面策略进行采样来模拟潜在的陷阱，弥合面向任务的对话训练和推理之间的差距。在没有持续预训练的方法中，我们的方法在 MultiWOZ 数据集系列上实现了最先进的 (SOTA) 性能，并且与预训练的 SOTA 方法也具有竞争力。</li>
</ul>

<h3>Title: LLsM: Generative Linguistic Steganography with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yihao Wang, Ruiqi Song, Ru Zhang, Jianyi Liu, Lingxiao Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15656">https://arxiv.org/abs/2401.15656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15656">https://arxiv.org/pdf/2401.15656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15656]] LLsM: Generative Linguistic Steganography with Large Language Model(https://arxiv.org/abs/2401.15656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Linguistic Steganography (LS) tasks aim to generate steganographic texts (stego) based on secret information. Only authorized recipients can perceive the existence of secret information in the texts and accurately extract it, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the generated stego is difficult to contain specific discourse characteristics such as style, genre, and theme. As a result, the stego are often easily detectable, compromising covert communication. To address these problems, this paper proposes a novel scheme named LLsM, a generative LS based on a Large Language Model (LLM). We fine-tuned the LLM LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse characteristics are used as guiding information and inputted into the fine-tuned LLM in the form of Prompt together with secret information. The candidate pool, derived from sampling and truncation, undergoes range encoding to ensure the stego imitate natural text distribution. Experiments demonstrate that LLsM performs superior to prevalent baselines regarding text quality, statistical analysis, discourse matching, and anti-steganalysis. In particular, LLsM's MAUVE surpasses that of some baselines by 70%-80%, and its anti-steganalysis performance is 30%-40% higher. Notably, we also present the long stego generated by LLsM, showing its potential superiority in long LS tasks.</li>
<li><strong>摘要：</strong>语言隐写术（LS）任务旨在基于秘密信息生成隐写文本（stego）。只有经过授权的接收者才能察觉到文本中秘密信息的存在并准确地提取出来，从而保护隐私。然而，现有方案生成的隐写体可控性差，生成的隐写体难以包含特定的语体、体裁、主题等话语特征。因此，隐写通常很容易被检测到，从而损害秘密通信。为了解决这些问题，本文提出了一种名为 LLsM 的新方案，即基于大语言模型（LLM）的生成 LS。我们使用包含丰富话语特征的大规模构建数据集对LLM LLaMA2进行了微调，这使得微调后的LLM能够以可控的方式生成具有特定话语的文本。然后将话语特征作为引导信息，与秘密信息一起以Prompt的形式输入到微调后的LLM中。通过采样和截断得出的候选池经过范围编码，以确保隐写模仿自然文本分布。实验表明，LLsM 在文本质量、统计分析、话语匹配和反隐写分析方面的表现优于流行的基线。特别是，LLsM的MAUVE超越了一些基线70%-80%，其抗隐写分析性能高出30%-40%。值得注意的是，我们还展示了 LLsM 生成的长隐写，显示了其在长 LS 任务中的潜在优势。</li>
</ul>

<h3>Title: YODA: Teacher-Student Progressive Learning for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianqiao Lu, Wanjun Zhong, Yufei Wang, Zhijiang Guo, Qi Zhu, Wenyong Huang, Yanlin Wang, Fei Mi, Baojun Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15670">https://arxiv.org/abs/2401.15670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15670">https://arxiv.org/pdf/2401.15670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15670]] YODA: Teacher-Student Progressive Learning for Language Models(https://arxiv.org/abs/2401.15670)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have demonstrated adeptness in a range of tasks, they still lag behind human learning efficiency. This disparity is often linked to the inherent human capacity to learn from basic examples, gradually generalize and handle more complex problems, and refine their skills with continuous feedback. Inspired by this, this paper introduces YODA, a novel teacher-student progressive learning framework that emulates the teacher-student education process to improve the efficacy of model fine-tuning. The framework operates on an interactive \textit{basic-generalized-harder} loop. The teacher agent provides tailored feedback on the student's answers, and systematically organizes the education process. This process unfolds by teaching the student basic examples, reinforcing understanding through generalized questions, and then enhancing learning by posing questions with progressively enhanced complexity. With the teacher's guidance, the student learns to iteratively refine its answer with feedback, and forms a robust and comprehensive understanding of the posed questions. The systematic procedural data, which reflects the progressive learning process of humans, is then utilized for model training. Taking math reasoning as a testbed, experiments show that training LLaMA2 with data from YODA improves SFT with significant performance gain (+17.01\% on GSM8K and +9.98\% on MATH). In addition, we find that training with curriculum learning further improves learning robustness.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）已经表现出对一系列任务的熟练程度，但它们仍然落后于人类的学习效率。这种差异通常与人类固有的从基本例子中学习、逐渐概括和处理更复杂的问题以及通过持续反馈来完善技能的能力有关。受此启发，本文介绍了YODA，一种新颖的师生渐进式学习框架，通过模拟师生教育过程来提高模型微调的功效。该框架在交互式 \textit{basic-generalized-harder} 循环上运行。教师代理针对学生的答案提供量身定制的反馈，并系统地组织教育过程。这个过程通过教授学生基本示例、通过一般性问题加强理解、然后通过提出逐渐增强复杂性的问题来加强学习。在教师的指导下，学生学会通过反馈迭代地完善其答案，并对所提出的问题形成牢固而全面的理解。反映人类渐进学习过程的系统程序数据然后用于模型训练。以数学推理为测试平台，实验表明，使用 YODA 的数据训练 LLaMA2 可以显着提高 SFT 性能（GSM8K 上 +17.01\%，MATH 上 +9.98\%）。此外，我们发现课程学习的培训进一步提高了学习的稳健性。</li>
</ul>

<h3>Title: One for all: A novel Dual-space Co-training baseline for Large-scale  Multi-View Clustering</h3>
<ul>
<li><strong>Authors: </strong>Zisen Kong, Zhiqiang Fu, Dongxia Chang, Yiming Wang, Yao Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15691">https://arxiv.org/abs/2401.15691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15691">https://arxiv.org/pdf/2401.15691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15691]] One for all: A novel Dual-space Co-training baseline for Large-scale  Multi-View Clustering(https://arxiv.org/abs/2401.15691)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel multi-view clustering model, named Dual-space Co-training Large-scale Multi-view Clustering (DSCMC). The main objective of our approach is to enhance the clustering performance by leveraging co-training in two distinct spaces. In the original space, we learn a projection matrix to obtain latent consistent anchor graphs from different views. This process involves capturing the inherent relationships and structures between data points within each view. Concurrently, we employ a feature transformation matrix to map samples from various views to a shared latent space. This transformation facilitates the alignment of information from multiple views, enabling a comprehensive understanding of the underlying data distribution. We jointly optimize the construction of the latent consistent anchor graph and the feature transformation to generate a discriminative anchor graph. This anchor graph effectively captures the essential characteristics of the multi-view data and serves as a reliable basis for subsequent clustering analysis. Moreover, the element-wise method is proposed to avoid the impact of diverse information between different views. Our algorithm has an approximate linear computational complexity, which guarantees its successful application on large-scale datasets. Through experimental validation, we demonstrate that our method significantly reduces computational complexity while yielding superior clustering performance compared to existing approaches.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种新颖的多视图聚类模型，称为双空间协同训练大规模多视图聚类（DSCMC）。我们方法的主要目标是通过利用两个不同空间中的协同训练来提高聚类性能。在原始空间中，我们学习投影矩阵以从不同视图获得潜在一致的锚图。此过程涉及捕获每个视图内数据点之间的固有关系和结构。同时，我们采用特征转换矩阵将来自不同视图的样本映射到共享的潜在空间。这种转换有助于协调来自多个视图的信息，从而能够全面了解底层数据分布。我们联合优化潜在一致锚图的构造和特征转换以生成判别性锚图。该锚图有效地捕捉了多视图数据的本质特征，为后续的聚类分析提供了可靠的基础。此外，提出了element-wise方法来避免不同视图之间不同信息的影响。我们的算法具有近似线性的计算复杂度，这保证了其在大规模数据集上的成功应用。通过实验验证，我们证明我们的方法显着降低了计算复杂性，同时与现有方法相比，产生了卓越的聚类性能。</li>
</ul>

<h3>Title: RE-GAINS & EnCHANT: Intelligent Tool Manipulation Systems For Enhanced  Query Responses</h3>
<ul>
<li><strong>Authors: </strong>Sahil Girhepuje, Siva Sankar Sajeev, Purvam Jain, Arya Sikder, Adithya Rama Varma, Ryan George, Akshay Govind Srinivasan, Mahendra Kurup, Ashmit Sinha, Sudip Mondal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15724">https://arxiv.org/abs/2401.15724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15724">https://arxiv.org/pdf/2401.15724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15724]] RE-GAINS & EnCHANT: Intelligent Tool Manipulation Systems For Enhanced  Query Responses(https://arxiv.org/abs/2401.15724)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of LLMs, they still suffer from tool invocation and tool chaining due to inadequate input queries and/or tool argument descriptions. We propose two novel frameworks, RE-GAINS and EnCHANT, enabling LLMs to tackle tool manipulation for solving complex user queries by making API calls. EnCHANT is an open-source solution that makes use of an LLM format enforcer, an LLM(OpenChat 3.5) and a retriever(ToolBench's API Retriever). RE-GAINS is based on OpenAI models and embeddings using a special prompt based on the RAP paper. Both solutions cost less than $0.01 per query with minimal latency, therefore showcasing the usefulness of the frameworks.</li>
<li><strong>摘要：</strong>尽管法学硕士取得了显着的成功，但由于输入查询和/或工具参数描述不充分，它们仍然受到工具调用和工具链的困扰。我们提出了两个新颖的框架：RE-GAINS 和 EnCHANT，使法学硕士能够处理工具操作，通过 API 调用来解决复杂的用户查询。 EnCHANT 是一个开源解决方案，它利用了 LLM 格式强制执行器、LLM(OpenChat 3.5) 和检索器（ToolBench 的 API Retriever）。 RE-GAINS 基于 OpenAI 模型和嵌入，使用基于 RAP 论文的特殊提示。这两种解决方案的每次查询成本均低于 0.01 美元，延迟最小，因此展示了框架的实用性。</li>
</ul>

<h3>Title: Fine-Tuned Large Language Models for Symptom Recognition from Spanish  Clinical Text</h3>
<ul>
<li><strong>Authors: </strong>Mai A. Shaaban, Abbas Akkasi, Adnan Khan, Majid Komeili, Mohammad Yaqub</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15780">https://arxiv.org/abs/2401.15780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15780">https://arxiv.org/pdf/2401.15780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15780]] Fine-Tuned Large Language Models for Symptom Recognition from Spanish  Clinical Text(https://arxiv.org/abs/2401.15780)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The accurate recognition of symptoms in clinical reports is significantly important in the fields of healthcare and biomedical natural language processing. These entities serve as essential building blocks for clinical information extraction, enabling retrieval of critical medical insights from vast amounts of textual data. Furthermore, the ability to identify and categorize these entities is fundamental for developing advanced clinical decision support systems, aiding healthcare professionals in diagnosis and treatment planning. In this study, we participated in SympTEMIST, a shared task on the detection of symptoms, signs and findings in Spanish medical documents. We combine a set of large language models fine-tuned with the data released by the organizers.</li>
<li><strong>摘要：</strong>临床报告中症状的准确识别在医疗保健和生物医学自然语言处理领域非常重要。这些实体作为临床信息提取的重要构建块，能够从大量文本数据中检索关键的医学见解。此外，识别和分类这些实体的能力对于开发先进的临床决策支持系统、帮助医疗保健专业人员进行诊断和治疗规划至关重要。在这项研究中，我们参加了 SympTEMIST，这是一项关于检测西班牙医疗文件中的症状、体征和发现的共享任务。我们将一组大型语言模型与组织者发布的数据进行了微调。</li>
</ul>

<h3>Title: UnMASKed: Quantifying Gender Biases in Masked Language Models through  Linguistically Informed Job Market Prompts</h3>
<ul>
<li><strong>Authors: </strong>Iñigo Parra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15798">https://arxiv.org/abs/2401.15798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15798">https://arxiv.org/pdf/2401.15798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15798]] UnMASKed: Quantifying Gender Biases in Masked Language Models through  Linguistically Informed Job Market Prompts(https://arxiv.org/abs/2401.15798)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have become pivotal in the realm of technological advancements. While their capabilities are vast and transformative, they often include societal biases encoded in the human-produced datasets used for their training. This research delves into the inherent biases present in masked language models (MLMs), with a specific focus on gender biases. This study evaluated six prominent models: BERT, RoBERTa, DistilBERT, BERT-multilingual, XLM-RoBERTa, and DistilBERT-multilingual. The methodology employed a novel dataset, bifurcated into two subsets: one containing prompts that encouraged models to generate subject pronouns in English, and the other requiring models to return the probabilities of verbs, adverbs, and adjectives linked to the prompts' gender pronouns. The analysis reveals stereotypical gender alignment of all models, with multilingual variants showing comparatively reduced biases.</li>
<li><strong>摘要：</strong>语言模型（LM）已成为技术进步领域的关键。虽然它们的能力巨大且具有变革性，但它们通常包含在用于训练的人类生成的数据集中编码的社会偏见。这项研究深入探讨了掩码语言模型 (MLM) 中存在的固有偏见，特别关注性别偏见。这项研究评估了六种著名模型：BERT、RoBERTa、DistilBERT、BERT-多语言、XLM-RoBERTa 和 DistilBERT-多语言。该方法采用了一个新颖的数据集，分为两个子集：一个包含鼓励模型生成英语主语代词的提示，另一个要求模型返回与提示的性别代词相关的动词、副词和形容词的概率。分析揭示了所有模型的刻板性别一致性，多语言变体显示出相对较少的偏见。</li>
</ul>

<h3>Title: OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for  Medication Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Weicong Tan, Weiqing Wang, Xin Zhou, Wray Buntine, Gordon Bingham</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15814">https://arxiv.org/abs/2401.15814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15814">https://arxiv.org/pdf/2401.15814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15814]] OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for  Medication Recommendation(https://arxiv.org/abs/2401.15814)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Most existing medication recommendation models learn representations for medical concepts based on electronic health records (EHRs) and make recommendations with learnt representations. However, most medications appear in the dataset for limited times, resulting in insufficient learning of their representations. Medical ontologies are the hierarchical classification systems for medical terms where similar terms are in the same class on a certain level. In this paper, we propose OntoMedRec, the logically-pretrained and model-agnostic medical Ontology Encoders for Medication Recommendation that addresses data sparsity problem with medical ontologies. We conduct comprehensive experiments on benchmark datasets to evaluate the effectiveness of OntoMedRec, and the result shows the integration of OntoMedRec improves the performance of various models in both the entire EHR datasets and the admissions with few-shot medications. We provide the GitHub repository for the source code on https://anonymous.4open.science/r/OntoMedRec-D123</li>
<li><strong>摘要：</strong>大多数现有的药物推荐模型都会根据电子健康记录（EHR）学习医学概念的表示，并利用学习到的表示来提出建议。然而，大多数药物在数据集中出现的时间有限，导致对其表示的学习不充分。医学本体是医学术语的分层分类系统，其中相似的术语在一定级别上属于同一类。在本文中，我们提出了 OntoMedRec，一种用于药物推荐的逻辑预训练且与模型无关的医学本体编码器，可解决医学本体的数据稀疏问题。我们对基准数据集进行了全面的实验来评估 OntoMedRec 的有效性，结果表明 OntoMedRec 的集成提高了各种模型在整个 EHR 数据集和少量药物入院的性能。我们在 https://anonymous.4open.science/r/OntoMedRec-D123 上提供源代码的 GitHub 存储库</li>
</ul>

<h3>Title: Look Around! Unexpected gains from training on environments in the  vicinity of the target</h3>
<ul>
<li><strong>Authors: </strong>Serena Bono, Spandan Madan, Ishaan Grover, Mao Yasueda, Cynthia Breazeal, Hanspeter Pfister, Gabriel Kreiman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15856">https://arxiv.org/abs/2401.15856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15856">https://arxiv.org/pdf/2401.15856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15856]] Look Around! Unexpected gains from training on environments in the  vicinity of the target(https://arxiv.org/abs/2401.15856)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Solutions to Markov Decision Processes (MDP) are often very sensitive to state transition probabilities. As the estimation of these probabilities is often inaccurate in practice, it is important to understand when and how Reinforcement Learning (RL) agents generalize when transition probabilities change. Here we present a new methodology to evaluate such generalization of RL agents under small shifts in the transition probabilities. Specifically, we evaluate agents in new environments (MDPs) in the vicinity of the training MDP created by adding quantifiable, parametric noise into the transition function of the training MDP. We refer to this process as Noise Injection, and the resulting environments as $\delta$-environments. This process allows us to create controlled variations of the same environment with the level of the noise serving as a metric of distance between environments. Conventional wisdom suggests that training and testing on the same MDP should yield the best results. However, we report several cases of the opposite -- when targeting a specific environment, training the agent in an alternative noise setting can yield superior outcomes. We showcase this phenomenon across $60$ different variations of ATARI games, including PacMan, Pong, and Breakout.</li>
<li><strong>摘要：</strong>马尔可夫决策过程 (MDP) 的解决方案通常对状态转换概率非常敏感。由于这些概率的估计在实践中通常不准确，因此了解强化学习 (RL) 代理在转移概率发生变化时何时以及如何进行泛化非常重要。在这里，我们提出了一种新的方法来评估 RL 智能体在转移概率小幅变化下的泛化能力。具体来说，我们通过将可量化的参数噪声添加到训练 MDP 的转换函数中来创建训练 MDP 附近的新环境 (MDP) 中的代理。我们将此过程称为噪声注入，并将生成的环境称为$\delta$-环境。这个过程使我们能够创建同一环境的受控变化，并将噪声水平作为环境之间距离的度量。传统观点认为，在同一个 MDP 上进行训练和测试应该会产生最佳结果。然而，我们报告了几个相反的案例——当针对特定环境时，在替代噪声环境中训练代理可以产生更好的结果。我们在 60 美元不同版本的 ATARI 游戏中展示了这一现象，包括 PacMan、Pong 和 Breakout。</li>
</ul>

<h3>Title: DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in  BERT pretraining</h3>
<ul>
<li><strong>Authors: </strong>Wen Liang, Youzhi Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15861">https://arxiv.org/abs/2401.15861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15861">https://arxiv.org/pdf/2401.15861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15861]] DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in  BERT pretraining(https://arxiv.org/abs/2401.15861)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce DrBERT (Decoder-refined BERT), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language Understanding (NLU) tasks. In our approach, we utilize the original BERT model as the encoder, making only changes to the decoder without altering the encoder. This approach does not necessitate extensive modifications to the model's architecture and can be seamlessly integrated into existing fine-tuning pipelines and services, offering an efficient and effective enhancement strategy. Compared to other methods, while we also incur a moderate training cost for the decoder during the pretraining process, our approach does not introduce additional training costs during the fine-tuning phase. We test multiple enhanced decoder structures after pretraining and evaluate their performance on the GLUE benchmark. Our results demonstrate that DrBERT, having only undergone subtle refinements to the model structure during pretraining, significantly enhances model performance without escalating the inference time and serving budget.</li>
<li><strong>摘要：</strong>BERT（来自 Transformers 的双向编码器表示）通过其在众多任务中的卓越性能彻底改变了自然语言处理领域。然而，大多数研究人员主要集中在与模型结构相关的增强上，例如相对位置嵌入和更有效的注意机制。其他人则深入研究了与掩码语言模型相关的预训练技巧，包括全词掩码。 DeBERTa 引入了适用于 BERT 编码器模型的增强型解码器进行预训练，事实证明非常有效。我们认为，围绕增强型掩码语言建模解码器的设计和研究一直被低估。在本文中，我们提出了几种增强解码器的设计，并介绍了 DrBERT（Decoder-refined BERT），这是一种新的建模训练方法。通常，预训练的 BERT 模型会针对特定的自然语言理解 (NLU) 任务进行微调。在我们的方法中，我们利用原始的 BERT 模型作为编码器，仅对解码器进行更改，而不更改编码器。这种方法不需要对模型的架构进行大量修改，并且可以无缝集成到现有的微调管道和服务中，从而提供高效且有效的增强策略。与其他方法相比，虽然我们在预训练过程中也会为解码器带来适度的训练成本，但我们的方法在微调阶段不会引入额外的训练成本。我们在预训练后测试了多个增强的解码器结构，并在 GLUE 基准上评估它们的性能。我们的结果表明，DrBERT 仅在预训练期间对模型结构进行了细微的改进，就显着提高了模型性能，而无需增加推理时间和服务预算。</li>
</ul>

<h3>Title: lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold  Gap</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Hsien Tsai, Yun-Da Tsai, Shou-De Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15879">https://arxiv.org/abs/2401.15879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15879">https://arxiv.org/pdf/2401.15879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15879]] lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold  Gap(https://arxiv.org/abs/2401.15879)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Good arm identification (GAI) is a pure-exploration bandit problem in which a single learner outputs an arm as soon as it is identified as a good arm. A good arm is defined as an arm with an expected reward greater than or equal to a given threshold. This paper focuses on the GAI problem under a small threshold gap, which refers to the distance between the expected rewards of arms and the given threshold. We propose a new algorithm called lil'HDoC to significantly improve the total sample complexity of the HDoC algorithm. We demonstrate that the sample complexity of the first $\lambda$ output arm in lil'HDoC is bounded by the original HDoC algorithm, except for one negligible term, when the distance between the expected reward and threshold is small. Extensive experiments confirm that our algorithm outperforms the state-of-the-art algorithms in both synthetic and real-world datasets.</li>
<li><strong>摘要：</strong>好手臂识别（GAI）是一个纯粹的探索老虎机问题，其中单个学习者一旦被识别为好手臂就输出该手臂。好的手臂被定义为预期奖励大于或等于给定阈值的手臂。本文主要研究小阈值差距下的 GAI 问题，小阈值差距是指手臂的预期奖励与给定阈值之间的距离。我们提出了一种名为 lil'HDoC 的新算法，以显着提高 HDoC 算法的总样本复杂度。我们证明，当预期奖励和阈值之间的距离很小时，lil'HDoC 中第一个 $\lambda$ 输出臂的样本复杂度受原始 HDoC 算法的限制，除了一个可忽略不计的项。大量的实验证实，我们的算法在合成数据集和现实数据集中都优于最先进的算法。</li>
</ul>

<h3>Title: Corrective Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15884">https://arxiv.org/abs/2401.15884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15884">https://arxiv.org/pdf/2401.15884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15884]] Corrective Retrieval Augmented Generation(https://arxiv.org/abs/2401.15884)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval augmented generation, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）不可避免地会出现幻觉，因为生成文本的准确性不能仅通过它们封装的参数知识来保证。尽管检索增强生成（RAG）是法学硕士的实用补充，但它在很大程度上依赖于检索到的文档的相关性，这引发了人们对检索出错时模型如何表现的担忧。为此，我们提出了纠正检索增强生成（CRAG）来提高生成的鲁棒性。具体来说，轻量级检索评估器被设计为评估查询检索到的文档的整体质量，返回一个置信度，基于该置信度可以触发不同的知识检索动作。由于从静态和有限的语料库中检索只能返回次优文档，因此大规模网络搜索被用作增强检索结果的扩展。此外，针对检索到的文档设计了分解然后重组算法，有选择地关注关键信息并过滤掉其中的不相关信息。 CRAG 是即插即用的，可以与各种基于 RAG 的方法无缝耦合。在涵盖短格式和长格式生成任务的四个数据集上进行的实验表明，CRAG 可以显着提高基于 RAG 的方法的性能。</li>
</ul>

<h3>Title: A Gated MLP Architecture for Learning Topological Dependencies in  Spatio-Temporal Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yun Young Choi, Minho Lee, Sun Woo Park, Seunghwan Lee, Joohwan Ko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15894">https://arxiv.org/abs/2401.15894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15894">https://arxiv.org/pdf/2401.15894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15894]] A Gated MLP Architecture for Learning Topological Dependencies in  Spatio-Temporal Graphs(https://arxiv.org/abs/2401.15894)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) and Transformer have been increasingly adopted to learn the complex vector representations of spatio-temporal graphs, capturing intricate spatio-temporal dependencies crucial for applications such as traffic datasets. Although many existing methods utilize multi-head attention mechanisms and message-passing neural networks (MPNNs) to capture both spatial and temporal relations, these approaches encode temporal and spatial relations independently, and reflect the graph's topological characteristics in a limited manner. In this work, we introduce the Cycle to Mixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivial invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP). The Cy2Mixer is composed of three blocks based on MLPs: A message-passing block for encapsulating spatial information, a cycle message-passing block for enriching topological information through cyclic subgraphs, and a temporal block for capturing temporal properties. We bolster the effectiveness of Cy2Mixer with mathematical evidence emphasizing that our cycle message-passing block is capable of offering differentiated information to the deep learning model compared to the message-passing block. Furthermore, empirical evaluations substantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-art performances across various traffic benchmark datasets.</li>
<li><strong>摘要：</strong>图神经网络 (GNN) 和 Transformer 越来越多地被用来学习时空图的复杂向量表示，捕获对于流量数据集等应用至关重要的复杂时空依赖性。尽管许多现有方法利用多头注意力机制和消息传递神经网络（MPNN）来捕获空间和时间关系，但这些方法独立地编码时间和空间关系，并以有限的方式反映图的拓扑特征。在这项工作中，我们介绍了 Cycle to Mixer (Cy2Mixer)，这是一种新颖的时空 GNN，基于带有门控多层感知器 (gMLP) 的时空图的拓扑非平凡不变量。 Cy2Mixer 由三个基于 MLP 的块组成：用于封装空间信息的消息传递块、用于通过循环子图丰富拓扑信息的循环消息传递块以及用于捕获时间属性的时间块。我们通过数学证据增强了 Cy2Mixer 的有效性，强调与消息传递块相比，我们的循环消息传递块能够为深度学习模型提供差异化​​信息。此外，实证评估证实了 Cy2Mixer 的功效，在各种流量基准数据集上展示了最先进的性能。</li>
</ul>

<h3>Title: E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15927">https://arxiv.org/abs/2401.15927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15927">https://arxiv.org/pdf/2401.15927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15927]] E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for  Large Language Models(https://arxiv.org/abs/2401.15927)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the accelerating development of Large Language Models (LLMs), many LLMs are beginning to be used in the Chinese K-12 education domain. The integration of LLMs and education is getting closer and closer, however, there is currently no benchmark for evaluating LLMs that focuses on the Chinese K-12 education domain. Therefore, there is an urgent need for a comprehensive natural language processing benchmark to accurately assess the capabilities of various LLMs in the Chinese K-12 education domain. To address this, we introduce the E-EVAL, the first comprehensive evaluation benchmark specifically designed for the Chinese K-12 education field. The E-EVAL consists of 4,351 multiple-choice questions at the primary, middle, and high school levels across a wide range of subjects, including Chinese, English, Politics, History, Ethics, Physics, Chemistry, Mathematics, and Geography. We conducted a comprehensive evaluation of E-EVAL on advanced LLMs, including both English-dominant and Chinese-dominant models. Findings show that Chinese-dominant models perform well compared to English-dominant models, with many scoring even above the GPT 4.0. However, almost all models perform poorly in complex subjects such as mathematics. We also found that most Chinese-dominant LLMs did not achieve higher scores at the primary school level compared to the middle school level. We observe that the mastery of higher-order knowledge by the model does not necessarily imply the mastery of lower-order knowledge as well. Additionally, the experimental results indicate that the Chain of Thought (CoT) technique is effective only for the challenging science subjects, while Few-shot prompting is more beneficial for liberal arts subjects. With E-EVAL, we aim to analyze the strengths and limitations of LLMs in educational applications, and to contribute to the progress and development of Chinese K-12 education and LLMs.</li>
<li><strong>摘要：</strong>随着大语言模型（LLM）的加速发展，许多LLM开始应用于中国K-12教育领域。 LLM与教育的融合越来越紧密，但目前尚无针对中国K-12教育领域的LLM评估基准。因此，迫切需要一个全面的自然语言处理基准来准确评估中国K-12教育领域各法学硕士的能力。针对这个问题，我们推出了E-EVAL，这是第一个专门针对中国K-12教育领域设计的综合评估基准。 E-EVAL 包含 4,351 道小学、初中和高中级别的多项选择题，涵盖语文、英语、政治、历史、伦理、物理、化学、数学和地理等广泛学科。我们对高级法学硕士的E-EVAL进行了全面评估，包括以英语为主的模式和以中文为主的模式。调查结果显示，与以英语为主的模型相比，以中文为主的模型表现良好，许多得分甚至高于 GPT 4.0。然而，几乎所有模型在数学等复杂学科中都表现不佳。我们还发现，与中学相比，大多数以华人为主的法学硕士在小学阶段的成绩并没有更高。我们观察到，模型对高阶知识的掌握并不一定意味着对低阶知识的掌握。此外，实验结果表明，思想链（CoT）技术仅对具有挑战性的理科科目有效，而少镜头提示对文科科目更有益。通过E-EVAL，我们旨在分析法学硕士在教育应用中的优势和局限性，为中国K-12教育和法学硕士的进步和发展做出贡献。</li>
</ul>

<h3>Title: Self-Supervised Learning in Event Sequences: A Comparative Study and  Hybrid Approach of Generative Modeling and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Viktor Moskvoretskii, Dmitry Osin, Egor Shvetsov, Igor Udovichenko, Maxim Zhelnin, Andrey Dukhovny, Anna Zhimerikina, Albert Efimov, Evgeny Burnaev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15935">https://arxiv.org/abs/2401.15935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15935">https://arxiv.org/pdf/2401.15935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15935]] Self-Supervised Learning in Event Sequences: A Comparative Study and  Hybrid Approach of Generative Modeling and Contrastive Learning(https://arxiv.org/abs/2401.15935)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare. We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research. Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-supervised methods consistently outperform the supervised approach on our datasets.</li>
<li><strong>摘要：</strong>本研究调查了自我监督学习技术以获得事件序列的表示。它是各种应用中的关键模式，包括但不限于银行、电子商务和医疗保健。我们对自我监督学习中的生成方法和对比方法进行了全面的研究，并独立应用它们。我们发现没有单一的至高无上的方法。因此，我们探索结合这些方法的潜在好处。为了实现这一目标，我们引入了一种新颖的方法，将生成嵌入和对比嵌入作为不同的模式，从当代多模态研究中汲取灵感。生成方法和对比方法通常被视为相互排斥的，从而为它们的组合探索留下了空白。我们的结果表明，这种对齐模型的性能至少与现有方法相当，并且大部分超过了现有方法，并且在各种任务中更加通用。此外，我们证明自监督方法在我们的数据集上始终优于监督方法。</li>
</ul>

<h3>Title: A Class-aware Optimal Transport Approach with Higher-Order Moment  Matching for Unsupervised Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Tuan Nguyen, Van Nguyen, Trung Le, He Zhao, Quan Hung Tran, Dinh Phung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15952">https://arxiv.org/abs/2401.15952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15952">https://arxiv.org/pdf/2401.15952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15952]] A Class-aware Optimal Transport Approach with Higher-Order Moment  Matching for Unsupervised Domain Adaptation(https://arxiv.org/abs/2401.15952)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. In this paper, we introduce a novel approach called class-aware optimal transport (OT), which measures the OT distance between a distribution over the source class-conditional distributions and a mixture of source and target data distribution. Our class-aware OT leverages a cost function that determines the matching extent between a given data example and a source class-conditional distribution. By optimizing this cost function, we find the optimal matching between target examples and source class-conditional distributions, effectively addressing the data and label shifts that occur between the two domains. To handle the class-aware OT efficiently, we propose an amortization solution that employs deep neural networks to formulate the transportation probabilities and the cost function. Additionally, we propose minimizing class-aware Higher-order Moment Matching (HMM) to align the corresponding class regions on the source and target domains. The class-aware HMM component offers an economical computational approach for accurately evaluating the HMM distance between the two distributions. Extensive experiments on benchmark datasets demonstrate that our proposed method significantly outperforms existing state-of-the-art baselines.</li>
<li><strong>摘要：</strong>无监督域适应（UDA）旨在将知识从标记的源域转移到未标记的目标域。在本文中，我们介绍了一种称为类感知最优传输（OT）的新方法，该方法测量源类条件分布上的分布与源和目标数据分布的混合之间的 OT 距离。我们的类感知 OT 利用成本函数来确定给定数据示例和源类条件分布之间的匹配程度。通过优化这个成本函数，我们找到了目标示例和源类条件分布之间的最佳匹配，有效地解决了两个域之间发生的数据和标签偏移问题。为了有效地处理类感知 OT，我们提出了一种摊销解决方案，采用深度神经网络来制定运输概率和成本函数。此外，我们建议最小化类感知高阶矩匹配（HMM），以对齐源域和目标域上的相应类区域。类感知 HMM 组件提供了一种经济的计算方法，用于准确评估两个分布之间的 HMM 距离。对基准数据集的大量实验表明，我们提出的方法显着优于现有的最先进的基线。</li>
</ul>

<h3>Title: Scalable Federated Unlearning via Isolated and Coded Sharding</h3>
<ul>
<li><strong>Authors: </strong>Yijing Lin, Zhipeng Gao, Hongyang Du, Dusit Niyato, Gui Gui, Shuguang Cui, Jinke Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15957">https://arxiv.org/abs/2401.15957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15957">https://arxiv.org/pdf/2401.15957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15957]] Scalable Federated Unlearning via Isolated and Coded Sharding(https://arxiv.org/abs/2401.15957)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Federated unlearning has emerged as a promising paradigm to erase the client-level data effect without affecting the performance of collaborative learning models. However, the federated unlearning process often introduces extensive storage overhead and consumes substantial computational resources, thus hindering its implementation in practice. To address this issue, this paper proposes a scalable federated unlearning framework based on isolated sharding and coded computing. We first divide distributed clients into multiple isolated shards across stages to reduce the number of clients being affected. Then, to reduce the storage overhead of the central server, we develop a coded computing mechanism by compressing the model parameters across different shards. In addition, we provide the theoretical analysis of time efficiency and storage effectiveness for the isolated and coded sharding. Finally, extensive experiments on two typical learning tasks, i.e., classification and generation, demonstrate that our proposed framework can achieve better performance than three state-of-the-art frameworks in terms of accuracy, retraining time, storage overhead, and F1 scores for resisting membership inference attacks.</li>
<li><strong>摘要：</strong>联合取消学习已成为一种有前途的范例，可以在不影响协作学习模型性能的情况下消除客户端级数据效应。然而，联合遗忘过程通常会引入大量的存储开销并消耗大量的计算资源，从而阻碍了其在实践中的实现。为了解决这个问题，本文提出了一种基于隔离分片和编码计算的可扩展的联合取消学习框架。我们首先将分布式客户端跨阶段划分为多个隔离的分片，以减少受影响的客户端数量。然后，为了减少中央服务器的存储开销，我们通过跨不同分片压缩模型参数来开发编码计算机制。此外，我们还提供了隔离和编码分片的时间效率和存储有效性的理论分析。最后，对两个典型学习任务（即分类和生成）进行的大量实验表明，我们提出的框架在准确性、重新训练时间、存储开销和 F1 分数方面可以比三个最先进的框架实现更好的性能。抵抗成员推理攻击。</li>
</ul>

<h3>Title: Response Generation for Cognitive Behavioral Therapy with Large Language  Models: Comparative Study with Socratic Questioning</h3>
<ul>
<li><strong>Authors: </strong>Kenta Izumi, Hiroki Tanaka, Kazuhiro Shidara, Hiroyoshi Adachi, Daisuke Kanayama, Takashi Kudo, Satoshi Nakamura</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15966">https://arxiv.org/abs/2401.15966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15966">https://arxiv.org/pdf/2401.15966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15966]] Response Generation for Cognitive Behavioral Therapy with Large Language  Models: Comparative Study with Socratic Questioning(https://arxiv.org/abs/2401.15966)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Dialogue systems controlled by predefined or rule-based scenarios derived from counseling techniques, such as cognitive behavioral therapy (CBT), play an important role in mental health apps. Despite the need for responsible responses, it is conceivable that using the newly emerging LLMs to generate contextually relevant utterances will enhance these apps. In this study, we construct dialogue modules based on a CBT scenario focused on conventional Socratic questioning using two kinds of LLMs: a Transformer-based dialogue model further trained with a social media empathetic counseling dataset, provided by Osaka Prefecture (OsakaED), and GPT-4, a state-of-the art LLM created by OpenAI. By comparing systems that use LLM-generated responses with those that do not, we investigate the impact of generated responses on subjective evaluations such as mood change, cognitive change, and dialogue quality (e.g., empathy). As a result, no notable improvements are observed when using the OsakaED model. When using GPT-4, the amount of mood change, empathy, and other dialogue qualities improve significantly. Results suggest that GPT-4 possesses a high counseling ability. However, they also indicate that even when using a dialogue model trained with a human counseling dataset, it does not necessarily yield better outcomes compared to scenario-based dialogues. While presenting LLM-generated responses, including GPT-4, and having them interact directly with users in real-life mental health care services may raise ethical issues, it is still possible for human professionals to produce example responses or response templates using LLMs in advance in systems that use rules, scenarios, or example responses.</li>
<li><strong>摘要：</strong>由源自咨询技术（例如认知行为疗法（CBT））的预定义或基于规则的场景控制的对话系统在心理健康应用程序中发挥着重要作用。尽管需要负责任的回应，但可以想象的是，使用新兴的法学硕士来生成上下文相关的话语将增强这些应用程序。在本研究中，我们基于 CBT 场景构建对话模块，重点关注传统苏格拉底式提问，使用两种法学硕士：基于 Transformer 的对话模型，并使用大阪府 (OsakaED) 提供的社交媒体同理心咨询数据集和 GPT 进一步训练。 -4，由 OpenAI 创建的最先进的法学硕士。通过比较使用 LLM 生成的响应的系统与不使用 LLM 生成的响应的系统，我们研究了生成的响应对主观评估的影响，例如情绪变化、认知变化和对话质量（例如同理心）。因此，使用 OsakaED 模型时没有观察到显着的改进。使用 GPT-4 时，情绪变化量、同理心和其他对话质量显着提高。结果表明GPT-4具有较高的咨询能力。然而，他们还表明，即使使用经过人类咨询数据集训练的对话模型，与基于场景的对话相比，也不一定会产生更好的结果。虽然呈现 LLM 生成的响应（包括 GPT-4）并让它们直接与现实生活中的心理健康护理服务中的用户互动可能会引发道德问题，但人类专业人员仍然有可能提前使用 LLM 生成示例响应或响应模板在使用规则、场景或示例响应的系统中。</li>
</ul>

<h3>Title: Sample Weight Estimation Using Meta-Updates for Online Continual  Learning</h3>
<ul>
<li><strong>Authors: </strong>Hamed Hemati, Damian Borth</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15973">https://arxiv.org/abs/2401.15973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15973">https://arxiv.org/pdf/2401.15973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15973]] Sample Weight Estimation Using Meta-Updates for Online Continual  Learning(https://arxiv.org/abs/2401.15973)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>The loss function plays an important role in optimizing the performance of a learning system. A crucial aspect of the loss function is the assignment of sample weights within a mini-batch during loss computation. In the context of continual learning (CL), most existing strategies uniformly treat samples when calculating the loss value, thereby assigning equal weights to each sample. While this approach can be effective in certain standard benchmarks, its optimal effectiveness, particularly in more complex scenarios, remains underexplored. This is particularly pertinent in training "in the wild," such as with self-training, where labeling is automated using a reference model. This paper introduces the Online Meta-learning for Sample Importance (OMSI) strategy that approximates sample weights for a mini-batch in an online CL stream using an inner- and meta-update mechanism. This is done by first estimating sample weight parameters for each sample in the mini-batch, then, updating the model with the adapted sample weights. We evaluate OMSI in two distinct experimental settings. First, we show that OMSI enhances both learning and retained accuracy in a controlled noisy-labeled data stream. Then, we test the strategy in three standard benchmarks and compare it with other popular replay-based strategies. This research aims to foster the ongoing exploration in the area of self-adaptive CL.</li>
<li><strong>摘要：</strong>损失函数在优化学习系统的性能方面发挥着重要作用。损失函数的一个重要方面是在损失计算期间分配小批量内的样本权重。在持续学习（CL）的背景下，大多数现有策略在计算损失值时统一对待样本，从而为每个样本分配相同的权重。虽然这种方法在某些标准基准测试中可能有效，但其最佳有效性，特别是在更复杂的场景中，仍未得到充分探索。这在“野外”训练中尤其重要，例如自我训练，其中使用参考模型自动进行标记。本文介绍了样本重要性在线元学习 (OMSI) 策略，该策略使用内部更新和元更新机制来近似在线 CL 流中小批量的样本权重。这是通过首先估计小批量中每个样本的样本权重参数，然后使用调整后的样本权重更新模型来完成的。我们在两种不同的实验环境中评估 OMSI。首先，我们证明 OMSI 增强了受控噪声标记数据流中的学习和保留准确性。然后，我们在三个标准基准测试中测试该策略，并将其与其他流行的基于重放的策略进行比较。本研究旨在促进自适应 CL 领域的持续探索。</li>
</ul>

<h3>Title: Deep Embedding Clustering Driven by Sample Stability</h3>
<ul>
<li><strong>Authors: </strong>Zhanwen Cheng, Feijiang Li, Jieting Wang, Yuhua Qian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15989">https://arxiv.org/abs/2401.15989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15989">https://arxiv.org/pdf/2401.15989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15989]] Deep Embedding Clustering Driven by Sample Stability(https://arxiv.org/abs/2401.15989)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Deep clustering methods improve the performance of clustering tasks by jointly optimizing deep representation learning and clustering. While numerous deep clustering algorithms have been proposed, most of them rely on artificially constructed pseudo targets for performing clustering. This construction process requires some prior knowledge, and it is challenging to determine a suitable pseudo target for clustering. To address this issue, we propose a deep embedding clustering algorithm driven by sample stability (DECS), which eliminates the requirement of pseudo targets. Specifically, we start by constructing the initial feature space with an autoencoder and then learn the cluster-oriented embedding feature constrained by sample stability. The sample stability aims to explore the deterministic relationship between samples and all cluster centroids, pulling samples to their respective clusters and keeping them away from other clusters with high determinacy. We analyzed the convergence of the loss using Lipschitz continuity in theory, which verifies the validity of the model. The experimental results on five datasets illustrate that the proposed method achieves superior performance compared to state-of-the-art clustering approaches.</li>
<li><strong>摘要：</strong>深度聚类方法通过联合优化深度表示学习和聚类来提高聚类任务的性能。虽然已经提出了许多深度聚类算法，但大多数都依赖于人工构造的伪目标来执行聚类。这个构建过程需要一些先验知识，并且确定合适的聚类伪目标具有挑战性。为了解决这个问题，我们提出了一种由样本稳定性（DECS）驱动的深度嵌入聚类算法，该算法消除了伪目标的要求。具体来说，我们首先使用自动编码器构建初始特征空间，然后学习受样本稳定性约束的面向集群的嵌入特征。样本稳定性旨在探索样本与所有簇质心之间的确定性关系，以高确定性将样本拉到各自的簇中并使其远离其他簇。我们从理论上利用Lipschitz连续性分析了损失的收敛性，验证了模型的有效性。五个数据集的实验结果表明，与最先进的聚类方法相比，所提出的方法实现了优越的性能。</li>
</ul>

<h3>Title: GPS: Graph Contrastive Learning via Multi-scale Augmented Views from  Adversarial Pooling</h3>
<ul>
<li><strong>Authors: </strong>Wei Ju, Yiyang Gu, Zhengyang Mao, Ziyue Qiao, Yifang Qin, Xiao Luo, Hui Xiong, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16011">https://arxiv.org/abs/2401.16011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16011">https://arxiv.org/pdf/2401.16011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16011]] GPS: Graph Contrastive Learning via Multi-scale Augmented Views from  Adversarial Pooling(https://arxiv.org/abs/2401.16011)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Self-supervised graph representation learning has recently shown considerable promise in a range of fields, including bioinformatics and social networks. A large number of graph contrastive learning approaches have shown promising performance for representation learning on graphs, which train models by maximizing agreement between original graphs and their augmented views (i.e., positive views). Unfortunately, these methods usually involve pre-defined augmentation strategies based on the knowledge of human experts. Moreover, these strategies may fail to generate challenging positive views to provide sufficient supervision signals. In this paper, we present a novel approach named Graph Pooling ContraSt (GPS) to address these issues. Motivated by the fact that graph pooling can adaptively coarsen the graph with the removal of redundancy, we rethink graph pooling and leverage it to automatically generate multi-scale positive views with varying emphasis on providing challenging positives and preserving semantics, i.e., strongly-augmented view and weakly-augmented view. Then, we incorporate both views into a joint contrastive learning framework with similarity learning and consistency learning, where our pooling module is adversarially trained with respect to the encoder for adversarial robustness. Experiments on twelve datasets on both graph classification and transfer learning tasks verify the superiority of the proposed method over its counterparts.</li>
<li><strong>摘要：</strong>自监督图表示学习最近在生物信息学和社交网络等一系列领域显示出了巨大的前景。大量图对比学习方法在图表示学习方面表现出了良好的性能，这些方法通过最大化原始图与其增强视图（即正面视图）之间的一致性来训练模型。不幸的是，这些方法通常涉及基于人类专家知识的预定义增强策略。此外，这些策略可能无法产生具有挑战性的积极观点来提供足够的监督信号。在本文中，我们提出了一种名为图池对比（GPS）的新方法来解决这些问题。由于图池化可以通过消除冗余来自适应地粗化图，我们重新思考图池化并利用它来自动生成多尺度正视图，并不同程度地强调提供具有挑战性的正视图和保留语义，即强增强视图和弱增强视图。然后，我们将两种视图合并到具有相似性学习和一致性学习的联合对比学习框架中，其中我们的池模块针对编码器进行对抗性训练，以实现对抗性鲁棒性。在图分类和迁移学习任务上对十二个数据集进行的实验验证了所提出的方法相对于同类方法的优越性。</li>
</ul>

<h3>Title: Finding Challenging Metaphors that Confuse Pretrained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Li, Frank Guerin, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16012">https://arxiv.org/abs/2401.16012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16012">https://arxiv.org/pdf/2401.16012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16012]] Finding Challenging Metaphors that Confuse Pretrained Language Models(https://arxiv.org/abs/2401.16012)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Metaphors are considered to pose challenges for a wide spectrum of NLP tasks. This gives rise to the area of computational metaphor processing. However, it remains unclear what types of metaphors challenge current state-of-the-art models. In this paper, we test various NLP models on the VUA metaphor dataset and quantify to what extent metaphors affect models' performance on various downstream tasks. Analysis reveals that VUA includes a large number of metaphors that pose little difficulty to downstream tasks. We would like to shift the attention of researchers away from these metaphors to instead focus on challenging metaphors. To identify hard metaphors, we propose an automatic pipeline that identifies metaphors that challenge a particular model. Our analysis demonstrates that our detected hard metaphors contrast significantly with VUA and reduce the accuracy of machine translation by 16\%, QA performance by 4\%, NLI by 7\%, and metaphor identification recall by over 14\% for various popular NLP systems.</li>
<li><strong>摘要：</strong>隐喻被认为对广泛的 NLP 任务提出了挑战。这就产生了计算隐喻处理领域。然而，目前尚不清楚哪些类型的隐喻挑战了当前最先进的模型。在本文中，我们在 VUA 隐喻数据集上测试了各种 NLP 模型，并量化了隐喻对模型在各种下游任务上的性能的影响程度。分析表明，VUA包含大量隐喻，对下游任务造成的难度不大。我们希望将研究人员的注意力从这些隐喻上转移到具有挑战性的隐喻上。为了识别硬隐喻，我们提出了一个自动管道来识别挑战特定模型的隐喻。我们的分析表明，我们检测到的硬隐喻与 VUA 形成显着对比，对于各种流行的 NLP，机器翻译的准确性降低了 16\%，QA 性能降低了 4\%，NLI 降低了 7\%，隐喻识别召回率降低了 14\% 以上系统。</li>
</ul>

<h3>Title: Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules  in Vector-symbolic Architectures</h3>
<ul>
<li><strong>Authors: </strong>Michael Hersche, Francesco di Stefano, Thomas Hofmann, Abu Sebastian, Abbas Rahimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16024">https://arxiv.org/abs/2401.16024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16024">https://arxiv.org/pdf/2401.16024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16024]] Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules  in Vector-symbolic Architectures(https://arxiv.org/abs/2401.16024)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Abstract reasoning is a cornerstone of human intelligence, and replicating it with artificial intelligence (AI) presents an ongoing challenge. This study focuses on efficiently solving Raven's progressive matrices (RPM), a visual test for assessing abstract reasoning abilities, by using distributed computation and operators provided by vector-symbolic architectures (VSA). Instead of hard-coding the rule formulations associated with RPMs, our approach can learn the VSA rule formulations (hence the name Learn-VRF) with just one pass through the training data. Yet, our approach, with compact parameters, remains transparent and interpretable. Learn-VRF yields accurate predictions on I-RAVEN's in-distribution data, and exhibits strong out-of-distribution capabilities concerning unseen attribute-rule pairs, significantly outperforming pure connectionist baselines including large language models. Our code is available at https://github.com/IBM/learn-vector-symbolic-architectures-rule-formulations.</li>
<li><strong>摘要：</strong>抽象推理是人类智能的基石，用人工智能 (AI) 复制它是一个持续的挑战。本研究的重点是通过使用矢量符号架构 (VSA) 提供的分布式计算和运算符来有效求解 Raven 渐进矩阵 (RPM)，这是一种用于评估抽象推理能力的视觉测试。我们的方法无需对与 RPM 相关的规则公式进行硬编码，只需一次训练数据即可学习 VSA 规则公式（因此称为 Learn-VRF）。然而，我们的方法具有紧凑的参数，仍然是透明且可解释的。 Learn-VRF 可对 I-RAVEN 的分布内数据进行准确预测，并在未见过的属性规则对方面表现出强大的分布外能力，显着优于包括大型语言模型在内的纯联结主义基线。我们的代码可从 https://github.com/IBM/learn-vector-symbolic-architectures-rule-formulations 获取。</li>
</ul>

<h3>Title: Simple Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhengpeng Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16025">https://arxiv.org/abs/2401.16025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16025">https://arxiv.org/pdf/2401.16025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16025]] Simple Policy Optimization(https://arxiv.org/abs/2401.16025)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose SPO (Simple Policy Optimization) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. SPO can effectively enforce the trust region constraints in almost all environments, while still maintaining the simplicity of a first-order algorithm. Comparative experiments in Atari 2600 environments show that SPO sometimes provides stronger performance than PPO. Code is available at https://github.com/MyRepositories-hub/Simple-Policy-Optimization.</li>
<li><strong>摘要：</strong>PPO（邻近策略优化）算法在许多领域表现出了优异的性能，它被认为是TRPO（信任区域策略优化）算法的简单版本。然而，PPO中的比率裁剪操作可能并不总是有效地强制信任域约束，这可能是影响算法稳定性的潜在因素。在本文中，我们提出了SPO（简单策略优化）算法，该算法针对旧策略和当前策略之间的KL散度引入了一种新颖的裁剪方法。 SPO 可以在几乎所有环境中有效地实施信任域约束，同时仍然保持一阶算法的简单性。 Atari 2600 环境中的对比实验表明，SPO 有时提供比 PPO 更强的性能。代码可在 https://github.com/MyRepositories-hub/Simple-Policy-Optimization 获取。</li>
</ul>

<h3>Title: Type-based Neural Link Prediction Adapter for Complex Query Answering</h3>
<ul>
<li><strong>Authors: </strong>Lingning Song, Yi Zu, Shan Lu, Jieyue He</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16045">https://arxiv.org/abs/2401.16045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16045">https://arxiv.org/pdf/2401.16045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16045]] Type-based Neural Link Prediction Adapter for Complex Query Answering(https://arxiv.org/abs/2401.16045)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Answering complex logical queries on incomplete knowledge graphs (KGs) is a fundamental and challenging task in multi-hop reasoning. Recent work defines this task as an end-to-end optimization problem, which significantly reduces the training cost and enhances the generalization of the model by a pretrained link predictors for query answering. However, most existing proposals ignore the critical semantic knowledge inherently available in KGs, such as type information, which could help answer complex logical queries. To this end, we propose TypE-based Neural Link Prediction Adapter (TENLPA), a novel model that constructs type-based entity-relation graphs to discover the latent relationships between entities and relations by leveraging type information in KGs. Meanwhile, in order to effectively combine type information with complex logical queries, an adaptive learning mechanism is introduced, which is trained by back-propagating during the complex query answering process to achieve adaptive adjustment of neural link predictors. Experiments on 3 standard datasets show that TENLPA model achieves state-of-the-art performance on complex query answering with good generalization and robustness.</li>
<li><strong>摘要：</strong>回答不完整知识图（KG）上的复杂逻辑查询是多跳推理中的一项基本且具有挑战性的任务。最近的工作将该任务定义为端到端优化问题，通过用于查询应答的预训练链接预测器显着降低了训练成本并增强了模型的泛化能力。然而，大多数现有提案忽略了知识图谱中固有的关键语义知识，例如类型信息，这可以帮助回答复杂的逻辑查询。为此，我们提出了基于 TypE 的神经链接预测适配器（TENLPA），这是一种新颖的模型，它构建基于类型的实体关系图，通过利用知识图谱中的类型信息来发现实体和关系之间的潜在关系。同时，为了有效地将类型信息与复杂逻辑查询结合起来，引入了自适应学习机制，在复杂查询回答过程中通过反向传播进行训练，以实现神经链接预测器的自适应调整。在 3 个标准数据集上的实验表明，TENLPA 模型在复杂查询应答方面实现了最先进的性能，具有良好的泛化性和鲁棒性。</li>
</ul>

<h3>Title: Non-Fluent Synthetic Target-Language Data Improve Neural Machine  Translation</h3>
<ul>
<li><strong>Authors: </strong>Víctor M. Sánchez-Cartagena, Miquel Esplà-Gomis, Juan Antonio Pérez-Ortiz, Felipe Sánchez-Martínez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16086">https://arxiv.org/abs/2401.16086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16086">https://arxiv.org/pdf/2401.16086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16086]] Non-Fluent Synthetic Target-Language Data Improve Neural Machine  Translation(https://arxiv.org/abs/2401.16086)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>When the amount of parallel sentences available to train a neural machine translation is scarce, a common practice is to generate new synthetic training samples from them. A number of approaches have been proposed to produce synthetic parallel sentences that are similar to those in the parallel data available. These approaches work under the assumption that non-fluent target-side synthetic training samples can be harmful and may deteriorate translation performance. Even so, in this paper we demonstrate that synthetic training samples with non-fluent target sentences can improve translation performance if they are used in a multilingual machine translation framework as if they were sentences in another language. We conducted experiments on ten low-resource and four high-resource translation tasks and found out that this simple approach consistently improves translation performance as compared to state-of-the-art methods for generating synthetic training samples similar to those found in corpora. Furthermore, this improvement is independent of the size of the original training corpus, the resulting systems are much more robust against domain shift and produce less hallucinations.</li>
<li><strong>摘要：</strong>当可用于训练神经机器翻译的并行句子数量稀缺时，常见的做法是从中生成新的合成训练样本。已经提出了许多方法来生成与可用并行数据中的句子相似的合成并行句子。这些方法的工作原理是假设不流畅的目标端合成训练样本可能是有害的，并且可能会降低翻译性能。即便如此，在本文中，我们证明，如果将具有不流畅目标句子的合成训练样本像另一种语言的句子一样用于多语言机器翻译框架中，则可以提高翻译性能。我们对十个低资源和四个高资源翻译任务进行了实验，发现与生成类似于语料库中的合成训练样本的最先进方法相比，这种简单的方法持续提高了翻译性能。此外，这种改进与原始训练语料库的大小无关，所得系统对于域转移更加稳健，并且产生更少的幻觉。</li>
</ul>

<h3>Title: Fairness in Algorithmic Recourse Through the Lens of Substantive  Equality of Opportunity</h3>
<ul>
<li><strong>Authors: </strong>Andrew Bell, Joao Fonseca, Carlo Abrate, Francesco Bonchi, Julia Stoyanovich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16088">https://arxiv.org/abs/2401.16088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16088">https://arxiv.org/pdf/2401.16088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16088]] Fairness in Algorithmic Recourse Through the Lens of Substantive  Equality of Opportunity(https://arxiv.org/abs/2401.16088)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Algorithmic recourse -- providing recommendations to those affected negatively by the outcome of an algorithmic system on how they can take action and change that outcome -- has gained attention as a means of giving persons agency in their interactions with artificial intelligence (AI) systems. Recent work has shown that even if an AI decision-making classifier is ``fair'' (according to some reasonable criteria), recourse itself may be unfair due to differences in the initial circumstances of individuals, compounding disparities for marginalized populations and requiring them to exert more effort than others. There is a need to define more methods and metrics for evaluating fairness in recourse that span a range of normative views of the world, and specifically those that take into account time. Time is a critical element in recourse because the longer it takes an individual to act, the more the setting may change due to model or data drift. This paper seeks to close this research gap by proposing two notions of fairness in recourse that are in normative alignment with substantive equality of opportunity, and that consider time. The first considers the (often repeated) effort individuals exert per successful recourse event, and the second considers time per successful recourse event. Building upon an agent-based framework for simulating recourse, this paper demonstrates how much effort is needed to overcome disparities in initial circumstances. We then proposes an intervention to improve the fairness of recourse by rewarding effort, and compare it to existing strategies.</li>
<li><strong>摘要：</strong>算法资源——向那些受到算法系统结果负面影响的人提供如何采取行动并改变结果的建议——作为一种在人们与人工智能（AI）系统交互时赋予人们代理权的手段而受到关注。最近的研究表明，即使人工智能决策分类器是“公平的”（根据一些合理的标准），由于个体初始环境的差异，追索权本身也可能是不公平的，这加剧了边缘化群体的差异，并要求他们比别人付出更多的努力。有必要定义更多的方法和指标来评估追索权的公平性，这些方法和指标涵盖一系列世界规范观点，特别是那些考虑时间的方法和指标。时间是追索权的关键因素，因为个人采取行动的时间越长，由于模型或数据漂移而导致的设置变化就越大。本文试图通过提出两种追索公平的概念来缩小这一研究差距，这两种概念与机会的实质性平等保持规范一致，并且考虑了时间。第一个考虑个人为每个成功的追索事件付出的（经常重复的）努力，第二个考虑每个成功的追索事件的时间。本文以基于代理的模拟追索权框架为基础，展示了需要付出多少努力来克服初始情况下的差异。然后，我们提出了一种干预措施，通过奖励努力来提高追索权的公平性，并将其与现有策略进行比较。</li>
</ul>

<h3>Title: Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and  Prompt Engineering May Not Help You</h3>
<ul>
<li><strong>Authors: </strong>Felix Friedrich, Katharina Hämmerl, Patrick Schramowski, Jindrich Libovicky, Kristian Kersting, Alexander Fraser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16092">https://arxiv.org/abs/2401.16092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16092">https://arxiv.org/pdf/2401.16092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16092]] Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and  Prompt Engineering May Not Help You(https://arxiv.org/abs/2401.16092)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption that each gender should be equally likely to be generated, but that there are also big differences across languages. Furthermore, we investigate prompt engineering strategies, i.e. the use of indirect, neutral formulations, as a possible remedy for these biases. Unfortunately, they help only to a limited extent and result in worse text-to-image alignment. Consequently, this work calls for more research into diverse representations across languages in image generators.</li>
<li><strong>摘要：</strong>文本到图像生成模型最近在图像质量、灵活性和文本对齐方面取得了惊人的成果，因此被用于快速增长的应用中。通过多语言能力的提高，现在更大的社区可以使用这种技术。然而，正如我们将要展示的，多语言模型与单语言模型类似地遭受（性别）偏见。此外，人们自然期望这些模型将在不同语言之间提供相似的结果，但事实并非如此，语言之间存在重要差异。因此，我们提出了一个新的基准 MAGBIG，旨在促进没有性别偏见的多语言模型研究。我们利用 MAGBIG 研究多语言 T2I 模型是否会放大性别偏见。为此，我们使用多语言提示来请求特定职业或特征的人的肖像图像（使用形容词）。我们的结果表明，模型不仅偏离了每种性别产生的可能性应该相同的规范假设，而且不同语言之间也存在很大差异。此外，我们研究了即时工程策略，即使用间接、中性的公式，作为这些偏见的可能补救措施。不幸的是，它们的作用有限，并且会导致文本到图像的对齐情况更差。因此，这项工作需要对图像生成器中跨语言的多样化表示进行更多研究。</li>
</ul>

<h3>Title: Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation  for Automatic Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16107">https://arxiv.org/abs/2401.16107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16107">https://arxiv.org/pdf/2401.16107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16107]] Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation  for Automatic Diagnosis(https://arxiv.org/abs/2401.16107)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Automatic diagnosis is a significant application of AI in healthcare, where diagnoses are generated based on the symptom description of patients. Previous works have approached this task directly by modeling the relationship between the normalized symptoms and all possible diseases. However, in the clinical diagnostic process, patients are initially consulted by a general practitioner and, if necessary, referred to specialists in specific domains for a more comprehensive evaluation. The final diagnosis often emerges from a collaborative consultation among medical specialist groups. Recently, large language models have shown impressive capabilities in natural language understanding. In this study, we adopt tuning-free LLM-based agents as medical practitioners and propose the Agent-derived Multi-Specialist Consultation (AMSC) framework to model the diagnosis process in the real world by adaptively fusing probability distributions of agents over potential diseases. Experimental results demonstrate the superiority of our approach compared with baselines. Notably, our approach requires significantly less parameter updating and training time, enhancing efficiency and practical utility. Furthermore, we delve into a novel perspective on the role of implicit symptoms within the context of automatic diagnosis.</li>
<li><strong>摘要：</strong>自动诊断是人工智能在医疗保健领域的重要应用，根据患者的症状描述生成诊断。以前的工作通过对正常症状与所有可能疾病之间的关系进行建模来直接完成这项任务。然而，在临床诊断过程中，患者首先会接受全科医生的咨询，如有必要，还会转诊至特定领域的专家进行更全面的评估。最终诊断通常来自医学专家小组的协作咨询。最近，大型语言模型在自然语言理解方面表现出了令人印象深刻的能力。在本研究中，我们采用基于 LLM 的免调优代理作为医疗从业者，并提出代理衍生的多专家咨询（AMSC）框架，通过自适应融合代理对潜在疾病的概率分布来建模现实世界中的诊断过程。实验结果证明了我们的方法与基线相比的优越性。值得注意的是，我们的方法需要显着减少参数更新和训练时间，从而提高效率和实用性。此外，我们深入研究了自动诊断背景下隐含症状的作用的新视角。</li>
</ul>

<h3>Title: On the generalization of learned constraints for ASP solving in temporal  domains</h3>
<ul>
<li><strong>Authors: </strong>Javier Romero, Torsten Schaub, Klaus Strauch</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16124">https://arxiv.org/abs/2401.16124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16124">https://arxiv.org/pdf/2401.16124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16124]] On the generalization of learned constraints for ASP solving in temporal  domains(https://arxiv.org/abs/2401.16124)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The representation of a dynamic problem in ASP usually boils down to using copies of variables and constraints, one for each time stamp, no matter whether it is directly encoded or via an action or temporal language. The multiplication of variables and constraints is commonly done during grounding and the solver is completely ignorant about the temporal relationship among the different instances. On the other hand, a key factor in the performance of today's ASP solvers is conflict-driven constraint learning. Our question is now whether a constraint learned for particular time steps can be generalized and reused at other time stamps, and ultimately whether this enhances the overall solver performance on temporal problems. Knowing full well the domain of time, we study conditions under which learned dynamic constraints can be generalized. We propose a simple translation of the original logic program such that, for the translated programs, the learned constraints can be generalized to other time points. Additionally, we identify a property of temporal problems that allows us to generalize all learned constraints to all time steps. It turns out that this property is satisfied by many planning problems. Finally, we empirically evaluate the impact of adding the generalized constraints to an ASP solver</li>
<li><strong>摘要：</strong>ASP 中动态问题的表示通常归结为使用变量和约束的副本，每个时间戳对应一个副本，无论它是直接编码还是通过动作或时间语言进行编码。变量和约束的相乘通常是在接地期间完成的，并且求解器完全不知道不同实例之间的时间关系。另一方面，当今 ASP 求解器性能的一个关键因素是冲突驱动的约束学习。现在我们的问题是，针对特定时间步骤学习的约束是否可以在其他时间戳上推广和重用，以及最终是否可以提高时态问题的整体求解器性能。在充分了解时间域的情况下，我们研究了可以推广学习到的动态约束的条件。我们提出了对原始逻辑程序的简单翻译，这样对于翻译后的程序，学习到的约束可以推广到其他时间点。此外，我们还确定了时间问题的一个属性，该属性使我们能够将所有学习到的约束推广到所有时间步长。事实证明，这个房产是满足了很多规划问题的。最后，我们凭经验评估向 ASP 求解器添加广义约束的影响</li>
</ul>

<h3>Title: BooleanOCT: Optimal Classification Trees based on multivariate Boolean  Rules</h3>
<ul>
<li><strong>Authors: </strong>Jiancheng Tu, Wenqi Fan, Zhibin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16133">https://arxiv.org/abs/2401.16133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16133">https://arxiv.org/pdf/2401.16133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16133]] BooleanOCT: Optimal Classification Trees based on multivariate Boolean  Rules(https://arxiv.org/abs/2401.16133)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The global optimization of classification trees has demonstrated considerable promise, notably in enhancing accuracy, optimizing size, and thereby improving human comprehensibility. While existing optimal classification trees substantially enhance accuracy over greedy-based tree models like CART, they still fall short when compared to the more complex black-box models, such as random forests. To bridge this gap, we introduce a new mixed-integer programming (MIP) formulation, grounded in multivariate Boolean rules, to derive the optimal classification tree. Our methodology integrates both linear metrics, including accuracy, balanced accuracy, and cost-sensitive cost, as well as nonlinear metrics such as the F1-score. The approach is implemented in an open-source Python package named BooleanOCT. We comprehensively benchmark these methods on the 36 datasets from the UCI machine learning repository. The proposed models demonstrate practical solvability on real-world datasets, effectively handling sizes in the tens of thousands. Aiming to maximize accuracy, this model achieves an average absolute improvement of 3.1\% and 1.5\% over random forests in small-scale and medium-sized datasets, respectively. Experiments targeting various objectives, including balanced accuracy, cost-sensitive cost, and F1-score, demonstrate the framework's wide applicability and its superiority over contemporary state-of-the-art optimal classification tree methods in small to medium-scale datasets.</li>
<li><strong>摘要：</strong>分类树的全局优化已显示出相当大的前景，特别是在提高准确性、优化大小、从而提高人类可理解性方面。虽然现有的最优分类树大大提高了 CART 等基于贪婪的树模型的准确性，但与随机森林等更复杂的黑盒模型相比，它们仍然存在不足。为了弥补这一差距，我们引入了一种新的混合整数规划（MIP）公式，该公式基于多元布尔规则，以导出最佳分类树。我们的方法集成了线性指标（包括准确性、平衡准确性和成本敏感成本）以及非线性指标（例如 F1 分数）。该方法在名为 BooleanOCT 的开源 Python 包中实现。我们在 UCI 机器学习存储库的 36 个数据集上对这些方法进行了全面的基准测试。所提出的模型展示了对现实世界数据集的实际解决能力，有效处理数万个数据集。为了最大限度地提高准确性，该模型在小规模和中型数据集中比随机森林分别实现了 3.1% 和 1.5% 的平均绝对改进。针对各种目标（包括平衡精度、成本敏感成本和 F1 分数）的实验证明了该框架的广泛适用性及其在中小型数据集中相对于当代最先进的最优分类树方法的优越性。</li>
</ul>

<h3>Title: X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme  Multi-Profile Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Namju Kwak, Taesup Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16137">https://arxiv.org/abs/2401.16137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16137">https://arxiv.org/pdf/2401.16137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16137]] X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme  Multi-Profile Scenarios(https://arxiv.org/abs/2401.16137)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) techniques, such as adapter tuning, aim to fine-tune a pre-trained language model (PLM) using a minimal number of parameters for a specific task or profile. Although adapter tuning provides increased parameter efficiency compared to full-model fine-tuning, it introduces a small set of additional parameters attached to a PLM for each profile. This can become problematic in practical applications with multiple profiles, particularly when a significant increase in the number of profiles linearly boosts the total number of additional parameters. To mitigate this issue, we introduce X-PEFT, a novel PEFT method that leverages a multitude of given adapters by fine-tuning an extremely small set of compact tensors for a new profile, which serve as binary masks to adaptively select the given adapters. To efficiently validate our proposed method, we implement it using a large number of trained or untrained (random) adapters. We evaluate the performance of X-PEFT through LaMP and GLUE tasks and demonstrate that it either matches or surpasses the effectiveness of conventional adapter tuning, despite reducing the memory requirements per profile by a factor of 10,000 compared to it.</li>
<li><strong>摘要：</strong>参数高效微调 (PEFT) 技术（例如适配器调优）旨在针对特定任务或配置文件使用最少数量的参数来微调预训练的语言模型 (PLM)。尽管与全模型微调相比，适配器调整提高了参数效率，但它为每个配置文件引入了附加到 PLM 的一小组附加参数。在具有多个配置文件的实际应用中，这可能会成为问题，特别是当配置文件数量的显着增加线性增加附加参数的总数时。为了缓解这个问题，我们引入了 X-PEFT，这是一种新颖的 PEFT 方法，它通过微调一组极小的紧凑张量来获得新的配置文件，从而利用大量给定的适配器，这些张量用作二进制掩码来自适应地选择给定的适配器。为了有效地验证我们提出的方法，我们使用大量经过训练或未经训练的（随机）适配器来实现它。我们通过 LaMP 和 GLUE 任务评估 X-PEFT 的性能，并证明它可以匹配或超过传统适配器调整的有效性，尽管与传统适配器调整相比，每个配置文件的内存需求减少了 10,000 倍。</li>
</ul>

<h3>Title: Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual  Perception</h3>
<ul>
<li><strong>Authors: </strong>Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16158">https://arxiv.org/abs/2401.16158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16158">https://arxiv.org/pdf/2401.16158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16158]] Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual  Perception(https://arxiv.org/abs/2401.16158)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag, agent</a></li>
<li><strong>Abstract: </strong>Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.</li>
<li><strong>摘要：</strong>基于多模式大语言模型（MLLM）的移动设备代理正在成为流行的应用程序。在本文中，我们介绍了 Mobile-Agent，一种自主的多模式移动设备代理。 Mobile-Agent 首先利用视觉感知工具来准确识别和定位应用程序前端界面中的视觉和文本元素。基于感知到的视觉上下文，它会自主规划和分解复杂的操作任务，并逐步导航移动应用程序进行操作。与以前依赖应用程序的 XML 文件或移动系统元数据的解决方案不同，Mobile-Agent 允许以视觉为中心的方式在不同的移动操作环境中提供更大的适应性，从而消除了特定于系统的定制的必要性。为了评估 Mobile-Agent 的性能，我们引入了 Mobile-Eval，这是评估移动设备操作的基准。基于Mobile-Eval，我们对Mobile-Agent进行了全面的评估。实验结果表明Mobile-Agent取得了显着的准确率和完成率。即使有挑战性的指令，例如多应用程序操作，Mobile-Agent 仍然可以完成要求。代码和模型将在 https://github.com/X-PLUG/MobileAgent 开源。</li>
</ul>

<h3>Title: A Survey on Structure-Preserving Graph Transformers</h3>
<ul>
<li><strong>Authors: </strong>Van Thuy Hoang, O-Joun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16176">https://arxiv.org/abs/2401.16176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16176">https://arxiv.org/pdf/2401.16176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16176]] A Survey on Structure-Preserving Graph Transformers(https://arxiv.org/abs/2401.16176)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The transformer architecture has shown remarkable success in various domains, such as natural language processing and computer vision. When it comes to graph learning, transformers are required not only to capture the interactions between pairs of nodes but also to preserve graph structures connoting the underlying relations and proximity between them, showing the expressive power to capture different graph structures. Accordingly, various structure-preserving graph transformers have been proposed and widely used for various tasks, such as graph-level tasks in bioinformatics and chemoinformatics. However, strategies related to graph structure preservation have not been well organized and systematized in the literature. In this paper, we provide a comprehensive overview of structure-preserving graph transformers and generalize these methods from the perspective of their design objective. First, we divide strategies into four main groups: node feature modulation, context node sampling, graph rewriting, and transformer architecture improvements. We then further divide the strategies according to the coverage and goals of graph structure preservation. Furthermore, we also discuss challenges and future directions for graph transformer models to preserve the graph structure and understand the nature of graphs.</li>
<li><strong>摘要：</strong>Transformer 架构在自然语言处理和计算机视觉等各个领域都取得了显着的成功。在图学习方面，变压器不仅需要捕获节点对之间的交互，还需要保留表示它们之间的潜在关系和邻近性的图结构，显示捕获不同图结构的表达能力。因此，人们提出了各种保留结构的图转换器，并将其广泛用于各种任务，例如生物信息学和化学信息学中的图级任务。然而，与图结构保存相关的策略在文献中尚未得到很好的组织和系统化。在本文中，我们全面概述了保留结构的图转换器，并从设计目标的角度概括了这些方法。首先，我们将策略分为四个主要组：节点特征调制、上下文节点采样、图重写和变压器架构改进。然后，我们根据图结构保存的覆盖范围和目标进一步划分策略。此外，我们还讨论了图转换器模型在保留图结构和理解图本质方面的挑战和未来方向。</li>
</ul>

<h3>Title: LLaMandement: Large Language Models for Summarization of French  Legislative Proposals</h3>
<ul>
<li><strong>Authors: </strong>Joseph Gesnouin, Yannis Tannier, Christophe Gomes Da Silva, Hatim Tapory, Camille Brier, Hugo Simon, Raphael Rozenberg, Hermann Woehrel, Mehdi El Yakaabi, Thomas Binder, Guillaume Marie, Emilie Caron, Mathile Nogueira, Thomas Fontas, Laure Puydebois, Marie Theophile, Stephane Morandi, Mael Petit, David Creissac, Pauline Ennouchy, Elise Valetoux, Celine Visade, Severine Balloux, Emmanuel Cortes, Pierre-Etienne Devineau, Ulrich Tan, Esther Mac Namara, Su Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16182">https://arxiv.org/abs/2401.16182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16182">https://arxiv.org/pdf/2401.16182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16182]] LLaMandement: Large Language Models for Summarization of French  Legislative Proposals(https://arxiv.org/abs/2401.16182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This report introduces LLaMandement, a state-of-the-art Large Language Model, fine-tuned by the French government and designed to enhance the efficiency and efficacy of processing parliamentary sessions (including the production of bench memoranda and documents required for interministerial meetings) by generating neutral summaries of legislative proposals. Addressing the administrative challenges of manually processing a growing volume of legislative amendments, LLaMandement stands as a significant legal technological milestone, providing a solution that exceeds the scalability of traditional human efforts while matching the robustness of a specialized legal drafter. We release all our fine-tuned models and training data to the community.</li>
<li><strong>摘要：</strong>本报告介绍了 LLaMandement，这是一种最先进的大型语言模型，由法国政府进行了微调，旨在提高处理议会会议的效率和功效（包括制作部长备忘录和部际会议所需的文件）通过生成立法提案的中立摘要。 LLaMandement 解决了手动处理越来越多的立法修正案所带来的行政挑战，是一个重要的法律技术里程碑，提供了一种超越传统人力工作可扩展性的解决方案，同时与专业法律起草者的稳健性相匹配。我们向社区发布了所有经过微调的模型和训练数据。</li>
</ul>

<h3>Title: On the Semantics of LM Latent Space: A Vocabulary-defined Approach</h3>
<ul>
<li><strong>Authors: </strong>Jian Gu, Chunyang Chen, Aldeida Aleti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16184">https://arxiv.org/abs/2401.16184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16184">https://arxiv.org/pdf/2401.16184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16184]] On the Semantics of LM Latent Space: A Vocabulary-defined Approach(https://arxiv.org/abs/2401.16184)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>In the realm of deep learning, understanding the latent space of language models (LMs) like transformers is crucial for refining their performance and interpretability. However, existing analyses often fall short in providing absolute and model-centric insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a fixed reference frame within the LM latent space, ensuring absolute semantic analysis grounded in LM vocabulary. Our approach transcends prior relative analyses, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasizing differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach surpasses state-of-the-art methods of retrieval-augmented generation and parameters-efficient finetuning, showcasing its efficacy and broad applicability. Our findings not only shed light on LM mechanics but also offer practical solutions for enhancing LM performance and interpretability.</li>
<li><strong>摘要：</strong>在深度学习领域，理解 Transformer 等语言模型 (LM) 的潜在空间对于提高其性能和可解释性至关重要。然而，现有的分析往往无法提供对 LM 语义的绝对且以模型为中心的见解，并且忽略了 LM 适应的基本方面。为此，我们引入了一种称为词汇定义语义的开创性方法，该方法在 LM 潜在空间内建立固定的参考框架，确保基于 LM 词汇的绝对语义分析。我们的方法超越了之前的相关分析，利用 LM 词汇来获得以模型为中心的见解。此外，我们提出了一种计算逻辑的新技术，强调可微性和局部各向同性，并引入神经聚类模块，用于在 LM 适应期间对数据表示进行语义校准。通过对不同文本理解数据集的广泛实验，我们的方法超越了最先进的检索增强生成和参数高效微调方法，展示了其功效和广泛的适用性。我们的研究结果不仅揭示了 LM 机制，还为增强 LM 性能和可解释性提供了实用的解决方案。</li>
</ul>

<h3>Title: MultiMUC: Multilingual Template Filling on MUC-4</h3>
<ul>
<li><strong>Authors: </strong>William Gantt, Shabnam Behzad, Hannah YoungEun An, Yunmo Chen, Aaron Steven White, Benjamin Van Durme, Mahsa Yarmohammadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16209">https://arxiv.org/abs/2401.16209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16209">https://arxiv.org/pdf/2401.16209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16209]] MultiMUC: Multilingual Template Filling on MUC-4(https://arxiv.org/abs/2401.16209)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>We introduce MultiMUC, the first multilingual parallel corpus for template filling, comprising translations of the classic MUC-4 template filling benchmark into five languages: Arabic, Chinese, Farsi, Korean, and Russian. We obtain automatic translations from a strong multilingual machine translation system and manually project the original English annotations into each target language. For all languages, we also provide human translations for sentences in the dev and test splits that contain annotated template arguments. Finally, we present baselines on MultiMUC both with state-of-the-art template filling models and with ChatGPT.</li>
<li><strong>摘要：</strong>我们推出了 MultiMUC，这是第一个用于模板填充的多语言并行语料库，包括将经典 MUC-4 模板填充基准翻译成五种语言：阿拉伯语、中文、波斯语、韩语和俄语。我们从强大的多语言机器翻译系统获得自动翻译，并将原始英语注释手动投影到每种目标语言。对于所有语言，我们还为开发和测试拆分中包含带注释的模板参数的句子提供人工翻译。最后，我们使用最先进的模板填充模型和 ChatGPT 提供 MultiMUC 的基线。</li>
</ul>

<h3>Title: Player Pressure Map - A Novel Representation of Pressure in Soccer for  Evaluating Player Performance in Different Game Contexts</h3>
<ul>
<li><strong>Authors: </strong>Chaoyi Gu, Jiaming Na, Yisheng Pei, Varuna De Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16235">https://arxiv.org/abs/2401.16235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16235">https://arxiv.org/pdf/2401.16235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16235]] Player Pressure Map - A Novel Representation of Pressure in Soccer for  Evaluating Player Performance in Different Game Contexts(https://arxiv.org/abs/2401.16235)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In soccer, contextual player performance metrics are invaluable to coaches. For example, the ability to perform under pressure during matches distinguishes the elite from the average. Appropriate pressure metric enables teams to assess players' performance accurately under pressure and design targeted training scenarios to address their weaknesses. The primary objective of this paper is to leverage both tracking and event data and game footage to capture the pressure experienced by the possession team in a soccer game scene. We propose a player pressure map to represent a given game scene, which lowers the dimension of raw data and still contains rich contextual information. Not only does it serve as an effective tool for visualizing and evaluating the pressure on the team and each individual, but it can also be utilized as a backbone for accessing players' performance. Overall, our model provides coaches and analysts with a deeper understanding of players' performance under pressure so that they make data-oriented tactical decisions.</li>
<li><strong>摘要：</strong>在足球比赛中，球员的表现指标对于教练来说是无价的。例如，在比赛中在压力下表现的能力将精英与普通人区分开来。适当的压力指标使团队能够准确评估球员在压力下的表现，并设计有针对性的训练场景来解决他们的弱点。本文的主要目标是利用跟踪和事件数据以及比赛镜头来捕捉控球球队在足球比赛场景中所经历的压力。我们提出了一个玩家压力图来表示给定的游戏场景，它降低了原始数据的维度，但仍然包含丰富的上下文信息。它不仅可以作为可视化和评估团队和每个人压力的有效工具，而且还可以用作了解球员表现的支柱。总的来说，我们的模型使教练和分析师能够更深入地了解球员在压力下的表现，以便他们做出以数据为导向的战术决策。</li>
</ul>

<h3>Title: Effective Communication with Dynamic Feature Compression</h3>
<ul>
<li><strong>Authors: </strong>Pietro Talli, Francesco Pase, Federico Chiariotti, Andrea Zanella, Michele Zorzi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, cs.MA, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16236">https://arxiv.org/abs/2401.16236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16236">https://arxiv.org/pdf/2401.16236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16236]] Effective Communication with Dynamic Feature Compression(https://arxiv.org/abs/2401.16236)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>The remote wireless control of industrial systems is one of the major use cases for 5G and beyond systems: in these cases, the massive amounts of sensory information that need to be shared over the wireless medium may overload even high-capacity connections. Consequently, solving the effective communication problem by optimizing the transmission strategy to discard irrelevant information can provide a significant advantage, but is often a very complex task. In this work, we consider a prototypal system in which an observer must communicate its sensory data to a robot controlling a task (e.g., a mobile robot in a factory). We then model it as a remote Partially Observable Markov Decision Process (POMDP), considering the effect of adopting semantic and effective communication-oriented solutions on the overall system performance. We split the communication problem by considering an ensemble Vector Quantized Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement Learning (DRL) agent to dynamically adapt the quantization level, considering both the current state of the environment and the memory of past messages. We tested the proposed approach on the well-known CartPole reference control problem, obtaining a significant performance increase over traditional approaches.</li>
<li><strong>摘要：</strong>工业系统的远程无线控制是 5G 及其他系统的主要用例之一：在这些情况下，需要通过无线介质共享的大量传感信息可能会导致高容量连接过载。因此，通过优化传输策略以丢弃不相关信息来解决有效通信问题可以提供显着的优势，但通常是一项非常复杂的任务。在这项工作中，我们考虑一个原型系统，其中观察者必须将其感知数据传递给控制任务的机器人（例如工厂中的移动机器人）。然后，我们将其建模为远程部分可观察马尔可夫决策过程（POMDP），考虑采用语义和有效的面向通信的解决方案对整体系统性能的影响。我们通过考虑集成矢量量化变分自动编码器（VQ-VAE）编码来分割通信问题，并训练深度强化学习（DRL）代理来动态调整量化级别，同时考虑环境的当前状态和过去的记忆消息。我们在著名的 CartPole 参考控制问题上测试了所提出的方法，与传统方法相比，性能显着提高。</li>
</ul>

<h3>Title: Clinically meaningful timeline summarisation in social media for mental  health monitoring</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Song, Jenny Chim, Adam Tsakalidis, Julia Ive, Dana Atzil-Slonim, Maria Liakata</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16240">https://arxiv.org/abs/2401.16240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16240">https://arxiv.org/pdf/2401.16240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16240]] Clinically meaningful timeline summarisation in social media for mental  health monitoring(https://arxiv.org/abs/2401.16240)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, code</a></li>
<li><strong>Abstract: </strong>We introduce the new task of clinically meaningful summarisation of social media user timelines, appropriate for mental health monitoring. We develop a novel approach for unsupervised abstractive summarisation that produces a two-layer summary consisting of both high-level information, covering aspects useful to clinical experts, as well as accompanying time sensitive evidence from a user's social media timeline. A key methodological novelty comes from the timeline summarisation component based on a version of hierarchical variational autoencoder (VAE) adapted to represent long texts and guided by LLM-annotated key phrases. The resulting timeline summary is input into a LLM (LLaMA-2) to produce the final summary containing both the high level information, obtained through instruction prompting, as well as corresponding evidence from the user's timeline. We assess the summaries generated by our novel architecture via automatic evaluation against expert written summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time.</li>
<li><strong>摘要：</strong>我们引入了一项新任务，即对社交媒体用户时间线进行具有临床意义的总结，适用于心理健康监测。我们开发了一种用于无监督抽象摘要的新颖方法，该方法生成两层摘要，其中包含高级信息，涵盖对临床专家有用的方面，以及来自用户社交媒体时间线的时间敏感证据。一个关键的方法论新颖性来自于时间线摘要组件，该组件基于分层变分自动编码器（VAE）版本，适合表示长文本并由法学硕士注释的关键短语引导。将生成的时间线摘要输入到 LLM (LLaMA-2)，以生成包含通过指令提示获得的高级信息以及来自用户时间线的相应证据的最终摘要。我们通过对专家书面摘要的自动评估以及与临床专家的人工评估来评估我们的新颖架构生成的摘要，结果表明 TH-VAE 的时间线摘要会产生逻辑连贯的摘要，具有丰富的临床实用性，并且在捕获方面优于仅法学硕士的方法随着时间的推移而变化。</li>
</ul>

<h3>Title: Employing Iterative Feature Selection in Fuzzy Rule-Based Binary  Classification</h3>
<ul>
<li><strong>Authors: </strong>Haoning Li, Cong Wang, Qinghua Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16244">https://arxiv.org/abs/2401.16244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16244">https://arxiv.org/pdf/2401.16244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16244]] Employing Iterative Feature Selection in Fuzzy Rule-Based Binary  Classification(https://arxiv.org/abs/2401.16244)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The feature selection in a traditional binary classification algorithm is always used in the stage of dataset preprocessing, which makes the obtained features not necessarily the best ones for the classification algorithm, thus affecting the classification performance. For a traditional rule-based binary classification algorithm, classification rules are usually deterministic, which results in the fuzzy information contained in the rules being ignored. To do so, this paper employs iterative feature selection in fuzzy rule-based binary classification. The proposed algorithm combines feature selection based on fuzzy correlation family with rule mining based on biclustering. It first conducts biclustering on the dataset after feature selection. Then it conducts feature selection again for the biclusters according to the feedback of biclusters evaluation. In this way, an iterative feature selection framework is build. During the iteration process, it stops until the obtained bicluster meets the requirements. In addition, the rule membership function is introduced to extract vectorized fuzzy rules from the bicluster and construct weak classifiers. The weak classifiers with good classification performance are selected by Adaptive Boosting and the strong classifier is constructed by "weighted average". Finally, we perform the proposed algorithm on different datasets and compare it with other peers. Experimental results show that it achieves good classification performance and outperforms its peers.</li>
<li><strong>摘要：</strong>传统二分类算法中的特征选择总是在数据集预处理阶段使用，这使得获得的特征不一定是分类算法的最佳特征，从而影响分类性能。对于传统的基于规则的二元分类算法，分类规则通常是确定性的，这导致规则中包含的模糊信息被忽略。为此，本文在基于模糊规则的二元分类中采用迭代特征选择。该算法将基于模糊相关族的特征选择与基于双聚类的规则挖掘相结合。它首先在特征选择后对数据集进行双聚类。然后根据双簇评估的反馈再次对双簇进行特征选择。这样就构建了一个迭代的特征选择框架。迭代过程中，直至得到的双簇满足要求时停止。此外，引入规则隶属函数从双簇中提取向量化模糊规则并构造弱分类器。通过Adaptive Boosting选择分类性能好的弱分类器，通过“加权平均”构造强分类器。最后，我们在不同的数据集上执行所提出的算法，并将其与其他同行进行比较。实验结果表明，它取得了良好的分类性能，优于同类产品。</li>
</ul>

<h3>Title: CO2: Efficient Distributed Training with Full Communication-Computation  Overlap</h3>
<ul>
<li><strong>Authors: </strong>Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16265">https://arxiv.org/abs/2401.16265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16265">https://arxiv.org/pdf/2401.16265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16265]] CO2: Efficient Distributed Training with Full Communication-Computation  Overlap(https://arxiv.org/abs/2401.16265)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.</li>
<li><strong>摘要：</strong>大型语言模型的根本成功取决于大规模分布式训练技术的有效实施。然而，建立一个具有高速通信互连的庞大、高性能集群成本高昂，而且只有知名实体才能访问。在这项工作中，我们的目标是降低这一障碍，并使有限带宽集群的大规模训练民主化。我们提出了一种称为 CO2 的新方法，它将本地更新和异步通信引入分布式数据并行训练，从而促进通信与计算的完全重叠。即使在通信带宽非常有限的广泛多节点集群上，CO2 也能够实现高可扩展性。我们进一步提出了陈旧间隙惩罚和外部动量裁剪技术以及 CO2，以增强其收敛性和训练稳定性。此外，CO2 与成熟的 ZeRO 系列优化器无缝集成，可通过大型模型训练减少模型状态的内存消耗。我们还提供了收敛性的数学证明，并建立了严格的上限。此外，我们通过一系列广泛的实际实验验证了我们的发现，这些实验涵盖了计算机视觉和自然语言处理领域的广泛任务。这些实验旨在展示 CO2 在包含多达 128 个 A100 GPU 的配置中部署时在收敛、泛化和可扩展性方面的能力。结果强调了 CO2 的出色能力，无论是在具有 800Gbps RDMA 还是 80Gbps TCP/IP 节点间连接的集群上，都可以极大地提高可扩展性。</li>
</ul>

<h3>Title: MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim  Verification</h3>
<ul>
<li><strong>Authors: </strong>Xia Zeng, Arkaitz Zubiaga</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16282">https://arxiv.org/abs/2401.16282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16282">https://arxiv.org/pdf/2401.16282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16282]] MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim  Verification(https://arxiv.org/abs/2401.16282)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence. In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision. We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure. Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources. MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE</li>
<li><strong>摘要：</strong>索赔验证是自动事实检查管道中的重要步骤，它根据证据评估索赔的真实性。在这项工作中，我们探索了少样本声明验证的潜力，其中只有非常有限的数据可用于监督。我们提出了 MAPLE（成对语言进化的微观分析），这是一种开创性的方法，通过小型 seq2seq 模型和新颖的语义度量来探索主张与其证据之间的一致性。其对微语言演化路径的创新利用，利用未标记的成对数据来促进索赔验证，同时对数据注释和计算资源的需求较低。 MAPLE 在三个事实检查数据集（FEVER、Climate FEVER 和 SciFact）上展示了相对于 SOTA 基线 SEED、PET 和 LLaMA 2 的显着性能改进。数据和代码可以在这里找到：https://github.com/XiaZeng0223/MAPLE</li>
</ul>

<h3>Title: Capturing Pertinent Symbolic Features for Enhanced Content-Based  Misinformation Detection</h3>
<ul>
<li><strong>Authors: </strong>Flavio Merenda, José Manuel Gómez-Pérez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16285">https://arxiv.org/abs/2401.16285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16285">https://arxiv.org/pdf/2401.16285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16285]] Capturing Pertinent Symbolic Features for Enhanced Content-Based  Misinformation Detection(https://arxiv.org/abs/2401.16285)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Preventing the spread of misinformation is challenging. The detection of misleading content presents a significant hurdle due to its extreme linguistic and domain variability. Content-based models have managed to identify deceptive language by learning representations from textual data such as social media posts and web articles. However, aggregating representative samples of this heterogeneous phenomenon and implementing effective real-world applications is still elusive. Based on analytical work on the language of misinformation, this paper analyzes the linguistic attributes that characterize this phenomenon and how representative of such features some of the most popular misinformation datasets are. We demonstrate that the appropriate use of pertinent symbolic knowledge in combination with neural language models is helpful in detecting misleading content. Our results achieve state-of-the-art performance in misinformation datasets across the board, showing that our approach offers a valid and robust alternative to multi-task transfer learning without requiring any additional training data. Furthermore, our results show evidence that structured knowledge can provide the extra boost required to address a complex and unpredictable real-world problem like misinformation detection, not only in terms of accuracy but also time efficiency and resource utilization.</li>
<li><strong>摘要：</strong>防止错误信息的传播具有挑战性。由于其极端的语言和领域可变性，误导性内容的检测提出了重大障碍。基于内容的模型通过学习社交媒体帖子和网络文章等文本数据的表示，成功地识别了欺骗性语言。然而，聚合这种异构现象的代表性样本并实现有效的现实应用仍然难以实现。基于对错误信息语言的分析工作，本文分析了表征这种现象的语言属性，以及一些最流行的错误信息数据集如何代表这些特征。我们证明，适当使用相关符号知识与神经语言模型相结合有助于检测误导性内容。我们的结果在错误信息数据集中实现了最先进的性能，这表明我们的方法为多任务迁移学习提供了有效且强大的替代方案，而无需任何额外的训练数据。此外，我们的结果表明，结构化知识可以提供解决复杂且不可预测的现实问题（例如错误信息检测）所需的额外提升，不仅在准确性方面，而且在时间效率和资源利用率方面。</li>
</ul>

<h3>Title: GAPS: Geometry-Aware Problem Solver</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Zhang, Yinghui Jiang, Yashar Moshfeghi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16287">https://arxiv.org/abs/2401.16287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16287">https://arxiv.org/pdf/2401.16287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16287]] GAPS: Geometry-Aware Problem Solver(https://arxiv.org/abs/2401.16287)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Geometry problem solving presents a formidable challenge within the NLP community. Existing approaches often rely on models designed for solving math word problems, neglecting the unique characteristics of geometry math problems. Additionally, the current research predominantly focuses on geometry calculation problems, while overlooking other essential aspects like proving. In this study, we address these limitations by proposing the Geometry-Aware Problem Solver (GAPS) model. GAPS is specifically designed to generate solution programs for geometry math problems of various types with the help of its unique problem-type classifier. To achieve this, GAPS treats the solution program as a composition of operators and operands, segregating their generation processes. Furthermore, we introduce the geometry elements enhancement method, which enhances the ability of GAPS to recognize geometry elements accurately. By leveraging these improvements, GAPS showcases remarkable performance in resolving geometry math problems. Our experiments conducted on the UniGeo dataset demonstrate the superiority of GAPS over the state-of-the-art model, Geoformer. Specifically, GAPS achieves an accuracy improvement of more than 5.3% for calculation tasks and an impressive 41.1% for proving tasks. Notably, GAPS achieves an impressive accuracy of 97.5% on proving problems, representing a significant advancement in solving geometry proving tasks.</li>
<li><strong>摘要：</strong>几何问题的解决在 NLP 社区中提出了巨大的挑战。现有的方法通常依赖于为解决数学应用问题而设计的模型，而忽略了几何数学问题的独特特征。此外，当前的研究主要集中在几何计算问题上，而忽略了证明等其他重要方面。在本研究中，我们通过提出几何感知问题求解器（GAPS）模型来解决这些局限性。 GAPS 专门设计用于借助其独特的问题类型分类器生成各种类型的几何数学问题的解决方案。为了实现这一点，GAPS 将解决方案视为运算符和操作数的组合，隔离它们的生成过程。此外，我们还引入了几何元素增强方法，增强了GAPS准确识别几何元素的能力。通过利用这些改进，GAPS 在解决几何数学问题方面展现出了卓越的性能。我们在 UniGeo 数据集上进行的实验证明了 GAPS 相对于最先进的模型 Geoformer 的优越性。具体来说，GAPS 在计算任务上实现了超过 5.3% 的精度提升，在证明任务上实现了令人印象深刻的 41.1% 的精度提升。值得注意的是，GAPS 在证明问题上达到了令人印象深刻的 97.5% 的准确率，代表着在解决几何证明任务方面的重大进步。</li>
</ul>

<h3>Title: Textual Entailment for Effective Triple Validation in Object Prediction</h3>
<ul>
<li><strong>Authors: </strong>Andrés García-Silva, Cristian Berrío, José Manuel Gómez-Pérez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16293">https://arxiv.org/abs/2401.16293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16293">https://arxiv.org/pdf/2401.16293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16293]] Textual Entailment for Effective Triple Validation in Object Prediction(https://arxiv.org/abs/2401.16293)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Knowledge base population seeks to expand knowledge graphs with facts that are typically extracted from a text corpus. Recently, language models pretrained on large corpora have been shown to contain factual knowledge that can be retrieved using cloze-style strategies. Such approach enables zero-shot recall of facts, showing competitive results in object prediction compared to supervised baselines. However, prompt-based fact retrieval can be brittle and heavily depend on the prompts and context used, which may produce results that are unintended or hallucinatory.We propose to use textual entailment to validate facts extracted from language models through cloze statements. Our results show that triple validation based on textual entailment improves language model predictions in different training regimes. Furthermore, we show that entailment-based triple validation is also effective to validate candidate facts extracted from other sources including existing knowledge graphs and text passages where named entities are recognized.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: Dual feature-based and example-based explanation methods</h3>
<ul>
<li><strong>Authors: </strong>Andrei V. Konstantinov, Boris V. Kozlov, Stanislav R. Kirpichenko, Lev V. Utkin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16294">https://arxiv.org/abs/2401.16294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16294">https://arxiv.org/pdf/2401.16294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16294]] Dual feature-based and example-based explanation methods(https://arxiv.org/abs/2401.16294)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>A new approach to the local and global explanation is proposed. It is based on selecting a convex hull constructed for the finite number of points around an explained instance. The convex hull allows us to consider a dual representation of instances in the form of convex combinations of extreme points of a produced polytope. Instead of perturbing new instances in the Euclidean feature space, vectors of convex combination coefficients are uniformly generated from the unit simplex, and they form a new dual dataset. A dual linear surrogate model is trained on the dual dataset. The explanation feature importance values are computed by means of simple matrix calculations. The approach can be regarded as a modification of the well-known model LIME. The dual representation inherently allows us to get the example-based explanation. The neural additive model is also considered as a tool for implementing the example-based explanation approach. Many numerical experiments with real datasets are performed for studying the approach. The code of proposed algorithms is available.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: Enhancing Molecular Property Prediction with Auxiliary Learning and  Task-Specific Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Vishal Dey, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16299">https://arxiv.org/abs/2401.16299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16299">https://arxiv.org/pdf/2401.16299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16299]] Enhancing Molecular Property Prediction with Auxiliary Learning and  Task-Specific Adaptation(https://arxiv.org/abs/2401.16299)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Pretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks. Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization. To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks. This could enable the GNNs to learn both general and task-specific features, which may benefit the target task. However, a major challenge is to determine the relatedness of auxiliary tasks with the target task. To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization. Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients ($\mathtt{RCGrad}$), that learns to align conflicting auxiliary task gradients through rotation. Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning. This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction.</li>
<li><strong>摘要：</strong>预训练的图神经网络已广泛应用于各种分子特性预测任务。尽管它们能够编码分子的结构和关系特征，但此类预训练 GNN 在目标任务上的传统微调可能会导致泛化能力较差。为了解决这个问题，我们通过将预训练的 GNN 与多个辅助任务联合训练来探索预训练的 GNN 对目标任务的适应。这可以使 GNN 学习一般特征和特定于任务的特征，这可能有利于目标任务。然而，一个主要的挑战是确定辅助任务与目标任务的相关性。为了解决这个问题，我们研究了多种策略来衡量辅助任务的相关性，并通过自适应地组合任务梯度或通过双层优化学习任务权重来整合这些任务。此外，我们提出了一种基于梯度手术的新颖方法，冲突梯度旋转（$\mathtt{RCGrad}$），它学习通过旋转来对齐冲突的辅助任务梯度。我们使用最先进的预训练 GNN 进行的实验证明了我们提出的方法的有效性，与微调相比提高了高达 7.7%。这表明将辅助任务与目标任务微调结合起来可以是提高预训练 GNN 分子属性预测泛化能力的有效方法。</li>
</ul>

<h3>Title: Machine Translation Meta Evaluation through Translation Accuracy  Challenge Sets</h3>
<ul>
<li><strong>Authors: </strong>Nikita Moghe, Arnisa Fazla, Chantal Amrhein, Tom Kocmi, Mark Steedman, Alexandra Birch, Rico Sennrich, Liane Guillou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16313">https://arxiv.org/abs/2401.16313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16313">https://arxiv.org/pdf/2401.16313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16313]] Machine Translation Meta Evaluation through Translation Accuracy  Challenge Sets(https://arxiv.org/abs/2401.16313)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgement but without any insights about their behaviour across different error types. Challenge sets are used to probe specific dimensions of metric behaviour but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs. We introduce ACES, a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors. These phenomena range from simple alterations at the word/character level to more complex errors based on discourse and real-world knowledge. We conduct a large-scale study by benchmarking ACES on 50 metrics submitted to the WMT 2022 and 2023 metrics shared tasks. We benchmark metric performance, assess their incremental performance over successive campaigns, and measure their sensitivity to a range of linguistic phenomena. We also investigate claims that Large Language Models (LLMs) are effective as MT evaluators by evaluating on ACES. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods fail to demonstrate reliable performance. Our analyses indicate that most metrics ignore the source sentence, tend to prefer surface-level overlap and end up incorporating properties of base models which are not always beneficial. We expand ACES to include error span annotations, denoted as SPAN-ACES and we use this dataset to evaluate span-based error metrics showing these metrics also need considerable improvement. Finally, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing strategies to explicitly focus on the source sentence, focusing on semantic content and choosing the right base model for representations.</li>
<li><strong>摘要：</strong>最近的机器翻译（MT）指标通过与人类判断相关，但没有对不同误差类型的行为有任何洞察力来校准其有效性。挑战集用于探测指标行为的特定维度，但是这样的数据集很少，它们要么集中于有限数量的现象或有限数量的语言对。我们介绍了ACE，这是一个跨越146对语言对的对比挑战，旨在发现指标是否可以识别68个翻译精度错误。这些现象的范围从单词/字符级别的简单更改到基于话语和现实世界知识的更复杂的错误。我们进行了一项大规模研究，对提交给 WMT 2022 和 2023 指标共享任务的 50 个指标进行 ACES 基准测试。我们基于度量标准性能，评估他们对连续运动的增量表现，并衡量它们对一系列语言现象的敏感性。我们还通过在 ACES 上进行评估来调查大型语言模型 (LLM) 作为 MT 评估器的有效性。我们的结果表明，不同的公制家庭在不同现象中挣扎，基于LLM的方法无法证明可靠的性能。我们的分析表明，大多数指标忽略了源句子，倾向于偏爱表面级别的重叠，最终融合了并非总是有益的基本模型的属性。我们将ACE扩展到包括错误跨度注释，称为SPAN-ack，并使用此数据集评估基于跨度的错误指标，显示这些指标也需要大量改进。最后，我们提供了一组建议，以构建更好的MT指标，包括专注于错误标签而不是分数，结合，设计策略，以明确专注于源句子，专注于语义内容并为表示形式选择正确的基础模型。</li>
</ul>

<h3>Title: Defining and Extracting generalizable interaction primitives from DNNs</h3>
<ul>
<li><strong>Authors: </strong>Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16318">https://arxiv.org/abs/2401.16318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16318">https://arxiv.org/pdf/2401.16318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16318]] Defining and Extracting generalizable interaction primitives from DNNs(https://arxiv.org/abs/2401.16318)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.</li>
<li><strong>摘要：</strong>将深度神经网络 (DNN) 编码的知识忠实地总结为一些符号原始模式而不丢失太多信息，这是可解释人工智能的核心挑战。为此，任等人。 (2023c) 导出了一系列定理来证明 DNN 的推理分数可以解释为输入变量之间的一小组相互作用。 However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.</li>
</ul>

<h3>Title: Tradeoffs Between Alignment and Helpfulness in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yotam Wolf, Noam Wies, Dorin Shteyman, Binyamin Rothberg, Yoav Levine, Amnon Shashua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16332">https://arxiv.org/abs/2401.16332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16332">https://arxiv.org/pdf/2401.16332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16332]] Tradeoffs Between Alignment and Helpfulness in Language Models(https://arxiv.org/abs/2401.16332)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally decreases, it does so quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment.</li>
<li><strong>摘要：</strong>语言模型对齐已成为人工智能安全的重要组成部分，通过增强期望的行为并抑制不良行为，允许人类和语言模型之间的安全交互。通常通过调整模型或插入预设对齐提示来完成。最近，表示工程（一种通过改变训练后表示来改变模型行为的方法）被证明可以有效地调整 LLM（Zou 等人，2023a）。表示工程在面向对齐的任务中产生了收益，例如抵抗对抗性攻击和减少社会偏见，但也被证明会导致模型执行基本任务的能力下降。在本文中，我们研究了模型一致性增加和有用性降低之间的权衡。我们提出了一个理论框架，为这两个量提供了界限，并通过经验证明了它们的相关性。有趣的是，我们发现虽然有用性通常会降低，但它与表示工程向量的范数呈二次方关系，而对齐度则随之线性增加，这表明使用表示工程是有效的。我们根据经验验证了我们的发现，并绘制了表示工程对于对齐的有用性的界限。</li>
</ul>

<h3>Title: Iterative Data Smoothing: Mitigating Reward Overfitting and  Overoptimization in RLHF</h3>
<ul>
<li><strong>Authors: </strong>Banghua Zhu, Michael I. Jordan, Jiantao Jiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16335">https://arxiv.org/abs/2401.16335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16335">https://arxiv.org/pdf/2401.16335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16335]] Iterative Data Smoothing: Mitigating Reward Overfitting and  Overoptimization in RLHF(https://arxiv.org/abs/2401.16335)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.</li>
<li><strong>摘要：</strong>人类反馈强化学习 (RLHF) 是一种关键技术，它将语言模型与以人为中心的价值观紧密结合起来。 RLHF 的初始阶段涉及使用排名数据中的奖励模型来学习人类价值观。据观察，奖励模型的性能在经过一个时期的训练后会下降，并且针对学习的奖励模型进行过多优化最终会阻碍真正的目标。本文深入研究了这些问题，利用理论见解来设计改进的奖励学习算法，称为“迭代数据平滑”（IDS）。核心思想是，在每个训练周期中，我们不仅用数据更新模型，还使用模型更新日期，用软标签替换硬标签。我们的实证研究结果强调了这种方法相对于传统方法的优越性能。</li>
</ul>

<h3>Title: FedFair^3: Unlocking Threefold Fairness in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Simin Javaherian, Sanjeev Panta, Shelby Williams, Md Sirajul Islam, Li Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16350">https://arxiv.org/abs/2401.16350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16350">https://arxiv.org/pdf/2401.16350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16350]] FedFair^3: Unlocking Threefold Fairness in Federated Learning(https://arxiv.org/abs/2401.16350)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an emerging paradigm in machine learning without exposing clients' raw data. In practical scenarios with numerous clients, encouraging fair and efficient client participation in federated learning is of utmost importance, which is also challenging given the heterogeneity in data distribution and device properties. Existing works have proposed different client-selection methods that consider fairness; however, they fail to select clients with high utilities while simultaneously achieving fair accuracy levels. In this paper, we propose a fair client-selection approach that unlocks threefold fairness in federated learning. In addition to having a fair client-selection strategy, we enforce an equitable number of rounds for client participation and ensure a fair accuracy distribution over the clients. The experimental results demonstrate that FedFair^3, in comparison to the state-of-the-art baselines, achieves 18.15% less accuracy variance on the IID data and 54.78% on the non-IID data, without decreasing the global accuracy. Furthermore, it shows 24.36% less wall-clock training time on average.</li>
<li><strong>摘要：</strong>联邦学习 (FL) 是机器学习中的一种新兴范例，无需暴露客户的原始数据。在拥有众多客户端的实际场景中，鼓励客户端公平高效地参与联邦学习至关重要，但考虑到数据分布和设备属性的异构性，这也具有挑战性。现有的工作提出了不同的考虑公平性的客户选择方法；然而，他们未能选择具有高效用的客户，同时达到公平的准确性水平。在本文中，我们提出了一种公平的客户选择方法，可以实现联邦学习的三重公平。除了制定公平的客户选择策略外，我们还强制执行公平的客户参与轮数，并确保客户的公平准确性分配。实验结果表明，与最先进的基线相比，FedFair^3 在 IID 数据上的精度方差降低了 18.15%，在非 IID 数据上的精度方差降低了 54.78%，而没有降低全局精度。此外，挂钟训练时间平均减少了 24.36%。</li>
</ul>

<h3>Title: TQCompressor: improving tensor decomposition methods in neural networks  via permutations</h3>
<ul>
<li><strong>Authors: </strong>V. Abronin, A. Naumov, D. Mazur, D. Bystrov, K. Tsarova, Ar. Melnikov, I. Oseledets, S. Dolgov, R. Brasher, M. Perelshtein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16367">https://arxiv.org/abs/2401.16367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16367">https://arxiv.org/pdf/2401.16367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16367]] TQCompressor: improving tensor decomposition methods in neural networks  via permutations(https://arxiv.org/abs/2401.16367)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, rag</a></li>
<li><strong>Abstract: </strong>We introduce TQCompressor, a novel method for neural network model compression with improved tensor decompositions. We explore the challenges posed by the computational and storage demands of pre-trained language models in NLP tasks and propose a permutation-based enhancement to Kronecker decomposition. This enhancement makes it possible to reduce loss in model expressivity which is usually associated with factorization. We demonstrate this method applied to the GPT-2$_{small}$. The result of the compression is TQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in the GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further enhance the performance of the TQCompressedGPT-2 through a training strategy involving multi-step knowledge distillation, using only a 3.1% of the OpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative evaluations, marking an advancement in the efficient and effective deployment of models in resource-constrained environments.</li>
<li><strong>摘要：</strong>我们介绍 TQCompressor，这是一种改进张量分解的神经网络模型压缩新方法。我们探讨了 NLP 任务中预训练语言模型的计算和存储需求所带来的挑战，并提出了一种基于排列的克罗内克分解增强方法。这种增强可以减少通常与因式分解相关的模型表达能力的损失。我们演示了应用于 GPT-2$_{small}$ 的方法。压缩的结果是TQCompressedGPT-2模型，特征为8100万。参数与 1.24 亿相比。在 GPT-2$_{小}$ 中。我们公开提供 TQCompressedGPT-2。我们通过涉及多步知识蒸馏的训练策略进一步增强了 TQCompressedGPT-2 的性能，仅使用了 3.1% 的 OpenWebText。 TQCompressedGPT-2 在对比评估中超越了 DistilGPT-2 和 KnGPT-2，标志着在资源受限环境中高效部署模型的进步。</li>
</ul>

<h3>Title: Rephrasing the Web: A Recipe for Compute and Data-Efficient Language  Modeling</h3>
<ul>
<li><strong>Authors: </strong>Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16380">https://arxiv.org/abs/2401.16380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16380">https://arxiv.org/pdf/2401.16380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16380]] Rephrasing the Web: A Recipe for Compute and Data-Efficient Language  Modeling(https://arxiv.org/abs/2401.16380)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as "like Wikipedia" or in "question-answer format" to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data.</li>
<li><strong>摘要：</strong>大型语言模型是在大量网络碎片上进行训练的，这些网络碎片通常是非结构化的、嘈杂的、措辞糟糕的。当前的缩放定律表明，从此类数据中学习需要大量的计算和数据，而这些计算和数据随着所训练的模型的大小而增长。这是不可行的，因为与预训练相关的大量计算成本和持续时间，以及网络上即将出现的高质量数据的稀缺。在这项工作中，我们提出了 Web Rephrase Augmented Pre-training ($\textbf{WRAP}$)，它使用现成的指令调整模型，提示以特定风格解释网络上的文档，例如“像维基百科”或以“问答格式”联合对法学硕士进行真实和合成改写的预训练。首先，我们表明，在自然有噪声的 C4 数据集上使用 WRAP，可以将预训练速度加快 $\sim3x$。在相同的预训练计算预算下，它在 Pile 的不同子集上平均将困惑度提高了 10% 以上，并将 13 个任务的零样本问题答案准确性提高了 2% 以上。其次，我们研究了重新措辞风格对模型性能的影响，深入了解训练数据的组成如何影响 OOD 设置中法学硕士的性能。我们的收益归因于这样一个事实：重新表述的合成数据比真实数据具有更高的效用，因为它（i）包含密切反映下游评估风格的风格多样性，并且（ii）比网络抓取的数据具有更高的“质量”。</li>
</ul>

<h3>Title: Continual Learning with Pre-Trained Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16386">https://arxiv.org/abs/2401.16386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16386">https://arxiv.org/pdf/2401.16386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16386]] Continual Learning with Pre-Trained Models: A Survey(https://arxiv.org/abs/2401.16386)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves. Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones. Typical CL methods build the model from scratch to grow with incoming data. However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities. This paper presents a comprehensive survey of the latest advancements in PTM-based CL. We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages. Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons. The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT</li>
<li><strong>摘要：</strong>如今，现实世界的应用程序经常面对流数据，这需要学习系统随着数据的演变吸收新的知识。持续学习（CL）旨在实现这一目标，同时克服学习新知识时对旧知识的灾难性遗忘。典型的 CL 方法从头开始构建模型，并随着传入数据而增长。然而，预训练模型 (PTM) 时代的到来引发了巨大的研究兴趣，特别是在利用 PTM 强大的表征能力方面。本文对基于 PTM 的 CL 的最新进展进行了全面的调查。我们将现有方法分为三个不同的组，对它们的相似点、差异以及各自的优缺点进行比较分析。此外，我们还提供了一项对比各种最先进方法的实证研究，以强调对比较公平性的担忧。重现这些评估的源代码位于：https://github.com/sun-hailong/LAMDA-PILOT</li>
</ul>

<h3>Title: Zero-shot Imitation Policy via Search in Demonstration Dataset</h3>
<ul>
<li><strong>Authors: </strong>Federco Malato, Florian Leopold, Andrew Melnik, Ville Hautamaki</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16398">https://arxiv.org/abs/2401.16398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16398">https://arxiv.org/pdf/2401.16398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16398]] Zero-shot Imitation Policy via Search in Demonstration Dataset(https://arxiv.org/abs/2401.16398)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Behavioral cloning uses a dataset of demonstrations to learn a policy. To overcome computationally expensive training procedures and address the policy adaptation problem, we propose to use latent spaces of pre-trained foundation models to index a demonstration dataset, instantly access similar relevant experiences, and copy behavior from these situations. Actions from a selected similar situation can be performed by the agent until representations of the agent's current situation and the selected experience diverge in the latent space. Thus, we formulate our control problem as a dynamic search problem over a dataset of experts' demonstrations. We test our approach on BASALT MineRL-dataset in the latent representation of a Video Pre-Training model. We compare our model to state-of-the-art, Imitation Learning-based Minecraft agents. Our approach can effectively recover meaningful demonstrations and show human-like behavior of an agent in the Minecraft environment in a wide variety of scenarios. Experimental results reveal that performance of our search-based approach clearly wins in terms of accuracy and perceptual evaluation over learning-based models.</li>
<li><strong>摘要：</strong>行为克隆使用演示数据集来学习策略。为了克服计算成本高昂的训练过程并解决策略适应问题，我们建议使用预训练基础模型的潜在空间来索引演示数据集，立即访问类似的相关经验，并从这些情况中复制行为。代理可以执行来自选定的类似情况的动作，直到代理当前情况的表示和选定的经验在潜在空间中出现分歧。因此，我们将控制问题表述为专家演示数据集上的动态搜索问题。我们在视频预训练模型的潜在表示中的 BASALT MineRL 数据集上测试了我们的方法。我们将我们的模型与最先进的、基于模仿学习的 Minecraft 代理进行比较。我们的方法可以有效地恢复有意义的演示，并在 Minecraft 环境中的各种场景中展示代理的类人行为。实验结果表明，我们基于搜索的方法的性能在准确性和感知评估方面明显优于基于学习的模型。</li>
</ul>

<h3>Title: Scaling Sparse Fine-Tuning to Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alan Ansell, Ivan Vulić, Hannah Sterz, Anna Korhonen, Edoardo M. Ponti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16405">https://arxiv.org/abs/2401.16405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16405">https://arxiv.org/pdf/2401.16405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16405]] Scaling Sparse Fine-Tuning to Large Language Models(https://arxiv.org/abs/2401.16405)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, code</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning (SFT) methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any given time, for a desired density level, we maintain an array of parameter indices and the deltas of these parameters relative to their pretrained values. We iterate among: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LLMs on standard dataset mixtures, finding that SFT is often superior to popular parameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in terms of performance and comparable in terms of run time. We additionally show that SFT is compatible with both quantization and efficient optimizers, to facilitate scaling to ever-larger model sizes. We release the code for SFT at https://github.com/AlanAnsell/peft and for the instruction-tuning experiments at https://github.com/ducdauge/sft-llm.</li>
<li><strong>摘要：</strong>由于参数数量庞大，大型语言模型 (LLM) 很难完全微调（例如，通过指令或人工反馈）。一系列参数高效的稀疏微调 (SFT) 方法已被证明在性能方面很有前景，但它们的内存需求与 LLM 的大小成比例地增加。在这项工作中，我们将稀疏微调扩展到最先进的 LLM，例如 LLaMA 2 7B 和 13B。在任何给定时间，对于所需的密度水平，我们维护一组参数索引以及这些参数相对于其预训练值的增量。我们迭代：（a）更新活跃的增量，（b）修剪索引（基于其增量的大小变化）和（c）索引的重新增长。对于再生，我们基于一些候选参数的累积梯度或使用高效 SM3 优化器估计的近似动量来探索两个标准。我们在标准数据集混合上对 LLM 进行指令调整进行实验，发现 SFT 在性能方面通常优于 LoRA（低秩自适应）等流行的参数高效微调方法，并且在运行时间方面具有可比性。我们还表明，SFT 与量化和高效优化器兼容，以促进扩展到更大的模型大小。我们在 https://github.com/AlanAnsell/peft 发布了 SFT 代码，在 https://github.com/ducdauge/sft-llm 发布了指令调优实验的代码。</li>
</ul>

<h3>Title: Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length  Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang, Di He, Jingjing Xu, Zhi Zhang, Hongxia Yang, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16421">https://arxiv.org/abs/2401.16421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16421">https://arxiv.org/pdf/2401.16421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16421]] Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length  Extrapolation(https://arxiv.org/abs/2401.16421)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.</li>
<li><strong>摘要：</strong>在这项工作中，我们利用语言序列的内在分割，设计了一种新的位置编码方法，称为双层位置编码（BiPE）。对于每个位置，我们的 BiPE 混合了段内编码和段间编码。片段内编码识别片段内的位置，并帮助模型通过绝对位置编码捕获其中的语义信息。段间编码指定段索引，对段之间的关系进行建模，并旨在通过相对位置编码提高外推能力。理论分析表明，位置信息的这种解开使学习更加有效。实证结果还表明，我们的 BiPE 在不同文本模式的各种任务中具有卓越的长度外推能力。</li>
</ul>

<h3>Title: Strategic Usage in a Multi-Learner Setting</h3>
<ul>
<li><strong>Authors: </strong>Eliot Shekhtman, Sarah Dean</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16422">https://arxiv.org/abs/2401.16422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16422">https://arxiv.org/pdf/2401.16422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16422]] Strategic Usage in a Multi-Learner Setting(https://arxiv.org/abs/2401.16422)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Real-world systems often involve some pool of users choosing between a set of services. With the increase in popularity of online learning algorithms, these services can now self-optimize, leveraging data collected on users to maximize some reward such as service quality. On the flipside, users may strategically choose which services to use in order to pursue their own reward functions, in the process wielding power over which services can see and use their data. Extensive prior research has been conducted on the effects of strategic users in single-service settings, with strategic behavior manifesting in the manipulation of observable features to achieve a desired classification; however, this can often be costly or unattainable for users and fails to capture the full behavior of multi-service dynamic systems. As such, we analyze a setting in which strategic users choose among several available services in order to pursue positive classifications, while services seek to minimize loss functions on their observations. We focus our analysis on realizable settings, and show that naive retraining can still lead to oscillation even if all users are observed at different times; however, if this retraining uses memory of past observations, convergent behavior can be guaranteed for certain loss function classes. We provide results obtained from synthetic and real-world data to empirically validate our theoretical findings.</li>
<li><strong>摘要：</strong>现实世界的系统通常涉及一些用户在一组服务之间进行选择。随着在线学习算法的日益普及，这些服务现在可以自我优化，利用收集的用户数据来最大化服务质量等奖励。另一方面，用户可以战略性地选择使用哪些服务来追求自己的奖励功能，在此过程中掌握哪些服务可以查看和使用他们的数据。先前已经对单一服务环境中战略用户的影响进行了广泛的研究，战略行为体现在操纵可观察的特征以实现所需的分类；然而，这对于用户来说通常成本高昂或无法实现，并且无法捕获多服务动态系统的完整行为。因此，我们分析了一种设置，其中战略用户在几种可用服务中进行选择以追求积极的分类，而服务则寻求最小化其观察的损失函数。我们将分析重点放在可实现的设置上，并表明即使在不同时间观察所有用户，幼稚的再训练仍然会导致振荡；然而，如果这种再训练使用过去观察的记忆，则可以保证某些损失函数类别的收敛行为。我们提供从合成和现实数据中获得的结果，以实证验证我们的理论发现。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
