<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-18</h1>
<h3>Title: EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research</h3>
<ul>
<li><strong>Authors: </strong>Houping Yue, Zixiang Di, Mei Jiang, Bingdong Li, Hao Hao, Yu Song, Bo Jiang, Aimin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15034">https://arxiv.org/abs/2602.15034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15034">https://arxiv.org/pdf/2602.15034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15034]] EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research(https://arxiv.org/abs/2602.15034)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.</li>
<li><strong>摘要：</strong>虽然大型语言模型（LLM）正在重塑社会科学人工智能（AI4SS）的范式，但严格评估其学术写作能力仍然是一项重大挑战。现有的基准主要强调单次、整体生成，因此缺乏反映复杂的学术研究工作流程所需的细粒度评估。为了填补这一空白，我们推出了EduResearchBench，这是第一个致力于教育学术写作的综合评估平台。 EduResearchBench 基于我们的分层原子任务分解 (HATD) 框架，该框架将端到端研究工作流程分解为 6 ​​个专门研究模块（例如定量分析、定性研究和政策研究），涵盖 24 个细粒度原子任务。这种分类法实现了自动化评估流程，减轻了整体评分的关键限制，其中总分通常会掩盖特定的能力瓶颈，而是针对具体缺陷提供细粒度的诊断反馈。此外，认识到学术写作固有的高认知负荷，我们提出了一种课程学习策略，逐步培养从基础技能到复杂的方法论推理和论证的能力。利用 55K 原始学术样本，我们策划了 11K 个高质量指令对来训练 EduWrite，这是一种专门的教育学术写作模型。实验表明，EduWrite (30B) 在多个核心指标上明显优于较大的通用模型 (72B)，这表明在垂直领域，数据质量密度和分层分阶段的训练课程比参数规模更具决定性。</li>
</ul>

<h3>Title: Indic-TunedLens: Interpreting Multilingual Models in Indian Languages</h3>
<ul>
<li><strong>Authors: </strong>Mihir Panchal, Deeksha Varshney, Mamta, Asif Ekbal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15038">https://arxiv.org/abs/2602.15038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15038">https://arxiv.org/pdf/2602.15038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15038]] Indic-TunedLens: Interpreting Multilingual Models in Indian Languages(https://arxiv.org/abs/2602.15038)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at this https URL. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>多语言大语言模型 (LLM) 越来越多地部署在印度等语言多样化的地区，但大多数可解释性工具仍然是针对英语量身定制的。之前的研究表明，法学硕士通常在以英语为中心的表征空间中运作，这使得跨语言可解释性成为一个紧迫的问题。我们引入了 Indic-TunedLens，这是一种专门针对印度语言的新颖的可解释性框架，可以学习共享的仿射变换。与直接解码中间激活的标准 Logit Lens 不同，Indic-TunedLens 调整每种目标语言的隐藏状态，将它们与目标输出分布对齐，以实现更忠实地解码模型表示。我们使用 MMLU 基准评估了 10 种印度语言的框架，发现它比 SOTA 可解释性方法有显着改进，特别是对于形态丰富、资源匮乏的语言。我们的结果为多语言转换器的分层语义编码提供了重要的见解。我们的模型可通过此 https URL 获取。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding</h3>
<ul>
<li><strong>Authors: </strong>Tahir Hussain (1), Saddam Hussain Khan (2) ((1) Artificial Intelligence Lab, Department of Computer Systems Engineering, University of Engineering and Applied Sciences (UEAS), Swat, Pakistan (2) Interdisciplinary Research Center for Smart Mobility and Logistics (IRC-SML), King Fahad University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15139">https://arxiv.org/abs/2602.15139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15139">https://arxiv.org/pdf/2602.15139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15139]] CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding(https://arxiv.org/abs/2602.15139)</code><input type="text"></li>
<li><strong>Keywords: </strong>long context</a></li>
<li><strong>Abstract: </strong>Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.</li>
<li><strong>摘要：</strong>由于领域特定语义、较长的上下文依赖性和概念敏感推理，对经典伊斯兰文本的准确 QA 仍然具有挑战性。因此，提出了一种新的 CGRA DeBERTa（一种概念引导的残差域增强变压器框架），可以增强圣训语料库的神学 QA。 CGRA DeBERTa 建立在定制的 DeBERTa 变压器主干之上，具有基于 LoRA 的轻量级适配和残差概念感知门控机制。定制的 DeBERTa 嵌入块可以学习全局和位置上下文，而概念引导残差块则融合了来自 12 个核心术语的伊斯兰概念词典中的神学先验。此外，概念门控机制通过重要性加权注意力选择性地放大语义关键标记，应用从 1.04 到 3.00 的差异缩放。这种设计保留了上下文完整性，增强了特定领域的语义表示，并在保持计算效率的同时实现了准确、高效的跨度提取。本文报告了使用来自《布哈里圣训实录》和《穆斯林圣训实录》文本中的 42591 个 QA 对的专门构建的数据集训练 CGRA 的结果。虽然 BERT 的 EM 得分为 75.87，DeBERTa 的 EM 得分为 89.77，但我们的模型得分为 97.85，因此在绝对规模上超过了它们 8.08，同时由于参数高效门控而增加了大约 8 的推理开销。定性评估显示出更好的提取和区分以及神学精确度。这项研究提出了圣训问答系统，该系统高效、可解释且准确，并且规模提供了具有必要神学细微差别的教育材料。</li>
</ul>

<h3>Title: AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking</h3>
<ul>
<li><strong>Authors: </strong>Herbert Ullrich, Jan Drchal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15190">https://arxiv.org/abs/2602.15190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15190">https://arxiv.org/pdf/2602.15190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15190]] AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking(https://arxiv.org/abs/2602.15190)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.</li>
<li><strong>摘要：</strong>在本文中，我们展示了 AVerImaTeC 共享任务中的第三名系统，该系统将我们去年的检索增强生成 (RAG) 管道与反向图像搜索 (RIS) 模块相结合。尽管它很简单，但我们的系统通过 OpenAI Batch API 使用 GPT5.1，每次事实检查一次多模式 LLM 调用，平均只需 0.013 美元，从而提供具有竞争力的性能。我们的系统也很容易复制和调整，仅由三个解耦模块组成 - 基于相似性搜索的文本检索模块、基于 API 访问的 RIS 的图像检索模块以及使用 GPT5.1 的生成模块 - 这就是为什么我们建议将其作为进一步实验的可行起点。我们发布其代码和提示，以及我们的向量存储和对该方案运行成本的见解以及进一步改进的方向。</li>
</ul>

<h3>Title: OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction</h3>
<ul>
<li><strong>Authors: </strong>Skyler Hallinan, Thejas Venkatesh, Xiang Ren, Sai Praneeth Karimireddy, Ashwin Paranjape, Yuhao Zhang, Jack Hessel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15197">https://arxiv.org/abs/2602.15197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15197">https://arxiv.org/pdf/2602.15197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15197]] OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction(https://arxiv.org/abs/2602.15197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general "search" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.</li>
<li><strong>摘要：</strong>工具调用对于大型语言模型 (LLM) 代理完成实际任务至关重要。虽然大多数现有基准测试都假设使用简单、记录完善的工具，但现实世界的工具（例如通用“搜索”API）通常是不透明的，缺乏明确的最佳实践或故障模式。 LLM 代理能否通过交互并随后改进文档来提高使用不透明工具的环境中的性能？为了研究这个问题，我们创建了 OpaqueToolsBench，这是一个由三个不同的面向任务的环境组成的基准测试：通用函数调用、交互式下棋和长轨迹代理搜索。每个环境都提供了未指定的工具，模型必须学会有效地使用这些工具来完成任务。 OpaqueToolsBench 的结果表明，当工具不透明时，自动记录工具的现有方法既昂贵又不可靠。为了解决这个问题，我们提出了一个简单的框架 ToolObserver，它通过观察工具调用轨迹的执行反馈来迭代地完善工具文档。即使在相对困难的设置中，我们的方法也优于 OpaqueToolsBench 上跨数据集的现有方法。此外，对于测试时工具探索设置，我们的方法也很高效，消耗的总令牌比最佳基线少 3.5-7.5 倍。</li>
</ul>

<h3>Title: Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement</h3>
<ul>
<li><strong>Authors: </strong>Stephan Ludwig, Peter J. Danaher, Xiaohao Yang, Yu-Ting Lin, Ehsan Abedin, Dhruv Grewal, Lan Du</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.EM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15312">https://arxiv.org/abs/2602.15312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15312">https://arxiv.org/pdf/2602.15312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15312]] Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement(https://arxiv.org/abs/2602.15312)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.</li>
<li><strong>摘要：</strong>从非结构化文本中准确衡量消费者情绪和评价仍然是营销研究和实践的核心挑战。本研究引入了语言提取器 (LX)，这是一种经过微调的大型语言模型，根据消费者创作的文本进行训练，该模型还标有消费者对 16 种与消费相关的情绪的自我报告评级和四种评估结构：信任、承诺、推荐和情绪。 LX 的性能始终优于领先模型，包括 GPT-4 Turbo、RoBERTa 和 DeepSeek，在开放式调查响应中实现了 81% 的宏观 F1 准确率，在第三方注释的 Amazon 和 Yelp 评论中实现了 95% 以上的准确率。 LX 在在线零售数据中的应用，使用看似无关的回归，证实评论表达的情绪可以预测产品评级，进而预测购买行为。大多数情绪影响是由产品评级调节的，尽管不满和平静等一些情绪直接影响购买，这表明情绪基调提供了超出星级评级的有意义的信号。为了支持其使用，我们提供了无代码、免费的 LX Web 应用程序，可以对消费者创作的文本进行可扩展的分析。在为消费者感知测量建立新的方法论基础时，本研究展示了利用大型语言模型推进营销研究和实践的新方法，从而实现从消费者数据中有效检测营销结构。</li>
</ul>

<h3>Title: Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory</h3>
<ul>
<li><strong>Authors: </strong>Zihao Tang, Xin Yu, Ziyu Xiao, Zengxuan Wen, Zelin Li, Jiaxi Zhou, Hualei Wang, Haohua Wang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15313">https://arxiv.org/abs/2602.15313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15313">https://arxiv.org/pdf/2602.15313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15313]] Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory(https://arxiv.org/abs/2602.15313)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.</li>
<li><strong>摘要：</strong>AI 记忆，特别是模型如何组织和检索历史消息，对于大型语言模型 (LLM) 变得越来越有价值，但现有方法（RAG 和 Graph-RAG）主要通过基于相似性的机制来检索记忆。虽然高效，但这种 System-1 式检索难以应对需要全局推理或全面覆盖所有相关信息的场景。在这项工作中，我们提出了 Mnemis，这是一种新颖的记忆框架，它将 System-1 相似性搜索与互补的 System-2 机制（称为全局选择）集成在一起。 Mnemis 将内存组织成用于相似性检索的基本图和支持自上而下、有意遍历语义层次结构的层次图。通过结合两种检索途径的互补优势，Mnemis 检索出语义和结构相关的记忆项目。 Mnemis 在长期内存基准测试中的所有比较方法中均实现了最先进的性能，在使用 GPT-4.1-mini 的 LoCoMo 上得分为 93.9，在 LongMemEval-S 上得分为 91.6。</li>
</ul>

<h3>Title: NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Rong Fu, Yang Li, Zeyu Zhang, Jiekai Wu, Yaohua Liu, Shuaishuai Cao, Yangchen Zeng, Yuhang Zhang, Xiaojing Du, Chuang Zhao, Kangning Cui, Simon Fong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15353">https://arxiv.org/abs/2602.15353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15353">https://arxiv.org/pdf/2602.15353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15353]] NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering(https://arxiv.org/abs/2602.15353)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.</li>
<li><strong>摘要：</strong>大型预训练语言模型和神经推理系统已经推进了许多自然语言任务，但它们仍然受到需要精确、结构化多跳推理的知识密集型查询的挑战。知识图为事实基础提供了紧凑的符号基础，但将图结构与神经模型集成起来并非易事：天真地将图事实嵌入到提示中会导致效率低下和脆弱，而纯粹的符号或搜索繁重的方法可能在检索方面成本高昂，并且缺乏基于梯度的细化。我们引入了 NeuroSymActive，这是一个模块化框架，它将可微分的神经符号推理层与用于知识图问答的主动的、价值引导的探索控制器相结合。该方法将软统一风格的符号模块与神经路径评估器和优先考虑高价值路径扩展的蒙特卡洛风格探索策略结合起来。标准 KGQA 基准的实证结果表明，与常见的检索增强基线相比，NeuroSymActive 获得了很高的答案准确性，同时减少了昂贵的图形查找和模型调用的数量。</li>
</ul>

<h3>Title: Far Out: Evaluating Language Models on Slang in Australian and Indian English</h3>
<ul>
<li><strong>Authors: </strong>Deniz Kaya Dilsiz, Dipankar Srirag, Aditya Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15373">https://arxiv.org/abs/2602.15373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15373">https://arxiv.org/pdf/2602.15373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15373]] Far Out: Evaluating Language Models on Slang in Australian and Indian English(https://arxiv.org/abs/2602.15373)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: \textsc{web}, containing 377 web-sourced usage examples from Urban Dictionary, and \textsc{gen}, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on \textsc{web} versus \textsc{gen} datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.</li>
<li><strong>摘要：</strong>语言模型在处理非标准语言变体的文本时表现出系统的性能差距，但它们理解多种语言的特定俚语的能力仍未得到充分探索。我们对七种最先进的语言模型中印度英语 (en-IN) 和澳大利亚英语 (en-AU) 的俚语认知进行了综合评估。我们构建了两个互补的数据集：\textsc{web}，包含来自 Urban Dictionary 的 377 个网络来源的用法示例；\textsc{gen}，包含这些俚语术语在不同场景中综合生成的 1,492 个用法。我们评估三个任务的语言模型：目标词预测（TWP）、引导目标词预测（TWP$^*$）和目标词选择（TWS）。我们的结果揭示了四个关键发现：(1) TWS 与 TWP 和 TWP$^*$ 相比，平均模型性能更高，平均准确度得分分别从 0.03 增加到 0.49 (2) \textsc{web} 与 \textsc{gen} 数据集上的平均模型性能更强，TWP 和 TWP$^*$ 任务的平均相似度得分分别增加 0.03 和 0.05 (3) 当所有任务平均时，en-IN 任务优于 en-AU模型和数据集，TWS 表现出最大的差异，平均准确度从 0.44 提高到 0.54。这些发现强调了特定语言的生成能力和判别能力之间的根本不对称，特别是在俚语表达的背景下，尽管使用的是技术丰富的语言，例如英语。</li>
</ul>

<h3>Title: Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework</h3>
<ul>
<li><strong>Authors: </strong>Mengze Hong, Chen Jason Zhang, Zichang Guo, Hanlin Gu, Di Jiang, Li Qing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15377">https://arxiv.org/abs/2602.15377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15377">https://arxiv.org/pdf/2602.15377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15377]] Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework(https://arxiv.org/abs/2602.15377)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Customer service automation has seen growing demand within digital transformation. Existing approaches either rely on modular system designs with extensive agent orchestration or employ over-simplified instruction schemas, providing limited guidance and poor generalizability. This paper introduces an orchestration-free framework using Task-Oriented Flowcharts (TOFs) to enable end-to-end automation without manual intervention. We first define the components and evaluation metrics for TOFs, then formalize a cost-efficient flowchart construction algorithm to abstract procedural knowledge from service dialogues. We emphasize local deployment of small language models and propose decentralized distillation with flowcharts to mitigate data scarcity and privacy issues in model training. Extensive experiments validate the effectiveness in various service tasks, with superior quantitative and application performance compared to strong baselines and market products. By releasing a web-based system demonstration with case studies, we aim to promote streamlined creation of future service automation.</li>
<li><strong>摘要：</strong>客户服务自动化在数字化转型中的需求不断增长。现有方法要么依赖于具有广泛代理编排的模块化系统设计，要么采用过于简化的指令模式，提供的指导有限且通用性差。本文介绍了一种使用面向任务的流程图 (TOF) 的免编排框架，无需人工干预即可实现端到端自动化。我们首先定义 TOF 的组件和评估指标，然后形式化一种经济高效的流程图构建算法，以从服务对话中抽象出程序知识。我们强调小语言模型的本地部署，并提出用流程图进行去中心化蒸馏，以缓解模型训练中的数据稀缺和隐私问题。大量的实验验证了各种服务任务的有效性，与强大的基线和市场产品相比，具有卓越的定量和应用性能。通过发布带有案例研究的基于网络的系统演示，我们的目标是促进未来服务自动化的简化创建。</li>
</ul>

<h3>Title: Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language</h3>
<ul>
<li><strong>Authors: </strong>Prathamesh Devadiga, Paras Chopra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15378">https://arxiv.org/abs/2602.15378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15378">https://arxiv.org/pdf/2602.15378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15378]] Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language(https://arxiv.org/abs/2602.15378)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).</li>
<li><strong>摘要：</strong>大型语言模型可以用训练数据中几乎不存在的语言进行对话吗？我们通过图卢语的案例研究来调查这个问题，图卢语是一种德拉威语言，拥有超过 200 万使用者，但数字化程度极低。我们不是对法学硕士进行微调，而是研究仅结构化提示是否可以在受控提示下引发基本的会话能力。我们通过结合显式语法文档、抑制相关语言中高概率标记的负面约束、罗马化标准化以及通过自我游戏生成质量控制的合成数据，系统地解决了 Tulu 缺乏训练数据所带来的各种挑战。我们的方法对三个 LLM（Gemini 2.0 Flash、GPT-4o、Llama 3.1 70B）手动策划的保留集进行评估并由母语人士验证，将词汇污染从 80% 减少到 5%，同时实现 85% 的语法准确性。跨模型分析表明，负面约束提供了一致的改进（12--18 个百分点），而语法文档效果因模型架构而异​​（8--22 个百分点）。</li>
</ul>

<h3>Title: The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Xiaoze Liu, Ruowang Zhang, Weichen Yu, Siheng Xiong, Liu He, Feijie Wu, Hoin Jung, Matt Fredrikson, Xiaoqian Wang, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15382">https://arxiv.org/abs/2602.15382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15382">https://arxiv.org/pdf/2602.15382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15382]] The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems(https://arxiv.org/abs/2602.15382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at this https URL</li>
<li><strong>摘要：</strong>由大型语言模型支持的多代理系统（MAS）已经解锁了高级协作推理，但它们仍然受到离散文本通信效率低下的束缚，这会带来巨大的运行时开销和信息量化损失。虽然潜在状态传输提供了高带宽替代方案，但现有方法要么采用同质发送器-接收器架构，要么依赖于特定对的学习转换器，从而限制了具有不相交流形的不同模型系列的可扩展性和模块化性。在这项工作中，我们提出了 Vision Wormhole，这是一种新颖的框架，它重新利用视觉语言模型 (VLM) 的视觉界面，以实现与模型无关的、无文本的通信。通过引入通用视觉编解码器，我们将异构推理轨迹映射到共享的连续潜在空间中，并将它们直接注入接收者的视觉路径中，有效地将视觉编码器视为代理间心灵感应的通用端口。我们的框架采用中心辐射型拓扑结构，将成对对齐复杂度从 O(N^2) 降低到 O(N)，并利用无标签、师生蒸馏目标将高速视觉通道与文本路径的稳健推理模式对齐。跨异构模型系列（例如 Qwen-VL、Gemma）的大量实验表明，Vision Wormhole 在受控比较中减少了端到端挂钟时间，同时保持了与标准基于文本的 MAS 相当的推理保真度。代码可在此 https URL 获取</li>
</ul>

<h3>Title: Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Joonatan Laato, Veera Schroderus, Jenna Kanerva, Jenni Kauppi, Virpi Lummaa, Filip Ginter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15436">https://arxiv.org/abs/2602.15436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15436">https://arxiv.org/pdf/2602.15436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15436]] Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs(https://arxiv.org/abs/2602.15436)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly. We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.</li>
<li><strong>摘要：</strong>数字化的历史档案使大规模研究日常社会生活成为可能，但直接从文本中提取的信息往往无法直接定量地回答历史学家或社会学家提出的研究问题。我们在大量芬兰二战卡累利阿撤离者家庭访谈中探讨了这个问题。之前的工作从这些访谈中提取了超过 35 万次提到的休闲活动和组织成员资格，产生了 7.1 万个独特的活动和组织名称——数量太多，无法直接分析。我们开发了一个分类框架，捕捉参与的关键方面（活动/组织的类型、通常的社交程度、发生的频率以及体力要求）。我们注释一个黄金标准集以进行可靠的评估，然后测试大型语言模型是否可以大规模应用相同的模式。在多个模型运行中使用简单的投票方法，我们发现开放权重法学硕士可以紧密匹配专家的判断。最后，我们应用该方法来标记 35 万个实体，为社会融合和相关结果的下游研究生成结构化资源。</li>
</ul>

<h3>Title: TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jiasi Shen, Jing Tang, Jianguo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15449">https://arxiv.org/abs/2602.15449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15449">https://arxiv.org/pdf/2602.15449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15449]] TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models(https://arxiv.org/abs/2602.15449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 正在改变编码范式，称为“vibe 编码”，但合成算法复杂且健壮的代码仍然是一个严峻的挑战。激励法学硕士的深度推理能力对于克服这一障碍至关重要。强化微调（RFT）已成为满足这一需求的一种有前景的策略。然而，大多数现有方法忽视了测试用例中固有的异构难度和粒度，导致奖励信号分布不平衡，从而导致训练期间梯度更新出现偏差。为了解决这个问题，我们提出了测试驱动和能力自适应的课程强化微调（TAROT）。 TAROT针对每个问题系统地构建了四层测试套件（基础、中级、复杂、边缘），为课程设计和评估提供可控的难度环境。至关重要的是，TAROT 将课程进展与原始奖励分数脱钩，从而实现基于能力的评估和从课程政策组合中进行原则性选择，而不是偶然的测试用例难度组合。这种设计促进了稳定的优化和更有效的能力获取。大量的实验结果表明，代码生成中 RFT 的最佳课程与模型的固有能力密切相关，能力较差的模型通过从易到难的进展获得更大的收益，而能力更强的模型在从难优先的课程中表现出色。 TAROT 提供了一种可重复的方法，可以根据模型的能力自适应地定制课程设计，从而持续提高生成代码的功能正确性和鲁棒性。所有代码和数据均在此 https URL 上发布，以促进可重复性并推进社区研究。</li>
</ul>

<h3>Title: In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Aflah Khan, Mahsa Amani, Soumi Das, Bishwamittra Ghosh, Qinyuan Wu, Krishna P. Gummadi, Manish Gupta, Abhilasha Ravichander</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15456">https://arxiv.org/abs/2602.15456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15456">https://arxiv.org/pdf/2602.15456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15456]] In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations(https://arxiv.org/abs/2602.15456)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.</li>
<li><strong>摘要：</strong>基于大型语言模型（LLM）的代理越来越多地被部署为在线平台上信息的接口。这些代理对从平台后端数据库或通过网络搜索检索到的信息进行过滤、优先排序和综合。 In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them.它们还有助于解释诸如在先前的工作中观察到的新闻推荐中观察到的左倾倾斜等现象。 Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.</li>
</ul>

<h3>Title: Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit</h3>
<ul>
<li><strong>Authors: </strong>Aswathy Velutharambath, Amelie Wührl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15504">https://arxiv.org/abs/2602.15504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15504">https://arxiv.org/pdf/2602.15504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15504]] Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit(https://arxiv.org/abs/2602.15504)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from this https URL</li>
<li><strong>摘要：</strong>患者对治疗的期望对治疗的成功有很大影响。虽然主要在临床环境中进行研究，但像 Reddit 医疗版块这样的在线患者平台可能具有互补的见解：患者觉得没有必要或不舒服在其他地方分享的治疗期望。尽管如此，还没有研究调查用户在网上讨论的期望类型以及他们如何表达这些期望。据推测，这是因为之前没有在自然语言处理（NLP）中研究过期望。因此，我们引入了期望检测的任务，认为期望与许多应用相关，包括意见挖掘和产品设计。随后，我们提出了一个医疗领域的案例研究，其中期望的提取尤为重要。我们贡献了 RedHOTExpect，这是一个 Reddit 帖子语料库（4.5K 帖子）来研究这方面的期望。我们使用大型语言模型 (LLM) 对数据进行银标签并手动验证其质量（标签准确率约 78%）。在此基础上，我们分析了哪些语言模式表征了期望，并探讨了患者的期望及其原因。我们发现，与心理健康背景相比，在有关身体或治疗相关疾病的帖子中，乐观和积极主动的框架更为明显，而且在我们的数据集中，患者大多讨论益处而不是负面结果。 RedHOTExpect 语料库可以从此 https URL 获取</li>
</ul>

<h3>Title: Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Xiangyan Chen, Yujian Gan, Matthew Purver</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15509">https://arxiv.org/abs/2602.15509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15509">https://arxiv.org/pdf/2602.15509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15509]] Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination(https://arxiv.org/abs/2602.15509)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.</li>
<li><strong>摘要：</strong>当前大型语言模型（LLM）中的幻觉倾向会对对话系统产生负面影响。此类幻觉会产生事实上不正确的响应，可能会误导用户并破坏系统信任。 Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.</li>
</ul>

<h3>Title: DependencyAI: Detecting AI Generated Text through Dependency Parsing</h3>
<ul>
<li><strong>Authors: </strong>Sara Ahmed, Tracy Hammond</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15514">https://arxiv.org/abs/2602.15514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15514">https://arxiv.org/pdf/2602.15514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15514]] DependencyAI: Detecting AI Generated Text through Dependency Parsing(https://arxiv.org/abs/2602.15514)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks. We introduce DependencyAI, a simple and interpretable approach for detecting AI-generated text using only the labels of linguistic dependency relations. Our method achieves competitive performance across monolingual, multi-generator, and multilingual settings. To increase interpretability, we analyze feature importance to reveal syntactic structures that distinguish AI-generated from human-written text. We also observe a systematic overprediction of certain models on unseen domains, suggesting that generator-specific writing styles may affect cross-domain generalization. Overall, our results demonstrate that dependency relations alone provide a robust signal for AI-generated text detection, establishing DependencyAI as a strong linguistically grounded, interpretable, and non-neural network baseline.</li>
<li><strong>摘要：</strong>As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks.我们引入了 DependencyAI，这是一种简单且可解释的方法，用于仅使用语言依赖关系的标签来检测人工智能生成的文本。我们的方法在单语言、多生成器和多语言设置中实现了具有竞争力的性能。为了提高可解释性，我们分析了特征重要性，以揭示区分人工智能生成的文本和人类编写的文本的句法结构。我们还观察到某些模型在未知领域的系统性过度预测，这表明特定于生成器的写作风格可能会影响跨领域泛化。总的来说，我们的结果表明，依赖关系本身就为 AI 生成的文本检测提供了强大的信号，将 DependencyAI 建立为强大的语言基础、可解释和非神经网络基线。</li>
</ul>

<h3>Title: ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Zhao, Tong Zhu, Zhi Zhang, Tiantian Fan, Jinluan Yang, Kun Kuang, Zhongyu Wei, Fei Wu, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15521">https://arxiv.org/abs/2602.15521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15521">https://arxiv.org/pdf/2602.15521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15521]] ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns(https://arxiv.org/abs/2602.15521)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.</li>
<li><strong>摘要：</strong>专家混合 (MoE) 有效地扩展了模型容量，同时通过稀疏专家激活保持了计算效率。然而，从头开始培训高质量的教育部成本高昂。一个有前途的替代方案是将预训练的密集模型转换为稀疏 MoE。现有的密集到 MoE 方法分为两类：\textbf{动态结构修剪}，将密集模型转换为具有适度稀疏性的 MoE 架构，以平衡性能和推理效率；\textbf{downcycling} 方法，使用预训练的密集模型来初始化高度稀疏的 MoE 架构。然而，现有方法打破了密集模型中的内在激活模式，导致专家构建不理想。在这项工作中，我们认为门控线性单元（GLU）机制为密集到 MoE 的转换提供了一个自然的蓝图。我们表明，GLU 的细粒度神经激活模式揭示了粗粒度结构，揭示了由持续激活的通用神经元和动态激活的专用神经元组成的固有 MoE 架构。利用这一发现，我们引入了 ExpertWeaver，这是一个免训练框架，可根据神经元的激活模式对神经元进行分区，并使用层自适应配置构建共享专家和专门的路由专家。我们的实验表明，ExpertWeaver 的性能显着优于现有方法，无论是作为免训练的动态结构修剪技术还是作为卓越 MoE 初始化的降级策略。</li>
</ul>

<h3>Title: ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Nicol Visser, Simon Malan, Danel Slabbert, Herman Kamper</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15537">https://arxiv.org/abs/2602.15537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15537">https://arxiv.org/pdf/2602.15537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15537]] ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling(https://arxiv.org/abs/2602.15537)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.</li>
<li><strong>摘要：</strong>纯语音语言模型旨在直接从原始音频中学习语言，无需文本资源。一个关键的挑战是来自自监督语音编码器的离散标记会导致序列过长，从而激发了最近对类音节单元的研究。然而，Sylber 和 SyllableLM 等方法依赖于复杂的多阶段训练管道。我们提出了 ZeroSyl，一种简单的免训练方法，可以直接从冻结的 WavLM 模型中提取音节边界和嵌入。使用 WavLM 中间层中的 L2 特征规范，ZeroSyl 实现了具有竞争力的音节分割性能。生成的片段经过均值池化，使用 K 均值进行离散化，并用于训练语言模型。 ZeroSyl 在词汇、句法和叙述基准方面优于先前的音节标记器。缩放实验表明，虽然更细粒度的单元有利于词汇任务，但我们发现的音节单元对于句法建模表现出更好的缩放行为。</li>
</ul>

<h3>Title: Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite</h3>
<ul>
<li><strong>Authors: </strong>Tim Fischer, Chris Biemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15540">https://arxiv.org/abs/2602.15540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15540">https://arxiv.org/pdf/2602.15540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15540]] Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite(https://arxiv.org/abs/2602.15540)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.</li>
<li><strong>摘要：</strong>This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives 实现了灵活的、以方面为中心的文档集群管道，具有人机交互的细化功能。 We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.</li>
</ul>

<h3>Title: Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Yihan Wang, Peiyu Liu, Runyu Chen, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15564">https://arxiv.org/abs/2602.15564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15564">https://arxiv.org/pdf/2602.15564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15564]] Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL(https://arxiv.org/abs/2602.15564)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at this https URL</li>
<li><strong>摘要：</strong>文本到 SQL 最近取得了令人瞩目的进展，但仍然难以在现实场景中有效应用。这种差距源于对单一静态工作流程的依赖，从根本上限制了对分布外和长尾场景的可扩展性。我们不要求用户通过广泛的实验来选择合适的方法，而是尝试使系统能够在推理时自适应地构建工作流程。通过理论和实证分析，我们证明最佳动态策略始终优于最佳静态工作流程，性能提升从根本上是由候选工作流程之间的异质性驱动的。受此启发，我们提出了 SquRL，这是一种强化学习框架，可以增强法学硕士在自适应工作流程构建中的推理能力。我们设计了一个基于规则的奖励函数，并引入了两种有效的训练机制：动态演员屏蔽以鼓励更广泛的探索，以及伪奖励以提高训练效率。对广泛使用的文本到 SQL 基准的实验表明，动态工作流构建始终优于最佳的静态工作流方法，在复杂和分布外查询方面的优势尤其明显。 The codes are available at this https URL</li>
</ul>

<h3>Title: STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng, Jiang Wu, Yinuo Wang, Yang Guan, Kehua Sheng, Bo Zhang, Keqiang Li, Jingliang Duan, Shengbo Eben Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15620">https://arxiv.org/abs/2602.15620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15620">https://arxiv.org/pdf/2602.15620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15620]] STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens(https://arxiv.org/abs/2602.15620)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% over GRPO, 20-Entropy and JustRL.</li>
<li><strong>摘要：</strong>强化学习（RL）显着改善了大型语言模型推理，但现有的 RL 微调方法严重依赖熵正则化和重新加权等启发式技术来保持稳定性。在实践中，他们经常会经历后期性能崩溃，导致推理质量下降和训练不稳定。我们得出，强化学习中 token-wise 策略梯度的大小与 token 概率和局部策略熵负相关。在此结果的基础上，我们证明训练的不稳定性是由一小部分令牌（大约 0.01%）驱动的，我们将其称为 \emph{spurious tokens}。当这些标记出现在正确的响应中时，它们对推理结果贡献不大，但继承了完整的序列级奖励，导致异常放大的梯度更新。受这一观察的启发，我们提出了用于大规模模型细化的虚假令牌感知策略优化（STAPO），它有选择地掩盖此类更新并重新规范化有效令牌的损失。在使用 Qwen 1.7B、8B 和 14B 基本模型的六个数学推理基准测试中，STAPO 始终表现出卓越的熵稳定性，并比 GRPO、20-Entropy 和 JustRL 实现了 7.13% 的平均性能提升。</li>
</ul>

<h3>Title: LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Khaled Khamis, Hesham Ali</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15675">https://arxiv.org/abs/2602.15675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15675">https://arxiv.org/pdf/2602.15675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15675]] LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models(https://arxiv.org/abs/2602.15675)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.</li>
<li><strong>摘要：</strong>尽管神经文本转语音 (TTS) 取得了进步，但许多阿拉伯方言变体仍然很少得到解决，大多数资源集中在现代阿拉伯语口语 (MSA) 和海湾方言上，导致埃及阿拉伯语（最广泛理解的阿拉伯方言）资源严重不足。我们通过引入 NileTTS 来解决这一差距：来自不同领域（包括医疗、销售和一般对话）的两位演讲者的 38 小时转录语音。我们使用新颖的合成管道构建此数据集：大型语言模型（LLM）生成埃及阿拉伯语内容，然后使用音频合成工具将其转换为自然语音，然后进行自动转录和说话人二值化以及手动质量验证。我们在数据集上对 XTTS v2（一种最先进的多语言 TTS 模型）进行微调，并根据在其他阿拉伯语方言上训练的基线模型进行评估。我们的贡献包括：(1) 第一个公开的埃及阿拉伯语 TTS 数据集，(2) 用于方言 TTS 的可重复合成数据生成管道，以及 (3) 开源微调模型。所有资源均被释放以推进埃及阿拉伯语语音合成研究。</li>
</ul>

<h3>Title: Revisiting Northrop Frye's Four Myths Theory with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Edirlei Soares de Lima, Marco A. Casanova, Antonio L. Furtado</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15678">https://arxiv.org/abs/2602.15678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15678">https://arxiv.org/pdf/2602.15678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15678]] Revisiting Northrop Frye's Four Myths Theory with Large Language Models(https://arxiv.org/abs/2602.15678)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $\kappa$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.</li>
<li><strong>摘要：</strong>诺斯罗普·弗莱的四种基本叙事类型（喜剧、浪漫、悲剧、讽刺）的理论深刻地影响了文学批评，但他的框架的计算方法主要关注叙事模式而不是人物功能。在本文中，我们提出了一个新的角色功能框架，通过研究原型角色如何在弗莱的流派中以不同的方式表现出来，补充基于模式的分析。借鉴荣格原型理论，我们通过将它们映射到荣格的心理结构组成部分，推导出四种普遍的角色功能（主角、导师、对手、同伴）。然后，根据原型作品，这些功能被专门化为十六种特定类型的角色。为了验证这个框架，我们使用六种最先进的大型语言模型 (LLM) 进行了一项多模型研究，以评估 40 部叙事作品中的人物角色对应关系。验证使用正样本（160 个有效对应）和负样本（30 个无效对应）来评估模型是否既识别有效对应又拒绝无效对应。法学硕士取得了显着的成绩（平均平衡准确度为 82.5%），模型间一致性很强（Fleiss' $\kappa$ = 0.600），表明所提出的对应关系捕获了系统的结构模式。表现因类型（72.7% 至 89.9%）和角色（52.5% 至 99.2%）而异，定性分析显示，差异反映了真实的叙事属性，包括浪漫中的功能分布和讽刺中故意的原型颠覆。这种基于角色的方法展示了法学硕士支持的计算叙事学方法的潜力，并为叙事生成方法和交互式讲故事应用程序的未来发展奠定了基础。</li>
</ul>

<h3>Title: A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Meirav Segal, Noa Linder, Omer Antverg, Gil Gekker, Tomer Fichman, Omri Bodenheimer, Edan Maor, Omer Nevo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15689">https://arxiv.org/abs/2602.15689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15689">https://arxiv.org/pdf/2602.15689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15689]] A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models(https://arxiv.org/abs/2602.15689)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.</li>
<li><strong>摘要：</strong>大型语言模型和基于 LLM 的代理越来越多地用于本质上具有双重用途的网络安全任务。现有的拒绝方法，涵盖学术政策框架和商业部署系统，通常依赖于广泛的基于主题的禁令或以进攻为重点的分类法。因此，它们可能会产生不一致的决策，过度限制合法的防御者，并且在混淆或请求分段的情况下表现得很脆弱。我们认为，有效的拒绝需要明确地对进攻风险和防御收益之间的权衡进行建模，而不是仅仅依赖于意图或进攻分类。在本文中，我们介绍了一个基于内容的框架，用于设计和审核网络拒绝策略，使攻防权衡变得明确。该框架从五个维度描述了请求：进攻性行动贡献、进攻性风险、技术复杂性、防御性收益和合法用户的预期频率，基于请求的技术实质而不是陈述的意图。我们证明，这种基于内容的方法解决了当前前沿模型行为中的不一致问题，并允许组织构建可调整的、具有风险意识的拒绝策略。</li>
</ul>

<h3>Title: Rethinking Metrics for Lexical Semantic Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Roksana Goworek, Haim Dubossarsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15716">https://arxiv.org/abs/2602.15716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15716">https://arxiv.org/pdf/2602.15716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15716]] Rethinking Metrics for Lexical Semantic Change Detection(https://arxiv.org/abs/2602.15716)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.</li>
<li><strong>摘要：</strong>词汇语义变化检测（LSCD）越来越依赖于上下文化的语言模型嵌入，但大多数方法仍然使用一小组语义变化指标来量化变化，主要是平均成对距离（APD）和单词原型的余弦距离（PRT）。我们引入了平均最小距离 (AMD) 和对称平均最小距离 (SAMD)，这是通过跨时间段内单词使用之间的局部对应来量化语义变化的新度量。在多种语言、编码器模型和表示空间中，我们表明 AMD 通常可以提供更强大的性能，特别是在降维和使用非专用编码器的情况下，而 SAMD 在使用专用编码器时表现出色。我们建议 LSCD 可能会受益于考虑 APD 和 PRT 之外的替代语义变化指标，AMD 为基于上下文的嵌入分析提供了强大的选项。</li>
</ul>

<h3>Title: Causal Effect Estimation with Latent Textual Treatments</h3>
<ul>
<li><strong>Authors: </strong>Omri Feldman, Amar Venugopal, Jann Spiess, Amir Feder</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.EM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15730">https://arxiv.org/abs/2602.15730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15730">https://arxiv.org/pdf/2602.15730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15730]] Causal Effect Estimation with Latent Textual Treatments(https://arxiv.org/abs/2602.15730)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.</li>
<li><strong>摘要：</strong>了解文本对下游结果的因果影响是许多应用中的核心任务。估计这种影响需要研究人员进行受控实验，系统地改变文本特征。虽然大型语言模型 (LLM) 有望生成文本，但生成和评估受控变异需要更加仔细的关注。在本文中，我们提出了一个用于潜在文本干预的生成和因果估计的端到端管道。 Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments.我们证明，对因果效应的天真估计存在显着偏差，因为文本本质上将治疗和协变量信息混为一谈。 We describe the estimation bias induced in this setting and propose a solution based on covariate residualization.我们的实证结果表明，我们的流程有效地诱导了目标特征的变化并减轻了估计误差，为文本作为处理设置中的因果效应估计提供了坚实的基础。</li>
</ul>

<h3>Title: Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac</h3>
<ul>
<li><strong>Authors: </strong>Chahan Vidal-Gorène (CJM, LIPN), Bastien Kindt (UCL), Florian Cafiero (PSL, CJM)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15753">https://arxiv.org/abs/2602.15753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15753">https://arxiv.org/pdf/2602.15753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15753]] Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac(https://arxiv.org/abs/2602.15753)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.</li>
<li><strong>摘要：</strong>低资源语言对自然语言处理任务（例如词形还原和词性（POS）标记）提出了持续的挑战。本文研究了最新的大型语言模型 (LLM)（包括 GPT-4 变体和开放权重 Mistral 模型）的能力，以在少样本和零样本设置中为四种历史和语言资源贫乏的语言解决这些任务：古希腊语、古典亚美尼亚语、古格鲁吉亚语和叙利亚语。使用由对齐训练和域外测试语料库组成的新颖基准，我们评估了基础模型在词形还原和词性标记方面的性能，并将其与 PIE（特定于任务的 RNN 基线）进行比较。我们的结果表明，即使没有微调，法学硕士也能在少数样本设置中在大多数语言的词性标记和词形还原方面实现具有竞争力或卓越的性能。对于以复杂形态和非拉丁文字为特征的语言来说，仍然存在重大挑战，但我们证明，法学硕士是在缺乏数据的情况下启动语言注释任务的可靠且相关的选择，可以作为注释的有效辅助。</li>
</ul>

<h3>Title: Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos</h3>
<ul>
<li><strong>Authors: </strong>Laura De Grazia, Danae Sánchez Villegas, Desmond Elliott, Mireia Farrús, Mariona Taulé</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15757">https://arxiv.org/abs/2602.15757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15757">https://arxiv.org/pdf/2602.15757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15757]] Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos(https://arxiv.org/abs/2602.15757)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.</li>
<li><strong>摘要：</strong>网络性别歧视以多种形式出现，这使得其检测具有挑战性。尽管自动化工具可以增强对性别歧视内容的识别，但它们通常仅限于二元分类。因此，由于缺乏细粒度、上下文敏感的标签，性别歧视的更微妙表现可能仍未被发现。为了解决这个问题，我们做出了以下贡献：（1）我们提出了 FineMuSe，一个新的西班牙语多模态性别歧视检测数据集，其中包括二进制和细粒度注释； (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.</li>
</ul>

<h3>Title: ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Manav Nitin Kapadnis, Lawanya Baghel, Atharva Naik, Carolyn Rosé</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15758">https://arxiv.org/abs/2602.15758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15758">https://arxiv.org/pdf/2602.15758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15758]] ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models(https://arxiv.org/abs/2602.15758)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.</li>
<li><strong>摘要：</strong>虽然多模态大型语言模型 (MLLM) 在单轮图表生成方面表现强劲，但它们支持现实世界探索性数据分析的能力仍未得到充分开发。在实践中，用户通过多轮交互迭代地完善可视化，这需要保持共同点、跟踪先前的编辑并适应不断变化的偏好。 We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset.与之前的一次性基准测试不同，ChartEditBench 评估持续的、上下文感知的编辑。 We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench 为基础的、意图感知的多模式编程建立了一个具有挑战性的测试平台。</li>
</ul>

<h3>Title: ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution</h3>
<ul>
<li><strong>Authors: </strong>Yahia Alqurnawi, Preetom Biswas, Anmol Rao, Tejas Anvekar, Chitta Baral, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15769">https://arxiv.org/abs/2602.15769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15769">https://arxiv.org/pdf/2602.15769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15769]] ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution(https://arxiv.org/abs/2602.15769)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (mLLM) 通常用于回答结构化数据中的问题，例如 Markdown、JSON 和图像中的表格。虽然这些模型通常可以给出正确的答案，但用户还需要知道这些答案来自哪里。在这项工作中，我们研究结构化数据归因/引用，这是模型指向支持答案的特定行和列的能力。我们评估了不同表格格式和提示策略的多个 mLLM。我们的结果显示问题回答和证据归因之间存在明显差距。尽管问答准确度仍然中等，但所有模型中的归因准确度要低得多，对于 JSON 输入而言几乎是随机的。我们还发现，模型在引用行方面比引用列方面更可靠，并且在文本格式方面比图像方面更困难。最后，我们观察到模型系列之间存在显着差异。总体而言，我们的研究结果表明，当前的 mLLM 在为结构化数据提供细粒度、可信的归因方面并不可靠，这限制了它们在需要透明度和可追溯性的应用程序中的使用。</li>
</ul>

<h3>Title: *-PLUIE: Personalisable metric with Llm Used for Improved Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Quentin Lemesle, Léane Jourdan, Daisy Munson, Pierre Alain, Jonathan Chevelu, Arnaud Delhay, Damien Lolive</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15778">https://arxiv.org/abs/2602.15778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15778">https://arxiv.org/pdf/2602.15778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15778]] *-PLUIE: Personalisable metric with Llm Used for Improved Evaluation(https://arxiv.org/abs/2602.15778)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.</li>
<li><strong>摘要：</strong>评估自动生成文本的质量通常依赖于法学硕士作为法官（LLM-judge）方法。虽然有效，但这些方法的计算成本很高并且需要后处理。为了解决这些限制，我们以 ParaPLUIE 为基础，这是一种基于困惑度的 LLM 判断指标，可以在不生成文本的情况下估计“是/否”答案的置信度。我们介绍 *-PLUIE，ParaPLUIE 的任务特定提示变体，并评估它们与人类判断的一致性。我们的实验表明，个性化的 *-PLUIE 与人类评分具有更强的相关性，同时保持较低的计算成本。</li>
</ul>

<h3>Title: Avey-B</h3>
<ul>
<li><strong>Authors: </strong>Devang Acharya, Mohammad Hammoud</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15814">https://arxiv.org/abs/2602.15814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15814">https://arxiv.org/pdf/2602.15814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15814]] Avey-B(https://arxiv.org/abs/2602.15814)</code><input type="text"></li>
<li><strong>Keywords: </strong>long context</a></li>
<li><strong>Abstract: </strong>Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.</li>
<li><strong>摘要：</strong>在计算和内存预算紧张的情况下，紧凑型预训练双向编码器仍然是工业 NLP 的支柱。它们的有效性源于自注意力能够通过序列级并行性提供高质量的双向上下文化，正如 BERT 风格架构所普及的那样。最近，Avey 作为一种自回归、无需注意的替代方案被引入，它自然地允许仅编码器的适应。在本文中，我们针对仅编码器范式重新制定了 Avey，并对其架构提出了多项创新，包括解耦的静态和动态参数化、面向稳定性的归一化和神经压缩。结果表明，这种重新设计的架构与四种广泛使用的基于 Transformer 的编码器相比具有优势，在标准标记分类和信息检索基准上始终优于它们，同时更有效地扩展到长上下文。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
