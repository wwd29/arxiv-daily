<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-12</h1>
<h3>Title: What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Luciano Floridi, Jessica Morley, Claudio Novelli, David Watson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10080">https://arxiv.org/abs/2512.10080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10080">https://arxiv.org/pdf/2512.10080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10080]] What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models(https://arxiv.org/abs/2512.10080)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.</li>
<li><strong>摘要：</strong>本文探讨了当前大型语言模型 (LLM) 中推理的工作原理，该模型使用标记完成方法。它检查了它们的随机性及其与人类溯因推理的相似性。争论的焦点是，这些法学硕士根据学习到的模式创建文本，而不是执行实际的溯因推理。当他们的输出看起来是溯因推理时，这主要是因为他们接受了包含推理结构的人类生成文本的训练。使用示例来展示法学硕士如何在不基于真理、语义、验证或理解的情况下，并且不进行任何真正的溯因推理的情况下，产生合理的想法、模仿常识推理并给出解释性答案。这种双重性质，即模型具有随机基础，但在使用中表现出溯因性，对于如何评估和应用法学硕士具有重要影响。它们可以帮助产生想法并支持人类思维，但必须严格评估它们的输出，因为它们无法识别真相或验证其解释。文章最后提出了对这些观点的五个反对意见，指出了分析中的一些局限性，并提供了总体评估。</li>
</ul>

<h3>Title: Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yumou Wei, John Stamper, Paulo F. Carvalho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10110">https://arxiv.org/abs/2512.10110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10110">https://arxiv.org/pdf/2512.10110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10110]] Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models(https://arxiv.org/abs/2512.10110)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.</li>
<li><strong>摘要：</strong>我们探索使用小语言模型（SLM）来自动生成问题，作为学习分析研究中普遍使用的大型语言模型的补充。我们提出了一种新颖的问题生成管道，它利用 SLM 的文本生成和概率推理能力来生成高质量的问题。采用“生成然后验证”策略，我们的管道首先执行扩展生成，以创建大量候选问题，并通过基于新颖的概率推理的选择性验证来完善它们。我们进行了两项评估研究，一项由七位人类专家参与，另一项由大型语言模型 (LLM) 参与，以评估生成问题的质量。大多数法官（人类或法学硕士）都认为生成的问题有明确的答案，并且通常与预期的学习目标非常一致。我们的研究结果表明，在充分利用其优势的精心设计的流程的指​​导下，SLM 可以有效地生成高质量的问题。</li>
</ul>

<h3>Title: Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing</h3>
<ul>
<li><strong>Authors: </strong>Zhongjie Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10121">https://arxiv.org/abs/2512.10121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10121">https://arxiv.org/pdf/2512.10121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10121]] Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing(https://arxiv.org/abs/2512.10121)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).</li>
<li><strong>摘要：</strong>垂直领域长篇文本生成的核心是当前大型语言模型（LLM）面临的“不可能的三位一体”：同时实现低幻觉、深层逻辑连贯性和个性化表达。这项研究表明，这一瓶颈源于现有的生成范式屈服于统计平滑陷阱，这种现象忽视了专家级写作中不可或缺的高熵信息获取和结构化认知过程。为了解决这一限制，我们提出了 DeepNews 框架，这是一种代理工作流程，可以明确模拟经验丰富的财经记者的隐性认知过程。该框架集成了三个核心模块：首先，基于信息觅食理论的双粒度检索机制，强制执行10:1的饱和信息输入比例，以减轻幻觉输出；第二，模式引导的战略规划，利用领域专家知识库（叙述模式）和原子块来打造强大的逻辑骨架的过程；第三，对抗性约束提示，这是一种部署策略的技术，包括节奏中断和逻辑雾，以破坏模型生成文本中固有的概率平滑性。实验描绘了深度财务报告中一个显着的知识悬崖：当检索到的上下文低于 15,000 个字符时，内容真实性就会崩溃，而超过 30,000 个字符的高冗余输入可以将无幻觉率 (HFR) 稳定在 85% 以上。在与中国顶级科技媒体进行的生态有效性盲测中，基于上一代模型（DeepSeek-V3-0324）构建的 DeepNews 系统实现了 25% 的提交接受率，显着优于最先进（SOTA）模型（GPT-5）零样本生成的 0% 接受率。</li>
</ul>

<h3>Title: PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset</h3>
<ul>
<li><strong>Authors: </strong>Moonsoo Park, Jeongseok Yun, Bohyung Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10148">https://arxiv.org/abs/2512.10148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10148">https://arxiv.org/pdf/2512.10148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10148]] PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset(https://arxiv.org/abs/2512.10148)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.</li>
<li><strong>摘要：</strong>在用户信息有限的领域（例如食品配送平台），个性化评论响应的生成提出了重大挑战。虽然大型语言模型 (LLM) 提供强大的文本生成功能，但在缺乏上下文用户数据时，它们通常会生成通用响应，从而降低参与度和有效性。在这项工作中，我们提出了一个两阶段的提示框架，可以直接从简短的评论文本中推断出显性的（例如，用户陈述的偏好）和隐性的（例如，人口统计或风格线索）角色。然后，将这些推断出的角色属性合并到响应生成提示中，以生成用户定制的回复。为了鼓励多样化但忠实的世代，我们在推理过程中调整解码温度。我们使用从韩国食品配送应用程序收集的真实数据集来评估我们的方法，并评估其对精度、多样性和语义一致性的影响。我们的研究结果强调了角色增强提示在增强自动响应的相关性和个性化方面的有效性，而无需模型微调。</li>
</ul>

<h3>Title: Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Lama Alssum, Hani Itani, Hasan Abed Al Kader Hammoud, Philip Torr, Adel Bibi, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10150">https://arxiv.org/abs/2512.10150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10150">https://arxiv.org/pdf/2512.10150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10150]] Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning(https://arxiv.org/abs/2512.10150)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的民主化，其安全性变得越来越重要。在本文中，我们研究了法学硕士适应新任务所带来的安全性下降。我们将这种安全性妥协归因于灾难性遗忘，并将微调时保持安全性的问题框定为持续学习（CL）问题。我们考虑微调即服务设置，用户将数据上传到服务提供商，以获得擅长用户所选任务的定制模型。我们采用了文献中的几种 CL 方法，并系统地评估了它们减轻安全退化的能力。其中包括基于正则化、基于内存和模型合并方法。我们考虑两种情况，（1）良性用户数据和（2）中毒用户数据。我们的结果表明，CL 方法始终比标准微调实现更低的攻击成功率。其中，DER 在保持任务效用的同时优于其他 CL 方法和现有的安全保护基线。这些发现概括了三个下游任务（GSM8K、SST2、Code）和三个模型系列（LLaMA2-7B、Mistral-7B、Gemma-2B），将 CL 确立为保护安全的实用解决方案。</li>
</ul>

<h3>Title: AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding</h3>
<ul>
<li><strong>Authors: </strong>Gyutaek Oh, Sangjoon Park, Byung-Hoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10195">https://arxiv.org/abs/2512.10195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10195">https://arxiv.org/pdf/2512.10195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10195]] AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding(https://arxiv.org/abs/2512.10195)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.</li>
<li><strong>摘要：</strong>最近，评估大型语言模型 (LLM) 已成为在医学领域安全可靠地应用 LLM 的一个关键问题。尽管已经提出了各种静态医学问答（QA）基准，但许多方面仍未得到充分探索，例如法学硕士在动态、交互式临床多轮对话情况下生成响应的有效性以及超越简单准确性的多方面评估策略的识别。然而，正式评估动态的、交互式的临床情况受到可能的患者状态和交互轨迹的巨大组合空间的阻碍，使得很难标准化和定量测量此类场景。在这里，我们介绍 AutoMedic，一个多代理模拟框架，可以自动评估法学硕士作为临床对话代理。 AutoMedic 将现成的静态 QA 数据集转换为虚拟患者档案，从而实现法学硕士代理人之间真实且基于临床的多轮临床对话。然后根据我们的 CARE 指标评估各种临床对话代理的表现，该指标提供了临床对话准确性、效率/策略、同理心和鲁棒性的多方面评估标准。我们的研究结果经过人类专家的验证，证明了 AutoMedic 作为临床会话代理自动评估框架的有效性，为会话医学应用中法学硕士的有效开发提供了实用指南。</li>
</ul>

<h3>Title: Multilingual VLM Training: Adapting an English-Trained VLM to French</h3>
<ul>
<li><strong>Authors: </strong>Jules Lahmi, Alexis Roger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10336">https://arxiv.org/abs/2512.10336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10336">https://arxiv.org/pdf/2512.10336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10336]] Multilingual VLM Training: Adapting an English-Trained VLM to French(https://arxiv.org/abs/2512.10336)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.</li>
<li><strong>摘要：</strong>近年来，人工智能取得了巨大进步，特别是在能够理解视觉和文本数据的视觉语言模型（VLM）的开发方面。然而，这些进步仍然主要限于英语，降低了非英语使用者的可访问性。将这些功能扩展到更广泛的语言至关重要。本文探讨了使英语训练的 VLM 适应不同语言的挑战。为此，我们将探索并比较不同方法的性能和计算成本。我们考虑基于翻译的管道、LoRA 微调以及将视觉适应与语言适应分开的两阶段微调策略。为了评估这些方法，我们结合使用翻译成目标语言的标准多模式基准和本地专家的手动评估。结果表明，数据集翻译仍然是多语言 VLM 性能的主要瓶颈，数据质量限制了训练和评估的有效性。这些发现表明，未来的努力应该集中在母语数据集收集和改进的翻译策略上。</li>
</ul>

<h3>Title: Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale</h3>
<ul>
<li><strong>Authors: </strong>Zhaodong Wang, Zhenting Qi, Sherman Wong, Nathan Hu, Samuel Lin, Jun Ge, Erwin Gao, Yining Yang, Ben Maurer, Wenlin Chen, David Recordon, Yilun Du, Minlan Yu, Ying Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10398">https://arxiv.org/abs/2512.10398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10398">https://arxiv.org/pdf/2512.10398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10398]] Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale(https://arxiv.org/abs/2512.10398)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.</li>
<li><strong>摘要：</strong>现实世界的人工智能软件工程需要编码代理能够对大量存储库进行推理，在长时间会话中保持持久内存，并在测试时稳健地协调复杂的工具链。现有的开源编码代理提供了透明度，但在推向这些工业规模的工作负载时经常达不到要求，而专有编码代理提供了强大的实用性能，但可扩展性、可解释性和可控性有限。我们推出了孔子代码代理（CCA），这是一种可以在工业规模上运行的开源人工智能软件工程师。 CCA 构建在 Kongfun SDK 之上，这是一个开源代理开发平台，围绕三个互补的视角进行设计：代理体验 (AX)、用户体验 (UX) 和开发人员体验 (DX)。该 SDK 引入了一个统一的编排器，具有用于长上下文推理的分层工作内存、用于跨会话持续学习的持久笔记系统以及用于强大工具使用的模块化扩展模块。此外，元代理通过构建-测试-改进循环自动合成、评估和细化代理配置，从而能够在新任务、环境和工具堆栈上快速开发代理。 CCA 通过这些机制在 Kongfun SDK 上实例化，在现实世界的软件工程任务中提供了强大的性能。在 SWE-Bench-Pro 上，CCA 实现了 54.3% 的最先进的 Resolve@1 性能，比之前的编码代理有了显着提高。孔子 SDK 和 CCA 共同为人工智能代理提供了透明、可扩展和可复制的基础，弥合了研究原型和生产级系统之间的差距，并支持工业规模的代理开发和部署。</li>
</ul>

<h3>Title: Sliding Window Attention Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yijiong Yu, Jiale Liu, Qingyun Wu, Huazheng Wang, Ji Pei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10411">https://arxiv.org/abs/2512.10411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10411">https://arxiv.org/pdf/2512.10411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10411]] Sliding Window Attention Adaptation(https://arxiv.org/abs/2512.10411)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at this https URL</li>
<li><strong>摘要：</strong>基于 Transformer 的大型语言模型 (LLM) 中的自注意力机制与输入长度成二次方缩放，使得长上下文推理变得昂贵。滑动窗口注意力 (SWA) 将这种成本降低为线性复杂度，但天真地在推理时为经过充分注意力 (FA) 预训练的模型启用完整的 SWA，会因训练与推理不匹配而导致严重的长上下文性能下降。这让我们想知道：经过 FA 预训练的 LLM 是否可以在不进行预训练的情况下很好地适应 SWA？我们通过提出滑动窗口注意适应（SWAA）来对此进行研究，这是一组结合了五种方法以实现更好适应的实用方法：（1）仅在预填充期间应用 SWA； (2) 保留“sink”代币； (3) FA/SWA层交错； （4）思想链（CoT）； (5)微调。我们的实验表明，SWA 适应是可行的，但并不简单：单一方法是不够的，但特定的协同组合可以有效恢复原始的长上下文性能。我们进一步分析了不同 SWAA 配置的性能与效率权衡，并为不同场景提供了推荐方案。我们的代码可在此 https URL 获取</li>
</ul>

<h3>Title: Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers</h3>
<ul>
<li><strong>Authors: </strong>Youmin Ko, Sungjong Seo, Hyunjoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10422">https://arxiv.org/abs/2512.10422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10422">https://arxiv.org/pdf/2512.10422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10422]] Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers(https://arxiv.org/abs/2512.10422)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{this https URL}</li>
<li><strong>摘要：</strong>由于大型语言模型 (LLM) 倾向于生成与事实不准确的输出，因此检索增强生成 (RAG) 作为减轻仅利用 LLM 的缺点的关键手段而受到了广泛关注。然而，现有的用于简单和多跳问答（QA）的 RAG 方法仍然容易出现错误检索和幻觉。为了解决这些限制，我们提出了 CoopRAG，一种用于问答任务的新型 RAG 框架，其中检索器和法学硕士通过交换信息知识相互协作，并且检索器模型的早期层和后期层相互协作，以准确地对与给定查询相关的检索到的文档进行排名。在此框架中，我们（i）将问题展开为子问题和推理链，其中不确定的位置被屏蔽，（ii）检索与子问题和推理链增强的问题相关的文档，（iii）通过对比检索器的层对文档进行重新排序，以及（iv）通过LLM填充屏蔽位置来重建推理链。我们的实验表明，在检索和 QA 性能方面，CoopRAG 在三个多跳 QA 数据集以及一个简单的 QA 数据集上始终优于最先进的 QA 方法。我们的代码可用。\footnote{此 https URL}</li>
</ul>

<h3>Title: T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Stoianov, Danil Taranets, Olga Tsymboi, Ramil Latypov, Almaz Dautov, Vladislav Kruglikov, Nikita Surkov, German Abramov, Pavel Gein, Dmitry Abulkhanov, Mikhail Gashkov, Viktor Zelenkovskiy, Artem Batalov, Aleksandr Medvedev, Anatolii Potapov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10430">https://arxiv.org/abs/2512.10430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10430">https://arxiv.org/pdf/2512.10430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10430]] T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground(https://arxiv.org/abs/2512.10430)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.</li>
<li><strong>摘要：</strong>我们推出 T-pro 2.0，这是一个开放式的俄罗斯法学硕士，用于混合推理和高效推理。该模型支持直接应答和推理跟踪生成，使用西里尔字母密集标记器和经过调整的 EAGLE 推测解码管道来减少延迟。为了实现可重复和可扩展的研究，我们发布了模型权重、T-Wix 500k 指令语料库、T-Math 推理基准以及 Hugging Face 上的 EAGLE 权重。这些资源允许用户学习俄语推理并扩展或调整模型和推理管道。公共网络演示公开了推理和非推理模式，并说明了我们的推理堆栈跨域实现的加速。因此，T-pro 2.0 作为一个可访问的开放系统，用于构建和评估高效、实用的俄罗斯法学硕士申请。</li>
</ul>

<h3>Title: Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature</h3>
<ul>
<li><strong>Authors: </strong>Agniva Maiti, Prajwal Panth, Suresh Chandra Satapathy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10435">https://arxiv.org/abs/2512.10435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10435">https://arxiv.org/pdf/2512.10435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10435]] Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature(https://arxiv.org/abs/2512.10435)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.</li>
<li><strong>摘要：</strong>科学文献的完整性和可靠性正面临对抗性文本生成技术的严重威胁，特别是使用自动释义工具来掩盖抄袭。这些工具生成“痛苦的短语”，统计上不可能的同义词（例如“人工智能”的“假意识”），它们保留了本地语法，同时模糊了原始来源。大多数现有的检测方法严重依赖于静态阻止列表或通用领域语言模型，这些模型对于新颖的混淆存在较高的假阴性率，并且无法确定抄袭内容的来源。在本文中，我们提出了对抗性抄袭的语义重建（SRAP），该框架不仅旨在检测这些异常，而且还可以在数学上恢复原始术语。我们使用两阶段架构：（1）使用标记级伪困惑度通过特定领域的屏蔽语言模型（SciBERT）进行统计异常检测，（2）使用密集向量检索（FAISS）和句子级对齐（SBERT）进行基于源的语义重建。在对抗性科学文本的并行语料库上进行的实验表明，虽然零样本基线完全失败（恢复精度为 0.00%），但我们的检索增强方法实现了 23.67% 的恢复精度，显着优于基线方法。我们还表明，静态决策边界对于行话较多的科学文本中的稳健检测是必要的，因为动态阈值在高方差下会失败。 SRAP 通过将混淆的表达式链接回其最可能的源文档来实现取证分析。</li>
</ul>

<h3>Title: Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT</h3>
<ul>
<li><strong>Authors: </strong>Nour El Houda Ben Chaabene, Hamza Hammami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10440">https://arxiv.org/abs/2512.10440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10440">https://arxiv.org/pdf/2512.10440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10440]] Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT(https://arxiv.org/abs/2512.10440)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.</li>
<li><strong>摘要：</strong>Claude、Mistral IA 和 GPT-4 等大型语言模型 (LLM) 在 NLP 方面表现出色，但缺乏结构化知识，导致事实不一致。我们通过 KG-BERT 集成知识图（KG）来解决这个问题，以增强基础和推理。实验表明，在知识密集型任务（例如问答和实体链接）方面取得了显着的进步。这种方法提高了事实可靠性，并实现了更具情境意识的下一代法学硕士。</li>
</ul>

<h3>Title: Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis</h3>
<ul>
<li><strong>Authors: </strong>Nour El Houda Ben Chaabene, Hamza Hammami, Laid Kahloul</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10441">https://arxiv.org/abs/2512.10441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10441">https://arxiv.org/pdf/2512.10441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10441]] Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis(https://arxiv.org/abs/2512.10441)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.</li>
<li><strong>摘要：</strong>本文提出了一种具有心理意识的对话代理，旨在提高教育环境中的学习表现和情绪健康。该系统结合了大型语言模型 (LLM)、知识图增强型 BERT (KG-BERT) 和双向长短期记忆 (LSTM)，注重实时对学生的认知和情感状态进行分类。与之前仅限于辅导或情感支持的聊天机器人不同，我们的方法利用多模态数据（包括文本语义、韵律语音特征和时间行为趋势）来推断参与度、压力和概念理解。一项针对大学生的试点研究表明，与基线方法相比，动机有所提高，压力减轻，学业成绩也有所提高。这些结果强调了整合语义推理、多模态融合和时间建模以支持适应性、以学生为中心的教育干预的前景。</li>
</ul>

<h3>Title: Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lars G.B. Johnsen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10453">https://arxiv.org/abs/2512.10453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10453">https://arxiv.org/pdf/2512.10453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10453]] Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs(https://arxiv.org/abs/2512.10453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation. We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.</li>
<li><strong>摘要：</strong>什么算作句法结构的证据？在传统的生成语法中，语法上的系统对比，例如主语助动词倒装和寄生间隙的许可，被视为内部的层次语法的证据。在本文中，我们测试仅在表面形式上训练的大型语言模型（LLM）是否以暗示底层结构表示的方式再现这些对比。我们关注两个经典结构：主语-辅助倒置（测试主语边界的识别）和寄生间隙许可（测试抽象依赖结构）。我们使用提示来评估包括 GPT-4 和 LLaMA-3 在内的模型，从而得出可接受性评级。结果表明，法学硕士能够可靠地区分两种结构中的语法变体和非语法变体，因此支持它们对结构敏感而不仅仅是线性顺序。与认知知识不同，结构概括是从表面形式的预测训练中产生的，这表明在没有明确编码的情况下对句法的功能敏感性。</li>
</ul>

<h3>Title: XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Iñaki Lacunza, José Javier Saiz, Alexander Shvets, Aitor Gonzalez-Agirre, Marta Villegas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10545">https://arxiv.org/abs/2512.10545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10545">https://arxiv.org/pdf/2512.10545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10545]] XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs(https://arxiv.org/abs/2512.10545)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.</li>
<li><strong>摘要：</strong>当前的大型语言模型 (LLM) 是根据大量文本数据进行训练的，这些文本数据主要来自几种主流语言。研究表明，这种对高资源语言（例如英语）的过度依赖会影响法学硕士在中资源和低资源语言方面的表现。为了缓解这个问题，我们建议（i）通过在领域重新权重 DoGE 算法中训练一个小型代理模型来优化语言分布，我们将其扩展到 XDoGE 以实现多语言设置，以及（ii）重新调整数据并使用已建立的语言权重从头开始或在连续预训练阶段（CPT）中训练全尺寸模型。我们的目标是六种具有多种地理关系以及语系内部和语系间关系的语言，即英语和西班牙语（高资源）、葡萄牙语和加泰罗尼亚语（中等资源）、加利西亚语和巴斯克语（低资源）。我们用 Salamandra-2b 进行实验，它是这些语言的一个有前途的模型。我们使用 IberoBench 框架进行定量评估，研究大量数据重复对小语种的影响以及欠采样对主导语言的影响。最后，我们发布了一个新的有前途的 IberianLLM-7B-Instruct 模型，以伊比利亚语言和英语为中心，我们从头开始对其进行预训练，并使用带有 XDoGE 权重的 CPT 进一步改进。</li>
</ul>

<h3>Title: Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models</h3>
<ul>
<li><strong>Authors: </strong>Amartya Roy, Elamparithy M, Kripabandhu Ghosh, Ponnurangam Kumaraguru, Adrian de Wynter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10561">https://arxiv.org/abs/2512.10561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10561">https://arxiv.org/pdf/2512.10561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10561]] Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models(https://arxiv.org/abs/2512.10561)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 支撑着大型语言模型 (LLM) 的最新进展，尽管其在因果推理中的作用和表现仍不清楚。因果推理需要多跳组合和严格的连接控制，并且对输入的虚假词汇关系的依赖可能会提供误导性的结果。我们假设，由于编码器和编码器解码器架构能够将输入投影到潜在空间，因此与仅解码器模型相比，它们更适合所述多跳联合推理。为此，我们在自然语言和非自然语言场景中将所有上述架构的微调版本与零次和少量 ICL 进行比较。我们发现仅 ICL 不足以进行可靠的因果推理，通常会过度关注不相关的输入特征。特别是，仅解码器模型对于分布变化明显很脆弱，而经过微调的编码器和编码器解码器模型可以在我们的测试中更稳健地泛化，包括非自然语言分割。这两种架构只能在大规模上被仅解码器架构所匹配或超越。我们最后指出，为了实现成本效益、短期鲁棒因果推理，具有针对性微调的编码器或编码器解码器架构是更可取的。</li>
</ul>

<h3>Title: RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Hang Ding, Qiming Feng, Dongqi Liu, Qi Zhao, Tao Yao, Shuo Wang, Dongsheng Chen, Jian Li, Zhenye Gan, Jiangning Zhang, Chengjie Wang, Yabiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10575">https://arxiv.org/abs/2512.10575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10575">https://arxiv.org/pdf/2512.10575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10575]] RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems(https://arxiv.org/abs/2512.10575)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.</li>
<li><strong>摘要：</strong>奖励建模已成为使大型语言模型 (LLM) 与人类偏好保持一致的基石。然而，当扩展到角色扮演等主观和开放式领域时，现有的奖励模型表现出严重退化，难以捕捉细致入微且基于角色的人类判断。为了解决这一差距，我们引入了 RoleRMBench，这是角色扮演对话中奖励建模的第一个系统基准，涵盖从叙事管理到角色一致性和参与度的七个细粒度功能。对 RoleRMBench 的评估揭示了通用奖励模型与人类判断之间存在巨大且一致的差距，特别是在叙事和风格维度上。我们进一步提出了 RoleRM，一种用连续隐式偏好（CIP）训练的奖励模型，它将主观评价重新表述为多种结构策略下的连续一致的成对监督。综合实验表明，RoleRM 平均超过强大的开源和闭源奖励模型 24% 以上，证明了叙事连贯性和风格保真度方面的巨大进步。我们的研究结果强调了连续偏好表示和注释一致性的重要性，为以人为中心的对话系统中的主观对齐奠定了基础。</li>
</ul>

<h3>Title: AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Bo Yang, Lanfei Feng, Yunkui Chen, Yu Zhang, Jianyu Zhang, Xiao Xu, Nueraili Aierken, Shijian Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10624">https://arxiv.org/abs/2512.10624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10624">https://arxiv.org/pdf/2512.10624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10624]] AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence(https://arxiv.org/abs/2512.10624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.</li>
<li><strong>摘要：</strong>尽管多模态大语言模型取得了快速进展，但农业应用仍然受到缺乏多语言语音数据、统一的多模态架构和综合评估基准的限制。为了应对这些挑战，我们提出了 AgriGPT-Omni，这是一个农业全方位框架，它将语音、视觉和文本集成在一个统一的框架中。首先，我们构建了一个可扩展的数据合成和收集管道，将农业文本和图像转换为训练数据，从而产生了迄今为止最大的农业语音数据集，包括跨六种语言的 492K 合成语音样本和 1.4K 真实语音样本。其次，基于此，我们通过三阶段范式训练第一个农业全模型：文本知识注入、渐进式多模态对齐和基于GRPO的强化学习，实现跨语言和模态的统一推理。第三，我们提出了 AgriBench-Omni-2K，这是第一个农业三模态基准，涵盖多种语音-视觉-文本任务和多语言切片，具有标准化协议和可重复工具。实验表明，AgriGPT-Omni 在多语言和多模式推理以及现实世界语音理解方面显着优于通用基线。所有模型、数据、基准和代码都将被发布，以促进可重复研究、包容性农业智能和资源匮乏地区的可持续人工智能发展。</li>
</ul>

<h3>Title: From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Smiljana Antonijevic Ubois</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10630">https://arxiv.org/abs/2512.10630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10630">https://arxiv.org/pdf/2512.10630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10630]] From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages(https://arxiv.org/abs/2512.10630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.</li>
<li><strong>摘要：</strong>大型语言模型通常是在英语等主导语言上进行训练的，它们对低资源语言的表示通常反映了源语言材料中存在的文化和语言偏见。本研究以塞尔维亚语为案例，探讨了影响人工智能时代低资源语言语言技术发展的结构、历史和社会技术因素。通过对十位学者和从业者（包括语言学家、数字人文学家和人工智能开发人员）的半结构化访谈，它追溯了塞尔维亚文本遗产历史性破坏所带来的挑战，而当代问题推动了简化、工程优先的方法优先考虑功能而非语言的细微差别，这些挑战加剧了这些挑战。其中包括肤浅的音译、对英语训练模型的依赖、数据偏见以及缺乏文化特异性的数据集管理。为了应对这些挑战，该研究提出了 Data Care，这是一个基于 CARE 原则（集体利益、控制权、责任和道德）的框架，它将偏见缓解从事后技术修复重新定义为语料库设计、注释和治理的一个组成部分，并将 Data Care 定位为在传统法学硕士发展再现现有权力不平衡和文化盲点的背景下构建包容性、可持续和文化基础的语言技术的可复制模型。</li>
</ul>

<h3>Title: Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Rebekka Görge, Sujan Sai Gannamaneni, Tabea Naeven, Hammam Abdelwahab, Héctor Allende-Cid, Armin B. Cremers, Lennard Helmer, Michael Mock, Anna Schmitz, Songkai Xue, Elif Yildirir, Maximilian Poretschkin, Stefan Wrobel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10734">https://arxiv.org/abs/2512.10734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10734">https://arxiv.org/pdf/2512.10734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10734]] Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation(https://arxiv.org/abs/2512.10734)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.</li>
<li><strong>摘要：</strong>用于训练大型语言模型 (LLM) 的文本数据表现出多方面的偏见表现，包括有害语言和倾斜的人口分布。 《欧洲人工智能法案》等法规要求识别和减轻数据中针对受保护群体的偏见，最终目标是防止不公平的模型输出。但缺乏实际指导和可操作性。我们提出了一个全面的数据偏差检测和缓解管道，包括四个组件，可解决两种数据偏差类型，即表示偏差和可配置敏感属性的（显式）构造型。首先，我们利用根据质量标准创建的法学硕士生成的单词列表来检测相关的组标签。其次，使用人口代表性得分来量化代表性偏差。第三，我们使用社会语言学过滤来检测和减轻刻板印象。最后，我们通过语法和上下文感知的反事实数据增强来补偿表示偏差。我们使用性别、宗教和年龄的例子进行双重评估。首先，通过人工验证和基线比较来评估每个单独组件对数据去偏的有效性。研究结果表明，我们成功减少了文本数据集中的表征偏差和（明确的）刻板印象。其次，通过对多个模型（0.6B-8B 参数）进行偏差基准测试来评估数据去偏差对模型偏差减少的影响，并在去偏差文本数据集上进行微调。该评估表明，对去偏差数据进行微调的法学硕士在偏差基准上的表现并不一致，暴露了当前评估方法中的关键差距，并强调需要进行有针对性的数据操作来解决明显的模型偏差。</li>
</ul>

<h3>Title: Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Songyang Gao, Yuzhe Gu, Zijian Wu, Lingkai Kong, Wenwei Zhang, Zhongrui Cai, Fan Zheng, Tianyou Ma, Junhao Shen, Haiteng Zhao, Duanyang Zhang, Huilun Zhang, Kuikun Liu, Chengqi Lyu, Yanhui Duan, Chiyu Chen, Ningsheng Ma, Jianfei Gao, Han Lyu, Dahua Lin, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10739">https://arxiv.org/abs/2512.10739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10739">https://arxiv.org/pdf/2512.10739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10739]] Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving(https://arxiv.org/abs/2512.10739)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了重大进展。这一进步也与可靠验证者的自动化监督密不可分。然而，当前基于结果的验证者（OV）无法检查长推理思想链（CoT）中不可靠的中间步骤。与此同时，当前基于流程的验证器 (PV) 难以可靠地检测复杂的长 CoT 中的错误，这是由于人工注释成本过高而导致高质量注释稀缺的限制。因此，我们提出了基于\textbf{O}结果的\textbf{P}过程\textbf{V}erifier（OPV），它验证长CoT总结结果的基本原理过程，以实现准确有效的验证并实现大规模注释。为了增强所提出的验证者的能力，我们采用带有专家注释的迭代主动学习框架，以更少的注释成本逐步提高 OPV 的验证能力。具体来说，在每次迭代中，当前最佳 OPV 中最不确定的情况都会被注释，然后用于通过拒绝微调 (RFT) 和 RLVR 为下一轮训练新的 OPV。大量的实验证明了OPV的优越性能和广泛的适用性。它在我们保留的 \textsc{\thisbench} 上取得了新的最先进结果，优于 Qwen3-Max-Preview 等更大的开源模型，F1 分数为 83.1，而 F1 分数为 76.3。此外，OPV 可以有效检测合成数据集中的误报，与专家评估紧密结合。与策略模型协作时，OPV 始终能带来性能提升，例如，随着计算预算的扩展，AIME2025 上的 DeepSeek-R1-Distill-Qwen-32B 的准确性从 55.2\% 提高到 73.3\%。</li>
</ul>

<h3>Title: TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage</h3>
<ul>
<li><strong>Authors: </strong>Elroy Galbraith, Chadwick Sutherland, Donahue Morgan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10741">https://arxiv.org/abs/2512.10741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10741">https://arxiv.org/pdf/2512.10741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10741]] TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage(https://arxiv.org/abs/2512.10741)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails. The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss. We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.</li>
<li><strong>摘要：</strong>紧急语音识别系统在非标准英语品种上表现出系统性性能下降，从而在为加勒比人口提供的服务方面造成了严重差距。我们推出了 TRIDENT（调度员授权的全国分诊的转录和路由智能），这是一种三层调度员支持架构，旨在构建紧急呼叫输入，以便人类应用已建立的分诊协议（用于例行操作的 ESI 和用于大规模伤亡事件的 START），即使在自动语音识别失败时也是如此。该系统结合了加勒比口音的 ASR、通过大型语言模型提取本地实体以及生物声学遇险检测，为调度员提供三个互补信号：转录置信度、结构化临床实体和声音压力指标。我们的主要见解是，较低的 ASR 置信度，而不是代表系统故障，可以作为一个有价值的队列优先级信号，特别是当与升高的声音困扰标记相结合时，表明呼叫者处于危机中，其语音可能已转向基础语言语域。互补的洞察力驱动实体提取层：训练有素的响应者和组成的旁观者可以在不增加声音压力的情况下报告危及生命的紧急情况，需要语义分析来捕获副语言特征错过的临床指标。我们描述了压力引起的语码转换的架构设计、心理语言学研究的理论基础，以及灾难场景下离线操作的部署注意事项。这项工作为口音弹性紧急人工智能建立了一个框架，确保加勒比声音能够公平地获得已建立的国家分类协议。对加勒比紧急呼叫的实证验证仍然是未来的工作。</li>
</ul>

<h3>Title: OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification</h3>
<ul>
<li><strong>Authors: </strong>Zijian Wu, Lingkai Kong, Wenwei Zhang, Songyang Gao, Yuzhe Gu, Zhongrui Cai, Tianyou Ma, Yuhong Liu, Zhi Wang, Runyuan Ma, Guangyu Wang, Wei Li, Conghui He, Dahua Lin, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10756">https://arxiv.org/abs/2512.10756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10756">https://arxiv.org/pdf/2512.10756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10756]] OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification(https://arxiv.org/abs/2512.10756)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了重大进展。这一进步也与可靠验证者的自动化监督密不可分。然而，当前基于结果的验证者（OV）无法检查长推理思想链（CoT）中不可靠的中间步骤。与此同时，当前基于流程的验证器 (PV) 难以可靠地检测复杂的长 CoT 中的错误，这是由于人工注释成本过高而导致高质量注释稀缺的限制。因此，我们提出了基于结果的过程验证器（OPV），它验证长 CoT 总结结果的基本原理过程，以实现准确高效的验证并实现大规模注释。为了增强所提出的验证者的能力，我们采用带有专家注释的迭代主动学习框架，以更少的注释成本逐步提高 OPV 的验证能力。具体来说，在每次迭代中，当前最佳 OPV 中最不确定的情况都会被注释，然后用于通过拒绝微调 (RFT) 和 RLVR 为下一轮训练新的 OPV。大量的实验证明了OPV的优越性能和广泛的适用性。它在我们的 OPV-Bench 上取得了最先进的结果，其 F1 分数为 83.1，优于 Qwen3-Max-Preview 等更大的开源模型，而 F1 分数为 76.3。此外，OPV 可以有效检测合成数据集中的误报，与专家评估紧密结合。与策略模型协作时，OPV 始终能带来性能提升，例如，随着计算预算的扩大，AIME2025 上 DeepSeek-R1-Distill-Qwen-32B 的准确性从 55.2% 提高到 73.3%。</li>
</ul>

<h3>Title: Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Kevin Glocker, Kätriin Kukk, Romina Oji, Marcel Bollmann, Marco Kuhlmann, Jenny Kunz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10772">https://arxiv.org/abs/2512.10772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10772">https://arxiv.org/pdf/2512.10772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10772]] Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation(https://arxiv.org/abs/2512.10772)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.</li>
<li><strong>摘要：</strong>实现包括中等和较低资源语言的高性能语言模型仍然是一个挑战。与特定于语言的适应相比，大规模多语言模型仍然表现不佳，尤其是在较小的模型规模下。在这项工作中，我们研究了扩展作为使预训练模型适应新目标语言的有效策略。通过对大约 FLOP 匹配的模型进行全面的缩放消融，我们测试了升级英语基础模型是否能够比标准的持续预训练更有效、更节省资源的适应。我们发现，一旦接触到足够的目标语言数据，较大的升级模型可以匹配或超过持续对更多数据进行预训练的较小模型的性能，这证明了扩展对数据效率的好处。缩放还有助于保留基本模型的英语功能，从而减少灾难性遗忘。最后，我们探讨是否可以合并这种规模化的、特定于语言的模型来构建模块化且灵活的多语言系统。我们发现，虽然合并仍然不如联合多语言训练有效，但大规模合并比小型合并表现更好。我们观察到合并方法之间存在巨大的性能差异，这表明通过专门用于语言级集成的合并方法有改进的潜力。</li>
</ul>

<h3>Title: Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting</h3>
<ul>
<li><strong>Authors: </strong>Manurag Khullar, Utkarsh Desai, Poorva Malviya, Aman Dalmia, Zheyuan Ryan Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10780">https://arxiv.org/abs/2512.10780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10780">https://arxiv.org/pdf/2512.10780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10780]] Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting(https://arxiv.org/abs/2512.10780)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地部署在印度的高风险临床应用中。在许多这样的环境中，印度语言的使用者经常使用罗马化文本而不是本地脚本进行交流，但现有的研究很少使用现实世界的数据来评估这种拼写变化。我们研究罗马化如何影响关键领域法学硕士的可靠性：孕产妇和新生儿医疗保健分诊。我们根据用户生成的查询（涵盖五种印度语言和尼泊尔语）的真实世界数据集对领先的法学硕士进行基准测试。我们的结果表明，罗马化消息的性能持续下降，F1 分数落后于本机脚本 5-12 分。在我们位于印度的合作孕产妇保健组织中，这一差距可能会导致近 200 万次分诊错误。至关重要的是，脚本的这种性能差距并不是由于临床推理的失败造成的。我们证明法学硕士通常可以正确推断罗马化查询的语义意图。然而，在罗马化输入中存在拼写噪声的情况下，它们的最终分类输出仍然很脆弱。我们的研究结果强调了基于法学硕士的卫生系统中的一个关键安全盲点：看似理解罗马化输入的模型可能仍然无法可靠地对其采取行动。</li>
</ul>

<h3>Title: The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality</h3>
<ul>
<li><strong>Authors: </strong>Aileen Cheng, Alon Jacovi, Amir Globerson, Ben Golan, Charles Kwong, Chris Alberti, Connie Tao, Eyal Ben-David, Gaurav Singh Tomar, Lukas Haas, Yonatan Bitton, Adam Bloniarz, Aijun Bai, Andrew Wang, Anfal Siddiqui, Arturo Bajuelos Castillo, Aviel Atias, Chang Liu, Corey Fry, Daniel Balle, Deepanway Ghosal, Doron Kukliansky, Dror Marcus, Elena Gribovskaya, Eran Ofek, Honglei Zhuang, Itay Laish, Jan Ackermann, Lily Wang, Meg Risdal, Megan Barnes, Michael Fink, Mohamed Amin, Moran Ambar, Natan Potikha, Nikita Gupta, Nitzan Katz, Noam Velan, Ofir Roval, Ori Ram, Polina Zablotskaia, Prathamesh Bang, Priyanka Agrawal, Rakesh Ghiya, Sanjay Ganapathy, Simon Baumgartner, Sofia Erell, Sushant Prakash, Thibault Sellam, Vikram Rao, Xuanhui Wang, Yaroslav Akulov, Yulong Yang, Zhen Yang, Zhixin Lai, Zhongru Wu, Anca Dragan, Avinatan Hassidim, Fernando Pereira, Slav Petrov, Srinivasan Venkatachary, Tulsee Doshi, Yossi Matias, Sasha Goldshtein, Dipanjan Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10791">https://arxiv.org/abs/2512.10791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10791">https://arxiv.org/pdf/2512.10791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10791]] The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality(https://arxiv.org/abs/2512.10791)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at this https URL .</li>
<li><strong>摘要：</strong>我们推出了 FACTS Leaderboard，这是一个在线排行榜套件和一组相关的基准，可全面评估语言模型在不同场景中生成事实准确文本的能力。该套件通过汇总四个不同子排行榜上模型的表现，提供了对事实性的整体衡量：(1) FACTS Multimodal，衡量对基于图像的问题的回答的真实性； (2) FACTS Parametric，通过回答内部参数的闭门陈述问题来评估模型的世界知识； (3) FACTS Search，评估信息查找场景中的事实性，其中模型必须使用搜索 API； (4) FACTS Grounding (v2)，评估长篇回答是否基于所提供的文件，其特点是显着改进的法官模型。每个子排行榜均采用自动判断模型对模型响应进行评分，最终的套件分数是四个部分的平均值，旨在对模型的整体真实性提供稳健且平衡的评估。 FACTS 排行榜套件将得到积极维护，包含公共和私人分裂，以允许外部参与，同时维护其完整性。可以在此 https URL 找到它。</li>
</ul>

<h3>Title: LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Michael Schlee, Christoph Weisser, Timo Kivimäki, Melchizedek Mashiku, Benjamin Saefken</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10793">https://arxiv.org/abs/2512.10793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10793">https://arxiv.org/pdf/2512.10793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10793]] LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification(https://arxiv.org/abs/2512.10793)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.</li>
<li><strong>摘要：</strong>LabelFusion 是一种用于文本分类的融合集成，它学习将传统的基于 Transformer 的分类器（例如 RoBERTa）与一个或多个大型语言模型（LLM，例如 OpenAI GPT、Google Gemini 或 DeepSeek）相结合，以跨多类和多标签任务提供准确且具有成本意识的预测。该软件包提供了一个简单的高级接口（AutoFusionClassifier），可以以最少的配置端到端地训练完整的管道，并为高级用户提供灵活的 API。在底层，LabelFusion 通过将 ML 主干的嵌入与 LLM 派生的每类分数（通过结构化提示工程策略获得）连接起来，集成了来自两个来源的矢量信号，并将这种联合表示馈送到生成最终预测的紧凑多层感知器 (FusionMLP) 中。这种学习融合方法捕捉了 LLM 推理和传统基于 Transformer 的分类器的互补优势，在跨领域产生强大的性能 - 在 AG News 上实现 92.4% 的准确率，在 10 类Reuters 21578 主题分类上实现 92.3% 的准确率 - 同时实现准确度、延迟和成本之间的实际权衡。</li>
</ul>

<h3>Title: Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity</h3>
<ul>
<li><strong>Authors: </strong>Hauke Licht</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10882">https://arxiv.org/abs/2512.10882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10882">https://arxiv.org/pdf/2512.10882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10882]] Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity(https://arxiv.org/abs/2512.10882)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.</li>
<li><strong>摘要：</strong>情感是政治的核心，分析情感在政治传播中的作用有着悠久的传统。随着研究越来越多地利用视听材料来分析情感的表达，多模态生成人工智能的出现有望取得巨大进步。然而，我们缺乏关于多模式人工智能在情感分析中有效性的证据。本文通过评估当前多模态大语言模型（mLLM）在基于视频的人类标记视频记录的两个互补数据集中的情绪唤醒分析中来解决这一差距。我发现在理想情况下，mLLM 的情绪唤醒评级非常可靠，并且几乎没有显示出人口统计学偏见的迹象。然而，在现实世界议会辩论中发言者的录音中，mLLM 的唤醒评级未能兑现这一承诺，对下游统计推断可能产生负面影响。因此，这项研究强调需要对政治分析中新兴的生成人工智能方法进行持续、彻底的评估，并提供一个合适的可复制框架。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
