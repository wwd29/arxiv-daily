<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-15</h1>
<h3>Title: Large Models of What? Mistaking Engineering Achievements for Human Linguistic Agency</h3>
<ul>
<li><strong>Authors: </strong>Abeba Birhane, Marek McGann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Large Models of What? Mistaking Engineering Achievements for Human Linguistic Agency(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>In this paper we argue that key, often sensational and misleading, claims regarding linguistic capabilities of Large Language Models (LLMs) are based on at least two unfounded assumptions; the assumption of language completeness and the assumption of data completeness. Language completeness assumes that a distinct and complete thing such as `a natural language' exists, the essential characteristics of which can be effectively and comprehensively modelled by an LLM. The assumption of data completeness relies on the belief that a language can be quantified and wholly captured by data. Work within the enactive approach to cognitive science makes clear that, rather than a distinct and complete thing, language is a means or way of acting. Languaging is not the kind of thing that can admit of a complete or comprehensive modelling. From an enactive perspective we identify three key characteristics of enacted language; embodiment, participation, and precariousness, that are absent in LLMs, and likely incompatible in principle with current architectures. We argue that these absences imply that LLMs are not now and cannot in their present form be linguistic agents the way humans are. We illustrate the point in particular through the phenomenon of `algospeak', a recently described pattern of high stakes human language activity in heavily controlled online environments. On the basis of these points, we conclude that sensational and misleading claims about LLM agency and capabilities emerge from a deep misconception of both what human language is and what LLMs are.</li>
<li><strong>摘要：</strong>在本文中，我们认为，关于大型语言模型 (LLM) 语言能力的关键主张（通常具有煽情性和误导性）至少基于两个毫无根据的假设：语言完整性假设和数据完整性假设。语言完整性假设存在一种独特而完整的事物，例如“自然语言”，其基本特征可以通过 LLM 有效而全面地建模。数据完整性假设依赖于这样一种信念：语言可以被量化并完全被数据捕获。认知科学中的实施方法研究表明，语言是一种手段或行为方式，而不是一种独特而完整的东西。语言不是那种可以进行完整或全面建模的东西。从实施的角度来看，我们确定了实施语言的三个关键特征：具体化、参与和不稳定性，这些特征在 LLM 中是不存在的，并且可能在原则上与当前架构不兼容。我们认为，这些缺失意味着 LLM 现在不是也不可能以目前的形式成为像人类一样的语言主体。我们特别通过“算法语言”现象来说明这一点，算法语言是最近描述的在严格控制的在线环境中高风险人类语言活动的模式。基于这些观点，我们得出结论，关于 LLM 主体和能力的耸人听闻和误导性说法源于对人类语言和 LLM 的深刻误解。</li>
</ul>

<h3>Title: MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Valentin Hoffman, Tomasz Limisiewicz, Yulia Tsvetkov, Noah A. Smith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In multilingual settings, non-Latin scripts and low-resource languages are usually disadvantaged in terms of language models' utility, efficiency, and cost. Specifically, previous studies have reported multiple modeling biases that the current tokenization algorithms introduce to non-Latin script languages, the main one being over-segmentation. In this work, we propose MAGNET; multilingual adaptive gradient-based tokenization to reduce over-segmentation via adaptive gradient-based subword tokenization. MAGNET learns to predict segment boundaries between byte tokens in a sequence via sub-modules within the model, which act as internal boundary predictors (tokenizers). Previous gradient-based tokenization methods aimed for uniform compression across sequences by integrating a single boundary predictor during training and optimizing it end-to-end through stochastic reparameterization alongside the next token prediction objective. However, this approach still results in over-segmentation for non-Latin script languages in multilingual settings. In contrast, MAGNET offers a customizable architecture where byte-level sequences are routed through language-script-specific predictors, each optimized for its respective language script. This modularity enforces equitable segmentation granularity across different language scripts compared to previous methods. Through extensive experiments, we demonstrate that in addition to reducing segmentation disparities, MAGNET also enables faster language modelling and improves downstream utility.</li>
<li><strong>摘要：</strong>在多语言环境中，非拉丁文字和资源匮乏的语言通常在语言模型的实用性、效率和成本方面处于劣势。具体而言，先前的研究报告了当前标记化算法为非拉丁文字语言引入的多种建模偏差，其中最主要的是过度分割。在这项工作中，我们提出了 MAGNET；多语言自适应梯度标记化，通过基于自适应梯度的子词标记化来减少过度分割。MAGNET 通过模型中的子模块学习预测序列中字节标记之间的分段边界，这些子模块充当内部边界预测器（标记器）。以前的基于梯度的标记化方法旨在通过在训练期间集成单个边界预测器并通过随机重新参数化对其进行端到端优化以及下一个标记预测目标来实现跨序列的统一压缩。然而，这种方法仍然会导致多语言环境中非拉丁文字语言的过度分割。相比之下，MAGNET 提供了一种可定制的架构，其中字节级序列通过特定于语言脚本的预测器进行路由，每个预测器都针对其各自的语言脚本进行了优化。与以前的方法相比，这种模块化可以实现不同语言脚本之间的公平分割粒度。通过大量实验，我们证明，除了减少分割差异之外，MAGNET 还可以加快语言建模速度并提高下游效用。</li>
</ul>

<h3>Title: Rule-Based, Neural and LLM Back-Translation: Comparative Insights from a Variant of Ladin</h3>
<ul>
<li><strong>Authors: </strong>Samuel Frontull, Georg Moser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Rule-Based, Neural and LLM Back-Translation: Comparative Insights from a Variant of Ladin(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper explores the impact of different back-translation approaches on machine translation for Ladin, specifically the Val Badia variant. Given the limited amount of parallel data available for this language (only 18k Ladin-Italian sentence pairs), we investigate the performance of a multilingual neural machine translation model fine-tuned for Ladin-Italian. In addition to the available authentic data, we synthesise further translations by using three different models: a fine-tuned neural model, a rule-based system developed specifically for this language pair, and a large language model. Our experiments show that all approaches achieve comparable translation quality in this low-resource scenario, yet round-trip translations highlight differences in model performance.</li>
<li><strong>摘要：</strong>本文探讨了不同的回译方法对拉丹语机器翻译的影响，特别是对 Val Badia 变体的影响。鉴于此语言可用的并行数据量有限（仅 18k 个拉丹语-意大利语句子对），我们研究了针对拉丹语-意大利语微调的多语言神经机器翻译模型的性能。除了可用的真实数据外，我们还使用三个不同的模型来合成进一步的翻译：微调神经模型、专门为此语言对开发的基于规则的系统和大型语言模型。我们的实验表明，在这种资源匮乏的情况下，所有方法都能实现相当的翻译质量，但往返翻译凸显了模型性能的差异。</li>
</ul>

<h3>Title: Proving that Cryptic Crossword Clue Answers are Correct</h3>
<ul>
<li><strong>Authors: </strong>Martin Andrews, Sam Witteveen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Proving that Cryptic Crossword Clue Answers are Correct(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Cryptic crossword clues are challenging cognitive tasks, for which new test sets are released on a daily basis by multiple international newspapers. Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and `wordplay' that proves that the answer is correct (i.e. a human solver can be confident that an answer is correct without needing crossing words to confirm it). Using an existing cryptic wordplay proving framework (operating on Python proofs created by an LLM), we show that it is possible to distinguish between correct answers and almost-correct ones based upon whether the wordplay `works'.</li>
<li><strong>摘要：</strong>填字游戏线索是一项具有挑战性的认知任务，多家国际报纸每天都会发布新的测试集。每条填字游戏线索都包含填字游戏网格中要放置的答案的定义（与常规填字游戏相同），以及证明答案正确的“文字游戏”（即，人类解谜者可以确信答案是正确的，而无需通过填字游戏来确认）。使用现有的填字游戏证明框架（基于 LLM 创建的 Python 证明），我们表明可以根据文字游戏是否“有效”来区分正确答案和几乎正确的答案。</li>
</ul>

<h3>Title: Fault Diagnosis in Power Grids with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Liu Jing, Amirul Rahman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Fault Diagnosis in Power Grids with Large Language Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, chain-of-thought, tree-of-thought</a></li>
<li><strong>Abstract: </strong>Power grid fault diagnosis is a critical task for ensuring the reliability and stability of electrical infrastructure. Traditional diagnostic systems often struggle with the complexity and variability of power grid data. This paper proposes a novel approach that leverages Large Language Models (LLMs), specifically ChatGPT and GPT-4, combined with advanced prompt engineering to enhance fault diagnosis accuracy and explainability. We designed comprehensive, context-aware prompts to guide the LLMs in interpreting complex data and providing detailed, actionable insights. Our method was evaluated against baseline techniques, including standard prompting, Chain-of-Thought (CoT), and Tree-of-Thought (ToT) methods, using a newly constructed dataset comprising real-time sensor data, historical fault records, and component descriptions. Experimental results demonstrate significant improvements in diagnostic accuracy, explainability quality, response coherence, and contextual understanding, underscoring the effectiveness of our approach. These findings suggest that prompt-engineered LLMs offer a promising solution for robust and reliable power grid fault diagnosis.</li>
<li><strong>摘要：</strong>电网故障诊断是确保电力基础设施可靠性和稳定性的关键任务。传统的诊断系统经常难以应对电网数据的复杂性和多变性。本文提出了一种新方法，该方法利用大型语言模型 (LLM)，特别是 ChatGPT 和 GPT-4，结合先进的提示工程来提高故障诊断的准确性和可解释性。我们设计了全面的、上下文感知的提示来指导 LLM 解释复杂数据并提供详细的可操作见解。我们的方法与基线技术进行了评估，包括标准提示、思维链 (CoT) 和思维树 (ToT) 方法，使用新构建的数据集，该数据集包含实时传感器数据、历史故障记录和组件描述。实验结果表明，诊断准确性、可解释性质量、响应连贯性和上下文理解都有显著提高，凸显了我们方法的有效性。这些发现表明，提示工程 LLM 为稳健可靠的电网故障诊断提供了一种有希望的解决方案。</li>
</ul>

<h3>Title: Evaluating Nuanced Bias in Large Language Model Free Response Answers</h3>
<ul>
<li><strong>Authors: </strong>Jennifer Healey, Laurie Byrum, Md Nadeem Akhtar, Moumita Sinha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Evaluating Nuanced Bias in Large Language Model Free Response Answers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Pre-trained large language models (LLMs) can now be easily adapted for specific business purposes using custom prompts or fine tuning. These customizations are often iteratively re-engineered to improve some aspect of performance, but after each change businesses want to ensure that there has been no negative impact on the system's behavior around such critical issues as bias. Prior methods of benchmarking bias use techniques such as word masking and multiple choice questions to assess bias at scale, but these do not capture all of the nuanced types of bias that can occur in free response answers, the types of answers typically generated by LLM systems. In this paper, we identify several kinds of nuanced bias in free text that cannot be similarly identified by multiple choice tests. We describe these as: confidence bias, implied bias, inclusion bias and erasure bias. We present a semi-automated pipeline for detecting these types of bias by first eliminating answers that can be automatically classified as unbiased and then co-evaluating name reversed pairs using crowd workers. We believe that the nuanced classifications our method generates can be used to give better feedback to LLMs, especially as LLM reasoning capabilities become more advanced.</li>
<li><strong>摘要：</strong>现在，可以使用自定义提示或微调，轻松调整预训练的大型语言模型 (LLM) 以适应特定的业务目的。这些自定义通常经过迭代重新设计，以改善某些方面的性能，但每次更改之后，企业都希望确保系统在诸如偏见等关键问题上的行为不会受到负面影响。先前的基准偏差方法使用诸如单词掩码和多项选择题之类的技术来大规模评估偏见，但这些方法无法捕捉自由回答答案中可能出现的所有细微偏见类型，而自由回答答案通常是由 LLM 系统生成的答案类型。在本文中，我们确定了自由文本中几种无法通过多项选择测试同样识别的细微偏见。我们将它们描述为：置信偏差、隐含偏差、包含偏差和擦除偏差。我们提出了一种半自动化流程来检测这些类型的偏见，首先消除可以自动归类为无偏见的答案，然后使用众包工作者共同评估姓名反转对。我们相信，我们的方法生成的细微分类可用于为 LLM 提供更好的反馈，特别是当 LLM 推理能力变得更加先进时。</li>
</ul>

<h3>Title: Automatic Pruning of Fine-tuning Datasets for Transformer-based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Tayaranian, Seyyed Hasan Mozafari, Brett H. Meyer, James J. Clark, Warren J. Gross</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Automatic Pruning of Fine-tuning Datasets for Transformer-based Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Transformer-based language models have shown state-of-the-art performance on a variety of natural language understanding tasks. To achieve this performance, these models are first pre-trained on general corpus and then fine-tuned on downstream tasks. Previous work studied the effect of pruning the training set of the downstream tasks on the performance of the model on its evaluation set. In this work, we propose an automatic dataset pruning method for the training set of fine-tuning tasks. Our method is based on the model's success rate in correctly classifying each training data point. Unlike previous work which relies on user feedback to determine subset size, our method automatically extracts training subsets that are adapted for each pair of model and fine-tuning task. Our method provides multiple subsets for use in dataset pruning that navigate the trade-off between subset size and evaluation accuracy. Our largest subset, which we also refer to as the winning ticket subset, is on average $3 \times$ smaller than the original training set of the fine-tuning task. Our experiments on 5 downstream tasks and 2 language models show that, on average, fine-tuning on the winning ticket subsets results in a $0.1 \%$ increase in the evaluation performance of the model.</li>
<li><strong>摘要：</strong>基于 Transformer 的语言模型在各种自然语言理解任务中都表现出了最佳性能。为了实现这种性能，这些模型首先在通用语料库上进行预训练，然后在下游任务上进行微调。先前的研究研究了修剪下游任务的训练集对模型在其评估集上的性能的影响。在这项工作中，我们提出了一种用于微调任务训练集的自动数据集修剪方法。我们的方法基于模型正确分类每个训练数据点的成功率。与之前依赖用户反馈来确定子集大小的工作不同，我们的方法会自动提取适合每对模型和微调任务的训练子集。我们的方法提供了多个子集用于数据集修剪，以在子集大小和评估准确性之间进行权衡。我们最大的子集，也称为中奖彩票子集，平均比微调任务的原始训练集小 3 倍。我们对 5 个下游任务和 2 个语言模型进行的实验表明，平均而言，对中奖彩票子集进行微调可使模型的评估性能提高 $0.1 \%$。</li>
</ul>

<h3>Title: Characterizing Prompt Compression Methods for Long Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Kurt Keutzer, Amir Gholami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Characterizing Prompt Compression Methods for Long Context Inference(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>long context, prompt</a></li>
<li><strong>Abstract: </strong>Long context inference presents challenges at the system level with increased compute and memory requirements, as well as from an accuracy perspective in being able to reason over long contexts. Recently, several methods have been proposed to compress the prompt to reduce the context length. However, there has been little work on comparing the different proposed methods across different tasks through a standardized analysis. This has led to conflicting results. To address this, here we perform a comprehensive characterization and evaluation of different prompt compression methods. In particular, we analyze extractive compression, summarization-based abstractive compression, and token pruning methods. Surprisingly, we find that extractive compression often outperforms all the other approaches, and enables up to 10x compression with minimal accuracy degradation. Interestingly, we also find that despite several recent claims, token pruning methods often lag behind extractive compression. We only found marginal improvements on summarization tasks.</li>
<li><strong>摘要：</strong>长上下文推理在系统层面带来了挑战，因为它对计算和内存的要求更高，而且从准确性角度来看，它需要能够对长上下文进行推理。最近，已经提出了几种压缩提示以减少上下文长度的方法。然而，很少有人通过标准化分析来比较不同任务中提出的不同方法。这导致了相互矛盾的结果。为了解决这个问题，我们在这里对不同的提示压缩方法进行了全面的表征和评估。特别是，我们分析了提取压缩、基于摘要的抽象压缩和标记修剪方法。令人惊讶的是，我们发现提取压缩通常优于所有其他方法，并且能够实现高达 10 倍的压缩，而准确性下降最小。有趣的是，我们还发现，尽管最近有几种说法，但标记修剪方法往往落后于提取压缩。我们只发现摘要任务方面的微小改进。</li>
</ul>

<h3>Title: Self-Evolving GPT: A Lifelong Autonomous Experiential Learner</h3>
<ul>
<li><strong>Authors: </strong>Jinglong Gao, Xiao Ding, Yiming Cui, Jianbai Zhao, Hepeng Wang, Ting Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Self-Evolving GPT: A Lifelong Autonomous Experiential Learner(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions. To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them. Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities. Additionally, we provide a detailed analysis of the behavior of our framework at each step.</li>
<li><strong>摘要：</strong>为了提高大型语言模型 (LLM) 的性能，研究人员探索了通过提示为 LLM 提供文本任务解决经验。然而，他们依靠人工努力来获取和应用这些经验来完成每项任务，这对于日益增长的 LLM 需求和用户问题的多样性来说是不可行的。为了解决这个问题，我们设计了一个基于 LLM 的终身自主体验学习框架，以探索 LLM 是否可以模仿人类学习和利用经验的能力。它通过经验传递和归纳自主学习和积累经验，对输入问题的类型进行分类，以选择为它们使用哪些积累的经验。在六个广泛使用的 NLP 数据集上的实验结果表明，我们的框架在每个中间步骤中都表现可靠，并有效提高了 GPT-3.5 和 GPT-4 的性能。这验证了使用 LLM 模仿人类体验式学习和应用能力的可行性。此外，我们还对框架在每个步骤中的行为进行了详细分析。</li>
</ul>

<h3>Title: Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses. Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery. In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators. We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination. Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework. Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments. Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance. However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated. These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area.</li>
<li><strong>摘要：</strong>生物医学知识的快速增长已经超出了我们有效提取见解和产生新假设的能力。大型语言模型 (LLM) 已成为一种有前途的工具，可以彻底改变知识交互并可能加速生物医学发现。在本文中，我们对 LLM 作为生物医学假设生成器进行了全面评估。我们从生物医学文献中构建了一个背景假设对数据集，并根据出版日期将其仔细划分为训练、可见和不可见的测试集，以减轻数据污染。使用此数据集，我们评估了顶级指导模型在零样本、少样本和微调设置中的假设生成能力。为了加强对不确定性的探索，这是科学发现的一个关键方面，我们在评估框架中加入了工具使用和多智能体交互。此外，我们提出了四个基于广泛文献综述的新指标来评估生成的假设的质量，同时考虑了基于 LLM 的评估和人工评估。我们的实验得出了两个关键发现：1）LLM 可以生成新颖且经过验证的假设，即使在训练期间未见过的文献上进行测试也是如此；2）通过多智能体交互和工具使用增加不确定性可以促进多样化候选生成并提高零样本假设生成性能。然而，我们还观察到，通过少量学习和工具使用整合额外知识可能并不总能带来性能提升，这凸显了需要仔细考虑所纳入的外部知识的类型和范围。这些发现强调了 LLM 作为生物医学假设生成的强大辅助手段的潜力，并为指导该领域的进一步研究提供了宝贵的见解。</li>
</ul>

<h3>Title: Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Ye Liu, Jiajun Zhu, Kai Zhang, Haoyu Tang, Yanghai Zhang, Xukai Liu, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Augmented Fake News Detection (DAFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DAFND first identifies the keywords of each news article through a Detection Module. Subsequently, DAFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to derive its respective two prediction results. Finally, a Determination Module further integrates these two predictions and derives the final result. Extensive experiments on two publicly available datasets show the efficacy of our proposed method, particularly in low-resource settings.</li>
<li><strong>摘要：</strong>少量假新闻检测 (FS-FND) 旨在在资源极其匮乏的场景中区分虚假新闻与真实新闻。由于虚假新闻在社交媒体上的广泛传播和有害影响，这项任务引起了越来越多的关注。大型语言模型 (LLM) 凭借其丰富的先验知识和出色的上下文学习能力表现出色。然而，现有方法面临着理解模糊性和信息稀缺性等重大限制，这严重削弱了 LLM 的潜力。为了解决这些缺点，我们提出了一种双视角增强假新闻检测 (DAFND) 模型，旨在从内部和外部两个角度增强 LLM。具体来说，DAFND 首先通过检测模块识别每篇新闻文章的关键词。随后，DAFND 创造性地设计了一个调查模块来检索与当前新闻有关的内部和外部有价值的信息，然后是另一个判断模块来得出各自的两个预测结果。最后，确定模块进一步整合这两个预测并得出最终结果。在两个公开可用的数据集上进行的大量实验证明了我们提出的方法的有效性，特别是在资源匮乏的环境中。</li>
</ul>

<h3>Title: Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Ke Ji, Peng Wang, Wenjun Ke, Guozheng Li, Jiajun Liu, Jingsheng Gao, Ziyu Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Recently, various pre-trained language models (PLMs) have been proposed to prove their impressive performances on a wide range of few-shot tasks. However, limited by the unstructured prior knowledge in PLMs, it is difficult to maintain consistent performance on complex structured scenarios, such as hierarchical text classification (HTC), especially when the downstream data is extremely scarce. The main challenge is how to transfer the unstructured semantic space in PLMs to the downstream domain hierarchy. Unlike previous work on HTC which directly performs multi-label classification or uses graph neural network (GNN) to inject label hierarchy, in this work, we study the HTC problem under a few-shot setting to adapt knowledge in PLMs from an unstructured manner to the downstream hierarchy. Technically, we design a simple yet effective method named Hierarchical Iterative Conditional Random Field (HierICRF) to search the most domain-challenging directions and exquisitely crafts domain-hierarchy adaptation as a hierarchical iterative language modeling problem, and then it encourages the model to make hierarchical consistency self-correction during the inference, thereby achieving knowledge transfer with hierarchical consistency preservation. We perform HierICRF on various architectures, and extensive experiments on two popular HTC datasets demonstrate that prompt with HierICRF significantly boosts the few-shot HTC performance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29% to 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot settings, while remaining SOTA hierarchical consistency performance.</li>
<li><strong>摘要：</strong>最近，人们提出了各种预训练语言模型 (PLM)，以证明它们在各种少样本任务中表现出色。然而，由于 PLM 中非结构化的先验知识有限，很难在复杂的结构化场景（如分层文本分类 (HTC)）上保持一致的性能，尤其是在下游数据极其稀缺的情况下。主要挑战是如何将 PLM 中的非结构化语义空间转移到下游域层次结构。与之前关于 HTC 直接执行多标签分类或使用图神经网络 (GNN) 注入标签层次结构的工作不同，在本文中，我们研究了少样本设置下的 HTC 问题，以使 PLM 中的知识从非结构化方式适应下游层次结构。技术上，我们设计了一种简单而有效的方法，即分层迭代条件随机场 (HierICRF) 来搜索领域最具挑战性的方向，并精心设计了领域层次自适应作为分层迭代语言建模问题，然后它鼓励模型在推理过程中进行层次一致性自我校正，从而实现在保持层次一致性的情况下的知识迁移。我们在各种架构上执行了 HierICRF，并在两个流行的 HTC 数据集上进行了大量的实验，表明使用 HierICRF 提示可以显着提高少样本 HTC 性能，平均 Micro-F1 提高了 28.80% 至 1.50%，Macro-F1 提高了 36.29% 至 1.5%，高于之前在少样本设置下最先进的 (SOTA) 基线，同时保持 SOTA 层次一致性性能。</li>
</ul>

<h3>Title: Empowering Few-Shot Relation Extraction with The Integration of Traditional RE Methods and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ye Liu, Kai Zhang, Aoran Gan, Linan Yue, Feng Hu, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Empowering Few-Shot Relation Extraction with The Integration of Traditional RE Methods and Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE) that utilizes limited training instances, appeals to more researchers in Natural Language Processing (NLP) due to its capability to extract textual information in extremely low-resource scenarios. The primary methodologies employed for FSRE have been fine-tuning or prompt tuning techniques based on Pre-trained Language Models (PLMs). Recently, the emergence of Large Language Models (LLMs) has prompted numerous researchers to explore FSRE through In-Context Learning (ICL). However, there are substantial limitations associated with methods based on either traditional RE models or LLMs. Traditional RE models are hampered by a lack of necessary prior knowledge, while LLMs fall short in their task-specific capabilities for RE. To address these shortcomings, we propose a Dual-System Augmented Relation Extractor (DSARE), which synergistically combines traditional RE models with LLMs. Specifically, DSARE innovatively injects the prior knowledge of LLMs into traditional RE models, and conversely enhances LLMs' task-specific aptitude for RE through relation extraction augmentation. Moreover, an Integrated Prediction module is employed to jointly consider these two respective predictions and derive the final results. Extensive experiments demonstrate the efficacy of our proposed method.</li>
<li><strong>摘要：</strong>少样本关系提取 (FSRE) 是关系提取 (RE) 的一个子任务，它利用有限的训练实例，由于能够在资源极其匮乏的情况下提取文本信息，因此吸引了更多自然语言处理 (NLP) 研究人员的关注。FSRE 采用的主要方法是基于预训练语言模型 (PLM) 的微调或快速调整技术。最近，大型语言模型 (LLM) 的出现促使众多研究人员通过上下文学习 (ICL) 探索 FSRE。然而，基于传统 RE 模型或 LLM 的方法存在很大的局限性。传统的 RE 模型因缺乏必要的先验知识而受到阻碍，而 LLM 在 RE 的任务特定能力方面存在不足。为了解决这些缺点，我们提出了一种双系统增强关系提取器 (DSARE)，它将传统 RE 模型与 LLM 协同结合。具体来说，DSARE 创新地将 LLM 的先验知识注入到传统的 RE 模型中，并通过关系提取增强功能反过来增强了 LLM 对 RE 的任务特定能力。此外，还采用了集成预测模块来共同考虑这两个各自的预测并得出最终结果。大量实验证明了我们提出的方法的有效性。</li>
</ul>

<h3>Title: Towards Chapter-to-Chapter Context-Aware Literary Translation via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linghao Jin, Li An, Xuezhe Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Towards Chapter-to-Chapter Context-Aware Literary Translation via Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Discourse phenomena in existing document-level translation datasets are sparse, which has been a fundamental obstacle in the development of context-aware machine translation models. Moreover, most existing document-level corpora and context-aware machine translation methods rely on an unrealistic assumption on sentence-level alignments. To mitigate these issues, we first curate a novel dataset of Chinese-English literature, which consists of 160 books with intricate discourse structures. Then, we propose a more pragmatic and challenging setting for context-aware translation, termed chapter-to-chapter (Ch2Ch) translation, and investigate the performance of commonly-used machine translation models under this setting. Furthermore, we introduce a potential approach of finetuning large language models (LLMs) within the domain of Ch2Ch literary translation, yielding impressive improvements over baselines. Through our comprehensive analysis, we unveil that literary translation under the Ch2Ch setting is challenging in nature, with respect to both model learning methods and translation decoding algorithms.</li>
<li><strong>摘要：</strong>现有文档级翻译数据集中的话语现象比较稀疏，这一直是上下文感知机器翻译模型开发的一个根本障碍。此外，大多数现有的文档级语料库和上下文感知机器翻译方法都依赖于不切实际的句子级对齐假设。为了缓解这些问题，我们首先整理了一个新颖的中英文学数据集，其中包含 160 本具有复杂话语结构的书籍。然后，我们提出了一种更实用、更具挑战性的上下文感知翻译设置，称为章节到章节 (Ch2Ch) 翻译，并研究此设置下常用机器翻译模型的性能。此外，我们介绍了一种在 Ch2Ch 文学翻译领域内微调大型语言模型 (LLM) 的潜在方法，与基线相比取得了令人瞩目的改进。通过全面的分析，我们发现在 Ch2Ch 设置下的文学翻译本质上具有挑战性，无论是对于模型学习方法还是翻译解码算法而言。</li>
</ul>

<h3>Title: Robustness of LLMs to Perturbations in Text</h3>
<ul>
<li><strong>Authors: </strong>Ayush Singh, Navpreet Singh, Shubham Vatsal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Robustness of LLMs to Perturbations in Text(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Having a clean dataset has been the foundational assumption of most natural language processing (NLP) systems. However, properly written text is rarely found in real-world scenarios and hence, oftentimes invalidates the aforementioned foundational assumption. Recently, Large language models (LLMs) have shown impressive performance, but can they handle the inevitable noise in real-world data? This work tackles this critical question by investigating LLMs' resilience against morphological variations in text. To that end, we artificially introduce varying levels of noise into a diverse set of datasets and systematically evaluate LLMs' robustness against the corrupt variations of the original text. Our findings show that contrary to popular beliefs, generative LLMs are quiet robust to noisy perturbations in text. This is a departure from pre-trained models like BERT or RoBERTa whose performance has been shown to be sensitive to deteriorating noisy text. Additionally, we test LLMs' resilience on multiple real-world benchmarks that closely mimic commonly found errors in the wild. With minimal prompting, LLMs achieve a new state-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and Lexical Semantic Change (LSC). To empower future research, we also release a dataset annotated by humans stating their preference for LLM vs. human-corrected outputs along with the code to reproduce our results.</li>
<li><strong>摘要：</strong>拥有干净的数据集一直是大多数自然语言处理 (NLP) 系统的基本假设。然而，在现实世界中很少能找到正确书写的文本，因此，上述基本假设常常是错误的。最近，大型语言模型 (LLM) 表现出色，但它们能处理现实世界数据中不可避免的噪音吗？这项工作通过研究 LLM 对文本形态变化的适应性来解决这个关键问题。为此，我们人为地将不同程度的噪音引入一组不同的数据集，并系统地评估 LLM 对原始文本损坏变化的鲁棒性。我们的研究结果表明，与普遍看法相反，生成式 LLM 对文本中的噪声扰动非常稳健。这与 BERT 或 RoBERTa 等预训练模型不同，这些模型的性能已被证明对恶化的噪声文本很敏感。此外，我们在多个现实世界基准上测试了 LLM 的适应性，这些基准与自然界中常见的错误非常相似。只需极少的提示，LLM 就能在语法错误纠正 (GEC) 和词汇语义变化 (LSC) 基准任务上达到新的最高水平。为了支持未来的研究，我们还发布了一个由人类注释的数据集，其中说明了他们对 LLM 输出与人工纠正输出的偏好，以及用于重现我们结果的代码。</li>
</ul>

<h3>Title: Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Jiaming Zhou, Haoqin Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Self-Prompt Tuning: Enable Autonomous Role-Playing in LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in LLMs have showcased their remarkable role-playing capabilities, able to accurately simulate the dialogue styles and cognitive processes of various roles based on different instructions and contexts. Studies indicate that assigning LLMs the roles of experts, a strategy known as role-play prompting, can enhance their performance in the corresponding domains. However, the prompt needs to be manually designed for the given problem, requiring certain expertise and iterative modifications. To this end, we propose self-prompt tuning, making LLMs themselves generate role-play prompts through fine-tuning. Leveraging the LIMA dataset as our foundational corpus, we employ GPT-4 to annotate role-play prompts for each data points, resulting in the creation of the LIMA-Role dataset. We then fine-tune LLMs like Llama-2-7B and Mistral-7B on LIMA-Role. Consequently, the self-prompt tuned LLMs can automatically generate expert role prompts for any given question. We extensively evaluate self-prompt tuned LLMs on widely used NLP benchmarks and open-ended question test. Our empirical results illustrate that self-prompt tuned LLMs outperform standard instruction tuned baselines across most datasets. This highlights the great potential of utilizing fine-tuning to enable LLMs to self-prompt, thereby automating complex prompting strategies. We release the dataset, models, and code at this \href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}{url}.</li>
<li><strong>摘要：</strong>LLM 的最新进展展示了其卓越的角色扮演能力，能够根据不同的指令和上下文准确模拟各种角色的对话风格和认知过程。研究表明，为 LLM 分配专家角色（一种称为角色扮演提示的策略）可以提高其在相应领域的表现。但是，提示需要针对给定问题手动设计，需要一定的专业知识和迭代修改。为此，我们提出了自我提示调整，使 LLM 本身通过微调生成角色扮演提示。利用 LIMA 数据集作为我们的基础语料库，我们使用 GPT-4 为每个数据点注释角色扮演提示，从而创建了 LIMA-Role 数据集。然后，我们在 LIMA-Role 上对 Llama-2-7B 和 Mistral-7B 等 LLM 进行微调。因此，自我提示调整的 LLM 可以为任何给定问题自动生成专家角色提示。我们在广泛使用的 NLP 基准和开放式问题测试中广泛评估了自提示调整的 LLM。我们的实证结果表明，自提示调整的 LLM 在大多数数据集上的表现都优于标准指令调整的基线。这凸显了利用微调使 LLM 能够自我提示的巨大潜力，从而自动化复杂的提示策略。我们在此 \href{https://anonymous.4open.science/r/Self-Prompt-Tuning-739E/}url} 上发布了数据集、模型和代码。</li>
</ul>

<h3>Title: Benchmarking Language Model Creativity: A Case Study on Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Yining Lu, Dixuan Wang, Tianjian Li, Dongwei Jiang, Daniel Khashabi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Benchmarking Language Model Creativity: A Case Study on Code Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>As LLMs become increasingly prevalent, it is interesting to consider how ``creative'' these models can be. From cognitive science, creativity consists of at least two key characteristics: \emph{convergent} thinking (purposefulness to achieve a given goal) and \emph{divergent} thinking (adaptability to new environments or constraints) \citep{runco2003critical}. In this work, we introduce a framework for quantifying LLM creativity that incorporates the two characteristics. This is achieved by (1) Denial Prompting pushes LLMs to come up with more creative solutions to a given problem by incrementally imposing new constraints on the previous solution, compelling LLMs to adopt new strategies, and (2) defining and computing the NeoGauge metric which examines both convergent and divergent thinking in the generated creative responses by LLMs. We apply the proposed framework on Codeforces problems, a natural data source for collecting human coding solutions. We quantify NeoGauge for various proprietary and open-source models and find that even the most creative model, GPT-4, still falls short of demonstrating human-like creativity. We also experiment with advanced reasoning strategies (MCTS, self-correction, etc.) and observe no significant improvement in creativity. As a by-product of our analysis, we release NeoCoder dataset for reproducing our results on future models.</li>
<li><strong>摘要：</strong>随着 LLM 变得越来越普遍，思考这些模型的“创造性”程度是件有趣的事情。从认知科学的角度来看，创造力至少包含两个关键特征：\emph{收敛}思维（有目的地实现给定目标）和 \emph{发散}思维（对新环境或约束的适应性）\citep{runco​​2003critical}。在这项工作中，我们引入了一个量化 LLM 创造力的框架，该框架结合了这两个特征。这是通过 (1) 拒绝提示推动 LLM 提出更有创意的解决方案，通过逐步对先前的解决方案施加新的约束，迫使 LLM 采用新策略，以及 (2) 定义和计算 NeoGauge 指标，该指标检查 LLM 生成的创造性反应中的收敛和发散思维。我们将提出的框架应用于 Codeforces 问题，这是一个收集人类编码解决方案的自然数据源。我们对各种专有和开源模型的 NeoGauge 进行了量化，发现即使是最具创造力的模型 GPT-4 也未能展现出类似人类的创造力。我们还尝试了高级推理策略（MCTS、自我修正等），但未观察到创造力的显著提高。作为分析的副产品，我们发布了 NeoCoder 数据集，以便在未来的模型上重现我们的结果。</li>
</ul>

<h3>Title: One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Bo Wang, Tsunenori Mine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel and comprehensive solution to enhance both the robustness and efficiency of question answering (QA) systems through supervised contrastive learning (SCL). Training a high-performance QA system has become straightforward with pre-trained language models, requiring only a small amount of data and simple fine-tuning. However, despite recent advances, existing QA systems still exhibit significant deficiencies in functionality and training efficiency. We address the functionality issue by defining four key tasks: user input intent classification, out-of-domain input detection, new intent discovery, and continual learning. We then leverage a unified SCL-based representation learning method to efficiently build an intra-class compact and inter-class scattered feature space, facilitating both known intent classification and unknown intent detection and discovery. Consequently, with minimal additional tuning on downstream tasks, our approach significantly improves model efficiency and achieves new state-of-the-art performance across all tasks.</li>
<li><strong>摘要：</strong>本文提出了一种新颖而全面的解决方案，通过监督对比学习 (SCL) 来增强问答 (QA) 系统的稳健性和效率。使用预训练语言模型，训练高性能 QA 系统变得非常简单，只需要少量数据和简单的微调。然而，尽管最近取得了进展，但现有的 QA 系统在功能和训练效率方面仍然存在重大缺陷。我们通过定义四个关键任务来解决功能问题：用户输入意图分类、域外输入检测、新意图发现和持续学习。然后，我们利用统一的基于 SCL 的表示学习方法有效地构建类内紧凑和类间分散的特征空间，促进已知意图分类以及未知意图检测和发现。因此，只需对下游任务进行最少的额外调整，我们的方法就可以显著提高模型效率并在所有任务中实现新的最佳性能。</li>
</ul>

<h3>Title: CompAct: Compressing Retrieved Documents Actively for Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] CompAct: Compressing Retrieved Documents Actively for Question Answering(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation supports language models to strengthen their factual groundings by providing external contexts. However, language models often face challenges when given extensive information, diminishing their effectiveness in solving questions. Context compression tackles this issue by filtering out irrelevant information, but current methods still struggle in realistic scenarios where crucial information cannot be captured with a single-step approach. To overcome this limitation, we introduce CompAct, a novel framework that employs an active strategy to condense extensive documents without losing key information. Our experiments demonstrate that CompAct brings significant improvements in both performance and compression rate on multi-hop question-answering (QA) benchmarks. CompAct flexibly operates as a cost-efficient plug-in module with various off-the-shelf retrievers or readers, achieving exceptionally high compression rates (47x).</li>
<li><strong>摘要：</strong>检索增强生成通过提供外部上下文支持语言模型加强其事实基础。然而，语言模型在获得大量信息时往往会面临挑战，从而降低其解决问题的有效性。上下文压缩通过过滤掉不相关的信息来解决此问题，但当前方法在无法通过单步方法捕获关键信息的现实场景中仍然举步维艰。为了克服这一限制，我们引入了 CompAct，这是一个新颖的框架，它采用主动策略来压缩大量文档而不会丢失关键信息。我们的实验表明，CompAct 在多跳问答 (QA) 基准上显著提高了性能和压缩率。CompAct 可以灵活地作为经济高效的插件模块与各种现成的检索器或读取器配合使用，实现极高的压缩率 (47 倍)。</li>
</ul>

<h3>Title: New Desiderata for Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xiangkun Hu, Tong He, David Wipf</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] New Desiderata for Direct Preference Optimization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models in the past have typically relied on some form of reinforcement learning with human feedback (RLHF) to better align model responses with human preferences. However, because of oft-observed instabilities when implementing these RLHF pipelines, various reparameterization techniques have recently been introduced to sidestep the need for separately learning an RL reward model. Instead, directly fine-tuning for human preferences is achieved via the minimization of a single closed-form training objective, a process originally referred to as direct preference optimization (DPO) and followed by several notable descendants. Although effective in certain real-world settings, we introduce new evaluation criteria that serve to highlight unresolved shortcomings in the ability of existing DPO methods to interpolate between a pre-trained reference model and empirical measures of human preferences, as well as unavoidable trade-offs in how low- and high-quality responses are regularized and constraints are handled. Our insights then motivate an alternative DPO-like loss that provably mitigates these limitations. Empirical results serve to corroborate notable aspects of our analyses.</li>
<li><strong>摘要：</strong>过去，大型语言模型通常依赖某种形式的强化学习和人类反馈 (RLHF) 来更好地将模型响应与人类偏好对齐。然而，由于在实施这些 RLHF 管道时经常观察到不稳定性，最近引入了各种重新参数化技术来避免单独学习 RL 奖励模型的需要。相反，直接微调人类偏好是通过最小化单个闭式训练目标来实现的，这个过程最初被称为直接偏好优化 (DPO)，随后出现了几个著名的后代。尽管在某些现实环境中是有效的，但我们引入了新的评估标准，用于突出现有 DPO 方法在预训练参考模型和人类偏好的经验测量之间进行插值的能力方面尚未解决的缺点，以及在如何规范低质量和高质量响应以及处理约束方面不可避免的权衡。然后，我们的见解激发了一种替代的 DPO 类损失，可以证明可以减轻这些限制。实证结果证实了我们分析的显著方面。</li>
</ul>

<h3>Title: Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training</h3>
<ul>
<li><strong>Authors: </strong>Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models' ability to appropriately refuse generating unsafe content. We introduce a novel approach, Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising performance but also surpasses well-known models such as GPT-4 in defending against attacks. Importantly, our approach successfully defends recent advanced attack methods (e.g., CodeAttack) that have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be found at this https URL.</li>
<li><strong>摘要：</strong>本研究通过识别和解决安全调整数据中的拒绝位置偏差来解决大型语言模型 (LLM) 安全调整实践中的一个关键差距，这种偏差会损害模型适当拒绝生成不安全内容的能力。我们引入了一种新方法，即解耦拒绝训练 (DeRTa)，旨在使 LLM 能够在任何响应位置拒绝遵守有害提示，从而显著增强其安全能力。DeRTa 包含两个新组件：(1) 带有有害响应前缀的最大似然估计 (MLE)，通过在安全响应的开头附加一段有害响应来训练模型识别和避免不安全内容，以及 (2) 强化转换优化 (RTO)，使模型能够在整个有害响应序列中始终从潜在伤害过渡到安全拒绝。我们使用 LLaMA3 和 Mistral 模型系列在六种攻击场景中进行的实证评估表明，我们的方法不仅可以在不影响性能的情况下提高模型安全性，而且在防御攻击方面也超越了 GPT-4 等知名模型。重要的是，我们的方法成功防御了最近越狱 GPT-4 和 LLaMA3-70B-Instruct 的高级攻击方法（例如 CodeAttack）。我们的代码和数据可在此 https URL 中找到。</li>
</ul>

<h3>Title: Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors</h3>
<ul>
<li><strong>Authors: </strong>Nico Daheim, Jakub Macina, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) present an opportunity to scale high-quality personalized education to all. A promising approach towards this means is to build dialog tutoring models that scaffold students' problem-solving. However, even though existing LLMs perform well in solving reasoning questions, they struggle to precisely detect student's errors and tailor their feedback to these errors. Inspired by real-world teaching practice where teachers identify student errors and customize their response based on them, we focus on verifying student solutions and show how grounding to such verification improves the overall quality of tutor response generation. We collect a dataset of 1K stepwise math reasoning chains with the first error step annotated by teachers. We show empirically that finding the mistake in a student solution is challenging for current models. We propose and evaluate several verifiers for detecting these errors. Using both automatic and human evaluation we show that the student solution verifiers steer the generation model towards highly targeted responses to student errors which are more often correct with less hallucinations compared to existing baselines.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 为向所有人提供高质量的个性化教育提供了机会。实现这一目标的一种有前途的方法是构建对话辅导模型，以支持学生解决问题。然而，尽管现有的 LLM 在解决推理问题方面表现良好，但它们难以准确检测学生的错误并根据这些错误定制反馈。受现实世界教学实践的启发，教师识别学生的错误并根据这些错误定制他们的回答，我们专注于验证学生的解决方案，并展示如何以这种验证为基础提高导师回答生成的整体质量。我们收集了一个 1K 逐步数学推理链的数据集，其中第一个错误步骤由教师注释。我们通过经验表明，对于当前的模型来说，在学生解决方案中找到错误是一项挑战。我们提出并评估了几个用于检测这些错误的验证器。使用自动和人工评估，我们表明学生解决方案验证器引导生成模型朝着对学生错误的高度有针对性的响应方向发展，与现有基线相比，这些响应通常更正确，幻觉更少。</li>
</ul>

<h3>Title: Exploring the Effectiveness of Methods for Persona Extraction</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Zaitsev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Exploring the Effectiveness of Methods for Persona Extraction(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>The paper presents a study of methods for extracting information about dialogue participants and evaluating their performance in Russian. To train models for this task, the Multi-Session Chat dataset was translated into Russian using multiple translation models, resulting in improved data quality. A metric based on the F-score concept is presented to evaluate the effectiveness of the extraction models. The metric uses a trained classifier to identify the dialogue participant to whom the persona belongs. Experiments were conducted on MBart, FRED-T5, Starling-7B, which is based on the Mistral, and Encoder2Encoder models. The results demonstrated that all models exhibited an insufficient level of recall in the persona extraction task. The incorporation of the NCE Loss improved the model's precision at the expense of its recall. Furthermore, increasing the model's size led to enhanced extraction of personas.</li>
<li><strong>摘要：</strong>本文介绍了一种提取对话参与者信息并评估其俄语表现的方法。为了训练该任务的模型，使用多种翻译模型将多会话聊天数据集翻译成俄语，从而提高了数据质量。提出了一种基于 F 分数概念的指标来评估提取模型的有效性。该指标使用经过训练的分类器来识别角色所属的对话参与者。实验在 MBart、FRED-T5、基于 Mistral 的 Starling-7B 和 Encoder2Encoder 模型上进行。结果表明，所有模型在角色提取任务中都表现出召回率不足。加入 NCE 损失提高了模型的精确度，但牺牲了召回率。此外，增加模型的大小可以增强角色的提取。</li>
</ul>

<h3>Title: Does Incomplete Syntax Influence Korean Language Model? Focusing on Word Order and Case Markers</h3>
<ul>
<li><strong>Authors: </strong>Jong Myoung Kim, Young-Jun Lee, Yong-jin Han, Sangkeun Jung, Ho-Jin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Does Incomplete Syntax Influence Korean Language Model? Focusing on Word Order and Case Markers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Syntactic elements, such as word order and case markers, are fundamental in natural language processing. Recent studies show that syntactic information boosts language model performance and offers clues for people to understand their learning mechanisms. Unlike languages with a fixed word order such as English, Korean allows for varied word sequences, despite its canonical structure, due to case markers that indicate the functions of sentence components. This study explores whether Korean language models can accurately capture this flexibility. We note that incomplete word orders and omitted case markers frequently appear in ordinary Korean communication. To investigate this further, we introduce the Syntactically Incomplete Korean (SIKO) dataset. Through SIKO, we assessed Korean language models' flexibility with incomplete syntax and confirmed the dataset's training value. Results indicate these models reflect Korean's inherent flexibility, accurately handling incomplete inputs. Moreover, fine-tuning with SIKO enhances the ability to handle common incomplete Korean syntactic forms. The dataset's simple construction process, coupled with significant performance enhancements, solidifies its standing as an effective data augmentation technique.</li>
<li><strong>摘要：</strong>句法元素（例如词序和格标记）是自然语言处理的基础。最近的研究表明，句法信息可以提高语言模型的性能，并为人们了解其学习机制提供线索。与英语等具有固定词序的语言不同，尽管韩语具有规范结构，但由于格标记指示了句子成分的功能，因此韩语允许使用不同的词序。本研究探讨了韩语模型是否能够准确捕捉这种灵活性。我们注意到，在普通的韩语交流中经常出现不完整的词序和省略的格标记。为了进一步研究这一点，我们引入了句法不完整韩语 (SIKO) 数据集。通过 SIKO，我们评估了韩语模型对不完整句法的灵活性，并确认了数据集的训练价值。结果表明，这些模型反映了韩语固有的灵活性，可以准确处理不完整的输入。此外，使用 SIKO 进行微调可以增强处理常见不完整韩语句法形式的能力。该数据集的简单构建过程加上显着的性能增强，巩固了其作为有效数据增强技术的地位。</li>
</ul>

<h3>Title: Pronunciation Assessment with Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaiqi Fu, Linkai Peng, Nan Yang, Shuran Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Pronunciation Assessment with Multi-modal Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), renowned for their powerful conversational abilities, are widely recognized as exceptional tools in the field of education, particularly in the context of automated intelligent instruction systems for language learning. In this paper, we propose a scoring system based on LLMs, motivated by their positive impact on text-related scoring tasks. Specifically, the speech encoder first maps the learner's speech into contextual features. The adapter layer then transforms these features to align with the text embedding in latent space. The assessment task-specific prefix and prompt text are embedded and concatenated with the features generated by the modality adapter layer, enabling the LLMs to predict accuracy and fluency scores. Our experiments demonstrate that the proposed scoring systems achieve competitive results compared to the baselines on the Speechocean762 datasets. Moreover, we also conducted an ablation study to better understand the contributions of the prompt text and training strategy in the proposed scoring system.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 以其强大的对话能力而闻名，被广泛认为是教育领域的卓越工具，特别是在语言学习的自动智能教学系统方面。在本文中，我们提出了一种基于 LLM 的评分系统，其动机是它们对与文本相关的评分任务产生积极影响。具体来说，语音编码器首先将学习者的语音映射到上下文特征中。然后，适配器层将这些特征转换为与潜在空间中的文本嵌入对齐。评估任务特定的前缀和提示文本被嵌入并与模态适配器层生成的特征连接在一起，使 LLM 能够预测准确性和流利度分数。我们的实验表明，与 Speechocean762 数据集上的基线相比，所提出的评分系统取得了有竞争力的结果。此外，我们还进行了一项消融研究，以更好地了解提示文本和训练策略在所提出的评分系统中的贡献。</li>
</ul>

<h3>Title: The Sociolinguistic Foundations of Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jack Grieve, Sara Bartl, Matteo Fuoli, Jason Grafmiller, Weihang Huang, Alejandro Jawerbaum, Akira Murakami, Marcus Perlman, Dana Roemling, Bodo Winter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] The Sociolinguistic Foundations of Language Modeling(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a sociolinguistic perspective on language modeling. We claim that large language models are inherently models of varieties of language, and we consider how this insight can inform the development and deployment of large language models. We begin by presenting a technical definition of the concept of a variety of language as developed in sociolinguistics. We then discuss how this perspective can help address five basic challenges in language modeling: social bias, domain adaptation, alignment, language change, and scale. Ultimately, we argue that it is crucial to carefully define and compile training corpora that accurately represent the specific varieties of language being modeled to maximize the performance and societal value of large language models.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了语言建模的社会语言学视角。我们认为大型语言模型本质上是语言变体的模型，并且我们考虑了这种见解如何为大型语言模型的开发和部署提供信息。我们首先介绍社会语言学中发展起来的语言变体概念的技术定义。然后，我们讨论这种观点如何帮助解决语言建模中的五个基本挑战：社会偏见、领域适应、一致性、语言变化和规模。最后，我们认为，认真定义和编译准确表示所建模的特定语言变体的训练语料库对于最大限度地提高大型语言模型的性能和社会价值至关重要。</li>
</ul>

<h3>Title: Context Embeddings for Efficient Answer Generation in RAG</h3>
<ul>
<li><strong>Authors: </strong>David Rau, Shuai Wang, Hervé Déjean, Stéphane Clinchant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Context Embeddings for Efficient Answer Generation in RAG(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, long context, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge of LLMs by extending the input with external information. As a consequence, the contextual inputs to the model become much longer which slows down decoding time directly translating to the time a user has to wait for an answer. We address this challenge by presenting COCOM, an effective context compression method, reducing long contexts to only a handful of Context Embeddings speeding up the generation time by a large margin. Our method allows for different compression rates trading off decoding time for answer quality. Compared to earlier methods, COCOM allows for handling multiple contexts more effectively, significantly reducing decoding time for long inputs. Our method demonstrates a speed-up of up to 5.69 $\times$ while achieving higher performance compared to existing efficient context compression methods.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过使用外部信息扩展输入，可以克服 LLM 知识有限的问题。因此，模型的上下文输入变得更长，这会减慢解码时间，直接转化为用户等待答案的时间。我们通过提出一种有效的上下文压缩方法 COCOM 来解决这一挑战，将长上下文减少到只有少数上下文嵌入，从而大大加快了生成时间。我们的方法允许不同的压缩率，以解码时间和答案质量为代价。与早期的方法相比，COCOM 可以更有效地处理多个上下文，显著减少长输入的解码时间。与现有的高效上下文压缩方法相比，我们的方法实现了高达 5.69 $\times$ 的速度提升，同时实现了更高的性能。</li>
</ul>

<h3>Title: H2O-Danube3 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Pascal Pfeiffer, Philipp Singer, Yauhen Babakhin, Gabor Fodor, Nischay Dhankhar, Sri Satish Ambati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] H2O-Danube3 Technical Report(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>We present H2O-Danube3, a series of small language models consisting of H2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T tokens. Our models are pre-trained on high quality Web data consisting of primarily English tokens in three stages with different data mixes before final supervised tuning for chat version. The models exhibit highly competitive metrics across a multitude of academic, chat, and fine-tuning benchmarks. Thanks to its compact architecture, H2O-Danube3 can be efficiently run on a modern smartphone, enabling local inference and rapid processing capabilities even on mobile devices. We make all models openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.</li>
<li><strong>摘要：</strong>我们推出了 H2O-Danube3，这是一系列小型语言模型，包括 H2O-Danube3-4B（在 6T 标记上训练）和 H2O-Danube3-500M（在 4T 标记上训练）。我们的模型在高质量 Web 数据上进行了预训练，这些数据主要由英语标记组成，分为三个阶段，使用不同的数据组合，然后最终对聊天版本进行监督调整。这些模型在众多学术、聊天和微调基准测试中表现出极具竞争力的指标。由于其紧凑的架构，H2O-Danube3 可以在现代智能手机上高效运行，即使在移动设备上也能实现本地推理和快速处理能力。我们根据 Apache 2.0 许可证公开提供所有模型，从而进一步以经济的方式将 LLM 普及到更广泛的受众。</li>
</ul>

<h3>Title: DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection</h3>
<ul>
<li><strong>Authors: </strong>Sangpil Youm, Brodie Mather, Chathuri Jayaweera, Juliana Prada, Bonnie Dorr</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Semantic role labeling (SRL) enriches many downstream applications, e.g., machine translation, question answering, summarization, and stance/belief detection. However, building multilingual SRL models is challenging due to the scarcity of semantically annotated corpora for multiple languages. Moreover, state-of-the-art SRL projection (XSRL) based on large language models (LLMs) yields output that is riddled with spurious role labels. Remediation of such hallucinations is not straightforward due to the lack of explainability of LLMs. We show that hallucinated role labels are related to naturally occurring divergence types that interfere with initial alignments. We implement Divergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging linguistically-informed alignment remediation followed by greedy First-Come First-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL projection without additional transformer-based machinery, beating XSRL in both human and automatic comparisons, and advancing beyond headwords to accommodate phrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our ground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3% (EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1% (EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our approach to other language pairs (e.g., English-Tagalog).</li>
<li><strong>摘要：</strong>语义角色标记 (SRL) 丰富了许多下游应用，例如机器翻译、问答、摘要和立场/信念检测。然而，由于多种语言的语义注释语料库稀缺，构建多语言 SRL 模型具有挑战性。此外，基于大型语言模型 (LLM) 的最先进的 SRL 投影 (XSRL) 产生的输出充斥着虚假的角色标签。由于 LLM 缺乏可解释性，因此纠正此类幻觉并不简单。我们表明幻觉角色标签与干扰初始对齐的自然发生的发散类型有关。我们实现了发散感知幻觉纠正的 SRL 投影 (DAHRS)，利用语言知情的对齐纠正，然后进行贪婪的先来先分配 (FCFA) SRL 投影。 DAHRS 无需额外的基于转换器的机制即可提高 SRL 投影的准确性，在人工和自动比较中均优于 XSRL，并且超越了词头，可以适应短语级 SRL 投影（例如 EN-FR、EN-ES）。使用 CoNLL-2009 作为基本事实，我们在 XSRL 上实现了更高的词级 F1：87.6% vs. 77.3%（EN-FR）和 89.0% vs. 82.7%（EN-ES）。人工短语级评估结果为 89.1%（EN-FR）和 91.0%（EN-ES）。我们还定义了一个发散度量，以使我们的方法适用于其他语言对（例如英语-他加禄语）。</li>
</ul>

<h3>Title: Transformer Layers as Painters</h3>
<ul>
<li><strong>Authors: </strong>Qi Sun, Marc Pickett, Aakash Kumar Nain, Llion Jones</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Transformer Layers as Painters(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite their nearly universal adoption for large language models, the internal workings of transformers are not well understood. We aim to better understand the impact of removing or reorganizing information throughout the layers of a pretrained transformer. Such an understanding could both yield better usage of existing models as well as to make architectural improvements to produce new variants. We present a series of empirical studies on frozen models that show that the lower and final layers of pretrained transformers differ from middle layers, but that middle layers have a surprising amount of uniformity. We further show that some classes of problems have robustness to skipping layers, running the layers in an order different from how they were trained, or running the layers in parallel. Our observations suggest that even frozen pretrained models may gracefully trade accuracy for latency by skipping layers or running layers in parallel.</li>
<li><strong>摘要：</strong>尽管 Transformer 几乎被广泛用于大型语言模型，但人们对其内部工作原理的了解还不够。我们的目标是更好地了解在预训练 Transformer 的各个层中删除或重新组织信息的影响。这样的理解既可以更好地利用现有模型，也可以改进架构以产生新的变体。我们对冻结模型进行了一系列实证研究，结果表明，预训练 Transformer 的底层和最后一层与中间层不同，但中间层具有令人惊讶的一致性。我们进一步表明，某些类别的问题对于跳过层、以不同于训练顺序的顺序运行层或并行运行层具有鲁棒性。我们的观察表明，即使是冻结的预训练模型也可以通过跳过层或并行运行层来优雅地以准确性换取延迟。</li>
</ul>

<h3>Title: Scalability of Bayesian Network Structure Elicitation with Large Language Models: a Novel Methodology and Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Nikolay Babakov, Ehud Reiter, Alberto Bugarin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Scalability of Bayesian Network Structure Elicitation with Large Language Models: a Novel Methodology and Comparative Analysis(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this work, we propose a novel method for Bayesian Networks (BNs) structure elicitation that is based on the initialization of several LLMs with different experiences, independently querying them to create a structure of the BN, and further obtaining the final structure by majority voting. We compare the method with one alternative method on various widely and not widely known BNs of different sizes and study the scalability of both methods on them. We also propose an approach to check the contamination of BNs in LLM, which shows that some widely known BNs are inapplicable for testing the LLM usage for BNs structure elicitation. We also show that some BNs may be inapplicable for such experiments because their node names are indistinguishable. The experiments on the other BNs show that our method performs better than the existing method with one of the three studied LLMs; however, the performance of both methods significantly decreases with the increase in BN size.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种新的贝叶斯网络 (BN) 结构引出方法，该方法基于初始化几个具有不同经验的 LLM，独立查询它们以创建 BN 的结构，并通过多数投票进一步获得最终结构。我们将该方法与一种替代方法在不同大小的各种广为人知和不广为人知的 BN 上进行了比较，并研究了这两种方法在它们上的可扩展性。我们还提出了一种检查 LLM 中 BN 污染的方法，该方法表明一些广为人知的 BN 不适用于测试 LLM 用于 BN 结构引出的用途。我们还表明，某些 BN 可能不适用于此类实验，因为它们的节点名称难以区分。在其他 BN 上的实验表明，我们的方法比使用所研究的三个 LLM 之一的现有方法表现更好；然而，两种方法的性能都会随着 BN 大小的增加而显著下降。</li>
</ul>

<h3>Title: Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text</h3>
<ul>
<li><strong>Authors: </strong>Lucio La Cava, Davide Costa, Andrea Tagarelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The significant progress in the development of Large Language Models has contributed to blurring the distinction between human and AI-generated text. The increasing pervasiveness of AI-generated text and the difficulty in detecting it poses new challenges for our society. In this paper, we tackle the problem of detecting and attributing AI-generated text by proposing WhosAI, a triplet-network contrastive learning framework designed to predict whether a given input text has been generated by humans or AI and to unveil the authorship of the text. Unlike most existing approaches, our proposed framework is conceived to learn semantic similarity representations from multiple generators at once, thus equally handling both detection and attribution tasks. Furthermore, WhosAI is model-agnostic and scalable to the release of new AI text-generation models by incorporating their generated instances into the embedding space learned by our framework. Experimental results on the TuringBench benchmark of 200K news articles show that our proposed framework achieves outstanding results in both the Turing Test and Authorship Attribution tasks, outperforming all the methods listed in the TuringBench benchmark leaderboards.</li>
<li><strong>摘要：</strong>大型语言模型开发中的重大进展有助于模糊人类和人工智能生成的文本之间的区别。人工智能生成的文本日益普及，检测难度也越来越大，这给我们的社会带来了新的挑战。在本文中，我们通过提出 WhosAI 来解决检测和归因人工智能生成的文本的问题，WhosAI 是一个三重网络对比学习框架，旨在预测给定的输入文本是由人类还是人工智能生成的，并揭示文本的作者。与大多数现有方法不同，我们提出的框架旨在同时从多个生成器中学习语义相似性表示，从而平等地处理检测和归因任务。此外，WhosAI 与模型无关，并且可扩展到新人工智能文本生成模型的发布，方法是将它们生成的实例合并到我们框架学习的嵌入空间中。在 200K 新闻文章的 TuringBench 基准测试上的实验结果表明，我们提出的框架在图灵测试和作者归属任务中都取得了优异的成绩，超越了 TuringBench 基准测试排行榜上列出的所有方法。</li>
</ul>

<h3>Title: SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</h3>
<ul>
<li><strong>Authors: </strong>Shraman Pramanick, Rama Chellappa, Subhashini Venugopalan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and focus solely on textual content. To address this limitation, we introduce SPIQA (Scientific Paper Image Question Answering), the first large-scale QA dataset specifically designed to interpret complex figures and tables within the context of scientific research articles across various domains of computer science. Leveraging the breadth of expertise and ability of multimodal large language models (MLLMs) to understand figures, we employ automatic and manual curation to create the dataset. We craft an information-seeking task involving multiple images that cover a wide variety of plots, charts, tables, schematic diagrams, and result visualizations. SPIQA comprises 270K questions divided into training, validation, and three different evaluation splits. Through extensive experiments with 12 prominent foundational models, we evaluate the ability of current multimodal systems to comprehend the nuanced aspects of research articles. Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows fine-grained, step-by-step assessment and improves model performance. We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research and the dataset's impact on revolutionizing how we interact with scientific literature.</li>
<li><strong>摘要：</strong>在长篇科学研究文章中寻找问题的答案是一个重要的研究领域，它可以帮助读者快速解决他们的疑问。然而，现有的基于科学论文的问答 (QA) 数据集规模有限，并且仅关注文本内容。为了解决这一限制，我们推出了 SPIQA（科学论文图像问答），这是第一个专门设计用于解释计算机科学各个领域科学研究文章中复杂图形和表格的大型 QA 数据集。利用多模态大型语言模型 (MLLM) 的专业知识和理解图形的能力，我们采用自动和手动管理来创建数据集。我们设计了一个信息搜索任务，涉及多个图像，涵盖各种图表、表格、示意图和结果可视化。SPIQA 包含 270K 个问题，分为训练、验证和三个不同的评估部分。通过对 12 个著名的基础模型进行大量实验，我们评估了当前多模态系统理解研究文章细微差别的能力。此外，我们提出了一种具有上下文检索的思想链 (CoT) 评估策略，可以进行细粒度、逐步评估并提高模型性能。我们进一步探索了使用附加文本信息提高性能的上限，强调了其对未来研究的巨大潜力以及数据集对我们与科学文献互动方式的革命性影响。</li>
</ul>

<h3>Title: Mitigating Entity-Level Hallucination in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Mitigating Entity-Level Hallucination in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and-answer interactions with LLMs. However, the widespread adoption of LLMs has revealed a significant challenge known as hallucination, wherein LLMs generate coherent yet factually inaccurate responses. This hallucination phenomenon has led to users' distrust in information retrieval systems based on LLMs. To tackle this challenge, this paper proposes Dynamic Retrieval Augmentation based on hallucination Detection (DRAD) as a novel method to detect and mitigate hallucinations in LLMs. DRAD improves upon traditional retrieval augmentation by dynamically adapting the retrieval process based on real-time hallucination detection. It features two main components: Real-time Hallucination Detection (RHD) for identifying potential hallucinations without external models, and Self-correction based on External Knowledge (SEK) for correcting these errors using external knowledge. Experiment results show that DRAD demonstrates superior performance in both detecting and mitigating hallucinations in LLMs. All of our code and data are open-sourced at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现彻底改变了用户访问信息的方式，从传统的搜索引擎转向使用 LLM 进行直接问答互动。然而，LLM 的广泛采用也暴露出一个重大挑战，即幻觉，即 LLM 会生成连贯但事实不准确的响应。这种幻觉现象导致用户不信任基于 LLM 的信息检索系统。为了应对这一挑战，本文提出了基于幻觉检测 (DRAD) 的动态检索增强作为一种检测和缓解 LLM 中幻觉的新方法。DRAD 通过基于实时幻觉检测动态调整检索过程来改进传统的检索增强。它有两个主要组成部分：实时幻觉检测 (RHD) 用于在没有外部模型的情况下识别潜在幻觉，以及基于外部知识的自我校正 (SEK) 用于使用外部知识纠正这些错误。实验结果表明，DRAD 在检测和缓解 LLM 中的幻觉方面表现出色。我们的所有代码和数据均在此 https URL 上开源。</li>
</ul>

<h3>Title: Open (Clinical) LLMs are Sensitive to Instruction Phrasings</h3>
<ul>
<li><strong>Authors: </strong>Alberto Mario Ceballos Arroyo, Monica Munnangi, Jiuding Sun, Karen Y.C. Zhang, Denis Jered McInerney, Byron C. Wallace, Silvio Amir</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Open (Clinical) LLMs are Sensitive to Instruction Phrasings(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Instruction-tuned Large Language Models (LLMs) can perform a wide range of tasks given natural language instructions to do so, but they are sensitive to how such instructions are phrased. This issue is especially concerning in healthcare, as clinicians are unlikely to be experienced prompt engineers and the potential consequences of inaccurate outputs are heightened in this domain. This raises a practical question: How robust are instruction-tuned LLMs to natural variations in the instructions provided for clinical NLP tasks? We collect prompts from medical doctors across a range of tasks and quantify the sensitivity of seven LLMs -- some general, others specialized -- to natural (i.e., non-adversarial) instruction phrasings. We find that performance varies substantially across all models, and that -- perhaps surprisingly -- domain-specific models explicitly trained on clinical data are especially brittle, compared to their general domain counterparts. Further, arbitrary phrasing differences can affect fairness, e.g., valid but distinct instructions for mortality prediction yield a range both in overall performance, and in terms of differences between demographic groups.</li>
<li><strong>摘要：</strong>指令调整的大型语言模型 (LLM) 可以根据自然语言指令执行各种任务，但它们对这些指令的措辞方式很敏感。这个问题在医疗保健领域尤其令人担忧，因为临床医生不太可能是经验丰富的快速工程师，而在这个领域，不准确的输出可能带来的后果会更加严重。这提出了一个实际问题：指令调整的 LLM 对临床 NLP 任务提供的指令的自然变化有多强健？我们收集了来自医生的一系列任务的提示，并量化了七个 LLM（一些是通用的，另一些是专门的）对自然（即非对抗性）指令措辞的敏感度。我们发现所有模型的性能差异很大，而且——也许令人惊讶的是——与通用领域的模型相比，明确在临床数据上训练的领域特定模型特别脆弱。此外，任意的措辞差异会影响公平性，例如，死亡率预测的有效但不同的指令在整体性能和人口群体之间的差异方面都产生了一个范围。</li>
</ul>

<h3>Title: ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts</h3>
<ul>
<li><strong>Authors: </strong>Amelia F. Hardy, Houjun Liu, Bernard Lange, Mykel J. Kochenderfer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text that is unintelligible and unlikely to arise. Here, we propose a reinforcement learning formulation of the LLM red-teaming task which allows us to discover prompts that both (1) trigger toxic outputs from a frozen defender and (2) have low perplexity as scored by the defender. We argue these cases are most pertinent in a red-teaming setting because of their likelihood to arise during normal use of the defender model. We solve this formulation through a novel online and weakly supervised variant of Identity Preference Optimization (IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy is capable of generating likely prompts that also trigger toxicity. Finally, we qualitatively analyze learned strategies, trade-offs of likelihood and toxicity, and discuss implications. Source code is available for this project at: this https URL.</li>
<li><strong>摘要：</strong>自动红队大型语言模型 (LLM) 的典型方案侧重于发现触发冻结语言模型（防御者）生成有害文本的提示。这通常会导致提示模型（对手）生成难以理解且不太可能出现的文本。在这里，我们提出了一种 LLM 红队任务的强化学习公式，它使我们能够发现 (1) 触发冻结防御者的有毒输出和 (2) 防御者评分低的困惑度的提示。我们认为这些情况在红队环境中最为相关，因为它们很可能在防御者模型的正常使用过程中出现。我们通过 GPT-2 和 GPT-2 XL 防御者的身份偏好优化 (IPO) 的新型在线和弱监督变体来解决这个公式。我们证明我们的策略能够生成可能触发毒性的提示。最后，我们定性分析学习策略、可能性和毒性的权衡，并讨论影响。该项目的源代码可在以下网址获取：此 https URL。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
