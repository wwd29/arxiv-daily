<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-28</h1>
<h3>Title: Enriching Word Usage Graphs with Cluster Definitions</h3>
<ul>
<li><strong>Authors: </strong>Mariia Fedorova, Andrey Kutuzov, Nikolay Arefyev, Dominik Schlechtweg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18024">https://arxiv.org/abs/2403.18024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18024">https://arxiv.org/pdf/2403.18024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18024]] Enriching Word Usage Graphs with Cluster Definitions(https://arxiv.org/abs/2403.18024)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present a dataset of word usage graphs (WUGs), where the existing WUGs for multiple languages are enriched with cluster labels functioning as sense definitions. They are generated from scratch by fine-tuned encoder-decoder language models. The conducted human evaluation has shown that these definitions match the existing clusters in WUGs better than the definitions chosen from WordNet by two baseline systems. At the same time, the method is straightforward to use and easy to extend to new languages. The resulting enriched datasets can be extremely helpful for moving on to explainable semantic change modeling.</li>
<li><strong>摘要：</strong>我们提出了一个单词使用图（WUG）数据集，其中多种语言的现有 WUG 通过充当语义定义的集群标签进行了丰富。它们是由微调的编码器-解码器语言模型从头开始生成的。进行的人工评估表明，这些定义比两个基线系统从 WordNet 选择的定义更好地匹配 WUG 中的现有集群。同时，该方法使用简单，易于扩展到新语言。由此产生的丰富数据集对于继续进行可解释的语义变化建模非常有帮助。</li>
</ul>

<h3>Title: Improving Pre-trained Language Model Sensitivity via Mask Specific  losses: A case study on Biomedical NER</h3>
<ul>
<li><strong>Authors: </strong>Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E Buchan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18025">https://arxiv.org/abs/2403.18025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18025">https://arxiv.org/pdf/2403.18025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18025]] Improving Pre-trained Language Model Sensitivity via Mask Specific  losses: A case study on Biomedical NER(https://arxiv.org/abs/2403.18025)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking.</li>
<li><strong>摘要：</strong>使语言模型 (LM) 适应新领域通常是通过在特定领域数据上微调预训练的 LM (PLM) 来实现的。微调将新知识引入 LM，使其能够理解并有效地执行目标领域任务。然而，如果忽略源域和目标域之间的广泛差异（例如词义），微调可能会无意中变得不敏感。例如，诸如“慢性”和“压力”之类的词在社交对话中可能会被轻视，但在临床上，这些词通常是一种担忧的表达。为了解决不敏感的微调问题，我们提出了掩模特定语言建模（MSLM），这是一种通过在微调过程中适当权衡特定领域术语（DS术语）的重要性来有效获取目标领域知识的方法。 MSLM 联合屏蔽 DS 术语和通用单词，然后通过确保 LM 因预测 DS 术语（与通用单词相比）不准确而受到更大的惩罚来学习特定于掩模的损失。我们的分析结果表明，MSLM 提高了 LM 的灵敏度和 DS 项的检测。我们凭经验表明，最佳掩蔽率不仅取决于 LM，还取决于数据集和序列的长度。我们提出的掩蔽策略优于先进的掩蔽策略，例如基于跨度和 PMI 的掩蔽。</li>
</ul>

<h3>Title: Supervisory Prompt Training</h3>
<ul>
<li><strong>Authors: </strong>Jean Ghislain Billa, Min Oh, Liang Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18051">https://arxiv.org/abs/2403.18051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18051">https://arxiv.org/pdf/2403.18051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18051]] Supervisory Prompt Training(https://arxiv.org/abs/2403.18051)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT advances LLMs by refining prompts to enhance performance and reduce hallucinations, offering an efficient and scalable alternative to traditional model fine-tuning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的性能在很大程度上依赖于提示的质量，而提示的质量通常是手动设计的且特定于任务，这使得它们成本高昂且不可扩展。我们提出了一种新颖的方法，即监督即时培训（SPT）。 SPT 使用双 LLM 系统自动生成高效的提示。在这个系统中，一个法学硕士（生成器）执行一项任务，而另一个法学硕士（校正器）提供反馈并生成改进的提示。与早期的技术相比，生成器和校正器随着时间的推移协作并不断改进其提示。我们还引入了 \textit{影响分数} 的概念来衡量提示的句子级有效性。我们的方法在四个基准上进行了测试，测试了法学硕士的幻觉水平。值得注意的是，我们能够将 GSM8K 上的 GPT-4 准确率从 65.8% 提高到 94.1%（提高了 28.3%）。 SPT 通过完善提示来提高表现和减少幻觉，从而为传统模型微调提供高效且可扩展的替代方案，从而推动法学硕士的发展。</li>
</ul>

<h3>Title: COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, Ge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18058">https://arxiv.org/abs/2403.18058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18058">https://arxiv.org/pdf/2403.18058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18058]] COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning(https://arxiv.org/abs/2403.18058)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&A communities, Wikis, examinations, and existing NLP datasets. This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset. Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses. The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets. We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks. Data are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）取得了重大进展，特别是在英语方面。这些进步使这些法学硕士能够以前所未有的准确性和流畅性理解和执行复杂的指令。然而，尽管取得了这些进步，中文指令调优的发展仍然存在明显差距。汉语独特的语言特征和文化深度给教学调整任务带来了挑战。现有的数据集要么源自以英语为中心的法学硕士，要么不适合与现实世界的中国用户的交互模式保持一致。为了弥补这一差距，我们引入了 COIG-CQIA，一个高质量的中文指令调优数据集。我们的目标是建立一个多样化、广泛的指令调整数据集，以更好地使模型行为与人类交互保持一致。为此，我们从中国互联网上的各种来源收集了高质量的人类编写的语料库，包括问答社区、维基百科、考试和现有的 NLP 数据集。该语料库经过严格过滤和仔细处理，形成 COIG-CQIA 数据集。此外，我们在深入评估和分析后，在 CQIA 的不同子集上训练各种规模的模型。我们的实验结果为选择和开发中文指令调整数据集提供了宝贵的见解。我们还发现，在 CQIA-Subset 上训练的模型在人类评估以及知识和安全基准方面取得了有竞争力的结果。数据可在 https://huggingface.co/datasets/m-a-p/COIG-CQIA 获取</li>
</ul>

<h3>Title: Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hai-Long Nguyen, Duc-Minh Nguyen, Tan-Minh Nguyen, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong, Ken Satoh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18093">https://arxiv.org/abs/2403.18093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18093">https://arxiv.org/pdf/2403.18093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18093]] Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large  Language Models(https://arxiv.org/abs/2403.18093)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.</li>
<li><strong>摘要：</strong>具有数十亿参数的大型语言模型，例如 GPT-3.5、GPT-4 和 LLaMA，越来越流行。许多研究探索了有效的激励技术，以利用这些法学硕士的力量来解决各种研究问题。由于法律文章数量庞大、篇幅较长，检索，特别是在法律数据领域，对直接应用提示技术提出了一项具有挑战性的任务。这项研究的重点是通过将提示作为检索系统的最后阶段来最大化提示的潜力，之前有两个阶段的支持：BM25 预排序和基于 BERT 的重新排序。在 COLIEE 2023 数据集上的实验表明，将法学硕士的提示技术集成到检索系统中可以显着提高检索准确性。然而，错误分析揭示了检索系统中存在的几个仍需要解决的问题。</li>
</ul>

<h3>Title: GPTs and Language Barrier: A Cross-Lingual Legal QA Examination</h3>
<ul>
<li><strong>Authors: </strong>Ha-Thanh Nguyen, Hiroaki Yamada, Ken Satoh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18098">https://arxiv.org/abs/2403.18098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18098">https://arxiv.org/pdf/2403.18098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18098]] GPTs and Language Barrier: A Cross-Lingual Legal QA Examination(https://arxiv.org/abs/2403.18098)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.</li>
<li><strong>摘要：</strong>在本文中，我们使用 COLIEE Task 4 数据集探索生成式预训练 Transformer (GPT) 在跨语言法律问答 (QA) 系统中的应用。在 COLIEE 任务 4 中，给定一个陈述和一组作为上下文的相关法律文章，目标是确定该陈述是否具有法律效力，即是否可以从提供的上下文文章中推断出该陈述，即也称为蕴含任务。通过对英语和日语提示和数据的四种不同组合进行基准测试，我们为 GPT 在多语言法律 QA 场景中的表现提供了宝贵的见解，有助于在法律领域开发更高效、更准确的跨语言 QA 解决方案。</li>
</ul>

<h3>Title: Large Language Models for Education: A Survey and Outlook</h3>
<ul>
<li><strong>Authors: </strong>Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18105">https://arxiv.org/abs/2403.18105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18105">https://arxiv.org/pdf/2403.18105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18105]] Large Language Models for Education: A Survey and Outlook(https://arxiv.org/abs/2403.18105)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的出现为教育领域带来了一个充满可能性的新时代。本调查论文从多方面的角度总结了法学硕士在教育环境中的各种技术，包括学生和教师帮助、适应性学习和商业工具。我们系统地回顾每个角度的技术进步，组织相关数据集和基准，并确定与在教育中部署法学硕士相关的风险和挑战。此外，我们概述了未来的研究机会，强调了潜在的有前途的方向。我们的调查旨在为教育工作者、研究人员和政策制定者提供全面的技术图景，以利用法学硕士的力量彻底改变教育实践并营造更有效的个性化学习环境。</li>
</ul>

<h3>Title: ChatGPT Role-play Dataset: Analysis of User Motives and Model  Naturalness</h3>
<ul>
<li><strong>Authors: </strong>Yufei Tao, Ameeta Agrawal, Judit Dombi, Tetyana Sydorenko, Jung In Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18121">https://arxiv.org/abs/2403.18121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18121">https://arxiv.org/pdf/2403.18121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18121]] ChatGPT Role-play Dataset: Analysis of User Motives and Model  Naturalness(https://arxiv.org/abs/2403.18121)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.</li>
<li><strong>摘要：</strong>ChatGPT 等交互式大型语言模型的最新进展已经彻底改变了各个领域；然而，他们在自然和角色扮演对话环境中的行为仍未得到充分研究。在我们的研究中，我们通过分析 ChatGPT 在正常方式和角色扮演环境下的交互，深入研究 ChatGPT 在不同环境下的对话中的行为，从而解决了这一差距。我们引入了一个包含广泛人类与人工智能对话的新颖数据集，并标注了用户动机和模型自然性，以检查（i）人类如何与对话式人工智能模型互动，以及（ii）人工智能模型响应的自然程度。我们的研究强调了与 ChatGPT 交互时用户动机的多样性和可变的人工智能自然度，不仅展示了人类与人工智能之间自然对话的微妙动态，而且还为提高人类与人工智能通信的有效性提供了新途径。</li>
</ul>

<h3>Title: For those who don't know (how) to ask: Building a dataset of technology  questions for digital newcomers</h3>
<ul>
<li><strong>Authors: </strong>Evan Lucas, Kelly S. Steelman, Leo C. Ureel, Charles Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18125">https://arxiv.org/abs/2403.18125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18125">https://arxiv.org/pdf/2403.18125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18125]] For those who don't know (how) to ask: Building a dataset of technology  questions for digital newcomers(https://arxiv.org/abs/2403.18125)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While the rise of large language models (LLMs) has created rich new opportunities to learn about digital technology, many on the margins of this technology struggle to gain and maintain competency due to lexical or conceptual barriers that prevent them from asking appropriate questions. Although there have been many efforts to understand factuality of LLM-created content and ability of LLMs to answer questions, it is not well understood how unclear or nonstandard language queries affect the model outputs. We propose the creation of a dataset that captures questions of digital newcomers and outsiders, utilizing data we have compiled from a decade's worth of one-on-one tutoring. In this paper we lay out our planned efforts and some potential uses of this dataset.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 的兴起创造了丰富的学习数字技术的新机会，但许多处于该技术边缘的人由于词汇或概念障碍而无法提出适当的问题，因此难以获得和保持能力。尽管已经做出了很多努力来了解法学硕士创建内容的真实性以及法学硕士回答问题的能力，但尚不清楚不明确或不标准的语言查询如何影响模型输出。我们建议创建一个数据集，利用我们从十年的一对一辅导中收集的数据来捕获数字新手和局外人的问题。在本文中，我们列出了我们计划的工作以及该数据集的一些潜在用途。</li>
</ul>

<h3>Title: Juru: Legal Brazilian Large Language Model from Reputable Sources</h3>
<ul>
<li><strong>Authors: </strong>Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18140">https://arxiv.org/abs/2403.18140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18140">https://arxiv.org/pdf/2403.18140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18140]] Juru: Legal Brazilian Large Language Model from Reputable Sources(https://arxiv.org/abs/2403.18140)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.</li>
<li><strong>摘要：</strong>与预训练大型语言模型相关的高计算成本限制了他们的研究。为了解决这个问题，出现了两种策略：领域专业化和使用高质量数据进行预训练。为了探索这些策略，我们专门使用了来自巴西著名法律来源的 19 亿个独特代币的 Sabi'a-2 Small 模型，并对法律和常识考试进行了几次评估。我们的模型 Juru 展示了领域专业化和减少预训练数据量的好处。然而，这种专业化是以降低同一语言中其他知识领域的性能为代价的。这项研究提供了越来越多的科学证据，表明预训练数据选择可以提高大型语言模型的性能，从而以较低的成本探索这些模型。</li>
</ul>

<h3>Title: Large Language Models Produce Responses Perceived to be Empathic</h3>
<ul>
<li><strong>Authors: </strong>Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18148">https://arxiv.org/abs/2403.18148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18148">https://arxiv.org/pdf/2403.18148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18148]] Large Language Models Produce Responses Perceived to be Empathic(https://arxiv.org/abs/2403.18148)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is important.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在许多任务上都表现出了令人惊讶的表现，包括编写表现出同理心的支持性信息。在这里，我们让这些模型生成移情信息，以响应描述常见生活经历的帖子，例如工作场所情况、养育子女、人际关系以及其他引起焦虑和愤怒的情况。在两项研究 (N=192, 202) 中，我们向人类评分者展示了由多个模型（GPT4 Turbo、Llama2 和 Mistral）编写的各种回复，并让人们根据这些回复的共情程度进行评分。我们发现，法学硕士生成的回复始终被认为比人类撰写的回复更具同理心。语言分析还表明，这些模型在标点符号、表情符号和某些单词的使用方面以独特的、可预测的“风格”写作。这些结果凸显了在同理心很重要的情况下使用法学硕士来增强人类同伴支持的潜力。</li>
</ul>

<h3>Title: Large Language Models as Financial Data Annotators: A Study on  Effectiveness and Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Toyin Aguda, Suchetha Siddagangappa, Elena Kochkina, Simerjot Kaur, Dongsheng Wang, Charese Smiley, Sameena Shah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18152">https://arxiv.org/abs/2403.18152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18152">https://arxiv.org/pdf/2403.18152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18152]] Large Language Models as Financial Data Annotators: A Study on  Effectiveness and Efficiency(https://arxiv.org/abs/2403.18152)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify outputs that may require expert attention. Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.</li>
<li><strong>摘要：</strong>由于领域专家的稀缺以及雇用专家的成本较高，收集金融领域的标记数据集具有挑战性。虽然大型语言模型（LLM）在通用领域数据集的数据注释任务中表现出了卓越的性能，但它们在特定领域数据集上的有效性仍未得到充分探索。为了解决这一差距，我们研究了法学硕士作为提取财务文档中关系的有效数据注释器的潜力。我们将三个法学硕士（GPT-4、PaLM 2 和 MPT Instruct）生成的注释与专家注释者和众包工作者进行比较。我们证明，当前最先进的法学硕士足以替代非专家众包工作者。我们使用各种提示和参数设置来分析模型，发现通过提供属于这些组的特定示例来自定义每个关系组的提示至关重要。此外，我们引入了可靠性指数（LLM-RelIndex），用于识别可能需要专家关注的输出。最后，我们进行了大量的时间、成本和错误分析，并为在特定领域的设置中收集和使用自动注释提供建议。</li>
</ul>

<h3>Title: Mechanisms of non-factual hallucinations in language models</h3>
<ul>
<li><strong>Authors: </strong>Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18167">https://arxiv.org/abs/2403.18167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18167">https://arxiv.org/pdf/2403.18167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18167]] Mechanisms of non-factual hallucinations in language models(https://arxiv.org/abs/2403.18167)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics for the two mechanistic causes of hallucinations. We also highlight how attribution features from our causal analysis can effectively construct hallucination detectors. Our work proposes a mechanistic understanding of LM factual errors.</li>
<li><strong>摘要：</strong>最先进的语言模型（LM）有时会产生与世界知识不一致的非事实幻觉。尽管在检测和减轻幻觉方面做出了广泛的努力，但了解其内部机制仍然难以捉摸。我们的研究调查了幻觉的机制原因，特别是非事实幻觉，其中机器学习错误地预测了对象属性以响应主题关系查询。通过因果中介分析和嵌入空间投影，我们确定了不同规模和设计的 LM 之间共有的幻觉的两个一般机制原因：1）下层 MLP 中的主题属性知识不足，2）未能在上层中选择正确的对象属性注意力头和 MLP。这两种机制表现出不同程度的主客关联、预测不确定性和扰动鲁棒性。此外，我们仔细检查了 LM 预训练检查点，揭示了幻觉的两种机械原因的不同学习动态。我们还强调了因果分析中的归因特征如何有效地构建幻觉检测器。我们的工作提出了对 LM 事实错误的机械理解。</li>
</ul>

<h3>Title: Exploring the Deceptive Power of LLM-Generated Fake News: A Study of  Real-World Detection Challenges</h3>
<ul>
<li><strong>Authors: </strong>Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18249">https://arxiv.org/abs/2403.18249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18249">https://arxiv.org/pdf/2403.18249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18249]] Exploring the Deceptive Power of LLM-Generated Fake News: A Study of  Real-World Detection Challenges(https://arxiv.org/abs/2403.18249)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展使得假新闻的产生成为可能，特别是在医疗保健等复杂领域。研究强调了法学硕士生成的假新闻在有或没有人工帮助的情况下的欺骗力之间的差距，但提示技术的潜力尚未得到充分探索。因此，这项工作旨在确定提示策略是否可以有效缩小这一差距。当前基于 LLM 的假新闻攻击需要人为干预来收集信息，并且经常会错过细节并且无法保持上下文一致性。因此，为了更好地理解威胁策略，我们提出了一种强大的假新闻攻击方法，称为条件变分自动编码器类提示（VLPrompt）。与当前方法不同，VLPrompt 无需额外收集数据，同时保持上下文连贯性并保留原始文本的复杂性。为了推动未来检测 VLPrompt 攻击的研究，我们创建了一个名为 VLPrompt 假新闻 (VLPFN) 的新数据集，其中包含真实和虚假文本。我们的实验（包括各种检测方法和新颖的人类研究指标）是为了评估它们在我们的数据集上的表现，并得出了大量的发现。</li>
</ul>

<h3>Title: MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Kaidi Jia, Rongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18253">https://arxiv.org/abs/2403.18253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18253">https://arxiv.org/pdf/2403.18253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18253]] MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation(https://arxiv.org/abs/2403.18253)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Metaphors are ubiquitous in daily life, yet detecting them poses a significant challenge. Previous approaches often struggled with improper application of language rules and overlooked the issue of data sparsity. To address these challenges, we introduce knowledge distillation and prompt learning into metaphor detection. Specifically, we devise a prompt learning template tailored for the metaphor detection task. By masking target words and providing relevant prompt information, we guide the model to accurately infer the contextual meaning of these words. This approach not only mitigates the interference from the literal meaning of target words but also ensures the proper utilization of MIP language rules for metaphor detection. Moreover, we employ a teacher model equipped with prior knowledge to generate meaningful soft labels, guiding the optimization process of the student model. The inclusion of soft labels, akin to label smoothing, helps alleviate the model's tendency towards over-confidence and effectively addresses the challenge of data sparsity. Experimental results demonstrate that our proposed model achieves state-of-the-art performance across multiple datasets.</li>
<li><strong>摘要：</strong>隐喻在日常生活中无处不在，但检测它们却构成了重大挑战。以前的方法经常遇到语言规则应用不当的问题，并且忽视了数据稀疏性的问题。为了应对这些挑战，我们将知识蒸馏和即时学习引入隐喻检测中。具体来说，我们设计了一个专为隐喻检测任务量身定制的即时学习模板。通过屏蔽目标词并提供相关提示信息，我们引导模型准确推断这些词的上下文含义。这种方法不仅减轻了目标词字面意义的干扰，而且确保了正确利用 MIP 语言规则进行隐喻检测。此外，我们采用配备先验知识的教师模型来生成有意义的软标签，指导学生模型的优化过程。包含软标签（类似于标签平滑）有助于减轻模型过度自信的倾向，并有效解决数据稀疏的挑战。实验结果表明，我们提出的模型在多个数据集上实现了最先进的性能。</li>
</ul>

<h3>Title: BlendX: Complex Multi-Intent Detection with Blended Patterns</h3>
<ul>
<li><strong>Authors: </strong>Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18277">https://arxiv.org/abs/2403.18277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18277">https://arxiv.org/pdf/2403.18277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18277]] BlendX: Complex Multi-Intent Detection with Blended Patterns(https://arxiv.org/abs/2403.18277)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field. The dataset is available at https://github.com/HYU-NLP/BlendX.</li>
<li><strong>摘要：</strong>面向任务的对话（TOD）系统通常是在假设每个话语代表一个单一意图的情况下设计的。然而，这种假设可能无法准确反映现实世界的情况，即用户经常在单个话语中表达多个意图。尽管人们对多意图检测 (MID) 越来越感兴趣，但现有的域内数据集（例如 MixATIS 和 MixSNIPS）在其表述方面存在局限性。为了解决这些问题，我们提出了 BlendX，这是一套精炼的数据集，具有比前身更多样化的模式，从而提高了其复杂性和多样性。对于数据集构建，我们利用基于规则的启发式方法以及生成工具（OpenAI 的 ChatGPT），该工具通过相似性驱动的话语选择策略进行了增强。为了确保所提出的数据集的质量，我们还引入了三个新颖的指标，用于评估与字数、连词使用和代词使用相关的话语的统计特性。 BlendX 上的大量实验表明，最先进的 MID 模型正在努力应对新数据集带来的挑战，这凸显了重新审视 MID 领域当前状态的必要性。该数据集可从 https://github.com/HYU-NLP/BlendX 获取。</li>
</ul>

<h3>Title: Few-Shot Recalibration of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18286">https://arxiv.org/abs/2403.18286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18286">https://arxiv.org/pdf/2403.18286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18286]] Few-Shot Recalibration of Language Models(https://arxiv.org/abs/2403.18286)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.</li>
<li><strong>摘要：</strong>最近的工作发现了从语言模型 (LM) 中提取经过良好校准的置信度估计的有前途的方法，其中模型的置信度分数反映了它正确的可能性。然而，虽然 LM 可能在广泛的分布上表现出良好的校准，但这通常隐藏了较窄范围内的严重错误校准（例如，对数学的系统性过度自信可以平衡对历史的系统性信心不足，从而产生总体上的完美校准）。为了对分布的任何切片获得良好校准的置信度估计，我们提出了一个用于少数样本切片特定重新校准的新框架。具体来说，我们训练一个重新校准模型，该模型从任何给定切片中获取一些未标记的示例，并预测一条曲线，该曲线重新映射置信度分数，以便该切片更加准确。我们训练有素的模型可以重新校准任意新切片，而无需使用该切片中的任何标记数据。这使我们能够识别特定领域的置信阈值，高于该阈值，LM 的预测可以被信任，低于该阈值，则应该放弃。实验表明，我们的小样本重新校准器始终优于现有校准方法，例如与温度缩放相比，MMLU 上的 PaLM2-Large 的校准误差提高了 16%。</li>
</ul>

<h3>Title: Dual Instruction Tuning with Large Language Models for Mathematical  Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yongwei Zhou, Tiejun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18295">https://arxiv.org/abs/2403.18295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18295">https://arxiv.org/pdf/2403.18295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18295]] Dual Instruction Tuning with Large Language Models for Mathematical  Reasoning(https://arxiv.org/abs/2403.18295)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Comprehensive experiments validate the effectiveness and domain generalization of the dual instruction tuning strategy across various mathematical reasoning tasks.</li>
<li><strong>摘要：</strong>最近的进展凸显了利用思想链 (CoT) 数据进行数学推理任务的大型语言模型 (LLM) 指令调整的成功。尽管法学硕士进行了微调，但挑战仍然存在，例如 CoT 生成中的错误、缺失和冗余步骤导致答案预测不准确。为了缓解这个问题，我们提出了一种双指令调整策略，从正向和反向方向仔细建模数学推理。这涉及引入中间推理状态预测任务（正向推理）和指令重构任务（反向推理），以增强法学硕士对指令的理解和执行。这些任务的训练实例是基于现有的数学指令调整数据集构建的。随后，法学硕士使用现有的数学指令和新创建的数据进行多任务微调。综合实验验证了双指令调优策略在各种数学推理任务中的有效性和领域泛化性。</li>
</ul>

<h3>Title: Can LLMs Converse Formally? Automatically Assessing LLMs in Translating  and Interpreting Formal Specifications</h3>
<ul>
<li><strong>Authors: </strong>Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18327">https://arxiv.org/abs/2403.18327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18327">https://arxiv.org/pdf/2403.18327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18327]] Can LLMs Converse Formally? Automatically Assessing LLMs in Translating  and Interpreting Formal Specifications(https://arxiv.org/abs/2403.18327)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.</li>
<li><strong>摘要：</strong>利益相关者通常使用自然语言描述系统需求，然后由领域专家将其转换为正式语法，从而导致设计成本增加。本文评估了大型语言模型 (LLM) 在自然语言描述和形式规范之间转换的能力。现有的工作已经评估了法学硕士生成正式语法（例如源代码）的能力，但此类实验通常是手工制作的，并且使用可能存在于法学硕士训练集中的问题，并且通常需要人工注释的数据集。我们提出了一种方法，可以将法学硕士的两个副本与现成的验证器结合使用，自动评估其翻译能力，而无需任何额外的人工输入。我们的方法使用语言语法生成形式语法来自动生成数据集。我们进行了实证评估来衡量此翻译任务的准确性，并表明 SOTA LLM 无法充分解决此任务，限制了它们当前在复杂系统设计中的效用。</li>
</ul>

<h3>Title: A Dataset for Pharmacovigilance in German, French, and Japanese:  Annotating Adverse Drug Reactions across Languages</h3>
<ul>
<li><strong>Authors: </strong>Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18336">https://arxiv.org/abs/2403.18336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18336">https://arxiv.org/pdf/2403.18336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18336]] A Dataset for Pharmacovigilance in German, French, and Japanese:  Annotating Adverse Drug Reactions across Languages(https://arxiv.org/abs/2403.18336)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.</li>
<li><strong>摘要：</strong>随着数字世界中出现越来越多的讨论，用户生成的数据源在发现药物不良反应 (ADR) 方面发挥着重要作用。然而，现有的临床语料库主要围绕英文科学文章。这项工作提供了从不同来源收集的有关 ADR 的多语言文本语料库，包括患者论坛、社交媒体以及德语、法语和日语的临床报告。我们的语料库包含涵盖 12 种实体类型、四种属性类型和 13 种关系类型的注释。它有助于开发现实世界的医疗保健多语言语言模型。我们提供统计数据来强调与语料库相关的某些挑战，并进行初步实验，为提取语言内和跨语言的实体和这些实体之间的关系提供强有力的基线。</li>
</ul>

<h3>Title: IterAlign: Iterative Constitutional Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui Li, Zheng Li, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18341">https://arxiv.org/abs/2403.18341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18341">https://arxiv.org/pdf/2403.18341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18341]] IterAlign: Iterative Constitutional Alignment of Large Language Models(https://arxiv.org/abs/2403.18341)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\%$ in harmlessness.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的快速发展，使 LLM 与人类价值观和社会规范保持一致以确保其可靠性和安全性变得至关重要。人们提出了利用人类反馈的强化学习（RLHF）和宪法人工智能（CAI）来实现法学硕士的调整。然而，这些方法要么需要大量的人工注释，要么需要明确的预定义宪法，这是劳动密集型和资源消耗型的。为了克服这些缺点，我们研究了基于宪法的LLM对齐，并提出了一种数据驱动的宪法发现和自对齐框架，称为IterAlign。 IterAlign 利用红队来揭示 LLM 的弱点，并使用更强大的 LLM 自动发现新的宪法。然后，这些章程将用于指导基础法学硕士的自我修正。这样的宪法发现管道可以迭代地自动运行，以发现专门针对当前法学硕士中的对齐差距的新宪法。多个安全基准数据集和多个基础 LLM 的实证结果表明，IterAlign 成功提高了真实性、乐于助人、无害性和诚实性，将 LLM 一致性提高了高达 13.5\%$ 的无害性。</li>
</ul>

<h3>Title: Quantifying and Mitigating Unimodal Biases in Multimodal Large Language  Models: A Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18346">https://arxiv.org/abs/2403.18346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18346">https://arxiv.org/pdf/2403.18346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18346]] Quantifying and Mitigating Unimodal Biases in Multimodal Large Language  Models: A Causal Perspective(https://arxiv.org/abs/2403.18346)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA) framework for limited-access MLLMs and the refinement of open-source MLLMs through fine-tuning. Extensive quantitative and qualitative experiments offer valuable insights for future research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展促进了多模式 LLM (MLLM) 的发展。尽管 MLLM 的能力令人印象深刻，但它们常常过度依赖单模态偏差（例如语言偏差和视觉偏差），导致在复杂的多模态任务中给出错误的答案。为了研究这个问题，我们提出了一个因果框架来解释视觉问答（VQA）问题中的偏差。在我们的框架内，我们设计了一个因果图来阐明 MLLM 对 VQA 问题的预测，并通过深入的因果分析来评估偏差的因果影响。受因果图的启发，我们引入了一个新颖的 MORE 数据集，由 12,000 个 VQA 实例组成。该数据集旨在挑战 MLLM 的能力，需要多跳推理和克服单峰偏差。此外，我们提出了两种策略来减轻单峰偏差并增强 MLLM 的推理能力，包括用于受限访问 MLLM 的分解-验证-答案（DeVA）框架以及通过微调对开源 MLLM 进行细化。广泛的定量和定性实验为未来的研究提供了宝贵的见解。</li>
</ul>

<h3>Title: Rejection Improves Reliability: Training LLMs to Refuse Unknown  Questions Using RL from Knowledge Feedback</h3>
<ul>
<li><strong>Authors: </strong>Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18349">https://arxiv.org/abs/2403.18349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18349">https://arxiv.org/pdf/2403.18349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18349]] Rejection Improves Reliability: Training LLMs to Refuse Unknown  Questions Using RL from Knowledge Feedback(https://arxiv.org/abs/2403.18349)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）经常产生错误的输出，称为幻觉，因为它们在辨别超出其知识范围的问题方面存在局限性。虽然解决幻觉一直是研究的焦点，但之前的努力主要集中在增强正确性上，而没有适当考虑排斥机制的重要性。在本文中，我们对拒绝的作用进行了全面检查，引入了模型可靠性的概念以及相应的指标。这些指标衡量模型提供准确响应的能力，同时巧妙地拒绝超出其知识边界的问题，从而最大限度地减少幻觉。为了提高法学硕士的固有可靠性，我们提出了一种新颖的对齐框架，称为知识反馈强化学习（RLKF）。 RLKF利用知识反馈动态确定模型的知识边界，并训练可靠的奖励模型以鼓励拒绝知识外的问题。数学题的实验结果证实了RLKF在显着提高LLM可靠性方面的实质性功效。</li>
</ul>

<h3>Title: Evaluation of Semantic Search and its Role in  Retrieved-Augmented-Generation (RAG) for Arabic Language</h3>
<ul>
<li><strong>Authors: </strong>Ali Mahboub, Muhy Eddin Za'ter, Bashar Alfrou, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18350">https://arxiv.org/abs/2403.18350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18350">https://arxiv.org/pdf/2403.18350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18350]] Evaluation of Semantic Search and its Role in  Retrieved-Augmented-Generation (RAG) for Arabic Language(https://arxiv.org/abs/2403.18350)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG).</li>
<li><strong>摘要：</strong>机器学习和深度学习的最新进展提出了语义相似性的概念，事实证明它在多种应用中非常有益，并且在很大程度上取代了关键字搜索。然而，评估语义相似性并在各种文档中对特定查询进行搜索仍然是一项复杂的任务。这种复杂性是由于任务的多面性、缺乏标准基准造成的，而这些挑战对于阿拉伯语来说则进一步放大。本文致力于为阿拉伯语语义搜索建立一个简单而有效的基准。此外，为了精确评估这些指标和数据集的有效性，我们在检索增强生成（RAG）的框架内进行语义搜索评估。</li>
</ul>

<h3>Title: BLADE: Enhancing Black-box Large Language Models with Small  Domain-Specific Models</h3>
<ul>
<li><strong>Authors: </strong>Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18365">https://arxiv.org/abs/2403.18365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18365">https://arxiv.org/pdf/2403.18365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18365]] BLADE: Enhancing Black-box Large Language Models with Small  Domain-Specific Models(https://arxiv.org/abs/2403.18365)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing a diverse range of tasks. However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs. Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models. BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities. Specifically, our method involves three steps: 1) pre-training the small LM with domain-specific data, 2) fine-tuning this model using knowledge instruction data, and 3) joint Bayesian optimization of the general LLM and the small LM. Extensive experiments conducted on public legal and medical benchmarks reveal that BLADE significantly outperforms existing approaches. This shows the potential of BLADE as an effective and cost-efficient solution in adapting general LLMs for vertical domains.</li>
<li><strong>摘要：</strong>ChatGPT 和 GPT-4 等大型语言模型 (LLM) 用途广泛，能够解决各种任务。然而，基于开放领域数据开发的一般法学硕士可能缺乏垂直领域任务所必需的特定领域知识，例如法律、医学等。为了解决这个问题，以前的方法要么进行持续的预训练特定领域的数据或采用检索增强来支持一般的法学硕士。不幸的是，这些策略要么成本密集，要么在实际应用中不可靠。为此，我们提出了一个名为 BLADE 的新颖框架，它通过小型领域特定模型增强了黑盒大型语言模型。 BLADE 由黑盒 LLM 和小型特定领域 LM 组成。小型法学硕士保留了特定领域的知识并提供专业见解，而通用法学硕士则提供强大的语言理解和推理能力。具体来说，我们的方法涉及三个步骤：1）使用特定领域的数据预训练小型LM，2）使用知识指令数据微调该模型，3）通用LLM和小型LM的联合贝叶斯优化。对公共法律和医学基准进行的广泛实验表明，BLADE 的性能明显优于现有方法。这显示了 BLADE 作为一种有效且经济高效的解决方案的潜力，可以使通用法学硕士适应垂直领域。</li>
</ul>

<h3>Title: Improving Attributed Text Generation of Large Language Models via  Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18381">https://arxiv.org/abs/2403.18381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18381">https://arxiv.org/pdf/2403.18381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18381]] Improving Attributed Text Generation of Large Language Models via  Preference Learning(https://arxiv.org/abs/2403.18381)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Moreover, inspired by the human citation process, we further propose a progressive preference optimization method by leveraging fine-grained information. Extensive experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate that APO achieves state-of-the-art citation F1 with higher answer quality.</li>
<li><strong>摘要：</strong>大型语言模型已广泛应用于自然语言处理中，但它们面临着生成不可靠内容的挑战。最近的工作旨在通过诉诸归因作为提供证据（即引用）的手段来减少错误信息和幻觉。然而，当前的归因方法通常侧重于检索阶段和自动评估，而忽略了反映人类学术写作中的引用机制以增强可信度。在本文中，我们通过将归因任务建模为偏好学习并引入自动偏好优化（APO）框架来解决这些挑战。首先，我们通过收集和过滤现有数据集，创建一个包含 6,330 个示例的训练后精选集合。其次，考虑到标记偏好数据的高成本，我们进一步提出了一种自动方法来合成归因偏好数据，得到 95,263 对。此外，受人类引用过程的启发，我们进一步提出了一种利用细粒度信息的渐进式偏好优化方法。对三个数据集（即 ASQA、StrategyQA 和 ELI5）的大量实验表明，APO 以更高的答案质量实现了最先进的引文 F1。</li>
</ul>

<h3>Title: BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text</h3>
<ul>
<li><strong>Authors: </strong>Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18421">https://arxiv.org/abs/2403.18421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18421">https://arxiv.org/pdf/2403.18421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18421]] BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text(https://arxiv.org/abs/2403.18421)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-preserving, economical and environmentally friendly foundations for particular NLP applications, such as in biomedicine. The model is available on the Hugging Face Hub: https://huggingface.co/stanford-crfm/BioMedLM.</li>
<li><strong>摘要：</strong>GPT-4 和 Med-PaLM 2 等模型在各种生物医学 NLP 任务中表现出了令人印象深刻的性能。然而，这些模型具有数千亿个参数，运行计算成本昂贵，需要用户通过互联网发送输入数据，并且需要在未知数据源上进行训练。更小、更有针对性的模型可以竞争吗？为了解决这个问题，我们构建并发布了 BioMedLM，这是一个 27 亿参数的 GPT 式自回归模型，专门在 PubMed 摘要和完整文章上进行训练。经过微调后，BioMedLM 可以产生强大的多项选择生物医学问答结果，可与更大的模型相媲美，例如在 MedMCQA (dev) 上获得 57.3% 的分数，在 MMLU 医学遗传学考试中获得 69.0% 的分数。 BioMedLM 还可以进行微调，为患者提出的医学主题问题提供有用的答案。这表明较小的模型有可能成为特定 NLP 应用（例如生物医学）的透明、隐私保护、经济和环保的基础。该模型可在 Hugging Face Hub 上找到：https://huggingface.co/stanford-crfm/BioMedLM。</li>
</ul>

<h3>Title: SemRoDe: Macro Adversarial Training to Learn Representations That are  Robust to Word-Level Attacks</h3>
<ul>
<li><strong>Authors: </strong>Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18423">https://arxiv.org/abs/2403.18423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18423">https://arxiv.org/pdf/2403.18423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18423]] SemRoDe: Macro Adversarial Training to Learn Representations That are  Robust to Word-Level Attacks(https://arxiv.org/abs/2403.18423)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness. We align the domains by incorporating a new distance-based objective. With this, our model is able to learn more generalized representations by aligning the model's high-level output features and therefore better handling unseen adversarial samples. This method can be generalized across word embeddings, even when they share minimal overlap at both vocabulary and word-substitution levels. To evaluate the effectiveness of our approach, we conduct experiments on BERT and RoBERTa models on three datasets. The results demonstrate promising state-of-the-art robustness.</li>
<li><strong>摘要：</strong>语言模型（LM）是自然语言处理任务不可或缺的工具，但它们面对对抗性攻击的脆弱性仍然令人担忧。虽然当前的研究已经探索了对抗性训练技术，但它们在防御单词级攻击方面的改进仍然有限。在这项工作中，我们提出了一种称为语义鲁棒防御（SemRoDe）的新方法，这是一种增强 LM 鲁棒性的宏观对抗训练策略。受到最近图像领域研究的启发，我们进行了调查并随后确认，在语言等离散数据设置中，通过单词替换生成的对抗性样本确实属于与基本域表现出较高 Wasserstein 距离的对抗性域。我们的方法学习了连接这两个领域的稳健表示。我们假设，如果样本不被投射到对抗域，而是投射到偏移最小的域，它将提高攻击的鲁棒性。我们通过纳入新的基于距离的目标来调整领域。这样，我们的模型就能够通过调整模型的高级输出特征来学习更通用的表示，从而更好地处理看不见的对抗性样本。这种方法可以推广到单词嵌入，即使它们在词汇和单词替换级别上的重叠最小。为了评估我们方法的有效性，我们在三个数据集上对 BERT 和 RoBERTa 模型进行了实验。结果证明了最先进的稳健性。</li>
</ul>

<h3>Title: TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions</h3>
<ul>
<li><strong>Authors: </strong>Jamshid Mozafari, Anubhav Jangra, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18426">https://arxiv.org/abs/2403.18426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18426">https://arxiv.org/pdf/2403.18426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18426]] TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions(https://arxiv.org/abs/2403.18426)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints. The effectiveness of hints varied, with success rates of 96%, 78%, and 36% for questions with easy, medium, and hard answers, respectively. Moreover, the proposed automatic evaluation methods showed a robust correlation with annotators' results. Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing automatic evaluation methods for hint assessment.</li>
<li><strong>摘要：</strong>如今，人们倾向于与大型语言模型进行对话，寻求问题的答案。在任何人都可以轻易获得这些答案的时代，刺激和保存人类的认知能力，以及保证人类保持良好的推理能力就变得至关重要。本研究通过提出提示（而不是最终答案或给出答案之前）作为可行的解决方案来解决此类需求。我们引入了一个用于事实类问题自动提示生成的框架，用它来构建 TriviaHG，这是一个新颖的大规模数据集，包含 160,230 个提示，对应于 TriviaQA 数据集中的 16,645 个问题。此外，我们提出了一种自动评估方法，用于测量提示的收敛性和熟悉性质量属性。为了评估 TriviaHG 数据集和提出的评估方法，我们招募了 10 个人来注释 2,791 个提示，并要求 6 个人使用提供的提示回答问题。提示的有效性各不相同，对于简单、中等和困难答案的问题，提示的成功率分别为 96%、78% 和 36%。此外，所提出的自动评估方法显示出与注释者结果的稳健相关性。最后，研究结果强调了三个关键见解：提示在解决未知问题中的促进作用、提示质量对答案难度的依赖以及采用自动评估方法进行提示评估的可行性。</li>
</ul>

<h3>Title: Can Language Beat Numerical Regression? Language-Based Multimodal  Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Inhwan Bae, Junoh Lee, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18447">https://arxiv.org/abs/2403.18447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18447">https://arxiv.org/pdf/2403.18447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18447]] Can Language Beat Numerical Regression? Language-Based Multimodal  Trajectory Prediction(https://arxiv.org/abs/2403.18447)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language model in understanding and reasoning high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task question and answering. We then train a numerical tokenizer with the prompt data. We encourage the tokenizer to separate the integer and decimal parts well, and leverage it to capture correlations between the consecutive numbers in the language model. Lastly, we train the language model using the numerical tokenizer and all of the question-answer prompts. Here, we propose a beam-search-based most-likely prediction and a temperature-based multimodal prediction to implement both deterministic and stochastic inferences. Applying our LMTraj, we show that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods. Code is publicly available at https://github.com/inhwanbae/LMTrajectory .</li>
<li><strong>摘要：</strong>语言模型在上下文理解和生成性能方面表现出了令人印象深刻的能力。受最近语言基础模型成功的启发，在本文中，我们提出了 LMTraj（基于语言的多模态轨迹预测器），它将轨迹预测任务重新转化为一种问答问题。与传统的数值回归模型将轨迹坐标序列视为连续信号不同，我们将它们视为像文本提示一样的离散信号。特别地，我们首先将轨迹坐标的输入空间转换为自然语言空间。这里，行人的整个时间序列轨迹被转换为文本提示，场景图像通过图像字幕描述为文本信息。然后将转换后的数字和图像数据包装到问答模板中以在语言模型中使用。接下来，为了指导语言模型理解和推理高级知识，例如场景上下文和行人之间的社会关系，我们引入了辅助多任务问答。然后，我们使用提示数据训练数字标记器。我们鼓励分词器很好地分离整数和小数部分，并利用它来捕获语言模型中连续数字之间的相关性。最后，我们使用数字标记器和所有问答提示来训练语言模型。在这里，我们提出了基于波束搜索的最有可能预测和基于温度的多模态预测来实现确定性和随机推理。应用我们的 LMTraj，我们表明基于语言的模型可以成为强大的行人轨迹预测器，并且优于现有的基于数值的预测器方法。代码可在 https://github.com/inhwanbae/LMTrajectory 上公开获取。</li>
</ul>

<h3>Title: SDSAT: Accelerating LLM Inference through Speculative Decoding with  Semantic Adaptive Tokens</h3>
<ul>
<li><strong>Authors: </strong>Chengbo Liu, Yong Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18647">https://arxiv.org/abs/2403.18647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18647">https://arxiv.org/pdf/2403.18647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18647]] SDSAT: Accelerating LLM Inference through Speculative Decoding with  Semantic Adaptive Tokens(https://arxiv.org/abs/2403.18647)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.</li>
<li><strong>摘要：</strong>我们提出了一种通过语义自适应令牌推测解码（SDSAT）的大型语言模型（LLM）加速方案。此设计的主要目标是增强 LLM 模型更准确地生成草案代币的能力，同时又不影响模型的准确性。核心策略包括：1）通过合并具有灵活解码能力的语义自适应令牌来微调模型，而不改变其结构，从而使它们能够生成高质量的草案令牌。 2）通过采用不影响标准token的训练方法，模型可以在原有框架上以最小的训练开销获得并行解码能力。 3）我们使用贪婪搜索和核采样设计了“两步起草然后验证”生成策略。在 CodeLlama-13B 和 7B 模型上进行的实验，速度分别提高了 3.5 倍和 3.0 倍以上。请参考https://github.com/hasuoshenyun/SDSAT。</li>
</ul>

<h3>Title: Fact Checking Beyond Training Set</h3>
<ul>
<li><strong>Authors: </strong>Payam Karisani, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18671">https://arxiv.org/abs/2403.18671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18671">https://arxiv.org/pdf/2403.18671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18671]] Fact Checking Beyond Training Set(https://arxiv.org/abs/2403.18671)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift. To our knowledge, there is no publicly available multi-topic fact checking dataset. Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets. We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data.</li>
<li><strong>摘要：</strong>评估日常声明的真实性非常耗时，并且在某些情况下需要领域专业知识。我们凭经验证明，常用的事实检查管道（称为检索器读取器）在对来自一个域的标记数据进行训练并在另一域中使用时会出现性能下降。之后，我们深入研究管道的每个组件，并提出新的算法来解决这个问题。我们提出了一种对抗性算法，使检索器组件对分布偏移具有鲁棒性。我们的核心思想是首先在标记的源数据上训练双编码器，然后使用未标记的目标数据对抗性地训练两个单独的文档和声明编码器。然后，我们关注读者组件，并建议对其进行训练，使其对主张和证据文件的顺序不敏感。我们的实证评估支持这样的假设：此类阅读器对分布变化表现出更高的鲁棒性。据我们所知，没有公开可用的多主题事实检查数据集。因此，我们提出了一种简单的自动方法来重新利用两个众所周知的事实检查数据集。然后，我们根据这些数据集构建八个事实检查场景，并将我们的模型与一组强大的基线模型进行比较，包括最近使用 GPT4 生成合成数据的领域适应模型。</li>
</ul>

<h3>Title: NL-ITI: Optimizing Probing and Intervention for Improvement of ITI  Method</h3>
<ul>
<li><strong>Authors: </strong>Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18680">https://arxiv.org/abs/2403.18680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18680">https://arxiv.org/pdf/2403.18680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18680]] NL-ITI: Optimizing Probing and Intervention for Improvement of ITI  Method(https://arxiv.org/abs/2403.18680)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in the behavior of LLM at the same time (as measured by Kullback-Leibler divergence).</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 很容易返回错误信息。这是人工智能领域的主要挑战之一。在我们的工作中，我们探索了推理时间干预（ITI）引入的范式。在第一阶段，它识别注意力头，其中包含最多数量的所需知识类型（例如，真实的）。然后，在推理过程中，LLM 激活会针对选定的注意力头子集进行转移。我们通过引入非线性探测和多令牌干预——非线性ITI（NL-ITI）进一步改进了ITI框架。 NL-ITI 在各种多项选择基准上进行了测试，包括 TruthfulQA，我们报告相对于基准 ITI 结果，MC1 指标提高了约 14%。 NL-ITI 在其他测试集上也取得了令人鼓舞的结果 - 在 MMLU 的商业道德子域上，MC1 比基线 LLaMA2-7B 提高了约 18%。此外，NL-ITI 表现更好，同时对 LLM 行为的侵入性较小（通过 Kullback-Leibler 散度测量）。</li>
</ul>

<h3>Title: The Invalsi Benchmark: measuring Language Models Mathematical and  Language understanding in Italian</h3>
<ul>
<li><strong>Authors: </strong>Andrea Esuli, Giovanni Puccetti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18697">https://arxiv.org/abs/2403.18697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18697">https://arxiv.org/pdf/2403.18697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18697]] The Invalsi Benchmark: measuring Language Models Mathematical and  Language understanding in Italian(https://arxiv.org/abs/2403.18697)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian. This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy. To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\% accuracy. We believe that the release of this dataset paves the way for improving future models mathematical and language understanding in Italian.</li>
<li><strong>摘要：</strong>虽然从所有方面来看，意大利语都是一种高资源语言，但目前还没有专门针对这种语言进行预训练的语言模型。这导致评估意大利语语言模型性能的可用基准数量减少。这项工作提出了两个新的基准来评估模型在意大利语数学理解和语言理解方面的性能。这些基准基于意大利学校系统内 11 至 18 岁学生进行的真实测试，因此得到了几位教学法和教育学专家的验证。为了验证这个数据集，我们评估了 9 种语言模型的性能，这些模型在用意大利语编写时表现最好，包括我们自己的微调模型。我们证明这是一个具有挑战性的基准，当前的语言模型的准确率仅限于 60%。我们相信，该数据集的发布为改善未来意大利语模型的数学和语言理解铺平了道路。</li>
</ul>

<h3>Title: CheckEval: Robust Evaluation Framework using Large Language Model via  Checklist</h3>
<ul>
<li><strong>Authors: </strong>Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18771">https://arxiv.org/abs/2403.18771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18771">https://arxiv.org/pdf/2403.18771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18771]] CheckEval: Robust Evaluation Framework using Large Language Model via  Checklist(https://arxiv.org/abs/2403.18771)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.</li>
<li><strong>摘要：</strong>我们推出了 CheckEval，一种使用大型语言模型的新颖评估框架，解决了当前评估方法中模糊性和不一致的挑战。 CheckEval 通过将评估标准划分为详细的子方面并为每个子方面构建布尔问题清单来解决这些挑战，从而简化评估。这种方法不仅使过程更具可解释性，而且通过关注特定的评估维度，显着增强了结果的稳健性和可靠性。通过使用 SummEval 基准的重点案例研究进行验证，CheckEval 表明与人类判断具有很强的相关性。此外，它展示了高度一致的注释者间协议。这些发现凸显了 CheckEval 在客观、灵活和精确评估方面的有效性。通过提供可定制的交互式框架，CheckEval 为法学硕士在评估中的使用制定了新标准，响应了该领域不断变化的需求，并为未来基于法学硕士的评估建立了明确的方法。</li>
</ul>

<h3>Title: Towards a World-English Language Model for On-Device Virtual Assistants</h3>
<ul>
<li><strong>Authors: </strong>Rricha Jalota, Lyan Verwimp, Markus Nussbaum-Thom, Amr Mousa, Arturo Argueta, Youssef Oualil</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18783">https://arxiv.org/abs/2403.18783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18783">https://arxiv.org/pdf/2403.18783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18783]] Towards a World-English Language Model for On-Device Virtual Assistants(https://arxiv.org/abs/2403.18783)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are generally language-, region-, and in some cases, device-dependent, which increases the effort to scale and maintain them. Combining NNLMs for one or more of the categories is one way to improve scalability. In this work, we combine regional variants of English to build a ``World English'' NNLM for on-device VAs. In particular, we investigate the application of adapter bottlenecks to model dialect-specific characteristics in our existing production NNLMs {and enhance the multi-dialect baselines}. We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks. Based on this insight and leveraging the design of our production models, we introduce a new architecture for World English NNLM that meets the accuracy, latency, and memory constraints of our single-dialect models.</li>
<li><strong>摘要：</strong>虚拟助理 (VA) 的神经网络语言模型 (NNLM) 通常与语言、区域相关，在某些情况下与设备相关，这增加了扩展和维护它们的工作量。将 NNLM 组合用于一个或多个类别是提高可扩展性的一种方法。在这项工作中，我们结合了英语的区域变体，为设备上的 VA 构建了“世界英语”NNLM。特别是，我们研究了适配器瓶颈的应用，以在我们现有的生产 NNLM 中对方言特定特征进行建模{并增强多方言基线}。我们发现适配器模块在方言建模方面比专门化整个子网络更有效。基于这一见解并利用我们生产模型的设计，我们为世界英语 NNLM 引入了一种新架构，该架构满足我们单方言模型的准确性、延迟和内存限制。</li>
</ul>

<h3>Title: Long-form factuality in large language models</h3>
<ul>
<li><strong>Authors: </strong>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18802">https://arxiv.org/abs/2403.18802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18802">https://arxiv.org/pdf/2403.18802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18802]] Long-form factuality in large language models(https://arxiv.org/abs/2403.18802)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall). Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在响应有关开放式主题的事实寻求提示时，通常会生成包含事实错误的内容。为了在开放领域对模型的长格式事实性进行基准测试，我们首先使用 GPT-4 生成 LongFact，这是一个包含 38 个主题的数千个问题的提示集。然后，我们建议 LLM 代理可以通过我们称为搜索增强事实评估器（SAFE）的方法用作长形式事实性的自动评估器。 SAFE 利用 LLM 将长格式响应分解为一组单独的事实，并使用多步骤推理过程评估每个事实的准确性，包括将搜索查询发送到 Google 搜索并确定搜索是否支持事实结果。此外，我们建议将 F1 分数扩展为长篇事实性的聚合指标。为此，我们平衡响应中支持的事实的百分比（精度）与相对于表示用户首选响应长度（召回）的超参数提供的事实的百分比。根据经验，我们证明 LLM 代理可以实现超人的评级性能 - 在一组约 16k 个单独事实上，SAFE 在 72% 的时间内与众包人类注释者一致，并且在 100 个分歧案例的随机子集上，SAFE 赢得了 76% 的结果时间。同时，SAFE 的成本比人类注释者便宜 20 倍以上。我们还在 LongFact 上对四个模型系列（Gemini、GPT、Claude 和 PaLM-2）的 13 种语言模型进行了基准测试，发现较大的语言模型通常可以实现更好的长格式事实性。 LongFact、SAFE 和所有实验代码均可在 https://github.com/google-deepmind/long-form-factuality 上获取。</li>
</ul>

<h3>Title: Projective Methods for Mitigating Gender Bias in Pre-trained Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Hillary Dawkins, Isar Nejadgholi, Daniel Gillis, Judi McCuaig</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18803">https://arxiv.org/abs/2403.18803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18803">https://arxiv.org/pdf/2403.18803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18803]] Projective Methods for Mitigating Gender Bias in Pre-trained Language  Models(https://arxiv.org/abs/2403.18803)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model.</li>
<li><strong>摘要：</strong>NLP 中性别偏见的缓解与消除静态词嵌入的偏见有着悠久的历史。最近，注意力已经转移到消除预训练语言模型的偏差上。我们研究了为词嵌入开发的最简单的投影去偏方法在应用于 BERT 内部表示时能在多大程度上发挥作用。投影方法实现速度快，使用少量保存的参数，并且不对现有模型参数进行更新。我们评估了这些方法在减少内在偏差方面的功效（通过 BERT 的下一句话预测任务来衡量），以及在微调时减轻下游环境中观察到的偏差的功效。为此，我们还对流行的性别偏见评估测试进行了批判性分析，以量化内在偏见，从而产生了增强的测试集和新的偏见衡量标准。我们发现，投影方法可以有效地缓解内在偏差和下游偏差，但这两个结果不一定相关。这一发现警告说，基于语言建模任务或下一句预测的内在偏见测试集不应成为开发去偏见语言模型的唯一基准。</li>
</ul>

<h3>Title: Is Modularity Transferable? A Case Study through the Lens of Knowledge  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18804">https://arxiv.org/abs/2403.18804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18804">https://arxiv.org/pdf/2403.18804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18804]] Is Modularity Transferable? A Case Study through the Lens of Knowledge  Distillation(https://arxiv.org/abs/2403.18804)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we propose a method that allows the transfer of modules between incompatible PLMs without any change in the inference complexity. The experiments on Named Entity Recognition, Natural Language Inference, and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity.</li>
<li><strong>摘要：</strong>模块化深度学习的兴起展示了其在各种自然语言处理应用中的潜力。参数高效微调 (PEFT) 模块化已被证明适用于从领域适应到多语言设置的各种用例。然而，所有这些工作都涵盖了在一个预训练语言模型 (PLM) 中训练和部署模块化组件的情况。这种特定于模型的设置对模块化架构试图实现的模块化程度构成了重大限制。我们询问当前的模块化方法是否可以在模型之间转移，以及我们是否可以将模块从更强大和更大的 PLM 转移到更小的 PLM。在这项工作中，我们的目标是通过知识蒸馏（通常用于模型压缩）的视角来填补这一空白，并提出一种极其简单的方法来在同系列 PLM 之间传输预先训练的、特定于任务的 PEFT 模块。此外，我们提出了一种方法，允许在不兼容的 PLM 之间传输模块，而不会改变推理复杂性。针对多种语言和 PEFT 方法的命名实体识别、自然语言推理和释义识别任务的实验展示了可转移模块化的初始潜力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
