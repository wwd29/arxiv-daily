<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-17</h1>
<h3>Title: Do Large Language Models Understand Verbal Indicators of Romantic Attraction?</h3>
<ul>
<li><strong>Authors: </strong>Sandra C. Matz, Heinrich Peters, Paul W. Eastwick, Moran Cerf, Eli J. Finkel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10989">https://arxiv.org/abs/2407.10989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10989">https://arxiv.org/pdf/2407.10989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10989]] Do Large Language Models Understand Verbal Indicators of Romantic Attraction?(https://arxiv.org/abs/2407.10989)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>What makes people 'click' on a first date and become mutually attracted to one another? While understanding and predicting the dynamics of romantic interactions used to be exclusive to human judgment, we show that Large Language Models (LLMs) can detect romantic attraction during brief getting-to-know-you interactions. Examining data from 964 speed dates, we show that ChatGPT (and Claude 3) can predict both objective and subjective indicators of speed dating success (r=0.12-0.23). ChatGPT's predictions of actual matching (i.e., the exchange of contact information) were not only on par with those of human judges who had access to the same information but incremental to speed daters' own predictions. While some of the variance in ChatGPT's predictions can be explained by common content dimensions (such as the valence of the conversations) the fact that there remains a substantial proportion of unexplained variance suggests that ChatGPT also picks up on conversational dynamics. In addition, ChatGPT's judgments showed substantial overlap with those made by the human observers (mean r=0.29), highlighting similarities in their representation of romantic attraction that is, partially, independent of accuracy.</li>
<li><strong>摘要：</strong>是什么让人们在第一次约会时“一拍即合”并相互吸引？虽然理解和预测浪漫互动的动态曾经是人类判断的专属，但我们发现大型语言模型 (LLM) 可以在短暂的了解互动中检测到浪漫的吸引力。通过检查 964 次快速约会的数据，我们发现 ChatGPT（和 Claude 3）可以预测快速约会成功的客观和主观指标（r=0.12-0.23）。ChatGPT 对实际匹配（即联系信息的交换）的预测不仅与能够访问相同信息的人类判断者相当，而且比快速约会者自己的预测更高。虽然 ChatGPT 预测中的一些差异可以通过常见的内容维度（例如对话的价数）来解释，但仍然存在相当大比例的无法解释的差异这一事实表明 ChatGPT 也能理解对话动态。此外，ChatGPT 的判断与人类观察者的判断有很大的重叠（平均 r=0.29），突显了它们对浪漫吸引力的表现的相似性，在一定程度上与准确性无关。</li>
</ul>

<h3>Title: MedBench: A Comprehensive, Standardized, and Reliable Benchmarking System for Evaluating Chinese Medical Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mianxin Liu, Jinru Ding, Jie Xu, Weiguo Hu, Xiaoyang Li, Lifeng Zhu, Zhian Bai, Xiaoming Shi, Benyou Wang, Haitao Song, Pengfei Liu, Xiaofan Zhang, Shanshan Wang, Kang Li, Haofen Wang, Tong Ruan, Xuanjing Huang, Xin Sun, Shaoting Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10990">https://arxiv.org/abs/2407.10990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10990">https://arxiv.org/pdf/2407.10990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10990]] MedBench: A Comprehensive, Standardized, and Reliable Benchmarking System for Evaluating Chinese Medical Large Language Models(https://arxiv.org/abs/2407.10990)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Ensuring the general efficacy and goodness for human beings from medical large language models (LLM) before real-world deployment is crucial. However, a widely accepted and accessible evaluation process for medical LLM, especially in the Chinese context, remains to be established. In this work, we introduce "MedBench", a comprehensive, standardized, and reliable benchmarking system for Chinese medical LLM. First, MedBench assembles the currently largest evaluation dataset (300,901 questions) to cover 43 clinical specialties and performs multi-facet evaluation on medical LLM. Second, MedBench provides a standardized and fully automatic cloud-based evaluation infrastructure, with physical separations for question and ground truth. Third, MedBench implements dynamic evaluation mechanisms to prevent shortcut learning and answer remembering. Applying MedBench to popular general and medical LLMs, we observe unbiased, reproducible evaluation results largely aligning with medical professionals' perspectives. This study establishes a significant foundation for preparing the practical applications of Chinese medical LLMs. MedBench is publicly accessible at this https URL.</li>
<li><strong>摘要：</strong>在实际部署之前，确保医学大型语言模型 (LLM) 对人类的普遍有效性和良好性至关重要。然而，医学 LLM 的广泛接受和可访问的评估流程，尤其是在中国背景下，仍有待建立。在这项工作中，我们引入了“MedBench”，这是一个全面、标准化和可靠的中国医学 LLM 基准测试系统。首先，MedBench 汇集了目前最大的评估数据集（300,901 个问题），涵盖 43 个临床专业，并对医学 LLM 进行多方面评估。其次，MedBench 提供了一个标准化、全自动的基于云的评估基础设施，将问题和基本事实物理分离。第三，MedBench 实施动态评估机制，以防止捷径学习和答案记忆。将 MedBench 应用于流行的通用和医学 LLM，我们观察到无偏见、可重复的评估结果，与医学专业人士的观点基本一致。这项研究为准备中国医学 LLM 的实际应用奠定了重要基础。MedBench 可通过此 https URL 公开访问。</li>
</ul>

<h3>Title: Classification of Geological Borehole Descriptions Using a Domain Adapted Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Hossein Ghorbanfekr, Pieter Jan Kerstens, Katrijn Dirix</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10991">https://arxiv.org/abs/2407.10991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10991">https://arxiv.org/pdf/2407.10991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10991]] Classification of Geological Borehole Descriptions Using a Domain Adapted Large Language Model(https://arxiv.org/abs/2407.10991)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Geological borehole descriptions contain detailed textual information about the composition of the subsurface. However, their unstructured format presents significant challenges for extracting relevant features into a structured format. This paper introduces GEOBERTje: a domain adapted large language model trained on geological borehole descriptions from Flanders (Belgium) in the Dutch language. This model effectively extracts relevant information from the borehole descriptions and represents it into a numeric vector space. Showcasing just one potential application of GEOBERTje, we finetune a classifier model on a limited number of manually labeled observations. This classifier categorizes borehole descriptions into a main, second and third lithology class. We show that our classifier outperforms both a rule-based approach and GPT-4 of OpenAI. This study exemplifies how domain adapted large language models enhance the efficiency and accuracy of extracting information from complex, unstructured geological descriptions. This offers new opportunities for geological analysis and modeling using vast amounts of data.</li>
<li><strong>摘要：</strong>地质钻孔描述包含有关地下成分的详细文本信息。然而，它们的非结构化格式对将相关特征提取为结构化格式提出了重大挑战。本文介绍了 GEOBERTje：一种领域适应的大型语言模型，该模型以荷兰语的佛兰德斯（比利时）地质钻孔描述为训练对象。该模型有效地从钻孔描述中提取了相关信息并将其表示为数字向量空间。仅展示 GEOBERTje 的一个潜在应用，我们在有限数量的手动标记观测上微调了分类器模型。该分类器将钻孔描述分为主要、第二和第三岩性类。我们表明，我们的分类器优于基于规则的方法和 OpenAI 的 GPT-4。这项研究举例说明了领域适应的大型语言模型如何提高从复杂的非结构化地质描述中提取信息的效率和准确性。这为使用大量数据进行地质分析和建模提供了新的机会。</li>
</ul>

<h3>Title: The Effects of Embodiment and Personality Expression on Learning in LLM-based Educational Agents</h3>
<ul>
<li><strong>Authors: </strong>Sinan Sonlu, Bennie Bendiksen, Funda Durupinar, Uğur Güdükbay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10993">https://arxiv.org/abs/2407.10993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10993">https://arxiv.org/pdf/2407.10993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10993]] The Effects of Embodiment and Personality Expression on Learning in LLM-based Educational Agents(https://arxiv.org/abs/2407.10993)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>This work investigates how personality expression and embodiment affect personality perception and learning in educational conversational agents. We extend an existing personality-driven conversational agent framework by integrating LLM-based conversation support tailored to an educational application. We describe a user study built on this system to evaluate two distinct personality styles: high extroversion and agreeableness and low extroversion and agreeableness. For each personality style, we assess three models: (1) a dialogue-only model that conveys personality through dialogue, (2) an animated human model that expresses personality solely through dialogue, and (3) an animated human model that expresses personality through both dialogue and body and facial animations. The results indicate that all models are positively perceived regarding both personality and learning outcomes. Models with high personality traits are perceived as more engaging than those with low personality traits. We provide a comprehensive quantitative and qualitative analysis of perceived personality traits, learning parameters, and user experiences based on participant ratings of the model types and personality styles, as well as users' responses to open-ended questions.</li>
<li><strong>摘要：</strong>这项研究调查了个性表达和体现如何影响教育对话代理中的个性感知和学习。我们通过集成针对教育应用量身定制的基于 LLM 的对话支持来扩展现有的个性驱动对话代理框架。我们描述了基于此系统的用户研究，以评估两种不同的个性风格：高外向性和亲和性以及低外向性和亲和性。对于每种个性风格，我们评估三种模型：（1）通过对话传达个性的对话模型，（2）仅通过对话表达个性的动画人体模型，以及（3）通过对话和身体和面部动画表达个性的动画人体模型。结果表明，所有模型在个性和学习成果方面都得到了积极的评价。具有高个性特征的模型被认为比具有低个性特征的模型更具吸引力。我们根据参与者对模型类型和个性风格的评分以及用户对开放式问题的回答，对感知的个性特征、学习参数和用户体验进行了全面的定量和定性分析。</li>
</ul>

<h3>Title: Panza: A Personalized Text Writing Assistant via Data Playback and Local Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Armand Nicolicioiu, Eugenia Iofinova, Eldar Kurtic, Mahdi Nikdan, Andrei Panferov, Ilia Markov, Nir Shavit, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10994">https://arxiv.org/abs/2407.10994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10994">https://arxiv.org/pdf/2407.10994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10994]] Panza: A Personalized Text Writing Assistant via Data Playback and Local Fine-Tuning(https://arxiv.org/abs/2407.10994)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The availability of powerful open-source large language models (LLMs) opens exciting use-cases, such as automated personal assistants that adapt to the user's unique data and demands. Two key desiderata for such assistants are personalization-in the sense that the assistant should reflect the user's own style-and privacy-in the sense that users may prefer to always store their personal data locally, on their own computing device. We present a new design for such an automated assistant, for the specific use case of personal assistant for email generation, which we call Panza. Specifically, Panza can be both trained and inferenced locally on commodity hardware, and is personalized to the user's writing style. Panza's personalization features are based on a new technique called data playback, which allows us to fine-tune an LLM to better reflect a user's writing style using limited data. We show that, by combining efficient fine-tuning and inference methods, Panza can be executed entirely locally using limited resources-specifically, it can be executed within the same resources as a free Google Colab instance. Finally, our key methodological contribution is a careful study of evaluation metrics, and of how different choices of system components (e.g. the use of Retrieval-Augmented Generation or different fine-tuning approaches) impact the system's performance.</li>
<li><strong>摘要：</strong>强大的开源大型语言模型 (LLM) 的出现开启了令人兴奋的用例，例如适应用户独特数据和需求的自动化个人助理。此类助理的两个关键需求是个性化（即助理应反映用户自己的风格）和隐私（即用户可能更喜欢始终将个人数据本地存储在自己的计算设备上）。我们针对电子邮件生成个人助理的特定用例，为此类自动化助理提出了一种新设计，我们称之为 Panza。具体而言，Panza 可以在商用硬件上进行本地训练和推理，并根据用户的写作风格进行个性化设置。Panza 的个性化功能基于一种称为数据回放的新技术，该技术允许我们使用有限的数据对 LLM 进行微调，以更好地反映用户的写作风格。我们表明，通过结合有效的微调和推理方法，Panza 可以使用有限的资源完全在本地执行 - 具体而言，它可以在与免费 Google Colab 实例相同的资源内执行。最后，我们的主要方法贡献是仔细研究评估指标，以及系统组件的不同选择（例如，使用检索增强生成或不同的微调方法）如何影响系统性能。</li>
</ul>

<h3>Title: LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content</h3>
<ul>
<li><strong>Authors: </strong>Jessica Foo, Shaun Khoo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10995">https://arxiv.org/abs/2407.10995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10995">https://arxiv.org/pdf/2407.10995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10995]] LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content(https://arxiv.org/abs/2407.10995)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly prevalent in a wide variety of applications, concerns about the safety of their outputs have become more significant. Most efforts at safety-tuning or moderation today take on a predominantly Western-centric view of safety, especially for toxic, hateful, or violent speech. In this paper, we describe LionGuard, a Singapore-contextualized moderation classifier that can serve as guardrails against unsafe LLM outputs. When assessed on Singlish data, LionGuard outperforms existing widely-used moderation APIs, which are not finetuned for the Singapore context, by 14% (binary) and up to 51% (multi-label). Our work highlights the benefits of localization for moderation classifiers and presents a practical and scalable approach for low-resource languages.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在各种应用中越来越普遍，对其输出的安全性的担忧也变得越来越重要。当今，大多数安全调整或审核工作都主要以西方为中心的安全观点进行，尤其是针对有毒、仇恨或暴力言论。在本文中，我们介绍了 LionGuard，这是一种新加坡语境化的审核分类器，可以作为不安全 LLM 输出的护栏。在对新加坡英语数据进行评估时，LionGuard 的表现优于现有的广泛使用的审核 API（未针对新加坡语境进行微调），高出 14%（二进制）和高达 51%（多标签）。我们的工作强调了本地化对审核分类器的好处，并提出了一种适用于低资源语言的实用且可扩展的方法。</li>
</ul>

<h3>Title: Visualization Literacy of Multimodal Large Language Models: A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Zhimin Li, Haichao Miao, Valerio Pascucci, Shusen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10996">https://arxiv.org/abs/2407.10996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10996">https://arxiv.org/pdf/2407.10996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10996]] Visualization Literacy of Multimodal Large Language Models: A Comparative Study(https://arxiv.org/abs/2407.10996)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The recent introduction of multimodal large language models (MLLMs) combine the inherent power of large language models (LLMs) with the renewed capabilities to reason about the multimodal context. The potential usage scenarios for MLLMs significantly outpace their text-only counterparts. Many recent works in visualization have demonstrated MLLMs' capability to understand and interpret visualization results and explain the content of the visualization to users in natural language. In the machine learning community, the general vision capabilities of MLLMs have been evaluated and tested through various visual understanding benchmarks. However, the ability of MLLMs to accomplish specific visualization tasks based on visual perception has not been properly explored and evaluated, particularly, from a visualization-centric perspective. In this work, we aim to fill the gap by utilizing the concept of visualization literacy to evaluate MLLMs. We assess MLLMs' performance over two popular visualization literacy evaluation datasets (VLAT and mini-VLAT). Under the framework of visualization literacy, we develop a general setup to compare different multimodal large language models (e.g., GPT4-o, Claude 3 Opus, Gemini 1.5 Pro) as well as against existing human baselines. Our study demonstrates MLLMs' competitive performance in visualization literacy, where they outperform humans in certain tasks such as identifying correlations, clusters, and hierarchical structures.</li>
<li><strong>摘要：</strong>最近推出的多模态大型语言模型 (MLLM) 将大型语言模型 (LLM) 的固有功能与推理多模态上下文的新能力相结合。MLLM 的潜在使用场景远远超过纯文本模型。可视化领域的许多最新研究都证明了 MLLM 能够理解和解释可视化结果，并以自然语言向用户解释可视化内容。在机器学习社区中，MLLM 的一般视觉能力已经通过各种视觉理解基准进行了评估和测试。然而，MLLM 基于视觉感知完成特定可视化任务的能力尚未得到适当的探索和评估，特别是从以可视化为中心的角度。在这项工作中，我们旨在通过利用可视化素养的概念来评估 MLLM，以填补这一空白。我们在两个流行的可视化素养评估数据集 (VLAT 和 mini-VLAT) 上评估了 MLLM 的性能。在可视化素养框架下，我们开发了一个通用设置来比较不同的多模态大型语言模型（例如 GPT4-o、Claude 3 Opus、Gemini 1.5 Pro）以及现有的人类基线。我们的研究证明了 MLLM 在可视化素养方面的竞争性能，它们在某些任务（例如识别相关性、聚类和层次结构）上的表现优于人类。</li>
</ul>

<h3>Title: Discrete Diffusion Language Model for Long Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Do Huu Dat, Do Duc Anh, Anh Tuan Luu, Wray Buntine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10998">https://arxiv.org/abs/2407.10998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10998">https://arxiv.org/pdf/2407.10998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10998]] Discrete Diffusion Language Model for Long Text Summarization(https://arxiv.org/abs/2407.10998)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While diffusion models excel at conditional generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation. In this work, we address the limitations of prior discrete diffusion models for conditional long-text generation, particularly in long sequence-to-sequence tasks such as abstractive summarization. Despite fast decoding speeds compared to autoregressive methods, previous diffusion models failed on the abstractive summarization task due to the incompatibility between the backbone architectures and the random noising process. To overcome these challenges, we introduce a novel semantic-aware noising process that enables Transformer backbones to handle long sequences effectively. Additionally, we propose CrossMamba, an adaptation of the Mamba model to the encoder-decoder paradigm, which integrates seamlessly with the random absorbing noising process. Our approaches achieve state-of-the-art performance on three benchmark summarization datasets: Gigaword, CNN/DailyMail, and Arxiv, outperforming existing discrete diffusion models on ROUGE metrics as well as possessing much faster speed in inference compared to autoregressive models.</li>
<li><strong>摘要：</strong>虽然扩散模型擅长条件生成高质量图像，但离散扩散模型的先前研究并未在条件长文本生成方面进行评估。在这项工作中，我们解决了先前离散扩散模型在条件长文本生成方面的局限性，特别是在抽象摘要等长序列到序列任务中。尽管与自回归方法相比，以前的扩散模型具有快速的解码速度，但由于主干架构和随机噪声过程之间的不兼容性，它们在抽象摘要任务上失败了。为了克服这些挑战，我们引入了一种新颖的语义感知噪声过程，使 Transformer 主干能够有效地处理长序列。此外，我们提出了 CrossMamba，这是 Mamba 模型对编码器-解码器范式的改编，它与随机吸收噪声过程无缝集成。我们的方法在三个基准摘要数据集上实现了最先进的性能：Gigaword、CNN/DailyMail 和 Arxiv，在 ROUGE 指标上优于现有的离散扩散模型，并且与自回归模型相比具有更快的推理速度。</li>
</ul>

<h3>Title: TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot</h3>
<ul>
<li><strong>Authors: </strong>Kaiqi Zhang, Shuai Yuan, Honghan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.10999">https://arxiv.org/abs/2407.10999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.10999">https://arxiv.org/pdf/2407.10999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.10999]] TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot(https://arxiv.org/abs/2407.10999)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the rapid development of large language models (LLM), the evaluation of LLM becomes increasingly important. Measuring text generation tasks such as summarization and article creation is very difficult. Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult. So far, the evaluation of LLM in business scenarios has mainly relied on manual, which is expensive and time-consuming. In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria. In addition, we try combining zero-shot and few-shot to make the judge model focus on more information. We also propose a prompt paradigm and an engineering approach to adjust and iterate the shots ,helping judge model to better understand the complex criteria. We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a correlation of over 80% with human judgments, outperforming even the inter-human correlation in some tasks. The code is released in this https URL</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的快速发展，LLM 的评估变得越来越重要。衡量摘要和文章创作等文本生成任务非常困难。特别是在特定应用领域（例如，面向企业或面向客户服务），内部评估标准不仅要满足通用标准（正确性、帮助性和创造性等），还要同时满足客户的特定需求和业务安全要求，这使评估变得更加困难。到目前为止，业务场景中对 LLM 的评估主要依赖于人工，这既昂贵又耗时。在本文中，我们提出了一种基于模型的评估方法：TALEC，它允许用户灵活地设置自己的评估标准，并使用上下文学习（ICL）来教会判断模型这些内部标准。此外，我们尝试结合零样本和少样本，使判断模型关注更多信息。我们还提出了一个提示范式和一种工程方法来调整和迭代镜头，帮助判断模型更好地理解复杂的标准。然后，我们将微调与 ICL 进行比较，发现微调可以被 ICL 取代。TALEC 表现出强大的能力来准确反映人类的偏好，并与人类的判断实现了 80% 以上的相关性，甚至在某些任务中超过了人与人之间的相关性。代码发布在此 https URL 中</li>
</ul>

<h3>Title: Autonomous Prompt Engineering in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daan Kepel, Konstantina Valogianni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11000">https://arxiv.org/abs/2407.11000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11000">https://arxiv.org/pdf/2407.11000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11000]] Autonomous Prompt Engineering in Large Language Models(https://arxiv.org/abs/2407.11000)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompt engineering is a crucial yet challenging task for optimizing the performance of large language models (LLMs) on customized tasks. This pioneering research introduces the Automatic Prompt Engineering Toolbox (APET), which enables GPT-4 to autonomously apply prompt engineering techniques. By leveraging sophisticated strategies such as Expert Prompting, Chain of Thought, and Tree of Thoughts, APET empowers GPT-4 to dynamically optimize prompts, resulting in substantial improvements in tasks like Word Sorting (4.4% increase) and Geometric Shapes (6.8% increase). Despite encountering challenges in complex tasks such as Checkmate in One (-14.8%), these findings demonstrate the transformative potential of APET in automating complex prompt optimization processes without the use of external data. Overall, this research represents a significant leap in AI development, presenting a robust framework for future innovations in autonomous AI systems and highlighting the ability of GPT-4 to bring prompt engineering theory to practice. It establishes a foundation for enhancing performance in complex task performance and broadening the practical applications of these techniques in real-world scenarios.</li>
<li><strong>摘要：</strong>提示工程是一项至关重要但又极具挑战性的任务，可用于优化大型语言模型 (LLM) 在定制任务上的性能。这项开创性的研究引入了自动提示工程工具箱 (APET)，它使 GPT-4 能够自主应用提示工程技术。通过利用专家提示、思维链和思维树等复杂策略，APET 使 GPT-4 能够动态优化提示，从而显著改善单词排序（增加 4.4%）和几何形状（增加 6.8%）等任务。尽管在诸如 Checkmate in One（-14.8%）等复杂任务中遇到了挑战，但这些发现证明了 APET 在无需使用外部数据的情况下自动化复杂提示优化过程的变革潜力。总体而言，这项研究代表了人工智能发展的重大飞跃，为自主人工智能系统的未来创新提供了一个强大的框架，并凸显了 GPT-4 将提示工程理论付诸实践的能力。它为提高复杂任务执行性能和扩大这些技术在现实场景中的实际应用奠定了基础。</li>
</ul>

<h3>Title: Generative AI Systems: A Systems-based Perspective on Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Jakub M. Tomczak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11001">https://arxiv.org/abs/2407.11001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11001">https://arxiv.org/pdf/2407.11001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11001]] Generative AI Systems: A Systems-based Perspective on Generative AI(https://arxiv.org/abs/2407.11001)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized AI systems by enabling communication with machines using natural language. Recent developments in Generative AI (GenAI) like Vision-Language Models (GPT-4V) and Gemini have shown great promise in using LLMs as multimodal systems. This new research line results in building Generative AI systems, GenAISys for short, that are capable of multimodal processing and content creation, as well as decision-making. GenAISys use natural language as a communication means and modality encoders as I/O interfaces for processing various data sources. They are also equipped with databases and external specialized tools, communicating with the system through a module for information retrieval and storage. This paper aims to explore and state new research directions in Generative AI Systems, including how to design GenAISys (compositionality, reliability, verifiability), build and train them, and what can be learned from the system-based perspective. Cross-disciplinary approaches are needed to answer open questions about the inner workings of GenAI systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过使用自然语言实现与机器的通信，彻底改变了 AI 系统。生成式 AI (GenAI) 的最新发展，如视觉语言模型 (GPT-4V) 和 Gemini，已显示出将 LLM 用作多模态系统的巨大前景。这条新的研究路线的成果是构建生成式 AI 系统（简称 GenAISys），该系统能够进行多模态处理和内容创建以及决策。GenAISys 使用自然语言作为通信手段，使用模态编码器作为处理各种数据源的 I/O 接口。它们还配备了数据库和外部专用工具，通过信息检索和存储模块与系统通信。本文旨在探索和陈述生成式 AI 系统的新研究方向，包括如何设计 GenAISys（组合性、可靠性、可验证性）、构建和训练它们，以及从基于系统的角度可以学到什么。需要采用跨学科方法来回答有关 GenAI 系统内部工作原理的未解决的问题。</li>
</ul>

<h3>Title: MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias</h3>
<ul>
<li><strong>Authors: </strong>Guorun Wang, Lucia Specia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11002">https://arxiv.org/abs/2407.11002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11002">https://arxiv.org/pdf/2407.11002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11002]] MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias(https://arxiv.org/abs/2407.11002)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Text-to-image models are known to propagate social biases. For example when prompted to generate images of people in certain professions, these models tend to systematically generate specific genders or ethnicity. In this paper, we show that this bias is already present in the text encoder of the model and introduce a Mixture-of-Experts approach by identifying text-encoded bias in the latent space and then creating a bias-identification gate. More specifically, we propose MoESD (Mixture of Experts Stable Diffusion) with BiAs (Bias Adapters) to mitigate gender bias. We also demonstrate that a special token is essential during the mitigation process. With experiments focusing on gender bias, we demonstrate that our approach successfully mitigates gender bias while maintaining image quality.</li>
<li><strong>摘要：</strong>众所周知，文本转图像模型会传播社会偏见。例如，当被提示生成某些职业的人的图像时，这些模型往往会系统地生成特定的性别或种族。在本文中，我们表明这种偏见已经存在于模型的文本编码器中，并通过识别潜在空间中的文本编码偏见，然后创建偏见识别门，引入了混合专家方法。更具体地说，我们提出了 MoESD（专家稳定扩散混合）和 BiAs（偏见适配器）来缓解性别偏见。我们还证明了在缓解过程中，特殊令牌是必不可少的。通过专注于性别偏见的实验，我们证明了我们的方法成功地缓解了性别偏见，同时保持了图像质量。</li>
</ul>

<h3>Title: Using Large Language Models in Public Transit Systems, San Antonio as a case study</h3>
<ul>
<li><strong>Authors: </strong>Ramya Jonnala, Gongbo Liang, Jeong Yang, Izzat Alsmadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11003">https://arxiv.org/abs/2407.11003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11003">https://arxiv.org/pdf/2407.11003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11003]] Using Large Language Models in Public Transit Systems, San Antonio as a case study(https://arxiv.org/abs/2407.11003)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The integration of large language models into public transit systems represents a significant advancement in urban transportation management and passenger experience. This study examines the impact of LLMs within San Antonio's public transit system, leveraging their capabilities in natural language processing, data analysis, and real time communication. By utilizing GTFS and other public transportation information, the research highlights the transformative potential of LLMs in enhancing route planning, reducing wait times, and providing personalized travel assistance. Our case study is the city of San Antonio as part of a project aiming to demonstrate how LLMs can optimize resource allocation, improve passenger satisfaction, and support decision making processes in transit management. We evaluated LLM responses to questions related to both information retrieval and also understanding. Ultimately, we believe that the adoption of LLMs in public transit systems can lead to more efficient, responsive, and user-friendly transportation networks, providing a model for other cities to follow.</li>
<li><strong>摘要：</strong>大型语言模型与公共交通系统的整合代表着城市交通管理和乘客体验的重大进步。本研究考察了 LLM 在圣安东尼奥公共交通系统中的影响，利用了其在自然语言处理、数据分析和实时通信方面的能力。通过利用 GTFS 和其他公共交通信息，该研究突出了 LLM 在增强路线规划、减少等待时间和提供个性化旅行帮助方面的变革潜力。我们的案例研究是圣安东尼奥市，该项目旨在展示 LLM 如何优化资源配置、提高乘客满意度并支持交通管理中的决策过程。我们评估了 LLM 对与信息检索和理解相关的问题的回答。最终，我们相信在公共交通系统中采用 LLM 可以带来更高效、响应更快、更用户友好的交通网络，为其他城市提供效仿的典范。</li>
</ul>

<h3>Title: The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Heng Huang, Catherine Cao, Vaishnavi Bhargava, Frederic Sala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11004">https://arxiv.org/abs/2407.11004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11004">https://arxiv.org/pdf/2407.11004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11004]] The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators(https://arxiv.org/abs/2407.11004)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large pretrained models can be used as annotators, helping replace or augment crowdworkers and enabling distilling generalist models into smaller specialist models. Unfortunately, this comes at a cost: employing top-of-the-line models often requires paying thousands of dollars for API calls, while the resulting datasets are static and challenging to audit. To address these challenges, we propose a simple alternative: rather than directly querying labels from pretrained models, we task models to generate programs that can produce labels. These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less. Our system, Alchemist, obtains comparable to or better performance than large language model-based annotation in a range of tasks for a fraction of the cost: on average, improvements amount to a 12.9% enhancement while the total labeling costs across all datasets are reduced by a factor of approximately 500x.</li>
<li><strong>摘要：</strong>大型预训练模型可用作注释器，帮助替代或增强众包工作者，并将通用模型提炼为较小的专业模型。不幸的是，这是有代价的：使用顶级模型通常需要为 API 调用支付数千美元，而生成的数据集是静态的，难以审核。为了应对这些挑战，我们提出了一个简单的替代方案：我们不是直接从预训练模型中查询标签，而是让模型生成可以生成标签的程序。这些程序可以在本地存储和应用、重复使用和扩展，而且成本要低几个数量级。我们的系统 Alchemist 在一系列任务中获得与大型语言模型注释相当或更好的性能，而成本仅为其一小部分：平均而言，改进量提高了 12.9%，而所有数据集的总标记成本降低了约 500 倍。</li>
</ul>

<h3>Title: RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Robert Friel, Masha Belyi, Atindriyo Sanyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11005">https://arxiv.org/abs/2407.11005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11005">https://arxiv.org/pdf/2407.11005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11005]] RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2407.11005)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at this https URL. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned RoBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 已成为将特定领域知识整合到由大型语言模型 (LLM) 支持的面向用户的聊天应用程序的标准架构模式。RAG 系统的特点是 (1) 文档检索器查询特定领域语料库以获取与输入查询相关的上下文信息，以及 (2) LLM 根据提供的查询和上下文生成响应。然而，由于缺乏统一的评估标准和带注释的数据集，对 RAG 系统的全面评估仍然是一个挑战。为此，我们推出了 RAGBench：第一个包含 100k 个示例的全面、大规模 RAG 基准数据集。它涵盖五个独特的行业特定领域和各种 RAG 任务类型。RAGBench 示例来自行业语料库（例如用户手册），因此它特别适用于行业应用。此外，我们形式化了 TRACe 评估框架：一组可解释且可操作的 RAG 评估指标，适用于所有 RAG 领域。我们在此 https URL 上发布带标签的数据集。 RAGBench 可解释标签有助于对 RAG 系统进行整体评估，从而提供可操作的反馈，以持续改进生产应用程序。通过广泛的基准测试，我们发现基于 LLM 的 RAG 评估方法在 RAG 评估任务上难以与经过微调的 RoBERTa 模型竞争。我们确定了现有方法的不足之处，并建议采用 RAGBench 和 TRACe 来推进 RAG 评估系统的状态。</li>
</ul>

<h3>Title: How Good Is It? Evaluating the Efficacy of Common versus Domain-Specific Prompts on Foundational Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Oluyemi Enoch Amujo, Shanchieh Jay Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11006">https://arxiv.org/abs/2407.11006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11006">https://arxiv.org/pdf/2407.11006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11006]] How Good Is It? Evaluating the Efficacy of Common versus Domain-Specific Prompts on Foundational Large Language Models(https://arxiv.org/abs/2407.11006)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have expanded into various domains. However, there remains a need to evaluate how these models perform when prompted with commonplace queries compared to domain-specific queries, which may be useful for benchmarking prior to fine-tuning domain-specific downstream tasks. This study evaluates LLMs, specifically Gemma-2B and Gemma-7B, across diverse domains, including cybersecurity, medicine, and finance, compared to common knowledge queries. This study employs a comprehensive methodology to evaluate foundational models, encompassing problem formulation, data analysis, and the development of novel outlier detection techniques. This methodological rigor enhances the credibility of the presented evaluation frameworks. This study focused on assessing inference time, response length, throughput, quality, and resource utilization and investigated the correlations between these factors. The results indicate that model size and types of prompts used for inference significantly influenced response length and quality. In addition, common prompts, which include various types of queries, generate diverse and inconsistent responses at irregular intervals. In contrast, domain-specific prompts consistently generate concise responses within a reasonable time. Overall, this study underscores the need for comprehensive evaluation frameworks to enhance the reliability of benchmarking procedures in multidomain AI research.</li>
<li><strong>摘要：</strong>最近，大型语言模型 (LLM) 已扩展到各个领域。然而，仍然需要评估这些模型在使用常见查询与特定领域查询提示时的表现，这可能有助于在微调特定领域的下游任务之前进行基准测试。本研究评估了不同领域的 LLM，特别是 Gemma-2B 和 Gemma-7B，并与常见知识查询进行了比较。本研究采用全面的方法来评估基础模型，包括问题制定、数据分析和新型异常值检测技术的开发。这种方法的严谨性提高了所提出的评估框架的可信度。本研究重点评估推理时间、响应长度、吞吐量、质量和资源利用率，并研究了这些因素之间的相关性。结果表明，模型大小和用于推理的提示类型显著影响响应长度和质量。此外，包括各种类型查询的常见提示会不定期生成多样化且不一致的响应。相比之下，特定领域的提示始终会在合理的时间内产生简洁的响应。总体而言，这项研究强调了需要全面的评估框架来提高多领域 AI 研究的基准测试程序的可靠性。</li>
</ul>

<h3>Title: Panacea: A foundation model for clinical trial search, summarization, design, and recruitment</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Lin, Hanwen Xu, Zifeng Wang, Sheng Wang, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11007">https://arxiv.org/abs/2407.11007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11007">https://arxiv.org/pdf/2407.11007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11007]] Panacea: A foundation model for clinical trial search, summarization, design, and recruitment(https://arxiv.org/abs/2407.11007)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Clinical trials are fundamental in developing new drugs, medical devices, and treatments. However, they are often time-consuming and have low success rates. Although there have been initial attempts to create large language models (LLMs) for clinical trial design and patient-trial matching, these models remain task-specific and not adaptable to diverse clinical trial tasks. To address this challenge, we propose a clinical trial foundation model named Panacea, designed to handle multiple tasks, including trial search, trial summarization, trial design, and patient-trial matching. We also assemble a large-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207 trial-related scientific papers, to infuse clinical knowledge into the model by pre-training. We further curate TrialInstruct, which has 200,866 of instruction data for fine-tuning. These resources enable Panacea to be widely applicable for a range of clinical trial tasks based on user requirements. We evaluated Panacea on a new benchmark, named TrialPanorama, which covers eight clinical trial tasks. Our method performed the best on seven of the eight tasks compared to six cutting-edge generic or medicine-specific LLMs. Specifically, Panacea showed great potential to collaborate with human experts in crafting the design of eligibility criteria, study arms, and outcome measures, in multi-round conversations. In addition, Panacea achieved 14.42% improvement in patient-trial matching, 41.78% to 52.02% improvement in trial search, and consistently ranked at the top for five aspects of trial summarization. Our approach demonstrates the effectiveness of Panacea in clinical trials and establishes a comprehensive resource, including training data, model, and benchmark, for developing clinical trial foundation models, paving the path for AI-based clinical trial development.</li>
<li><strong>摘要：</strong>临床试验是开发新药、医疗器械和新疗法的基础。然而，它们通常耗时长，成功率低。尽管最初有人尝试创建大型语言模型 (LLM) 用于临床试验设计和患者试验匹配，但这些模型仍然是特定于任务的，不适用于不同的临床试验任务。为了应对这一挑战，我们提出了一个名为 Panacea 的临床试验基础模型，旨在处理多项任务，包括试验搜索、试验总结、试验设计和患者试验匹配。我们还收集了一个名为 TrialAlign 的大型数据集，其中包含 793,279 份试验文件和 1,113,207 篇与试验相关的科学论文，通过预训练将临床知识注入模型。我们进一步整理了 TrialInstruct，它有 200,866 条指令数据用于微调。这些资源使 Panacea 能够根据用户需求广泛应用于一系列临床试验任务。我们根据一个名为 TrialPanorama 的新基准对 Panacea 进行了评估，该基准涵盖了八项临床试验任务。与六种尖端的通用或医学专用法学硕士相比，我们的方法在八项任务中的七项上表现最佳。具体而言，Panacea 在多轮对话中表现出与人类专家合作设计资格标准、研究分支和结果测量方面的巨大潜力。此外，Panacea 在患者试验匹配方面实现了 14.42% 的改进，在试验搜索方面实现了 41.78% 至 52.02% 的改进，并在试验总结的五个方面始终名列前茅。我们的方法证明了 Panacea 在临床试验中的有效性，并为开发临床试验基础模型建立了包括训练数据、模型和基准在内的全面资源，为基于 AI 的临床试验开发铺平了道路。</li>
</ul>

<h3>Title: Figuring out Figures: Using Textual References to Caption Scientific Figures</h3>
<ul>
<li><strong>Authors: </strong>Stanley Cao, Kevin Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11008">https://arxiv.org/abs/2407.11008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11008">https://arxiv.org/pdf/2407.11008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11008]] Figuring out Figures: Using Textual References to Caption Scientific Figures(https://arxiv.org/abs/2407.11008)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Figures are essential channels for densely communicating complex ideas in scientific papers. Previous work in automatically generating figure captions has been largely unsuccessful and has defaulted to using single-layer LSTMs, which no longer achieve state-of-the-art performance. In our work, we use the SciCap datasets curated by Hsu et al. and use a variant of a CLIP+GPT-2 encoder-decoder model with cross-attention to generate captions conditioned on the image. Furthermore, we augment our training pipeline by creating a new dataset MetaSciCap that incorporates textual metadata from the original paper relevant to the figure, such as the title, abstract, and in-text references. We use SciBERT to encode the textual metadata and use this encoding alongside the figure embedding. In our experimentation with different models, we found that the CLIP+GPT-2 model performs better when it receives all textual metadata from the SciBERT encoder in addition to the figure, but employing a SciBERT+GPT2 model that uses only the textual metadata achieved optimal performance.</li>
<li><strong>摘要：</strong>图表是科学论文中密集传达复杂思想的重要渠道。之前在自动生成图表标题方面的工作基本上没有成功，并且默认使用单层 LSTM，而这不再能达到最先进的性能。在我们的工作中，我们使用由 Hsu 等人策划的 SciCap 数据集，并使用具有交叉注意的 CLIP+GPT-2 编码器-解码器模型的变体来生成以图像为条件的标题。此外，我们通过创建一个新的数据集 MetaSciCap 来增强我们的训练流程，该数据集包含与图表相关的原始论文中的文本元数据，例如标题、摘要和文内引用。我们使用 SciBERT 对文本元数据进行编码，并将此编码与图表嵌入一起使用。在我们对不同模型的实验中，我们发现，当 CLIP+GPT-2 模型除了图形之外还从 SciBERT 编码器接收所有文本元数据时，其表现会更好，但采用仅使用文本元数据的 SciBERT+GPT2 模型可以获得最佳性能。</li>
</ul>

<h3>Title: CharED: Character-wise Ensemble Decoding for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Gu, Eva Tuecke, Dmitriy Katz, Raya Horesh, David Alvarez-Melis, Mikhail Yurochkin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11009">https://arxiv.org/abs/2407.11009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11009">https://arxiv.org/pdf/2407.11009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11009]] CharED: Character-wise Ensemble Decoding for Large Language Models(https://arxiv.org/abs/2407.11009)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable potential for problem solving, with open source models achieving increasingly impressive performance on benchmarks measuring areas from logical reasoning to mathematical ability. Ensembling models can further improve capabilities across a variety of domains. However, conventional methods of combining models at inference time such as shallow fusion necessitate a shared vocabulary and tokenization, and alternatives like fine-tuning for domain-specific performance are both time consuming and computationally expensive. We therefore present an inference-time ensembling algorithm aimed at "averaging" outputs from multiple LLMs and illustrate its improved performance across multiple domains compared to its constituent models alone. Character-wise ensemble decoding, CharED, finds the marginal distribution of each character for an individual model and performs a weighted average to generate an output, character by character. In coding, math, and toxicity benchmarks, we find our proposed model able to combine complimentary strengths of multiple LLMs, regardless of vocabulary, tokenization, or model size.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出解决问题的非凡潜力，开源模型在从逻辑推理到数学能力的基准测试中取得了越来越令人印象深刻的表现。集成模型可以进一步提高各种领域的能力。然而，在推理时组合模型的传统方法（例如浅层融合）需要共享词汇表和标记化，而微调特定领域性能等替代方案既耗时又耗费计算资源。因此，我们提出了一种推理时集成算法，旨在“平均”来自多个 LLM 的输出，并说明其与单独使用其组成模型相比在多个领域中的性能有所提高。逐字符集成解码 CharED 可找到单个模型的每个字符的边际分布，并执行加权平均以逐个字符生成输出。在编码、数学和毒性基准测试中，我们发现我们提出的模型能够结合多个 LLM 的互补优势，而不管词汇表、标记化或模型大小如何。</li>
</ul>

<h3>Title: Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit Reasoning and Precise Spatio-Temporal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Devashish Vikas Gupta, Azeez Syed Ali Ishaqui, Divya Kiran Kadiyala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11014">https://arxiv.org/abs/2407.11014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11014">https://arxiv.org/pdf/2407.11014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11014]] Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit Reasoning and Precise Spatio-Temporal Retrieval(https://arxiv.org/abs/2407.11014)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown promising results in learning and contextualizing information from different forms of data. Recent advancements in foundational models, particularly those employing self-attention mechanisms, have significantly enhanced our ability to comprehend the semantics of diverse data types. One such area that could highly benefit from multi-modality is in understanding geospatial data, which inherently has multiple modalities. However, current Natural Language Processing (NLP) mechanisms struggle to effectively address geospatial queries. Existing pre-trained LLMs are inadequately equipped to meet the unique demands of geospatial data, lacking the ability to retrieve precise spatio-temporal data in real-time, thus leading to significantly reduced accuracy in answering complex geospatial queries. To address these limitations, we introduce Geode--a pioneering system designed to tackle zero-shot geospatial question-answering tasks with high precision using spatio-temporal data retrieval. Our approach represents a significant improvement in addressing the limitations of current LLM models, demonstrating remarkable improvement in geospatial question-answering abilities compared to existing state-of-the-art pre-trained models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在从不同形式的数据中学习和情境化信息方面表现出了良好的效果。基础模型的最新进展，尤其是那些采用自注意力机制的模型，大大增强了我们理解不同数据类型语义的能力。理解地理空间数据就是一个可以从多模态中受益匪浅的领域，地理空间数据本身就具有多种模态。然而，当前的自然语言处理 (NLP) 机制难以有效解决地理空间查询。现有的预训练 LLM 不足以满足地理空间数据的独特需求，缺乏实时检索精确时空数据的能力，从而导致回答复杂地理空间查询的准确性显著降低。为了解决这些限制，我们引入了 Geode——一种开创性的系统，旨在使用时空数据检索以高精度解决零样本地理空间问答任务。我们的方法在解决当前 LLM 模型的局限性方面取得了显著的进步，与现有的最先进的预训练模型相比，地理空间问答能力有了显着的提高。</li>
</ul>

<h3>Title: Does ChatGPT Have a Mind?</h3>
<ul>
<li><strong>Authors: </strong>Simon Goldstein, Benjamin A. Levinstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11015">https://arxiv.org/abs/2407.11015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11015">https://arxiv.org/pdf/2407.11015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11015]] Does ChatGPT Have a Mind?(https://arxiv.org/abs/2407.11015)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This paper examines the question of whether Large Language Models (LLMs) like ChatGPT possess minds, focusing specifically on whether they have a genuine folk psychology encompassing beliefs, desires, and intentions. We approach this question by investigating two key aspects: internal representations and dispositions to act. First, we survey various philosophical theories of representation, including informational, causal, structural, and teleosemantic accounts, arguing that LLMs satisfy key conditions proposed by each. We draw on recent interpretability research in machine learning to support these claims. Second, we explore whether LLMs exhibit robust dispositions to perform actions, a necessary component of folk psychology. We consider two prominent philosophical traditions, interpretationism and representationalism, to assess LLM action dispositions. While we find evidence suggesting LLMs may satisfy some criteria for having a mind, particularly in game-theoretic environments, we conclude that the data remains inconclusive. Additionally, we reply to several skeptical challenges to LLM folk psychology, including issues of sensory grounding, the "stochastic parrots" argument, and concerns about memorization. Our paper has three main upshots. First, LLMs do have robust internal representations. Second, there is an open question to answer about whether LLMs have robust action dispositions. Third, existing skeptical challenges to LLM representation do not survive philosophical scrutiny.</li>
<li><strong>摘要：</strong>本文探讨了像 ChatGPT 这样的大型语言模型 (LLM) 是否具有思想的问题，特别关注它们是否具有真正的民间心理学，包括信仰、欲望和意图。我们通过研究两个关键方面来解决这个问题：内部表征和行动倾向。首先，我们调查了各种表征哲学理论，包括信息、因果、结构和目的语义理论，认为 LLM 满足每种理论提出的关键条件。我们借鉴了机器学习中最近的可解释性研究来支持这些说法。其次，我们探讨 LLM 是否表现出执行行动的强大倾向，这是民间心理学的必要组成部分。我们考虑了两种著名的哲学传统，即解释主义和表征主义，以评估 LLM 的行动倾向。虽然我们发现有证据表明 LLM 可能满足拥有思想的一些标准，特别是在博弈论环境中，但我们得出的结论是数据仍然不确定。此外，我们回应了对 LLM 民间心理学的几个怀疑论挑战，包括感觉基础问题、“随机鹦鹉”论证和对记忆的担忧。我们的论文有三个主要结论。首先，LLM 确实具有强大的内部表征。其次，关于 LLM 是否具有强大的行动倾向，有一个悬而未决的问题需要回答。第三，现有的对 LLM 表征的怀疑论挑战经不起哲学的审视。</li>
</ul>

<h3>Title: LongLaMP: A Benchmark for Personalized Long-form Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A. Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim Lipka, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11016">https://arxiv.org/abs/2407.11016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11016">https://arxiv.org/pdf/2407.11016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11016]] LongLaMP: A Benchmark for Personalized Long-form Text Generation(https://arxiv.org/abs/2407.11016)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Long-text generation is seemingly ubiquitous in real-world applications of large language models such as generating an email or writing a review. Despite the fundamental importance and prevalence of long-text generation in many practical applications, existing work on personalized generation has focused on the generation of very short text. To overcome these limitations, we study the problem of personalized long-text generation, that is, generating long-text that is personalized for a specific user while being practically useful for the vast majority of real-world applications that naturally require the generation of longer text. In this work, we demonstrate the importance of user-specific personalization for long-text generation tasks and develop the Long-text Language Model Personalization (LongLaMP) Benchmark. LongLaMP provides a comprehensive and diverse evaluation framework for personalized long-text generation. Extensive experiments on LongLaMP for zero-shot and fine-tuned language tasks demonstrate the effectiveness of the proposed benchmark and its utility for developing and evaluating techniques for personalized long-text generation across a wide variety of long-text generation tasks. The results highlight the importance of personalization across a wide variety of long-text generation tasks. Finally, we release the benchmark for others to use for this important problem.</li>
<li><strong>摘要：</strong>长文本生成在大型语言模型的实际应用中似乎无处不在，例如生成电子邮件或撰写评论。尽管长文本生成在许多实际应用中具有根本的重要性和普遍性，但现有的个性化生成工作主要集中在生成非常短的文本上。为了克服这些限制，我们研究了个性化长文本生成的问题，即生成针对特定用户个性化的长文本，同时对绝大多数自然需要生成较长文本的实际应用具有实际用途。在这项工作中，我们展示了用户特定个性化对于长文本生成任务的重要性，并开发了长文本语言模型个性化 (LongLaMP) 基准。LongLaMP 为个性化长文本生成提供了一个全面而多样的评估框架。在 LongLaMP 上针对零样本和微调语言任务进行的大量实验证明了所提出的基准的有效性及其在开发和评估各种长文本生成任务中的个性化长文本生成技术的实用性。结果强调了个性化在各种长文本生成任务中的重要性。最后，我们发布了基准，供其他人使用来解决这个重要问题。</li>
</ul>

<h3>Title: Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in Self-Improving Generation</h3>
<ul>
<li><strong>Authors: </strong>Jihyun Janice Ahn, Ryo Kamoi, Lu Cheng, Rui Zhang, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11017">https://arxiv.org/abs/2407.11017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11017">https://arxiv.org/pdf/2407.11017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11017]] Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in Self-Improving Generation(https://arxiv.org/abs/2407.11017)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Mainstream LLM research has primarily focused on enhancing their generative capabilities. However, even the most advanced LLMs experience uncertainty in their outputs, often producing varied results on different runs or when faced with minor changes in input, despite no substantial change in content. Given multiple responses from the same LLM to the same input, we advocate leveraging the LLMs' discriminative capability to reduce this generative uncertainty, aiding in identifying the correct answers. Specifically, we propose and analyze three discriminative prompts: direct, inverse, and hybrid, to explore the potential of both closed-source and open-source LLMs in self-improving their generative performance on two benchmark datasets. Our insights reveal which discriminative prompt is most promising and when to use it. To our knowledge, this is the first work to systematically analyze LLMs' discriminative capacity to address generative uncertainty.</li>
<li><strong>摘要：</strong>主流 LLM 研究主要侧重于增强其生成能力。然而，即使是最先进的 LLM 也会在输出中遇到不确定性，尽管内容没有实质性变化，但在不同的运行中或面对输入的微小变化时，往往会产生不同的结果。考虑到同一 LLM 对同一输入的多个响应，我们主张利用 LLM 的判别能力来减少这种生成不确定性，帮助识别正确答案。具体来说，我们提出并分析了三种判别提示：直接、逆向和混合，以探索闭源和开源 LLM 在两个基准数据集上自我改进其生成性能的潜力。我们的见解揭示了哪种判别提示最有前景以及何时使用它。据我们所知，这是第一项系统地分析 LLM 的判别能力以解决生成不确定性的工作。</li>
</ul>

<h3>Title: FarsInstruct: Empowering Large Language Models for Persian Instruction Understanding</h3>
<ul>
<li><strong>Authors: </strong>Hojjat Mokhtarabadi, Ziba Zamani, Abbas Maazallahi, Hossein Manshaei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11186">https://arxiv.org/abs/2407.11186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11186">https://arxiv.org/pdf/2407.11186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11186]] FarsInstruct: Empowering Large Language Models for Persian Instruction Understanding(https://arxiv.org/abs/2407.11186)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Instruction-tuned large language models, such as T0, have demonstrated remarkable capabilities in following instructions across various domains. However, their proficiency remains notably deficient in many low-resource languages. To address this challenge, we introduce FarsInstruct: a comprehensive instruction dataset designed to enhance the instruction-following ability of large language models specifically for the Persian language, a significant yet underrepresented language globally. FarsInstruct encompasses a wide range of task types and datasets, each containing a mix of straightforward to complex manual written instructions, as well as translations from Public Pool of Prompts, ensuring a rich linguistic and cultural representation. Furthermore, we introduce Co-CoLA, a framework designed to enhance the multi-task adaptability of LoRA-tuned models. Through extensive experimental analyses, our study showcases the effectiveness of FarsInstruct dataset coupled with training by Co-CoLA framework, in improving the performance of large language models within the Persian context. As of the current writing, FarsInstruct comprises more than 200 templates across 21 distinct datasets, and we intend to update it consistently, thus augmenting its applicability.</li>
<li><strong>摘要：</strong>指令调优的大型语言模型（例如 T0）已展示出在各个领域遵循指令的卓越能力。然而，它们在许多资源匮乏的语言中仍然明显缺乏熟练度。为了应对这一挑战，我们推出了 FarsInstruct：一个全面的指令数据集，旨在增强大型语言模型的指令遵循能力，特别是针对波斯语，这是一种全球重要但代表性不足的语言。FarsInstruct 涵盖了广泛的任务类型和数据集，每个数据集都包含从简单到复杂的手动书面说明，以及来自公共提示池的翻译，确保丰富的语言和文化表现。此外，我们推出了 Co-CoLA，这是一个旨在增强 LoRA 调优模型的多任务适应性的框架。通过大量的实验分析，我们的研究展示了 FarsInstruct 数据集与 Co-CoLA 框架训练相结合的有效性，可以提高波斯语环境中大型语言模型的性能。截至撰写本文时，FarsInstruct 包含 21 个不同数据集的 200 多个模板，我们打算不断更新它，从而增强其适用性。</li>
</ul>

<h3>Title: Actuation without production bias</h3>
<ul>
<li><strong>Authors: </strong>James Kirby, Morgan Sonderegger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11202">https://arxiv.org/abs/2407.11202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11202">https://arxiv.org/pdf/2407.11202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11202]] Actuation without production bias(https://arxiv.org/abs/2407.11202)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Phonetic production bias is the external force most commonly invoked in computational models of sound change, despite the fact that it is not responsible for all, or even most, sound changes. Furthermore, the existence of production bias alone cannot account for how changes do or do not propagate throughout a speech community. While many other factors have been invoked by (socio)phoneticians, including but not limited to contact (between subpopulations) and differences in social evaluation (of variants, groups, or individuals), these are not typically modeled in computational simulations of sound change. In this paper, we consider whether production biases have a unique dynamics in terms of how they impact the population-level spread of change in a setting where agents learn from multiple teachers. We show that, while the dynamics conditioned by production bias are not unique, it is not the case that all perturbing forces have the same dynamics: in particular, if social weight is a function of individual teachers and the correlation between a teacher's social weight and the extent to which they realize a production bias is weak, change is unlikely to propagate. Nevertheless, it remains the case that changes initiated from different sources may display a similar dynamics. A more nuanced understanding of how population structure interacts with individual biases can thus provide a (partial) solution to the `non-phonologization problem'.</li>
<li><strong>摘要：</strong>语音生成偏差是语音变化计算模型中最常被提及的外部力量，尽管它并不是造成所有甚至大多数语音变化的原因。此外，仅凭生成偏差的存在无法解释变化如何在整个语音社区中传播或不传播。虽然（社会）语音学家已经提出了许多其他因素，包括但不限于接触（亚群之间）和社会评价差异（变体、群体或个人），但这些因素通常不会在语音变化的计算模拟中建模。在本文中，我们考虑了在代理从多位教师那里学习的环境中，生成偏差在如何影响变化在人群层面的传播方面是否具有独特的动态。我们表明，虽然由生成偏差决定的动态并不独特，但并非所有扰动力量都具有相同的动态：特别是，如果社会权重是个别教师的函数，并且教师的社会权重与他们意识到生成偏差的程度之间的相关性很弱，则变化不太可能传播。尽管如此，不同来源引发的变化可能表现出类似的动态。因此，更细致地了解人口结构如何与个人偏见相互作用可以为“非音系化问题”提供（部分）解决方案。</li>
</ul>

<h3>Title: Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into Consistency and Robustness</h3>
<ul>
<li><strong>Authors: </strong>Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11229">https://arxiv.org/abs/2407.11229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11229">https://arxiv.org/pdf/2407.11229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11229]] Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into Consistency and Robustness(https://arxiv.org/abs/2407.11229)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally, we identify areas for improvement and propose future research directions to build more robust and reliable CQA systems. This study sheds light on the limitations of current models and paves the way for future advancements in the field.</li>
<li><strong>摘要：</strong>图表问答 (CQA) 是视觉语言理解的一个重要领域。然而，该领域当前的视觉语言模型 (VLM) 的稳健性和一致性仍未得到充分探索。本文在专门为本研究开发的综合数据集上评估了最先进的 VLM，涵盖了各种问题类别和图表格式。我们研究了两个关键方面：1) 模型处理不同程度的图表和问题复杂度的能力，2) 它们在同一基础数据的不同视觉表示中的稳健性。我们的分析揭示了基于问题和图表类型的显著性能差异，突出了当前模型的优点和缺点。此外，我们确定了需要改进的领域并提出了未来的研究方向，以构建更强大、更可靠的 CQA 系统。这项研究揭示了当前模型的局限性，并为该领域的未来发展铺平了道路。</li>
</ul>

<h3>Title: Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11282">https://arxiv.org/abs/2407.11282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11282">https://arxiv.org/pdf/2407.11282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11282]] Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models(https://arxiv.org/abs/2407.11282)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are employed across various high-stakes domains, where the reliability of their outputs is crucial. One commonly used method to assess the reliability of LLMs' responses is uncertainty estimation, which gauges the likelihood of their answers being correct. While many studies focus on improving the accuracy of uncertainty estimations for LLMs, our research investigates the fragility of uncertainty estimation and explores potential attacks. We demonstrate that an attacker can embed a backdoor in LLMs, which, when activated by a specific trigger in the input, manipulates the model's uncertainty without affecting the final output. Specifically, the proposed backdoor attack method can alter an LLM's output probability distribution, causing the probability distribution to converge towards an attacker-predefined distribution while ensuring that the top-1 prediction remains unchanged. Our experimental results demonstrate that this attack effectively undermines the model's self-evaluation reliability in multiple-choice questions. For instance, we achieved a 100 attack success rate (ASR) across three different triggering strategies in four models. Further, we investigate whether this manipulation generalizes across different prompts and domains. This work highlights a significant threat to the reliability of LLMs and underscores the need for future defenses against such attacks. The code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 应用于各种高风险领域，其输出的可靠性至关重要。评估 LLM 响应可靠性的一种常用方法是不确定性估计，它可以衡量其答案正确的可能性。虽然许多研究都侧重于提高 LLM 不确定性估计的准确性，但我们的研究调查了不确定性估计的脆弱性并探索了潜在的攻击。我们证明攻击者可以在 LLM 中嵌入后门，当输入中的特定触发器激活后门时，它会操纵模型的不确定性而不影响最终输出。具体来说，所提出的后门攻击方法可以改变 LLM 的输出概率分布，使概率分布收敛到攻击者预定义的分布，同时确保 top-1 预测保持不变。我们的实验结果表明，这种攻击有效地破坏了模型在多项选择题中的自我评估可靠性。例如，我们在四个模型中的三种不同触发策略中实现了 100 的攻击成功率 (ASR)。此外，我们研究了这种操纵是否适用于不同的提示和领域。这项研究凸显了对 LLM 可靠性的重大威胁，并强调了未来防御此类攻击的必要性。代码可在此 https URL 上获取。</li>
</ul>

<h3>Title: Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models</h3>
<ul>
<li><strong>Authors: </strong>Matthew Perez, Aneesha Sampath, Minxue Niu, Emily Mower Provost</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11345">https://arxiv.org/abs/2407.11345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11345">https://arxiv.org/pdf/2407.11345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11345]] Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models(https://arxiv.org/abs/2407.11345)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Aphasia is a language disorder that can lead to speech errors known as paraphasias, which involve the misuse, substitution, or invention of words. Automatic paraphasia detection can help those with Aphasia by facilitating clinical assessment and treatment planning options. However, most automatic paraphasia detection works have focused solely on binary detection, which involves recognizing only the presence or absence of a paraphasia. Multiclass paraphasia detection represents an unexplored area of research that focuses on identifying multiple types of paraphasias and where they occur in a given speech segment. We present novel approaches that use a generative pretrained transformer (GPT) to identify paraphasias from transcripts as well as two end-to-end approaches that focus on modeling both automatic speech recognition (ASR) and paraphasia classification as multiple sequences vs. a single sequence. We demonstrate that a single sequence model outperforms GPT baselines for multiclass paraphasia detection.</li>
<li><strong>摘要：</strong>失语症是一种语言障碍，会导致言语错误，即语言错乱，包括误用、替换或发明词语。自动语言错乱检测可以帮助失语症患者，促进临床评估和治疗计划选择。然而，大多数自动语言错乱检测工作仅侧重于二元检测，即仅识别语言错乱的存在与否。多类语言错乱检测代表了一个尚未探索的研究领域，其重点是识别多种类型的语言错乱以及它们在给定语音片段中发生的位置。我们提出了使用生成预训练转换器 (GPT) 从转录本中识别语言错乱的新方法，以及两种端到端方法，这些方法侧重于将自动语音识别 (ASR) 和语言错乱分类建模为多个序列而不是单个序列。我们证明，单个序列模型在多类语言错乱检测方面优于 GPT 基线。</li>
</ul>

<h3>Title: Ancient Korean Archive Translation: Comparison Analysis on Statistical phrase alignment, LLM in-context learning, and inter-methodological approach</h3>
<ul>
<li><strong>Authors: </strong>Sojung Lucia Kim, Taehong Jang, Joonmo Ahn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11368">https://arxiv.org/abs/2407.11368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11368">https://arxiv.org/pdf/2407.11368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11368]] Ancient Korean Archive Translation: Comparison Analysis on Statistical phrase alignment, LLM in-context learning, and inter-methodological approach(https://arxiv.org/abs/2407.11368)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>This study aims to compare three methods for translating ancient texts with sparse corpora: (1) the traditional statistical translation method of phrase alignment, (2) in-context LLM learning, and (3) proposed inter methodological approach - statistical machine translation method using sentence piece tokens derived from unified set of source-target corpus. The performance of the proposed approach in this study is 36.71 in BLEU score, surpassing the scores of SOLAR-10.7B context learning and the best existing Seq2Seq model. Further analysis and discussion are presented.</li>
<li><strong>摘要：</strong>本研究旨在比较三种使用稀疏语料库翻译古文本的方法：（1）传统的短语对齐统计翻译方法，（2）上下文 LLM 学习，以及（3）提出的跨方法论方法 - 使用从统一的源-目标语料库集派生的句子片段标记的统计机器翻译方法。本研究中提出的方法在 BLEU 分数上的表现为 36.71，超过了 SOLAR-10.7B 上下文学习和现有最佳 Seq2Seq 模型的分数。进一步的分析和讨论。</li>
</ul>

<h3>Title: Reliable Reasoning Beyond Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Nasim Borazjanizadeh, Steven T. Piantadosi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11373">https://arxiv.org/abs/2407.11373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11373">https://arxiv.org/pdf/2407.11373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11373]] Reliable Reasoning Beyond Natural Language(https://arxiv.org/abs/2407.11373)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite their linguistic competence, Large Language models (LLMs) often exhibit limitations in their ability to reason reliably and flexibly. To address this, we propose a neurosymbolic approach that prompts LLMs to extract and encode all relevant information from a problem statement as logical code statements, and then use a logic programming language (Prolog) to conduct the iterative computations of explicit deductive reasoning. Our approach significantly enhances the performance of LLMs on the standard mathematical reasoning benchmark, GSM8k, and the Navigate dataset from the BIG-bench dataset. Additionally, we introduce a novel dataset, the Non-Linear Reasoning (NLR) dataset, consisting of 55 unique word problems that target the shortcomings of the next token prediction paradigm of LLMs and require complex non-linear reasoning but only basic arithmetic skills to solve. Our findings demonstrate that the integration of Prolog enables LLMs to achieve high performance on the NLR dataset, which even the most advanced language models (including GPT4) fail to solve using text only.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 具有语言能力，但它们在可靠和灵活推理的能力方面往往存在局限性。为了解决这个问题，我们提出了一种神经符号方法，促使 LLM 从问题陈述中提取并编码所有相关信息作为逻辑代码语句，然后使用逻辑编程语言 (Prolog) 进行显式演绎推理的迭代计算。我们的方法显著提高了 LLM 在标准数学推理基准 GSM8k 和 BIG-bench 数据集的 Navigate 数据集上的性能。此外，我们引入了一个新数据集，即非线性推理 (NLR) 数据集，它由 55 个独特的单词问题组成，这些问题针对 LLM 的下一个标记预测范式的缺点，需要复杂的非线性推理，但只需要基本的算术技能即可解决。我们的研究结果表明，Prolog 的集成使 LLM 能够在 NLR 数据集上实现高性能，即使是最先进的语言模型（包括 GPT4）也无法仅使用文本来解决。</li>
</ul>

<h3>Title: InvAgent: A Large Language Model based Multi-Agent System for Inventory Management in Supply Chains</h3>
<ul>
<li><strong>Authors: </strong>Yinzhu Quan, Zefang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11384">https://arxiv.org/abs/2407.11384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11384">https://arxiv.org/pdf/2407.11384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11384]] InvAgent: A Large Language Model based Multi-Agent System for Inventory Management in Supply Chains(https://arxiv.org/abs/2407.11384)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Supply chain management (SCM) involves coordinating the flow of goods, information, and finances across various entities to deliver products efficiently. Effective inventory management is crucial in today's volatile, uncertain, complex, and ambiguous (VUCA) world. Previous research has demonstrated the superiority of heuristic methods and reinforcement learning applications in inventory management. However, the application of large language models (LLMs) as autonomous agents in multi-agent systems for inventory management remains underexplored. This study introduces a novel approach using LLMs to manage multi-agent inventory systems. Leveraging their zero-shot learning capabilities, our model, InvAgent, enhances resilience and improves efficiency across the supply chain network. Our contributions include utilizing LLMs for zero-shot learning to enable adaptive and informed decision-making without prior training, providing significant explainability and clarity through Chain-of-Thought (CoT), and demonstrating dynamic adaptability to varying demand scenarios while minimizing costs and avoiding stockouts. Extensive evaluations across different scenarios highlight the efficiency of our model in SCM.</li>
<li><strong>摘要：</strong>供应链管理 (SCM) 涉及协调各个实体之间的货物、信息和资金流动，以高效地交付产品。在当今动荡、不确定、复杂和模糊 (VUCA) 的世界中，有效的库存管理至关重要。先前的研究表明，启发式方法和强化学习应用在库存管理中具有优越性。然而，大型语言模型 (LLM) 作为库存管理多智能体系统中的自主智能体的应用仍未得到充分探索。本研究介绍了一种使用 LLM 管理多智能体库存系统的新方法。利用其零样本学习能力，我们的模型 InvAgent 增强了弹性并提高了整个供应链网络的效率。我们的贡献包括利用 LLM 进行零样本学习，以便在没有事先培训的情况下实现自适应和明智的决策，通过思维链 (CoT) 提供显着的可解释性和清晰度，并展示对不同需求场景的动态适应性，同时最大限度地降低成本并避免缺货。对不同场景的广泛评估凸显了我们的模型在 SCM 中的效率。</li>
</ul>

<h3>Title: Revisiting the Impact of Pursuing Modularity for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Deokyeong Kang, Ki Jung Seo, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11406">https://arxiv.org/abs/2407.11406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11406">https://arxiv.org/pdf/2407.11406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11406]] Revisiting the Impact of Pursuing Modularity for Code Generation(https://arxiv.org/abs/2407.11406)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Modular programming, which aims to construct the final program by integrating smaller, independent building blocks, has been regarded as a desirable practice in software development. However, with the rise of recent code generation agents built upon large language models (LLMs), a question emerges: is this traditional practice equally effective for these new tools? In this work, we assess the impact of modularity in code generation by introducing a novel metric for its quantitative measurement. Surprisingly, unlike conventional wisdom on the topic, we find that modularity is not a core factor for improving the performance of code generation models. We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code.</li>
<li><strong>摘要：</strong>模块化编程旨在通过集成较小的独立构建块来构建最终程序，这在软件开发中被视为一种理想的做法。然而，随着最近基于大型语言模型 (LLM) 的代码生成代理的兴起，一个问题出现了：这种传统做法对这些新工具是否同样有效？在这项工作中，我们通过引入一种新的定量测量指标来评估模块化对代码生成的影响。令人惊讶的是，与该主题的传统观点不同，我们发现模块化并不是提高代码生成模型性能的核心因素。我们还探讨了为什么 LLM 不偏爱模块化代码而不是非模块化代码的潜在解释。</li>
</ul>

<h3>Title: Representation Bias in Political Sample Simulations with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weihong Qi, Hanjia Lyu, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11409">https://arxiv.org/abs/2407.11409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11409">https://arxiv.org/pdf/2407.11409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11409]] Representation Bias in Political Sample Simulations with Large Language Models(https://arxiv.org/abs/2407.11409)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This study seeks to identify and quantify biases in simulating political samples with Large Language Models, specifically focusing on vote choice and public opinion. Using the GPT-3.5-Turbo model, we leverage data from the American National Election Studies, German Longitudinal Election Study, Zuobiao Dataset, and China Family Panel Studies to simulate voting behaviors and public opinions. This methodology enables us to examine three types of representation bias: disparities based on the the country's language, demographic groups, and political regime types. The findings reveal that simulation performance is generally better for vote choice than for public opinions, more accurate in English-speaking countries, more effective in bipartisan systems than in multi-partisan systems, and stronger in democratic settings than in authoritarian regimes. These results contribute to enhancing our understanding and developing strategies to mitigate biases in AI applications within the field of computational social science.</li>
<li><strong>摘要：</strong>本研究旨在识别和量化使用大型语言模型模拟政治样本时的偏见，特别关注投票选择和公众舆论。使用 GPT-3.5-Turbo 模型，我们利用来自美国全国选举研究、德国纵向选举研究、左表数据集和中国家庭面板研究的数据来模拟投票行为和公众舆论。这种方法使我们能够研究三种类型的代表性偏见：基于国家语言、人口群体和政治体制类型的差异。研究结果表明，模拟结果在投票选择方面通常比公众舆论更好，在英语国家更准确，在两党制中比在多党制中更有效，在民主环境中比在威权政体中更强。这些结果有助于增进我们对计算社会科学领域人工智能应用中偏见的理解和制定缓解偏见的策略。</li>
</ul>

<h3>Title: SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions</h3>
<ul>
<li><strong>Authors: </strong>Shicheng Liu, Sina J. Semnani, Harold Triedman, Jialiang Xu, Isaac Dan Zhao, Monica S. Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11417">https://arxiv.org/abs/2407.11417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11417">https://arxiv.org/pdf/2407.11417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11417]] SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions(https://arxiv.org/abs/2407.11417)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent work integrating Large Language Models (LLMs) has led to significant improvements in the Knowledge Base Question Answering (KBQA) task. However, we posit that existing KBQA datasets that either have simple questions, use synthetically generated logical forms, or are based on small knowledge base (KB) schemas, do not capture the true complexity of KBQA tasks. To address this, we introduce the SPINACH dataset, an expert-annotated KBQA dataset collected from forum discussions on Wikidata's "Request a Query" forum with 320 decontextualized question-SPARQL pairs. Much more complex than existing datasets, SPINACH calls for strong KBQA systems that do not rely on training data to learn the KB schema, but can dynamically explore large and often incomplete schemas and reason about them. Along with the dataset, we introduce the SPINACH agent, a new KBQA approach that mimics how a human expert would write SPARQLs for such challenging questions. Experiments on existing datasets show SPINACH's capability in KBQA, achieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10 datasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6% of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH dataset, SPINACH agent outperforms all baselines, including the best GPT-4-based KBQA agent, by 38.1% in F1.</li>
<li><strong>摘要：</strong>最近，集成大型语言模型 (LLM) 的工作已显著改善了知识库问答 (KBQA) 任务。然而，我们认为现有的 KBQA 数据集要么包含简单问题，要么使用合成生成的逻辑形式，要么基于小型知识库 (KB) 模式，无法捕捉 KBQA 任务的真正复杂性。为了解决这个问题，我们引入了 SPINACH 数据集，这是一个专家注释的 KBQA 数据集，收集自 Wikidata 的“请求查询”论坛上的论坛讨论，包含 320 个去语境化的问题-SPARQL 对。SPINACH 比现有数据集复杂得多，它需要强大的 KBQA 系统，这些系统不依赖训练数据来学习 KB 模式，但可以动态探索大型且通常不完整的模式并对其进行推理。除了数据集之外，我们还引入了 SPINACH 代理，这是一种新的 KBQA 方法，可以模仿人类专家如何为这些具有挑战性的问题编写 SPARQL。在现有数据集上进行的实验表明，SPINACH 在 KBQA 方面表现出色，在 QALD-7、QALD-9 Plus 和 QALD-10 数据集上分别以 30.1%、27.0% 和 10.0% 的 F1 成绩创下了新高，与 WikiWebQuestions 上经过微调的 LLaMA SOTA 模型的差距在 1.6% 以内。在我们的新 SPINACH 数据集上，SPINACH 代理在 F1 成绩上优于所有基线，包括基于 GPT-4 的最佳 KBQA 代理，高出 38.1%。</li>
</ul>

<h3>Title: States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly</h3>
<ul>
<li><strong>Authors: </strong>Junhao Chen, Shengding Hu, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11421">https://arxiv.org/abs/2407.11421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11421">https://arxiv.org/pdf/2407.11421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11421]] States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly(https://arxiv.org/abs/2407.11421)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit various emergent abilities. Among these abilities, some might reveal the internal working mechanisms of models. In this paper, we uncover a novel emergent capability in models: the intrinsic ability to perform extended sequences of calculations without relying on chain-of-thought step-by-step solutions. Remarkably, the most advanced models can directly output the results of two-digit number additions with lengths extending up to 15 addends. We hypothesize that the model emerges Implicit Discrete State Representations (IDSRs) within its hidden states and performs symbolic calculations internally. To test this hypothesis, we design a sequence of experiments that look into the hidden states. Specifically, we first confirm that IDSRs exist. Then, we provide interesting observations about the formation of IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that models indeed use IDSRs to produce the final answers. However, we also discover that these state representations are far from lossless in current open-sourced models, leading to inaccuracies in their final performance. Our work presents a novel exploration of LLMs' symbolic calculation abilities and the underlying mechanisms.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出各种新兴能力。在这些能力中，有些可能揭示模型的内部工作机制。在本文中，我们发现了模型中一种新颖的新兴能力：无需依赖思路链式逐步解决方案即可执行扩展计算序列的内在能力。值得注意的是，最先进的模型可以直接输出两位数加法的结果，长度最多可达 15 个加数。我们假设该模型在其隐藏状态中出现隐式离散状态表示 (IDSR) 并在内部执行符号计算。为了检验这一假设，我们设计了一系列研究隐藏状态的实验。具体来说，我们首先确认 IDSR 存在。然后，我们从层、数字和序列的角度提供了有关 IDSR 形成的有趣观察结果。最后，我们确认模型确实使用 IDSR 来产生最终答案。然而，我们还发现这些状态表示在当前的开源模型中远非无损，导致其最终性能不准确。我们的工作对 LLM 的符号计算能力及其底层机制进行了新颖的探索。</li>
</ul>

<h3>Title: Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Niloofar Mireshghallah, Maria Antoniak, Yash More, Yejin Choi, Golnoosh Farnadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11438">https://arxiv.org/abs/2407.11438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11438">https://arxiv.org/pdf/2407.11438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11438]] Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations in the Wild(https://arxiv.org/abs/2407.11438)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Measuring personal disclosures made in human-chatbot interactions can provide a better understanding of users' AI literacy and facilitate privacy research for large language models (LLMs). We run an extensive, fine-grained analysis on the personal disclosures made by real users to commercial GPT models, investigating the leakage of personally identifiable and sensitive information. To understand the contexts in which users disclose to chatbots, we develop a taxonomy of tasks and sensitive topics, based on qualitative and quantitative analysis of naturally occurring conversations. We discuss these potential privacy harms and observe that: (1) personally identifiable information (PII) appears in unexpected contexts such as in translation or code editing (48% and 16% of the time, respectively) and (2) PII detection alone is insufficient to capture the sensitive topics that are common in human-chatbot interactions, such as detailed sexual preferences or specific drug use habits. We believe that these high disclosure rates are of significant importance for researchers and data curators, and we call for the design of appropriate nudging mechanisms to help users moderate their interactions.</li>
<li><strong>摘要：</strong>衡量人机交互中个人身份信息披露有助于更好地了解用户的 AI 素养，并促进大型语言模型 (LLM) 的隐私研究。我们对真实用户向商业 GPT 模型的个人身份信息披露进行了广泛、细粒度的分析，调查了个人身份信息和敏感信息的泄露情况。为了了解用户向聊天机器人披露信息的背景，我们基于对自然对话的定性和定量分析，开发了一个任务和敏感主题的分类法。我们讨论了这些潜在的隐私危害，并观察到：(1) 个人身份信息 (PII) 出现在翻译或代码编辑等意外环境中（分别占 48% 和 16% 的时间）；(2) 仅靠 PII 检测不足以捕捉人机交互中常见的敏感话题，例如详细的性偏好或特定的药物使用习惯。我们认为这些高披露率对于研究人员和数据管理者来说非常重要，我们呼吁设计适当的推动机制来帮助用户调节他们的互动。</li>
</ul>

<h3>Title: Scientific QA System with Verifiable Answers</h3>
<ul>
<li><strong>Authors: </strong>Adela Ljajić, Miloš Košprdić, Bojana Bašaragin, Darija Medvecki, Lorenzo Cassano, Nikola Milošević</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11485">https://arxiv.org/abs/2407.11485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11485">https://arxiv.org/pdf/2407.11485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11485]] Scientific QA System with Verifiable Answers(https://arxiv.org/abs/2407.11485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce the VerifAI project, a pioneering open-source scientific question-answering system, designed to provide answers that are not only referenced but also automatically vetted and verifiable. The components of the system are (1) an Information Retrieval system combining semantic and lexical search techniques over scientific papers (PubMed), (2) a Retrieval-Augmented Generation (RAG) module using fine-tuned generative model (Mistral 7B) and retrieved articles to generate claims with references to the articles from which it was derived, and (3) a Verification engine, based on a fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task using SciFACT dataset. The verification engine cross-checks the generated claim and the article from which the claim was derived, verifying whether there may have been any hallucinations in generating the claim. By leveraging the Information Retrieval and RAG modules, this http URL excels in generating factual information from a vast array of scientific sources. At the same time, the Verification engine rigorously double-checks this output, ensuring its accuracy and reliability. This dual-stage process plays a crucial role in acquiring and confirming factual information, significantly enhancing the information landscape. Our methodology could significantly enhance scientists' productivity, concurrently fostering trust in applying generative language models within scientific domains, where hallucinations and misinformation are unacceptable.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 VerifAI 项目，这是一个开创性的开源科学问答系统，旨在提供不仅有参考价值而且可以自动审查和验证的答案。该系统的组成部分包括 (1) 一个结合了科学论文 (PubMed) 的语义和词汇搜索技术的信息检索系统，(2) 一个使用微调生成模型 (Mistral 7B) 和检索到的文章的检索增强生成 (RAG) 模块，以生成引用其来源文章的声明，以及 (3) 一个验证引擎，基于使用 SciFACT 数据集的自然语言推理任务上微调的 DeBERTa 和 XLM-RoBERTa 模型。验证引擎交叉检查生成的声明和声明来源的文章，验证在生成声明时是否可能存在幻觉。通过利用信息检索和 RAG 模块，这个 http URL 擅长从大量科学来源生成事实信息。同时，验证引擎会严格地复查输出结果，确保其准确性和可靠性。这一双阶段过程在获取和确认事实信息方面发挥着至关重要的作用，大大改善了信息格局。我们的方法可以大大提高科学家的生产力，同时培养人们对在科学领域应用生成语言模型的信任，因为幻觉和错误信息是不可接受的。</li>
</ul>

<h3>Title: Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise</h3>
<ul>
<li><strong>Authors: </strong>Qimin Yang, Rongsheng Wang, Jiexin Chen, Runqi Su, Tao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11536">https://arxiv.org/abs/2407.11536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11536">https://arxiv.org/pdf/2407.11536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11536]] Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise(https://arxiv.org/abs/2407.11536)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been widely applied in various professional fields. By fine-tuning the models using domain specific question and answer datasets, the professional domain knowledge and Q\&A abilities of these models have significantly improved, for example, medical professional LLMs that use fine-tuning of doctor-patient Q\&A data exhibit extraordinary disease diagnostic abilities. However, we observed that despite improvements in specific domain knowledge, the performance of medical LLM in long-context understanding has significantly declined, especially compared to general language models with similar parameters. The purpose of this study is to investigate the phenomenon of reduced performance in understanding long-context in medical LLM. We designed a series of experiments to conduct open-book professional knowledge exams on all models to evaluate their ability to read long-context. By adjusting the proportion and quantity of general data and medical data in the process of fine-tuning, we can determine the best data composition to optimize the professional model and achieve a balance between long-context performance and specific domain knowledge.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已广泛应用于各个专业领域，通过使用特定领域的问答数据集对模型进行微调，这些模型的专业领域知识和问答能力得到了显著提升，例如，使用医患问答数据进行微调的医学专业LLM表现出了非凡的疾病诊断能力。然而，我们观察到，尽管特定领域知识有所提升，但医学LLM在长上下文理解方面的表现却明显下降，尤其是与具有相似参数的通用语言模型相比。本研究旨在调查医学LLM中长上下文理解性能下降的现象。我们设计了一系列实验，对所有模型进行开卷专业知识考试，以评估其阅读长上下文的能力。通过在微调过程中调整通用数据和医学数据的比例和数量，我们可以确定最佳数据组成以优化专业模型，并在长上下文性能和特定领域知识之间取得平衡。</li>
</ul>

<h3>Title: How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yin Jou Huang, Rafik Hadfi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11549">https://arxiv.org/abs/2407.11549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11549">https://arxiv.org/pdf/2407.11549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11549]] How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models(https://arxiv.org/abs/2407.11549)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Psychological evidence reveals the influence of personality traits on decision-making. For instance, agreeableness is generally associated with positive outcomes in negotiations, whereas neuroticism is often linked to less favorable outcomes. This paper introduces a simulation framework centered on Large Language Model (LLM) agents endowed with synthesized personality traits. The agents negotiate within bargaining domains and possess customizable personalities and objectives. The experimental results show that the behavioral tendencies of LLM-based simulations could reproduce behavioral patterns observed in human negotiations. The contribution is twofold. First, we propose a simulation methodology that investigates the alignment between the linguistic and economic capabilities of LLM agents. Secondly, we offer empirical insights into the strategic impact of Big-Five personality traits on the outcomes of bilateral negotiations. We also provide a case study based on synthesized bargaining dialogues to reveal intriguing behaviors, including deceitful and compromising behaviors.</li>
<li><strong>摘要：</strong>心理学证据揭示了性格特征对决策的影响。例如，宜人性通常与谈判中的积极结果相关，而神经质通常与不太有利的结果相关。本文介绍了一个以具有合成性格特征的大型语言模型 (LLM) 代理为中心的模拟框架。这些代理在谈判领域内进行谈判，并具有可定制的个性和目标。实验结果表明，基于 LLM 的模拟的行为倾向可以重现人类谈判中观察到的行为模式。贡献是双重的。首先，我们提出了一种模拟方法来研究 LLM 代理的语言能力和经济能力之间的一致性。其次，我们对“大五”性格特征对双边谈判结果的战略影响提供了实证见解。我们还提供了一个基于合成谈判对话的案例研究，以揭示有趣的行为，包括欺骗和妥协行为。</li>
</ul>

<h3>Title: Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization</h3>
<ul>
<li><strong>Authors: </strong>Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11550">https://arxiv.org/abs/2407.11550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11550">https://arxiv.org/pdf/2407.11550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11550]] Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization(https://arxiv.org/abs/2407.11550)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models have excelled in various fields but encounter efficiency limitations due to the extensive KV cache required for long sequences inference. Many efforts try to evict non-critical cache elements during runtime, thereby reducing cache size within a given memory budget while preserving generation quality. Our reexamination of their underlying principles discerns that prevailing strategies essentially aim to minimize an upper bound of eviction loss within a specific budget allocation. However, we observe that the current practice of uniformly allocating budgets across different attention heads during the eviction procedure tends to degrade the quality of generation posten-eviction. In light of these findings, we propose a simple yet effective adaptive allocation algorithm that not only theoretically ensures its loss upper bound does not exceed that of previous uniform allocation methods, but also effectively aligns with the characteristics of the self-attention mechanism, thus practically reducing the upper bound. Further, integrating this algorithm with two of the most advanced methods yields Ada-SnapKV and Ada-Pyramid. Extensive experimental validation across 16 datasets and the Needle-in-a-Haystack test confirm that Ada-SnapKV and Ada-Pyramid achieve further enhancements, establishing new benchmarks in state-of-the-art performance.</li>
<li><strong>摘要：</strong>大型语言模型在各个领域都表现出色，但由于长序列推理需要大量的 KV 缓存，因此遇到了效率限制。许多努力都试图在运行时驱逐非关键缓存元素，从而在给定的内存预算内减少缓存大小，同时保持生成质量。我们重新审视了它们的基本原理，发现现行策略本质上旨在在特定预算分配内最小化驱逐损失的上限。然而，我们观察到，在驱逐过程中在不同注意力头之间均匀分配预算的当前做法往往会降低驱逐后的生成质量。鉴于这些发现，我们提出了一种简单而有效的自适应分配算法，该算法不仅在理论上确保其损失上限不超过以前的均匀分配方法，而且有效地与自注意力机制的特点保持一致，从而在实践中降低了上限。此外，将该算法与两种最先进的方法相结合，产生了 Ada-SnapKV 和 Ada-Pyramid。跨 16 个数据集的大量实验验证和 Needle-in-a-Haystack 测试证实，Ada-SnapKV 和 Ada-Pyramid 取得了进一步的增强，并在最先进的性能方面树立了新的标杆。</li>
</ul>

<h3>Title: AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11591">https://arxiv.org/abs/2407.11591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11591">https://arxiv.org/pdf/2407.11591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11591]] AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization(https://arxiv.org/abs/2407.11591)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the advances in the abstractive summarization task using Large Language Models (LLM), there is a lack of research that asses their abilities to easily adapt to different domains. We evaluate the domain adaptation abilities of a wide range of LLMs on the summarization task across various domains in both fine-tuning and in-context learning settings. We also present AdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a domain benchmark and a set of metrics to facilitate the analysis of domain adaptation. Our results demonstrate that LLMs exhibit comparable performance in the in-context learning setting, regardless of their parameter scale.</li>
<li><strong>摘要：</strong>尽管使用大型语言模型 (LLM) 进行抽象摘要任务取得了进展，但缺乏评估其轻松适应不同领域的能力的研究。我们在微调和上下文学习设置中评估了各种 LLM 在各个领域的摘要任务上的领域适应能力。我们还介绍了第一个领域适应评估套件 AdaptEval。AdaptEval 包括一个领域基准和一组指标，以促进领域适应性的分析。我们的结果表明，无论参数规模如何，LLM 在上下文学习设置中都表现出相当的性能。</li>
</ul>

<h3>Title: The Foundations of Tokenization: Statistical and Computational Concerns</h3>
<ul>
<li><strong>Authors: </strong>Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim Vieira, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11606">https://arxiv.org/abs/2407.11606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11606">https://arxiv.org/pdf/2407.11606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11606]] The Foundations of Tokenization: Statistical and Computational Concerns(https://arxiv.org/abs/2407.11606)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Tokenization - the practice of converting strings of characters over an alphabet into sequences of tokens over a vocabulary - is a critical yet under-theorized step in the NLP pipeline. Notably, it remains the only major step not fully integrated into widely used end-to-end neural models. This paper aims to address this theoretical gap by laying the foundations of tokenization from a formal perspective. By articulating and extending basic properties about the category of stochastic maps, we propose a unified framework for representing and analyzing tokenizer models. This framework allows us to establish general conditions for the use of tokenizers. In particular, we formally establish the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. Additionally, we discuss statistical and computational concerns crucial for the design and implementation of tokenizer models. The framework and results advanced in this paper represent a step toward a robust theoretical foundation for neural language modeling.</li>
<li><strong>摘要：</strong>标记化（将字母表中的字符串转换为词汇表中的标记序列的做法）是 NLP 流程中一个关键但理论化程度较低的步骤。值得注意的是，它仍然是唯一未完全集成到广泛使用的端到端神经模型中的主要步骤。本文旨在通过从形式化的角度奠定标记化的基础来解决这一理论空白。通过阐明和扩展随机映射类别的基本属性，我们提出了一个用于表示和分析标记器模型的统一框架。该框架使我们能够为标记器的使用建立一般条件。特别是，我们正式建立了标记器模型的必要和充分条件，以保持统计估计量的一致性。此外，我们讨论了对标记器模型的设计和实现至关重要的统计和计算问题。本文提出的框架和结果代表了朝着神经语言建模的强大理论基础迈出的一步。</li>
</ul>

<h3>Title: A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting</h3>
<ul>
<li><strong>Authors: </strong>He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11638">https://arxiv.org/abs/2407.11638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11638">https://arxiv.org/pdf/2407.11638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11638]] A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting(https://arxiv.org/abs/2407.11638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have demonstrated great potential in various data mining tasks, such as knowledge question answering, mathematical reasoning, and commonsense reasoning. However, the reasoning capability of LLMs on temporal event forecasting has been under-explored. To systematically investigate their abilities in temporal event forecasting, we conduct a comprehensive evaluation of LLM-based methods for temporal event forecasting. Due to the lack of a high-quality dataset that involves both graph and textual data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on this dataset, we design a series of baseline methods, characterized by various input formats and retrieval augmented generation(RAG) modules. From extensive experiments, we find that directly integrating raw texts into the input of LLMs does not enhance zero-shot extrapolation performance. In contrast, incorporating raw texts in specific complex events and fine-tuning LLMs significantly improves performance. Moreover, enhanced with retrieval modules, LLM can effectively capture temporal relational patterns hidden in historical events. Meanwhile, issues such as popularity bias and the long-tail problem still persist in LLMs, particularly in the RAG-based method. These findings not only deepen our understanding of LLM-based event forecasting methods but also highlight several promising research directions.We consider that this comprehensive evaluation, along with the identified research opportunities, will significantly contribute to future research on temporal event forecasting through LLMs.</li>
<li><strong>摘要：</strong>最近，大型语言模型 (LLM) 在各种数据挖掘任务中表现出巨大潜力，例如知识问答、数学推理和常识推理。然而，LLM 在时间事件预测方面的推理能力尚未得到充分开发。为了系统地研究它们在时间事件预测中的能力，我们对基于 LLM 的时间事件预测方法进行了全面评估。由于缺乏涉及图形和文本数据的高质量数据集，我们首先构建了一个基准数据集，名为 MidEast-TE-mini。基于此数据集，我们设计了一系列基线方法，其特点是各种输入格式和检索增强生成 (RAG) 模块。通过大量实验，我们发现直接将原始文本集成到 LLM 的输入中并不能提高零样本外推性能。相反，将原始文本纳入特定的复杂事件并微调 LLM 可以显著提高性能。此外，通过检索模块的增强，LLM 可以有效地捕获隐藏在历史事件中的时间关系模式。同时，诸如流行偏见和长尾问题等问题仍然存在于 LLM 中，尤其是在基于 RAG 的方法中。这些发现不仅加深了我们对基于 LLM 的事件预测方法的理解，而且还突出了几个有前途的研究方向。我们认为，这种全面的评估以及确定的研究机会将为未来通过 LLM 进行时间事件预测的研究做出重大贡献。</li>
</ul>

<h3>Title: ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues</h3>
<ul>
<li><strong>Authors: </strong>John Mendonça, Isabel Trancoso, Alon Lavie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11660">https://arxiv.org/abs/2407.11660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11660">https://arxiv.org/pdf/2407.11660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11660]] ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues(https://arxiv.org/abs/2407.11660)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Despite being heralded as the new standard for dialogue evaluation, the closed-source nature of GPT-4 poses challenges for the community. Motivated by the need for lightweight, open source, and multilingual dialogue evaluators, this paper introduces GenResCoh (Generated Responses targeting Coherence). GenResCoh is a novel LLM generated dataset comprising over 130k negative and positive responses and accompanying explanations seeded from XDailyDialog and XPersona covering English, French, German, Italian, and Chinese. Leveraging GenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators trained to assess response coherence across multiple languages. Experimental results demonstrate that ECoh achieves multilingual detection capabilities superior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based on a much smaller architecture. Furthermore, the explanations provided by ECoh closely align in terms of quality with those generated by the teacher model.</li>
<li><strong>摘要：</strong>尽管被誉为对话评估的新标准，但 GPT-4 的闭源性质给社区带来了挑战。受对轻量级、开源和多语言对话评估器的需求的推动，本文介绍了 GenResCoh（以连贯性为目标的生成响应）。GenResCoh 是一个新颖的 LLM 生成数据集，包含来自 XDailyDialog 和 XPersona 的超过 13 万个负面和正面响应以及随附的解释，涵盖英语、法语、德语、意大利语和中文。利用 GenResCoh，我们提出了 ECoh（连贯性评估），这是一组经过训练的评估器，用于评估多种语言的响应连贯性。实验结果表明，尽管 ECoh 基于更小的架构，但它在 GenResCoh 上实现了优于教师模型（GPT-3.5-Turbo）的多语言检测能力。此外，ECoh 提供的解释在质量方面与教师模型生成的解释非常接近。</li>
</ul>

<h3>Title: MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11681">https://arxiv.org/abs/2407.11681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11681">https://arxiv.org/pdf/2407.11681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11681]] MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models(https://arxiv.org/abs/2407.11681)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network compressing, especially in pruning medium-size networks. However, the substantial memory requirements involved in calculating gradients with backpropagation impede the utilization of gradients in guiding LLM pruning. As a result, most pruning strategies for LLMs rely on gradient-free criteria, such as weight magnitudes or a mix of magnitudes and activations. In this paper, we devise a hybrid pruning criterion, which appropriately integrates magnitude, activation, and gradient to capitalize on feature map sensitivity for pruning LLMs. To overcome memory requirement barriers, we estimate gradients using only forward passes. Based on this, we propose a Memory-effIcieNt structured prunIng procedure for LLMs (MINI-LLM) to remove no-critical channels and multi-attention heads. Experimental results demonstrate the superior performance of MINI-LLM over existing gradient-free methods on three LLMs: LLaMA, BLOOM, and OPT across various downstream tasks (classification, multiple-choice, and generation), while MINI-LLM maintains a GPU memory footprint akin to gradient-free methods.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的规模急剧增长，压缩和加速这些模型的趋势日益明显。先前的研究强调了梯度在神经网络压缩中的重要性评分的实用性，尤其是在修剪中型网络时。然而，使用反向传播计算梯度所需的大量内存阻碍了梯度在指导 LLM 修剪中的利用。因此，大多数 LLM 的修剪策略都依赖于无梯度标准，例如权重幅度或幅度和激活的混合。在本文中，我们设计了一种混合修剪标准，它适当地整合了幅度、激活和梯度，以利用特征图敏感性来修剪 LLM。为了克服内存需求障碍，我们仅使用前向传递来估计梯度。基于此，我们提出了一种内存效率结构化 LLM (MINI-LLM) 修剪程序，以删除非关键通道和多注意头。实验结果表明，MINI-LLM 在三种 LLM：LLaMA、BLOOM 和 OPT 等各种下游任务（分类、多项选择和生成）中的表现优于现有的无梯度方法，同时 MINI-LLM 保持的 GPU 内存占用与无梯度方法类似。</li>
</ul>

<h3>Title: CCoE: A Compact LLM with Collaboration of Experts</h3>
<ul>
<li><strong>Authors: </strong>Shaomang Huang, Jianfeng Pan, Hanzhong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11686">https://arxiv.org/abs/2407.11686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11686">https://arxiv.org/pdf/2407.11686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11686]] CCoE: A Compact LLM with Collaboration of Experts(https://arxiv.org/abs/2407.11686)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the domain of Large Language Model (LLM), LLMs demonstrate significant capabilities in natural language understanding and generation. With the growing needs of applying LLMs on various domains, it is a research question that how to efficiently train and build a model that has expertise in different domains but with a low training cost. We propose CCoE architecture, a framework of easily coupling multiple strong domain experts together to fuse into a big LLM, provides a collective way of utilizing the different domain expert LLMs. Besides, training a large collaborative of multiple expert LLMs requires a high requirements on training sources. CCoE bypasses this problem through isolating other experts and train each expert separately. The design of CCoE assembles multiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE layer could have one or more expert LLMs. Expert LLMs have different number of layers and have been well-trained for different domain tasks. Each expert is fine-tuned to be able to achieve the comparable results with SOTA domain LLMs. We start from 5 experts in the domain of Code, Math, Law, text-to-SQL and Medical. The results indicate that our CCoE framework can easily and efficiently boost nearly 10%-20% performance on original base model in different domains but using less resources on training, as well as inference.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 领域，LLM 在自然语言理解和生成方面表现出显著的能力。随着 LLM 在各个领域应用需求的不断增长，如何高效地训练和构建一个在不同领域都具有专业知识但训练成本较低的模型是一个研究问题。我们提出了 CCoE 架构，这是一个将多个强大的领域专家轻松耦合在一起以融合成一个大型 LLM 的框架，提供了一种利用不同领域专家 LLM 的集体方式。此外，训练多个专家 LLM 的大型协作对训练源的要求很高。CCoE 通过隔离其他专家并分别训练每个专家来绕过这个问题。CCoE 的设计通过 CoE（专家协作）层组装多个专家 LLM。每个 CoE 层可以有一个或多个专家 LLM。专家 LLM 具有不同数量的层，并且已经针对不同领域的任务进行了良好的训练。每个专家都经过微调，以便能够实现与 SOTA 领域 LLM 相当的结果。我们从代码、数学、法律、文本转 SQL 和医学领域的 5 位专家开始。结果表明，我们的 CCoE 框架可以轻松高效地将原始基础模型在不同领域的性能提升近 10%-20%，但在训练和推理上消耗的资源更少。</li>
</ul>

<h3>Title: How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies</h3>
<ul>
<li><strong>Authors: </strong>Alina Leidinger, Richard Rogers</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11733">https://arxiv.org/abs/2407.11733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11733">https://arxiv.org/pdf/2407.11733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11733]] How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies(https://arxiv.org/abs/2407.11733)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>With the widespread availability of LLMs since the release of ChatGPT and increased public scrutiny, commercial model development appears to have focused their efforts on 'safety' training concerning legal liabilities at the expense of social impact evaluation. This mimics a similar trend which we could observe for search engine autocompletion some years prior. We draw on scholarship from NLP and search engine auditing and present a novel evaluation task in the style of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by using four metrics, namely refusal rates, toxicity, sentiment and regard, with and without safety system prompts. Our findings indicate an improvement to stereotyping outputs with the system prompt, but overall a lack of attention by LLMs under study to certain harms classified as toxic, particularly for prompts about peoples/ethnicities and sexual orientation. Mentions of intersectional identities trigger a disproportionate amount of stereotyping. Finally, we discuss the implications of these findings about stereotyping harms in light of the coming intermingling of LLMs and search and the choice of stereotyping mitigation policy to adopt. We address model builders, academics, NLP practitioners and policy makers, calling for accountability and awareness concerning stereotyping harms, be it for training data curation, leader board design and usage, or social impact measurement.</li>
<li><strong>摘要：</strong>自 ChatGPT 发布以来，法学硕士课程广为普及，公众监督力度不断加大，商业模式开发似乎将精力集中在与法律责任有关的“安全”培训上，而忽略了社会影响评估。这与几年前我们在搜索引擎自动完成方面观察到的趋势类似。我们借鉴了 NLP 和搜索引擎审计方面的研究成果，提出了一种以自动完成提示形式呈现的新颖评估任务，以评估法学硕士课程中的刻板印象。我们使用四个指标来评估法学硕士课程，即拒绝率、毒性、情绪和尊重，包括有无安全系统提示。我们的研究结果表明，使用系统提示可以改善刻板印象输出，但总体而言，所研究的法学硕士课程对某些被归类为有毒的危害缺乏关注，尤其是关于人/种族和性取向的提示。提及交叉身份会引发过多的刻板印象。最后，我们讨论了这些关于刻板印象危害的发现对 LLM 和搜索即将融合以及选择采用刻板印象缓解政策的影响。我们向模型构建者、学者、NLP 从业者和政策制定者发出呼吁，呼吁对刻板印象危害承担责任并提高认识，无论是在训练数据管理、排行榜设计和使用，还是社会影响衡量方面。</li>
</ul>

<h3>Title: Vectoring Languages</h3>
<ul>
<li><strong>Authors: </strong>Joseph Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11766">https://arxiv.org/abs/2407.11766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11766">https://arxiv.org/pdf/2407.11766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11766]] Vectoring Languages(https://arxiv.org/abs/2407.11766)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in large language models (LLM) have stirred up global attention, and the research has been accelerating non-stop since then. Philosophers and psychologists have also been researching the structure of language for decades, but they are having a hard time finding a theory that directly benefits from the breakthroughs of LLMs. In this article, we propose a novel structure of language that reflects well on the mechanisms behind language models and go on to show that this structure is also better at capturing the diverse nature of language compared to previous methods. An analogy of linear algebra is adapted to strengthen the basis of this perspective. We further argue about the difference between this perspective and the design philosophy for current language models. Lastly, we discuss how this perspective can lead us to research directions that may accelerate the improvements of science fastest.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新突破引起了全球关注，研究也一直在加速进行。几十年来，哲学家和心理学家也一直在研究语言的结构，但他们很难找到一种直接受益于 LLM 突破的理论。在本文中，我们提出了一种新颖的语言结构，它很好地反映了语言模型背后的机制，并进一步表明这种结构与以前的方法相比更能捕捉语言的多样性。我们采用了线性代数的类比来加强这一观点的基础。我们进一步讨论了这种观点与当前语言模型设计理念之间的差异。最后，我们讨论了这种观点如何引导我们找到可能最快加速科学进步的研究方向。</li>
</ul>

<h3>Title: Robust Utility-Preserving Text Anonymization Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Yang, Xiaodan Zhu, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11770">https://arxiv.org/abs/2407.11770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11770">https://arxiv.org/pdf/2407.11770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11770]] Robust Utility-Preserving Text Anonymization Based on Large Language Models(https://arxiv.org/abs/2407.11770)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Text anonymization is crucial for sharing sensitive data while maintaining privacy. Existing techniques face the emerging challenges of re-identification attack ability of Large Language Models (LLMs), which have shown advanced capability in memorizing detailed information and patterns as well as connecting disparate pieces of information. In defending against LLM-based re-identification attacks, anonymization could jeopardize the utility of the resulting anonymized data in downstream tasks -- the trade-off between privacy and data utility requires deeper understanding within the context of LLMs. This paper proposes a framework composed of three LLM-based components -- a privacy evaluator, a utility evaluator, and an optimization component, which work collaboratively to perform anonymization. To provide a practical model for large-scale and real-time environments, we distill the anonymization capabilities into a lightweight model using Direct Preference Optimization (DPO). Extensive experiments demonstrate that the proposed models outperform baseline models, showing robustness in reducing the risk of re-identification while preserving greater data utility in downstream tasks. Our code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>文本匿名化对于在保持隐私的同时共享敏感数据至关重要。现有技术面临着大型语言模型 (LLM) 重新识别攻击能力的新兴挑战，这些模型在记忆详细信息和模式以及连接不同信息方面表现出了先进的能力。在防御基于 LLM 的重新识别攻击时，匿名化可能会危及生成的匿名数据在下游任务中的效用——隐私和数据效用之间的权衡需要在 LLM 的背景下有更深入的理解。本文提出了一个由三个基于 LLM 的组件组成的框架——隐私评估器、效用评估器和优化组件，它们协同工作以执行匿名化。为了为大规模和实时环境提供实用模型，我们使用直接偏好优化 (DPO) 将匿名化功能提炼为轻量级模型。大量实验表明，所提出的模型优于基线模型，在降低重新识别风险的同时在下游任务中保留了更高的数据效用，表现出稳健性。我们的代码和数据集可在此 https URL 上获取。</li>
</ul>

<h3>Title: Educational Personalized Learning Path Planning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chee Ng, Yuen Fung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11773">https://arxiv.org/abs/2407.11773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11773">https://arxiv.org/pdf/2407.11773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11773]] Educational Personalized Learning Path Planning with Large Language Models(https://arxiv.org/abs/2407.11773)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Educational Personalized Learning Path Planning (PLPP) aims to tailor learning experiences to individual learners' needs, enhancing learning efficiency and engagement. Despite its potential, traditional PLPP systems often lack adaptability, interactivity, and transparency. This paper proposes a novel approach integrating Large Language Models (LLMs) with prompt engineering to address these challenges. By designing prompts that incorporate learner-specific information, our method guides LLMs like LLama-2-70B and GPT-4 to generate personalized, coherent, and pedagogically sound learning paths. We conducted experiments comparing our method with a baseline approach across various metrics, including accuracy, user satisfaction, and the quality of learning paths. The results show significant improvements in all areas, particularly with GPT-4, demonstrating the effectiveness of prompt engineering in enhancing PLPP. Additional long-term impact analysis further validates our method's potential to improve learner performance and retention. This research highlights the promise of LLMs and prompt engineering in advancing personalized education.</li>
<li><strong>摘要：</strong>教育个性化学习路径规划 (PLPP) 旨在根据个人学习者的需求定制学习体验，提高学习效率和参与度。尽管具有潜力，但传统的 PLPP 系统通常缺乏适应性、交互性和透明度。本文提出了一种将大型语言模型 (LLM) 与提示工程相结合的新方法来应对这些挑战。通过设计包含学习者特定信息的提示，我们的方法可以指导 LLama-2-70B 和 GPT-4 等 LLM 生成个性化、连贯且教学合理的学习路径。我们进行了实验，将我们的方法与基线方法在各种指标上进行了比较，包括准确性、用户满意度和学习路径的质量。结果显示所有领域都有显着改善，尤其是 GPT-4，证明了提示工程在增强 PLPP 方面的有效性。额外的长期影响分析进一步验证了我们的方法在提高学习者表现和保留率方面的潜力。这项研究凸显了 LLM 和提示工程在推进个性化教育方面的前景。</li>
</ul>

<h3>Title: Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to Detect Machine Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Seyedeh Fatemeh Ebrahimi, Karim Akhavan Azari, Amirmasoud Iravani, Arian Qazvini, Pouya Sadeghi, Zeinab Sadat Taghavi, Hossein Sameti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11774">https://arxiv.org/abs/2407.11774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11774">https://arxiv.org/pdf/2407.11774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11774]] Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to Detect Machine Generated Text(https://arxiv.org/abs/2407.11774)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Detecting Machine-Generated Text (MGT) has emerged as a significant area of study within Natural Language Processing. While language models generate text, they often leave discernible traces, which can be scrutinized using either traditional feature-based methods or more advanced neural language models. In this research, we explore the effectiveness of fine-tuning a RoBERTa-base transformer, a powerful neural architecture, to address MGT detection as a binary classification task. Focusing specifically on Subtask A (Monolingual-English) within the SemEval-2024 competition framework, our proposed system achieves an accuracy of 78.9% on the test dataset, positioning us at 57th among participants. Our study addresses this challenge while considering the limited hardware resources, resulting in a system that excels at identifying human-written texts but encounters challenges in accurately discerning MGTs.</li>
<li><strong>摘要：</strong>检测机器生成文本 (MGT) 已成为自然语言处理领域的一个重要研究领域。虽然语言模型会生成文本，但它们通常会留下可辨别的痕迹，可以使用传统的基于特征的方法或更先进的神经语言模型来仔细检查这些痕迹。在这项研究中，我们探索了微调基于 RoBERTa 的转换器（一种强大的神经架构）以将 MGT 检测作为二元分类任务的有效性。我们提出的系统专注于 SemEval-2024 竞赛框架中的子任务 A（单语-英语），在测试数据集上的准确率达到 78.9%，在参与者中排名第 57 位。我们的研究在考虑硬件资源有限的情况下解决了这一挑战，从而产生了一个擅长识别人类书写文本但在准确辨别 MGT 方面遇到挑战的系统。</li>
</ul>

<h3>Title: SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinbo Wu, Max Hartman, Vidhata Arjun Jayaraman, Lav R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11780">https://arxiv.org/abs/2407.11780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11780">https://arxiv.org/pdf/2407.11780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11780]] SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models(https://arxiv.org/abs/2407.11780)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited impressive capabilities in various domains, particularly in general language understanding. However these models, trained on massive text data, may not be finely optimized for specific tasks triggered by instructions. Continual instruction tuning is crucial to adapt LLMs to evolving tasks and domains, ensuring their effectiveness and relevance across a wide range of applications. In the context of continual instruction tuning, where models are sequentially trained on different tasks, catastrophic forgetting can occur, leading to performance degradation on previously learned tasks. This work addresses the catastrophic forgetting in continual instruction learning for LLMs through a switching mechanism for routing computations to parameter-efficient tuned models. We demonstrate the effectiveness of our method through experiments on continual instruction tuning of different natural language generation tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各个领域都展现出了令人印象深刻的能力，特别是在一般语言理解方面。然而，这些在海量文本数据上训练的模型可能无法针对指令触发的特定任务进行精细优化。持续指令调整对于使 LLM 适应不断发展的任务和领域至关重要，可确保其在广泛应用中的有效性和相关性。在持续指令调整的背景下，模型会在不同任务上进行顺序训练，可能会发生灾难性遗忘，导致先前学习的任务的性能下降。这项工作通过一种切换机制将计算路由到参数高效的调整模型，解决了 LLM 持续指令学习中的灾难性遗忘问题。我们通过对不同自然语言生成任务的持续指令调整实验证明了我们方法的有效性。</li>
</ul>

<h3>Title: Large Language Models as Misleading Assistants in Conversation</h3>
<ul>
<li><strong>Authors: </strong>Betty Li Hou, Kejian Shi, Jason Phang, James Aung, Steven Adler, Rosie Campbell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11789">https://arxiv.org/abs/2407.11789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11789">https://arxiv.org/pdf/2407.11789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11789]] Large Language Models as Misleading Assistants in Conversation(https://arxiv.org/abs/2407.11789)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are able to provide assistance on a wide range of information-seeking tasks. However, model outputs may be misleading, whether unintentionally or in cases of intentional deception. We investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users. We compare outcomes of (1) when the model is prompted to provide truthful assistance, (2) when it is prompted to be subtly misleading, and (3) when it is prompted to argue for an incorrect answer. Our experiments show that GPT-4 can effectively mislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up to a 23% drop in accuracy on the task compared to when a truthful assistant is used. We also find that providing the user model with additional context from the passage partially mitigates the influence of the deceptive model. This work highlights the ability of LLMs to produce misleading information and the effects this may have in real-world situations.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 能够为各种信息搜索任务提供帮助。然而，模型输出可能会产生误导，无论是无意的还是故意欺骗。我们使用 LLM 作为人类用户的代理，研究了 LLM 在阅读理解任务中提供帮助的背景下的欺骗能力。我们比较了以下情况的结果：(1) 当模型被提示提供真实的帮助时，(2) 当模型被提示进行巧妙的误导时，以及 (3) 当模型被提示为错误答案辩护时。我们的实验表明，GPT-4 可以有效误导 GPT-3.5-Turbo 和 GPT-4，与使用真实的助手相比，欺骗性助手会导致任务准确率下降高达 23%。我们还发现，为用户模型提供来自文章的额外上下文可以部分减轻欺骗性模型的影响。这项研究凸显了法学硕士产生误导信息的能力及其在现实世界中可能产生的影响。</li>
</ul>

<h3>Title: PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation</h3>
<ul>
<li><strong>Authors: </strong>Branden Butler, Sixing Yu, Arya Mazaheri, Ali Jannesari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11798">https://arxiv.org/abs/2407.11798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11798">https://arxiv.org/pdf/2407.11798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11798]] PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation(https://arxiv.org/abs/2407.11798)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Inference of Large Language Models (LLMs) across computer clusters has become a focal point of research in recent times, with many acceleration techniques taking inspiration from CPU speculative execution. These techniques reduce bottlenecks associated with memory bandwidth, but also increase end-to-end latency per inference run, requiring high speculation acceptance rates to improve performance. Combined with a variable rate of acceptance across tasks, speculative inference techniques can result in reduced performance. Additionally, pipeline-parallel designs require many user requests to maintain maximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative acceleration technique to reduce inter-token latency and improve system utilization for single-request scenarios while also improving tolerance to low speculation acceptance rates and low-bandwidth interconnects. PipeInfer exhibits up to a 2.15$\times$ improvement in generation speed over standard speculative inference. PipeInfer achieves its improvement through Continuous Asynchronous Speculation and Early Inference Cancellation, the former improving latency and generation speed by running single-token inference simultaneously with several speculative runs, while the latter improves speed and latency by skipping the computation of invalidated runs, even in the middle of inference.</li>
<li><strong>摘要：</strong>近年来，跨计算机集群的大型语言模型 (LLM) 推理已成为研究的焦点，许多加速技术都从 CPU 推测执行中汲取灵感。这些技术减少了与内存带宽相关的瓶颈，但也增加了每次推理运行的端到端延迟，需要较高的推测接受率才能提高性能。结合跨任务的可变接受率，推测推理技术可能会导致性能下降。此外，管道并行设计需要许多用户请求才能保持最大利用率。作为补救措施，我们提出了 PipeInfer，这是一种流水线推测加速技术，可减少令牌间延迟并提高单请求场景的系统利用率，同时提高对低推测接受率和低带宽互连的容忍度。与标准推测推理相比，PipeInfer 的生成速度提高了 2.15$\times$。 PipeInfer 通过持续异步推测和早期推理取消实现了改进，前者通过同时运行单令牌推理和多个推测运行来提高延迟和生成速度，而后者通过跳过无效运行的计算（即使在推理过程中）来提高速度和延迟。</li>
</ul>

<h3>Title: GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text</h3>
<ul>
<li><strong>Authors: </strong>Kyle Hamilton, Luca Longo, Bojan Bozic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11827">https://arxiv.org/abs/2407.11827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11827">https://arxiv.org/pdf/2407.11827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11827]] GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text(https://arxiv.org/abs/2407.11827)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>While the use of machine learning for the detection of propaganda techniques in text has garnered considerable attention, most approaches focus on "black-box" solutions with opaque inner workings. Interpretable approaches provide a solution, however, they depend on careful feature engineering and costly expert annotated data. Additionally, language features specific to propagandistic text are generally the focus of rhetoricians or linguists, and there is no data set labeled with such features suitable for machine learning. This study codifies 22 rhetorical and linguistic features identified in literature related to the language of persuasion for the purpose of annotating an existing data set labeled with propaganda techniques. To help human experts annotate natural language sentences with these features, RhetAnn, a web application, was specifically designed to minimize an otherwise considerable mental effort. Finally, a small set of annotated data was used to fine-tune GPT-3.5, a generative large language model (LLM), to annotate the remaining data while optimizing for financial cost and classification accuracy. This study demonstrates how combining a small number of human annotated examples with GPT can be an effective strategy for scaling the annotation process at a fraction of the cost of traditional annotation relying solely on human experts. The results are on par with the best performing model at the time of writing, namely GPT-4, at 10x less the cost. Our contribution is a set of features, their properties, definitions, and examples in a machine-readable format, along with the code for RhetAnn and the GPT prompts and fine-tuning procedures for advancing state-of-the-art interpretable propaganda technique detection.</li>
<li><strong>摘要：</strong>虽然使用机器学习检测文本中的宣传技巧已引起广泛关注，但大多数方法都侧重于内部运作不透明的“黑盒”解决方案。可解释的方法提供了一种解决方案，但它们依赖于精心的特征工程和昂贵的专家注释数据。此外，宣传文本特有的语言特征通常是修辞学家或语言学家关注的重点，并且没有适合机器学习的带有此类特征的数据集。本研究将与说服语言相关的文献中确定的 22 个修辞和语言特征编纂成法典，目的是注释带有宣传技巧标记的现有数据集。为了帮助人类专家使用这些特征注释自然语言句子，专门设计了 Web 应用程序 RhetAnn，以最大限度地减少原本相当大的脑力劳动。最后，使用一小部分注释数据对生成式大型语言模型 (LLM) GPT-3.5 进行微调，以注释剩余数据，同时优化财务成本和分类准确性。这项研究表明，将少量人工注释示例与 GPT 相结合是一种有效的策略，可以以传统注释的一小部分成本扩展注释过程，而传统注释的成本仅为完全依赖人类专家的成本。结果与撰写本文时表现最佳的模型 GPT-4 相当，但成本却低 10 倍。我们的贡献是一组机器可读格式的特征、它们的属性、定义和示例，以及 RhetAnn 的代码和 GPT 提示和微调程序，用于推进最先进的可解释宣传技术检测。</li>
</ul>

<h3>Title: LoFTI: Localization and Factuality Transfer to Indian Locales</h3>
<ul>
<li><strong>Authors: </strong>Sona Elza Simon (1), Soumen Kumar Mondal (1), Abhishek Singhania (2), Sayambhu Sen (2), Preethi Jyothi (1) ((1) Indian Institute of Technology Bombay, (2) Amazon Alexa)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11833">https://arxiv.org/abs/2407.11833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11833">https://arxiv.org/pdf/2407.11833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11833]] LoFTI: Localization and Factuality Transfer to Indian Locales(https://arxiv.org/abs/2407.11833)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) encode vast amounts of world knowledge acquired via training on large web-scale datasets crawled from the internet. However, these datasets typically exhibit a geographical bias towards English-speaking Western countries. This results in LLMs producing biased or hallucinated responses to queries that require answers localized to other geographical regions. In this work, we introduce a new benchmark named LoFTI (Localization and Factuality Transfer to Indian Locales) that can be used to evaluate an LLM's localization and factual text transfer capabilities. LoFTI consists of factual statements about entities in source and target locations; the source locations are spread across the globe and the target locations are all within India with varying degrees of hyperlocality (country, states, cities). The entities span a wide variety of categories. We use LoFTI to evaluate Mixtral, GPT-4 and two other Mixtral-based approaches well-suited to the task of localized factual transfer. We demonstrate that LoFTI is a high-quality evaluation benchmark and all the models, including GPT-4, produce skewed results across varying levels of hyperlocality.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 编码了大量的世界知识，这些知识是通过对从互联网上爬取的大型网络规模数据集进行训练而获得的。然而，这些数据集通常表现出对讲英语的西方国家的地理偏见。这导致 LLM 对需要本地化到其他地理区域的答案的查询产生有偏见或幻觉的响应。在这项工作中，我们引入了一个名为 LoFTI（本地化和事实转移到印度地区）的新基准，可用于评估 LLM 的本地化和事实文本传输能力。LoFTI 由关于源位置和目标位置的实体的事实陈述组成；源位置遍布全球，目标位置都在印度境内，具有不同程度的超本地化（国家、州、城市）。实体涵盖了各种各样的类别。我们使用 LoFTI 来评估 Mixtral、GPT-4 和其他两种基于 Mixtral 的方法，这些方法非常适合本地化事实转移的任务。我们证明 LoFTI 是一个高质量的评估基准，并且包括 GPT-4 在内的所有模型都会在不同程度的超本地化中产生偏差的结果。</li>
</ul>

<h3>Title: InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Haishuo Fang, Xiaodan Zhu, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11843">https://arxiv.org/abs/2407.11843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11843">https://arxiv.org/pdf/2407.11843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11843]] InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback(https://arxiv.org/abs/2407.11843)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>A crucial requirement for deploying LLM-based agents in real-life applications is robustness against risky or irreversible mistakes. However, existing research lacks a focus on the preemptive evaluation of reasoning trajectories performed by LLM agents, leading to a gap in ensuring safe and reliable operations. To explore better solutions, this paper introduces InferAct, a novel approach that leverages the Theory-of-Mind capability of LLMs to proactively detect potential errors before critical actions are executed (e.g., "buy-now" in automatic online trading or web shopping). InferAct is also capable of integrating human feedback to prevent irreversible risks and enhance the actor agent's decision-making process. Experiments on three widely used tasks demonstrate the effectiveness of InferAct. The proposed solution presents a novel approach and concrete contributions toward developing LLM agents that can be safely deployed in different environments involving critical decision-making.</li>
<li><strong>摘要：</strong>在实际应用中部署基于 LLM 的代理的一个关键要求是能够抵御风险或不可逆转的错误。然而，现有研究缺乏对 LLM 代理执行的推理轨迹的预先评估，导致在确保安全可靠的操作方面存在差距。为了探索更好的解决方案，本文介绍了 InferAct，这是一种利用 LLM 的心理理论能力在执行关键操作（例如，自动在线交易或网络购物中的“立即购买”）之前主动检测潜在错误的新方法。InferAct 还能够整合人工反馈，以防止不可逆转的风险并增强参与者代理的决策过程。对三个广泛使用的任务的实验证明了 InferAct 的有效性。所提出的解决方案提出了一种新方法，并为开发可在涉及关键决策的不同环境中安全部署的 LLM 代理做出了具体贡献。</li>
</ul>

<h3>Title: Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection</h3>
<ul>
<li><strong>Authors: </strong>Gaetan Lopez Latouche, Marc-André Carbonneau, Ben Swanson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11854">https://arxiv.org/abs/2407.11854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11854">https://arxiv.org/pdf/2407.11854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11854]] Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection(https://arxiv.org/abs/2407.11854)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Grammatical Error Detection (GED) methods rely heavily on human annotated error corpora. However, these annotations are unavailable in many low-resource languages. In this paper, we investigate GED in this context. Leveraging the zero-shot cross-lingual transfer capabilities of multilingual pre-trained language models, we train a model using data from a diverse set of languages to generate synthetic errors in other languages. These synthetic error corpora are then used to train a GED model. Specifically we propose a two-stage fine-tuning pipeline where the GED model is first fine-tuned on multilingual synthetic data from target languages followed by fine-tuning on human-annotated GED corpora from source languages. This approach outperforms current state-of-the-art annotation-free GED methods. We also analyse the errors produced by our method and other strong baselines, finding that our approach produces errors that are more diverse and more similar to human errors.</li>
<li><strong>摘要：</strong>语法错误检测 (GED) 方法严重依赖于人工注释的错误语料库。然而，这些注释在许多资源匮乏的语言中是不可用的。在本文中，我们在此背景下研究 GED。利用多语言预训练语言模型的零样本跨语言迁移能力，我们使用来自多种语言的数据训练模型以生成其他语言的合成错误。然后使用这些合成错误语料库来训练 GED 模型。具体来说，我们提出了一个两阶段微调流程，其中 GED 模型首先在来自目标语言的多语言合成数据上进行微调，然后在来自源语言的人工注释 GED 语料库上进行微调。这种方法优于目前最先进的无注释 GED 方法。我们还分析了我们的方法和其他强基线产生的错误，发现我们的方法产生的错误更加多样化，也更类似于人为错误。</li>
</ul>

<h3>Title: Scaling Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Biao Zhang, Garrett Tanzer, Orhan Firat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11855">https://arxiv.org/abs/2407.11855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11855">https://arxiv.org/pdf/2407.11855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11855]] Scaling Sign Language Translation(https://arxiv.org/abs/2407.11855)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Sign language translation (SLT) addresses the problem of translating information from a sign language in video to a spoken language in text. Existing studies, while showing progress, are often limited to narrow domains and/or few sign languages and struggle with open-domain tasks. In this paper, we push forward the frontier of SLT by scaling pretraining data, model size, and number of translation directions. We perform large-scale SLT pretraining on different data including 1) noisy multilingual YouTube SLT data, 2) parallel text corpora, and 3) SLT data augmented by translating video captions to other languages with off-the-shelf machine translation models. We unify different pretraining tasks with task-specific prompts under the encoder-decoder architecture, and initialize the SLT model with pretrained (m/By)T5 models across model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL to 42 spoken languages) demonstrate the significance of data/model scaling and cross-lingual cross-modal transfer, as well as the feasibility of zero-shot SLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT benchmarks covering 5 sign languages. Experiments show substantial quality improvements over the vanilla baselines, surpassing the previous state-of-the-art (SOTA) by wide margins.</li>
<li><strong>摘要：</strong>手语翻译 (SLT) 解决了将视频中的手语信息翻译成文本中的口语的问题。现有研究虽然取得了进展，但往往局限于狭窄领域和/或少数手语，难以完成开放领域任务。在本文中，我们通过扩展预训练数据、模型大小和翻译方向数量来推动 SLT 的发展。我们对不同的数据进行大规模 SLT 预训练，包括 1) 嘈杂的多语言 YouTube SLT 数据、2) 并行文本语料库和 3) 通过使用现成的机器翻译模型将视频字幕翻译成其他语言而增强的 SLT 数据。我们在编码器-解码器架构下将不同的预训练任务与特定于任务的提示统一起来，并使用跨模型大小的预训练 (m/By)T5 模型初始化 SLT 模型。 How2Sign 和 FLEURS-ASL#0（从 ASL 到 42 种口语）上的 SLT 预训练结果证明了数据/模型扩展和跨语言跨模态迁移的重要性，以及零样本 SLT 的可行性。我们在 5 个下游开放域 SLT 基准上对预训练的 SLT 模型进行了微调，涵盖了 5 种手语。实验表明，与 vanilla 基线相比，质量有了显著提高，远远超过了之前的最佳水平 (SOTA)。</li>
</ul>

<h3>Title: Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction</h3>
<ul>
<li><strong>Authors: </strong>Tiziano Labruna, Bernardo Magnini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11857">https://arxiv.org/abs/2407.11857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11857">https://arxiv.org/pdf/2407.11857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11857]] Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction(https://arxiv.org/abs/2407.11857)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Task-oriented dialogues must maintain consistency both within the dialogue itself, ensuring logical coherence across turns, and with the conversational domain, accurately reflecting external knowledge. We propose to conceptualize dialogue consistency as a Constraint Satisfaction Problem (CSP), wherein variables represent segments of the dialogue referencing the conversational domain, and constraints among variables reflect dialogue properties, including linguistic, conversational, and domain-based aspects. To demonstrate the feasibility of the approach, we utilize a CSP solver to detect inconsistencies in dialogues re-lexicalized by an LLM. Our findings indicate that: (i) CSP is effective to detect dialogue inconsistencies; and (ii) consistent dialogue re-lexicalization is challenging for state-of-the-art LLMs, achieving only a 0.15 accuracy rate when compared to a CSP solver. Furthermore, through an ablation study, we reveal that constraints derived from domain knowledge pose the greatest difficulty in being respected. We argue that CSP captures core properties of dialogue consistency that have been poorly considered by approaches based on component pipelines.</li>
<li><strong>摘要：</strong>任务导向型对话必须保持对话本身的一致性，确保对话轮次之间的逻辑连贯性，并与对话领域保持一致，准确反映外部知识。我们建议将对话一致性概念化为约束满足问题 (CSP)，其中变量代表对话中引用对话领域的部分，变量之间的约束反映对话属性，包括语言、对话和基于领域的方面。为了证明该方法的可行性，我们使用 CSP 求解器来检测 LLM 重新词汇化的对话中的不一致性。我们的研究结果表明：(i) CSP 可有效检测对话不一致之处；(ii) 一致的对话重新词汇化对于最先进的 LLM 来说具有挑战性，与 CSP 求解器相比，准确率仅为 0.15。此外，通过消融研究，我们发现来自领域知识的约束最难被遵守。我们认为，CSP 捕捉到了对话一致性的核心属性，而基于组件管道的方法并未充分考虑这些属性。</li>
</ul>

<h3>Title: What's Wrong? Refining Meeting Summaries with LLM Feedback</h3>
<ul>
<li><strong>Authors: </strong>Frederic Kirstein, Terry Ruas, Bela Gipp</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11919">https://arxiv.org/abs/2407.11919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11919">https://arxiv.org/pdf/2407.11919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11919]] What's Wrong? Refining Meeting Summaries with LLM Feedback(https://arxiv.org/abs/2407.11919)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Meeting summarization has become a critical task since digital encounters have become a common practice. Large language models (LLMs) show great potential in summarization, offering enhanced coherence and context understanding compared to traditional methods. However, they still struggle to maintain relevance and avoid hallucination. We introduce a multi-LLM correction approach for meeting summarization using a two-phase process that mimics the human review process: mistake identification and summary refinement. We release QMSum Mistake, a dataset of 200 automatically generated meeting summaries annotated by humans on nine error types, including structural, omission, and irrelevance errors. Our experiments show that these errors can be identified with high accuracy by an LLM. We transform identified mistakes into actionable feedback to improve the quality of a given summary measured by relevance, informativeness, conciseness, and coherence. This post-hoc refinement effectively improves summary quality by leveraging multiple LLMs to validate output quality. Our multi-LLM approach for meeting summarization shows potential for similar complex text generation tasks requiring robustness, action planning, and discussion towards a goal.</li>
<li><strong>摘要：</strong>自从数字会议成为一种常见做法以来，会议总结已成为一项关键任务。大型语言模型 (LLM) 在总结方面表现出巨大潜力，与传统方法相比，它们提供了增强的连贯性和上下文理解。然而，它们仍然难以保持相关性并避免幻觉。我们引入了一种多 LLM 校正方法，用于会议总结，使用模仿人工审查过程的两阶段过程：错误识别和总结细化。我们发布了 QMSum Mistake，这是一个由 200 个自动生成的会议总结组成的数据集，由人工注释了九种错误类型，包括结构性、遗漏和不相关错误。我们的实验表明，这些错误可以通过 LLM 高精度地识别。我们将已识别的错误转化为可操作的反馈，以提高给定总结的质量，以相关性、信息量、简洁性和连贯性为衡量标准。这种事后细化通过利用多个 LLM 来验证输出质量，有效地提高了总结质量。我们用于会议总结的多 LLM 方法显示出完成类似复杂文本生成任务的潜力，这些任务需要稳健性、行动规划和针对目标的讨论。</li>
</ul>

<h3>Title: Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11930">https://arxiv.org/abs/2407.11930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11930">https://arxiv.org/pdf/2407.11930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11930]] Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering(https://arxiv.org/abs/2407.11930)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the first hallucination dataset with localized error annotations for human-written and model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k span-level error annotations for five different error types by expert annotators, along with preference judgments. Using our collected data, we thoroughly analyze the shortcomings of long-form answers and find that they lack comprehensiveness and provide unhelpful references. We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-informed refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces hallucination and improves answer quality. Furthermore, humans find answers generated by our approach comprehensive and highly prefer them (84%) over the baseline answers.</li>
<li><strong>摘要：</strong>长篇问答系统 (LFQA) 旨在为复杂问题提供全面而深入的答案，以增强理解力。然而，如此详细的回答容易产生幻觉和事实不一致，对忠实的评估提出了挑战。这项工作引入了 HaluQuestQA，这是第一个带有局部错误注释的幻觉数据集，用于人工编写和模型生成的 LFQA 答案。HaluQuestQA 包含 698 个 QA 对，其中有 4.7k 个跨度级错误注释，由专家注释者针对五种不同的错误类型进行注释，并附带偏好判断。利用我们收集的数据，我们彻底分析了长篇答案的缺点，发现它们缺乏全面性，并且提供的参考资料没有帮助。我们在这个数据集上训练了一个自动反馈模型，该模型可以在信息不完整的情况下预测错误跨度并提供相关解释。最后，我们提出了一种基于提示的方法，即错误知情细化，该方法使用来自学习反馈模型的信号来细化生成的答案，我们表明这可以减少幻觉并提高答案质量。此外，人类发现我们的方法生成的答案全面，并且比基线答案更喜欢它们（84％）。</li>
</ul>

<h3>Title: NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?</h3>
<ul>
<li><strong>Authors: </strong>Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11963">https://arxiv.org/abs/2407.11963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11963">https://arxiv.org/pdf/2407.11963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11963]] NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?(https://arxiv.org/abs/2407.11963)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In evaluating the long-context capabilities of large language models (LLMs), identifying content relevant to a user's query from original long documents is a crucial prerequisite for any LLM to answer questions based on long text. We present NeedleBench, a framework consisting of a series of progressively more challenging tasks for assessing bilingual long-context capabilities, spanning multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and different depth ranges, allowing the strategic insertion of critical data points in different text depth zones to rigorously test the retrieval and reasoning capabilities of models in diverse contexts. We use the NeedleBench framework to assess how well the leading open-source models can identify key information relevant to the question and apply that information to reasoning in bilingual long texts. Furthermore, we propose the Ancestral Trace Challenge (ATC) to mimic the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks, providing a simple method for evaluating LLMs in dealing with complex long-context situations. Our results suggest that current LLMs have significant room for improvement in practical long-context applications, as they struggle with the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks. All codes and resources are available at OpenCompass: this https URL.</li>
<li><strong>摘要：</strong>在评估大型语言模型 (LLM) 的长上下文能力时，从原始长文档中识别与用户查询相关的内容是任何 LLM 回答基于长文本的问题的关键先决条件。我们提出了 NeedleBench，这是一个由一系列逐渐更具挑战性的任务组成的框架，用于评估双语长上下文能力，涵盖多个长度间隔（4k、8k、32k、128k、200k、1000k 及以上）和不同的深度范围，允许在不同的文本深度区域中战略性地插入关键数据点，以严格测试模型在不同上下文中的检索和推理能力。我们使用 NeedleBench 框架来评估领先的开源模型识别与问题相关的关键信息并将该信息应用于双语长文本推理的能力。此外，我们提出了祖先踪迹挑战 (ATC) 来模拟可能出现在现实世界长语境任务中的逻辑推理挑战的复杂性，从而为评估 LLM 处理复杂长语境情况的能力提供了一种简单的方法。我们的结果表明，当前的 LLM 在实际长语境应用中有很大的改进空间，因为它们难以应对可能出现在现实世界长语境任务中的逻辑推理挑战的复杂性。所有代码和资源均可在 OpenCompass 上找到：此 https URL。</li>
</ul>

<h3>Title: Does Refusal Training in LLMs Generalize to the Past Tense?</h3>
<ul>
<li><strong>Authors: </strong>Maksym Andriushchenko, Nicolas Flammarion</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.11969">https://arxiv.org/abs/2407.11969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.11969">https://arxiv.org/pdf/2407.11969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.11969]] Does Refusal Training in LLMs Generalize to the Past Tense?(https://arxiv.org/abs/2407.11969)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art LLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For example, the success rate of this simple attack on GPT-4o increases from 1% using direct requests to 88% using 20 past tense reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when past tense examples are explicitly included in the fine-tuning data. Overall, our findings highlight that the widely used alignment techniques -- such as SFT, RLHF, and adversarial training -- employed to align the studied models can be brittle and do not always generalize as intended. We provide code and jailbreak artifacts at this https URL.</li>
<li><strong>摘要：</strong>拒绝训练被广泛用于防止 LLM 产生有害、不良或非法的输出。我们发现，当前拒绝训练方法中存在一个奇怪的泛化差距：简单地用过去时态重新表述有害请求（例如，将“如何制作燃烧瓶？”改为“人们是如何制作燃烧瓶的？”）通常足以越狱许多最先进的 LLM。我们使用 GPT-3.5 Turbo 作为重新表述模型，在 Llama-3 8B、GPT-3.5 Turbo、Gemma-2 9B、Phi-3-Mini、GPT-4o 和 R2D2 模型上系统地评估了这种方法。例如，使用 GPT-4 作为越狱法官，对来自 JailbreakBench 的有害请求进行 20 次过去时态重新表述尝试，这种对 GPT-4o 的简单攻击的成功率从使用直接请求的 1% 增加到 88%。有趣的是，我们还发现，未来时态的重新表述效果较差，这表明拒绝护栏倾向于认为过去的历史问题比假设的未来问题更为温和。此外，我们对 GPT-3.5 Turbo 进行微调的实验表明，当微调数据中明确包含过去时态示例时，防御过去的重新表述是可行的。总体而言，我们的研究结果强调，用于对齐研究模型的广泛使用的对齐技术（例如 SFT、RLHF 和对抗性训练）可能很脆弱，并且并不总是按预期进行推广。我们在此 https URL 上提供代码和越狱工件。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
