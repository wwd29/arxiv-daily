<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-06</h1>
<h3>Title: Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03587">https://arxiv.org/abs/2412.03587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03587">https://arxiv.org/pdf/2412.03587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03587]] Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models(https://arxiv.org/abs/2412.03587)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large-scale pre-trained models achieve great success, and fine-tuning, which tunes a pre-trained model on a task-specific dataset, is the standard practice to utilize these models for downstream tasks. Recent work has developed adapter-tuning, but these approaches either still require a relatively high resource usage. Through our investigation, we show that each adapter in adapter-tuning does not have the same impact on task performance and resource usage. Based on our findings, we propose SAFE, which gradually freezes less-important adapters that do not contribute to adaptation during the early training steps. In our experiments, SAFE reduces memory usage, computation amount, and training time by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or better performance compared to the baseline. We also demonstrate that SAFE induces regularization effect, thereby smoothing the loss landscape.</li>
<li><strong>摘要：</strong>基于 Transformer 的大规模预训练模型取得了巨大的成功，而微调（在特定于任务的数据集上调整预训练模型）是将这些模型用于下游任务的标准做法。最近的研究已经开发出适配器调优，但是这些方法仍然需要相对较高的资源使用率。通过调查，我们表明适配器调优中的每个适配器对任务性能和资源使用的影响并不相同。基于我们的研究结果，我们提出了 SAFE，它在早期训练步骤中逐渐冻结对适应没有贡献的不太重要的适配器。在我们的实验中，SAFE 分别将内存使用量、计算量和训练时间减少了 42.85\%、34.59\% 和 11.82\%，同时实现了与基线相当或更好的性能。我们还证明 SAFE 会产生正则化效应，从而平滑损失格局。</li>
</ul>

<h3>Title: CovidLLM: A Robust Large Language Model with Missing Value Adaptation and Multi-Objective Learning Strategy for Predicting Disease Severity and Clinical Outcomes in COVID-19 Patients</h3>
<ul>
<li><strong>Authors: </strong>Shengjun Zhu (1), Siyu Liu (2), Yang Li (3), Qing Lei, Hongyan Hou, Hewei Jiang, Shujuan Guo, Feng Wang, Rongshang Chen, Xionglin Fan, Shengce Tao, Jiaxin Cai ((1) School of Mathematics and Statistics, Xiamen University of Technology, Xiamen, China, (2) School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China, (3) Shanghai Center for Systems Biomedicine, Key Laboratory of Systems Biomedicine (Ministry of Education), Shanghai Jiao Tong University, Shanghai, China)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03593">https://arxiv.org/abs/2412.03593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03593">https://arxiv.org/pdf/2412.03593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03593]] CovidLLM: A Robust Large Language Model with Missing Value Adaptation and Multi-Objective Learning Strategy for Predicting Disease Severity and Clinical Outcomes in COVID-19 Patients(https://arxiv.org/abs/2412.03593)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Coronavirus Disease 2019 (COVID-19), which emerged in 2019, has caused millions of deaths worldwide. Although effective vaccines have been developed to mitigate severe symptoms, certain populations, particularly the elderly and those with comorbidities, remain at high risk for severe outcomes and increased mortality. Consequently, early identification of the severity and clinical outcomes of the disease in these patients is vital to prevent adverse prognoses. Although traditional machine learning and deep learning models have been widely employed in this area, the potential of large language models (LLMs) remains largely unexplored. Our research focuses primarily on constructing specialized prompts and adopting multi-objective learning strategies. We started by selecting serological indicators that significantly correlate with clinical outcomes and disease severity to serve as input data for the model. Blood test samples often contain numerous missing values, and traditional models generally rely on imputation to handle these gaps in the data. In contrast, LLMs offer the advantage of robust semantic understanding. By setting prompts, we can explicitly inform the model when a feature's value is missing, without the need for imputation. For the multi-objective learning strategy, the model is designed to first predict disease severity and then predict clinical outcomes. Given that LLMs utilize both the input text and the generated tokens as input for generating the next token, the predicted severity is used as a basis for generating the clinical outcome. During the fine-tuning of the LLM, the two objectives influence and improve each other. Our experiments were implemented based on the ChatGLM model. The results demonstrate the effectiveness of LLMs in this task, suggesting promising potential for further development.</li>
<li><strong>摘要：</strong>2019 年爆发的冠状病毒病 (COVID-19) 已导致全球数百万人死亡。尽管已经开发出有效的疫苗来缓解严重症状，但某些人群，尤其是老年人和患有合并症的人，仍然面临严重的后果和死亡率增加的高风险。因此，尽早识别这些患者的疾病严重程度和临床结果对于预防不良预后至关重要。尽管传统的机器学习和深度学习模型已广泛应用于这一领域，但大型语言模型 (LLM) 的潜力仍未得到充分开发。我们的研究主要侧重于构建专门的提示和采用多目标学习策略。我们首先选择与临床结果和疾病严重程度显着相关的血清学指标作为模型的输入数据。血液测试样本通常包含大量缺失值，传统模型通常依靠归纳来处理数据中的这些缺口。相比之下，LLM 具有强大的语义理解优势。通过设置提示，我们可以明确地通知模型何时缺少特征值，而无需归纳。对于多目标学习策略，该模型旨在首先预测疾病严重程度，然后预测临床结果。鉴于 LLM 使用输入文本和生成的标记作为生成下一个标记的输入，因此预测的严重程度被用作生成临床结果的基础。在 LLM 的微调过程中，这两个目标相互影响和改进。我们的实验是基于 ChatGLM 模型实施的。结果证明了 LLM 在该任务中的有效性，表明其具有进一步发展的潜力。</li>
</ul>

<h3>Title: BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching</h3>
<ul>
<li><strong>Authors: </strong>Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03594">https://arxiv.org/abs/2412.03594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03594">https://arxiv.org/pdf/2412.03594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03594]] BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching(https://arxiv.org/abs/2412.03594)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Many LLM tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix. The KV context that is about to be reused may prematurely be evicted with the implicit cache management. Even if not evicted, the lifetime of the shared KV context is extended since requests sharing the same context are not scheduled together, resulting in larger memory usage. These streaming oriented systems schedule the requests in the first-come-first-serve or similar order. As a result, the requests with larger ratio of decoding steps may be scheduled too late to be able to mix with the prefill chunks to increase the hardware utilization. Besides, the token and request number based batching can limit the size of token-batch, which keeps the GPU from saturating for the iterations dominated by decoding tokens. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best, which also shrinks the lifetime of common KV memory. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by 1.1x to 2x on a set of microbenchmarks and two typical industry workloads.</li>
<li><strong>摘要：</strong>许多LLM任务都是大批量甚至离线执行的，其性能指标是吞吐量。这些任务通常表现出前缀共享的特征，其中不同的提示输入可以部分显示公共前缀。然而，现有的LLM推理引擎倾向于优化流式请求，并且在支持具有前缀共享特性的大批量任务方面表现出局限性。现有的解决方案使用基于LRU的缓存来重用公共前缀的KV上下文。即将被重用的KV上下文可能会因隐式缓存管理而过早被驱逐。即使没有被驱逐，共享KV上下文的生命周期也会延长，因为共享相同上下文的请求不会被一起调度，从而导致更大的内存使用量。这些面向流的系统以先到先得或类似的顺序调度请求。因此，具有较大解码步骤比率的请求可能被调度得太晚，无法与预填充块混合以提高硬件利用率。此外，基于令牌和请求数的批处理会限制令牌批的大小，从而防止 GPU 在以解码令牌为主导的迭代中达到饱和。我们提出了 BatchLLM 来解决上述问题。BatchLLM 全局明确标识公共前缀。共享相同前缀的请求将被一起调度以最佳地重用 KV 上下文，这也会缩短公共 KV 内存的生命周期。BatchLLM 对请求进行重新排序，并首先调度具有较大解码比率的请求，以更好地将解码令牌与后者的预填充块混合，并应用以内存为中心的令牌批处理来扩大令牌批处理大小，这有助于提高 GPU 利用率。广泛的评估表明，在一组微基准测试和两个典型的行业工作负载上，BatchLLM 的性能比 vLLM 好 1.1 倍到 2 倍。</li>
</ul>

<h3>Title: The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?</h3>
<ul>
<li><strong>Authors: </strong>Sourav Banerjee, Ayushi Agarwal, Eishkaran Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03597">https://arxiv.org/abs/2412.03597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03597">https://arxiv.org/pdf/2412.03597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03597]] The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?(https://arxiv.org/abs/2412.03597)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and MMLU. These vulnerabilities manifest through benchmark exploitation, dataset contamination, and evaluation bias, creating a false perception of progress in language understanding capabilities. Through extensive review of contemporary evaluation approaches, we identify significant limitations in static benchmark designs, human evaluation protocols, and LLM-as-judge frameworks, all of which compromise the reliability of current performance assessments. As LLM capabilities evolve and existing benchmarks become redundant, we lay the groundwork for new evaluation methods that resist manipulation, minimize data contamination, and assess domain-specific tasks. This requires frameworks that are adapted dynamically, addressing current limitations and providing a more accurate reflection of LLM performance.</li>
<li><strong>摘要：</strong>追求大型语言模型 (LLM) 中的排行榜排名造成了一个根本性的悖论：模型在标准化测试中表现出色，但未能展示真正的语言理解和适应性。我们对 NLP 评估框架的系统分析揭示了整个评估范围内普遍存在的漏洞，从基本指标到复杂的基准，如 GLUE 和 MMLU。这些漏洞通过基准利用、数据集污染和评估偏差表现出来，造成了对语言理解能力进步的错误看法。通过对当代评估方法的广泛审查，我们发现静态基准设计、人工评估协议和 LLM-as-judge 框架存在重大局限性，所有这些都损害了当前绩效评估的可靠性。随着 LLM 能力的发展和现有基准变得多余，我们为新的评估方法奠定了基础，这些方法可以抵抗操纵、最大限度地减少数据污染并评估特定领域的任务。这需要动态调整的框架，以解决当前的局限性并更准确地反映 LLM 性能。</li>
</ul>

<h3>Title: CPTQuant -- A Novel Mixed Precision Post-Training Quantization Techniques for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amitash Nanda, Sree Bhargavi Balija, Debashis Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03599">https://arxiv.org/abs/2412.03599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03599">https://arxiv.org/pdf/2412.03599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03599]] CPTQuant -- A Novel Mixed Precision Post-Training Quantization Techniques for Large Language Models(https://arxiv.org/abs/2412.03599)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models have transformed the comprehension and generation of natural language tasks, but they come with substantial memory and computational requirements. Quantization techniques have emerged as a promising avenue for addressing these challenges while preserving accuracy and making energy efficient. We propose CPTQuant, a comprehensive strategy that introduces correlation-based (CMPQ), pruning-based (PMPQ), and Taylor decomposition-based (TDMPQ) mixed precision techniques. CMPQ adapts the precision level based on canonical correlation analysis of different layers. PMPQ optimizes precision layer-wise based on their sensitivity to sparsity. TDMPQ modifies precision using Taylor decomposition to assess each layer's sensitivity to input perturbation. These strategies allocate higher precision to more sensitive layers while diminishing precision to robust layers. CPTQuant assesses the performance across BERT, OPT-125M, OPT-350M, OPT-1.3B, and OPT-2.7B. We demonstrate up to 4x compression and a 2x-fold increase in efficiency with minimal accuracy drop compared to Hugging Face FP16. PMPQ stands out for achieving a considerably higher model compression. Sensitivity analyses across various LLMs show that the initial and final 30% of layers exhibit higher sensitivities than the remaining layers. PMPQ demonstrates an 11% higher compression ratio than other methods for classification tasks, while TDMPQ achieves a 30% greater compression ratio for language modeling tasks.</li>
<li><strong>摘要：</strong>大型语言模型已经改变了自然语言任务的理解和生成，但它们对内存和计算的要求很高。量化技术已成为解决这些挑战的一种有前途的途径，同时保持准确性并提高能源效率。我们提出了 CPTQuant，这是一种全面的策略，它引入了基于相关性 (CMPQ)、基于修剪 (PMPQ) 和基于泰勒分解 (TDMPQ) 的混合精度技术。CMPQ 根据不同层的典型相关性分析调整精度级别。PMPQ 根据它们对稀疏性的敏感性逐层优化精度。TDMPQ 使用泰勒分解来修改精度，以评估每个层对输入扰动的敏感性。这些策略将更高的精度分配给更敏感的层，同时降低稳健层的精度。CPTQuant 评估了 BERT、OPT-125M、OPT-350M、OPT-1.3B 和 OPT-2.7B 的性能。与 Hugging Face FP16 相比，我们实现了高达 4 倍的压缩率和 2 倍的效率提升，同时准确率下降幅度很小。PMPQ 因实现显著更高的模型压缩率而脱颖而出。跨各种 LLM 的敏感性分析表明，初始和最终 30% 的层表现出比其余层更高的敏感性。PMPQ 在分类任务中比其他方法高出 11% 的压缩率，而 TDMPQ 在语言建模任务中实现了 30% 的压缩率。</li>
</ul>

<h3>Title: CBEval: A framework for evaluating and interpreting cognitive biases in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ammar Shaikh, Raj Abhijit Dandekar, Sreedath Panat, Rajat Dandekar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03605">https://arxiv.org/abs/2412.03605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03605">https://arxiv.org/pdf/2412.03605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03605]] CBEval: A framework for evaluating and interpreting cognitive biases in LLMs(https://arxiv.org/abs/2412.03605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Rapid advancements in Large Language models (LLMs) has significantly enhanced their reasoning capabilities. Despite improved performance on benchmarks, LLMs exhibit notable gaps in their cognitive processes. Additionally, as reflections of human-generated data, these models have the potential to inherit cognitive biases, raising concerns about their reasoning and decision making capabilities. In this paper we present a framework to interpret, understand and provide insights into a host of cognitive biases in LLMs. Conducting our research on frontier language models we're able to elucidate reasoning limitations and biases, and provide reasoning behind these biases by constructing influence graphs that identify phrases and words most responsible for biases manifested in LLMs. We further investigate biases such as round number bias and cognitive bias barrier revealed when noting framing effect in language models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展显著增强了它们的推理能力。尽管在基准测试中表现有所提高，但 LLM 在认知过程方面仍表现出明显的差距。此外，作为人类生成数据的反映，这些模型有可能继承认知偏见，从而引发人们对其推理和决策能力的担忧。在本文中，我们提出了一个框架来解释、理解和洞察 LLM 中的一系列认知偏见。通过对前沿语言模型进行研究，我们能够阐明推理的局限性和偏见，并通过构建影响图来识别对 LLM 中表现出的偏见最负责的短语和单词，从而提供这些偏见背后的推理。我们进一步研究了在注意语言模型中的框架效应时发现的偏见，例如整数偏见和认知偏见障碍。</li>
</ul>

<h3>Title: Evaluating Language Models as Synthetic Data Generators</h3>
<ul>
<li><strong>Authors: </strong>Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee, Yizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03679">https://arxiv.org/abs/2412.03679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03679">https://arxiv.org/pdf/2412.03679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03679]] Evaluating Language Models as Synthetic Data Generators(https://arxiv.org/abs/2412.03679)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness.</li>
<li><strong>摘要：</strong>鉴于语言模型 (LM) 后训练中合成数据的使用越来越多，LM 生成高质量数据的能力几乎与其直接解决问题的能力一样重要。虽然之前的研究主要集中在开发有效的数据生成方法，但它们缺乏在统一环境中对不同 LM 作为数据生成器进行系统比较。为了解决这一差距，我们提出了 AgoraBench，这是一个基准，它提供了标准化的设置和指标来评估 LM 的数据生成能力。通过使用 6 个 LM 合成 126 万个训练实例并训练 99 个学生模型，我们发现了有关 LM 数据生成能力的关键见解。首先，我们观察到 LM 表现出不同的优势。例如，GPT-4o 擅长生成新问题，而 Claude-3.5-Sonnet 在增强现有问题方面表现更好。此外，我们的分析表明，LM 的数据生成能力不一定与其解决问题的能力相关。相反，数据质量的多个内在特征（包括响应质量、困惑度和教学难度）共同充当了更好的指标。最后，我们证明输出格式的战略选择和成本意识模型选择会显著影响数据生成的效率。</li>
</ul>

<h3>Title: From Language Models over Tokens to Language Models over Characters</h3>
<ul>
<li><strong>Authors: </strong>Tim Vieira, Ben LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian DuSell, John Terilla, Timothy J. O'Donnell, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03719">https://arxiv.org/abs/2412.03719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03719">https://arxiv.org/pdf/2412.03719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03719]] From Language Models over Tokens to Language Models over Characters(https://arxiv.org/abs/2412.03719)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Modern language models are internally -- and mathematically -- distributions over token strings rather than \emph{character} strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent analyses are very sensitive to the specification of the prompt (e.g., if the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. We find that -- even with a small computation budget -- our method is able to accurately approximate the character-level distribution (less than 0.00021 excess bits / character) at reasonably fast speeds (46.3 characters / second) on the Llama 3.1 8B language model.</li>
<li><strong>摘要：</strong>现代语言模型在内部（和数学上）分布在标记字符串而非 \emph{character} 字符串上，这给在其上构建用户应用程序的程序员带来了许多挑战。例如，如果提示被指定为字符串，则必须先对其进行标记化，然后才能将其传递给标记级语言模型。因此，标记器和后续分析对提示的指定非常敏感（例如，提示是否以空格结尾）。本文介绍了将标记级语言模型转换为字符级语言模型的算法。我们提出了精确和近似算法。在本文的实证部分，我们对实际运行时间和近似质量进行了基准测试。我们发现，即使计算预算很少，我们的方法也能够在 Llama 3.1 8B 语言模型上以相当快的速度（46.3 个字符/秒）准确地近似字符级分布（少于 0.00021 个多余位/字符）。</li>
</ul>

<h3>Title: Language Model Meets Prototypes: Towards Interpretable Text Classification Models through Prototypical Networks</h3>
<ul>
<li><strong>Authors: </strong>Ximing Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03761">https://arxiv.org/abs/2412.03761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03761">https://arxiv.org/pdf/2412.03761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03761]] Language Model Meets Prototypes: Towards Interpretable Text Classification Models through Prototypical Networks(https://arxiv.org/abs/2412.03761)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pretrained transformer-based Language Models (LMs) are well-known for their ability to achieve significant improvement on NLP tasks, but their black-box nature, which leads to a lack of interpretability, has been a major concern. My dissertation focuses on developing intrinsically interpretable models when using LMs as encoders while maintaining their superior performance via prototypical networks. I initiated my research by investigating enhancements in performance for interpretable models of sarcasm detection. My proposed approach focuses on capturing sentiment incongruity to enhance accuracy while offering instance-based explanations for the classification decisions. Later, I developed a novel white-box multi-head graph attention-based prototype network designed to explain the decisions of text classification models without sacrificing the accuracy of the original black-box LMs. In addition, I am working on extending the attention-based prototype network with contrastive learning to redesign an interpretable graph neural network, aiming to enhance both the interpretability and performance of the model in document classification.</li>
<li><strong>摘要：</strong>基于预训练转换器的语言模型 (LM) 因其能够在 NLP 任务上取得显著改进的能力而闻名，但它们的黑盒特性导致缺乏可解释性，这一直是一个主要问题。我的论文重点是开发使用 LM 作为编码器时本质上可解释的模型，同时通过原型网络保持其卓越的性能。我的研究始于研究可解释的讽刺检测模型的性能增强。我提出的方法侧重于捕捉情绪不一致以提高准确性，同时为分类决策提供基于实例的解释。后来，我开发了一种新颖的白盒多头图基于注意的原型网络，旨在解释文本分类模型的决策，而不会牺牲原始黑盒 LM 的准确性。此外，我正在努力使用对比学习扩展基于注意的原型网络以重新设计可解释的图神经网络，旨在提高模型在文档分类中的可解释性和性能。</li>
</ul>

<h3>Title: The broader spectrum of in-context learning</h3>
<ul>
<li><strong>Authors: </strong>Andrew Kyle Lampinen, Stephanie C. Y. Chan, Aaditya K. Singh, Murray Shanahan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03782">https://arxiv.org/abs/2412.03782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03782">https://arxiv.org/pdf/2412.03782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03782]] The broader spectrum of in-context learning(https://arxiv.org/abs/2412.03782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>The ability of language models to learn a task from a few examples in context has generated substantial interest. Here, we provide a perspective that situates this type of supervised few-shot learning within a much broader spectrum of meta-learned in-context learning. Indeed, we suggest that any distribution of sequences in which context non-trivially decreases loss on subsequent predictions can be interpreted as eliciting a kind of in-context learning. We suggest that this perspective helps to unify the broad set of in-context abilities that language models exhibit $\unicode{x2014}$ such as adapting to tasks from instructions or role play, or extrapolating time series. This perspective also sheds light on potential roots of in-context learning in lower-level processing of linguistic dependencies (e.g. coreference or parallel structures). Finally, taking this perspective highlights the importance of generalization, which we suggest can be studied along several dimensions: not only the ability to learn something novel, but also flexibility in learning from different presentations, and in applying what is learned. We discuss broader connections to past literature in meta-learning and goal-conditioned agents, and other perspectives on learning and adaptation. We close by suggesting that research on in-context learning should consider this broader spectrum of in-context capabilities and types of generalization.</li>
<li><strong>摘要：</strong>语言模型能够从上下文中的几个示例中学习一项任务，这一能力引起了人们的极大兴趣。在这里，我们提供了一种观点，将这种类型的监督式小样本学习置于更广泛的元学习上下文学习范围内。事实上，我们认为，任何序列分布，如果上下文非平凡地降低了后续预测的损失，都可以解释为引发一种上下文学习。我们认为这种观点有助于统一语言模型表现出的广泛的上下文能力 $\unicode{x2014}$，例如根据指令或角色扮演适应任务，或推断时间序列。这种观点还揭示了上下文学习在语言依赖关系的低级处理中的潜在根源（例如共指或平行结构）。最后，从这个角度来看，强调了泛化的重要性，我们认为可以从几个维度来研究泛化：不仅是学习新事物的能力，还有从不同呈现方式中学习的灵活性，以及​​应用所学知识的灵活性。我们讨论了元学习和目标条件代理与过去文献的更广泛联系，以及学习和适应的其他观点。最后，我们建议，情境学习研究应考虑这种更广泛的情境能力和泛化类型。</li>
</ul>

<h3>Title: Agent AI with LangGraph: A Modular Framework for Enhancing Machine Translation Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jialin Wang, Zhihua Duan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03801">https://arxiv.org/abs/2412.03801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03801">https://arxiv.org/pdf/2412.03801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03801]] Agent AI with LangGraph: A Modular Framework for Enhancing Machine Translation Using Large Language Models(https://arxiv.org/abs/2412.03801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper explores the transformative role of Agent AI and LangGraph in advancing the automation and effectiveness of machine translation (MT). Agents are modular components designed to perform specific tasks, such as translating between particular languages, with specializations like TranslateEnAgent, TranslateFrenchAgent, and TranslateJpAgent for English, French, and Japanese translations, respectively. These agents leverage the powerful semantic capabilities of large language models (LLMs), such as GPT-4o, to ensure accurate, contextually relevant translations while maintaining modularity, scalability, and context retention. LangGraph, a graph-based framework built on LangChain, simplifies the creation and management of these agents and their workflows. It supports dynamic state management, enabling agents to maintain dialogue context and automates complex workflows by linking agents and facilitating their collaboration. With flexibility, open-source community support, and seamless integration with LLMs, LangGraph empowers agents to deliver high-quality translations. Together, Agent AI and LangGraph create a cohesive system where LangGraph orchestrates agent interactions, ensuring that user inputs are analyzed, routed, and processed efficiently. Experimental results demonstrate the potential of this system to enhance multilingual translation accuracy and scalability. By highlighting modular design and automated workflows, this paper sets the stage for further innovations in intelligent machine translation services.</li>
<li><strong>摘要：</strong>本文探讨了 Agent AI 和 LangGraph 在提高机器翻译 (MT) 自动化和效率方面的变革性作用。Agent 是模块化组件，旨在执行特定任务，例如在特定语言之间进行翻译，其专业化功能包括分别用于英语、法语和日语翻译的 TranslateEnAgent、TranslateFrenchAgent 和 TranslateJpAgent。这些代理利用大型语言模型 (LLM)（例如 GPT-4o）强大的语义功能来确保准确、上下文相关的翻译，同时保持模块化、可扩展性和上下文保留。LangGraph 是基于 LangChain 构建的基于图形的框架，它简化了这些代理及其工作流程的创建和管理。它支持动态状态管理，使代理能够维护对话上下文，并通过链接代理并促进其协作来自动化复杂的工作流程。凭借灵活性、开源社区支持以及与 LLM 的无缝集成，LangGraph 使代理能够提供高质量的翻译。 Agent AI 和 LangGraph 共同创建了一个有凝聚力的系统，其中 LangGraph 协调代理交互，确保高效地分析、路由和处理用户输入。实验结果证明了该系统在提高多语言翻译准确性和可扩展性方面的潜力。通过强调模块化设计和自动化工作流程，本文为智能机器翻译服务的进一步创新奠定了基础。</li>
</ul>

<h3>Title: Beyond the Binary: Capturing Diverse Preferences With Reward Regularization</h3>
<ul>
<li><strong>Authors: </strong>Vishakh Padmakumar, Chuanyang Jin, Hannah Rose Kirk, He He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03822">https://arxiv.org/abs/2412.03822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03822">https://arxiv.org/pdf/2412.03822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03822]] Beyond the Binary: Capturing Diverse Preferences With Reward Regularization(https://arxiv.org/abs/2412.03822)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed via public-facing interfaces to interact with millions of users, each with diverse preferences. Despite this, preference tuning of LLMs predominantly relies on reward models trained using binary judgments where annotators select the preferred choice out of pairs of model outputs. In this work, we argue that this reliance on binary choices does not capture the broader, aggregate preferences of the target user in real-world tasks. We propose a taxonomy that identifies two dimensions of subjectivity where different users disagree on the preferred output-namely, the Plurality of Responses to Prompts, where prompts allow for multiple correct answers, and the Indistinguishability of Responses, where candidate outputs are paraphrases of each other. We show that reward models correlate weakly with user preferences in these cases. As a first step to address this issue, we introduce a simple yet effective method that augments existing binary preference datasets with synthetic preference judgments to estimate potential user disagreement. Incorporating these via a margin term as a form of regularization during model training yields predictions that better align with the aggregate user preferences.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地通过面向公众的界面部署，以与数百万具有不同偏好的用户进行交互。尽管如此，LLM 的偏好调整主要依赖于使用二元判断训练的奖励模型，其中注释者从模型输出对中选择首选选项。在这项工作中，我们认为这种对二元选择的依赖并不能捕捉目标用户在现实世界任务中更广泛的总体偏好。我们提出了一种分类法，该分类法识别了两个主观维度，其中不同的用户对首选输出存在分歧 - 即对提示的多元响应，其中提示允许多个正确答案，以及响应的不可区分性，其中候选输出是彼此的释义。我们表明，在这些情况下，奖励模型与用户偏好的相关性较弱。作为解决这个问题的第一步，我们引入了一种简单而有效的方法，该方法使用合成偏好判断来增强现有的二元偏好数据集，以估计潜在的用户分歧。在模型训练期间通过边际项将这些作为正则化的一种形式，可以得到更符合总体用户偏好的预测。</li>
</ul>

<h3>Title: Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Shiwen Ni, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03847">https://arxiv.org/abs/2412.03847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03847">https://arxiv.org/pdf/2412.03847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03847]] Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration(https://arxiv.org/abs/2412.03847)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Intelligent dialogue systems are increasingly used in modern education and psychological counseling fields, but most existing systems are limited to a single domain, cannot deal with both educational and psychological issues, and often lack accuracy and professionalism when dealing with complex issues. To address these problems, this paper proposes an intelligent dialog system that combines educational and psychological counseling functions. The system consists of multiple AI agent, including security detection agent, intent identification agent, educational LLM agent, and psychological LLM agent, which work in concert to ensure the provision of accurate educational knowledge Q\&A and psychological support services. Specifically, the system recognizes user-input intentions through an intention classification model and invokes a retrieval-enhanced educational grand model and a psychological grand model fine-tuned with psychological data in order to provide professional educational advice and psychological support.</li>
<li><strong>摘要：</strong>智能对话系统在现代教育与心理咨询领域应用越来越广泛，但现有的系统大多局限于单一领域，无法同时处理教育和心理问题，在处理复杂问题时往往缺乏准确性和专业性。针对这些问题，本文提出了一种融合教育和心理咨询功能的智能对话系统。该系统由多个AI Agent组成，包括安全检测Agent、意图识别Agent、教育LLM Agent和心理LLM Agent，它们协同工作，确保提供精准的教育知识问答和心理支持服务。具体来说，系统通过意图分类模型识别用户输入意图，并调用检索增强的教育大模型和利用心理数据微调的心理大模型，提供专业的教育建议和心理支持。</li>
</ul>

<h3>Title: Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models</h3>
<ul>
<li><strong>Authors: </strong>Swarnava Sinha Roy, Ayan Kundu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03886">https://arxiv.org/abs/2412.03886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03886">https://arxiv.org/pdf/2412.03886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03886]] Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models(https://arxiv.org/abs/2412.03886)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Integrated Gradients is a well-known technique for explaining deep learning models. It calculates feature importance scores by employing a gradient based approach computing gradients of the model output with respect to input features and accumulating them along a linear path. While this works well for continuous features spaces, it may not be the most optimal way to deal with discrete spaces like word embeddings. For interpreting LLMs (Large Language Models), there exists a need for a non-linear path where intermediate points, whose gradients are to be computed, lie close to actual words in the embedding space. In this paper, we propose a method called Uniform Discretized Integrated Gradients (UDIG) based on a new interpolation strategy where we choose a favorable nonlinear path for computing attribution scores suitable for predictive language models. We evaluate our method on two types of NLP tasks- Sentiment Classification and Question Answering against three metrics viz Log odds, Comprehensiveness and Sufficiency. For sentiment classification, we have used the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for Question Answering, we have used the fine-tuned BERT model on SQuAD dataset. Our approach outperforms the existing methods in almost all the metrics.</li>
<li><strong>摘要：</strong>积分梯度是一种用于解释深度学习模型的著名技术。它通过采用基于梯度的方法计算特征重要性得分，即计算模型输出相对于输入特征的梯度并沿线性路径累积它们。虽然这种方法对于连续特征空间很有效，但它可能不是处理离散空间（如词嵌入）的最佳方法。为了解释 LLM（大型语言模型），需要一条非线性路径，其中要计算梯度的中间点靠近嵌入空间中的实际单词。在本文中，我们提出了一种基于新插值策略的称为均匀离散积分梯度 (UDIG) 的方法，其中我们选择一条有利的非线性路径来计算适用于预测语言模型的归因得分。我们根据三个指标（即对数几率、全面性和充分性）在两种类型的 NLP 任务（情绪分类和问答）上评估我们的方法。对于情绪分类，我们使用了 SST2、IMDb 和 Rotten Tomatoes 数据集进行基准测试；对于问答，我们在 SQuAD 数据集上使用经过微调的 BERT 模型。我们的方法在几乎所有指标上都优于现有方法。</li>
</ul>

<h3>Title: A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Xiachong Feng, Longxu Dou, Ella Li, Qinghao Wang, Haochuan Wang, Yu Guo, Chang Ma, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03920">https://arxiv.org/abs/2412.03920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03920">https://arxiv.org/pdf/2412.03920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03920]] A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios(https://arxiv.org/abs/2412.03920)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.</li>
<li><strong>摘要：</strong>博弈论场景已成为评估基于大型语言模型 (LLM) 的社交代理的社交智能的关键。虽然许多研究已经在这种环境下探索了这些代理，但缺乏对当前进展进行总结的全面调查。为了弥补这一差距，我们系统地回顾了博弈论场景中基于 LLM 的社交代理的现有研究。我们的调查将研究结果分为三个核心部分：游戏框架、社交代理和评估协议。游戏框架涵盖各种游戏场景，从以选择为重点的游戏到以交流为重点的游戏。社交代理部分探索代理的偏好、信念和推理能力。评估协议涵盖了用于评估代理性能的游戏无关指标和游戏特定指标。通过反思当前的研究并确定未来的研究方向，本调查提供了推进博弈论场景中社交代理的开发和评估的见解。</li>
</ul>

<h3>Title: MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yunhe Pang, Bo Chen, Fanjin Zhang, Yanghui Rao, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03930">https://arxiv.org/abs/2412.03930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03930">https://arxiv.org/pdf/2412.03930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03930]] MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model(https://arxiv.org/abs/2412.03930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of academic publications has exacerbated the issue of author name ambiguity in online digital libraries. Despite advances in name disambiguation algorithms, cumulative errors continue to undermine the reliability of academic systems. It is estimated that over 10% paper-author assignments are rectified when constructing the million-scale WhoIsWho benchmark. Existing endeavors to detect incorrect assignments are either semantic-based or graph-based approaches, which fall short of making full use of the rich text attributes of papers and implicit structural features defined via the co-occurrence of paper attributes. To this end, this paper introduces a structure-enhanced language model that combines key structural features from graph-based methods with fine-grained semantic features from rich paper attributes to detect incorrect assignments. The proposed model is trained with a highly effective multi-modal multi-turn instruction tuning framework, which incorporates task-guided instruction tuning, text-attribute modality, and structural modality. Experimental results demonstrate that our model outperforms previous approaches, achieving top performance on the leaderboard of KDD Cup 2024. Our code has been publicly available.</li>
<li><strong>摘要：</strong>学术出版物的快速增长加剧了在线数字图书馆中作者姓名歧义的问题。尽管姓名消歧算法取得了进展，但累积错误仍在继续破坏学术系统的可靠性。据估计，在构建百万级 WhoIsWho 基准时，超过 10% 的论文作者分配得到了纠正。现有的检测错误分配的方法要么是基于语义的，要么是基于图的，这些方法未能充分利用论文的丰富文本属性和通过论文属性的共现定义的隐式结构特征。为此，本文介绍了一种结构增强语言模型，该模型将基于图的方法的关键结构特征与丰富论文属性的细粒度语义特征相结合，以检测错误分配。所提出的模型使用高效的多模态多轮指令调整框架进行训练，该框架结合了任务引导指令调整、文本属性模态和结构模态。实验结果表明，我们的模型优于以前的方法，在 KDD Cup 2024 的排行榜上取得了最佳表现。我们的代码已经公开。</li>
</ul>

<h3>Title: MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM</h3>
<ul>
<li><strong>Authors: </strong>Changcheng Li, Xiangyu Wang, Qiuju Chen, Xiren Zhou, Huanhuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.03987">https://arxiv.org/abs/2412.03987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.03987">https://arxiv.org/pdf/2412.03987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.03987]] MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM(https://arxiv.org/abs/2412.03987)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown limitations in tasks requiring complex logical reasoning and multi-step problem-solving. To address these challenges, researchers have employed carefully designed prompts and flowcharts, simulating human cognitive processes to enhance LLM performance, such as the Chain of Thought approach. In this paper, we introduce MTMT (Multi-thinking Modes Tree), a novel method that interacts with LLMs to construct a thought tree, simulating various advanced cognitive processes, including but not limited to association, counterfactual thinking, task decomposition, and comparison. By breaking down the original complex task into simpler sub-questions, MTMT facilitates easier problem-solving for LLMs, enabling more effective utilization of the latent knowledge within LLMs. We evaluate the performance of MTMT under different parameter configurations, using GPT-4o mini as the base model. Our results demonstrate that integrating multiple modes of thinking significantly enhances the ability of LLMs to handle complex tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在需要复杂逻辑推理和多步骤问题解决的任务中表现出局限性。为了应对这些挑战，研究人员采用了精心设计的提示和流程图，模拟人类的认知过程来提高 LLM 的性能，例如思维链方法。在本文中，我们介绍了 MTMT（多思维模式树），这是一种与 LLM 交互以构建思维树的新方法，模拟各种高级认知过程，包括但不限于联想、反事实思维、任务分解和比较。通过将原始复杂任务分解为更简单的子问题，MTMT 有助于 LLM 更轻松地解决问题，从而更有效地利用 LLM 中的潜在知识。我们使用 GPT-4o mini 作为基础模型，评估了不同参数配置下 MTMT 的性能。我们的结果表明，整合多种思维模式可显著增强 LLM 处理复杂任务的能力。</li>
</ul>

<h3>Title: Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Lingfeng Ming, Bo Zeng, Chenyang Lyu, Tianqi Shi, Yu Zhao, Xue Yang, Yefeng Liu, Yiyu Wang, Linlong Xu, Yangyang Liu, Xiaohu Zhao, Hao Wang, Heng Liu, Hao Zhou, Huifeng Yin, Zifu Shang, Haijun Li, Longyue Wang, Weihua Luo, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04003">https://arxiv.org/abs/2412.04003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04003">https://arxiv.org/pdf/2412.04003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04003]] Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement(https://arxiv.org/abs/2412.04003)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 近年来取得了显著进展，但其优异表现仍然主要局限于世界主要语言，主要是英语。许多 LLM 在多语言任务中仍然面临挑战，尤其是在资源匮乏的语言方面。为了解决这个问题，我们推出了 Marco-LLM：用于跨语言增强的大规模多语言训练 LLM。我们为几种资源匮乏的语言收集了大量多语言数据，并使用 Qwen2 模型进行了广泛的持续预训练。这项努力产生了一个名为 Marco-LLM 的多语言 LLM。通过对 MMMLU、AGIEval、Belebele、Flores-200、XCOPA 等各种多语言基准的全面评估，Marco-LLM 已显示出比最先进的 LLM 有显著改进。此外，Marco-LLM 在任意机器翻译任务中都取得了显着的增强，展示了我们多语言 LLM 的有效性。 Marco-LLM 是开创性的多语言 LLM，旨在不仅在多语言任务（包括资源匮乏的语言）中表现出色，而且还能在英语和其他主要语言中保持强劲表现，从而缩小资源丰富和资源匮乏的语言能力之间的性能差距。通过连接语言，这一努力表明了我们致力于确保 LLM 在各种语言中准确运行的决心。</li>
</ul>

<h3>Title: Hostility Detection in UK Politics: A Dataset on Online Abuse Targeting MPs</h3>
<ul>
<li><strong>Authors: </strong>Mugdha Pandya, Mali Jin, Kalina Bontcheva, Diana Maynard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04046">https://arxiv.org/abs/2412.04046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04046">https://arxiv.org/pdf/2412.04046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04046]] Hostility Detection in UK Politics: A Dataset on Online Abuse Targeting MPs(https://arxiv.org/abs/2412.04046)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Numerous politicians use social media platforms, particularly X, to engage with their constituents. This interaction allows constituents to pose questions and offer feedback but also exposes politicians to a barrage of hostile responses, especially given the anonymity afforded by social media. They are typically targeted in relation to their governmental role, but the comments also tend to attack their personal identity. This can discredit politicians and reduce public trust in the government. It can also incite anger and disrespect, leading to offline harm and violence. While numerous models exist for detecting hostility in general, they lack the specificity required for political contexts. Furthermore, addressing hostility towards politicians demands tailored approaches due to the distinct language and issues inherent to each country (e.g., Brexit for the UK). To bridge this gap, we construct a dataset of 3,320 English tweets spanning a two-year period manually annotated for hostility towards UK MPs. Our dataset also captures the targeted identity characteristics (race, gender, religion, none) in hostile tweets. We perform linguistic and topical analyses to delve into the unique content of the UK political data. Finally, we evaluate the performance of pre-trained language models and large language models on binary hostility detection and multi-class targeted identity type classification tasks. Our study offers valuable data and insights for future research on the prevalence and nature of politics-related hostility specific to the UK.</li>
<li><strong>摘要：</strong>许多政客使用社交媒体平台（尤其是 X）与选民互动。这种互动允许选民提出问题并提供反馈，但也使政客面临一连串敌对回应，尤其是考虑到社交媒体提供的匿名性。他们通常因其政府角色而成为攻击目标，但评论也倾向于攻击他们的个人身份。这可能会损害政客的声誉并降低公众对政府的信任。它还会煽动愤怒和不尊重，导致线下伤害和暴力。虽然存在许多用于检测一般敌意的模型，但它们缺乏政治背景所需的特异性。此外，由于每个国家都有不同的语言和固有问题（例如英国脱欧），解决对政客的敌意需要量身定制的方法。为了弥补这一差距，我们构建了一个包含 3,320 条英文推文的数据集，这些推文跨越两年，并手动注释了对英国议员的敌意。我们的数据集还捕获了敌对推文中的目标身份特征（种族、性别、宗教、无）。我们进行语言和主题分析，深入研究英国政治数据的独特内容。最后，我们评估预训练语言模型和大型语言模型在二元敌意检测和多类别目标身份类型分类任务中的表现。我们的研究为未来研究英国政治相关敌意的普遍性和性质提供了宝贵的数据和见解。</li>
</ul>

<h3>Title: GEITje 7B Ultra: A Conversational Model for Dutch</h3>
<ul>
<li><strong>Authors: </strong>Bram Vanroy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04092">https://arxiv.org/abs/2412.04092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04092">https://arxiv.org/pdf/2412.04092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04092]] GEITje 7B Ultra: A Conversational Model for Dutch(https://arxiv.org/abs/2412.04092)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models have rapidly evolved, predominantly focusing on English while often neglecting extensive pretraining in other languages. This approach has required initiatives to adapt powerful, English-centric models to other linguistic contexts through finetuning. For Dutch, such a recent endeavour is ``GEITje'' a model originally derived from the English-based Mistral 7B. Building on this fundamental work, the current research extends the capabilities of GEITje by supervised finetuning on newly created high-quality synthetic conversational datasets, along with an additional preference alignment procedure on a synthetic feedback dataset. Both the developed models and the created datasets are openly available.</li>
<li><strong>摘要：</strong>语言模型发展迅速，主要侧重于英语，而往往忽略了对其他语言的大量预训练。这种方法需要通过微调将强大的以英语为中心的模型适应其他语言环境。对于荷兰语来说，最近的一项努力是“GEITje”，该模型最初源自基于英语的 Mistral 7B。在此基础工作的基础上，当前的研究通过对新创建的高质量合成对话数据集进行监督微调，以及对合成反馈数据集进行额外的偏好对齐程序，扩展了 GEITje 的功能。开发的模型和创建的数据集都是公开可用的。</li>
</ul>

<h3>Title: GRAF: Graph Retrieval Augmented by Facts for Legal Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Cristian-George Crăciun, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Mihaela-Claudia Cercel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04119">https://arxiv.org/abs/2412.04119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04119">https://arxiv.org/pdf/2412.04119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04119]] GRAF: Graph Retrieval Augmented by Facts for Legal Question Answering(https://arxiv.org/abs/2412.04119)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-trained Language Models (PLMs) have shown remarkable performances in recent years, setting a new paradigm for NLP research and industry. The legal domain has received some attention from the NLP community partly due to its textual nature. Some tasks from this domain are represented by question-answering (QA) tasks. This work explores the legal domain Multiple-Choice QA (MCQA) for a low-resource language. The contribution of this work is multi-fold. We first introduce JuRO, the first openly available Romanian legal MCQA dataset, comprising three different examinations and a number of 10,836 total questions. Along with this dataset, we introduce CROL, an organized corpus of laws that has a total of 93 distinct documents with their modifications from 763 time spans, that we leveraged in this work for Information Retrieval (IR) techniques. Moreover, we are the first to propose Law-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is derived from the aforementioned corpus. Lastly, we propose a novel approach for MCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive results with generally accepted SOTA methods and even exceeds them in most settings.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 近年来表现出色，为 NLP 研究和行业树立了新典范。法律领域受到了 NLP 社区的关注，部分原因是其文本性质。该领域的一些任务以问答 (QA) 任务为代表。这项工作探索了低资源语言的法律领域多项选择题问答 (MCQA)。这项工作的贡献是多方面的。我们首先介绍 JuRO，这是第一个公开的罗马尼亚法律 MCQA 数据集，包含三种不同的考试和总共 10,836 个问题。除了这个数据集之外，我们还介绍了 CROL，这是一个有组织的法律语料库，共有 93 份不同的文档及其来自 763 个时间跨度的修改，我们在本文中利用它进行信息检索 (IR) 技术。此外，我们首次提出了针对罗马尼亚语的知识图谱 (KG) Law-RoG，该 KG 源自上述语料库。最后，我们提出了一种新颖的 MCQA 方法，即基于事实的增强图检索 (GRAF)，该方法与普遍接受的 SOTA 方法相比取得了有竞争力的结果，甚至在大多数情况下超过了它们。</li>
</ul>

<h3>Title: Reducing Tool Hallucination via Reliability Alignment</h3>
<ul>
<li><strong>Authors: </strong>Hongshen Xu, Su Zhu, Zihan Wang, Hang Zheng, Da Ma, Ruisheng Cao, Shuai Fan, Lu Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04141">https://arxiv.org/abs/2412.04141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04141">https://arxiv.org/pdf/2412.04141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04141]] Reducing Tool Hallucination via Reliability Alignment(https://arxiv.org/abs/2412.04141)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have extended their capabilities beyond language generation to interact with external systems through tool calling, offering powerful potential for real-world applications. However, the phenomenon of tool hallucinations, which occur when models improperly select or misuse tools, presents critical challenges that can lead to flawed task execution and increased operational costs. This paper investigates the concept of reliable tool calling and highlights the necessity of addressing tool hallucinations. We systematically categorize tool hallucinations into two main types: tool selection hallucination and tool usage hallucination. To mitigate these issues, we propose a reliability-focused alignment framework that enhances the model's ability to accurately assess tool relevance and usage. By proposing a suite of evaluation metrics and evaluating on StableToolBench, we further demonstrate the effectiveness of our framework in mitigating tool hallucination and improving the overall system reliability of LLM tool calling.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已将其功能扩展到语言生成之外，通过工具调用与外部系统交互，为实际应用提供了强大的潜力。然而，当模型不当选择或误用工具时，就会出现工具幻觉现象，这带来了严峻的挑战，可能导致任务执行出现缺陷并增加运营成本。本文探讨了可靠工具调用的概念，并强调了解决工具幻觉的必要性。我们系统地将工具幻觉分为两种主要类型：工具选择幻觉和工具使用幻觉。为了缓解这些问题，我们提出了一个以可靠性为重点的对齐框架，以增强模型准确评估工具相关性和使用情况的能力。通过提出一套评估指标并在 StableToolBench 上进行评估，我们进一步证明了我们的框架在缓解工具幻觉和提高 LLM 工具调用的整体系统可靠性方面的有效性。</li>
</ul>

<h3>Title: AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in Dialectal Arabic</h3>
<ul>
<li><strong>Authors: </strong>Nathaniel R. Robinson, Shahd Abdelmoneim, Kelly Marchisio, Sebastian Ruder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04193">https://arxiv.org/abs/2412.04193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04193">https://arxiv.org/pdf/2412.04193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04193]] AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in Dialectal Arabic(https://arxiv.org/abs/2412.04193)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Dialectal Arabic (DA) varieties are under-served by language technologies, particularly large language models (LLMs). This trend threatens to exacerbate existing social inequalities and limits language modeling applications, yet the research community lacks operationalized LLM performance measurements in DA. We present a method that comprehensively evaluates LLM fidelity, understanding, quality, and diglossia in modeling DA. We evaluate nine LLMs in eight DA varieties across these four dimensions and provide best practice recommendations. Our evaluation suggests that LLMs do not produce DA as well as they understand it, but does not suggest deterioration in quality when they do. Further analysis suggests that current post-training can degrade DA capabilities, that few-shot examples can overcome this and other LLM deficiencies, and that otherwise no measurable features of input text correlate well with LLM DA performance.</li>
<li><strong>摘要：</strong>方言阿拉伯语 (DA) 变体在语言技术（尤其是大型语言模型 (LLM)）中服务不足。这种趋势有可能加剧现有的社会不平等并限制语言建模应用，但研究界缺乏在 DA 中可操作的 LLM 性能测量。我们提出了一种全面评估 LLM 在建模 DA 中的保真度、理解力、质量和双语现象的方法。我们从这四个维度评估了八种 DA 变体中的九种 LLM，并提供了最佳实践建议。我们的评估表明，LLM 生成的 DA 不如它们理解的那么好，但这并不意味着质量会下降。进一步的分析表明，当前的后训练会降低 DA 能力，少量样本可以克服这个和其他 LLM 缺陷，否则输入文本的可测量特征与 LLM DA 性能没有很好的相关性。</li>
</ul>

<h3>Title: A Context-aware Framework for Translation-mediated Conversations</h3>
<ul>
<li><strong>Authors: </strong>José Pombal, Sweta Agrawal, Patrick Fernandes, Emmanouil Zaranis, André F. T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04205">https://arxiv.org/abs/2412.04205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04205">https://arxiv.org/pdf/2412.04205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04205]] A Context-aware Framework for Translation-mediated Conversations(https://arxiv.org/abs/2412.04205)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Effective communication is fundamental to any interaction, yet challenges arise when participants do not share a common language. Automatic translation systems offer a powerful solution to bridge language barriers in such scenarios, but they introduce errors that can lead to misunderstandings and conversation breakdown. A key issue is that current systems fail to incorporate the rich contextual information necessary to resolve ambiguities and omitted details, resulting in literal, inappropriate, or misaligned translations. In this work, we present a framework to improve large language model-based translation systems by incorporating contextual information in bilingual conversational settings. During training, we leverage context-augmented parallel data, which allows the model to generate translations sensitive to conversational history. During inference, we perform quality-aware decoding with context-aware metrics to select the optimal translation from a pool of candidates. We validate both components of our framework on two task-oriented domains: customer chat and user-assistant interaction. Across both settings, our framework consistently results in better translations than state-of-the-art systems like GPT-4o and TowerInstruct, as measured by multiple automatic translation quality metrics on several language pairs. We also show that the resulting model leverages context in an intended and interpretable way, improving consistency between the conveyed message and the generated translations.</li>
<li><strong>摘要：</strong>有效沟通是任何互动的基础，但当参与者不讲同一种语言时，就会出现挑战。自动翻译系统提供了一种强大的解决方案来弥合这种情况下语言障碍，但它们会引入错误，从而导致误解和对话中断。一个关键问题是，当前的系统未能结合解决歧义和遗漏细节所需的丰富语境信息，导致翻译不准确、不恰当或不一致。在这项工作中，我们提出了一个框架，通过在双语对话环境中结合语境信息来改进基于大型语言模型的翻译系统。在训练过程中，我们利用语境增强的并行数据，这使模型能够生成对对话历史敏感的翻译。在推理过程中，我们使用语境感知指标执行质量感知解码，以从候选池中选择最佳翻译。我们在两个面向任务的领域验证了我们框架的两个组件：客户聊天和用户助理交互。在这两种设置下，我们的框架始终能产生比 GPT-4o 和 TowerInstruct 等先进系统更好的翻译效果，这是通过对多个语言对的多个自动翻译质量指标来衡量的。我们还表明，生成的模型以预期且可解释的方式利用上下文，从而提高了所传达信息与生成的翻译之间的一致性。</li>
</ul>

<h3>Title: Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots</h3>
<ul>
<li><strong>Authors: </strong>Maria Paola Priola</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04235">https://arxiv.org/abs/2412.04235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04235">https://arxiv.org/pdf/2412.04235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04235]] Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots(https://arxiv.org/abs/2412.04235)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>I combine detection and mitigation techniques to addresses hallucinations in Large Language Models (LLMs). Mitigation is achieved in a question-answering Retrieval-Augmented Generation (RAG) framework while detection is obtained by introducing the Negative Missing Information Scoring System (NMISS), which accounts for contextual relevance in responses. While RAG mitigates hallucinations by grounding answers in external data, NMISS refines the evaluation by identifying cases where traditional metrics incorrectly flag contextually accurate responses as hallucinations. I use Italian health news articles as context to evaluate LLM performance. Results show that Gemma2 and GPT-4 outperform the other models, with GPT-4 producing answers closely aligned with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral benefit significantly from NMISS, highlighting their ability to provide richer contextual information. This combined approach offers new insights into the reduction and more accurate assessment of hallucinations in LLMs, with applications in real-world healthcare tasks and other domains.</li>
<li><strong>摘要：</strong>我结合检测和缓解技术来解决大型语言模型 (LLM) 中的幻觉问题。缓解是通过问答检索增强生成 (RAG) 框架实现的，而检测是通过引入负缺失信息评分系统 (NMISS) 实现的，该系统考虑了响应中的上下文相关性。虽然 RAG 通过将答案建立在外部数据中来缓解幻觉，但 NMISS 通过识别传统指标错误地将上下文准确的响应标记为幻觉的情况来改进评估。我使用意大利健康新闻文章作为背景来评估 LLM 性能。结果表明，Gemma2 和 GPT-4 的表现优于其他模型，GPT-4 产生的答案与参考响应紧密相关。中层模型（例如 Llama2、Llama3 和 Mistral）从 NMISS 中受益匪浅，突显了它们提供更丰富上下文信息的能力。这种综合方法为减少和更准确地评估法学硕士中的幻觉提供了新的见解，并可应用于现实世界的医疗保健任务和其他领域。</li>
</ul>

<h3>Title: CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations</h3>
<ul>
<li><strong>Authors: </strong>Subash Neupane, Himanshu Tripathi, Shaswata Mitra, Sean Bozorgzad, Sudip Mittal, Shahram Rahimi, Amin Amirlatifi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04254">https://arxiv.org/abs/2412.04254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04254">https://arxiv.org/pdf/2412.04254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04254]] CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations(https://arxiv.org/abs/2412.04254)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents ClinicSum, a novel framework designed to automatically generate clinical summaries from patient-doctor conversations. It utilizes a two-module architecture: a retrieval-based filtering module that extracts Subjective, Objective, Assessment, and Plan (SOAP) information from conversation transcripts, and an inference module powered by fine-tuned Pre-trained Language Models (PLMs), which leverage the extracted SOAP data to generate abstracted clinical summaries. To fine-tune the PLM, we created a training dataset of consisting 1,473 conversations-summaries pair by consolidating two publicly available datasets, FigShare and MTS-Dialog, with ground truth summaries validated by Subject Matter Experts (SMEs). ClinicSum's effectiveness is evaluated through both automatic metrics (e.g., ROUGE, BERTScore) and expert human assessments. Results show that ClinicSum outperforms state-of-the-art PLMs, demonstrating superior precision, recall, and F-1 scores in automatic evaluations and receiving high preference from SMEs in human assessment, making it a robust solution for automated clinical summarization.</li>
<li><strong>摘要：</strong>本文介绍了 ClinicSum，这是一种旨在自动从医患对话中生成临床摘要的新型框架。它采用了双模块架构：基于检索的过滤模块，从对话记录中提取主观、客观、评估和计划 (SOAP) 信息；推理模块由经过微调的预训练语言模型 (PLM) 提供支持，利用提取的 SOAP 数据生成抽象的临床摘要。为了微调 PLM，我们通过整合两个公开可用的数据集 FigShare 和 MTS-Dialog 以及由主题专家 (SME) 验证的真实摘要，创建了一个由 1,473 个对话摘要对组成的训练数据集。ClinicSum 的有效性通过自动指标（例如 ROUGE、BERTScore）和专家人工评估来评估。结果表明，ClinicSum 的表现优于最先进的 PLM，在自动评估中表现出卓越的精确度、召回率和 F-1 分数，并在人工评估中获得 SME 的高度青睐，使其成为自动临床总结的强大解决方案。</li>
</ul>

<h3>Title: Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier</h3>
<ul>
<li><strong>Authors: </strong>John Dang, Shivalika Singh, Daniel D'souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04261">https://arxiv.org/abs/2412.04261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04261">https://arxiv.org/pdf/2412.04261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04261]] Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier(https://arxiv.org/abs/2412.04261)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce the Aya Expanse model family, a new generation of 8B and 32B parameter multilingual language models, aiming to address the critical challenge of developing highly performant multilingual models that match or surpass the capabilities of monolingual models. By leveraging several years of research at Cohere For AI and Cohere, including advancements in data arbitrage, multilingual preference training, and model merging, Aya Expanse sets a new state-of-the-art in multilingual performance. Our evaluations on the Arena-Hard-Auto dataset, translated into 23 languages, demonstrate that Aya Expanse 8B and 32B outperform leading open-weight models in their respective parameter classes, including Gemma 2, Qwen 2.5, and Llama 3.1, achieving up to a 76.6% win-rate. Notably, Aya Expanse 32B outperforms Llama 3.1 70B, a model with twice as many parameters, achieving a 54.0% win-rate. In this short technical report, we present extended evaluation results for the Aya Expanse model family and release their open-weights, together with a new multilingual evaluation dataset m-ArenaHard.</li>
<li><strong>摘要：</strong>我们推出了 Aya Expanse 模型系列，这是新一代 8B 和 32B 参数多语言模型，旨在解决开发高性能多语言模型的关键挑战，这些模型可以匹敌甚至超越单语模型的功能。通过利用 Cohere For AI 和 Cohere 多年的研究成果，包括数据套利、多语言偏好训练和模型合并方面的进步，Aya Expanse 在多语言性能方面创下了新高。我们对 Arena-Hard-Auto 数据集（翻译成 23 种语言）的评估表明，Aya Expanse 8B 和 32B 的表现优于各自参数类别中领先的开放权重模型，包括 Gemma 2、Qwen 2.5 和 Llama 3.1，胜率高达 76.6%。值得注意的是，Aya Expanse 32B 的表现优于参数多一倍的 Llama 3.1 70B，胜率达到 54.0%。在这份简短的技术报告中，我们展示了 Aya Expanse 模型系列的扩展评估结果，并发布了它们的公开权重，以及新的多语言评估数据集 m-ArenaHard。</li>
</ul>

<h3>Title: Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic</h3>
<ul>
<li><strong>Authors: </strong>Zaid Alyafeai, Michael Pieler, Hannah Teufel, Jonathan Tow, Marco Bellagente, Duy Phung, Nikhil Pinnaparaju, Reshinth Adithyan, Paulo Rocha, Maksym Zhuravinskyi, Carlos Riquelme</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04277">https://arxiv.org/abs/2412.04277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04277">https://arxiv.org/pdf/2412.04277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04277]] Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic(https://arxiv.org/abs/2412.04277)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive results in multiple domains of natural language processing (NLP) but are mainly focused on the English language. Recently, more LLMs have incorporated a larger proportion of multilingual text to represent low-resource languages. In Arabic NLP, several Arabic-centric LLMs have shown remarkable results on multiple benchmarks in the past two years. However, most Arabic LLMs have more than 7 billion parameters, which increases their hardware requirements and inference latency, when compared to smaller LLMs. This paper introduces Arabic Stable LM 1.6B in a base and chat version as a small but powerful Arabic-centric LLM. Our Arabic Stable LM 1.6B chat model achieves impressive results on several benchmarks beating multiple models with up to 8x the parameters. In addition, we show the benefit of mixing in synthetic instruction tuning data by augmenting our fine-tuning data with a large synthetic dialogue dataset.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言处理 (NLP) 的多个领域都取得了令人印象深刻的成果，但主要集中在英语上。最近，越来越多的 LLM 采用了更大比例的多语言文本来表示低资源语言。在阿拉伯语 NLP 中，过去两年中，几个以阿拉伯语为中心的 LLM 在多个基准测试中都取得了显著的成果。然而，大多数阿拉伯语 LLM 都有超过 70 亿个参数，与较小的 LLM 相比，这增加了它们的硬件要求和推理延迟。本文介绍了阿拉伯语稳定 LM 1.6B 的基本版本和聊天版本，这是一个小而强大的以阿拉伯语为中心的 LLM。我们的阿拉伯语稳定 LM 1.6B 聊天模型在多个基准测试中取得了令人印象深刻的成果，击败了参数多达 8 倍的多个模型。此外，我们通过使用大型合成对话数据集扩充我们的微调数据，展示了混合合成指令调整数据的好处。</li>
</ul>

<h3>Title: Evolutionary Pre-Prompt Optimization for Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mathurin Videau, Alessandro Leite, Marc Schoenauer, Olivier Teytaud</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04291">https://arxiv.org/abs/2412.04291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04291">https://arxiv.org/pdf/2412.04291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04291]] Evolutionary Pre-Prompt Optimization for Mathematical Reasoning(https://arxiv.org/abs/2412.04291)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent advancements have highlighted that large language models (LLMs), when given a small set of task-specific examples, demonstrate remarkable proficiency, a capability that extends to complex reasoning tasks. In particular, the combination of few-shot learning with the chain-of-thought (CoT) approach has been pivotal in steering models towards more logically consistent conclusions. This paper explores the optimization of example selection for designing effective CoT pre-prompts and shows that the choice of the optimization algorithm, typically in favor of comparison-based methods such as evolutionary computation, significantly enhances efficacy and feasibility. Specifically, thanks to a limited exploitative and overfitted optimization, Evolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the naive few-shot approach exceeding 10 absolute points in exact match scores on benchmark datasets such as GSM8k and MathQA. These gains are consistent across various contexts and are further amplified when integrated with self-consistency (SC)</li>
<li><strong>摘要：</strong>最近的进展表明，大型语言模型 (LLM) 在给定一小组特定于任务的示例时表现出非凡的熟练程度，这种能力可以扩展到复杂的推理任务。特别是，将少量学习与思维链 (CoT) 方法相结合对于引导模型得出更合乎逻辑的一致结论至关重要。本文探讨了设计有效 CoT 预提示的示例选择优化，并表明优化算法的选择（通常倾向于基于比较的方法，例如进化计算）可以显着提高效率和可行性。具体而言，由于有限的利用性和过度拟合优化，进化预提示优化 (EPPO) 比简单的少量方法有所改进，在基准数据集（例如 GSM8k 和 MathQA）上的精确匹配得分中绝对分数超过 10 分。这些收益在各种情况下都是一致的，并且在与自洽 (SC) 结合时会进一步放大</li>
</ul>

<h3>Title: ALMA: Alignment with Minimal Annotation</h3>
<ul>
<li><strong>Authors: </strong>Michihiro Yasunaga, Leonid Shamis, Chunting Zhou, Andrew Cohen, Jason Weston, Luke Zettlemoyer, Marjan Ghazvininejad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04305">https://arxiv.org/abs/2412.04305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04305">https://arxiv.org/pdf/2412.04305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04305]] ALMA: Alignment with Minimal Annotation(https://arxiv.org/abs/2412.04305)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent approaches to large language model (LLM) alignment typically require millions of human annotations or rely on external aligned models for synthetic data generation. This paper introduces ALMA: Alignment with Minimal Annotation, demonstrating that effective alignment can be achieved using only 9,000 labeled examples -- less than 1% of conventional approaches. ALMA generates large amounts of high-quality synthetic alignment data through new techniques: diverse prompt synthesis via few-shot learning, diverse response generation with multiple model checkpoints, and judge (reward model) enhancement through score aggregation and self-distillation. Using only a pretrained Llama3 base model, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves performance close to Llama3-Instruct across diverse alignment benchmarks (e.g., 0.1% difference on AlpacaEval 2.0 score). These results are achieved with a multi-round, self-bootstrapped data synthesis and training recipe that continues to improve for 10 rounds, surpassing the typical 3-round ceiling of previous methods. These results suggest that base models already possess sufficient knowledge for effective alignment, and that synthetic data generation methods can expose it.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 对齐的最新方法通常需要数百万条人工注释或依赖外部对齐模型来生成合成数据。本文介绍了 ALMA：使用最少注释进行对齐，表明仅使用 9,000 个标记示例即可实现有效对齐——不到传统方法的 1%。ALMA 通过新技术生成大量高质量的合成对齐数据：通过少量学习进行多样化提示合成、使用多个模型检查点生成多样化响应以及通过分数聚合和自我提炼增强判断（奖励模型）。仅使用预训练的 Llama3 基础模型、5,000 个 SFT 示例和 4,000 个判断注释，ALMA 在各种对齐基准上实现了接近 Llama3-Instruct 的性能（例如，AlpacaEval 2.0 分数相差 0.1%）。这些结果是通过多轮自引导数据合成和训练配方实现的，该配方持续改进了 10 轮，超过了以前方法通常的 3 轮上限。这些结果表明，基础模型已经拥有足够的知识来进行有效对齐，而合成数据生成方法可以揭示这些知识。</li>
</ul>

<h3>Title: The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Fredrik Carlsson, Fangyu Liu, Daniel Ward, Murathan Kurfali, Joakim Nivre</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04318">https://arxiv.org/abs/2412.04318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04318">https://arxiv.org/pdf/2412.04318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04318]] The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation(https://arxiv.org/abs/2412.04318)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces the counter-intuitive generalization results of overfitting pre-trained large language models (LLMs) on very small datasets. In the setting of open-ended text generation, it is well-documented that LLMs tend to generate repetitive and dull sequences, a phenomenon that is especially apparent when generating using greedy decoding. This issue persists even with state-of-the-art LLMs containing billions of parameters, trained via next-token prediction on large datasets. We find that by further fine-tuning these models to achieve a near-zero training loss on a small set of samples -- a process we refer to as hyperfitting -- the long-sequence generative capabilities are greatly enhanced. Greedy decoding with these Hyperfitted models even outperform Top-P sampling over long-sequences, both in terms of diversity and human preferences. This phenomenon extends to LLMs of various sizes, different domains, and even autoregressive image generation. We further find this phenomena to be distinctly different from that of Grokking and double descent. Surprisingly, our experiments indicate that hyperfitted models rarely fall into repeating sequences they were trained on, and even explicitly blocking these sequences results in high-quality output. All hyperfitted models produce extremely low-entropy predictions, often allocating nearly all probability to a single token.</li>
<li><strong>摘要：</strong>本文介绍了在非常小的数据集上过度拟合预训练大型语言模型 (LLM) 的反直觉泛化结果。在开放式文本生成环境中，有充分证据表明 LLM 倾向于生成重复且枯燥的序列，这种现象在使用贪婪解码生成时尤其明显。即使使用包含数十亿个参数的最先进的 LLM，通过对大型数据集进行下一个标记预测进行训练，这个问题仍然存在。我们发现，通过进一步微调这些模型以在一小组样本上实现接近零的训练损失——我们将这一过程称为超拟合——长序列生成能力得到极大增强。使用这些超拟合模型的贪婪解码甚至在多样性和人类偏好方面都优于长序列上的 Top-P 采样。这种现象扩展到各种大小、不同领域的 LLM，甚至自回归图像生成。我们进一步发现这种现象与 Grokking 和双下降的现象截然不同。令人惊讶的是，我们的实验表明，超拟合模型很少会陷入训练时所用的重复序列，甚至明确阻止这些序列也会产生高质量的输出。所有超拟合模型都会产生极低熵的预测，通常会将几乎所有概率分配给单个标记。</li>
</ul>

<h3>Title: Understanding Student Sentiment on Mental Health Support in Colleges Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Palak Sood, Chengyang He, Divyanshu Gupta, Yue Ning, Ping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04326">https://arxiv.org/abs/2412.04326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04326">https://arxiv.org/pdf/2412.04326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04326]] Understanding Student Sentiment on Mental Health Support in Colleges Using Large Language Models(https://arxiv.org/abs/2412.04326)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Mental health support in colleges is vital in educating students by offering counseling services and organizing supportive events. However, evaluating its effectiveness faces challenges like data collection difficulties and lack of standardized metrics, limiting research scope. Student feedback is crucial for evaluation but often relies on qualitative analysis without systematic investigation using advanced machine learning methods. This paper uses public Student Voice Survey data to analyze student sentiments on mental health support with large language models (LLMs). We created a sentiment analysis dataset, SMILE-College, with human-machine collaboration. The investigation of both traditional machine learning methods and state-of-the-art LLMs showed the best performance of GPT-3.5 and BERT on this new dataset. The analysis highlights challenges in accurately predicting response sentiments and offers practical insights on how LLMs can enhance mental health-related research and improve college mental health services. This data-driven approach will facilitate efficient and informed mental health support evaluation, management, and decision-making.</li>
<li><strong>摘要：</strong>大学心理健康支持通过提供咨询服务和组织支持活动对教育学生至关重要。然而，评估其有效性面临着数据收集困难和缺乏标准化指标等挑战，限制了研究范围。学生反馈对于评估至关重要，但通常依赖于定性分析，而没有使用先进的机器学习方法进行系统调查。本文使用公开的学生声音调查数据，通过大型语言模型 (LLM) 分析学生对心理健康支持的情绪。我们通过人机协作创建了一个情绪分析数据集 SMILE-College。对传统机器学习方法和最先进的 LLM 的调查显示，GPT-3.5 和 BERT 在这个新数据集上表现最佳。该分析强调了准确预测反应情绪的挑战，并提供了关于 LLM 如何加强心理健康相关研究和改善大学心理健康服务的实用见解。这种数据驱动的方法将促进高效和明智的心理健康支持评估、管理和决策。</li>
</ul>

<h3>Title: Retrieval-Augmented Machine Translation with Unstructured Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Jiaan Wang, Fandong Meng, Yingxue Zhang, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04342">https://arxiv.org/abs/2412.04342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04342">https://arxiv.org/pdf/2412.04342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04342]] Retrieval-Augmented Machine Translation with Unstructured Knowledge(https://arxiv.org/abs/2412.04342)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) introduces additional information to enhance large language models (LLMs). In machine translation (MT), previous work typically retrieves in-context examples from paired MT corpora, or domain-specific knowledge from knowledge graphs, to enhance models' MT ability. However, a large amount of world knowledge is organized in unstructured documents, and might not be fully paired across different languages. In this paper, we study retrieval-augmented MT using unstructured documents. Specifically, we build RAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples collected via GPT-4o and human translators. Besides, documents from different languages are also provided to supply the knowledge to these samples. Based on RAGtrans, we further propose a multi-task training method to teach LLMs how to use information from multilingual documents during their translation. The method uses existing multilingual corpora to create auxiliary training objectives without additional labeling requirements. Extensive experiments show that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 引入了额外的信息来增强大型语言模型 (LLM)。在机器翻译 (MT) 中，以前的工作通常从成对的机器翻译语料库中检索上下文示例，或从知识图谱中检索领域特定知识，以增强模型的机器翻译能力。然而，大量的世界知识是以非结构化文档的形式组织的，并且可能无法在不同语言之间完全配对。在本文中，我们使用非结构化文档研究检索增强机器翻译。具体来说，我们构建了 RAGtrans，这是第一个训练和评估 LLM 检索增强机器翻译能力的基准。RAGtrans 包含通过 GPT-4o 和人工翻译收集的 79K 机器翻译样本。此外，还提供来自不同语言的文档来为这些样本提供知识。基于 RAGtrans，我们进一步提出了一种多任务训练方法，教 LLM 如何在翻译过程中使用来自多语言文档的信息。该方法使用现有的多语言语料库来创建辅助训练目标，而无需额外的标记要求。大量实验表明，该方法可将 LLM 的 BLEU 分数提高 1.58-3.09 并将 COMET 分数提高 1.00-2.03。</li>
</ul>

<h3>Title: Establishing Task Scaling Laws via Compute-Efficient Model Ladders</h3>
<ul>
<li><strong>Authors: </strong>Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, Hannaneh Hajishirzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04403">https://arxiv.org/abs/2412.04403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04403">https://arxiv.org/pdf/2412.04403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04403]] Establishing Task Scaling Laws via Compute-Efficient Model Ladders(https://arxiv.org/abs/2412.04403)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. We train a set of small-scale "ladder" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws.</li>
<li><strong>摘要：</strong>我们开发了任务缩放律和模型阶梯，以预测过度训练环境中预训练语言模型 (LM) 的单个任务性能。语言建模损失的标准幂律无法准确模拟任务性能。因此，我们利用两步预测方法：首先使用模型和数据大小来预测特定于任务的损失，然后使用此任务损失来预测任务性能。我们训练一组小规模的“阶梯”模型，收集数据点以拟合两个预测步骤的参数化函数，并对两个目标模型进行预测：一个训练到 4T 标记的 7B 模型和一个训练到 5T 标记的 13B 模型。训练阶梯模型仅花费目标模型所用计算量的 1%。在以排序分类格式编写的四个多项选择任务中，我们可以预测两个目标模型的准确率在 2 个绝对误差点以内。我们对其他四个任务的预测误差更高（平均绝对误差 6.9），并且发现这些任务通常是任务指标方差更大的任务。我们还发现，使用较少的计算来训练较少的阶梯模型往往会降低预测效果。最后，我们通过经验表明，我们的设计选择和两步方法在建立缩放定律方面具有卓越的性能。</li>
</ul>

<h3>Title: Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04454">https://arxiv.org/abs/2412.04454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04454">https://arxiv.org/pdf/2412.04454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04454]] Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction(https://arxiv.org/abs/2412.04454)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at this https URL.</li>
<li><strong>摘要：</strong>图形用户界面 (GUI) 对人机交互至关重要，但由于视觉环境的复杂性和多变性，自动化 GUI 任务仍然具有挑战性。现有方法通常依赖于 GUI 的文本表示，这会限制通用性、效率和可扩展性。在本文中，我们介绍了 Aguvis，这是一个统一的纯视觉框架，适用于跨各种平台运行的自主 GUI 代理。我们的方法利用基于图像的观察和将自然语言中的指令应用于视觉元素，并采用一致的动作空间来确保跨平台泛化。为了解决以前工作的局限性，我们在模型中集成了明确的规划和推理，增强了其自主导航和与复杂数字环境交互的能力。我们构建了一个大规模的 GUI 代理轨迹数据集，结合了多模态推理和基础，并采用了一个两阶段训练流程，首先关注一般的 GUI 基础，然后是规划和推理。通过全面的实验，我们证明了 Aguvis 在离线和真实在线场景中都超越了之前最先进的方法，据我们所知，它实现了第一个完全自主的纯视觉 GUI 代理，能够独立执行任务而无需与外部闭源模型协作。我们在此 https URL 上开源了所有数据集、模型和训练配方，以方便未来的研究。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
