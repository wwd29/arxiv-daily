<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-28</h1>
<h3>Title: GCoder: Improving Large Language Model for Generalized Graph Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Qifan Zhang, Xiaobin Hong, Jianheng Tang, Nuo Chen, Yuhan Li, Wenzhong Li, Jing Tang, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19084">https://arxiv.org/abs/2410.19084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19084">https://arxiv.org/pdf/2410.19084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19084]] GCoder: Improving Large Language Model for Generalized Graph Problem Solving(https://arxiv.org/abs/2410.19084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong reasoning abilities, making them suitable for complex tasks such as graph computation. Traditional reasoning steps paradigm for graph problems is hindered by unverifiable steps, limited long-term reasoning, and poor generalization to graph variations. To overcome these limitations, we introduce GCoder, a code-based LLM designed to enhance problem-solving in generalized graph computation problems. Our method involves constructing an extensive training dataset, GraphWild, featuring diverse graph formats and algorithms. We employ a multi-stage training process, including Supervised Fine-Tuning (SFT) and Reinforcement Learning from Compiler Feedback (RLCF), to refine model capabilities. For unseen tasks, a hybrid retrieval technique is used to augment performance. Experiments demonstrate that GCoder outperforms GPT-4o, with an average accuracy improvement of 16.42% across various graph computational problems. Furthermore, GCoder efficiently manages large-scale graphs with millions of nodes and diverse input formats, overcoming the limitations of previous models focused on the reasoning steps paradigm. This advancement paves the way for more intuitive and effective graph problem-solving using LLMs. Code and data are available at here: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展示出强大的推理能力，使其适合于图形计算等复杂任务。图问题的传统推理步骤范式受到不可验证步骤、长期推理有限以及对图形变化的泛化能力较差的阻碍。为了克服这些限制，我们引入了 GCoder，这是一种基于代码的 LLM，旨在增强广义图计算问题的求解能力。我们的方法涉及构建一个广泛的训练数据集 GraphWild，其中包含多种图形格式和算法。我们采用多阶段训练过程，包括监督微调 (SFT) 和编译器反馈强化学习 (RLCF)，以改进模型功能。对于看​​不见的任务，使用混合检索技术来增强性能。实验表明，GCoder 优于 GPT-4o，在各种图形计算问题中的平均准确率提高了 16.42%。此外，GCoder 可以高效地管理具有数百万个节点和多种输入格式的大规模图，从而克服了以前专注于推理步骤范式的模型的局限性。这一进步为使用 LLM 更直观、更有效地解决图问题铺平了道路。代码和数据可在此处获取：此 https URL。</li>
</ul>

<h3>Title: LLM Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Dylan Wilson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19117">https://arxiv.org/abs/2410.19117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19117">https://arxiv.org/pdf/2410.19117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19117]] LLM Tree Search(https://arxiv.org/abs/2410.19117)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This project aims to investigate a novel sequence generation method inspired by the AlphaGo paradigm, adapting it for use with large language models (LLMs). The proposed approach involves creating search trees of different possible completions and evaluating these completions based on model confidence. By considering various paths in the search tree and scoring them according to the model's confidence in each completion, we can generate diverse and high-quality sequences. This research explores the implementation of this paradigm by using confidence as a proxy for response quality akin to beam search \citep{vijayakumar2016diverse}. The primary goal of this paper is to outline the paradigm and demonstrate its potential, rather than focusing on achieving perfect results. The paper will outline the reasons why we believe this paradigm has the potential to improve LLMs in the following manners: 1) increase output quality, 2) decrease errors, 3) eliminate or reduce the compound error problems, 4) generate diverse and creative completions, 5) allow for iterative problem-solving, and 6) self-training. We expect this approach to yield a set of diverse and coherent sequences, offering insights into balancing exploration and exploitation in sequence generation. Potential applications include creative text generation tasks, such as storytelling and content creation, as well as other natural language processing domains, like machine translation and automated summarization. The goal is that the model will be far more effective as it will be able to consider many possible variations allowing it to find the ideal completion. This research aims to contribute to the understanding of effective search strategies in sequence generation and their impact on generating high-quality, varied textual outputs.</li>
<li><strong>摘要：</strong>该项目旨在研究一种受 AlphaGo 范式启发的新型序列生成方法，使其适用于大型语言模型 (LLM)。所提出的方法涉及创建不同可能完成的搜索树，并根据模型置信度评估这些完成。通过考虑搜索树中的各种路径并根据模型对每个完成的置信度对其进行评分，我们可以生成多样化且高质量的序列。本研究探索了此范式的实现，方法是使用置信度作为类似于波束搜索 \citep{vijayakumar2016diverse} 的响应质量代理。本文的主要目标是概述范式并展示其潜力，而不是专注于实现完美的结果。本文将概述我们认为此范式有可能通过以下方式改进 LLM 的原因：1) 提高输出质量，2) 减少错误，3) 消除或减少复合错误问题，4) 生成多样化和创造性的完成，5) 允许迭代解决问题，以及 6) 自我训练。我们期望这种方法能够产生一组多样化且连贯的序列，为平衡序列生成中的探索和利用提供见解。潜在的应用包括创造性的文本生成任务，例如讲故事和内容创作，以及其他自然语言处理领域，例如机器翻译和自动摘要。目标是该模型将更加有效，因为它将能够考虑许多可能的变化，从而找到理想的完成。这项研究旨在帮助理解序列生成中的有效搜索策略及其对生成高质量、多样化的文本输出的影响。</li>
</ul>

<h3>Title: Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design</h3>
<ul>
<li><strong>Authors: </strong>Ruisi Cai, Yeonju Ro, Geon-Woo Kim, Peihao Wang, Babak Ehteshami Bejnordi, Aditya Akella, Zhangyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19123">https://arxiv.org/abs/2410.19123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19123">https://arxiv.org/pdf/2410.19123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19123]] Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design(https://arxiv.org/abs/2410.19123)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的激增导致了混合专家 (MoE) 架构的采用，该架构动态利用专门的子网络来提高效率和性能。尽管 MoE 模型有很多好处，但由于模型架构和系统策略之间的设计选择不一致，它在推理过程中面临着重大挑战，包括内存管理效率低下和批处理不理想。此外，从头开始训练 MoE 的传统方法在成本方面越来越高昂。在本文中，我们提出了一个新颖的框架 Read-ME，它将预先训练的密集 LLM 转换为较小的 MoE 模型（与“升级”通用 MoE 不同），避免了从头开始训练的高成本。我们的方法采用激活稀疏性来提取专家。为了组成专家，我们研究了广泛采用的分层路由器设计并展示了其冗余，因此我们引入了与 MoE 主干分离的预门控路由器，以促进系统友好的预计算和前瞻调度，增强专家感知的批处理和缓存。因此，我们的协同设计解决了算法和系统方面的关键差距，为资源受限环境中的 LLM 推理建立了可扩展且高效的替代方案。Read-ME 的表现优于其他类似规模的流行开源密集模型，在 MMLU 上实现了高达 10.1% 的改进，并将平均端到端延迟提高了 6.1%。代码可从以下网址获取：此 https URL。</li>
</ul>

<h3>Title: Retrieving Implicit and Explicit Emotional Events Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guimin Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19128">https://arxiv.org/abs/2410.19128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19128">https://arxiv.org/pdf/2410.19128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19128]] Retrieving Implicit and Explicit Emotional Events Using Large Language Models(https://arxiv.org/abs/2410.19128)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have garnered significant attention in recent years due to their impressive performance. While considerable research has evaluated these models from various perspectives, the extent to which LLMs can perform implicit and explicit emotion retrieval remains largely unexplored. To address this gap, this study investigates LLMs' emotion retrieval capabilities in commonsense. Through extensive experiments involving multiple models, we systematically evaluate the ability of LLMs on emotion retrieval. Specifically, we propose a supervised contrastive probing method to verify LLMs' performance for implicit and explicit emotion retrieval, as well as the diversity of the emotional events they retrieve. The results offer valuable insights into the strengths and limitations of LLMs in handling emotion retrieval.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其出色的性能而近年来备受关注。虽然大量研究从各个角度评估了这些模型，但 LLM 执行隐性和显性情绪检索的程度仍未得到充分探索。为了填补这一空白，本研究调查了 LLM 在常识中的情绪检索能力。通过涉及多个模型的大量实验，我们系统地评估了 LLM 在情绪检索方面的能力。具体来说，我们提出了一种监督对比探测方法来验证 LLM 在隐性和显性情绪检索方面的表现，以及它们检索的情绪事件的多样性。结果为 LLM 在处理情绪检索方面的优势和局限性提供了宝贵的见解。</li>
</ul>

<h3>Title: Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback</h3>
<ul>
<li><strong>Authors: </strong>Lester James V. Miranda, Yizhong Wang, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A. Smith, Hannaneh Hajishirzi, Pradeep Dasigi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19133">https://arxiv.org/abs/2410.19133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19133">https://arxiv.org/pdf/2410.19133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19133]] Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback(https://arxiv.org/abs/2410.19133)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future.</li>
<li><strong>摘要：</strong>从人类反馈中学习使得语言模型 (LM) 与人类偏好保持一致。然而，直接收集人类偏好可能成本高昂、耗时长，并且方差很大。一个有吸引力的替代方案是从 LM 中提取偏好作为合成注释的来源，因为它们比人类注释更一致、更便宜、扩展性更好；然而，它们也容易出现偏差和错误。在这项工作中，我们引入了一个路由框架，该框架结合了来自人类和 LM 的输入，以实现更好的注释质量，同时降低人类注释的总成本。我们方法的关键是识别将从人类注释中受益的偏好实例。我们将其表述为优化问题：给定一个偏好数据集和一个评估指标，我们训练一个性能预测模型来预测奖励模型在人类和 LM 注释的任意组合上的性能，并采用路由策略来选择最大化预测性能的组合。我们在 MultiPref 上训练性能预测模型，这是一个新的偏好数据集，包含 10K 个实例，与人类和 LM 标签配对。我们表明，使用我们的路由框架，LM 和直接人类偏好的混合选择比单独使用其中一种方式可以获得更好的奖励模型性能。我们在另外三个数据集上模拟了选择性人类偏好收集，并表明我们的方法可以很好地推广到这三个数据集。我们分析了路由模型中的特征，以确定可以从人类反馈中受益的实例的特征，例如具有中等安全问题或中等意图复杂性的提示。我们发布了本研究中使用的数据集、注释平台和源代码，以促进未来更高效、更准确的偏好收集。</li>
</ul>

<h3>Title: AlignCap: Aligning Speech Emotion Captioning to Human Preferences</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Liang, Haoxiang Shi, Hanhui Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19134">https://arxiv.org/abs/2410.19134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19134">https://arxiv.org/pdf/2410.19134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19134]] AlignCap: Aligning Speech Emotion Captioning to Human Preferences(https://arxiv.org/abs/2410.19134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Speech Emotion Captioning (SEC) has gradually become an active research task. The emotional content conveyed through human speech are often complex, and classifying them into fixed categories may not be enough to fully capture speech emotions. Describing speech emotions through natural language may be a more effective approach. However, existing SEC methods often produce hallucinations and lose generalization on unseen speech. To overcome these problems, we propose AlignCap, which Aligning Speech Emotion Captioning to Human Preferences based on large language model (LLM) with two properties: 1) Speech-Text Alignment, which minimizing the divergence between the LLM's response prediction distributions for speech and text inputs using knowledge distillation (KD) Regularization. 2) Human Preference Alignment, where we design Preference Optimization (PO) Regularization to eliminate factuality and faithfulness hallucinations. We also extract emotional clues as a prompt for enriching fine-grained information under KD-Regularization. Experiments demonstrate that AlignCap presents stronger performance to other state-of-the-art methods on Zero-shot SEC task.</li>
<li><strong>摘要：</strong>语音情感字幕（SEC）逐渐成为一项活跃的研究任务。通过人类语音传达的情感内容通常很复杂，将它们归类为固定的类别可能不足以完全捕捉语音情感。通过自然语言描述语音情感可能是一种更有效的方法。然而，现有的 SEC 方法经常产生幻觉，并且对看不见的语音失去泛化能力。为了克服这些问题，我们提出了 AlignCap，它基于大型语言模型（LLM）将语音情感字幕与人类偏好对齐，具有两个属性：1）语音-文本对齐，使用知识蒸馏（KD）正则化最小化 LLM 对语音和文本输入的响应预测分布之间的差异。2）人类偏好对齐，我们设计了偏好优化（PO）正则化来消除事实和忠诚幻觉。我们还提取情感线索作为在 KD 正则化下丰富细粒度信息的提示。实验表明，AlignCap 在 Zero-shot SEC 任务上比其他最先进的方法表现出更强的性能。</li>
</ul>

<h3>Title: Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use</h3>
<ul>
<li><strong>Authors: </strong>Mohit Chandra, Siddharth Sriraman, Gaurav Verma, Harneet Singh Khanuja, Jose Suarez Campayo, Zihang Li, Michael L. Birnbaum, Munmun De Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19155">https://arxiv.org/abs/2410.19155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19155">https://arxiv.org/pdf/2410.19155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19155]] Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use(https://arxiv.org/abs/2410.19155)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Adverse Drug Reactions (ADRs) from psychiatric medications are the leading cause of hospitalizations among mental health patients. With healthcare systems and online communities facing limitations in resolving ADR-related issues, Large Language Models (LLMs) have the potential to fill this gap. Despite the increasing capabilities of LLMs, past research has not explored their capabilities in detecting ADRs related to psychiatric medications or in providing effective harm reduction strategies. To address this, we introduce the Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment (ADRA) framework to systematically evaluate LLM performance in detecting ADR expressions and delivering expert-aligned mitigation strategies. Our analyses show that LLMs struggle with understanding the nuances of ADRs and differentiating between types of ADRs. While LLMs align with experts in terms of expressed emotions and tone of the text, their responses are more complex, harder to read, and only 70.86% aligned with expert strategies. Furthermore, they provide less actionable advice by a margin of 12.32% on average. Our work provides a comprehensive benchmark and evaluation framework for assessing LLMs in strategy-driven tasks within high-risk domains.</li>
<li><strong>摘要：</strong>精神类药物的不良反应 (ADR) 是导致精神疾病患者住院的主要原因。由于医疗保健系统和在线社区在解决 ADR 相关问题方面面临限制，大型语言模型 (LLM) 有可能填补这一空白。尽管 LLM 的功能不断增强，但过去的研究尚未探索其在检测与精神类药物相关的 ADR 或提供有效的减害策略方面的能力。为了解决这个问题，我们引入了 Psych-ADR 基准和不良药物反应评估 (ADRA) 框架，以系统地评估 LLM 在检测 ADR 表达和提供专家一致的缓解策略方面的表现。我们的分析表明，LLM 很难理解 ADR 的细微差别并区分不同类型的 ADR。虽然 LLM 在表达的情绪和文本语气方面与专家一致，但他们的回答更复杂、更难阅读，并且只有 70.86% 与专家策略一致。此外，他们提供的建议可操作性较差，平均低 12.32%。我们的工作为评估高风险领域中战略驱动任务中的法学硕士提供了全面的基准和评估框架。</li>
</ul>

<h3>Title: Enriching GNNs with Text Contextual Representations for Detecting Disinformation Campaigns on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Bruno Croso Cunha da Silva, Thomas Palmeira Ferraz, Roseli De Deus Lopes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19193">https://arxiv.org/abs/2410.19193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19193">https://arxiv.org/pdf/2410.19193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19193]] Enriching GNNs with Text Contextual Representations for Detecting Disinformation Campaigns on Social Media(https://arxiv.org/abs/2410.19193)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Disinformation on social media poses both societal and technical challenges. While previous studies have integrated textual information into propagation networks, they have yet to fully leverage the advancements in Transformer-based language models for high-quality contextual text representations. This work investigates the impact of incorporating textual features into Graph Neural Networks (GNNs) for fake news detection. Our experiments demonstrate that contextual representations improve performance by 9.3% in Macro F1 over static ones and 33.8% over GNNs without textual features. However, noisy data augmentation degrades performance and increases instability. We expect our methodology to open avenues for further research, and all code is made publicly available.</li>
<li><strong>摘要：</strong>社交媒体上的虚假信息带来了社会和技术挑战。虽然先前的研究已将文本信息集成到传播网络中，但它们尚未充分利用基于 Transformer 的语言模型的进步来实现高质量的上下文文本表示。这项工作研究了将文本特征纳入图神经网络 (GNN) 对假新闻检测的影响。我们的实验表明，上下文表示在 Macro F1 中的表现比静态表示提高了 9.3%，比没有文本特征的 GNN 提高了 33.8%。然而，嘈杂的数据增强会降低性能并增加不稳定性。我们希望我们的方法能够为进一步的研究开辟道路，并且所有代码都已公开。</li>
</ul>

<h3>Title: Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yue Li, Zhixue Zhao, Carolina Scarton</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19195">https://arxiv.org/abs/2410.19195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19195">https://arxiv.org/pdf/2410.19195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19195]] Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models(https://arxiv.org/abs/2410.19195)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) performance is known to be sensitive to the prompt design, yet the impact of class label options in zero-shot classification has been largely overlooked. This study presents the first comprehensive empirical study investigating how label option (e.g., lexical choice, order, and elaboration) influences zero-shot ICL classification performance. Our findings reveal that lexical choices for label names (e.g., agree this http URL in stance classification) play an important role, with effects also linked to label orders. An analysis of the model internal states further shows that optimal label names tend to activate fewer outlier neurons in the feed forward network. Based on this observation, we propose Label set Optimization via Activation Distribution kurtosiS (LOADS), a post-hoc approach requiring no gradient propagation. LOADS not only demonstrates effectiveness with only 100 unlabelled samples across different model types and sizes, but also shows cross-lingual transferability.</li>
<li><strong>摘要：</strong>众所周知，上下文学习 (ICL) 性能对提示设计很敏感，但零样本分类中类标签选项的影响却被人们忽视了。本研究首次全面实证研究了标签选项（例如词汇选择、顺序和详细说明）如何影响零样本 ICL 分类性能。我们的研究结果表明，标签名称的词汇选择（例如，在立场分类中同意此 http URL）起着重要作用，其影响也与标签顺序有关。对模型内部状态的分析进一步表明，最佳标签名称往往会激活前馈网络中较少的异常神经元。基于这一观察，我们提出了通过激活分布峰度 (LOADS) 进行标签集优化，这是一种不需要梯度传播的事后方法。LOADS 不仅在不同模型类型和大小中仅使用 100 个未标记样本就证明了其有效性，而且还展示了跨语言的可迁移性。</li>
</ul>

<h3>Title: Can Stories Help LLMs Reason? Curating Information Space Through Narrative</h3>
<ul>
<li><strong>Authors: </strong>Vahid Sadiri Javadi, Johanne R. Trippas, Yash Kumar Lal, Lucie Flek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19221">https://arxiv.org/abs/2410.19221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19221">https://arxiv.org/pdf/2410.19221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19221]] Can Stories Help LLMs Reason? Curating Information Space Through Narrative(https://arxiv.org/abs/2410.19221)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Narratives are widely recognized as a powerful tool for structuring information and facilitating comprehension of complex ideas in various domains such as science communication. This paper investigates whether incorporating narrative elements can assist Large Language Models (LLMs) in solving complex problems more effectively. We propose a novel approach, Story of Thought (SoT), integrating narrative structures into prompting techniques for problem-solving. This approach involves constructing narratives around problem statements and creating a framework to identify and organize relevant information. Our experiments show that using various LLMs with SoT consistently surpasses using them with other techniques on physics, chemistry, math, and biology questions in both the GPQA and JEEBench datasets. The narrative-based information curation process in SoT enhances problem comprehension by contextualizing critical in-domain information and highlighting causal relationships within the problem space.</li>
<li><strong>摘要：</strong>叙述被广泛认为是构建信息和促进理解科学传播等各个领域复杂思想的有力工具。本文探讨了结合叙述元素是否可以帮助大型语言模型 (LLM) 更有效地解决复杂问题。我们提出了一种新颖的方法，即思想故事 (SoT)，将叙述结构整合到解决问题的提示技术中。这种方法涉及围绕问题陈述构建叙述，并创建一个框架来识别和组织相关信息。我们的实验表明，在 GPQA 和 JEEBench 数据集中，在物理、化学、数学和生物学问题上使用各种 LLM 与 SoT 的效果始终优于将它们与其他技术一起使用。SoT 中基于叙述的信息管理过程通过将关键领域内信息情境化并强调问题空间内的因果关系来增强问题理解。</li>
</ul>

<h3>Title: Developing a Tutoring Dialog Dataset to Optimize LLMs for Educational Use</h3>
<ul>
<li><strong>Authors: </strong>Menna Fateen, Tsunenori Mine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19231">https://arxiv.org/abs/2410.19231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19231">https://arxiv.org/pdf/2410.19231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19231]] Developing a Tutoring Dialog Dataset to Optimize LLMs for Educational Use(https://arxiv.org/abs/2410.19231)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have shown promise for scalable educational applications, but their use in dialog-based tutoring systems remains challenging due to the need for effective pedagogical strategies and the high costs associated with expert-curated datasets. Our study explores the use of smaller, more affordable LLMs for one-on-one tutoring in the context of solving reading comprehension problems. We developed a synthetic tutoring dialog dataset, evaluated by human teachers, and fine-tuned a smaller LLM using this dataset. Furthermore, we conducted an interactive experiment comparing the performance of the fine-tuned model with a larger model in real-world tutoring scenarios. Our results show that the fine-tuned model performs on par with the larger model but at a lower cost, demonstrating a viable, cost-effective approach for implementing LLM-based tutoring systems in educational settings.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展已显示出可扩展教育应用的前景，但由于需要有效的教学策略以及专家策划的数据集的高成本，它们在基于对话的辅导系统中的使用仍然具有挑战性。我们的研究探索了在解决阅读理解问题的背景下使用较小、更实惠的 LLM 进行一对一辅导。我们开发了一个合成的辅导对话数据集，由人类教师评估，并使用该数据集对较小的 LLM 进行了微调。此外，我们进行了一项交互式实验，比较了微调模型与较大模型在现实辅导场景中的表现。我们的结果表明，微调模型的性能与较大模型相当，但成本更低，展示了一种在教育环境中实施基于 LLM 的辅导系统的可行、经济高效的方法。</li>
</ul>

<h3>Title: The Reopening of Pandora's Box: Analyzing the Role of LLMs in the Evolving Battle Against AI-Generated Fake News</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Wenbo Zhang, Sai Koneru, Hangzhi Guo, Bonam Mingole, S. Shyam Sundar, Sarah Rajtmajer, Amulya Yadav</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19250">https://arxiv.org/abs/2410.19250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19250">https://arxiv.org/pdf/2410.19250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19250]] The Reopening of Pandora's Box: Analyzing the Role of LLMs in the Evolving Battle Against AI-Generated Fake News(https://arxiv.org/abs/2410.19250)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the rise of AI-generated content spewed at scale from large language models (LLMs), genuine concerns about the spread of fake news have intensified. The perceived ability of LLMs to produce convincing fake news at scale poses new challenges for both human and automated fake news detection systems. To address this gap, this work presents the findings from a university-level competition which aimed to explore how LLMs can be used by humans to create fake news, and to assess the ability of human annotators and AI models to detect it. A total of 110 participants used LLMs to create 252 unique fake news stories, and 84 annotators participated in the detection tasks. Our findings indicate that LLMs are ~68% more effective at detecting real news than humans. However, for fake news detection, the performance of LLMs and humans remains comparable (~60% accuracy). Additionally, we examine the impact of visual elements (e.g., pictures) in news on the accuracy of detecting fake news stories. Finally, we also examine various strategies used by fake news creators to enhance the credibility of their AI-generated content. This work highlights the increasing complexity of detecting AI-generated fake news, particularly in collaborative human-AI settings.</li>
<li><strong>摘要：</strong>随着人工智能生成内容从大型语言模型 (LLM) 大规模涌现，人们对虚假新闻传播的担忧也愈演愈烈。LLM 能够大规模制作令人信服的虚假新闻，这对人类和自动虚假新闻检测系统都提出了新的挑战。为了弥补这一差距，这项研究展示了一项大学级竞赛的结果，该竞赛旨在探索人类如何使用 LLM 制作虚假新闻，并评估人类注释者和人工智能模型检测虚假新闻的能力。共有 110 名参与者使用 LLM 创建了 252 个独特的虚假新闻故事，84 名注释者参与了检测任务。我们的研究结果表明，LLM 在检测真实新闻方面的效率比人类高出约 68%。然而，对于虚假新闻检测，LLM 和人类的表现仍然相当（准确率约为 60%）。此外，我们还研究了新闻中的视觉元素（例如图片）对检测虚假新闻故事准确性的影响。最后，我们还研究了虚假新闻创建者为提高其 AI 生成内容的可信度而使用的各种策略。这项研究凸显了检测 AI 生成的虚假新闻的复杂性日益增加，尤其是在人机协作环境中。</li>
</ul>

<h3>Title: Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, Wen Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19258">https://arxiv.org/abs/2410.19258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19258">https://arxiv.org/pdf/2410.19258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19258]] Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning(https://arxiv.org/abs/2410.19258)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64 & 128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark.</li>
<li><strong>摘要：</strong>键值 (KV) 缓存是一种常用技术，用于提高大型语言模型 (LLM) 的计算效率，但其内存开销会随着输入长度的增加而迅速增长。先前的研究表明，并非所有标记对于文本生成都同样重要，因此提出了层级 KV 缓存压缩以有选择地保留关键信息。认识到注意力头在生成中的不同作用，我们提出了 HeadKV（一种头级 KV 缓存压缩方法）和 HeadKV-R2（利用一种新颖的上下文推理能力估计进行压缩）。我们的方法在单个头的级别上运行，估计它们对于需要检索和推理能力的上下文 QA 任务的重要性。针对各种基准（LongBench、LooGLE）、模型架构（例如 Llama-3-8B-Instruct、Mistral-7B-Instruct）和长上下文能力测试的大量实验表明，我们的头部级 KV 缓存压缩明显优于强基线，尤其是在资源匮乏的设置（KV 大小 = 64 和 128）下。值得注意的是，在上下文问答基准上，我们的方法仅保留了 1.5% 的 KV 缓存，却实现了完整 KV 缓存 97% 的性能。</li>
</ul>

<h3>Title: Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning</h3>
<ul>
<li><strong>Authors: </strong>Yujian Liu, Shiyu Chang, Tommi Jaakkola, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19290">https://arxiv.org/abs/2410.19290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19290">https://arxiv.org/pdf/2410.19290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19290]] Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning(https://arxiv.org/abs/2410.19290)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called Prereq-Tune to address this knowledge inconsistency and reduce hallucinations. Fundamentally, Prereq-Tune disentangles the learning of skills and knowledge, so the model learns only the task skills without being impacted by the knowledge inconsistency. To achieve this, Prereq-Tune introduces an additional prerequisite learning stage to learn the necessary knowledge for SFT, allowing subsequent SFT to focus only on task skills. Prereq-Tune can also be combined with fictitious synthetic data to enhance the grounding of LLM outputs to their internal knowledge. Experiments show that Prereq-Tune outperforms existing baselines in improving LLM's factuality across short QA and long-form generation tasks. It also opens new possibilities for knowledge-controlled generation in LLMs. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>最近的研究发现，导致 LLM 幻觉的一个因素是预训练和微调之间的知识不一致，不熟悉的微调数据会误导 LLM 编造出看似合理但错误的输出。在本文中，我们提出了一种名为 Prereq-Tune 的新型微调策略来解决这种知识不一致问题并减少幻觉。从根本上说，Prereq-Tune 将技能和知识的学习分开，因此模型只学习任务技能，而不会受到知识不一致的影响。为了实现这一点，Prereq-Tune 引入了一个额外的先决条件学习阶段来学习 SFT 所需的知识，使后续的 SFT 只关注任务技能。Prereq-Tune 还可以与虚构的合成数据相结合，以增强 LLM 输出与其内部知识的联系。实验表明，Prereq-Tune 在提高 LLM 在短篇问答和长篇生成任务中的真实性方面优于现有基准。它还为 LLM 中的知识控制生成开辟了新的可能性。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19317">https://arxiv.org/abs/2410.19317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19317">https://arxiv.org/pdf/2410.19317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19317]] FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs(https://arxiv.org/abs/2410.19317)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The growing use of large language model (LLM)-based chatbots has raised concerns about fairness. Fairness issues in LLMs can lead to severe consequences, such as bias amplification, discrimination, and harm to marginalized communities. While existing fairness benchmarks mainly focus on single-turn dialogues, multi-turn scenarios, which in fact better reflect real-world conversations, present greater challenges due to conversational complexity and potential bias accumulation. In this paper, we propose a comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios, \textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with each stage comprising two tasks. To ensure coverage of diverse bias types and attributes, we draw from existing fairness datasets and employ our template to construct a multi-turn dialogue dataset, \texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias classifiers including Llama-Guard-3 and human validation to ensure robustness. Experiments and analyses on \texttt{FairMT-10K} reveal that in multi-turn dialogue scenarios, current LLMs are more likely to generate biased responses, and there is significant variation in performance across different tasks and models. Based on this, we curate a challenging dataset, \texttt{FairMT-1K}, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show the current state of fairness in LLMs and showcase the utility of this novel approach for assessing fairness in more realistic multi-turn dialogue contexts, calling for future work to focus on LLM fairness improvement and the adoption of \texttt{FairMT-1K} in such efforts.</li>
<li><strong>摘要：</strong>基于大型语言模型 (LLM) 的聊天机器人的使用日益增多，引发了人们对公平性的担忧。LLM 中的公平性问题可能导致严重后果，例如偏见放大、歧视和对边缘化社区的伤害。虽然现有的公平性基准主要关注单轮对话，但多轮场景实际上更能反映现实世界的对话，由于对话复杂性和潜在的偏见积累，带来了更大的挑战。在本文中，我们为多轮对话场景中的 LLM 提出了一个全面的公平性基准，\textbf{FairMT-Bench}。具体来说，我们制定了一个针对 LLM 公平性能力的任务分类法，涵盖三个阶段：上下文理解、用户交互和指令权衡，每个阶段包含两个任务。为了确保覆盖各种偏见类型和属性，我们从现有的公平性数据集中汲取灵感，并使用我们的模板构建多轮对话数据集 \texttt{FairMT-10K}。为了进行评估，我们采用了 GPT-4，以及包括 Llama-Guard-3 在内的偏差分类器和人工验证，以确保稳健性。在 \texttt{FairMT-10K} 上进行的实验和分析表明，在多轮对话场景中，当前的 LLM 更有可能产生有偏差的响应，并且不同任务和模型之间的性能存在显著差异。基于此，我们整理了一个具有挑战性的数据集 \texttt{FairMT-1K}，并在该数据集上测试了 15 个当前最先进的 (SOTA) LLM。结果显示了 LLM 中公平性的当前状态，并展示了这种新方法在更现实的多轮对话环境中评估公平性的实用性，呼吁未来的工作重点是提高 LLM 公平性，并在这些工作中采用 \texttt{FairMT-1K}。</li>
</ul>

<h3>Title: Two are better than one: Context window extension with multi-grained self-injection</h3>
<ul>
<li><strong>Authors: </strong>Wei Han, Pan Zhou, Soujanya Poria, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19318">https://arxiv.org/abs/2410.19318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19318">https://arxiv.org/pdf/2410.19318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19318]] Two are better than one: Context window extension with multi-grained self-injection(https://arxiv.org/abs/2410.19318)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The limited context window of contemporary large language models (LLMs) remains a huge barrier to their broader application across various domains. While continual pre-training on long-context data is a straightforward and effective solution, it incurs substantial costs in terms of data acquisition and computational resources. To alleviate this issue, we propose SharedLLM, a novel approach grounded in the design philosophy of multi-grained context compression and query-aware information retrieval. SharedLLM is composed of two short-context LLMs such as LLaMA-2, termed upper model and lower model. The lower model functions as a compressor while the upper model acts as a decoder. The upper model receives compressed, multi-grained context information from the lower model and performs context-aware modeling on the running text. Information transfer between the compressor and decoder occurs only at the lowest layers to refrain from long forward paths in the lower model and redundant cross-attention modules in the upper model. Based on this architecture, we introduce a specialized tree-style data structure to efficiently encode, store and retrieve multi-grained contextual information for text chunks. This structure, combined with a search algorithm, enables rapid encoding and retrieval of relevant information from various levels of the tree based on the input query. This entire process, wherein the sender and receiver are derived from the same LLM layer, is referred to as self-injection.</li>
<li><strong>摘要：</strong>当代大型语言模型 (LLM) 的上下文窗口有限仍然是其在各个领域更广泛应用的巨大障碍。虽然对长上下文数据进行持续预训练是一种直接有效的解决方案，但它在数据获取和计算资源方面会产生大量成本。为了缓解这个问题，我们提出了 SharedLLM，这是一种基于多粒度上下文压缩和查询感知信息检索设计理念的新方法。SharedLLM 由两个短上下文 LLM（如 LLaMA-2）组成，称为上层模型和下层模型。下层模型用作压缩器，而上层模型用作解码器。上层模型从下层模型接收压缩的多粒度上下文信息，并对正在运行的文本执行上下文感知建模。压缩器和解码器之间的信息传输仅发生在最低层，以避免下层模型中的长前向路径和上层模型中的冗余交叉注意模块。基于此架构，我们引入了一种专门的树形数据结构，以高效地编码、存储和检索文本块的多粒度上下文信息。此结构与搜索算法相结合，可根据输入查询快速编码和检索树中各个层级的相关信息。整个过程称为自注入，其中发送方和接收方来自同一 LLM 层。</li>
</ul>

<h3>Title: AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Mou, Jingcong Liang, Jiayu Lin, Xinnong Zhang, Xiawei Liu, Shiyue Yang, Rong Ye, Lei Chen, Haoyu Kuang, Xuanjing Huang, Zhongyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19346">https://arxiv.org/abs/2410.19346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19346">https://arxiv.org/pdf/2410.19346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19346]] AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios(https://arxiv.org/abs/2410.19346)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly leveraged to empower autonomous agents to simulate human beings in various fields of behavioral research. However, evaluating their capacity to navigate complex social interactions remains a challenge. Previous studies face limitations due to insufficient scenario diversity, complexity, and a single-perspective focus. To this end, we introduce AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense employs a bottom-up approach to create 1,225 diverse social scenarios constructed from extensive scripts. We evaluate LLM-driven agents through multi-turn interactions, emphasizing both goal completion and implicit reasoning. We analyze goals using ERG theory and conduct comprehensive experiments. Our findings highlight that LLMs struggle with goals in complex social scenarios, especially high-level growth needs, and even GPT-4o requires improvement in private information reasoning.</li>
<li><strong>摘要：</strong>在行为研究的各个领域，大型语言模型 (LLM) 越来越多地被用来使自主代理能够模拟人类。然而，评估它们处理复杂社交互动的能力仍然是一个挑战。先前的研究面临局限性，因为场景多样性不足、复杂性和单一视角。为此，我们推出了 AgentSense：通过交互场景对语言代理的社交智能进行基准测试。借鉴戏剧理论，AgentSense 采用自下而上的方法创建了 1,225 个由大量脚本构建的不同社交场景。我们通过多轮交互评估 LLM 驱动的代理，强调目标完成和隐性推理。我们使用 ERG 理论分析目标并进行全面的实验。我们的研究结果表明，LLM 在复杂的社交场景中难以实现目标，尤其是高水平的成长需求，甚至 GPT-4o 也需要改进私人信息推理。</li>
</ul>

<h3>Title: Interleaving Text and Number Embeddings to Solve Mathemathics Problems</h3>
<ul>
<li><strong>Authors: </strong>Marvin Alberts, Gianmarco Gabrieli, Irina Espejo Morales</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19353">https://arxiv.org/abs/2410.19353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19353">https://arxiv.org/pdf/2410.19353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19353]] Interleaving Text and Number Embeddings to Solve Mathemathics Problems(https://arxiv.org/abs/2410.19353)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Integrating text and numbers effectively is a crucial step towards enhancing Large Language Models (LLMs) capabilities in assisting in scientific tasks. While most current approaches rely on discrete tokenization of numbers, for instance, conversion to scientific notation or base 10-decomposition, a recent approach proposed a continuous numerical encoding as an inductive bias. In this paper, we build upon this approach by introducing more expressive numerical embeddings. Our method addresses key shortcomings, including the elimination of numerical artefacts and the ability to handle a wide range of magnitudes without clipping. Our work presents two key contributions. First, we employ an MLP to assign distinct directions in the embedding space to different numbers. Our second contribution is the introduction of a routing layer that differentiates between numerical and text embeddings. We hypothesise that this combined approach enables the model to distinguish between text and number distributions while maintaining its capacity for arithmetic operations. Using only a 45 M parameter encoder-decoder architecture our method achieves a $R^2$=0.9988 over a wide range of magnitude ($10^{-3},10^{8}$). In addition, we empirically observe a reduction of the numerical artefacts and biases observed compared to the baselines.</li>
<li><strong>摘要：</strong>有效地整合文本和数字是增强大型语言模型 (LLM) 协助科学任务的能力的关键一步。虽然大多数当前方法都依赖于数字的离散标记化，例如转换为科学计数法或十进制分解，但最近的一种方法提出了一种连续数字编码作为归纳偏差。在本文中，我们在此方法的基础上引入了更具表现力的数字嵌入。我们的方法解决了关键的缺点，包括消除数字伪影和处理各种幅度而不进行剪辑的能力。我们的工作提出了两个关键贡献。首先，我们使用 MLP 将嵌入空间中的不同方向分配给不同的数字。我们的第二个贡献是引入了一个区分数字和文本嵌入的路由层。我们假设这种组合方法使模型能够区分文本和数字分布，同时保持其算术运算能力。我们的方法仅使用 45 M 参数编码器-解码器架构，便可在很宽的幅度范围内（$10^{-3},10^{8}$）实现 $R^2$=0.9988。此外，我们通过经验观察到，与基线相比，观察到的数值伪影和偏差有所减少。</li>
</ul>

<h3>Title: Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liam Barkley, Brink van der Merwe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19385">https://arxiv.org/abs/2410.19385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19385">https://arxiv.org/pdf/2410.19385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19385]] Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models(https://arxiv.org/abs/2410.19385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是一种功能强大的计算模型，在大量人类可读文本语料库上进行训练，使其能够执行通用语言理解和生成。LLM 在各种自然语言处理 (NLP) 任务中表现出色，在业界和学术界都引起了极大关注。尽管取得了这些成功，但 LLM 经常会产生不准确的结果，通常称为幻觉。提示工程，即设计和制定 LLM 执行特定任务的指令的过程，已成为缓解幻觉的关键方法。本文对旨在减少 LLM 中幻觉的不同提示策略和框架进行了全面的实证评估。将各种提示技术应用于广泛的基准数据集，以评估每种方法的准确性和幻觉率。此外，本文还研究了工具调用代理（使用外部工具增强其语言生成以外功能的 LLM）对同一基准中幻觉率的影响。研究结果表明，最佳提示技术取决于问题的类型，简单的技术在减少幻觉方面通常比复杂的方法更有效。此外，研究还表明，由于外部工具使用的复杂性增加，LLM 代理的幻觉发生率可能会显著提高。</li>
</ul>

<h3>Title: KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western Cultures</h3>
<ul>
<li><strong>Authors: </strong>Hamna, Deepthi Sudharsan, Agrima Seth, Ritvik Budhiraja, Deepika Khullar, Vyshak Jain, Kalika Bali, Aditya Vashistha, Sameer Segal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19419">https://arxiv.org/abs/2410.19419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19419">https://arxiv.org/pdf/2410.19419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19419]] KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western Cultures(https://arxiv.org/abs/2410.19419)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated the ability to generate compelling text and visual stories. However, their outputs are predominantly aligned with the sensibilities of the Global North, often resulting in an outsider's gaze on other cultures. As a result, non-Western communities have to put extra effort into generating culturally specific stories. To address this challenge, we developed a visual storytelling pipeline called KAHANI that generates culturally grounded visual stories for non-Western cultures. Our pipeline leverages off-the-shelf models GPT-4 Turbo and Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I prompting techniques, we capture the cultural context from user's prompt and generate vivid descriptions of the characters and scene compositions. To evaluate the effectiveness of KAHANI, we conducted a comparative user study with ChatGPT-4 (with DALL-E3) in which participants from different regions of India compared the cultural relevance of stories generated by the two tools. Results from the qualitative and quantitative analysis performed on the user study showed that KAHANI was able to capture and incorporate more Culturally Specific Items (CSIs) compared to ChatGPT-4. In terms of both its cultural competence and visual story generation quality, our pipeline outperformed ChatGPT-4 in 27 out of the 36 comparisons.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 和文本转图像 (T2I) 模型已证明能够生成引人入胜的文本和视觉故事。然而，它们的输出主要与全球北方的情感保持一致，往往导致局外人对其他文化的关注。因此，非西方社区必须付出额外的努力来生成具有特定文化的故事。为了应对这一挑战，我们开发了一个名为 KAHANI 的视觉叙事管道，可以为非西方文化生成具有文化根基的视觉故事。我们的管道利用现成的模型 GPT-4 Turbo 和 Stable Diffusion XL (SDXL)。通过使用思维链 (CoT) 和 T2I 提示技术，我们从用户的提示中捕捉文化背景，并生成人物和场景构图的生动描述。为了评估 KAHANI 的有效性，我们与 ChatGPT-4（与 DALL-E3）进行了一项比较用户研究，其中来自印度不同地区的参与者比较了这两种工具生成的故事的文化相关性。对用户研究进行的定性和定量分析结果表明，与 ChatGPT-4 相比，KAHANI 能够捕捉和整合更多文化特定项目 (CSI)。在文化能力和视觉故事生成质量方面，我们的流程在 36 次比较中的 27 次中均优于 ChatGPT-4。</li>
</ul>

<h3>Title: Intelligent Understanding of Large Language Models in Traditional Chinese Medicine Based on Prompt Engineering Framework</h3>
<ul>
<li><strong>Authors: </strong>Yirui Chen, Qinyu Xiao, Jia Yi, Jing Chen, Mengyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19451">https://arxiv.org/abs/2410.19451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19451">https://arxiv.org/pdf/2410.19451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19451]] Intelligent Understanding of Large Language Models in Traditional Chinese Medicine Based on Prompt Engineering Framework(https://arxiv.org/abs/2410.19451)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper explores the application of prompt engineering to enhance the performance of large language models (LLMs) in the domain of Traditional Chinese Medicine (TCM). We propose TCM-Prompt, a framework that integrates various pre-trained language models (PLMs), templates, tokenization, and verbalization methods, allowing researchers to easily construct and fine-tune models for specific TCM-related tasks. We conducted experiments on disease classification, syndrome identification, herbal medicine recommendation, and general NLP tasks, demonstrating the effectiveness and superiority of our approach compared to baseline methods. Our findings suggest that prompt engineering is a promising technique for improving the performance of LLMs in specialized domains like TCM, with potential applications in digitalization, modernization, and personalized medicine.</li>
<li><strong>摘要：</strong>本文探讨了快速工程在提升中医 (TCM) 领域大型语言模型 (LLM) 性能方面的应用。我们提出了 TCM-Prompt，这是一个集成了各种预训练语言模型 (PLM)、模板、标记化和言语化方法的框架，可让研究人员轻松构建和微调特定 TCM 相关任务的模型。我们在疾病分类、综合征识别、草药推荐和一般 NLP 任务上进行了实验，证明了我们的方法与基线方法相比的有效性和优越性。我们的研究结果表明，快速工程是一种有前途的技术，可以提高 TCM 等专业领域 LLM 的性能，并有可能应用于数字化、现代化和个性化医疗。</li>
</ul>

<h3>Title: ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework</h3>
<ul>
<li><strong>Authors: </strong>Hengyuan Zhang, Chenming Shang, Sizhe Wang, Dongdong Zhang, Feng Yao, Renliang Sun, Yiyao Yu, Yujiu Yang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19453">https://arxiv.org/abs/2410.19453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19453">https://arxiv.org/pdf/2410.19453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19453]] ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework(https://arxiv.org/abs/2410.19453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Although fine-tuning Large Language Models (LLMs) with multilingual data can rapidly enhance the multilingual capabilities of LLMs, they still exhibit a performance gap between the dominant language (e.g., English) and non-dominant ones due to the imbalance of training data across languages. To further enhance the performance of non-dominant languages, we propose ShifCon, a Shift-based Contrastive framework that aligns the internal forward process of other languages toward that of the dominant one. Specifically, it shifts the representations of non-dominant languages into the dominant language subspace, allowing them to access relatively rich information encoded in the model parameters. The enriched representations are then shifted back into their original language subspace before generation. Moreover, we introduce a subspace distance metric to pinpoint the optimal layer area for shifting representations and employ multilingual contrastive learning to further enhance the alignment of representations within this area. Experiments demonstrate that our ShifCon framework significantly enhances the performance of non-dominant languages, particularly for low-resource ones. Further analysis offers extra insights to verify the effectiveness of ShifCon and propel future research</li>
<li><strong>摘要：</strong>尽管使用多语言数据对大型语言模型 (LLM) 进行微调可以快速增强 LLM 的多语言能力，但由于不同语言之间的训练数据不平衡，它们在主导语言（例如英语）和非主导语言之间的性能差距仍然存在。为了进一步提高非主导语言的性能，我们提出了 ShifCon，这是一个基于 Shift 的对比框架，它将其他语言的内部前向过程与主导语言的过程对齐。具体来说，它将非主导语言的表示转移到主导语言子空间，使它们能够访问模型参数中编码的相对丰富的信息。然后将丰富的表示移回其原始语言子空间，然后再生成。此外，我们引入了一个子空间距离度量来确定转移表示的最佳层区域，并采用多语言对比学习来进一步增强该区域内表示的对齐。实验表明，我们的 ShifCon 框架显著提高了非主导语言的性能，尤其是对于资源匮乏的语言。进一步的分析提供了额外的见解，以验证 ShifCon 的有效性并推动未来的研究</li>
</ul>

<h3>Title: A Debate-Driven Experiment on LLM Hallucinations and Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Ray Li, Tanishka Bagade, Kevin Martinez, Flora Yasmin, Grant Ayala, Michael Lam, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19485">https://arxiv.org/abs/2410.19485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19485">https://arxiv.org/pdf/2410.19485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19485]] A Debate-Driven Experiment on LLM Hallucinations and Accuracy(https://arxiv.org/abs/2410.19485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved a degree of success in generating coherent and contextually relevant text, yet they remain prone to a significant challenge known as hallucination: producing information that is not substantiated by the input or external knowledge. Previous efforts to mitigate hallucinations have focused on techniques such as fine-tuning models on high-quality datasets, incorporating fact-checking mechanisms, and developing adversarial training methods. While these approaches have shown some promise, they often address the issue at the level of individual model outputs, leaving unexplored the effects of inter-model interactions on hallucination. This study investigates the phenomenon of hallucination in LLMs through a novel experimental framework where multiple instances of GPT-4o-Mini models engage in a debate-like interaction prompted with questions from the TruthfulQA dataset. One model is deliberately instructed to generate plausible but false answers while the other models are asked to respond truthfully. The experiment is designed to assess whether the introduction of misinformation by one model can challenge the truthful majority to better justify their reasoning, improving performance on the TruthfulQA benchmark. The findings suggest that inter-model interactions can offer valuable insights into improving the accuracy and robustness of LLM outputs, complementing existing mitigation strategies.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在生成连贯且上下文相关的文本方面取得了一定程度的成功，但它们仍然容易出现一个重大挑战，即幻觉：产生未经输入或外部知识证实的信息。之前缓解幻觉的努力主要集中在一些技术上，例如在高质量数据集上微调模型、结合事实核查机制和开发对抗性训练方法。虽然这些方法显示出一些希望，但它们通常在单个模型输出层面上解决这个问题，而没有探索模型间相互作用对幻觉的影响。这项研究通过一个新颖的实验框架调查了 LLM 中的幻觉现象，其中多个 GPT-4o-Mini 模型实例进行类似辩论的交互，并由 TruthfulQA 数据集中的问题提示。一个模型被故意指示生成合理但错误的答案，而其他模型则被要求如实回答。该实验旨在评估一个模型引入错误信息是否会挑战大多数诚实的人，使他们更好地证明自己的推理，从而提高 TruthfulQA 基准测试中的表现。研究结果表明，模型间交互可以为提高 LLM 输出的准确性和稳健性提供宝贵的见解，补充现有的缓解策略。</li>
</ul>

<h3>Title: Graph Linearization Methods for Reasoning on Graphs with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Christos Xypolopoulos, Guokan Shang, Xiao Fei, Giannis Nikolentzos, Hadi Abdine, Iakovos Evdaimon, Michail Chatzianastasis, Giorgos Stamou, Michalis Vazirgiannis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19494">https://arxiv.org/abs/2410.19494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19494">https://arxiv.org/pdf/2410.19494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19494]] Graph Linearization Methods for Reasoning on Graphs with Large Language Models(https://arxiv.org/abs/2410.19494)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models have evolved to process multiple modalities beyond text, such as images and audio, which motivates us to explore how to effectively leverage them for graph machine learning tasks. The key question, therefore, is how to transform graphs into linear sequences of tokens, a process we term graph linearization, so that LLMs can handle graphs naturally. We consider that graphs should be linearized meaningfully to reflect certain properties of natural language text, such as local dependency and global alignment, in order to ease contemporary LLMs, trained on trillions of textual tokens, better understand graphs. To achieve this, we developed several graph linearization methods based on graph centrality, degeneracy, and node relabeling schemes. We then investigated their effect on LLM performance in graph reasoning tasks. Experimental results on synthetic graphs demonstrate the effectiveness of our methods compared to random linearization baselines. Our work introduces novel graph representations suitable for LLMs, contributing to the potential integration of graph machine learning with the trend of multi-modal processing using a unified transformer model.</li>
<li><strong>摘要：</strong>大型语言模型已经发展到可以处理文本以外的多种模态，例如图像和音频，这促使我们探索如何有效地利用它们进行图机器学习任务。因此，关键问题是如何将图转换为线性标记序列，这个过程我们称之为图线性化，以便 LLM 可以自然地处理图。我们认为图应该有意义地线性化以反映自然语言文本的某些属性，例如局部依赖性和全局对齐，以便让当代 LLM（在数万亿个文本标记上进行训练）更好地理解图。为了实现这一点，我们开发了几种基于图中心性、退化和节点重新标记方案的图线性化方法。然后，我们研究了它们对图推理任务中 LLM 性能的影响。与随机线性化基线相比，合成图上的实验结果证明了我们的方法的有效性。我们的工作引入了适合 LLM 的新型图表示，有助于将图机器学习与使用统一转换器模型进行多模态处理的趋势相结合。</li>
</ul>

<h3>Title: Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Anthony Cui, Pranav Nandyalam, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19499">https://arxiv.org/abs/2410.19499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19499">https://arxiv.org/pdf/2410.19499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19499]] Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization(https://arxiv.org/abs/2410.19499)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and efficacy of prompt optimization for Large Language Models (LLMs). Building on ProTeGi, MAPO uses positive natural language "gradients" and a momentum-based extension to refine prompts effectively. By tracking gradient history, MAPO avoids local minima and oscillations. It also utilizes beam search and an Upper Confidence Bound (UCB) algorithm for balanced candidate expansion and selection. Benchmark testing shows that MAPO achieves faster convergence time with fewer API calls and higher F1 scores than ProTeGi, proving it as a robust and scalable solution for automated prompt engineering in LLMs.</li>
<li><strong>摘要：</strong>动量辅助提示优化 (MAPO) 提高了大型语言模型 (LLM) 提示优化的效率和功效。基于 ProTeGi，MAPO 使用正自然语言“梯度”和基于动量的扩展来有效地优化提示。通过跟踪梯度历史，MAPO 可以避免局部最小值和振荡。它还利用波束搜索和上置信边界 (UCB) 算法来平衡候选扩展和选择。基准测试表明，MAPO 比 ProTeGi 实现了更快的收敛时间、更少的 API 调用和更高的 F1 分数，证明它是 LLM 中自动提示工程的强大且可扩展的解决方案。</li>
</ul>

<h3>Title: SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jahyun Koo, Yerin Hwang, Yongil Kim, Taegwan Kang, Hyunkyung Bae, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19503">https://arxiv.org/abs/2410.19503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19503">https://arxiv.org/pdf/2410.19503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19503]] SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models(https://arxiv.org/abs/2410.19503)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model compression, with student-generated outputs (SGOs) being particularly notable for reducing the mismatch between training and inference. However, SGOs often produce noisy and biased sequences, which can lead to misguidance from the teacher model, especially in long sequences. To mitigate these challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel approach that strategically incorporates the teacher model during the student's sequence generation. SWITCH identifies discrepancies between the token probabilities of the teacher and student models, allowing the teacher to intervene selectively, particularly in long sequences that are more prone to teacher misguidance. Extensive experimental results across three model families and five instruction-following datasets show that SWITCH surpasses traditional KD methods, particularly excelling in the generation of long sequential data.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 取得了成功，但它们仍然面临着与高推理成本和内存需求相关的挑战。为了解决这些问题，知识蒸馏 (KD) 已成为一种流行的模型压缩方法，其中学生生成的输出 (SGO) 在减少训练和推理之间的不匹配方面尤其引人注目。然而，SGO 通常会产生嘈杂和有偏差的序列，这可能会导致教师模型的误导，尤其是在长序列中。为了缓解这些挑战，我们提出了 SWITCH（与老师一起学习知识蒸馏），这是一种在学生序列生成过程中战略性地整合教师模型的新方法。SWITCH 识别教师和学生模型的标记概率之间的差异，允许教师有选择地进行干预，特别是在更容易受到教师误导的长序列中。在三个模型系列和五个指令跟踪数据集中进行的大量实验结果表明，SWITCH 超越了传统的 KD 方法，尤其是在生成长序列数据方面表现出色。</li>
</ul>

<h3>Title: Detection of Human and Machine-Authored Fake News in Urdu</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Zain Ali, Yuxia Wang, Bernhard Pfahringer, Tony Smith</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19517">https://arxiv.org/abs/2410.19517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19517">https://arxiv.org/pdf/2410.19517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19517]] Detection of Human and Machine-Authored Fake News in Urdu(https://arxiv.org/abs/2410.19517)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The rise of social media has amplified the spread of fake news, now further complicated by large language models (LLMs) like ChatGPT, which ease the generation of highly convincing, error-free misinformation, making it increasingly challenging for the public to discern truth from falsehood. Traditional fake news detection methods relying on linguistic cues also becomes less effective. Moreover, current detectors primarily focus on binary classification and English texts, often overlooking the distinction between machine-generated true vs. fake news and the detection in low-resource languages. To this end, we updated detection schema to include machine-generated news with focus on the Urdu language. We further propose a hierarchical detection strategy to improve the accuracy and robustness. Experiments show its effectiveness across four datasets in various settings.</li>
<li><strong>摘要：</strong>社交媒体的兴起加剧了虚假新闻的传播，如今，大型语言模型 (LLM)（如 ChatGPT）进一步加剧了这一现象，它们简化了高度令人信服、无错误的虚假信息的生成，使公众越来越难以辨别真假。传统的依赖语言线索的虚假新闻检测方法也变得不那么有效。此外，当前的检测器主要侧重于二元分类和英文文本，往往忽略了机器生成的真假新闻之间的区别以及资源匮乏的语言中的检测。为此，我们更新了检测方案，以包括机器生成的新闻，重点关注乌尔都语。我们进一步提出了一种分层检测策略，以提高准确性和稳健性。实验证明了它在各种环境下的四个数据集中的有效性。</li>
</ul>

<h3>Title: ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems</h3>
<ul>
<li><strong>Authors: </strong>Ritvik Aggarwal Ishneet Sukhvinder Singh Ibrahim Allahverdiyev, Muhammad Taha, Aslihan Akalin, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19572">https://arxiv.org/abs/2410.19572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19572">https://arxiv.org/pdf/2410.19572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19572]] ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems(https://arxiv.org/abs/2410.19572)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems using large language models (LLMs) often generate inaccurate responses due to the retrieval of irrelevant or loosely related information. Existing methods, which operate at the document level, fail to effectively filter out such content. We propose LLM-driven chunk filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and filtering retrieved information at the chunk level. Our approach employs semantic chunking to divide documents into coherent sections and utilizes LLM-based relevance scoring to assess each chunk's alignment with the user's query. By filtering out less pertinent chunks before the generation phase, we significantly reduce hallucinations and improve factual accuracy. Experiments show that our method outperforms existing RAG models, achieving higher accuracy on tasks requiring precise information retrieval. This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications like fact-checking and multi-hop reasoning.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 的检索增强生成 (RAG) 系统通常会由于检索不相关或松散相关的信息而生成不准确的响应。现有的在文档级别运行的方法无法有效地过滤掉此类内容。我们提出了 LLM 驱动的块过滤 ChunkRAG，这是一个通过在块级别评估和过滤检索到的信息来增强 RAG 系统的框架。我们的方法采用语义分块将文档划分为连贯的部分，并利用基于 LLM 的相关性评分来评估每个块与用户查询的一致性。通过在生成阶段之前过滤掉不太相关的块，我们可以显著减少幻觉并提高事实准确性。实验表明，我们的方法优于现有的 RAG 模型，在需要精确信息检索的任务上实现了更高的准确性。这一进步提高了 RAG 系统的可靠性，使其特别有利于事实核查和多跳推理等应用。</li>
</ul>

<h3>Title: OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19609">https://arxiv.org/abs/2410.19609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19609">https://arxiv.org/pdf/2410.19609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19609]] OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization(https://arxiv.org/abs/2410.19609)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, agent</a></li>
<li><strong>Abstract: </strong>The rapid development of large language and multimodal models has sparked significant interest in using proprietary models, such as GPT-4o, to develop autonomous agents capable of handling real-world scenarios like web navigation. Although recent open-source efforts have tried to equip agents with the ability to explore environments and continuously improve over time, they are building text-only agents in synthetic environments where the reward signals are clearly defined. Such agents struggle to generalize to realistic settings that require multimodal perception abilities and lack ground-truth signals. In this paper, we introduce an open-source framework designed to facilitate the development of multimodal web agent that can autonomously conduct real-world exploration and improve itself. We first train the base model with imitation learning to gain the basic abilities. We then let the agent explore the open web and collect feedback on its trajectories. After that, it further improves its policy by learning from well-performing trajectories judged by another general-purpose model. This exploration-feedback-optimization cycle can continue for several iterations. Experimental results show that our web agent successfully improves itself after each iteration, demonstrating strong performance across multiple test sets.</li>
<li><strong>摘要：</strong>大型语言和多模态模型的快速发展引发了人们对使用专有模型（如 GPT-4o）开发能够处理网络导航等现实场景的自主代理的极大兴趣。尽管最近的开源努力试图让代理具备探索环境并随着时间推移不断改进的能力，但他们正在构建纯文本代理，而这些代理的奖励信号定义明确。这样的代理很难推广到需要多模态感知能力且缺乏地面实况信号的现实环境。在本文中，我们介绍了一个开源框架，旨在促进多模态网络代理的开发，该代理可以自主进行现实世界的探索并自我改进。我们首先用模仿学习训练基础模型以获得基本能力。然后，我们让代理探索开放网络并收集有关其轨迹的反馈。之后，它通过从另一个通用模型判断的良好轨迹中学习来进一步改进其策略。这种探索-反馈-优化循环可以持续多次迭代。实验结果表明，我们的 Web 代理在每次迭代后都成功地自我改进，在多个测试集上表现出强大的性能。</li>
</ul>

<h3>Title: AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Clemencia Siro, Yifei Yuan, Mohammad Aliannejadi, Maarten de Rijke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19692">https://arxiv.org/abs/2410.19692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19692">https://arxiv.org/pdf/2410.19692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19692]] AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs(https://arxiv.org/abs/2410.19692)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Generating diverse and effective clarifying questions is crucial for improving query understanding and retrieval performance in open-domain conversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration, and evaluaTion of Clarifying Questions), an end-to-end LLM-based framework addressing the challenges of scalability and adaptability faced by existing methods that rely on manual curation or template-based approaches. AGENT-CQ consists of two stages: a generation stage employing LLM prompting strategies to generate clarifying questions, and an evaluation stage (CrowdLLM) that simulates human crowdsourcing judgments using multiple LLM instances to assess generated questions and answers based on comprehensive quality metrics. Extensive experiments on the ClariQ dataset demonstrate CrowdLLM's effectiveness in evaluating question and answer quality. Human evaluation and CrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms baselines in various aspects of question and answer quality. In retrieval-based evaluation, LLM-generated questions significantly enhance retrieval effectiveness for both BM25 and cross-encoder models compared to human-generated questions.</li>
<li><strong>摘要：</strong>生成多样化且有效的澄清问题对于提高开放域对话搜索 (CS) 系统中的查询理解和检索性能至关重要。我们提出了 AGENT-CQ（澄清问题的自动生成和评估），这是一个基于 LLM 的端到端框架，旨在解决现有方法（依赖手动管理或基于模板的方法）所面临的可扩展性和适应性挑战。AGENT-CQ 包含两个阶段：生成阶段采用 LLM 提示策略来生成澄清问题；评估阶段（CrowdLLM）使用多个 LLM 实例模拟人类众包判断，根据综合质量指标评估生成的问题和答案。在 ClariQ 数据集上进行的大量实验证明了 CrowdLLM 在评估问答质量方面的有效性。人工评估和 CrowdLLM 表明，AGENT-CQ - 生成阶段在问答质量的各个方面始终优于基线。在基于检索的评估中，与人类生成的问题相比，LLM 生成的问题显著提高了 BM25 和跨编码器模型的检索效果。</li>
</ul>

<h3>Title: Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yifei Zhang, Hao Zhu, Aiwei Liu, Han Yu, Piotr Koniusz, Irwin King</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19694">https://arxiv.org/abs/2410.19694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19694">https://arxiv.org/pdf/2410.19694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19694]] Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs(https://arxiv.org/abs/2410.19694)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) has become a crucial technique for adapting pre-trained models to downstream tasks. However, the enormous size of LLMs poses significant challenges in terms of computational complexity and resource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising solution. However, there exists a gap between the practical performance of low-rank adaptations and its theoretical optimum. In this work, we propose eXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this gap by leveraging the power of ensemble learning. Inspired by gradient boosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations to refine model predictions. It achieves better performance than the standard LoRA, while enjoying the computational efficiency of rank-1 adaptations. We provide theoretical analysis to show the convergence and optimality of our approach, and conduct extensive experiments on a range of natural language processing tasks. The results demonstrate that XGBLoRA consistently outperforms standard LoRA and achieves performance comparable to full fine-tuning with significantly fewer trainable parameters. This work advances parameter-efficient fine-tuning for LLMs, and offers a promising solution for adapting LLMs to downstream tasks while optimizing performance and efficiency.</li>
<li><strong>摘要：</strong>微调大型语言模型 (LLM) 已成为将预训练模型适应下游任务的关键技术。然而，LLM 的巨大规模在计算复杂性和资源需求方面带来了重大挑战。低秩自适应 (LoRA) 已成为一种有前途的解决方案。然而，低秩自适应的实际性能与其理论最优值之间存在差距。在这项工作中，我们提出了 eXtreme Gradient Boosting LoRA (XGBLoRA)，这是一个新颖的框架，它通过利用集成学习的力量来弥合这一差距。受梯度提升的启发，XGBLoRA 迭代学习和合并一系列 LoRA 自适应以改进模型预测。它实现了比标准 LoRA 更好的性能，同时享有 1 级自适应的计算效率。我们提供理论分析来展示我们方法的收敛性和最优性，并对一系列自然语言处理任务进行了广泛的实验。结果表明，XGBLoRA 的表现始终优于标准 LoRA，并且以明显更少的可训练参数实现了与完全微调相当的性能。这项工作推进了 LLM 的参数高效微调，并为在优化性能和效率的同时使 LLM 适应下游任务提供了一种有前途的解决方案。</li>
</ul>

<h3>Title: 2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision</h3>
<ul>
<li><strong>Authors: </strong>Shilong Li, Yancheng He, Hui Huang, Xingyuan Bu, Jiaheng Liu, Hangyu Guo, Weixun Wang, Jihao Gu, Wenbo Su, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19720">https://arxiv.org/abs/2410.19720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19720">https://arxiv.org/pdf/2410.19720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19720]] 2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision(https://arxiv.org/abs/2410.19720)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Direct Preference Optimization (DPO) have significantly enhanced the alignment of Large Language Models (LLMs) with human preferences, owing to its simplicity and effectiveness. However, existing methods typically optimize a scalar score or ranking reward, thereby overlooking the multi-dimensional nature of human preferences. In this work, we propose to extend the preference of DPO to two dimensions: segments and aspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For the segment dimension, we divide the response into sentences and assign scores to each segment. For the aspect dimension, we meticulously design several criteria covering the response quality rubrics. With the 2-dimensional signals as feedback, we develop a 2D-DPO framework, decomposing the overall objective into multi-segment and multi-aspect objectives. Extensive experiments on popular benchmarks demonstrate that 2D-DPO performs better than methods that optimize for scalar or 1-dimensional preferences.</li>
<li><strong>摘要：</strong>直接偏好优化 (DPO) 的最新进展显著增强了大型语言模型 (LLM) 与人类偏好的一致性，这归功于其简单性和有效性。然而，现有方法通常优化标量分数或排名奖励，从而忽略了人类偏好的多维性。在这项工作中，我们建议将 DPO 的偏好扩展到两个维度：片段和方面。我们首先引入一个名为 HelpSteer-2D 的 2D 监督数据集。对于片段维度，我们将响应分为句子并为每个片段分配分数。对于方面维度，我们精心设计了几个涵盖响应质量评估标准的标准。以二维信号作为反馈，我们开发了一个 2D-DPO 框架，将总体目标分解为多片段和多方面目标。在流行基准上进行的大量实验表明，2D-DPO 的表现优于针对标量或一维偏好进行优化的方法。</li>
</ul>

<h3>Title: Counting Ability of Large Language Models and Impact of Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Juntai Cao, Chenyu You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19730">https://arxiv.org/abs/2410.19730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19730">https://arxiv.org/pdf/2410.19730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19730]] Counting Ability of Large Language Models and Impact of Tokenization(https://arxiv.org/abs/2410.19730)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the complexity class TC$^0$, making them theoretically incapable of solving tasks that demand increasingly deep reasoning as input length grows. Counting, a fundamental component of many reasoning tasks, also requires reasoning depth to grow linearly to be performed inductively. While previous studies have established the upper limits of counting ability in Transformer-based expert models (i.e., models specifically trained for counting tasks), these findings do not directly extend to general-purpose LLMs due to differences in reasoning mechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning can help alleviate some of the architectural limitations of Transformers in counting tasks. However, little attention has been paid to the role of tokenization in these models. Unlike expert models that often use character-level tokenization, LLMs typically rely on byte-level (BPE) tokenizers, which fundamentally alters the way reasoning is processed. Our work investigates the impact of tokenization on the counting abilities of LLMs, uncovering substantial performance variations based on input tokenization differences. We provide both theoretical and experimental analyses, offering insights into how tokenization choices can undermine models' theoretical computability, thereby inspiring the design of new tokenization methods to enhance reasoning in LLMs.</li>
<li><strong>摘要：</strong>Transformer 是现代大型语言模型 (LLM) 的支柱，但它面临着固有的架构限制，阻碍了其推理能力。与循环网络不同，Transformer 缺乏循环连接，因此只能进行恒定深度计算。这种限制将它们置于复杂度类别 TC$^0$ 中，从理论上讲，它们无法解决随着输入长度增加而需要越来越深的推理的任务。计数是许多推理任务的基本组成部分，也需要推理深度线性增长才能进行归纳。虽然先前的研究已经确定了基于 Transformer 的专家模型（即专门为计数任务训练的模型）的计数能力上限，但由于推理机制的差异，这些发现并不直接扩展到通用 LLM。最近的研究强调了思路链 (CoT) 推理如何帮助缓解 Transformer 在计数任务中的一些架构限制。然而，很少有人关注标记化在这些模型中的作用。与经常使用字符级标记化的专家模型不同，LLM 通常依赖于字节级 (BPE) 标记器，这从根本上改变了推理处理的方式。我们的工作调查了标记化对 LLM 计数能力的影响，发现了基于输入标记化差异的显著性能差异。我们提供理论和实验分析，深入了解标记化选择如何破坏模型的理论可计算性，从而启发设计新的标记化方法以增强 LLM 中的推理能力。</li>
</ul>

<h3>Title: Rethinking Visual Dependency in Long-Context Reasoning for Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Zhou, Zhi Rao, Jun Wan, Jianbing Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.19732">https://arxiv.org/abs/2410.19732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.19732">https://arxiv.org/pdf/2410.19732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.19732]] Rethinking Visual Dependency in Long-Context Reasoning for Large Vision-Language Models(https://arxiv.org/abs/2410.19732)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) excel in cross-model tasks but experience performance declines in long-context reasoning due to overreliance on textual information and reduced visual dependency. In this study, we empirically analyze LVLMs in long-context reasoning, revealing that increased context length leads to a higher dependence on language at the expense of visual dependency. To address this issue, we propose a novel training-free context pruning method that selectively removes less critical textual information. Our approach enhances visual dependency and reduces textual noise, thereby improving LVLM performance in long-context reasoning. We validate our method by constructing a long-context dataset, demonstrating its effectiveness across various LVLMs. Moreover, further analysis confirms the robustness of different token pruning strategies and preliminary explores scaling laws between pruning rates and context length.</li>
<li><strong>摘要：</strong>大型视觉语言模型 (LVLM) 在跨模型任务中表现出色，但由于过度依赖文本信息和降低视觉依赖性，在长上下文推理中性能下降。在本研究中，我们对长上下文推理中的 LVLM 进行了实证分析，结果表明，增加上下文长度会导致对语言的依赖性增加，而牺牲视觉依赖性。为了解决这个问题，我们提出了一种新颖的无需训练的上下文剪枝方法，可以有选择地删除不太重要的文本信息。我们的方法增强了视觉依赖性并减少了文本噪音，从而提高了 LVLM 在长上下文推理中的性能。我们通过构建长上下文数据集来验证我们的方法，证明了它在各种 LVLM 中的有效性。此外，进一步的分析证实了不同 token 剪枝策略的稳健性，并初步探讨了剪枝率和上下文长度之间的缩放规律。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
