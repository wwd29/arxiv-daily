<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-30</h1>
<h3>Title: Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions</h3>
<ul>
<li><strong>Authors: </strong>Sabrina Patania, Luca Annese, Cansu Koyuturk, Azzurra Ruggeri, Dimitri Ognibene</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21065">https://arxiv.org/abs/2507.21065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21065">https://arxiv.org/pdf/2507.21065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21065]] Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions(https://arxiv.org/abs/2507.21065)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations. We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition. Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets. These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在处理广泛的离线数据集方面表现出了显着的功能。但是，他们经常在获取和整合复杂的在线知识方面面临挑战。传统的AI训练范式主要基于监督的学习或强化学习，反映了独立探索的“ piagetian”模型。这些方法通常依赖于大型数据集和稀疏的反馈信号，从而限制了模型从交互中有效学习的能力。这项研究从维戈茨基的社会文化理论中汲取灵感，探讨了社会介导的学习范式解决这些局限性的潜力。我们介绍了一个充满活力的环境，称为“ AI社交体育馆”，AI学习者代理商与知识渊博的AI教师代理商进行二元教学对话。这些相互作用强调外部结构化对话是知识获取的核心机制，与仅取决于内部推论或模式识别的方法形成对比。我们的调查重点是不同的教学策略如何在本体习得的背景下影响AI学习过程。经验结果表明，这种对话方法尤其是那些涉及混合方向相互作用的方法，将自上而下的解释与学习者提出的质疑相结合，从而提高了LLM的获取和应用新知识的能力，超过了单向教学方法，并超过了对结构性知识的直接访问，典型的培训数据集中的形式。这些发现表明，将教学和心理学见解整合到AI和机器人培训中可以大大改善培训后知识的获取和反应质量。这种方法为迅速工程等现有策略提供了互补的途径</li>
</ul>

<h3>Title: Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing</h3>
<ul>
<li><strong>Authors: </strong>David James Woo, Yangyang Yu, Kai Guo, Yilin Huang, April Ka Yeng Fung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21073">https://arxiv.org/abs/2507.21073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21073">https://arxiv.org/pdf/2507.21073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21073]] Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing(https://arxiv.org/abs/2507.21073)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Text generated by artificial intelligence (AI) chatbots is increasingly used in English as a foreign language (EFL) writing contexts, yet its impact on students' expository writing process and compositions remains understudied. This research examines how EFL secondary students edit AI-generated text. Exploring editing behaviors in their expository writing process and in expository compositions, and their effect on human-rated scores for content, organization, language, and overall quality. Participants were 39 Hong Kong secondary students who wrote an expository composition with AI chatbots in a workshop. A convergent design was employed to analyze their screen recordings and compositions to examine students' editing behaviors and writing qualities. Analytical methods included qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis. We analyzed over 260 edits per dataset, and identified two editing patterns: one where students refined introductory units repeatedly before progressing, and another where they quickly shifted to extensive edits in body units (e.g., topic and supporting sentences). MLR analyses revealed that the number of AI-generated words positively predicted all score dimensions, while most editing variables showed minimal impact. These results suggest a disconnect between students' significant editing effort and improved composition quality, indicating AI supports but does not replace writing skills. The findings highlight the importance of genre-specific instruction and process-focused writing before AI integration. Educators should also develop assessments valuing both process and product to encourage critical engagement with AI text.</li>
<li><strong>摘要：</strong>人工智能（AI）聊天机器人生成的文本越来越多地用作外语（EFL）写作环境，但其对学生的说明性写作过程和作品的影响仍在研究中。这项研究研究了EFL中学学生如何编辑AI生成的文本。探索在说明性写作过程和说明性作品中探索编辑行为，以及它们对内容，组织，语言和整体质量的人等级分数的影响。参与者是39名香港中学学生，他们在研讨会上写了带有AI聊天机器人的说明性作品。采用收敛设计来分析其屏幕录制和作品，以检查学生的编辑行为和写作素质。分析方法包括定性编码，描述性统计，时间序列分析，人为评分和多个线性回归分析。我们每个数据集分析了260多个编辑，并确定了两种编辑模式：一个学生在进行前进之前反复完善介绍单元，而另一个则迅速转移到身体单位的广泛编辑（例如主题和支持句子）中。 MLR分析表明，AI生成的单词的数量积极地预测了所有分数维度，而大多数编辑变量显示出最小的影响。这些结果表明，学生的重大编辑工作与提高的组成质量之间存在断开连接，表明AI支持但不能取代写作技巧。这些发现突出了在AI集成之前，特定于流派的指导和以过程为中心的写作的重要性。教育工作者还应制定评估过程和产品的评估，以鼓励与AI文本进行严格的参与。</li>
</ul>

<h3>Title: Which symbol grounding problem should we try to solve?</h3>
<ul>
<li><strong>Authors: </strong>Vincent C. Müller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21080">https://arxiv.org/abs/2507.21080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21080">https://arxiv.org/pdf/2507.21080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21080]] Which symbol grounding problem should we try to solve?(https://arxiv.org/abs/2507.21080)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Floridi and Taddeo propose a condition of "zero semantic commitment" for solutions to the grounding problem, and a solution to it. I argue briefly that their condition cannot be fulfilled, not even by their own solution. After a look at Luc Steels' very different competing suggestion, I suggest that we need to re-think what the problem is and what role the 'goals' in a system play in formulating the problem. On the basis of a proper understanding of computing, I come to the conclusion that the only sensible grounding problem is how we can explain and re-produce the behavioral ability and function of meaning in artificial computational agents</li>
<li><strong>摘要：</strong>Floridi和Taddeo提出了解决方案问题的“零语义承诺”的条件，并提出了解决方案的解决方案。我简短地认为，即使他们自己的解决方案也无法满足他们的状况。看完卢克斯斯（Luc Steels）非常不同的竞争建议之后，我建议我们需要重新思考问题是什么，以及“目标”在制定问题中扮演的“目标”的角色。根据对计算的正确理解，我得出的结论是，唯一明智的接地问题是我们如何解释和重新产生人工计算剂中意义的行为能力和功能</li>
</ul>

<h3>Title: ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs</h3>
<ul>
<li><strong>Authors: </strong>Franck Bardol</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21083">https://arxiv.org/abs/2507.21083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21083">https://arxiv.org/pdf/2507.21083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21083]] ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs(https://arxiv.org/abs/2507.21083)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: this https URL</li>
<li><strong>摘要：</strong>诸如GPT-4之类的大型语言模型不仅根据提出的问题来调整他们的回答，而且还根据其情感上的措辞来调整他们的回答。我们从系统地改变了156个提示的情感语气 - 跨越有争议的和日常的话题 - 并分析它如何影响模型响应。我们的发现表明，GPT-4对一个负面问题的响应的可能性少于中立问题。这表明模型过度纠正，通常转向中立或积极性的“反弹”偏见。关于敏感主题（例如正义或政治），这种效果更加明显：基于音调的变化被抑制，表明一致性替代了。我们介绍了诸如“音平面”之类的概念 - 响应负响应的下限 - 并使用音调价值过渡矩阵来量化行为。基于1536维嵌入的可视化量证实了基于音调的语义漂移。我们的作品突出了一类未充满偏见的偏见，这些偏见是由提示中的情感框架驱动的，对AI的一致性和信任产生了影响。代码和数据可用：此HTTPS URL</li>
</ul>

<h3>Title: Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing</h3>
<ul>
<li><strong>Authors: </strong>Aly M. Kassem, Zhuan Shi, Negar Rostamzadeh, Golnoosh Farnadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21084">https://arxiv.org/abs/2507.21084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21084">https://arxiv.org/pdf/2507.21084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21084]] Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing(https://arxiv.org/abs/2507.21084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are frequently fine-tuned or unlearned to adapt to new tasks or eliminate undesirable behaviors. While existing evaluation methods assess performance after such interventions, there remains no general approach for detecting unintended side effects, such as unlearning biology content degrading performance on chemistry tasks, particularly when these effects are unpredictable or emergent. To address this issue, we introduce MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight framework for identifying these side effects using sparse model diffing. MNEME compares base and fine-tuned models on task-agnostic data (for example, The Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent accuracy in predicting side effects, aligning with known benchmarks and requiring no custom heuristics. Furthermore, we show that retraining on high-activation samples can partially reverse these effects. Our results demonstrate that sparse probing and diffing offer a scalable and automated lens into fine-tuning-induced model changes, providing practical tools for understanding and managing LLM behavior.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）经常进行微调或未读过以适应新任务或消除不良行为。尽管现有的评估方法评估了这种干预措施后的绩效，但仍未有一般的方法来检测意外副作用，例如在化学任务上降低了生物学内容降解性能，尤其是当这些影响不可预测或出现时。为了解决这个问题，我们介绍了Mneme，模型扩散以评估机械效应，这是使用稀疏模型扩散来识别这些副作用的轻量级框架。 Mneme比较了任务不合时宜数据（例如，堆，LMSYS-CHAT-1M）的基础和微调模型，而无需访问微调数据以隔离行为变化。在三种情况下应用于五个LLM：WMDP知识学习，紧急未对准和良性微调，Mneme在预测副作用方面的准确性高达95％，与已知的基准测试和不需要自定义启发式学。此外，我们表明对高激活样品进行重新培训可以部分扭转这些影响。我们的结果表明，稀疏的探测和扩散为微调引起的模型更改提供了可扩展和自动化的镜头，提供了理解和管理LLM行为的实用工具。</li>
</ul>

<h3>Title: Multi-Amateur Contrastive Decoding for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Jaydip Sen, Subhasis Dasgupta, Hetvi Waghela</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21086">https://arxiv.org/abs/2507.21086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21086">https://arxiv.org/pdf/2507.21086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21086]] Multi-Amateur Contrastive Decoding for Text Generation(https://arxiv.org/abs/2507.21086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Contrastive Decoding (CD) has emerged as an effective inference-time strategy for enhancing open-ended text generation by exploiting the divergence in output probabilities between a large expert language model and a smaller amateur model. Although CD improves coherence and fluency, its dependence on a single amateur restricts its capacity to capture the diverse and multifaceted failure modes of language generation, such as repetition, hallucination, and stylistic drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a generalization of the CD framework that employs an ensemble of amateur models to more comprehensively characterize undesirable generation patterns. MACD integrates contrastive signals through both averaging and consensus penalization mechanisms and extends the plausibility constraint to operate effectively in the multi-amateur setting. Furthermore, the framework enables controllable generation by incorporating amateurs with targeted stylistic or content biases. Experimental results across multiple domains, such as news, encyclopedic, and narrative, demonstrate that MACD consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning.</li>
<li><strong>摘要：</strong>对比度解码（CD）已成为一种有效的推理时间策略，用于通过利用大型专家语言模型和较小的业余模型之间的输出概率的差异来增强开放式文本生成。尽管CD提高了连贯性和流利度，但其对单个业余爱好的依赖限制了其捕获语言产生多样化和多方面的失败模式的能力，例如重复，幻觉和风格漂移。本文提出了多运动对比解码（MACD），这是对CD框架的概括，该框架采用了业余模型的集合来更全面地表征不良的生成模式。 MACD通过平均和共识惩罚机制整合了对比信号，并将合理性约束扩展到在多机构环境中有效运行。此外，该框架可以通过将业余爱好者与有针对性的风格或内容偏见结合在一起，从而实现可控制的生成。跨多个领域（例如新闻，百科全书和叙事）的实验结果表明，MACD在流利性，连贯性，多样性和适应性方面始终超过常规解码方法和原始CD方法，而无需额外的培训或精心调整。</li>
</ul>

<h3>Title: QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad AL-Smadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21095">https://arxiv.org/abs/2507.21095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21095">https://arxiv.org/pdf/2507.21095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21095]] QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning(https://arxiv.org/abs/2507.21095)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents our approach to the CheckThat! 2025 Task 1 on subjectivity detection, where systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author or presents an objective view on the covered topic. We propose a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. Our system leveraged pre-trained transformers with additional lexical features: for Arabic we used AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined with TF-IDF features through a gating mechanism. We evaluated our system in monolingual, multilingual, and zero-shot settings across multiple languages including English, Arabic, German, Italian, and several unseen languages. The results demonstrate the effectiveness of our approach, achieving competitive performance across different languages with notable success in the monolingual setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank 1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an ablation analysis that demonstrated the importance of combining TF-IDF features with the gating mechanism and the cross-lingual transfer for subjectivity detection. Furthermore, our analysis reveals the model's sensitivity to both the order of cross-lingual fine-tuning and the linguistic proximity of the training languages.</li>
<li><strong>摘要：</strong>本文介绍了我们进行检查的方法！ 2025任务1关于主观性检测的任务1，其中挑战系统以区分新闻文章的句子是否表达了作者的主观观点或对涵盖主题的客观观点。我们提出了一个具有功能增强的变压器体系结构，该体系结构将预训练的语言模型的上下文嵌入与统计和语言特征相结合。我们的系统利用了具有其他词汇特征的预训练的变压器：对于阿拉伯语，我们使用了Araelectra与部分语音（POS）标签（POS）标签和TF-IDF功能进行了增强，而对于其他语言，我们对跨语言DEBERTA〜V3模型进行了微调，并通过登台功能与TF-IDF相结合。我们通过多种语言，包括英语，阿拉伯语，德语，意大利语和几种看不见的语言评估了单语，多语言和零弹性设置的系统。结果证明了我们方法的有效性，在英语单语言环境中取得了显着成功的不同语言的竞争性能（宏观-F1 = 0.8052排名第一），德语（宏观-F1 = 0.8013的排名第三名），阿拉伯语（级别4，宏观-F1 = 0.5771）和Romanian（Romanian and Romanian）（级别）（级别）（级别）（MACRO-f1S）。我们还进行了消融分析，该分析证明了将TF-IDF特征与门控机制和跨语性转移进行主观性检测的重要性。此外，我们的分析揭示了该模型对跨语性微调顺序的敏感性和培训语言的语言邻近。</li>
</ul>

<h3>Title: Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting</h3>
<ul>
<li><strong>Authors: </strong>Chloe Ho, Ishneet Sukhvinder Singh, Diya Sharma, Tanvi Reddy Anumandla, Michael Lu, Vasu Sharma, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21099">https://arxiv.org/abs/2507.21099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21099">https://arxiv.org/pdf/2507.21099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21099]] Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting(https://arxiv.org/abs/2507.21099)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Search algorithms and user query relevance have given LLMs the ability to return relevant information, but the effect of content phrasing on ad visibility remains underexplored. We investigate how LLM-based rewriting of advertisements can improve their ranking in retrieval systems and inclusion in generated LLM responses, without modifying the retrieval model itself. We introduce a supervised fine-tuning framework with a custom loss balancing semantic relevance and content fidelity. To evaluate effectiveness, we propose two metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion frequency improvement). Our approach presents a scalable method to optimize ad phrasing, enhancing visibility in retrieval-based LLM workflows. Experiments across both instruction-based and few-shot prompting demonstrate that PPO trained models outperform both prompt engineering and supervised fine-tuning in most cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in instruction-based prompting. These results highlight the importance of how the ad is written before retrieval and prompt format and reinforcement learning in effective ad rewriting for LLM integrated retrieval systems.</li>
<li><strong>摘要：</strong>搜索算法和用户查询相关性使LLMS具有返回相关信息的能力，但是内容措辞对AD可见性的影响仍未得到充分兴奋。我们研究了基于LLM的广告的重写如何在不修改检索模型本身的情况下改善其在检索系统中的排名并包含在生成的LLM响应中。我们引入了一个有监督的微调框架，并具有定制的损失平衡语义相关性和内容保真度。为了评估有效性，我们提出了两个指标：deltamrr@k（排名改进）和deltadir@k（纳入频率提高）。我们的方法提出了一种可扩展的方法来优化广告措辞，从而在基于检索的LLM工作流中提高了可见度。在基于指令和少量启动的基于指令的实验表明，在大多数情况下，经过PPO训练的模型在及时及时进行了迅速工程和监督的微调，在基于指令的提示中达到了2.79 deltadir@5和0.0073 deltamrr@5。这些结果强调了在有效的LLM集成检索系统有效的AD重写中，在检索和及时的格式和强化学习之前编写广告的重要性。</li>
</ul>

<h3>Title: Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams</h3>
<ul>
<li><strong>Authors: </strong>Rob Manson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21107">https://arxiv.org/abs/2507.21107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21107">https://arxiv.org/pdf/2507.21107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21107]] Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams(https://arxiv.org/abs/2507.21107)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We propose Curved Inference - a geometric Interpretability framework that tracks how the residual stream trajectory of a large language model bends in response to shifts in semantic concern. Across 20 matched prompts spanning emotional, moral, perspective, logical, identity, environmental, and nonsense domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics, with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These metrics are computed under a pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry rather than raw coordinate structure. We find that concern-shifted prompts reliably alter internal activation trajectories in both models - with LLaMA exhibiting consistent, statistically significant scaling in both curvature and salience as concern intensity increases. Gemma also responds to concern but shows weaker differentiation between moderate and strong variants. Our results support a two-layer view of LLM geometry - a latent conceptual structure encoded in the embedding space, and a contextual trajectory shaped by prompt-specific inference. Curved Inference reveals how models navigate, reorient, or reinforce semantic meaning over depth, offering a principled method for diagnosing alignment, abstraction, and emergent inference dynamics. These findings offer fresh insight into semantic abstraction and model alignment through the lens of Curved Inference.</li>
<li><strong>摘要：</strong>我们提出了弯曲的推理 - 一种几何解释性框架，该框架跟踪大型语言模型的残留流轨迹如何响应语义关注的转移。在20个匹配的提示中，我们使用五个本机空间指标分析了跨越情感，道德，透视，逻辑，环境，环境和胡说八道的域，我们分析了Gemma3-1b和Llama3.2-3b，主要关注曲率（\ k {appa} _i）和salience（s（t））。这些度量标准是在源自无用的矩阵的回调语义度量下计算的，以确保所有测量值反映了令牌对准的几何形状，而不是原始的坐标结构。我们发现，关注转移的提示可可靠地改变了这两个模型中的内部激活轨迹 - 随着关注强度的增加，美洲驼在曲率和显着性上均表现出一致的，统计学上显着的缩放。 Gemma也对此表示反应，但显示出中度和强变体之间的差异较弱。我们的结果支持LLM几何形状的两层视图 - 嵌入空间中编码的潜在概念结构，以及由及时特定推断所塑造的上下文轨迹。弯曲的推论揭示了模型如何导航，重新定向或增强语义含义超过深度，从而提供了一种诊断比对，抽象和新兴推理动态的原则方法。这些发现为语义抽象和模型对齐方式提供了新的洞察力，并通过弯曲的推理晶状体进行了对齐。</li>
</ul>

<h3>Title: SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Kezhen Zhong, Basem Suleiman, Abdelkarim Erradi, Shijing Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21110">https://arxiv.org/abs/2507.21110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21110">https://arxiv.org/pdf/2507.21110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21110]] SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering(https://arxiv.org/abs/2507.21110)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>This paper introduces SemRAG, an enhanced Retrieval Augmented Generation (RAG) framework that efficiently integrates domain-specific knowledge using semantic chunking and knowledge graphs without extensive fine-tuning. Integrating domain-specific knowledge into large language models (LLMs) is crucial for improving their performance in specialized tasks. Yet, existing adaptations are computationally expensive, prone to overfitting and limit scalability. To address these challenges, SemRAG employs a semantic chunking algorithm that segments documents based on the cosine similarity from sentence embeddings, preserving semantic coherence while reducing computational overhead. Additionally, by structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance and correctness of retrieved information from the Knowledge Graph, outperforming traditional RAG methods. Furthermore, we investigate the optimization of buffer sizes for different data corpus, as optimizing buffer sizes tailored to specific datasets can further improve retrieval performance, as integration of knowledge graphs strengthens entity relationships for better contextual comprehension. The primary advantage of SemRAG is its ability to create an efficient, accurate domain-specific LLM pipeline while avoiding resource-intensive fine-tuning. This makes it a practical and scalable approach aligned with sustainability goals, offering a viable solution for AI applications in domain-specific fields.</li>
<li><strong>摘要：</strong>本文介绍了Semrag，这是一种增强的检索增强发电（RAG）框架，该框架有效地使用语义块和知识图有效地集成了特定于域的知识，而无需进行广泛的微调。将特定于领域的知识集成到大语言模型（LLMS）中对于改善其在专业任务中的表现至关重要。然而，现有的适应在计算上是昂贵的，容易拟合并限制可扩展性。为了应对这些挑战，Semrag采用了一种语义块算法，该算法基于句子嵌入的余弦相似性将文档划分，从而保留语义连贯性的同时减少了计算开销。此外，通过将检索到的信息构造到知识图中，Semrag捕获了实体之间的关系，从而提高了检索准确性和上下文理解。多台化抹布和Wikipedia数据集的实验结果表明，Semrag显着提高了从知识图中检索到的信息的相关性和正确性，表现优于传统的抹布方法。此外，我们研究了对不同数据语料库的缓冲尺寸的优化，因为对特定数据集量身定制的缓冲尺寸可以进一步提高检索性能，因为知识图的集成增强了实体关系，从而可以更好地理解上下文理解。 Semrag的主要优点是它具有创建高效，准确的域特异性LLM管道的能力，同时避免资源密集型微调。这使其成为一种与可持续性目标一致的实用且可扩展的方法，为特定领域的AI应用提供了可行的解决方案。</li>
</ul>

<h3>Title: TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law</h3>
<ul>
<li><strong>Authors: </strong>Zheng Hui, Yijiang River Dong, Ehsan Shareghi, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21134">https://arxiv.org/abs/2507.21134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21134">https://arxiv.org/pdf/2507.21134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21134]] TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law(https://arxiv.org/abs/2507.21134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: this https URL</li>
<li><strong>摘要：</strong>由于大型语言模型（LLM）越来越多地部署在法律，金融和医学等高风险领域，因此系统地评估其特定领域的安全性和合规性变得至关重要。尽管先前的工作主要集中在提高这些领域的LLM性能上，但它经常忽略了对域特异性安全风险的评估。为了弥合这一差距，我们首先根据医学伦理学的AMA原则，专业行为的ABA模型规则和CFA Institute伦理守则定义了LLMS特定领域的安全原则。在此基金会的基础上，我们介绍了Trident Bench，这是一种针对法律，金融和医疗领域的LLM安全性的基准。我们在三叉戟基台上评估了19个通用和域特有的模型，并表明它有效地揭示了关键的安全差距 - 强大的通才模型（例如GPT，GECH，Gemini）可以满足基本期望，而域特异性模型通常会与微妙的道德差异斗争。这凸显了迫切需要针对特定于域的安全性安全。通过介绍Trident Bench，我们的工作提供了研究法律和金融安全性LLM安全性的首批系统资源之一，并为未来的研究奠定了基础，旨在降低在专业监管领域中部署LLM的安全风险。代码和基准将在以下位置发布：此HTTPS URL</li>
</ul>

<h3>Title: TTS-1 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Oleg Atamanenko, Anna Chalova, Joseph Coombes, Nikki Cope, Phillip Dang, Zhifeng Deng, Jimmy Du, Michael Ermolenko, Feifan Fan, Yufei Feng, Cheryl Fichter, Pavel Filimonov, Louis Fischer, Kylan Gibbs, Valeria Gusarova, Pavel Karpik, Andreas Assad Kottner, Ian Lee, Oliver Louie, Jasmine Mai, Mikhail Mamontov, Suri Mao, Nurullah Morshed, Igor Poletaev, Florin Radu, Dmytro Semernia, Evgenii Shingarev, Vikram Sivaraja, Peter Skirko, Rinat Takhautdinov, Robert Villahermosa, Jean Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21138">https://arxiv.org/abs/2507.21138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21138">https://arxiv.org/pdf/2507.21138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21138]] TTS-1 Technical Report(https://arxiv.org/abs/2507.21138)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.</li>
<li><strong>摘要：</strong>我们介绍了Inworld TTS-1，这是两个基于变压器的自动回归文本到语音（TTS）模型的集合。我们最大的型号TTS-1-MAX具有8.8B参数，设计为苛刻应用中的最高质量和表现力。 TTS-1是我们最有效的模型，具有1.6B参数，用于实时语音合成和设备上的用例。通过缩放火车时间计算并应用了语音语言模型（SpeechLM）组件的预训练，微调和RL对齐的顺序过程，这两个模型都可以在各种基准上实现最新性能，表明纯粹依赖于演讲者的语音的纯粹质量。 Inworld TTS-1和TTS-1-Max可以以低潜伏期生成高分辨率的48 kHz语音，并通过音频标记支持11种具有精细元素情感控制和非语言发声的语言。我们还根据MIT许可开放培训和建模代码。</li>
</ul>

<h3>Title: Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question</h3>
<ul>
<li><strong>Authors: </strong>Rafael Rosales, Santiago Miret</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21168">https://arxiv.org/abs/2507.21168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21168">https://arxiv.org/pdf/2507.21168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21168]] Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question(https://arxiv.org/abs/2507.21168)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Effectively leveraging diversity has been shown to improve performance for various machine learning models, including large language models (LLMs). However, determining the most effective way of using diversity remains a challenge. In this work, we compare two diversity approaches for answering binary questions using LLMs: model diversity, which relies on multiple models answering the same question, and question interpretation diversity, which relies on using the same model to answer the same question framed in different ways. For both cases, we apply majority voting as the ensemble consensus heuristic to determine the final answer. Our experiments on boolq, strategyqa, and pubmedqa show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity. Furthermore, our analysis of GPT and LLaMa shows that model diversity typically produces results between the best and the worst ensemble members without clear improvement.</li>
<li><strong>摘要：</strong>有效利用多样性已被证明可以提高各种机器学习模型（包括大语言模型（LLM））的性能。但是，确定使用多样性的最有效方法仍然是一个挑战。在这项工作中，我们比较了使用LLMS回答二进制问题的两种多样性方法：模型多样性依赖于回答相同问题的多个模型和问题解释多样性，这些模型依赖于使用相同的模型以不同方式回答相同的问题。在这两种情况下，我们都将多数投票作为合奏共识启发式，以确定最终答案。我们对Boolq，StrategyQA和PubMedQA的实验表明，与模型多样性相比，问题解释多样性始终导致更好的集成精度。此外，我们对GPT和Llama的分析表明，模型多样性通常会在最佳和最差的合奏成员之间产生结果，而无需明显改进。</li>
</ul>

<h3>Title: Do Large Language Models Understand Morality Across Cultures?</h3>
<ul>
<li><strong>Authors: </strong>Hadi Mohammadi, Yasmeen F.S.S. Meijer, Efthymia Papadopoulou, Ayoub Bagheri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21319">https://arxiv.org/abs/2507.21319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21319">https://arxiv.org/pdf/2507.21319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21319]] Do Large Language Models Understand Morality Across Cultures?(https://arxiv.org/abs/2507.21319)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have established them as powerful tools across numerous domains. However, persistent concerns about embedded biases, such as gender, racial, and cultural biases arising from their training data, raise significant questions about the ethical use and societal consequences of these technologies. This study investigates the extent to which LLMs capture cross-cultural differences and similarities in moral perspectives. Specifically, we examine whether LLM outputs align with patterns observed in international survey data on moral attitudes. To this end, we employ three complementary methods: (1) comparing variances in moral scores produced by models versus those reported in surveys, (2) conducting cluster alignment analyses to assess correspondence between country groupings derived from LLM outputs and survey data, and (3) directly probing models with comparative prompts using systematically chosen token pairs. Our results reveal that current LLMs often fail to reproduce the full spectrum of cross-cultural moral variation, tending to compress differences and exhibit low alignment with empirical survey patterns. These findings highlight a pressing need for more robust approaches to mitigate biases and improve cultural representativeness in LLMs. We conclude by discussing the implications for the responsible development and global deployment of LLMs, emphasizing fairness and ethical alignment.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进步已将它们确立为跨众多领域的强大工具。但是，对嵌入式偏见的持续关注，例如由于培训数据引起的性别，种族和文化偏见，引发了有关这些技术的道德使用和社会后果的重大问题。这项研究研究了LLM在道德观点中捕获跨文化差异和相似性的程度。具体而言，我们检查了LLM输出是否与关于道德态度的国际调查数据中观察到的模式保持一致。为此，我们采用了三种互补方法：（1）比较模型所产生的道德得分方差与在调查中报告的方差，（2）进行集群一致性分析以评估来自LLM输出和调查数据的国家组之间的对应关系，以及（3）使用系统选择的代币Pairs Pairs Pairs Pairs Pairs。我们的结果表明，当前的LLM通常无法再现跨文化道德差异的完整范围，倾向于压缩差异，并且与经验调查模式表现出较低的比对。这些发现突出了对更强大的方法缓解偏见并改善LLM中的文化代表性的紧迫性。我们通过讨论对LLM的负责任发展和全球部署的影响，强调公平和道德一致性。</li>
</ul>

<h3>Title: A Deep Learning Automatic Speech Recognition Model for Shona Language</h3>
<ul>
<li><strong>Authors: </strong>Leslie Wellington Sirora, Mainford Mutandavari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21331">https://arxiv.org/abs/2507.21331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21331">https://arxiv.org/pdf/2507.21331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21331]] A Deep Learning Automatic Speech Recognition Model for Shona Language(https://arxiv.org/abs/2507.21331)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study presented the development of a deep learning-based Automatic Speech Recognition system for Shona, a low-resource language characterized by unique tonal and grammatical complexities. The research aimed to address the challenges posed by limited training data, lack of labelled data, and the intricate tonal nuances present in Shona speech, with the objective of achieving significant improvements in recognition accuracy compared to traditional statistical models. The research first explored the feasibility of using deep learning to develop an accurate ASR system for Shona. Second, it investigated the specific challenges involved in designing and implementing deep learning architectures for Shona speech recognition and proposed strategies to mitigate these challenges. Lastly, it compared the performance of the deep learning-based model with existing statistical models in terms of accuracy. The developed ASR system utilized a hybrid architecture consisting of a Convolutional Neural Network for acoustic modelling and a Long Short-Term Memory network for language modelling. To overcome the scarcity of data, data augmentation techniques and transfer learning were employed. Attention mechanisms were also incorporated to accommodate the tonal nature of Shona speech. The resulting ASR system achieved impressive results, with a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These metrics indicated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona. This study contributed to the advancement of ASR technology for under-resourced languages like Shona, ultimately fostering improved accessibility and communication for Shona speakers worldwide.</li>
<li><strong>摘要：</strong>这项研究介绍了Shona的基于深度学习的自动语音识别系统的发展，Shona是一种低资源语言，其特征是独特的音调和语法复杂性。该研究旨在应对有限的培训数据，缺乏标记的数据以及Shona语音中存在的复杂色调细微差别所带来的挑战，目的是与传统的统计模型相比，识别准确性取得了重大提高。该研究首先探讨了使用深度学习为Shona开发准确的ASR系统的可行性。其次，它调查了设计和实施深度学习架构的特定挑战，以供Shona语音识别以及提出缓解这些挑战的策略。最后，它在准确性方面将基于深度学习模型的性能与现有统计模型进行了比较。开发的ASR系统利用了由卷积神经网络组成的混合体系结构，用于声学建模和长期的短期记忆网络用于语言建模。为了克服数据的稀缺性，采用了数据增强技术和转移学习。还纳入了注意机制，以适应Shona演讲的音调性质。所得的ASR系统取得了令人印象深刻的结果，单词错误率为29％，音素错误率为12％，总体准确度为74％。这些指标表明，深度学习的潜力增强了舒纳（Shona）等资源不足语言的ASR准确性。这项研究促进了ASR技术在诸如Shona之类的资源不足的语言中的发展，最终促进了全球Shona演讲者的可访问性和沟通的改善。</li>
</ul>

<h3>Title: StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Satyananda Kashyap, Sola Shirai, Nandana Mihindukulasooriya, Horst Samulowitz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21340">https://arxiv.org/abs/2507.21340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21340">https://arxiv.org/pdf/2507.21340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21340]] StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation(https://arxiv.org/abs/2507.21340)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.</li>
<li><strong>摘要：</strong>从文本中提取结构化信息，例如可以增强表格数据的键值对，在许多企业用例中都非常有用。尽管大型语言模型（LLMS）已使许多自动化管道将自然语言转换为结构化格式，但仍然缺乏评估其提取质量的基准，尤其是在特定的特定组织中的特定领域或专注的文档。通过手动注释构建此类基准是劳动密集型的，并限制了基准的大小和可扩展性。在这项工作中，我们提出了structText，这是一个端到端框架，用于自动生成使用现有表格数据从文本中提取键值提取的高保真基准测试。它使用可用的表格数据作为结构化的地面真理，并遵循两个阶段的``计划 - 执行''管道来合成生成相应的自然语言文本。为了确保文本和结构化来源之间的对齐方式，我们引入了一种多维评估策略，该策略结合了（a）基于LLM的事实，幻觉和连贯性的判断，以及（b）测量数字和时间准确性的客观提取指标。我们在49个数据集中评估了71,539个示例的建议方法。结果表明，虽然LLMS具有强大的事实准确性并避免幻觉，但它们在产生可提取文本时与叙事连贯性斗争。值得注意的是，模型假定具有高保真度的数值和时间信息，但这些信息嵌入了抵制自动提取的叙述中。我们发布一个框架，包括数据集，评估工具和基线提取系统，以支持持续的研究。</li>
</ul>

<h3>Title: Turbocharging Web Automation: The Impact of Compressed History States</h3>
<ul>
<li><strong>Authors: </strong>Xiyue Zhu, Peng Tang, Haofu Liao, Srikar Appalaraju</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21369">https://arxiv.org/abs/2507.21369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21369">https://arxiv.org/pdf/2507.21369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21369]] Turbocharging Web Automation: The Impact of Compressed History States(https://arxiv.org/abs/2507.21369)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models have led to a leap forward in web automation. The current web automation approaches take the current web state, history actions, and language instruction as inputs to predict the next action, overlooking the importance of history states. However, the highly verbose nature of web page states can result in long input sequences and sparse information, hampering the effective utilization of history states. In this paper, we propose a novel web history compressor approach to turbocharge web automation using history states. Our approach employs a history compressor module that distills the most task-relevant information from each history state into a fixed-length short representation, mitigating the challenges posed by the highly verbose history states. Experiments are conducted on the Mind2Web and WebLINX datasets to evaluate the effectiveness of our approach. Results show that our approach obtains 1.2-5.4% absolute accuracy improvements compared to the baseline approach without history inputs.</li>
<li><strong>摘要：</strong>语言模型导致Web自动化方向飞跃。当前的Web自动化方法将当前的Web状态，历史记录动作和语言指令作为预测下一个动作的输入，从而忽略了历史状态的重要性。但是，网页状态的高度冗长性可能会导致长输入序列和稀疏信息，从而阻碍了历史状态的有效利用。在本文中，我们提出了一种新型的Web历史压缩机方法，用于使用历史状状态进行涡轮增压网络自动化。我们的方法采用历史记录压缩机模块，该模块将每个历史记录状态最重要的信息提炼成固定的短长表示，从而减轻了高度详细的历史记录状态所带来的挑战。实验是在Mind2Web和Weblinx数据集上进行的，以评估我们方法的有效性。结果表明，与没有历史记录输入的基线方法相比，我们的方法获得了1.2-5.4％的绝对精度提高。</li>
</ul>

<h3>Title: MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations</h3>
<ul>
<li><strong>Authors: </strong>Elias Lumer, Anmol Gulati, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, James A. Burke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21428">https://arxiv.org/abs/2507.21428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21428">https://arxiv.org/pdf/2507.21428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21428]] MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations(https://arxiv.org/abs/2507.21428)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）代理在动态搜索和合并相关工具或模型上下文协议（MCP）服务器方面显示出显着的自主功能。但是，固定上下文Windows Windows限制了多转交互的效率，需要重复，独立的工具使用。我们介绍了Memtool，这是一个短期内存框架，使LLM代理能够在多转交流中动态管理工具或MCP服务器上下文。 Memtool提供了三个代理体系结构：1）自主代理模式，授予完整的工具管理自治，2）工作流程模式，提供确定性控制的情况下，没有自治，以及3）混合模式，结合了自主和确定性控制。在ScaleMCP基准测试上评估13+ LLM的每个MEMTOOL模式，我们进行了100多次连续的用户交互，测量工具拆卸比（短期存储效率）和任务完成精度。在自主剂模式下，推理LLMS实现了高刀具驱动效率（在3窗口平均值上为90-94％），而中型型号的效率明显降低（0-60％）。工作流程和混合模式始终有效地管理工具拆卸，而在任务完成时自动驾驶和混合模式表现出色。我们根据任务准确性，代理和模型功能为每个孟买模式提供权衡和建议。</li>
</ul>

<h3>Title: Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour</h3>
<ul>
<li><strong>Authors: </strong>Tareq Alsaleh, Bilal Farooq</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21432">https://arxiv.org/abs/2507.21432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21432">https://arxiv.org/pdf/2507.21432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21432]] Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour(https://arxiv.org/abs/2507.21432)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.</li>
<li><strong>摘要：</strong>这项研究研究了用于旅行模式选择预测的开放访问，可部署的可因果关系模型（LLMS），并介绍了Litransmc，这是第一个为此任务开发的微调因果LLM。我们在三个陈述和揭示的偏好数据集中系统地基准了11个LLM（1-12B参数），测试396个配置并生成79,000多个合成通勤者预测。除了预测精度之外，我们还评估了使用伯托进行主题建模和新颖的解释强度指数产生的推理的模型，从而提供了对LLMS如何与行为理论保持一致的决策因素的首次结构化分析。 LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for相同的数据集。这种双重改进，即高即时级的准确性和近乎完美的分布校准，证明了创建集成预测和解释性的本地可部署LLM的可行性。通过将结构化的行为预测与自然语言推理相结合，这项工作将解锁了能够支持基于代理的模拟，策略测试和行为洞察力生成的对话性多任务传输模型的潜力。这些发现为将通用LLMS转变为运输研究和政策制定的专业的，可解释的工具的途径，同时保持隐私，降低成本以及通过当地部署扩大访问权限。</li>
</ul>

<h3>Title: Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench</h3>
<ul>
<li><strong>Authors: </strong>Reuben Narad, Siddharth Suresh, Jiayi Chen, Pine S.L. Dysart-Bricken, Bob Mankoff, Robert Nowak, Jifan Zhang, Lalit Jain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21476">https://arxiv.org/abs/2507.21476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21476">https://arxiv.org/pdf/2507.21476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21476]] Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench(https://arxiv.org/abs/2507.21476)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and this http URL, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.</li>
<li><strong>摘要：</strong>我们提出了一种旨在评估大型语言模型（LLMS）推理和解释卡通标题中精致幽默的能力的基准。随着推理模型越来越饱和数学和科学中的现有基准，对STEM领域以外的模型智能的新颖和挑战评估至关重要。推理从根本上涉及基于文本的幽默理解，需要识别卡通/字幕中的概念与外部文化参考，文字游戏和其他机制之间的联系。 Humorbench包括纽约人标题比赛和HTTP URL的大约300个独特的卡通捕获对，并带有专家注释的评估标题，可识别基本的笑话元素。 LLM根据其对识别笑话元素的幽默和能力的解释进行评估。为了在这项任务上表现良好，模型必须形成和检验有关概念之间关联的假设，并可能从初始解释回溯到最合理的解释。我们对当前SOTA模型的广泛基准测试揭示了三个关键见解：（1）LLM在STEM推理上有效地转移到幽默理解的进展； （2）专门针对STEM推理数据训练的模型在幽默方面仍然表现良好，证明了推理能力的强大可传递性； （3）通过增加思维代币预算来测试时间缩放，在幽默推理的不同模型中产生不同的结果。</li>
</ul>

<h3>Title: Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Arabelly, Jagrut Nemade, Robert D Nowak, Jifan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21482">https://arxiv.org/abs/2507.21482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21482">https://arxiv.org/pdf/2507.21482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21482]] Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs(https://arxiv.org/abs/2507.21482)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\%.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）表现出了各种领域的显着功能，但是为专业应用程序开发高性能模型通常需要大量的人类注释 - 这一过程耗时，劳动力密集且昂贵。在本文中，我们通过利用任务多样性作为有效数据选择的基本原则来解决监督填充（SFT）的标签有效学习问题。这与基于及时多样性的现有方法明显不同。我们的方法基于两个关键观察：1）通常很容易获得不同提示的任务标签； 2）预训练的模型在整个任务之间具有明显不同的置信度。我们将这些事实结合起来，设计了一种简单而有效的抽样策略：我们使用逆置信度加权策略选择了跨任务的示例。这会产生与接受更复杂采样程序训练的模型相当或更好的模型，同时实现了更容易实施和计算较少的密集型模型。值得注意的是，我们的实验结果表明，与完整数据集中的训练相比，该方法可以获得更好的准确性（MMLU得分增加4％）。在各种注释预算和两个指令填充数据集中，我们的算法始终以最佳现有方法的水平或更高，同时将注释成本降低高达80 \％。</li>
</ul>

<h3>Title: VN-MTEB: Vietnamese Massive Text Embedding Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Loc Pham, Tung Luu, Thu Vo, Minh Nguyen, Viet Hoang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21500">https://arxiv.org/abs/2507.21500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21500">https://arxiv.org/pdf/2507.21500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21500]] VN-MTEB: Vietnamese Massive Text Embedding Benchmark(https://arxiv.org/abs/2507.21500)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: this https URL</li>
<li><strong>摘要：</strong>就互联网流量和在线毒性而言，越南在顶级国家中占有一席之地。结果，在应用程序中实施用于建议和内容控制职责的嵌入模型至关重要。但是，在数量和任务多样性方面，缺乏大规模的测试数据集，使科学家在将其部署到现实世界中的大型项目中之前，有效地评估AI模型。为了解决这个重要问题，我们引入了越南基准VN-MTEB，用于嵌入模型，我们通过使用我们的新自动化框架从大型文本嵌入基准测试中翻译大量英语样本来创建。我们利用大语言模型（LLM）和尖端嵌入模型的优势来进行翻译和过滤过程，以保留高质量的样本，保证语言和语义忠诚的自然流动，同时保留命名命名的实体识别（NER）和代码码头。我们的全面基准包括来自专门为越南文本嵌入的六个任务的41个数据集。在我们的分析中，我们发现，使用旋转位置嵌入的模型更大，更复杂的模型越过那些在嵌入任务中使用绝对位置嵌入的模型。数据集可从HuggingFace获得：此HTTPS URL</li>
</ul>

<h3>Title: Persona Vectors: Monitoring and Controlling Character Traits in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21509">https://arxiv.org/abs/2507.21509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21509">https://arxiv.org/pdf/2507.21509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21509]] Persona Vectors: Monitoring and Controlling Character Traits in Language Models(https://arxiv.org/abs/2507.21509)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.</li>
<li><strong>摘要：</strong>大型语言模型通过模拟的“助手”角色与用户互动。虽然助手通常受过训练，以提供帮助，无害和诚实，但有时会偏离这些理想。在本文中，我们确定了模型激活空间媒介的指示，这些媒介具有多种特征，例如邪恶，si虫和幻觉的倾向。我们确认这些向量可用于监视部署时助理个性中的波动。然后，我们应用角色向量来预测和控制培训期间发生的人格转移。我们发现，鉴定后的意图和意外的人格变化都与相关角色向量的转变密切相关。这些转变可以通过事后干预来缓解，或者首先通过一种新的预防转向方法避免。此外，角色向量可用于标记培训数据，这些数据将在数据集级别和单个样本级别上产生不良的人格变化。我们提取角色向量的方法是自动化的，只有自然语言描述，就可以应用于任何感兴趣的人格特征。</li>
</ul>

<h3>Title: TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21526">https://arxiv.org/abs/2507.21526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21526">https://arxiv.org/pdf/2507.21526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21526]] TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling(https://arxiv.org/abs/2507.21526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static sparse attention methods typically degrade accuracy, while dynamic sparsity methods introduce additional computational overhead due to runtime sparse index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic sparsity methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance LLM inference efficiency.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）依赖于注意机制，其时间复杂性随输入序列长度二次增长，从而在预填充阶段创造了重要的计算瓶颈。现有的静态稀疏注意方法通常会降低精度，而动态稀疏方法则引入了由于运行时稀疏索引估计而引起的其他计算开销。为了解决这些局限性，我们提出了Trianglemix，这是一种新型的无训练静态注意力模式。 Trianglemix在浅层中采用密集的关注，并切换到更深层的三角形稀疏图案。广泛的实验表明，三角形将注意力的注意力降低了3.7倍至15.3倍，在深层中，序列长度从32K到128K，而无需牺牲模型的准确性，序列长度从32K到128K的序列长度降低了12％至32％。此外，三角形可以与动态稀疏方法无缝集成以实现进一步的加速，例如在128K时将零工加速19％，突出了其提高LLM推理效率的潜力。</li>
</ul>

<h3>Title: Automatic Classification of User Requirements from Online Feedback -- A Replication Study</h3>
<ul>
<li><strong>Authors: </strong>Meet Bhatt, Nic Boilard, Muhammad Rehan Chaudhary, Cole Thompson, Jacob Idoko, Aakash Sorathiya, Gouri Ginde</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21532">https://arxiv.org/abs/2507.21532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21532">https://arxiv.org/pdf/2507.21532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21532]] Automatic Classification of User Requirements from Online Feedback -- A Replication Study(https://arxiv.org/abs/2507.21532)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), "Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.</li>
<li><strong>摘要：</strong>自然语言处理（NLP）技术已广泛应用于需求工程（RE）字段，以支持分类和歧义检测等任务。尽管RE研究植根于实证研究，但它的重视为RE（NLP4RE）研究的重复关注有限。 NLP的快速前进的领域是为高效，机器辅助工作流提供新的机会，这可以将新的观点和结果带入最前沿。因此，我们复制并扩展了先前的NLP4RE研究（基线），“使用深度学习中的小型数据集环境中的在线反馈对用户要求进行分类”，该研究评估了从用户评论中进行需求分类的不同深度学习模型。我们使用公开发布的源代码重现了原始结果，从而有助于增强基线研究的外部有效性。然后，我们通过评估外部数据集上的模型性能并将结果与GPT-4O零弹射分类器进行比较来扩展设置。此外，我们准备了基线研究的复制研究ID卡，对于评估复制准备就绪很重要。结果表明，不同模型之间的可重复性水平各不相同，幼稚的贝叶斯表现出完美的可重复性。相反，伯特和其他模型显示出混合的结果。我们的发现表明，基线深度学习模型Bert和Elmo在外部数据集上表现出良好的概括功能，而GPT-4O显示出与传统基线机器学习模型相当的性能。此外，我们的评估证实了基线研究的复制准备。但是，缺少的环境设置文件将进一步增强准备。我们将这些丢失的信息包括在我们的复制软件包中，并为我们的研究提供复制研究ID卡，以进一步鼓励和支持我们的研究复制。</li>
</ul>

<h3>Title: MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jungyeon Lee, Kangmin Lee, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21544">https://arxiv.org/abs/2507.21544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21544">https://arxiv.org/pdf/2507.21544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21544]] MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation(https://arxiv.org/abs/2507.21544)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Knowledge conflict often arises in retrieval-augmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection -- especially when multi-hop reasoning is required -- and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.</li>
<li><strong>摘要：</strong>知识冲突通常是出现在检索增强的一代（RAG）系统中，在该系统中，检索文档可能彼此不一致或与模型的参数知识相矛盾。现有用于调查现象的基准具有明显的局限性，包括对回答设置的问题的关注，对实体替代技术的严重依赖以及冲突类型的范围有限。为了解决这些问题，我们提出了一个基于知识图（KG）的框架，该框架在两个相似但不同的上下文之间产生了多样化和微妙的冲突，同时通过KGS的明确关系结构确保了可解释性。基于我们的基准Magic的实验结果提供了有关LLM关于知识冲突的内部运作的有趣见解：开源和专有模型都在与冲突检测中挣扎 - 尤其是在需要多跳上推理的情况下 - 通常无法确定矛盾的确切来源。最后，我们介绍了深入的分析，这些分析是改善LLM在整合多样化（有时甚至相互矛盾的信息）方面的基础。</li>
</ul>

<h3>Title: Libra: Assessing and Improving Reward Model by Learning to Think</h3>
<ul>
<li><strong>Authors: </strong>Meng Zhou, Bei Li, Jiahao Liu, Xiaowen Shi, Yang Bai, Rongxiang Weng, Jingang Wang, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21645">https://arxiv.org/abs/2507.21645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21645">https://arxiv.org/pdf/2507.21645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21645]] Libra: Assessing and Improving Reward Model by Learning to Think(https://arxiv.org/abs/2507.21645)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. However, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations: 1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data.</li>
<li><strong>摘要：</strong>强化学习（RL）已大大提高了大语言模型的推理能力。但是，当前的奖励模型在挑战推理方案和主要的RL培训范式方面的表现不佳取决于基于规则或基于参考的奖励，这引起了两个关键局限性：1）依赖对明确注释的参考答案获得奖励； 2）约束输出格式的要求。这些局限性从根本上阻碍了RL数据扩展和模型推理性能的持续增强。为了解决这些限制，我们提出了一个综合框架，用于评估和改善复杂推理方案中奖励模型的性能。我们首先提出了一个面向推理的基准（天秤座基准），该基准是根据各种具有挑战性的数学问题和高级推理模型系统构建的，以解决在推理方案中现有奖励模型基准的局限性。我们进一步介绍了一种新颖的方法，可以通过学习与思维方法来改善生成奖励模型。根据提出的方法，我们开发了Libra-RM系列，这是一系列具有推理能力的生成奖励模型的集合，可以在各种基准上获得最新的结果。进行了全面的下游实验，实验结果证明了我们的天秤座台和下游应用之间的相关性，以及Libra-RM的潜力进一步通过未标记的数据改善推理模型。</li>
</ul>

<h3>Title: UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</h3>
<ul>
<li><strong>Authors: </strong>Raj Vardhan Tomar, Preslav Nakov, Yuxia Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21652">https://arxiv.org/abs/2507.21652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21652">https://arxiv.org/pdf/2507.21652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21652]] UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases(https://arxiv.org/abs/2507.21652)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at this https URL</li>
<li><strong>摘要：</strong>随着大型推理模型（LRMS）的发展越来越有能力，经营链（COT）推理引入了新的安全挑战。现有的基于SFT的安全对准研究主要集中在安全，高质量响应的过滤提示上，同时忽略了始终引起有害产出的硬提示。为了填补这一空白，我们介绍了Undagechain，这是一个由带有不同来源的硬提示构建的安全对齐数据集，在该数据集中确定了不安全的完成，并明确校正了安全响应。通过将模型暴露于不安全的行为并指导其校正，不足的技术可以增强安全性，同时保持一般的推理能力。我们在不合适的情况下微调了三个LRM，并将它们与最近的Safechain和Star-1进行比较，并在六个分发和五个分配基准中进行了比较。不合适的始终胜过先验数据集，即使是1K子集匹配或超过基线性能，也证明了基于校正的监督的有效性和概括性。我们在此HTTPS URL上发布数据集和代码</li>
</ul>

<h3>Title: Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</h3>
<ul>
<li><strong>Authors: </strong>Yang Wang, Chenghao Xiao, Yizhi Li, Stuart E. Middleton, Noura Al Moubayed, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21750">https://arxiv.org/abs/2507.21750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21750">https://arxiv.org/pdf/2507.21750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21750]] Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal(https://arxiv.org/abs/2507.21750)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) have driven substantial progress in natural language processing but remain vulnerable to adversarial attacks, raising concerns about their robustness in real-world applications. Previous studies have sought to mitigate the impact of adversarial attacks by introducing adversarial perturbations into the training process, either implicitly or explicitly. While both strategies enhance robustness, they often incur high computational costs. In this work, we propose a simple yet effective add-on module that enhances the adversarial robustness of PLMs by removing instance-level principal components, without relying on conventional adversarial defences or perturbing the original training data. Our approach transforms the embedding space to approximate Gaussian properties, thereby reducing its susceptibility to adversarial perturbations while preserving semantic relationships. This transformation aligns embedding distributions in a way that minimises the impact of adversarial noise on decision boundaries, enhancing robustness without requiring adversarial examples or costly training-time augmentation. Evaluations on eight benchmark datasets show that our approach improves adversarial robustness while maintaining comparable before-attack accuracy to baselines, achieving a balanced trade-off between robustness and generalisation.</li>
<li><strong>摘要：</strong>预训练的语言模型（PLM）在自然语言处理方面取得了重大进展，但仍然容易受到对抗性攻击的影响，从而引起了人们对它们在现实世界应用中的稳健性的关注。先前的研究试图通过隐式或显式地将对抗性扰动引入训练过程中，以减轻对抗攻击的影响。尽管两种策略都增强了鲁棒性，但它们通常会产生高计算成本。在这项工作中，我们提出了一个简单而有效的附加模块，该模块通过删除实例级的主组件来增强PLM的对抗性鲁棒性，而无需依赖常规的对抗防御或扰动原始培训数据。我们的方法将嵌入空间转化为近似高斯的特性，从而降低了其对对抗性扰动的敏感性，同时保持语义关系。这种转换以一种将对抗性噪声对决策界限的影响最小化的方式使嵌入分布对齐，增强了鲁棒性，而无需进行对抗性示例或昂贵的训练时间增加。对八个基准数据集的评估表明，我们的方法提高了对抗性的鲁棒性，同时保持了与基准的可比性的准确性，从而实现了稳健性和概括之间的平衡权衡。</li>
</ul>

<h3>Title: AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lian Yan, Haotian Wang, Chen Tang, Haifeng Liu, Tianyang Sun, Liangliang Liu, Yi Guan, Jingchi Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21773">https://arxiv.org/abs/2507.21773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21773">https://arxiv.org/pdf/2507.21773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21773]] AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models(https://arxiv.org/abs/2507.21773)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the agricultural domain, the deployment of large language models (LLMs) is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, we propose AgriEval, the first comprehensive Chinese agricultural benchmark with three main characteristics: (1) Comprehensive Capability Evaluation. AgriEval covers six major agriculture categories and 29 subcategories within agriculture, addressing four core cognitive scenarios: memorization, understanding, inference, and generation. (2) High-Quality Data. The dataset is curated from university-level examinations and assignments, providing a natural and robust benchmark for assessing the capacity of LLMs to apply knowledge and make expert-like decisions. (3) Diverse Formats and Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167 open-ended question-and-answer questions, establishing it as the most extensive agricultural benchmark available to date. We also present comprehensive experimental results over 51 open-source and commercial LLMs. The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, we conduct extensive experiments to investigate factors influencing model performance and propose strategies for enhancement. AgriEval is available at this https URL.</li>
<li><strong>摘要：</strong>在农业领域中，大型语言模型（LLM）的部署受到缺乏培训数据和评估基准的阻碍。为了减轻这个问题，我们提出了阿格雷世（Agrieval），这是第一个全面的中国农业基准，具有三个主要特征：（1）全面的能力评估。 Agrieval涵盖了农业中的六个主要农业类别和29个子类别，解决了四种核心认知情景：记忆，理解，推理和产生。 （2）高质量数据。该数据集是根据大学级的考试和作业策划的，为评估LLMS应用知识和做出类似专家的决策的能力提供了自然而强大的基准。 （3）各种格式和广泛规模。 Agrieval包括14,697个多项选择问题和2,167个开放式的问答问题，将其确定为迄今为止可用的最广泛的农业基准。我们还提出了超过51个开源和商业LLM的全面实验结果。实验结果表明，大多数现有的LLM都难以实现60％的准确性，从而强调了农业LLM的发展潜力。此外，我们进行了广泛的实验，以研究影响模型绩效的因素，并提出增强策略。 Agrieval可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: The Problem with Safety Classification is not just the Models</h3>
<ul>
<li><strong>Authors: </strong>Sowmya Vajjala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21782">https://arxiv.org/abs/2507.21782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21782">https://arxiv.org/pdf/2507.21782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21782]] The Problem with Safety Classification is not just the Models(https://arxiv.org/abs/2507.21782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Studying the robustness of Large Language Models (LLMs) to unsafe behaviors is an important topic of research today. Building safety classification models or guard models, which are fine-tuned models for input/output safety classification for LLMs, is seen as one of the solutions to address the issue. Although there is a lot of research on the safety testing of LLMs themselves, there is little research on evaluating the effectiveness of such safety classifiers or the evaluation datasets used for testing them, especially in multilingual scenarios. In this position paper, we demonstrate how multilingual disparities exist in 5 safety classification models by considering datasets covering 18 languages. At the same time, we identify potential issues with the evaluation datasets, arguing that the shortcomings of current safety classifiers are not only because of the models themselves. We expect that these findings will contribute to the discussion on developing better methods to identify harmful content in LLM inputs across languages.</li>
<li><strong>摘要：</strong>研究大语模型（LLM）对不安全行为的鲁棒性是当今研究的重要话题。建筑安全分类模型或警卫模型是用于LLM的输入/输出安全性分类的微调模型，被视为解决该问题的解决方案之一。尽管对LLM本身的安全测试进行了大量研究，但对于评估此类安全分类器或用于测试它们的评估数据集的有效性的研究很少，尤其是在多语言场景中。在该立场论文中，我们通过考虑涵盖18种语言的数据集，证明了5个安全分类模型中的多语言差异。同时，我们确定了评估数据集的潜在问题，认为当前安全分类器的缺点不仅是因为模型本身。我们希望这些发现将有助于讨论开发更好的方法，以识别跨语言中LLM输入中的有害内容。</li>
</ul>

<h3>Title: Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish</h3>
<ul>
<li><strong>Authors: </strong>Elena Alvarez-Mellado, Jordi Porta-Zamorano, Constantine Lignos, Julio Gonzalo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21813">https://arxiv.org/abs/2507.21813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21813">https://arxiv.org/pdf/2507.21813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21813]] Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish(https://arxiv.org/abs/2507.21813)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>This paper summarizes the main findings of ADoBo 2025, the shared task on anglicism identification in Spanish proposed in the context of IberLEF 2025. Participants of ADoBo 2025 were asked to detect English lexical borrowings (or anglicisms) from a collection of Spanish journalistic texts. Five teams submitted their solutions for the test phase. Proposed systems included LLMs, deep learning models, Transformer-based models and rule-based systems. The results range from F1 scores of 0.17 to 0.99, which showcases the variability in performance different systems can have for this task.</li>
<li><strong>摘要：</strong>本文总结了Adobo 2025的主要发现，这是在Iberlef 2025的背景下提出的西班牙语中的英词识别的共同任务。要求Adobo 2025的参与者从西班牙新闻文本集中检测出英语英语词汇借款（或英语）。五个团队为测试阶段提交了解决方案。拟议的系统包括LLM，深度学习模型，基于变压器的模型和基于规则的系统。结果范围从0.17到0.99的F1得分范围，它显示了性能不同系统的可变性。</li>
</ul>

<h3>Title: HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs</h3>
<ul>
<li><strong>Authors: </strong>Kaixuan Wang, Chenxin Diao, Jason T. Jacques, Zhongliang Guo, Shuai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21815">https://arxiv.org/abs/2507.21815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21815">https://arxiv.org/pdf/2507.21815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21815]] HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs(https://arxiv.org/abs/2507.21815)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms.</li>
<li><strong>摘要：</strong>数以百万计的人的福祉受到药物使用的危害的挑战。减少危害作为公共卫生策略旨在改善其健康成果并降低安全风险。一些大型语言模型（LLM）表现出了不错的医学知识，有望满足使用药物（PWUD）的人的信息需求。但是，它们在相关任务中的性能仍然在很大程度上尚未开发。我们介绍了Hripbench，这是一种基准测试，旨在评估LLM减少危害信息提供的准确性和安全风险。基准数据集HRIP-BASIC具有2,160个问题 - 答案对。范围涵盖了三个任务：检查安全界限，提供定量值以及推断使用多物质的使用风险。我们构建指导和抹布方案，以根据其固有的知识和域知识的整合来评估模型行为。我们的结果表明，最先进的LLM仍然难以提供准确的减少伤害信息，有时会对PWUD施加严重的安全风险。应谨慎限制使用LLMS在危害减少环境中，以避免诱发负面健康结果。警告：本文包含可能引起危害的非法内容。</li>
</ul>

<h3>Title: Modelling Adjectival Modification Effects on Semantic Plausibility</h3>
<ul>
<li><strong>Authors: </strong>Anna Golub, Beate Zywietz, Annerose Eichel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21828">https://arxiv.org/abs/2507.21828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21828">https://arxiv.org/pdf/2507.21828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21828]] Modelling Adjectival Modification Effects on Semantic Plausibility(https://arxiv.org/abs/2507.21828)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>While the task of assessing the plausibility of events such as ''news is relevant'' has been addressed by a growing body of work, less attention has been paid to capturing changes in plausibility as triggered by event modification. Understanding changes in plausibility is relevant for tasks such as dialogue generation, commonsense reasoning, and hallucination detection as it allows to correctly model, for example, ''gentle sarcasm'' as a sign of closeness rather than unkindness among friends [9]. In this work, we tackle the ADEPT challenge benchmark [6] consisting of 16K English sentence pairs differing by exactly one adjectival modifier. Our modeling experiments provide a conceptually novel method by using sentence transformers, and reveal that both they and transformer-based models struggle with the task at hand, and sentence transformers - despite their conceptual alignment with the task - even under-perform in comparison to models like RoBERTa. Furthermore, an in-depth comparison with prior work highlights the importance of a more realistic, balanced evaluation method: imbalances distort model performance and evaluation metrics, and weaken result trustworthiness.</li>
<li><strong>摘要：</strong>虽然越来越多的工作已经解决了评估“新闻相关”等事件的合理性的任务，但人们对捕获事件修改触发的合理性变化的关注减少了。理解合理性的变化与诸如对话生成，常识性推理和幻觉检测等任务有关，因为它可以正确建模，例如，“温柔的讽刺”是朋友之间的亲密而不是不友善的标志[9]。在这项工作中，我们解决了熟练的挑战基准[6]，该基准由16k英语句子对组成，这些句子对完全不同，与一个形容词修饰符不同。我们的建模实验通过使用句子变形金刚提供了一种概念上新颖的方法，并揭示了他们和基于变压器的模型在手头的任务上挣扎，尽管与罗伯塔这样的模型相比，他们的概念性与任务的概念相吻合 - 甚至表现不佳。此外，与先前工作的深入比较突出了一种更现实，平衡的评估方法的重要性：失衡扭曲模型性能和评估指标，并削弱了结果的可信度。</li>
</ul>

<h3>Title: Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences</h3>
<ul>
<li><strong>Authors: </strong>Andreas Reich, Claudia Thoms, Tobias Schrimpf</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21831">https://arxiv.org/abs/2507.21831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21831">https://arxiv.org/pdf/2507.21831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21831]] Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences(https://arxiv.org/abs/2507.21831)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.</li>
<li><strong>摘要：</strong>LLM正在看到任务自动化的广泛使用，包括社会科学中的自动编码。但是，即使研究人员提出了不同的提示策略，但它们的有效性在LLM和任务上也有所不同。通常，反复试验仍然很普遍。我们建议使用HALC $  -  $ $的一般管道，该管道允许为任何给定的编码任务和模型进行系统和可靠的最佳提示构建，从而允许集成任何认为相关的提示策略。为了调查LLM编码并验证我们的管道，我们向当地的LLM发送了1,512个个人提示，并收到了200万多个请求。我们基于少数专家编码（地面真相）来测试提示策略和LLM任务绩效。与这些专家编码相比，我们发现提示该提示可靠地为单个变量（$ {\ alpha} $ climate = .76; $ {\ alpha} $ MOWARK = .78）和两个变量（$ {\ alpha} $ climate = .71; $ climate = .71; $ {\ alpha} $ Moviles = .74）。我们的提示策略以使LLM与我们的CodeBook $  -  $相一致的方式建立，我们没有为LLM友好的码头优化代码簿。我们的论文提供了有关不同提示策略，关键影响因素以及为每个编码任务和模型确定可靠提示的有效性的见解。</li>
</ul>

<h3>Title: AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, Li Du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21836">https://arxiv.org/abs/2507.21836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21836">https://arxiv.org/pdf/2507.21836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21836]] AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning(https://arxiv.org/abs/2507.21836)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通过以推理为导向的训练增强时，会演变为强大的大型推理模型（LRMS）。工具集成推理（TIR）通过合并外部工具进一步扩展了其功能，但是现有的方法通常依赖于刚性，预定义的工具使用模式，这些模式可能会降低核心语言能力。受到人类自适应选择工具的能力的启发，我们引入了Autotir，这是一个增强学习框架，使LLMS能够自主决定是否以及在推理过程中调用哪种工具，而不是遵循静态的工具使用策略。 Autotir利用一种混合奖励机制，该机制共同优化了特定于任务的答案正确性，结构化输出依从性以及对不正确工具使用的惩罚，从而鼓励精确的推理和有效的工具集成。对各种知识密集型，数学和一般语言建模任务进行的广泛评估表明，自动疗法实现了卓越的整体性能，显着超过了基准，并且在工具使用行为中表现出卓越的概括。这些结果突出了强化学习在LLM中真正可推广和可扩展的TIR功能的希望。该代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoran Luo, Haihong E, Guanting Chen, Qika Lin, Yikai Guo, Fangzhi Xu, Zemin Kuang, Meina Song, Xiaobao Wu, Yifan Zhu, Luu Anh Tuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21892">https://arxiv.org/abs/2507.21892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21892">https://arxiv.org/pdf/2507.21892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21892]] Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning(https://arxiv.org/abs/2507.21892)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination, prompt, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality.</li>
<li><strong>摘要：</strong>检索增强的一代（RAG）通过合并外部知识来减轻LLM中的幻觉，但依赖于缺乏结构语义的基于块的检索。 GraphRag方法通过将知识作为实体关联图建模来改善抹布，但仍面临高建筑成本，固定的一次性检索以及对长篇文化推理和及时设计的依赖的挑战。为了应对这些挑战，我们建议通过端到端强化学习（RL）的Graph-R1（Agrage-R1）。它引入了轻量化知识超图结构，模型作为多转变代理 - 环境相互作用的模型，并通过端到端奖励机制优化了代理过程。标准抹布数据集的实验表明，图形R1在推理准确性，检索效率和发电质量方面优于传统的GraphRag和RL增强的抹布方法。</li>
</ul>

<h3>Title: Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Qinyuan Wu, Soumi Das, Mahsa Amani, Bishwamittra Ghosh, Mohammad Aflah Khan, Krishna P. Gummadi, Muhammad Bilal Zafar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21914">https://arxiv.org/abs/2507.21914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21914">https://arxiv.org/pdf/2507.21914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21914]] Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs(https://arxiv.org/abs/2507.21914)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Rote learning is a memorization technique based on repetition. It is commonly believed to hinder generalization by encouraging verbatim memorization rather than deeper understanding. This insight holds for even learning factual knowledge that inevitably requires a certain degree of memorization. In this work, we demonstrate that LLMs can be trained to generalize from rote memorized data. We introduce a two-phase memorize-then-generalize framework, where the model first rote memorizes factual subject-object associations using a semantically meaningless token and then learns to generalize by fine-tuning on a small set of semantically meaningful prompts. Extensive experiments over 8 LLMs show that the models can reinterpret rote memorized data through the semantically meaningful prompts, as evidenced by the emergence of structured, semantically aligned latent representations between the two. This surprising finding opens the door to both effective and efficient knowledge injection and possible risks of repurposing the memorized data for malicious usage.</li>
<li><strong>摘要：</strong>死记硬背学习是一种基于重复的记忆技术。人们普遍认为，通过鼓励逐字记忆而不是更深入的理解来阻碍概括。这种洞察力甚至是学习事实知识，不可避免地需要一定程度的记忆。在这项工作中，我们证明可以训练LLMS从死记硬背的数据中概括。我们引入了两相的记忆 - 然后将一般化的框架进行，该模型首先使用语义上毫无意义的令牌记住事实对象 - 对象的关联，然后学会通过对一系列语义上有意义的提示进行微调来概括以概括化。在8个LLMS上的广泛实验表明，这些模型可以通过语义上有意义的提示重新解释死记硬背的数据，这是由两者之间结构化的，语义上对齐的潜在表示的出现所证明的。这一令人惊讶的发现为有效，有效的知识注入和重新利用记忆数据的可能使用的风险打开了大门。</li>
</ul>

<h3>Title: Training language models to be warm and empathetic makes them less reliable and more sycophantic</h3>
<ul>
<li><strong>Authors: </strong>Lujain Ibrahim, Franziska Sofia Hafner, Luc Rocher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21919">https://arxiv.org/abs/2507.21919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21919">https://arxiv.org/pdf/2507.21919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21919]] Training language models to be warm and empathetic makes them less reliable and more sycophantic(https://arxiv.org/abs/2507.21919)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.</li>
<li><strong>摘要：</strong>人工智能（AI）开发人员越来越多地建立具有温暖和富有同情心的角色的语言模型，数以百万计的人现在用于建议，治疗和陪伴。在这里，我们展示了这是如何创造重大权衡的：优化语言模型以破坏其可靠性，尤其是在用户表达脆弱性时。我们对五个不同大小和体系结构的语言模型进行了对照实验，培训它们以产生更温暖，更善解人意的响应，然后将其评估为关键性任务。温暖的模型显示出比原始同行的错误率（+10至+30个百分点）更高，促进阴谋论，提供错误的事实信息并提供有问题的医疗建议。他们也更有可能验证不正确的用户信念，尤其是当用户消息表达悲伤时。重要的是，这些效果在不同的模型架构之间是一致的，尽管在标准基准上保留了性能，但仍会发生，这揭示了当前评估实践可能无法检测到的系统风险。随着类似人类的AI系统以空前的规模部署，我们的发现表明需要重新考虑我们如何发展和监督这些正在重塑人际关系和社会互动的系统。</li>
</ul>

<h3>Title: Post-Training Large Language Models via Reinforcement Learning from Self-Feedback</h3>
<ul>
<li><strong>Authors: </strong>Carel van Niekerk, Renato Vukovic, Benjamin Matthias Ruppik, Hsien-chin Lin, Milica Gašić</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21931">https://arxiv.org/abs/2507.21931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21931">https://arxiv.org/pdf/2507.21931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21931]] Post-Training Large Language Models via Reinforcement Learning from Self-Feedback(https://arxiv.org/abs/2507.21931)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards. RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering. By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通常会产生合理但校准较差的答案，从而将其可靠性限制在推理密集型任务上。我们提出了从自我反馈（RLSF）中学习的强化学习，这是一个训练后的阶段，它以模型自身的信心为固有的奖励，模仿了在没有外部反馈的情况下人类学习的方式。在冷冻LLM生成多个经过思考的解决方案之后，我们定义和计算每个最终答案的信心，并相应地对痕迹进行排名。然后，这些合成的偏好被用来通过标准偏好优化来微调政策，类似于RLHF，但不需要人类标签，金答案或外部策划的奖励。 RLSF同时（i）完善了模型的概率估计值 - 恢复行为良好的校准 - （ii）加强逐步推理，从而提高了算术推理和多项选择性问题的性能。通过将模型的不确定性变成有用的自我反馈，RLSF肯定了对内在模型行为的强化学习，这是LLM训练后管道中有原则的和数据有效的组成部分，并为LLM培训后的内在奖励提供了进一步的研究。</li>
</ul>

<h3>Title: Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Hu, Andrea Morales-Garzón, Jingyi Zheng, Maria Maistro, Daniel Hershcovich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21934">https://arxiv.org/abs/2507.21934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21934">https://arxiv.org/pdf/2507.21934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21934]] Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation(https://arxiv.org/abs/2507.21934)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.</li>
<li><strong>摘要：</strong>在跨文化食谱的适应中，目标不仅是确保文化适当性并保留原始菜肴的本质，而且还为各种饮食需求和偏好提供多种选择。检索增强产生（RAG）是一种有前途的方法，将目标食谱与大语言模型（LLMS）的相关性结合起来。但是，尚不清楚抹布是否可以产生各种适应性结果。我们的分析表明，RAG倾向于过于依赖几代人的有限部分，即使提供了多种多样的上下文输入，也无法产生各种产出。这揭示了具有多个有效答案的创意任务中抹布的关键局限性：它无法利用上下文多样性来产生各种答案。为了解决这个问题，我们提出了Carriagare，这是跨文化食谱适应的插件式抹布框架，可增强检索和上下文组织中的多样性。据我们所知，这是第一个明确旨在生成高度多样化的输出以适应多个用户偏好的抹布框架。我们的实验表明，与封闭式LLM相比，马车在多样性和配方适应质量方面达到了帕累托效率。</li>
</ul>

<h3>Title: Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyunwoo Yoo, Gail L. Rosen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21980">https://arxiv.org/abs/2507.21980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21980">https://arxiv.org/pdf/2507.21980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21980]] Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models(https://arxiv.org/abs/2507.21980)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models (LLMs) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that LLMs not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that LLMs can effectively reason over sparse, heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications.</li>
<li><strong>摘要：</strong>传统的机器学习模型努力在仅可用元数据的微生物组研究中概括，尤其是在小样本环境或具有异质标签格式的研究中。在这项工作中，我们探讨了大型语言模型（LLMS）将微生物样本分类为本体论类别，例如Empo 3和相关的生物学标签，以及仅使用环境元数据来预测病原体污染风险，特别是大肠杆菌的存在。我们将LLMS（例如Chatgpt-4O，Claude 3.7十四行诗，Grok-3和Llama 4）评估为零拍摄和少量设置，将它们的性能与传统模型（如多个现实世界中数据集中的随机森林）进行了比较。我们的结果表明，LLM不仅在本体分类中的表现优于基准，而且还表现出强大的污染风险预测能力，跨越地点和元数据分布的概括。这些发现表明，LLM可以有效地理解稀疏，异质生物元数据，并为环境微生物学和生物监视应用提供了一种有希望的仅元数据方法。</li>
</ul>

<h3>Title: DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router</h3>
<ul>
<li><strong>Authors: </strong>Minghao Guo, Qingcheng Zeng, Xujiang Zhao, Yanchi Liu, Wenchao Yu, Mengnan Du, Haifeng Chen, Wei Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22050">https://arxiv.org/abs/2507.22050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22050">https://arxiv.org/pdf/2507.22050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22050]] DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router(https://arxiv.org/abs/2507.22050)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在许多推理任务上都表现出色，但由于无法动态访问最新或特定领域的信息而与知识密集型查询斗争。检索增强的一代（RAG）已成为一种有前途的解决方案，使LLMS能够在外部来源中对其响应进行基础。但是，现有的抹布方法缺乏对查询和源边的细粒度控制，通常会导致嘈杂的检索和浅色推理。在这项工作中，我们介绍了DeepSive，这是一个代理RAG框架，该框架通过LLM-AS-A-A-Inkookledge-Router结合了信息。 DeepSive将复杂的查询分解为结构化的子问题，并递归将各个路由路由到最合适的知识源，从而通过多阶段的蒸馏过程过滤无关的信息。我们的设计强调了模块化，透明度和适应性，利用了代理系统设计的最新进展。对跨异质来源的多跳质量检查任务进行的实验表明，对传统的抹布方法的推理深度，检索精度和可解释性的改善。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
