<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-30</h1>
<h3>Title: Title:
          LLMs and Memorization: On Quality and Specificity of Copyright Compliance</h3>
<ul>
<li><strong>Authors: </strong>Felix B Mueller, Rebekka Görge, Anna K Bernzen, Janna C Pirk, Maximilian Poretschkin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLMs and Memorization: On Quality and Specificity of Copyright Compliance(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Memorization in large language models (LLMs) is a growing concern. LLMs have been shown to easily reproduce parts of their training data, including copyrighted work. This is an important problem to solve, as it may violate existing copyright laws as well as the European AI Act. In this work, we propose a systematic analysis to quantify the extent of potential copyright infringements in LLMs using European law as an example. Unlike previous work, we evaluate instruction-finetuned models in a realistic end-user scenario. Our analysis builds on a proposed threshold of 160 characters, which we borrow from the German Copyright Service Provider Act and a fuzzy text matching algorithm to identify potentially copyright-infringing textual reproductions. The specificity of countermeasures against copyright infringement is analyzed by comparing model behavior on copyrighted and public domain data. We investigate what behaviors models show instead of producing protected text (such as refusal or hallucination) and provide a first legal assessment of these behaviors. We find that there are huge differences in copyright compliance, specificity, and appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing a particularly low absolute number of potential copyright violations. Code will be published soon.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的记忆越来越受到关注。事实证明，LLM 可以轻松复制其部分训练数据，包括受版权保护的作品。这是一个需要解决的重要问题，因为它可能违反现有的版权法以及《欧洲人工智能法案》。在这项工作中，我们提出了一种系统分析，以欧洲法律为例，量化 LLM 中潜在版权侵权的程度。与以前的工作不同，我们在现实的最终用户场景中评估指令微调模型。我们的分析建立在拟议的 160 个字符的阈值上，我们借鉴了《德国版权服务提供商法案》和模糊文本匹配算法来识别可能侵犯版权的文本复制品。通过比较模型在受版权保护和公共领域数据上的行为，分析了针对版权侵权的对策的特异性。我们调查了模型除了生成受保护的文本之外还表现出哪些行为（例如拒绝或幻觉），并对这些行为进行了初步的法律评估。我们发现，在流行的 LLM 中，版权合规性、特异性和适当拒绝方面存在巨大差异。在我们的比较中，Alpaca、GPT 4、GPT 3.5 和 Luminous 表现最佳，而 OpenGPT-X、Alpaca 和 Luminous 产生的潜在版权侵权绝对数量特别低。代码将很快发布。</li>
</ul>

<h3>Title: Title:
          Learning diverse attacks on large language models for robust red-teaming and safety tuning</h3>
<ul>
<li><strong>Authors: </strong>Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, Moksh Jain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Learning diverse attacks on large language models for robust red-teaming and safety tuning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming typically uses reinforcement learning to fine-tune an attacker language model to generate prompts that elicit undesirable responses from a target LLM, as measured, for example, by an auxiliary toxicity classifier. We show that even with explicit regularization to favor novelty and diversity, existing approaches suffer from mode collapse or fail to generate effective attacks. As a flexible and probabilistically principled alternative, we propose to use GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts. We find that the attacks generated by our method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs. Finally, we demonstrate that models safety-tuned using a dataset of red-teaming prompts generated by our method are robust to attacks from other RL-based red-teaming approaches.</li>
<li><strong>摘要：</strong>红队演练，即识别引发有害响应的提示，是确保安全且负责任地部署大型语言模型 (LLM) 的关键步骤。开发针对多种攻击提示模式的有效保护需要发现各种攻击。自动红队演练通常使用强化学习来微调攻击者语言模型，以生成引发目标 LLM 不良响应的提示，例如通过辅助毒性分类器进行测量。我们表明，即使使用显式正则化来支持新颖性和多样性，现有方法仍会遭受模式崩溃或无法生成有效攻击。作为一种灵活且符合概率原则的替代方案，我们建议使用 GFlowNet 微调，然后进行二次平滑阶段，以训练攻击者模型生成多样化且有效的攻击提示。我们发现，我们的方法生成的攻击对各种目标 LLM 都有效，无论是否进行了安全调整，并且可以在目标 LLM 之间很好地转移。最后，我们证明，使用我们的方法生成的红队提示数据集进行安全调整的模型对于其他基于 RL 的红队方法的攻击具有很强的鲁棒性。</li>
</ul>

<h3>Title: Title:
          ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aparna Elangovan, Ling Liu, Lei Xu, Sravan Bodapati, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models -- which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars --Consistency, Scoring Critera, Differentiating, User Experience, Responsible, and Scalability.</li>
<li><strong>摘要：</strong>在本立场文件中，我们认为，对生成式大型语言模型 (LLM) 的人工评估应是一项多学科工作，借鉴用户体验研究和人类行为心理学等学科的见解，以确保实验设计和结果的可靠性。因此，这些评估的结论必须考虑可用性、美学和认知偏差等因素。我们强调了认知偏差如何将流畅的信息和真实性混为一谈，以及认知不确定性如何影响李克特评分的可靠性。此外，评估应区分日益强大的大型语言模型的能力和弱点——这需要有效的测试集。人工评估的可扩展性对于更广泛的采用也至关重要。因此，为了在生成式 NLP 时代设计一个有效的人工评估系统，我们提出了 ConSiDERS-The-Human 评估框架，该框架由 6 个支柱组成——一致性、评分标准、差异化、用户体验、责任和可扩展性。</li>
</ul>

<h3>Title: Title:
          Training LLMs to Better Self-Debug and Explain Code</h3>
<ul>
<li><strong>Authors: </strong>Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Binta Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, Anoop Deoras</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Training LLMs to Better Self-Debug and Explain Code(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose a training framework that significantly improves self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability, and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.</li>
<li><strong>摘要：</strong>在代码生成领域，自调试至关重要。它允许 LLM 根据执行反馈改进其生成的代码。这一点尤其重要，因为对于复杂的任务来说，一次性生成正确的解决方案是一项挑战。之前关于自调试的研究主要集中在通过为 LLM 提供少量示例来提示方法，这些方法在小型开源 LLM 上效果不佳。在这项工作中，我们提出了一个训练框架，可以显著提高 LLM 的自调试能力。直观地讲，我们观察到，对错误代码进行一系列解释，然后进行代码改进，有助于 LLM 更好地分析错误代码并进行改进。因此，我们提出了一种自动化管道，通过生成大量解释和改进轨迹并通过执行验证进行过滤，来收集高质量的代码解释和改进数据集。我们对成功和失败轨迹执行监督微调 (SFT) 和进一步强化学习 (RL)，并采用一种新颖的奖励设计，考虑代码解释和改进质量。在四个基准测试中，SFT 将 pass@1 提高了 15.92%，将 pass@10 提高了 9.30%。RL 训练将 pass@1 提高了 3.54%，将 pass@10 提高了 2.55%。经过训练的 LLM 表现出迭代改进能力，可以不断改进代码。最后，我们的人工评估表明，使用我们的框架训练的 LLM 可以生成更有用的代码解释，并帮助开发人员更好地理解源代码中的错误。</li>
</ul>

<h3>Title: Title:
          Recent Advances of Foundation Language Models-based Continual Learning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yutao Yang, Jie Zhou, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Liang He, Yuan Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Recent Advances of Foundation Language Models-based Continual Learning: A Survey(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. However, they still can not emulate human-like continuous learning due to catastrophic forgetting. Consequently, various continual learning (CL)-based methodologies have been developed to refine LMs, enabling them to adapt to new tasks without forgetting previous knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking, which is the gap that our survey aims to fill. We delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs). We divide these studies into offline CL and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Offline CL encompasses domain-incremental learning, task-incremental learning, and class-incremental learning, while online CL is subdivided into hard task boundary and blurry task boundary settings. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.</li>
<li><strong>摘要：</strong>最近，基础语言模型 (LM) 在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了重大成就。与传统的神经网络模型不同，基础语言模型通过在大量参数的大量无监督数据集上进行预训练来获取丰富的常识性知识，从而获得了强大的迁移学习能力。然而，由于灾难性遗忘，它们仍然无法模拟类似人类的持续学习。因此，人们开发了各种基于持续学习 (CL) 的方法来改进 LM，使它们能够适应新任务而不会忘记以前的知识。然而，对现有方法的系统分类和它们的性能比较仍然缺乏，这正是我们的调查旨在填补的空白。我们深入研究了基于 CL 的方法应用于基础语言模型的现有文献，例如预训练语言模型 (PLM)、大型语言模型 (LLM) 和视觉语言模型 (VLM)。我们将这些研究分为离线 CL 和在线 CL，包括传统方法、基于参数效率的方法、基于指令调整的方法和持续预训练方法。离线 CL 包括领域增量学习、任务增量学习和类增量学习，而在线 CL 细分为硬任务边界和模糊任务边界设置。此外，我们概述了 CL 研究中使用的典型数据集和指标，并对基于 LM 的持续学习的挑战和未来工作进行了详细分析。</li>
</ul>

<h3>Title: Title:
          Understanding Intrinsic Socioeconomic Biases in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mina Arzaghi, Florian Carichon, Golnoosh Farnadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Understanding Intrinsic Socioeconomic Biases in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地被整合到关键的决策过程中，例如贷款审批和签证申请，其中固有的偏见可能导致歧视性结果。在本文中，我们研究了 LLM 中人口统计属性与社会经济偏见之间的微妙关系，这是 LLM 公平性的一个重要但研究不足的领域。我们引入了一个包含一百万个英语句子的新数据集，以系统地量化不同人口群体的社会经济偏见。我们的研究结果揭示了 GPT-2 等成熟模型和 Llama 2 和 Falcon 等最先进模型中普遍存在的社会经济偏见。我们证明，当考虑交叉性时，这些偏见会被显著放大，LLM 表现出从姓名中提取多个人口统计属性的非凡能力，然后将它们与特定的社会经济偏见相关联。这项研究强调了在关键的现实世界应用中部署这些强大的模型时，迫切需要主动和强大的偏见缓解技术来防止歧视性结果。</li>
</ul>

<h3>Title: Title:
          Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Shubham Vatsal, Ayush Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable performance on many tasks in different domains. However, their performance in closed-book biomedical machine reading comprehension (MRC) has not been evaluated in depth. In this work, we evaluate GPT on four closed-book biomedical MRC benchmarks. We experiment with different conventional prompting techniques as well as introduce our own novel prompting method. To solve some of the retrieval problems inherent to LLMs, we propose a prompting strategy named Implicit Retrieval Augmented Generation (RAG) that alleviates the need for using vector databases to retrieve important chunks in traditional RAG setups. Moreover, we report qualitative assessments on the natural language generation outputs from our approach. The results show that our new prompting technique is able to get the best performance in two out of four datasets and ranks second in rest of them. Experiments show that modern-day LLMs like GPT even in a zero-shot setting can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在不同领域的许多任务上表现出色。然而，它们在闭卷生物医学机器阅读理解 (MRC) 中的表现尚未得到深入评估。在这项工作中，我们在四个闭卷生物医学 MRC 基准上评估了 GPT。我们尝试了不同的传统提示技术，并引入了我们自己的新提示方法。为了解决 LLM 固有的一些检索问题，我们提出了一种名为隐式检索增强生成 (RAG) 的提示策略，该策略减轻了在传统 RAG 设置中使用向量数据库来检索重要块的需要。此外，我们报告了对我们方法的自然语言生成输出的定性评估。结果表明，我们的新提示技术能够在四个数据集中的两个中获得最佳性能，并在其余数据集中排名第二。实验表明，即使在零样本设置下，像 GPT 这样的现代 LLM 也可以胜过监督模型，从而在两个基准上获得新的最先进 (SoTA) 结果。</li>
</ul>

<h3>Title: Title:
          Efficient Model-agnostic Alignment via Bayesian Persuasion</h3>
<ul>
<li><strong>Authors: </strong>Fengshuo Bai, Mingzhi Wang, Zhaowei Zhang, Boyuan Chen, Yinda Xu, Ying Wen, Yaodong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Efficient Model-agnostic Alignment via Bayesian Persuasion(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent. Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item. Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的最新进展，对齐已成为一种保持 LLM 与人类意图一致的有效技术。当前的方法主要涉及通过监督微调 (SFT) 或从人类反馈中强化学习 (RLHF) 进行直接训练，这两者都需要大量的计算资源和大量的地面实况数据。本文探讨了一种使用较小模型对齐黑盒大型模型的有效方法，引入了与模型无关的轻量级贝叶斯说服对齐框架。我们将这个问题形式化为从小模型的角度优化信号策略。在说服过程中，小模型 (Advisor) 观察信息项 (即状态) 并说服大模型 (Receiver) 以引出改进的响应。然后，接收器根据输入、来自 Advisor 的信号及其对信息项的更新信念生成响应。通过使用我们的框架进行训练，我们证明了 Advisor 可以显著提高各种接收器在一系列任务中的表现。我们从理论上分析了我们的说服框架，并为顾问的遗憾提供了上限，证实了其在学习最佳信号策略方面的有效性。我们的实证结果表明，GPT-2 可以显著提高各种模型的性能，数学推理能力平均提高 16.1%，代码生成能力平均提高 13.7%。我们希望我们的工作可以为从贝叶斯说服的角度重新思考对齐框架迈出第一步。</li>
</ul>

<h3>Title: Title:
          Contextual Position Encoding: Learning to Count What's Important</h3>
<ul>
<li><strong>Authors: </strong>Olga Golovneva, Tianlu Wang, Jason Weston, Sainbayar Sukhbaatar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Contextual Position Encoding: Learning to Count What's Important(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but is order-invariant. Incorporating position encoding (PE) makes it possible to address by position, such as attending to the i-th token. However, current PE methods use token counts to derive position, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence. In this paper, we propose a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the $i$-th particular word, noun, or sentence. We show that CoPE can solve the selective copy, counting and Flip-Flop tasks where popular position embeddings fail, and improves perplexity on language modeling and coding tasks.</li>
<li><strong>摘要：</strong>注意力机制是大型语言模型 (LLM) 的一个关键组成部分，它允许序列中的标记相互作用，但顺序不变。加入位置编码 (PE) 可以按位置寻址，例如关注第 i 个标记。但是，当前的 PE 方法使用标记计数来得出位置，因此无法推广到更高的抽象级别，例如关注第 i 个句子。在本文中，我们提出了一种新的位置编码方法，即上下文位置编码 (CoPE)，它允许通过仅在模型确定的某些标记上增加位置来根据上下文调整位置。这允许更一般的位置寻址，例如关注第 $i$ 个特定的单词、名词或句子。我们表明，CoPE 可以解决流行的位置嵌入无法解决的选择性复制、计数和触发器任务，并改善语言建模和编码任务的困惑度。</li>
</ul>

<h3>Title: Title:
          CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control</h3>
<ul>
<li><strong>Authors: </strong>Huanshuo Liu, Hao Zhang, Zhijiang Guo, Kuicai Dong, Xiangyang Li, Yi Quan Lee, Cong Zhang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has emerged as a promising solution for mitigating hallucinations of large language models (LLMs) with retrieved external knowledge. Adaptive RAG enhances this approach by dynamically assessing the retrieval necessity, aiming to balance external and internal knowledge usage. However, existing adaptive RAG methods primarily realize retrieval on demand by relying on superficially verbalize-based or probability-based feedback of LLMs, or directly fine-tuning LLMs via carefully crafted datasets, resulting in unreliable retrieval necessity decisions, heavy extra costs, and sub-optimal response generation. We present the first attempts to delve into the internal states of LLMs to mitigate such issues by introducing an effective probe-guided adaptive RAG framework, termed CtrlA. Specifically, CtrlA employs an honesty probe to regulate the LLM's behavior by manipulating its representations for increased honesty, and a confidence probe to monitor the internal states of LLM and assess confidence levels, determining the retrieval necessity during generation. Experiments show that CtrlA is superior to existing adaptive RAG methods on a diverse set of tasks, the honesty control can effectively make LLMs more honest and confidence monitoring is proven to be a promising indicator of retrieval trigger. Our codes are available at this https URL.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 已成为一种有前途的解决方案，可用于缓解大型语言模型 (LLM) 因检索到外部知识而产生的幻觉。自适应 RAG 通过动态评估检索必要性来增强这种方法，旨在平衡外部和内部知识的使用。然而，现有的自适应 RAG 方法主要依靠 LLM 的基于表面语言或概率的反馈来实现按需检索，或通过精心制作的数据集直接微调 LLM，从而导致检索必要性决策不可靠、额外成本高昂以及响应生成不理想。我们首次尝试深入研究 LLM 的内部状态以缓解此类问题，方法是引入一种有效的探测引导自适应 RAG 框架，称为 CtrlA。具体而言，CtrlA 使用诚实探测器通过操纵其表示来调节 LLM 的行为以提高诚实度，并使用置信度探测器来监控 LLM 的内部状态并评估置信度水平，从而确定生成过程中的检索必要性。实验表明，CtrlA 在多种任务上均优于现有的自适应 RAG 方法，诚实控制可以有效地使 LLM 更加诚实，并且置信度监控被证明是检索触发的有希望的指标。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jialiang Xu, Michael Moor, Jure Leskovec</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Despite impressive advances in recent multimodal large language models (MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle with knowledge-intensive tasks. To address this, we consider Reverse Image Retrieval (RIR) augmented generation, a simple yet effective strategy to augment MLLMs with web-scale reverse image search results. RIR robustly improves knowledge-intensive visual question answering (VQA) of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics. To our surprise, we discover that RIR helps the model to better access its own world knowledge. Concretely, our experiments suggest that RIR augmentation helps by providing further visual and textual cues without necessarily containing the direct answer to a query. In addition, we elucidate cases in which RIR can hurt performance and conduct a human evaluation. Finally, we find that the overall advantage of using RIR makes it difficult for an agent that can choose to use RIR to perform better than an approach where RIR is the default setting.</li>
<li><strong>摘要：</strong>尽管最近的多模态大型语言模型 (MLLM) 取得了令人瞩目的进展，但 GPT-4 套件等最先进的模型在知识密集型任务方面仍然举步维艰。为了解决这个问题，我们考虑使用反向图像检索 (RIR) 增强生成，这是一种简单而有效的策略，可以通过网络规模的反向图像搜索结果增强 MLLM。在开放式 VQA 评估指标方面，RIR 可显著提高 GPT-4V 的知识密集型视觉问答 (VQA) 37-43%、GPT-4 Turbo 25-27% 和 GPT-4o 18-20%。令我们惊讶的是，我们发现 RIR 有助于模型更好地访问自己的世界知识。具体来说，我们的实验表明，RIR 增强有助于提供进一步的视觉和文本提示，而不一定包含查询的直接答案。此外，我们阐明了 RIR 可能损害性能的情况并进行了人工评估。最后，我们发现使用 RIR 的整体优势使得选择使用 RIR 的代理很难比使用 RIR 作为默认设置的方法表现更好。</li>
</ul>

<h3>Title: Title:
          Genshin: General Shield for Natural Language Processing with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Peng, Tao Liu, Ying Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Genshin: General Shield for Natural Language Processing with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains. However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches. The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc. Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness. To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state. Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model. Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient. In our ablation study, we unearth several intriguing observations. Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP. Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) （例如 ChatGPT、Gemini 或 LLaMA）最近非常流行，在无数领域中展示了显著的进步和通用性。然而，LLM 会创建一个更大的黑箱，加剧不透明性，可解释性仅限于少数方法。LLM 的本质所包含的不确定性和不透明性限制了它们在金融欺诈、网络钓鱼等高风险领域的应用。当前的方法主要依赖于具有后验可解释算法的传统文本分类，攻击者可能会创建多用途的对抗样本来突破系统的防御，迫使用户在效率和稳健性之间做出权衡。为了解决这个问题，我们提出了一种名为 Genshin（具有大型语言模型的自然语言处理通用盾牌）的新型级联框架，利用 LLM 作为防御性一次性插件。与大多数试图将文本转换为新内容或结构内容的 LLM 应用程序不同，Genshin 使用 LLM 将文本恢复到其原始状态。 Genshin 旨在将 LLM 的通用性、中值模型的区分度和简单模型的可解释性结合起来。我们在情感分析和垃圾邮件检测任务上的实验表明，当前中值模型存在致命缺陷，而 LLM 的恢复能力令人振奋，证明了 Genshin 既有效又高效。在我们的消融研究中，我们发现了一些有趣的观察结果。利用 LLM 防御者（一种源自第四范式的工具），我们在 NLP 的第三范式中重现了 BERT 的 15% 最佳掩码率结果。此外，当使用 LLM 作为潜在的对抗工具时，攻击者能够执行几乎无语义损失的有效攻击。</li>
</ul>

<h3>Title: Title:
          Toxicity Detection for Free</h3>
<ul>
<li><strong>Authors: </strong>Zhanhao Hu, Julien Piet, Geng Zhao, Jiantao Jiao, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Toxicity Detection for Free(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we explore Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found significant gaps between benign and toxic prompts in the distribution of alternative refusal responses and in the distribution of the first response token's logits. These gaps can be used to detect toxicities: We show that a toy model based on the logits of specific starting tokens gets reliable performance, while requiring no training or additional computational cost. We build a more robust detector using a sparse logistic regression model on the first response token logits, which greatly exceeds SOTA detectors under multiple metrics.</li>
<li><strong>摘要：</strong>当前的 LLM 通常遵循安全要求，并倾向于拒绝有害提示。然而，LLM 可能无法拒绝有害提示，或者过于谨慎而拒绝良性示例。此外，最先进的毒性检测器在低 FPR 时 TPR 较低，在有毒示例罕见的实际应用中会产生高成本。在本文中，我们探索使用 LLM 内省 (MULI) 进行审核，它使用直接从 LLM 本身提取的信息来检测有毒提示。我们发现良性和有毒提示在替代拒绝响应的分布和第一个响应标记的 logit 的分布方面存在显著差距。这些差距可用于检测毒性：我们表明，基于特定起始标记的 logit 的玩具模型可以获得可靠的性能，同时不需要训练或额外的计算成本。我们使用稀疏逻辑回归模型在第一个响应标记 logit 上构建了一个更强大的检测器，它在多个指标下大大超过了 SOTA 检测器。</li>
</ul>

<h3>Title: Title:
          Language Generation with Strictly Proper Scoring Rules</h3>
<ul>
<li><strong>Authors: </strong>Chenze Shao, Fandong Meng, Yijin Liu, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Language Generation with Strictly Proper Scoring Rules(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation. Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory. The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities. Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text. In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules. Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score. Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities. Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B. Source code: \url{this https URL}.</li>
<li><strong>摘要：</strong>基于最大似然估计 (MLE) 的语言生成已成为文本生成的基本方法。最大似然估计通常通过最小化对数似然损失（在统计决策理论中也称为对数分数）来执行。对数分数是严格适当的，因为它鼓励诚实的预测，其中预期分数只有在模型报告真实概率时才会最大化。虽然存在许多严格适当的评分规则，但对数分数是其中唯一一个完全取决于观察到的样本概率的局部评分规则，使其能够处理指数级大的自然文本样本空间。在这项工作中，我们提出了一种将评分规则适应语言生成的直接策略，允许使用任何非局部评分规则进行语言建模。利用这一策略，我们使用两个经典的严格适当的评分规则 Brier 分数和 Spherical 分数作为对数分数的替代方案来训练语言生成模型。实验结果表明，只需替换损失函数，而不调整其他超参数，就可以显着提高模型的生成能力。此外，这些改进可以扩展到大型语言模型（LLM），例如 LLaMA-7B 和 LLaMA-13B。源代码：\url{this https URL}。</li>
</ul>

<h3>Title: Title:
          Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) suffer from serious unfaithful chain-of-thought (CoT) issues. Previous work attempts to measure and explain it but lacks in-depth analysis within CoTs and does not consider the interactions among all reasoning components jointly. In this paper, we first study the CoT faithfulness issue at the granularity of CoT steps, identify two reasoning paradigms: centralized reasoning and distributed reasoning, and find their relationship with faithfulness. Subsequently, we conduct a joint analysis of the causal relevance among the context, CoT, and answer during reasoning. The result proves that, when the LLM predicts answers, it can recall correct information missing in the CoT from the context, leading to unfaithfulness issues. Finally, we propose the inferential bridging method to mitigate this issue, in which we use the attribution method to recall information as hints for CoT generation and filter out noisy CoTs based on their semantic consistency and attribution scores. Extensive experiments demonstrate that our approach effectively alleviates the unfaithful CoT problem.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）存在严重的不忠实的思路链（CoT）问题。先前的研究试图测量和解释它，但缺乏对CoT内部的深入分析，也没有联合考虑所有推理组件之间的相互作用。在本文中，我们首先在CoT步骤的粒度上研究CoT忠实性问题，确定两种推理范式：集中式推理和分布式推理，并找到它们与忠实性的关系。随后，我们对推理过程中上下文、CoT和答案之间的因果相关性进行联合分析。结果证明，当LLM预测答案时，它可以从上下文中回忆起CoT中缺失的正确信息，从而导致不忠实问题。最后，我们提出了推理桥接方法来缓解这个问题，其中我们使用归因方法回忆信息作为CoT生成的提示，并根据它们的语义一致性和归因分数过滤掉有噪声的CoT。大量实验表明，我们的方法有效地缓解了不忠实的CoT问题。</li>
</ul>

<h3>Title: Title:
          Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets</h3>
<ul>
<li><strong>Authors: </strong>Peter Devine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Training Large Language Models (LLMs) with Reinforcement Learning from AI Feedback (RLAIF) aligns model outputs more closely with human preferences. This involves an evaluator model ranking multiple candidate responses to user prompts. However, the rankings from popular evaluator models such as GPT-4 can be inconsistent. We propose the Repeat Ranking method - where we evaluate the same responses multiple times and train only on those responses which are consistently ranked. Using 2,714 prompts in 62 languages, we generated responses from 7 top multilingual LLMs and had GPT-4 rank them five times each. Evaluating on MT-Bench chat benchmarks in six languages, our method outperformed the standard practice of training on all available prompts. Our work highlights the quality versus quantity trade-off in RLAIF dataset generation and offers a stackable strategy for enhancing dataset and thus model quality.</li>
<li><strong>摘要：</strong>使用人工智能反馈强化学习 (RLAIF) 训练大型语言模型 (LLM) 可使模型输出更贴近人类偏好。这涉及评估模型对用户提示的多个候选响应进行排名。但是，来自 GPT-4 等流行评估模型的排名可能不一致。我们提出了重复排名方法 - 我们多次评估相同的响应并仅对那些排名一致的响应进行训练。使用 62 种语言的 2,714 个提示，我们从 7 个顶级多语言 LLM 生成响应，并让 GPT-4 对它们分别进行五次排名。在六种语言的 MT-Bench 聊天基准上进行评估后，我们的方法优于在所有可用提示上进行训练的标准做法。我们的工作强调了 RLAIF 数据集生成中的质量与数量权衡，并提供了一种可堆叠的策略来增强数据集和模型质量。</li>
</ul>

<h3>Title: Title:
          Evaluating the External and Parametric Knowledge Fusion of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhang, Yuyang Zhang, Xiaoguang Li, Wenxuan Shi, Haonan Xu, Huanshuo Liu, Yasheng Wang, Lifeng Shang, Qun Liu, Yong Liu, Ruiming Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating the External and Parametric Knowledge Fusion of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Integrating external knowledge into large language models (LLMs) presents a promising solution to overcome the limitations imposed by their antiquated and static parametric memory. Prior studies, however, have tended to over-reliance on external knowledge, underestimating the valuable contributions of an LLMs' intrinsic parametric knowledge. The efficacy of LLMs in blending external and parametric knowledge remains largely unexplored, especially in cases where external knowledge is incomplete and necessitates supplementation by their parametric knowledge. We propose to deconstruct knowledge fusion into four distinct scenarios, offering the first thorough investigation of LLM behavior across each. We develop a systematic pipeline for data construction and knowledge infusion to simulate these fusion scenarios, facilitating a series of controlled experiments. Our investigation reveals that enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration. Nonetheless, we identify persistent challenges in memorizing and eliciting parametric knowledge, and determining parametric knowledge boundaries. Our findings aim to steer future explorations on harmonizing external and parametric knowledge within LLMs.</li>
<li><strong>摘要：</strong>将外部知识集成到大型语言模型 (LLM) 中，为克服其过时且静态的参数记忆所带来的限制提供了一种有希望的解决方案。然而，先前的研究往往过度依赖外部知识，低估了 LLM 内在参数知识的宝贵贡献。LLM 在融合外部和参数知识方面的功效在很大程度上仍未得到探索，特别是在外部知识不完整且需要通过其参数知识进行补充的情况下。我们建议将知识融合分解为四个不同的场景，首次彻底调查每个场景中的 LLM 行为。我们开发了一个系统的数据构建和知识注入管道来模拟这些融合场景，从而促进一系列受控实验。我们的研究表明，增强 LLM 中的参数知识可以显著增强其知识整合能力。尽管如此，我们发现在记忆和引出参数知识以及确定参数知识边界方面仍然存在持续的挑战。我们的研究结果旨在指导未来在 LLM 中协调外部和参数知识的探索。</li>
</ul>

<h3>Title: Title:
          BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Chen Wang, Minpeng Liao, Zhongqiang Huang, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent end-to-end approaches have shown promise in extending large language models (LLMs) to speech inputs, but face limitations in directly assessing and optimizing alignment quality and fail to achieve fine-grained alignment due to speech-text length mismatch. We introduce BLSP-KD, a novel approach for Bootstrapping Language-Speech Pretraining via Knowledge Distillation, which addresses these limitations through two key techniques. First, it optimizes speech-text alignment by minimizing the divergence between the LLM's next-token prediction distributions for speech and text inputs using knowledge distillation. Second, it employs a continuous-integrate-andfire strategy to segment speech into tokens that correspond one-to-one with text tokens, enabling fine-grained alignment. We also introduce Partial LoRA (PLoRA), a new adaptation method supporting LLM finetuning for speech inputs under knowledge distillation. Quantitative evaluation shows that BLSP-KD outperforms previous end-to-end baselines and cascaded systems with comparable scale of parameters, facilitating general instruction-following capabilities for LLMs with speech inputs. This approach provides new possibilities for extending LLMs to spoken language interactions.</li>
<li><strong>摘要：</strong>最近的端到端方法在将大型语言模型 (LLM) 扩展到语音输入方面显示出良好的前景，但在直接评估和优化对齐质量方面面临限制，并且由于语音-文本长度不匹配而无法实现细粒度对齐。我们介绍了一种通过知识蒸馏引导语言-语音预训练的新方法 BLSP-KD，它通过两种关键技术解决了这些限制。首先，它通过使用知识蒸馏来最小化 LLM 对语音和文本输入的下一个标记预测分布之间的差异，从而优化语音-文本对齐。其次，它采用持续积分和激发策略将语音分割成与文本标记一一对应的标记，从而实现细粒度对齐。我们还介绍了部分 LoRA (PLoRA)，这是一种新的自适应方法，支持知识蒸馏下对语音输入进行 LLM 微调。定量评估表明，BLSP-KD 的表现优于之前的端到端基线和具有相当参数规模的级联系统，有助于实现具有语音输入的 LLM 的一般指令跟随能力。这种方法为将 LLM 扩展到口语交互提供了新的可能性。</li>
</ul>

<h3>Title: Title:
          MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors</h3>
<ul>
<li><strong>Authors: </strong>Renzhi Wang, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Model editing aims to efficiently alter the behavior of Large Language Models (LLMs) within a desired scope, while ensuring no adverse impact on other inputs. Recent years have witnessed various model editing methods been proposed. However, these methods either exhibit poor overall performance or struggle to strike a balance between generalization and locality. We propose MOMoE, a model editing adapter utilizing a Mixture of Experts (MoE) architecture with a knowledge anchor routing strategy. MOMoE updates knowledge using a bypass MoE structure, keeping the original parameters unchanged to preserve the general ability of LLMs. And, the knowledge anchor routing ensures that inputs requiring similar knowledge are routed to the same expert, thereby enhancing the generalization of the updated knowledge. Experimental results show the superiority of our approach over both batch editing and sequential batch editing tasks, exhibiting exceptional overall performance alongside outstanding balance between generalization and locality. Our code will be available.</li>
<li><strong>摘要：</strong>模型编辑旨在有效地在所需范围内改变大型语言模型 (LLM) 的行为，同时确保不会对其他输入产生不利影响。近年来，人们提出了各种模型编辑方法。然而，这些方法要么整体性能不佳，要么难以在泛化和局部性之间取得平衡。我们提出了 MOMoE，这是一种模型编辑适配器，采用混合专家 (MoE) 架构和知识锚路由策略。MOMoE 使用旁路 MoE 结构更新知识，保持原始参数不变以保持 LLM 的通用能力。而且，知识锚路由确保需要类似知识的输入被路由到同一个专家，从而增强更新知识的泛化能力。实验结果表明，我们的方法优于批量编辑和顺序批量编辑任务，表现出卓越的整体性能以及泛化和局部性之间的出色平衡。我们的代码将可用。</li>
</ul>

<h3>Title: Title:
          Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions</h3>
<ul>
<li><strong>Authors: </strong>Zhe Hu, Tuo Liang, Jing Li, Yiren Lu, Yunlai Zhou, Yiran Qiao, Jing Ma, Yu Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large multimodal language models have demonstrated remarkable proficiency across a wide range of tasks. Yet, these models still struggle with understanding the nuances of human humor through juxtaposition, particularly when it involves nonlinear narratives that underpin many jokes and humor cues. This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction. We introduce the YesBut benchmark, which comprises tasks of varying difficulty aimed at assessing AI's capabilities in recognizing and interpreting these comics, ranging from literal content comprehension to deep narrative reasoning. Through extensive experimentation and analysis of recent commercial or open-sourced large (vision) language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics. Our results show that even state-of-the-art models still lag behind human performance on this task. Our findings offer insights into the current limitations and potential improvements for AI in understanding human creative expressions.</li>
<li><strong>摘要：</strong>大型多模态语言模型的最新进展已在广泛的任务中表现出非凡的熟练程度。然而，这些模型仍然难以通过并置来理解人类幽默的细微差别，特别是当它涉及支撑许多笑话和幽默线索的非线性叙事时。本文通过关注具有矛盾叙事的漫画来研究这一挑战，其中每个漫画由两个面板组成，形成一个幽默的矛盾。我们引入了 YesBut 基准，它包含不同难度的任务，旨在评估人工智能识别和解释这些漫画的能力，从文字内容理解到深度叙事推理。通过对最近的商业或开源大型（视觉）语言模型进行广泛的实验和分析，我们评估了它们理解这些漫画中固有的叙事幽默的复杂相互作用的能力。我们的结果表明，即使是最先进的模型在这项任务上仍然落后于人类的表现。我们的研究结果为人工智能在理解人类创造性表达方面的当前局限性和潜在改进提供了见解。</li>
</ul>

<h3>Title: Title:
          Faithful Chart Summarization with ChaTS-Pi</h3>
<ul>
<li><strong>Authors: </strong>Syrine Krichene, Francesco Piccinno, Fangyu Liu, Julian Martin Eisenschlos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Faithful Chart Summarization with ChaTS-Pi(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people. Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors. In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence. We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences. We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets.</li>
<li><strong>摘要：</strong>图表到摘要的生成有助于探索数据、传达见解并帮助视障人士。多模态生成模型已用于生成流畅的摘要，但它们可能会受到事实和感知错误的影响。在这项工作中，我们提出了 CHATS-CRITIC，这是一种用于评分忠实度的无参考图表摘要指标。CHATS-CRITIC 由一个用于从图表中恢复表格的图像到文本模型和一个用于逐句评分摘要的表格蕴涵模型组成。我们发现 CHATS-CRITIC 根据人工评分评估摘要质量的效果优于基于参考的指标（无论是学习的还是基于 n-gram 的），并且可以进一步用于通过删除不受支持的句子来修复候选摘要。然后，我们引入了 CHATS-PI，这是一种图表到摘要的管道，它在推理过程中利用 CHATS-CRITIC 来修复和排名来自任何图表摘要模型的采样候选。我们使用人工评估者来评估 CHATS-PI 和 CHATS-CRITIC，在两个流行的图表到摘要数据集上建立了最先进的结果。</li>
</ul>

<h3>Title: Title:
          PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Fangzhi Xu, Qika Lin, Tianzhe Zhao, Jiawei Han, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \textbf{PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.</li>
<li><strong>摘要：</strong>逻辑推理任务自提出以来就引起了人们的极大兴趣。面对这样的任务，当前的竞争模型，甚至是大型语言模型（例如 ChatGPT 和 PaLM 2），仍然表现不佳。以前有前途的 LM 在逻辑一致性建模和逻辑结构感知方面表现不佳。为此，我们通过将每个逻辑样本转换为推理路径来对逻辑推理任务进行建模，并提出了一个架构 \textbf{PathReasoner}。它从数据和模型的角度解决了该任务。为了扩大逻辑样本的多样性，我们提出了一种由等效逻辑公式支持的原子扩展策略，以形成新的推理路径。从模型的角度来看，我们设计了一个 Transformer 样式的块堆栈。特别是，我们提出了一个路径注意模块，将模型原子内和跨原子关系与高阶扩散策略结合起来。实验表明，PathReasoner 在两个逻辑推理基准上取得了有竞争力的表现，并且具有很强的泛化能力。</li>
</ul>

<h3>Title: Title:
          DGRC: An Effective Fine-tuning Framework for Distractor Generation in Chinese Multi-choice Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Runfeng Lin, Dacheng Xu, Huijiang Wang, Zebiao Chen, Yating Wang, Shouqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DGRC: An Effective Fine-tuning Framework for Distractor Generation in Chinese Multi-choice Reading Comprehension(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>When evaluating a learner's knowledge proficiency, the multiple-choice question is an efficient and widely used format in standardized tests. Nevertheless, generating these questions, particularly plausible distractors (incorrect options), poses a considerable challenge. Generally, the distractor generation can be classified into cloze-style distractor generation (CDG) and natural questions distractor generation (NQDG). In contrast to the CDG, utilizing pre-trained language models (PLMs) for NQDG presents three primary challenges: (1) PLMs are typically trained to generate ``correct'' content, like answers, while rarely trained to generate ``plausible" content, like distractors; (2) PLMs often struggle to produce content that aligns well with specific knowledge and the style of exams; (3) NQDG necessitates the model to produce longer, context-sensitive, and question-relevant distractors. In this study, we introduce a fine-tuning framework named DGRC for NQDG in Chinese multi-choice reading comprehension from authentic examinations. DGRC comprises three major components: hard chain-of-thought, multi-task learning, and generation mask patterns. The experiment results demonstrate that DGRC significantly enhances generation performance, achieving a more than 2.5-fold improvement in BLEU scores.</li>
<li><strong>摘要：</strong>在评估学习者的知识水平时，多项选择题是标准化考试中一种高效且广泛使用的形式。然而，生成这些问题，尤其是合理干扰项（错误选项），却带来了相当大的挑战。一般来说，干扰项生成可分为完形填空式干扰项生成（CDG）和自然问题干扰项生成（NQDG）。与 CDG 相比，使用预训练语言模型 (PLM) 进行 NQDG 存在三个主要挑战：（1）PLM 通常被训练为生成“正确”的内容，如答案，而很少被训练为生成“合理”的内容，如干扰项；（2）PLM 通常难以生成与特定知识和考试风格相符的内容；（3）NQDG 要求模型生成更长、上下文相关且与问题相关的干扰项。在本研究中，我们为 NQDG 引入了一个微调框架 DGRC，用于真实考试中的中文多选阅读理解。DGRC 包括三个主要组件：硬思路链、多任务学习和生成掩码模式。实验结果表明，DGRC 显著提高了生成性能，BLEU 分数提高了 2.5 倍以上。</li>
</ul>

<h3>Title: Title:
          Lower Bounds on the Expressivity of Recurrent Neural Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anej Svete, Franz Nowak, Anisha Mohamed Sahabdeen, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Lower Bounds on the Expressivity of Recurrent Neural Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The recent successes and spread of large neural language models (LMs) call for a thorough understanding of their computational ability. Describing their computational abilities through LMs' \emph{representational capacity} is a lively area of research. However, investigation into the representational capacity of neural LMs has predominantly focused on their ability to \emph{recognize} formal languages. For example, recurrent neural networks (RNNs) with Heaviside activations are tightly linked to regular languages, i.e., languages defined by finite-state automata (FSAs). Such results, however, fall short of describing the capabilities of RNN \emph{language models} (LMs), which are definitionally \emph{distributions} over strings. We take a fresh look at the representational capacity of RNN LMs by connecting them to \emph{probabilistic} FSAs and demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs.</li>
<li><strong>摘要：</strong>大型神经语言模型 (LM) 最近的成功和普及要求彻底了解它们的计算能力。通过 LM 的 \emph{表示能力} 描述它们的计算能力是一个活跃的研究领域。然而，对神经 LM 表示能力的研究主要集中在它们 \emph{识别} 形式语言的能力上。例如，具有 Heaviside 激活的循环神经网络 (RNN) 与常规语言紧密相关，即由有限状态自动机 (FSA) 定义的语言。然而，这样的结果不足以描述 RNN \emph{语言模型} (LM) 的功能，它们在定义上是字符串上的 \emph{分布}。我们通过将 RNN LM 与 \emph{概率} FSA 连接起来，重新审视 RNN LM 的表示能力，并证明具有线性有界精度的 RNN LM 可以表达任意常规 LM。</li>
</ul>

<h3>Title: Title:
          Faster Cascades via Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Faster Cascades via Speculative Decoding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for "hard" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.</li>
<li><strong>摘要：</strong>级联和推测解码是提高语言模型推理效率的两种常用方法。这两种方法都涉及交错不同大小的模型，但通过根本不同的机制：级联采用延迟规则，仅在“硬”输入时调用较大的模型，而推测解码使用推测执行主要在并行验证模式下调用较大的模型。这些机制提供了不同的好处：从经验上讲，级联通常能够产生比较大模型更好的质量，而从理论上讲，推测解码可以保证质量中立。在本文中，我们利用这两种方法的优点，设计了新的推测级联技术，通过推测执行来实现它们的延迟规则。我们描述了推测级联的最佳延迟规则，并使用插件近似最佳规则。通过在基准语言任务上使用 T5 模型进行实验，我们表明，与级联和推测解码基线相比，所提出的方法可以产生更好的成本质量权衡。</li>
</ul>

<h3>Title: Title:
          Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce $\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (i) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (ii) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\texttt{gpt2}$s to effectively improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small model pairs (e.g., $\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improve the length-controlled win rates of both white-box and black-box large models against $\texttt{gpt-4-turbo}$ (e.g., $34.4 \rightarrow 37.9$ for $\texttt{Llama-3-70B-Instruct}$ and $16.0 \rightarrow 20.1$ for $\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\approx 10.0$.</li>
<li><strong>摘要：</strong>大型语言模型通常会经过微调以符合人类偏好。然而，对大型语言模型进行微调可能具有挑战性。在这项工作中，我们引入了 $\textit{从弱到强搜索}$，将大型语言模型的对齐定义为测试时间贪婪搜索，以最大化小型调整和未调整模型之间的对数似然差异，同时从冻结的大型模型中采样。该方法既可用作 (i) 计算高效的模型升级策略，避免直接调整大型模型，又可用作 (ii) 从弱到强泛化的实例，可通过弱测试时间指导增强强模型。从经验上讲，我们展示了从弱到强搜索在不同任务中的灵活性。在受控情绪生成和摘要中，我们使用调整和未调整的 $\texttt{gpt2}$ 来有效改善大型模型的对齐，而无需额外的训练。至关重要的是，在更困难的指令跟踪基准测试 AlpacaEval 2.0 中，我们表明，重复使用现成的小模型对（例如，$\texttt{zephyr-7b-beta}$ 及其未调整版本）可以显着提高白盒和黑盒大模型对 $\texttt{gpt-4-turbo}$ 的长度控制胜率（例如，$\texttt{Llama-3-70B-Instruct}$ 为 $34.4 \rightarrow 37.9$，$\texttt{gpt-3.5-turbo-instruct}$ 为 $16.0 \rightarrow 20.1$），尽管小模型的胜率低至 $\approx 10.0$。</li>
</ul>

<h3>Title: Title:
          AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data</h3>
<ul>
<li><strong>Authors: </strong>Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, Cairong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence.</li>
<li><strong>摘要：</strong>开源大型语言模型 (LLM) 及其专门变体，尤其是代码 LLM，最近表现出色。然而，以前的代码 LLM 通常是针对质量和多样性有限的单源数据进行微调的，这可能不足以发挥预训练代码 LLM 的潜力。在本文中，我们介绍了 AlchemistCoder，这是一系列针对多源数据进行微调的代码 LLM，具有增强的代码生成和泛化能力。为了实现这一目标，我们率先揭示了多源代码语料库中各种风格和质量之间的固有冲突，并引入了事后重新标记的数据特定提示，称为 AlchemistPrompts，以协调不同的数据源和指令-响应对。此外，我们建议将数据构建过程作为代码理解任务纳入微调数据中，包括指令演化、数据过滤和代码审查。大量实验表明，AlchemistCoder 在所有相同大小（6.7B/7B）的模型中处于领先地位，并且可以与更大的模型（15B/33B/70B）相媲美甚至超越，展示了我们的方法在改进指令遵循能力和推进代码智能边界方面的有效性。</li>
</ul>

<h3>Title: Title:
          PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</h3>
<ul>
<li><strong>Authors: </strong>Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.</li>
<li><strong>摘要：</strong>开发智能儿科问诊系统为提高诊断效率提供了良好的前景，尤其是在医疗资源匮乏的中国。尽管中医大型语言模型 (LLM) 近年来取得了进展，但由于指令数据不足和训练程序脆弱，它们在儿科应用中的表现并不理想。为了解决上述问题，本文构建了 PedCorpus，这是一个高质量的数据集，包含来自儿科教科书、指南和知识图谱资源的 300,000 多条多任务指令，可满足不同的诊断需求。在精心设计的 PedCorpus 基础上，我们提出了 PediatricsGPT，这是第一个基于系统而强大的训练流程构建的中文儿科 LLM 助手。在持续的预训练阶段，我们引入了一种混合指令预训练机制，以减轻 LLM 内部注入的知识不一致性，以实现医学领域适应性。立即利用全参数监督微调 (SFT) 将一般医学知识模式纳入模型。之后，我们设计了一种直接跟随偏好优化，以增强类似儿科医生的人文主义反应的生成。在参数高效的二级 SFT 阶段，提出了一种通用-特定专家混合策略来解决医学通才和儿科专业知识掌握之间的能力冲突。基于指标、GPT-4 和医生对不同医生下游任务的评估的广泛结果表明，PediatricsGPT 的表现始终优于之前的中国医学 LLM。我们的模型和数据集将开源以供社区开发。</li>
</ul>

<h3>Title: Title:
          MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Michael Regan, Shira Wein, George Baker, Emilio Monti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Abstract Meaning Representation (AMR) is a semantic formalism that captures the core meaning of an utterance. There has been substantial work developing AMR corpora in English and more recently across languages, though the limited size of existing datasets and the cost of collecting more annotations are prohibitive. With both engineering and scientific questions in mind, we introduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graph annotations, currently the largest and most diverse of its kind: AMR graphs for 1,685 information-seeking utterances mapped to 50+ typologically diverse languages. We describe how we built our resource and its unique features before reporting on experiments using large language models for multilingual AMR and SPARQL parsing as well as applying AMRs for hallucination detection in the context of knowledge base question answering, with results shedding light on persistent issues using LLMs for structured parsing.</li>
<li><strong>摘要：</strong>摘要 意义表示 (AMR) 是一种语义形式，可以捕捉话语的核心意义。尽管现有数据集的规模有限，收集更多注释的成本过高，但人们已经投入了大量精力开发英语和最近跨语言的 AMR 语料库。考虑到工程和科学问题，我们推出了 MASSIVE-AMR，这是一个拥有超过 84,000 个文本到图形注释的数据集，目前是同类数据集中规模最大、种类最多的数据集：1,685 个信息搜索话语的 AMR 图形映射到 50 多种类型多样的语言。我们描述了如何构建我们的资源及其独特功能，然后报告了使用大型语言模型进行多语言 AMR 和 SPARQL 解析的实验，以及在知识库问答环境中将 AMR 应用于幻觉检测的实验，结果揭示了使用 LLM 进行结构化解析的持续存在的问题。</li>
</ul>

<h3>Title: Title:
          Expert-Guided Extinction of Toxic Tokens for Debiased Generation</h3>
<ul>
<li><strong>Authors: </strong>Xueyao Sun, Kaize Shi, Haoran Tang, Guandong Xu, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Expert-Guided Extinction of Toxic Tokens for Debiased Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可能会在生成过程中引发社会偏见，尤其是在使用有毒提示进行推理时。控制生成中的敏感属性在数据分布、通用性和效率方面面临挑战。具体而言，微调和检索需要大量无偏语料库，而直接提示需要精心策划的指令来纠正多轮思考中的输出，但对记忆和推理延迟提出了挑战。在这项工作中，我们提出了专家指导的有毒标记消除去偏生成 (EXPOSED)，以消除不具备上述要求的 LLM 的不良有害输出。EXPOSED 基于丰富的有毒语料库构建了一个去偏专家，以暴露和引出潜在的危险标记。然后，它将输出处理到 LLM，并通过抑制和减弱有毒标记来构建公平分布。EXPOSED 在三个 LLM 系列的公平性基准上进行了评估。大量实验表明，与其他基线相比，提出的 EXPOSED 在平衡公平性和生成性能的同时显著降低了潜在的社会偏见。</li>
</ul>

<h3>Title: Title:
          Are Large Language Models Chameleons?</h3>
<ul>
<li><strong>Authors: </strong>Mingmeng Geng, Sihong He, Roberto Trotta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Are Large Language Models Chameleons?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Do large language models (LLMs) have their own worldviews and personality tendencies? Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times. Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity. We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是否有自己的世界观和性格倾向？要求 LLM 回答主观问题的模拟进行了 100 多万次。将不同 LLM 的回答与欧洲社会调查 (ESS) 的真实数据进行比较表明，提示对偏见和变异性的影响是根本性的，突出了主要的文化、年龄和性别偏见。讨论了测量 LLM 和调查数据之间差异的方法，例如计算加权均值和受 Jaccard 相似性启发的新提出的测量方法。我们得出的结论是，在使用 LLM 模拟个人决策或集体行为之前，分析提示的稳健性和变异性非常重要，因为它们的模仿能力充其量只是近似的。</li>
</ul>

<h3>Title: Title:
          Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</h3>
<ul>
<li><strong>Authors: </strong>Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Nearest Neighbor Speculative Decoding for LLM Generation and Attribution(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 经常会产生幻觉，并且缺乏为其生成提供归因的能力。半参数 LM（例如 kNN-LM）通过使用非参数数据存储中的最近邻匹配来优化给定提示的 LM 输出来解决这些限制。但是，这些模型通常表现出较慢的推理速度并产生不流畅的文本。在本文中，我们介绍了最近邻推测解码 (NEST)，这是一种新颖的半参数语言建模方法，能够将任意长度的真实文本跨度合并到 LM 生成中并提供对其来源的归因。NEST 在每个推理步骤执行标记级检索，以计算半参数混合分布并识别语料库中有希望的跨度延续。然后，它使用近似推测解码过程，该过程接受检索到的跨度的前缀或生成新标记。 NEST 显著提高了基础 LM 在各种知识密集型任务中的生成质量和归因率，超越了传统的 kNN-LM 方法，并且在上下文检索增强方面具有竞争力。此外，NEST 还大大提高了生成速度，在应用于 Llama-2-Chat 70B 时，推理时间加快了 1.8 倍。</li>
</ul>

<h3>Title: Title:
          MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</h3>
<ul>
<li><strong>Authors: </strong>Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 近年来取得了长足进步，在不同任务上取得了前所未有的表现。然而，出于商业利益考虑，最具竞争力的模型（如 GPT、Gemini 和 Claude）一直被封闭在专有接口后面，没有公开训练细节。最近，许多机构开源了几个强大的 LLM，如 LLaMA-3，可与现有的闭源 LLM 相媲美。然而，只提供了模型的权重，大多数细节（例如中间检查点、预训练语料库和训练代码等）均未公开。为了提高 LLM 的透明度，研究社区已经形成，开源真正开放的 LLM（例如 Pythia、Amber、OLMo），其中提供了更多细节（例如预训练语料库和训练代码）。这些模型极大地推动了对这些大型模型的科学研究，包括它们的优势、劣势、偏差和风险。然而，我们观察到，现有的真正开放的 LLM 在推理、知识和编码任务上仍然不如现有的具有类似模型大小的最先进的 LLM。为此，我们开源了 MAP-Neo，这是一个功能强大且透明的双语语言模型，具有 7B 参数，在 4.5T 高质量 token 上从头开始训练。我们的 MAP-Neo 是第一个完全开源的双语 LLM，其性能与现有的最先进的 LLM 相当。此外，我们开源了所有细节以重现我们的 MAP-Neo，其中提供了清理后的预训练语料库、数据清理管道、检查点和经过优化的训练/评估框架。最后，我们希望我们的 MAP-Neo 能够增强和加强开放研究社区，并激发更多创新和创造力，以促进 LLM 的进一步改进。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
