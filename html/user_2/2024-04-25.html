<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-25</h1>
<h3>Title: XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>João Monteiro, Étienne Marcotte, Pierre-André Noël, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15420">https://arxiv.org/abs/2404.15420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15420">https://arxiv.org/pdf/2404.15420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15420]] XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference(https://arxiv.org/abs/2404.15420)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) approaches typically leverage prompting to condition decoder-only language model generation on reference information. Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable. However, caching transformer states can easily require almost as much space as the model parameters. When the right context isn't known in advance, caching ICL can be challenging. This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt. More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers. We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform ICL, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 方法通常利用提示来根据参考信息生成仅解码器的语言模型。由于自注意力操作的二次成本，上下文的即时处理效率低下，因此需要缓存。然而，缓存变压器状态很容易需要几乎与模型参数一样多的空间。当事先不知道正确的上下文时，缓存 ICL 可能会很困难。这项工作通过引入模型来解决这些限制，这些模型受编码器-解码器架构的启发，使用交叉注意力来在没有提示的情况下对参考文本进行条件生成。更准确地说，我们利用预训练的仅解码器模型，并且仅训练少量的添加层。我们使用问答（QA）作为测试平台来评估我们的模型执行条件生成的能力，并观察到它们的性能优于 ICL，与微调提示的 LLM 相当，并且相对于标准 KV 缓存大幅减少了空间占用：两个数量级。</li>
</ul>

<h3>Title: Large Language Models Spot Phishing Emails with Surprising Accuracy: A  Comparative Analysis of Performance</h3>
<ul>
<li><strong>Authors: </strong>Het Patel, Umair Rehman, Farkhund Iqbal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15485">https://arxiv.org/abs/2404.15485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15485">https://arxiv.org/pdf/2404.15485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15485]] Large Language Models Spot Phishing Emails with Surprising Accuracy: A  Comparative Analysis of Performance(https://arxiv.org/abs/2404.15485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Phishing, a prevalent cybercrime tactic for decades, remains a significant threat in today's digital world. By leveraging clever social engineering elements and modern technology, cybercrime targets many individuals, businesses, and organizations to exploit trust and security. These cyber-attackers are often disguised in many trustworthy forms to appear as legitimate sources. By cleverly using psychological elements like urgency, fear, social proof, and other manipulative strategies, phishers can lure individuals into revealing sensitive and personalized information. Building on this pervasive issue within modern technology, this paper aims to analyze the effectiveness of 15 Large Language Models (LLMs) in detecting phishing attempts, specifically focusing on a randomized set of "419 Scam" emails. The objective is to determine which LLMs can accurately detect phishing emails by analyzing a text file containing email metadata based on predefined criteria. The experiment concluded that the following models, ChatGPT 3.5, GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting phishing emails.</li>
<li><strong>摘要：</strong>网络钓鱼是几十年来流行的网络犯罪策略，仍然是当今数字世界的重大威胁。通过利用巧妙的社会工程元素和现代技术，网络犯罪以许多个人、企业和组织为目标，以利用信任和安全。这些网络攻击者通常以许多值得信赖的形式伪装成合法来源。通过巧妙地利用紧迫感、恐惧、社会认同等心理因素和其他操纵策略，网络钓鱼者可以引诱个人泄露敏感和个性化的信息。基于现代技术中这一普遍存在的问题，本文旨在分析 15 种大型语言模型 (LLM) 在检测网络钓鱼尝试方面的有效性，特别关注一组随机的“419 诈骗”电子邮件。目标是根据预定义的标准分析包含电子邮件元数据的文本文件，确定哪些法学硕士可以准确检测网络钓鱼电子邮件。实验得出结论，以下模型 ChatGPT 3.5、GPT-3.5-Turbo-Instruct 和 ChatGPT 在检测网络钓鱼电子邮件方面最有效。</li>
</ul>

<h3>Title: IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &  Correction Task On the Shoulders of Medical Agents</h3>
<ul>
<li><strong>Authors: </strong>Jean-Philippe Corbeil</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15488">https://arxiv.org/abs/2404.15488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15488">https://arxiv.org/pdf/2404.15488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15488]] IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &  Correction Task On the Shoulders of Medical Agents(https://arxiv.org/abs/2404.15488)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>In natural language processing applied to the clinical domain, utilizing large language models has emerged as a promising avenue for error detection and correction on clinical notes, a knowledge-intensive task for which annotated data is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a suite of four LLM-based medical agents. The MedReAct agent initiates the process by observing, analyzing, and taking action, generating trajectories to guide the search to target a potential error in the clinical notes. Subsequently, the MedEval agent employs five evaluators to assess the targeted error and the proposed correction. In cases where MedReAct's actions prove insufficient, the MedReFlex agent intervenes, engaging in reflective analysis and proposing alternative strategies. Finally, the MedFinalParser agent formats the final output, preserving the original style while ensuring the integrity of the error correction process. One core component of our method is our RAG pipeline based on our ClinicalCorp corpora. Among other well-known sources containing clinical guidelines and information, we preprocess and release the open-source MedWiki dataset for clinical RAG application. Our results demonstrate the central role of our RAG approach with ClinicalCorp leveraged through the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the MEDIQA-CORR 2024 final leaderboard.</li>
<li><strong>摘要：</strong>在应用于临床领域的自然语言处理中，利用大型语言模型已成为临床笔记错误检测和纠正的有前途的途径，这是一项知识密集型任务，注释数据稀缺。本文介绍了 MedReAct'N'MedReFlex，它利用了一套由四种基于 LLM 的医疗代理组成的套件。 MedReAct 代理通过观察、分析和采取行动来启动该过程，生成轨迹来指导搜索以瞄准临床记录中的潜在错误。随后，MedEval 代理雇用五名评估员来评估目标错误和建议的纠正。如果 MedReAct 的行动证明不够，MedReFlex 代理会进行干预，进行反思分析并提出替代策略。最后，MedFinalParser 代理格式化最终输出，保留原始样式，同时确保纠错过程的完整性。我们方法的核心组成部分是基于 ClinicalCorp 语料库的 RAG 管道。在包含临床指南和信息的其他知名来源中，我们预处理并发布了用于临床 RAG 应用的开源 MedWiki 数据集。我们的结果证明了我们通过 MedReAct'N'MedReFlex 框架与 ClinicalCorp 合作的 RAG 方法的核心作用。它在 MEDIQA-CORR 2024 年最终排行榜上排名第九。</li>
</ul>

<h3>Title: ToM-LM: Delegating Theory Of Mind Reasoning to External Symbolic  Executors in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weizhi Tang, Vaishak Belle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15515">https://arxiv.org/abs/2404.15515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15515">https://arxiv.org/pdf/2404.15515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15515]] ToM-LM: Delegating Theory Of Mind Reasoning to External Symbolic  Executors in Large Language Models(https://arxiv.org/abs/2404.15515)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM) refers to the ability of individuals to attribute mental states to others. While Large Language Models (LLMs) have shown some promise with ToM ability, they still struggle with complex ToM reasoning. Our approach leverages an external symbolic executor, specifically the SMCDEL model checker, and fine-tuning to improve the ToM reasoning ability of LLMs. In our approach, an LLM is first fine-tuned through pairs of natural language and symbolic formulation representation of ToM problems and is then instructed to generate the symbolic formulation with a one-shot in-context example. The generated symbolic formulation is then executed by the SMCDEL model checker to perform transparent and verifiable ToM reasoning and give the final result. We demonstrate that our approach, ToM-LM, shows a significant improvement over all the constructed baselines. Our study proposes a novel view about externalizing a particular component of ToM reasoning, mainly reasoning about beliefs, and suggests generalizing it to other aspects of ToM reasoning.</li>
<li><strong>摘要：</strong>心理理论（ToM）是指个体将心理状态归因于他人的能力。虽然大型语言模型 (LLM) 在 ToM 能力方面表现出了一些希望，但它们仍然难以应对复杂的 ToM 推理。我们的方法利用外部符号执行器，特别是 SMCDEL 模型检查器，并进行微调以提高法学硕士的 ToM 推理能力。在我们的方法中，法学硕士首先通过成对的自然语言和 ToM 问题的符号表述表示进行微调，然后指示使用一次性上下文示例生成符号表述。然后生成的符号公式由 SMCDEL 模型检查器执行，以进行透明且可验证的 ToM 推理并给出最终结果。我们证明，我们的方法 ToM-LM 比所有构建的基线都有显着改进。我们的研究提出了一种关于外化 ToM 推理的特定组成部分（主要是关于信念的推理）的新颖观点，并建议将其推广到 ToM 推理的其他方面。</li>
</ul>

<h3>Title: Towards Systematic Evaluation of Logical Reasoning Ability of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15522">https://arxiv.org/abs/2404.15522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15522">https://arxiv.org/pdf/2404.15522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15522]] Towards Systematic Evaluation of Logical Reasoning Ability of Large  Language Models(https://arxiv.org/abs/2404.15522)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really "reason" over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench.</li>
<li><strong>摘要：</strong>最近开发的大型语言模型（LLM）已被证明在广泛的语言理解任务中表现非常出色。但是，他们真的能对自然语言进行“推理”吗？这个问题一直受到广泛的研究关注，并且许多推理技能（例如常识、数字和定性）都得到了研究。然而，与“逻辑推理”相关的关键技能仍未得到充分探索。现有研究法学硕士推理能力的工作仅关注命题逻辑和一阶逻辑的几个推理规则（例如肯定前件和托伦斯）。针对上述限制，我们综合评估了法学硕士在命题逻辑、一阶逻辑和非单调逻辑的 25 种不同推理模式上的逻辑推理能力。为了实现系统评估，我们引入了 LogicBench，这是一个专注于单个推理规则的使用的自然语言问答数据集。我们使用思维链提示对 GPT-4、ChatGPT、Gemini、Llama-2 和 Mistral 等一系列法学硕士进行了详细分析。实验结果表明，现有的法学硕士在 LogicBench 上表现不佳；尤其是，他们在处理涉及复杂推理和否定的情况时遇到困难。此外，他们有时会忽略推理得出正确结论所必需的上下文信息。我们相信，我们的工作和发现有助于未来评估和增强法学硕士逻辑推理能力的研究。数据和代码可在 https://github.com/Mihir3009/LogicBench 获取。</li>
</ul>

<h3>Title: PRISM: Patient Records Interpretation for Semantic Clinical Trial  Matching using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shashi Kant Gupta, Aditya Basu, Mauro Nievas, Jerrin Thomas, Nathan Wolfrath, Adhitya Ramamurthi, Bradley Taylor, Anai N. Kothari, Therica M. Miller, Sorena Nadaf-Rahrov, Yanshan Wang, Hrituraj Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15549">https://arxiv.org/abs/2404.15549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15549">https://arxiv.org/pdf/2404.15549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15549]] PRISM: Patient Records Interpretation for Semantic Clinical Trial  Matching using Large Language Models(https://arxiv.org/abs/2404.15549)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Clinical trial matching is the task of identifying trials for which patients may be potentially eligible. Typically, this task is labor-intensive and requires detailed verification of patient electronic health records (EHRs) against the stringent inclusion and exclusion criteria of clinical trials. This process is manual, time-intensive, and challenging to scale up, resulting in many patients missing out on potential therapeutic options. Recent advancements in Large Language Models (LLMs) have made automating patient-trial matching possible, as shown in multiple concurrent research studies. However, the current approaches are confined to constrained, often synthetic datasets that do not adequately mirror the complexities encountered in real-world medical data. In this study, we present the first, end-to-end large-scale empirical evaluation of clinical trial matching using real-world EHRs. Our study showcases the capability of LLMs to accurately match patients with appropriate clinical trials. We perform experiments with proprietary LLMs, including GPT-4 and GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show that OncoLLM, despite its significantly smaller size, not only outperforms GPT-3.5 but also matches the performance of qualified medical doctors. All experiments were carried out on real-world EHRs that include clinical notes and available clinical trials from a single cancer center in the United States.</li>
<li><strong>摘要：</strong>临床试验匹配是确定患者可能有资格参加的试验的任务。通常，这项任务是劳动密集型的，需要根据临床试验严格的纳入和排除标准对患者电子健康记录 (EHR) 进行详细验证。这个过程是手动的、耗时的，并且难以扩大规模，导致许多患者错过了潜在的治疗选择。大型语言模型 (LLM) 的最新进展使得自动化患者试验匹配成为可能，正如多项并行研究所示。然而，当前的方法仅限于受限的、通常是合成的数据集，这些数据集不能充分反映现实世界医疗数据中遇到的复杂性。在这项研究中，我们首次使用真实世界的 EHR 对临床试验匹配进行了端到端的大规模实证评估。我们的研究展示了法学硕士将患者与适当的临床试验准确匹配的能力。我们使用专有的 LLM（包括 GPT-4 和 GPT-3.5）以及我们名为 OncoLLM 的定制微调模型进行实验，结果表明 OncoLLM 尽管尺寸小得多，但不仅优于 GPT-3.5，而且与 GPT-3.5 的性能相匹配。合格的医生。所有实验都是在现实世界的电子病历上进行的，其中包括来自美国单个癌症中心的临床记录和可用的临床试验。</li>
</ul>

<h3>Title: Retrieval Head Mechanistically Explains Long-Context Factuality</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, Yao Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15574">https://arxiv.org/abs/2404.15574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15574">https://arxiv.org/pdf/2404.15574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15574]] Retrieval Head Mechanistically Explains Long-Context Factuality(https://arxiv.org/abs/2404.15574)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context, hallucination, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.</li>
<li><strong>摘要：</strong>尽管长上下文语言模型最近取得了进展，但基于变压器的模型如何表现出从长上下文中的任意位置检索相关信息的能力仍然难以捉摸。本文旨在解决这个问题。我们对各种模型的系统调查表明，一种特殊类型的注意力头主要负责检索信息，我们将其称为检索头。我们发现了检索头的有趣特性：（1）通用性：所有具有长上下文能力的探索模型都有一组检索头； （2）稀疏：只有一小部分（小于5%）的注意力头被检索。 (3) 内在的：检索头已经存在于用短上下文预训练的模型中。当通过持续预训练来扩展上下文长度时，执行信息检索的仍然是同一组头。 (4)动态激活：以Llama-2 7B为例，无论上下文如何变化，12个检索头始终关注所需信息。其余的检索头在不同的上下文中被激活。 （5）因果性：完全剪枝检索头会导致无法检索到相关信息并产生幻觉，而随机剪枝非检索头不会影响模型的检索能力。我们进一步表明，检索头强烈影响思想链（CoT）推理，其中模型需要经常引用问题和先前生成的上下文。相反，模型使用其内在知识直接生成答案的任务受屏蔽检索头的影响较小。这些观察结果共同解释了模型的哪个内部部分从输入标记中寻求信息。我们相信我们的见解将促进未来关于减少幻觉、改进推理和压缩 KV 缓存的研究。</li>
</ul>

<h3>Title: Can Foundational Large Language Models Assist with Conducting  Pharmaceuticals Manufacturing Investigations?</h3>
<ul>
<li><strong>Authors: </strong>Hossein Salami (1), Brandye Smith-Goettler (2), Vijay Yadav (2) ((1) Digital Services, MMD, Merck & Co., Inc., Rahway, NJ, USA, (2) Digital Services, MMD, Merck & Co., Inc., West Point, PA, USA)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15578">https://arxiv.org/abs/2404.15578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15578">https://arxiv.org/pdf/2404.15578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15578]] Can Foundational Large Language Models Assist with Conducting  Pharmaceuticals Manufacturing Investigations?(https://arxiv.org/abs/2404.15578)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>General purpose Large Language Models (LLM) such as the Generative Pretrained Transformer (GPT) and Large Language Model Meta AI (LLaMA) have attracted much attention in recent years. There is strong evidence that these models can perform remarkably well in various natural language processing tasks. However, how to leverage them to approach domain-specific use cases and drive value remains an open question. In this work, we focus on a specific use case, pharmaceutical manufacturing investigations, and propose that leveraging historical records of manufacturing incidents and deviations in an organization can be beneficial for addressing and closing new cases, or de-risking new manufacturing campaigns. Using a small but diverse dataset of real manufacturing deviations selected from different product lines, we evaluate and quantify the power of three general purpose LLMs (GPT-3.5, GPT-4, and Claude-2) in performing tasks related to the above goal. In particular, (1) the ability of LLMs in automating the process of extracting specific information such as root cause of a case from unstructured data, as well as (2) the possibility of identifying similar or related deviations by performing semantic search on the database of historical records are examined. While our results point to the high accuracy of GPT-4 and Claude-2 in the information extraction task, we discuss cases of complex interplay between the apparent reasoning and hallucination behavior of LLMs as a risk factor. Furthermore, we show that semantic search on vector embedding of deviation descriptions can be used to identify similar records, such as those with a similar type of defect, with a high level of accuracy. We discuss further improvements to enhance the accuracy of similar record identification.</li>
<li><strong>摘要：</strong>通用大型语言模型（LLM），例如生成预训练变压器（GPT）和大型语言模型元人工智能（LLaMA）近年来引起了广泛关注。有强有力的证据表明这些模型可以在各种自然语言处理任务中表现出色。然而，如何利用它们来处理特定领域的用例并驱动价值仍然是一个悬而未决的问题。在这项工作中，我们专注于特定的用例，即药品制造调查，并建议利用组织中制造事件和偏差的历史记录有助于解决和结束新案例，或降低新制造活动的风险。使用从不同产品线中选择的小型但多样化的实际制造偏差数据集，我们评估和量化了三个通用 LLM（GPT-3.5、GPT-4 和 Claude-2）在执行与上述目标相关的任务时的能力。特别是，（1）法学硕士能够自动从非结构化数据中提取特定信息（例如案例的根本原因）的过程，以及（2）通过在数据库上执行语义搜索来识别相似或相关偏差的可能性的历史记录进行审查。虽然我们的结果表明 GPT-4 和 Claude-2 在信息提取任务中具有很高的准确性，但我们讨论了 LLM 的明显推理和幻觉行为之间复杂相互作用的情况作为风险因素。此外，我们还表明，偏差描述向量嵌入的语义搜索可用于高度准确地识别相似记录，例如具有相似类型缺陷的记录。我们讨论进一步的改进，以提高类似记录识别的准确性。</li>
</ul>

<h3>Title: Minimal Evidence Group Identification for Claim Verification</h3>
<ul>
<li><strong>Authors: </strong>Xiangci Li, Sihao Chen, Rajvi Kapadia, Jessica Ouyang, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15588">https://arxiv.org/abs/2404.15588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15588">https://arxiv.org/pdf/2404.15588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15588]] Minimal Evidence Group Identification for Claim Verification(https://arxiv.org/abs/2404.15588)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Claim verification in real-world settings (e.g. against a large collection of candidate evidences retrieved from the web) typically requires identifying and aggregating a complete set of evidence pieces that collectively provide full support to the claim. The problem becomes particularly challenging when there exists distinct sets of evidence that could be used to verify the claim from different perspectives. In this paper, we formally define and study the problem of identifying such minimal evidence groups (MEGs) for claim verification. We show that MEG identification can be reduced from Set Cover problem, based on entailment inference of whether a given evidence group provides full/partial support to a claim. Our proposed approach achieves 18.4% and 34.8% absolute improvements on the WiCE and SciFact datasets over LLM prompting. Finally, we demonstrate the benefits of MEGs in downstream applications such as claim generation.</li>
<li><strong>摘要：</strong>现实环境中的声明验证（例如，针对从网络检索到的大量候选证据）通常需要识别和聚合一组完整的证据，这些证据共同为声明提供全面支持。当存在可用于从不同角度验证主张的不同证据集时，问题变得尤其具有挑战性。在本文中，我们正式定义并研究了识别此类最小证据组（MEG）以进行声明验证的问题。我们证明，基于给定证据组是否为主张提供全部/部分支持的蕴涵推断，可以从 Set Cover 问题中减少 MEG 识别。与 LLM 提示相比，我们提出的方法在 WiCE 和 SciFact 数据集上实现了 18.4% 和 34.8% 的绝对改进。最后，我们展示了 MEG 在索赔生成等下游应用中的优势。</li>
</ul>

<h3>Title: Hybrid LLM/Rule-based Approaches to Business Insights Generation from  Structured Data</h3>
<ul>
<li><strong>Authors: </strong>Aliaksei Vertsel, Mikhail Rumiantsau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15604">https://arxiv.org/abs/2404.15604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15604">https://arxiv.org/pdf/2404.15604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15604]] Hybrid LLM/Rule-based Approaches to Business Insights Generation from  Structured Data(https://arxiv.org/abs/2404.15604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the field of business data analysis, the ability to extract actionable insights from vast and varied datasets is essential for informed decision-making and maintaining a competitive edge. Traditional rule-based systems, while reliable, often fall short when faced with the complexity and dynamism of modern business data. Conversely, Artificial Intelligence (AI) models, particularly Large Language Models (LLMs), offer significant potential in pattern recognition and predictive analytics but can lack the precision necessary for specific business applications. This paper explores the efficacy of hybrid approaches that integrate the robustness of rule-based systems with the adaptive power of LLMs in generating actionable business insights.</li>
<li><strong>摘要：</strong>在业务数据分析领域，从庞大且多样化的数据集中提取可行见解的能力对于做出明智的决策和保持竞争优势至关重要。传统的基于规则的系统虽然可靠，但在面对现代业务数据的复杂性和动态性时往往表现不佳。相反，人工智能 (AI) 模型，特别是大型语言模型 (LLM)，在模式识别和预测分析方面具有巨大潜力，但可能缺乏特定业务应用所需的精度。本文探讨了混合方法的功效，该方法将基于规则的系统的稳健性与法学硕士的自适应能力相结合，以生成可操作的业务见解。</li>
</ul>

<h3>Title: CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models  of Code</h3>
<ul>
<li><strong>Authors: </strong>Batu Guan, Yao Wan, Zhangqian Bi, Zheng Wang, Hongyu Zhang, Yulei Sui, Pan Zhou, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15639">https://arxiv.org/abs/2404.15639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15639">https://arxiv.org/pdf/2404.15639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15639]] CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models  of Code(https://arxiv.org/abs/2404.15639)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly used to automate code generation, it is often desired to know if the code is AI-generated and by which model, especially for purposes like protecting intellectual property (IP) in industry and preventing academic misconduct in education. Incorporating watermarks into machine-generated content is one way to provide code provenance, but existing solutions are restricted to a single bit or lack flexibility. We present CodeIP, a new watermarking technique for LLM-based code generation. CodeIP enables the insertion of multi-bit information while preserving the semantics of the generated code, improving the strength and diversity of the inerseted watermark. This is achieved by training a type predictor to predict the subsequent grammar type of the next token to enhance the syntactical and semantic correctness of the generated code. Experiments on a real-world dataset across five programming languages showcase the effectiveness of CodeIP.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 越来越多地用于自动化代码生成，人们通常希望知道代码是否是人工智能生成的以及由哪个模型生成的，特别是出于保护工业知识产权 (IP) 和防止学术不端行为等目的。教育。将水印合并到机器生成的内容中是提供代码来源的一种方法，但现有的解决方案仅限于单个位或缺乏灵活性。我们推出了 CodeIP，这是一种用于基于 LLM 的代码生成的新水印技术。 CodeIP 能够插入多位信息，同时保留生成代码的语义，从而提高插入水印的强度和多样性。这是通过训练类型预测器来预测下一个标记的后续语法类型以增强生成代码的语法和语义正确性来实现的。对五种编程语言的真实数据集进行的实验展示了 CodeIP 的有效性。</li>
</ul>

<h3>Title: Return of EM: Entity-driven Answer Set Expansion for QA Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Dongryeol Lee, Minwoo Lee, Kyungmin Min, Joonsuk Park, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15650">https://arxiv.org/abs/2404.15650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15650">https://arxiv.org/pdf/2404.15650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15650]] Return of EM: Entity-driven Answer Set Expansion for QA Evaluation(https://arxiv.org/abs/2404.15650)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, directly using large language models (LLMs) has been shown to be the most reliable method to evaluate QA models. However, it suffers from limited interpretability, high cost, and environmental harm. To address these, we propose to use soft EM with entity-driven answer set expansion. Our approach expands the gold answer set to include diverse surface forms, based on the observation that the surface forms often follow particular patterns depending on the entity type. The experimental results show that our method outperforms traditional evaluation methods by a large margin. Moreover, the reliability of our evaluation method is comparable to that of LLM-based ones, while offering the benefits of high interpretability and reduced environmental harm.</li>
<li><strong>摘要：</strong>最近，直接使用大型语言模型（LLM）已被证明是评估 QA 模型最可靠的方法。然而，它的可解释性有限、成本高和环境危害。为了解决这些问题，我们建议使用软 EM 和实体驱动的答案集扩展。基于表面形式通常根据实体类型遵循特定模式的观察，我们的方法扩展了黄金答案集以包括不同的表面形式。实验结果表明，我们的方法大大优于传统的评估方法。此外，我们的评估方法的可靠性与基于法学硕士的评估方法相当，同时具有高可解释性和减少环境危害的优点。</li>
</ul>

<h3>Title: KS-LLM: Knowledge Selection of Large Language Models with Evidence  Document for Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Xinxin Zheng, Feihu Che, Jinyang Wu, Shuai Zhang, Shuai Nie, Kang Liu, Jianhua Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15660">https://arxiv.org/abs/2404.15660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15660">https://arxiv.org/pdf/2404.15660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15660]] KS-LLM: Knowledge Selection of Large Language Models with Evidence  Document for Question Answering(https://arxiv.org/abs/2404.15660)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）存在幻觉问题，并且在应用于知识密集型任务时面临重大挑战。一种有前景的方法是利用证据文档作为额外的支持知识，这些知识可以通过检索或生成来获得。然而，现有的方法直接利用证据文档的全部内容，这可能会引入噪声信息并损害大型语言模型的性能。为了解决这个问题，我们提出了一种新颖的大型语言模型知识选择（KS-LLM）方法，旨在从证据文档中识别有价值的信息。 KS-LLM方法利用三元组从证据文档中有效地选择有利于回答问题的知识片段。具体来说，我们首先根据输入问题生成三元组，然后从证据文档中选择与三元组最相似的证据句子，最后将证据句子和三元组结合起来辅助大型语言模型生成答案。在 TriviaQA、WebQ 和 NQ 等多个问答数据集上的实验比较表明，所提出的方法超越了基线并取得了最佳结果。</li>
</ul>

<h3>Title: The Promise and Challenges of Using LLMs to Accelerate the Screening  Process of Systematic Reviews</h3>
<ul>
<li><strong>Authors: </strong>Aleksi Huotala, Miikka Kuutila, Paul Ralph, Mika Mäntylä</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15667">https://arxiv.org/abs/2404.15667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15667">https://arxiv.org/pdf/2404.15667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15667]] The Promise and Challenges of Using LLMs to Accelerate the Screening  Process of Systematic Reviews(https://arxiv.org/abs/2404.15667)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Systematic review (SR) is a popular research method in software engineering (SE). However, conducting an SR takes an average of 67 weeks. Thus, automating any step of the SR process could reduce the effort associated with SRs. Our objective is to investigate if Large Language Models (LLMs) can accelerate title-abstract screening by simplifying abstracts for human screeners, and automating title-abstract screening. We performed an experiment where humans screened titles and abstracts for 20 papers with both original and simplified abstracts from a prior SR. The experiment with human screeners was reproduced with GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also studied if different prompting techniques (Zero-shot (ZS), One-shot (OS), Few-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the screening performance of LLMs. Lastly, we studied if redesigning the prompt used in the LLM reproduction of screening leads to improved performance. Text simplification did not increase the screeners' screening performance, but reduced the time used in screening. Screeners' scientific literacy skills and researcher status predict screening performance. Some LLM and prompt combinations perform as well as human screeners in the screening tasks. Our results indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5. Additionally, Few-shot and One-shot prompting outperforms Zero-shot prompting. Using LLMs for text simplification in the screening process does not significantly improve human performance. Using LLMs to automate title-abstract screening seems promising, but current LLMs are not significantly more accurate than human screeners. To recommend the use of LLMs in the screening process of SRs, more research is needed. We recommend future SR studies publish replication packages with screening data to enable more conclusive experimenting with LLM screening.</li>
<li><strong>摘要：</strong>系统评审（SR）是软件工程（SE）中流行的研究方法。然而，进行 SR 平均需要 67 周。因此，自动化 SR 流程的任何步骤都可以减少与 SR 相关的工作量。我们的目标是调查大型语言模型 (LLM) 是否可以通过简化人工筛选人员的摘要和自动化标题摘要筛选来加速标题摘要筛选。我们进行了一项实验，人类筛选了 20 篇论文的标题和摘要，其中包括来自先前 SR 的原始摘要和简化摘要。使用 GPT-3.5 和 GPT-4 LLM 重现了人类筛选者的实验，以执行相同的筛选任务。我们还研究了不同的提示技术（零样本（ZS）、单样本（OS）、少样本（FS）和带思维链的少样本（FS-CoT））是否可以提高筛选性能法学硕士。最后，我们研究了重新设计 LLM 筛选再现中使用的提示是否会提高性能。文本简化并没有提高安检人员的安检性能，反而减少了安检所用的时间。筛查人员的科学素养技能和研究人员地位可预测筛查表现。一些法学硕士和提示组合在筛选任务中的表现与人工筛选员一样好。我们的结果表明 GPT-4 LLM 优于其前身 GPT-3.5。此外，少样本和单样本提示的效果优于零样本提示。在筛选过程中使用法学硕士进行文本简化并不能显着提高人类的表现。使用法学硕士来自动化标题摘要筛选似乎很有前途，但目前的法学硕士并不比人类筛选人员更准确。为了推荐在 SR 筛选过程中使用法学硕士，需要进行更多研究。我们建议未来的 SR 研究发布包含筛选数据的复制包，以便对 LLM 筛选进行更结论性的实验。</li>
</ul>

<h3>Title: Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yu Xia, Rui Wang, Xu Liu, Mingyan Li, Tong Yu, Xiang Chen, Julian McAuley, Shuai Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15676">https://arxiv.org/abs/2404.15676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15676">https://arxiv.org/pdf/2404.15676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15676]] Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs(https://arxiv.org/abs/2404.15676)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) has been a widely adopted prompting method, eliciting impressive reasoning abilities of Large Language Models (LLMs). Inspired by the sequential thought structure of CoT, a number of Chain-of-X (CoX) methods have been developed to address various challenges across diverse domains and tasks involving LLMs. In this paper, we provide a comprehensive survey of Chain-of-X methods for LLMs in different contexts. Specifically, we categorize them by taxonomies of nodes, i.e., the X in CoX, and application tasks. We also discuss the findings and implications of existing CoX methods, as well as potential future directions. Our survey aims to serve as a detailed and up-to-date resource for researchers seeking to apply the idea of CoT to broader scenarios.</li>
<li><strong>摘要：</strong>思想链（CoT）是一种广泛采用的提示方法，引发了大型语言模型（LLM）令人印象深刻的推理能力。受 CoT 顺序思维结构的启发，开发了许多 Chain-of-X (CoX) 方法来解决涉及法学硕士的不同领域和任务的各种挑战。在本文中，我们对不同背景下法学硕士的 Chain-of-X 方法进行了全面的调查。具体来说，我们通过节点分类法（即 CoX 中的 X）和应用程序任务对它们进行分类。我们还讨论了现有 CoX 方法的发现和影响，以及未来潜在的方向。我们的调查旨在为寻求将 CoT 理念应用到更广泛场景的研究人员提供详细且最新的资源。</li>
</ul>

<h3>Title: Nyonic Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Junfeng Tian, Rui Wang, Cong Li, Yudong Zhou, Jun Liu, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15702">https://arxiv.org/abs/2404.15702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15702">https://arxiv.org/pdf/2404.15702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15702]] Nyonic Technical Report(https://arxiv.org/abs/2404.15702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This report details the development and key achievements of our latest language model designed for custom large language models. The advancements introduced include a novel Online Data Scheduler that supports flexible training data adjustments and curriculum learning. The model's architecture is fortified with state-of-the-art techniques such as Rotary Positional Embeddings, QK-LayerNorm, and a specially crafted multilingual tokenizer to enhance stability and performance. Moreover, our robust training framework incorporates advanced monitoring and rapid recovery features to ensure optimal efficiency. Our Wonton 7B model has demonstrated competitive performance on a range of multilingual and English benchmarks. Future developments will prioritize narrowing the performance gap with more extensively trained models, thereby enhancing the model's real-world efficacy and adaptability.GitHub: \url{https://github.com/nyonicai/nyonic-public}</li>
<li><strong>摘要：</strong>本报告详细介绍了我们为定制大语言模型设计的最新语言模型的开发和主要成就。引入的进步包括一个新颖的在线数据调度程序，支持灵活的训练数据调整和课程学习。该模型的架构采用了最先进的技术，例如旋转位置嵌入、QK-LayerNorm 和专门设计的多语言分词器，以增强稳定性和性能。此外，我们强大的培训框架结合了先进的监控和快速恢复功能，以确保最佳效率。我们的 Wonton 7B 模型在一系列多语言和英语基准测试中表现出了具有竞争力的性能。未来的发展将优先考虑通过更广泛训练的模型来缩小性能差距，从而增强模型的现实世界功效和适应性。GitHub：\url{https://github.com/nyonicai/nyonic-public}</li>
</ul>

<h3>Title: No Train but Gain: Language Arithmetic for training-free Language  Adapters enhancement</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15737">https://arxiv.org/abs/2404.15737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15737">https://arxiv.org/pdf/2404.15737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15737]] No Train but Gain: Language Arithmetic for training-free Language  Adapters enhancement(https://arxiv.org/abs/2404.15737)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Modular deep learning is the state-of-the-art solution for lifting the curse of multilinguality, preventing the impact of negative interference and enabling cross-lingual performance in Multilingual Pre-trained Language Models. However, a trade-off of this approach is the reduction in positive transfer learning from closely related languages. In response, we introduce a novel method called language arithmetic, which enables training-free post-processing to address this limitation. Inspired by the task arithmetic framework, we apply learning via addition to the language adapters, transitioning the framework from a multi-task to a multilingual setup. The effectiveness of the proposed solution is demonstrated on three downstream tasks in a MAD-X-based set of cross-lingual schemes, acting as a post-processing procedure. Language arithmetic consistently improves the baselines with significant gains in the most challenging cases of zero-shot and low-resource applications. Our code and models are available at https://github.com/mklimasz/language-arithmetic .</li>
<li><strong>摘要：</strong>模块化深度学习是解除多语言诅咒、防止负面干扰的影响并在多语言预训练语言模型中实现跨语言性能的最先进的解决方案。然而，这种方法的一个缺点是减少了密切相关语言的正迁移学习。为此，我们引入了一种称为语言算术的新方法，它可以通过免训练的后处理来解决这一限制。受任务算术框架的启发，我们通过添加语言适配器来应用学习，将框架从多任务转换为多语言设置。所提出的解决方案的有效性在基于 MAD-X 的一组跨语言方案中的三个下游任务上得到了证明，充当后处理程序。语言算法持续改进基线，在零样本和低资源应用程序最具挑战性的情况下取得显着收益。我们的代码和模型可在 https://github.com/mklimasz/language-arithmetic 获取。</li>
</ul>

<h3>Title: Let's Think Dot by Dot: Hidden Computation in Transformer Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Jacob Pfau, William Merrill, Samuel R. Bowman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15758">https://arxiv.org/abs/2404.15758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15758">https://arxiv.org/pdf/2404.15758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15758]] Let's Think Dot by Dot: Hidden Computation in Transformer Language  Models(https://arxiv.org/abs/2404.15758)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.</li>
<li><strong>摘要：</strong>语言模型的思路链响应在大多数基准测试中提高了性能。然而，目前尚不清楚这些性能提升在多大程度上可以归因于类似人类的任务分解，或者仅仅是额外标记允许的更大计算量。我们表明，Transformer 可以使用无意义的填充标记（例如“......”）代替思路链来解决两个难以解决的算法任务，而这些任务在没有中间标记的情况下是无法解决的。然而，我们通过经验发现，学习使用填充标记很困难，需要特定的、密集的监督才能收敛。我们还对填充标记在一阶公式的量词深度方面有用的问题类型进行了理论描述。对于满足此特征的问题，思路链标记不需要提供有关多标记计算中涉及的中间计算步骤的信息。总之，我们的结果表明，额外的标记可以提供与标记选择无关的计算优势。中间标记可以充当填充标记这一事实引发了人们对大型语言模型的担忧，这些模型涉及不可审计的隐藏计算，而这些计算与观察到的思路链标记越来越脱节。</li>
</ul>

<h3>Title: A Comprehensive Survey on Evaluating Large Language Model Applications  in the Medical Industry</h3>
<ul>
<li><strong>Authors: </strong>Yining Huang, Keke Tang, Meilian Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15777">https://arxiv.org/abs/2404.15777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15777">https://arxiv.org/pdf/2404.15777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15777]] A Comprehensive Survey on Evaluating Large Language Model Applications  in the Medical Industry(https://arxiv.org/abs/2404.15777)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Since the inception of the Transformer architecture in 2017, Large Language Models (LLMs) such as GPT and BERT have evolved significantly, impacting various industries with their advanced capabilities in language understanding and generation. These models have shown potential to transform the medical field, highlighting the necessity for specialized evaluation frameworks to ensure their effective and ethical deployment. This comprehensive survey delineates the extensive application and requisite evaluation of LLMs within healthcare, emphasizing the critical need for empirical validation to fully exploit their capabilities in enhancing healthcare outcomes. Our survey is structured to provide an in-depth analysis of LLM applications across clinical settings, medical text data processing, research, education, and public health awareness. We begin by exploring the roles of LLMs in different medical applications, detailing how they are evaluated based on their performance in tasks such as clinical application, medical text data processing, information retrieval, data analysis, medical scientific writing, educational content generation etc. The subsequent sections delve into the methodologies employed in these evaluations, discussing the benchmarks and metrics used to assess the models' effectiveness, accuracy, and ethical alignment. Through this survey, we aim to equip healthcare professionals, researchers, and policymakers with a comprehensive understanding of the potential strengths and limitations of LLMs in medical applications. By providing detailed insights into the evaluation processes and the challenges faced in integrating LLMs into healthcare, this survey seeks to guide the responsible development and deployment of these powerful models, ensuring they are harnessed to their full potential while maintaining stringent ethical standards.</li>
<li><strong>摘要：</strong>自 2017 年 Transformer 架构诞生以来，GPT 和 BERT 等大型语言模型 (LLM) 取得了显着发展，以其先进的语言理解和生成能力影响着各个行业。这些模型显示出改变医学领域的潜力，凸显了专门评估框架的必要性，以确保其有效和道德的部署。这项全面的调查描绘了法学硕士在医疗保健领域的广泛应用和必要的评估，强调了对实证验证的迫切需要，以充分利用其提高医疗保健结果的能力。我们的调查旨在对临床环境、医学文本数据处理、研究、教育和公共卫生意识方面的法学硕士应用进行深入分析。我们首先探讨法学硕士在不同医学应用中的作用，详细介绍如何根据其在临床应用、医学文本数据处理、信息检索、数据分析、医学科学写作、教育内容生成等任务中的表现来评估他们。随后的部分深入研究了这些评估中使用的方法，讨论了用于评估模型的有效性、准确性和道德一致性的基准和指标。通过这项调查，我们的目标是让医疗保健专业人员、研究人员和政策制定者全面了解法学硕士在医疗应用中的潜在优势和局限性。通过提供对评估流程和将法学硕士融入医疗保健所面临的挑战的详细见解，本调查旨在指导这些强大模型的负责任的开发和部署，确保它们充分发挥潜力，同时保持严格的道德标准。</li>
</ul>

<h3>Title: Exploring LLM Prompting Strategies for Joint Essay Scoring and Feedback  Generation</h3>
<ul>
<li><strong>Authors: </strong>Maja Stahl, Leon Biermann, Andreas Nehring, Henning Wachsmuth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15845">https://arxiv.org/abs/2404.15845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15845">https://arxiv.org/pdf/2404.15845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15845]] Exploring LLM Prompting Strategies for Joint Essay Scoring and Feedback  Generation(https://arxiv.org/abs/2404.15845)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Individual feedback can help students improve their essay writing skills. However, the manual effort required to provide such feedback limits individualization in practice. Automatically-generated essay feedback may serve as an alternative to guide students at their own pace, convenience, and desired frequency. Large language models (LLMs) have demonstrated strong performance in generating coherent and contextually relevant text. Yet, their ability to provide helpful essay feedback is unclear. This work explores several prompting strategies for LLM-based zero-shot and few-shot generation of essay feedback. Inspired by Chain-of-Thought prompting, we study how and to what extent automated essay scoring (AES) can benefit the quality of generated feedback. We evaluate both the AES performance that LLMs can achieve with prompting only and the helpfulness of the generated essay feedback. Our results suggest that tackling AES and feedback generation jointly improves AES performance. However, while our manual evaluation emphasizes the quality of the generated essay feedback, the impact of essay scoring on the generated feedback remains low ultimately.</li>
<li><strong>摘要：</strong>个人反馈可以帮助学生提高论文写作技巧。然而，提供此类反馈所需的手动工作限制了实践中的个性化。自动生成的论文反馈可以作为一种替代方案，指导学生按照自己的节奏、便利性和所需的频率进行指导。大型语言模型 (LLM) 在生成连贯且上下文相关的文本方面表现出了强大的性能。然而，他们提供有用的论文反馈的能力尚不清楚。这项工作探索了基于法学硕士的零样本和少样本论文反馈的几种提示策略。受思想链提示的启发，我们研究了自动论文评分 (AES) 如何以及在多大程度上可以提高生成反馈的质量。我们评估了法学硕士仅通过提示即可实现的 AES 表现以及生成的论文反馈的有用性。我们的结果表明，处理 AES 和反馈生成共同提高了 AES 性能。然而，虽然我们的手动评估强调生成的论文反馈的质量，但论文评分对生成的反馈的影响最终仍然很低。</li>
</ul>

<h3>Title: From Complex to Simple: Enhancing Multi-Constraint Complex Instruction  Following Ability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15846">https://arxiv.org/abs/2404.15846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15846">https://arxiv.org/pdf/2404.15846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15846]] From Complex to Simple: Enhancing Multi-Constraint Complex Instruction  Following Ability of Large Language Models(https://arxiv.org/abs/2404.15846)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (i.e. Complex Instructions Following). Yet, it remains under-explored how to enhance the ability of LLMs to follow complex instructions with multiple constraints. To bridge the gap, we initially study what training data is effective in enhancing complex constraints following abilities. We found that training LLMs with instructions containing multiple constraints enhances their understanding of complex instructions, especially those with lower complexity levels. The improvement can even generalize to compositions of out-of-domain constraints. Additionally, we further propose methods addressing how to obtain and utilize the effective training data. Finally, we conduct extensive experiments to prove the effectiveness of our methods in terms of overall performance, training efficiency, and generalization abilities under four settings.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 必须遵循具有详细要求的指令（即遵循复杂指令）。然而，如何增强法学硕士遵循具有多重约束的复杂指令的能力仍有待探索。为了弥补这一差距，我们首先研究哪些训练数据可以有效增强复杂约束下的能力。我们发现，使用包含多个约束的指令训练法学硕士可以增强他们对复杂指令的理解，尤其是那些复杂程度较低的指令。这种改进甚至可以推广到域外约束的组合。此外，我们进一步提出了解决如何获取和利用有效训练数据的方法。最后，我们进行了大量的实验，以证明我们的方法在四种设置下的整体性能、训练效率和泛化能力方面的有效性。</li>
</ul>

<h3>Title: Detecting Conceptual Abstraction in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Michaela Regneri, Alhassan Abdelhalim, Sören Laue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15848">https://arxiv.org/abs/2404.15848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15848">https://arxiv.org/pdf/2404.15848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15848]] Detecting Conceptual Abstraction in LLMs(https://arxiv.org/abs/2404.15848)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present a novel approach to detecting noun abstraction within a large language model (LLM). Starting from a psychologically motivated set of noun pairs in taxonomic relationships, we instantiate surface patterns indicating hypernymy and analyze the attention matrices produced by BERT. We compare the results to two sets of counterfactuals and show that we can detect hypernymy in the abstraction mechanism, which cannot solely be related to the distributional similarity of noun pairs. Our findings are a first step towards the explainability of conceptual abstraction in LLMs.</li>
<li><strong>摘要：</strong>我们提出了一种在大型语言模型（LLM）中检测名词抽象的新颖方法。从分类关系中一组出于心理动机的名词对开始，我们实例化表明上位词的表面模式，并分析 BERT 生成的注意力矩阵。我们将结果与两组反事实进行比较，并表明我们可以在抽象机制中检测上位词，这不能仅仅与名词对的分布相似性相关。我们的研究结果是迈向法学硕士概念抽象可解释性的第一步。</li>
</ul>

<h3>Title: Assessing The Potential Of Mid-Sized Language Models For Clinical QA</h3>
<ul>
<li><strong>Authors: </strong>Elliot Bolton, Betty Xiong, Vijaytha Muralidharan, Joel Schamroth, Vivek Muralidharan, Christopher D. Manning, Roxana Daneshjou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15894">https://arxiv.org/abs/2404.15894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15894">https://arxiv.org/pdf/2404.15894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15894]] Assessing The Potential Of Mid-Sized Language Models For Clinical QA(https://arxiv.org/abs/2404.15894)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Large language models, such as GPT-4 and Med-PaLM, have shown impressive performance on clinical tasks; however, they require access to compute, are closed-source, and cannot be deployed on device. Mid-size models such as BioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B avoid these drawbacks, but their capacity for clinical tasks has been understudied. To help assess their potential for clinical use and help researchers decide which model they should use, we compare their performance on two clinical question-answering (QA) tasks: MedQA and consumer query answering. We find that Mistral 7B is the best performing model, winning on all benchmarks and outperforming models trained specifically for the biomedical domain. While Mistral 7B's MedQA score of 63.0% approaches the original Med-PaLM, and it often can produce plausible responses to consumer health queries, room for improvement still exists. This study provides the first head-to-head assessment of open source mid-sized models on clinical tasks.</li>
<li><strong>摘要：</strong>大型语言模型，如 GPT-4 和 Med-PaLM，在临床任务上表现出了令人印象深刻的表现；然而，它们需要访问计算，是闭源的，并且不能部署在设备上。 BioGPT-large、BioMedLM、LLaMA 2 和 Mistral 7B 等中型模型避免了这些缺点，但它们执行临床任务的能力尚未得到充分研究。为了帮助评估它们的临床使用潜力并帮助研究人员决定应该使用哪种模型，我们比较了它们在两项临床问答 (QA) 任务上的表现：MedQA 和消费者查询回答。我们发现 Mistral 7B 是性能最佳的模型，在所有基准测试中获胜，并且优于专门针对生物医学领域训练的模型。虽然 Mistral 7B 的 MedQA 分数为 63.0%，接近原始的 Med-PaLM，并且它通常可以对消费者的健康查询产生合理的响应，但仍然存在改进的空间。这项研究首次对临床任务中的开源中型模型进行了头对头评估。</li>
</ul>

<h3>Title: Generalization Measures for Zero-Shot Cross-Lingual Transfer</h3>
<ul>
<li><strong>Authors: </strong>Saksham Bassi, Duygu Ataman, Kyunghyun Cho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15928">https://arxiv.org/abs/2404.15928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15928">https://arxiv.org/pdf/2404.15928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15928]] Generalization Measures for Zero-Shot Cross-Lingual Transfer(https://arxiv.org/abs/2404.15928)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A model's capacity to generalize its knowledge to interpret unseen inputs with different characteristics is crucial to build robust and reliable machine learning systems. Language model evaluation tasks lack information metrics about model generalization and their applicability in a new setting is measured using task and language-specific downstream performance, which is often lacking in many languages and tasks. In this paper, we explore a set of efficient and reliable measures that could aid in computing more information related to the generalization capability of language models in cross-lingual zero-shot settings. In addition to traditional measures such as variance in parameters after training and distance from initialization, we also measure the effectiveness of sharpness in loss landscape in capturing the success in cross-lingual transfer and propose a novel and stable algorithm to reliably compute the sharpness of a model optimum that correlates to generalization.</li>
<li><strong>摘要：</strong>模型概括其知识以解释具有不同特征的未见输入的能力对于构建强大且可靠的机器学习系统至关重要。语言模型评估任务缺乏有关模型泛化的信息度量，并且它们在新环境中的适用性是使用任务和特定于语言的下游性能来衡量的，而这在许多语言和任务中通常是缺乏的。在本文中，我们探索了一组有效且可靠的措施，可以帮助计算更多与跨语言零样本设置中语言模型的泛化能力相关的信息。除了训练后参数的方差和初始化距离等传统测量方法之外，我们还测量了损失景观中锐度在捕获跨语言迁移成功方面的有效性，并提出了一种新颖且稳定的算法来可靠地计算损失景观的锐度与泛化相关的模型优化。</li>
</ul>

<h3>Title: Sequence can Secretly Tell You What to Discard</h3>
<ul>
<li><strong>Authors: </strong>Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15949">https://arxiv.org/abs/2404.15949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15949">https://arxiv.org/pdf/2404.15949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15949]] Sequence can Secretly Tell You What to Discard(https://arxiv.org/abs/2404.15949)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite their impressive performance on a wide range of tasks, require significant GPU memory and consume substantial computational resources. In addition to model weights, the memory occupied by KV cache increases linearly with sequence length, becoming a main bottleneck for inference. In this paper, we introduce a novel approach for optimizing the KV cache which significantly reduces its memory footprint. Through a comprehensive investigation, we find that on LLaMA2 series models, (i) the similarity between adjacent tokens' query vectors is remarkably high, and (ii) current query's attention calculation can rely solely on the attention information of a small portion of the preceding queries. Based on these observations, we propose CORM, a KV cache eviction policy that dynamically retains important key-value pairs for inference without finetuning the model. We validate that CORM reduces the inference memory usage of KV cache by up to 70% without noticeable performance degradation across six tasks in LongBench.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 尽管在各种任务上具有令人印象深刻的性能，但需要大量 GPU 内存并消耗大量计算资源。除了模型权重之外，KV缓存占用的内存随着序列长度线性增加，成为推理的主要瓶颈。在本文中，我们介绍了一种优化 KV 缓存的新方法，该方法可显着减少其内存占用。通过全面的调查，我们发现在LLaMA2系列模型上，（i）相邻token的查询向量之间的相似度非常高，并且（ii）当前查询的注意力计算可以仅依赖于先前查询的一小部分的注意力信息查询。基于这些观察，我们提出了 CORM，一种 KV 缓存驱逐策略，可以动态保留重要的键值对进行推理，而无需微调模型。我们验证了 CORM 将 KV 缓存的推理内存使用量减少了高达 70%，而 LongBench 中的六个任务的性能没有明显下降。</li>
</ul>

<h3>Title: The PRISM Alignment Project: What Participatory, Representative and  Individualised Human Feedback Reveals About the Subjective and Multicultural  Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, Scott A. Hale</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16019">https://arxiv.org/abs/2404.16019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16019">https://arxiv.org/pdf/2404.16019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16019]] The PRISM Alignment Project: What Participatory, Representative and  Individualised Human Feedback Reveals About the Subjective and Multicultural  Alignment of Large Language Models(https://arxiv.org/abs/2404.16019)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.</li>
<li><strong>摘要：</strong>人类反馈在大型语言模型 (LLM) 的协调中发挥着核心作用。然而，关于人类反馈收集的方法（如何）、领域（在哪里）、人员（谁）和目标（达到什么目的）仍然存在悬而未决的问题。为了解决这些问题，我们引入了 PRISM，这是一个新的数据集，它将来自 75 个国家的 1,500 名不同参与者的社会人口统计数据和陈述的偏好映射到他们与 21 名法学硕士进行的 8,011 次现场对话中的情境偏好和细粒度反馈。 PRISM 有助于 (i) 广泛的地理和人口参与人类反馈数据； (ii) 用于了解集体福利的两个人口普查代表性样本（英国和美国）； (iii) 个性化反馈，其中每个评级都与详细的参与者资料相关联，从而允许探索样本制品的个性化和归属。我们专注于收集以主观和多元文化观点为中心的对话，这些对话涉及充满价值和有争议的话题，我们预计这些话题中存在最多的人际和跨文化分歧。我们通过对话多样性、偏好多样性和福利结果的三个案例研究证明了 PRISM 的有用性，表明人类设定联盟规范很重要。除了提供丰富的社区资源外，我们还倡导更广泛地参与人工智能开发，并采用更具包容性的技术设计方法。</li>
</ul>

<h3>Title: Universal Adversarial Triggers Are Not Universal</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Meade, Arkil Patel, Siva Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16020">https://arxiv.org/abs/2404.16020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16020">https://arxiv.org/pdf/2404.16020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16020]] Universal Adversarial Triggers Are Not Universal(https://arxiv.org/abs/2404.16020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent work has developed optimization procedures to find token sequences, called adversarial triggers, which can elicit unsafe responses from aligned language models. These triggers are believed to be universally transferable, i.e., a trigger optimized on one model can jailbreak other models. In this paper, we concretely show that such adversarial triggers are not universal. We extensively investigate trigger transfer amongst 13 open models and observe inconsistent transfer. Our experiments further reveal a significant difference in robustness to adversarial triggers between models Aligned by Preference Optimization (APO) and models Aligned by Fine-Tuning (AFT). We find that APO models are extremely hard to jailbreak even when the trigger is optimized directly on the model. On the other hand, while AFT models may appear safe on the surface, exhibiting refusals to a range of unsafe instructions, we show that they are highly susceptible to adversarial triggers. Lastly, we observe that most triggers optimized on AFT models also generalize to new unsafe instructions from five diverse domains, further emphasizing their vulnerability. Overall, our work highlights the need for more comprehensive safety evaluations for aligned language models.</li>
<li><strong>摘要：</strong>最近的工作开发了优化程序来查找标记序列，称为对抗性触发器，它可以从对齐的语言模型中引发不安全的响应。这些触发器被认为是普遍可转移的，即在一种模型上优化的触发器可以越狱其他模型。在本文中，我们具体表明这种对抗性触发因素并不普遍。我们广泛研究了 13 个开放模型之间的触发转移，并观察到不一致的转移。我们的实验进一步揭示了偏好优化对齐模型（APO）和微调对齐模型（AFT）在对抗性触发的鲁棒性方面存在显着差异。我们发现，即使直接在模型上优化触发器，APO 模型也极难越狱。另一方面，虽然 AFT 模型表面上可能看起来很安全，表现出对一系列不安全指令的拒绝，但我们表明它们非常容易受到对抗性触发的影响。最后，我们观察到大多数在 AFT 模型上优化的触发器也泛化到来自五个不同领域的新的不安全指令，进一步强调了它们的漏洞。总的来说，我们的工作强调需要对一致的语言模型进行更全面的安全评估。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
