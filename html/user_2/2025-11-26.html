<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-26</h1>
<h3>Title: Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search</h3>
<ul>
<li><strong>Authors: </strong>Manil Shrestha, Edward Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19648">https://arxiv.org/abs/2511.19648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19648">https://arxiv.org/pdf/2511.19648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19648]] Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search(https://arxiv.org/abs/2511.19648)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.</li>
<li><strong>摘要：</strong>由于可能的推理路径的组合爆炸，知识图上的多跳问答在计算上仍然具有挑战性。最近的方法依赖于昂贵的大型语言模型（LLM）推理来进行实体链接和路径排名，限制了它们的实际部署。此外，法学硕士生成的答案通常缺乏可验证的结构化知识基础。我们提出了两种解决效率和可验证性的互补混合算法：(1) LLM 引导规划，使用单个 LLM 调用来预测通过广度优先搜索执行的关系序列，实现近乎完美的准确性 (micro-F1 > 0.90)，同时确保所有答案都基于知识图谱；(2) 嵌入引导神经搜索，通过轻量级融合文本和图形嵌入来完全消除 LLM 调用6.7M 参数边缘评分器，实现超过 100 倍的加速和具有竞争力的精度。通过知识蒸馏，我们将规划能力压缩为 4B 参数模型，以零 API 成本匹配大型模型性能。对 MetaQA 的评估表明，扎根推理始终优于无扎根生成，结构化规划比直接答案生成更具可转移性。我们的结果表明，可验证的多跳推理在推理时不需要大量模型，而是需要将符号结构与学习表示相结合的正确的架构归纳偏差。</li>
</ul>

<h3>Title: Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian</h3>
<ul>
<li><strong>Authors: </strong>Mobina Mehrazar, Mohammad Amin Yousefi, Parisa Abolfath Beygi, Behnam Bahrak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19719">https://arxiv.org/abs/2511.19719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19719">https://arxiv.org/pdf/2511.19719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19719]] Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian(https://arxiv.org/abs/2511.19719)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于在预测的同时生成自我解释，这种做法引发了人们对这些解释的准确性的担忧，尤其是在资源匮乏的语言中。本研究通过将模型识别的有影响力的单词与人类注释者识别的单词进行比较，评估了法学硕士在波斯语（一种低资源语言）情感分类背景下生成的解释的可信度。我们使用从令牌级对数概率得出的置信度分数来评估忠实度。测试了解释和预测顺序不同的两种提示策略（预测然后解释和解释然后预测）对解释忠实度的影响。我们的结果表明，虽然法学硕士取得了很强的分类性能，但它们生成的解释往往与忠实的推理相悖，彼此之间的一致性比人类判断的一致性更高。这些结果凸显了当前解释方法和指标的局限性，强调需要更稳健的方法来确保法学硕士在多语言和资源匮乏的情况下的可靠性。</li>
</ul>

<h3>Title: Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation</h3>
<ul>
<li><strong>Authors: </strong>Richard J. Young, Alice M. Matthews</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19739">https://arxiv.org/abs/2511.19739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19739">https://arxiv.org/pdf/2511.19739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19739]] Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation(https://arxiv.org/abs/2511.19739)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.</li>
<li><strong>摘要：</strong>特定领域的文本嵌入对于临床自然语言处理至关重要，但跨模型架构的系统比较仍然有限。本研究通过对源自权威医学教科书的 106,535 个心脏病学文本对进行低秩适应 (LoRA) 微调，评估了 10 个适用于心脏病学的基于 Transformer 的嵌入模型。结果表明，与较大的基于解码器的模型相比，仅编码器架构（尤其是 BioLinkBERT）实现了卓越的特定领域性能（分离分数：0.510），同时需要的计算资源显着减少。这些发现挑战了这样的假设：更大的语言模型必然会产生更好的特定领域嵌入，并为临床 NLP 系统开发提供实用指导。所有模型、训练代码和评估数据集都是公开的，以支持医学信息学的可重复研究。</li>
</ul>

<h3>Title: Gender Bias in Emotion Recognition by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Maureen Herbert, Katie Sun, Angelica Lim, Yasaman Etesam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19785">https://arxiv.org/abs/2511.19785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19785">https://arxiv.org/pdf/2511.19785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19785]] Gender Bias in Emotion Recognition by Large Language Models(https://arxiv.org/abs/2511.19785)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, "How does this person feel?". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展及其与日常生活的日益融合凸显了评估和确保其公平性的重要性。在这项工作中，我们研究了情感心理理论领域内的公平性，调查法学硕士在描述一个人及其环境并询问“这个人感觉如何？”时是否表现出性别偏见。此外，我们提出并评估了几种去偏差策略，证明实现有意义的偏差减少需要基于训练的干预措施，而不是仅仅依赖基于推理时间提示的方法，例如提示工程。</li>
</ul>

<h3>Title: Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shi-Wei Dai, Yan-Wei Shie, Tsung-Huan Yang, Lun-Wei Ku, Yung-Hui Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19852">https://arxiv.org/abs/2511.19852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19852">https://arxiv.org/pdf/2511.19852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19852]] Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs(https://arxiv.org/abs/2511.19852)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.</li>
<li><strong>摘要：</strong>个性化大型语言模型 (LLM) 已被证明是创建更具吸引力和愉快的用户人工智能交互的有效方法。虽然之前的研究已经探索过使用提示来引出法学硕士的特定人格特质，但他们并没有优化这些提示以最大化个性表达。为了解决这一限制，我们提出了PersonaPulse：法学硕士现实人格表达的动态档案优化，该框架利用法学硕士对人格特质的固有知识来迭代增强角色扮演提示，同时将情境反应基准作为评分工具，确保更现实和基于情境的评估来指导优化过程。定量评估表明，PersonaPulse 生成的提示优于之前根据心理学研究的个性描述设计的提示。此外，我们通过大量的实验探索模型尺寸和个性建模之间的关系。最后，我们发现，对于某些人格特质，可以通过暂停优化过程来部分控制人格唤起的程度。这些发现强调了及时优化在法学硕士塑造个性表达方面的重要性，为未来自适应人工智能交互的研究提供了宝贵的见解。</li>
</ul>

<h3>Title: A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction</h3>
<ul>
<li><strong>Authors: </strong>Farzad Ahmed, Joniel Augustine Jerome, Meliha Yetisgen, Özlem Uzuner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19858">https://arxiv.org/abs/2511.19858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19858">https://arxiv.org/pdf/2511.19858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19858]] A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction(https://arxiv.org/abs/2511.19858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction. Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning. Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections. Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.</li>
<li><strong>摘要：</strong>目的：临床文件包含可能危及患者安全的事实、诊断和管理错误。大型语言模型（LLM）可能有助于检测和纠正此类错误，但它们在不同提示策略下的行为仍不清楚。我们针对医疗错误处理的三个子任务：错误标志检测、错误句子检测和错误纠正，评估零样本提示、随机样本静态提示（SPR）和检索增强动态提示（RDP）。方法：使用 MEDEC 数据集，我们评估了九个指令调整的 LLM（GPT、Claude、Gemini 和 OpenAI o 系列模型）。我们使用准确率、召回率、假阳性率 (FPR) 以及用于纠错的 ROUGE-1、BLEURT 和 BERTScore 的总分来衡量性能。我们还分析了示例输出，以识别失败模式以及法学硕士和临床医生推理之间的差异。结果：零样本提示在两项检测任务中都显示出较低的召回率，经常遗漏缩写较多或非典型错误。 SPR 提高了召回率，但提高了 FPR。在所有九个法学硕士中，RDP 将 FPR 降低了约 15%，在错误句子检测中将召回率提高了 5% 到 10%，并生成了更准确的上下文更正。结论：在不同的法学硕士中，RDP 优于零样本和 SPR 提示。使用检索到的样本可以提高检测准确性，减少误报，并增强医疗纠错的可靠性。</li>
</ul>

<h3>Title: AppSelectBench: Application-Level Tool Selection Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Chen, Michael Solodko, Sen Wang, Jongwoo Ko, Junheng Hao, Colby Banbury, Sara Abdali, Saeed Amizadeh, Qing Xiao, Yinheng Li, Tianyu Ding, Kamran Ghasedi Dizaji, Suzhen Zheng, Hao Fan, Justin Wagle, Pashmina Cameron, Kazuhito Koishida</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19957">https://arxiv.org/abs/2511.19957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19957">https://arxiv.org/pdf/2511.19957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19957]] AppSelectBench: Application-Level Tool Selection Benchmark(https://arxiv.org/abs/2511.19957)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at this https URL.</li>
<li><strong>摘要：</strong>计算机使用代理（CUA）越来越多地配备外部工具，使它们能够执行复杂而现实的任务。为了使 CUA 有效运行，应用程序选择是一项基本能力，即在调用 API 等细粒度工具之前决定使用哪个应用程序。它确定代理是否初始化正确的环境，避免编排混乱，并有效地关注相关上下文。然而，现有的基准测试主要评估细粒度的 API 选择，对于模型是否可以在不同的应用程序之间进行推理和选择提供的见解有限。为了填补这一空白，我们引入了 AppSelectBench，这是一个用于评估 CUA 中应用程序选择的综合基准。 AppSelectBench 包含一个新颖的用户任务生成管道，可大规模生成现实、多样化且基于语义的用户意图，以及涵盖随机、启发式、零样本、少样本和检索增强设置的统一评估协议。 AppSelectBench 涵盖了一百种广泛使用的桌面应用程序，并包括十万多个现实的、多样化的、基于语义的用户任务。对闭源和开源大语言模型的大量实验揭示了应用程序间推理的系统优势和弱点，表明即使是最有能力的模型仍然难以做出一致的应用程序选择。总之，这些结果将 AppSelectBench 确立为研究和推进应用程序级推理的基础，这是智能 CUA 的一项重要但尚未充分开发的功能。可以通过此 https URL 获取源代码。</li>
</ul>

<h3>Title: $\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Hanwei Wu, Qingchen Hu, Zhenghan Tai, Jingrui Tian, Lei Ding, Jijun Chi, Hailin He, Tung Sum Thomas Kwok, Yufei Cui, Sicheng Lyu, Muzhi Li, Mingze Li, Xinyue Yu, Ling Zhou, Peng Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19987">https://arxiv.org/abs/2511.19987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19987">https://arxiv.org/pdf/2511.19987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19987]] $\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers(https://arxiv.org/abs/2511.19987)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.</li>
<li><strong>摘要：</strong>仅解码器重排序是检索增强生成 (RAG) 的核心。然而，通才模型忽略了金融和法律等高风险领域特定领域的细微差别，而幼稚的微调会导致表面形式过度拟合和灾难性遗忘。为了应对这一挑战，我们引入了 R2R，这是一种领域感知框架，它将动态专家路由与两阶段训练策略、泛化实体抽象 (EAG) 相结合。 EAG 通过屏蔽最具预测性的表面线索引入了一种反捷径机制，迫使重新排序器学习领域不变的相关模式，而不是记住数据集特定的实体。为了有效地激活领域专家，R2R 采用了一个轻量级的潜在语义路由器，该路由器从冻结的骨干解码器中探测内部表示，以便为每个查询选择最佳的 LoRA 专家。跨不同重排序主干和不同领域（法律、医疗和金融）的广泛实验表明，R2R 始终超越通才和单域微调基线。我们的结果证实，R2R 是一种与模型无关的模块化方法，用于领域专业化，具有强大的跨领域鲁棒性。</li>
</ul>

<h3>Title: Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test</h3>
<ul>
<li><strong>Authors: </strong>Mihir Sahasrabudhe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.19997">https://arxiv.org/abs/2511.19997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.19997">https://arxiv.org/pdf/2511.19997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.19997]] Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test(https://arxiv.org/abs/2511.19997)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.</li>
<li><strong>摘要：</strong>Transformer 理论上是反转不变的：它们的函数类不喜欢从左到右映射而不是从右到左映射。然而，对自然语言的实证研究反复报告了“逆转诅咒”，而最近关于法学硕士时间不对称性的研究表明，现实世界的语料库带有自己的时间箭头。这就留下了一个悬而未决的问题：方向性失败是源于语言统计，还是源于架构本身？我们通过完全合成的、熵控制的基准来消除这种模糊性，该基准被设计为定向学习的洁净室压力测试。使用具有可调分支因子 K 的随机字符串映射，我们构建了条件熵为零的正向任务和具有分析确定的熵层的逆向任务。这些楼层之上的过量损失表明，即使是经过训练的 GPT-2 模型也表现出强大的、可重复的方向优化差距（例如，K=5 时为 1.16 nat），远大于在相同数据上训练的 MLP。预训练的初始化改变了优化行为，但并没有消除这一差距，而 LoRA 在高熵逆映射上遇到了尖锐的容量墙。总之，这些结果隔离了因果 Transformer 训练固有的方向摩擦的最小的、无语义的特征，即使消除了语言先验、标记频率和语料库级别的时间不对称性，该特征仍然存在。我们的基准测试提供了一种受控工具，用于剖析现代序列模型中的方向偏差，并激发了更深入的机制研究，以了解为什么反演对于 Transformers 来说仍然更加困难。</li>
</ul>

<h3>Title: A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media</h3>
<ul>
<li><strong>Authors: </strong>Edward Ajayi, Martha Kachweka, Mawuli Deku, Emily Aiken</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20001">https://arxiv.org/abs/2511.20001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20001">https://arxiv.org/pdf/2511.20001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20001]] A Machine Learning Approach for Detection of Mental Health Conditions and Cyberbullying from Social Media(https://arxiv.org/abs/2511.20001)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Mental health challenges and cyberbullying are increasingly prevalent in digital spaces, necessitating scalable and interpretable detection systems. This paper introduces a unified multiclass classification framework for detecting ten distinct mental health and cyberbullying categories from social media data. We curate datasets from Twitter and Reddit, implementing a rigorous "split-then-balance" pipeline to train on balanced data while evaluating on a realistic, held-out imbalanced test set. We conducted a comprehensive evaluation comparing traditional lexical models, hybrid approaches, and several end-to-end fine-tuned transformers. Our results demonstrate that end-to-end fine-tuning is critical for performance, with the domain-adapted MentalBERT emerging as the top model, achieving an accuracy of 0.92 and a Macro F1 score of 0.76, surpassing both its generic counterpart and a zero-shot LLM baseline. Grounded in a comprehensive ethical analysis, we frame the system as a human-in-the-loop screening aid, not a diagnostic tool. To support this, we introduce a hybrid SHAPLLM explainability framework and present a prototype dashboard ("Social Media Screener") designed to integrate model predictions and their explanations into a practical workflow for moderators. Our work provides a robust baseline, highlighting future needs for multi-label, clinically-validated datasets at the critical intersection of online safety and computational mental health.</li>
<li><strong>摘要：</strong>心理健康挑战和网络欺凌在数字空间中越来越普遍，因此需要可扩展和可解释的检测系统。本文介绍了一个统一的多类分类框架，用于从社交媒体数据中检测十种不同的心理健康和网络欺凌类别。我们整理来自 Twitter 和 Reddit 的数据集，实施严格的“拆分然后平衡”管道来训练平衡数据，同时评估现实的、保留的不平衡测试集。我们对传统词汇模型、混合方法和几个端到端微调变压器进行了全面评估。我们的结果表明，端到端微调对于性能至关重要，领域适应型 MentalBERT 成为顶级模型，实现了 0.92 的准确率和 0.76 的 Macro F1 分数，超过了其通用模型和零样本 LLM 基线。基于全面的伦理分析，我们将该系统构建为人机交互筛查辅助工具，而不是诊断工具。为了支持这一点，我们引入了混合 SHAPLLM 可解释性框架，并提供了一个原型仪表板（“社交媒体筛选器”），旨在将模型预测及其解释集成到主持人的实际工作流程中。我们的工作提供了一个可靠的基线，强调了在线安全和计算心理健康的关键交叉点上对多标签、临床验证数据集的未来需求。</li>
</ul>

<h3>Title: Online-PVLM: Advancing Personalized VLMs with Online Concept Learning</h3>
<ul>
<li><strong>Authors: </strong>Huiyu Bai, Runze Wang, Zhuoyun Du, Yiyang Zhao, Fengji Zhang, Haoyu Chen, Xiaoyong Zhu, Bo Zheng, Xuejiao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20056">https://arxiv.org/abs/2511.20056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20056">https://arxiv.org/pdf/2511.20056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20056]] Online-PVLM: Advancing Personalized VLMs with Online Concept Learning(https://arxiv.org/abs/2511.20056)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.</li>
<li><strong>摘要：</strong>个性化视觉语言模型 (VLM) 因其在用户特定概念一致交互（例如识别用户的自行车）方面的强大能力而受到越来越多的关注。现有方法通常需要学习每个新概念的单独嵌入，这无法支持测试期间的实时适应。这种限制在大规模场景中变得尤其明显，在这种场景中，无法实现概念嵌入的有效检索。为了缩小这一差距，我们提出了 Online-PVLM，这是一种利用双曲表示进行在线概念学习的框架。我们的方法为测试时的概念嵌入生成提供了一种无需训练的范例，从而使个性化 VLM 的使用既可扩展又高效。此外，我们还开发了OP-Eval，这是一个全面的大规模基准测试，包含 1,292 个概念和超过 30K 个具有不同问题类型的高质量实例，旨在严格评估现实场景中的在线概念学习。大量的实验证明了我们提出的框架的最先进的性能。我们的源代码和数据集将可供使用。</li>
</ul>

<h3>Title: MTA: A Merge-then-Adapt Framework for Personalized Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaopeng Li, Yuanjin Zheng, Wanyu Wang, wenlin zhang, Pengyue Jia, Yiqi Wang, Maolin Wang, Xuetao Wei, Xiangyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20072">https://arxiv.org/abs/2511.20072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20072">https://arxiv.org/pdf/2511.20072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20072]] MTA: A Merge-then-Adapt Framework for Personalized Large Language Model(https://arxiv.org/abs/2511.20072)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.</li>
<li><strong>摘要：</strong>个性化大型语言模型 (PLLM) 旨在使模型输出与个人用户偏好保持一致，这是以用户为中心的应用程序的关键功能。然而，为每个用户微调单独模块的流行方法面临两个主要限制：（1）存储成本随用户数量线性扩展，导致该方法不可扩展； (2) 从头开始​​微调静态模型通常会给数据稀疏的用户带来次优的性能。为了应对这些挑战，我们提出了 MTA，即 PLLM 的“合并然后调整”框架。 MTA 包括三个关键阶段。首先，我们通过选择锚用户并在元 LoRA 模块中预训练元个性化特征来构建共享的元 LoRA 银行。其次，为了确保可扩展性并实现静态模型之外的动态个性化组合，我们引入了自适应 LoRA Fusion 阶段。这一阶段检索并动态合并最相关的锚定元 LoRA，以合成用户特定的元 LoRA，从而消除对用户特定存储的需求并支持更灵活的个性化。第三，我们提出了用于少样本个性化阶段的 LoRA 堆栈，该阶段在合并的 LoRA 之上应用了额外的超低秩、轻量级 LoRA 模块。微调该模块可以在几次拍摄设置下实现有效的个性化。 LaMP 基准的大量实验表明，我们的方法在多个任务上优于现有的 SOTA 方法。</li>
</ul>

<h3>Title: More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Duc Anh Vu, Thong Nguyen, Cong-Duy Nguyen, Viet Anh Nguyen, Anh Tuan Luu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20086">https://arxiv.org/abs/2511.20086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20086">https://arxiv.org/pdf/2511.20086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20086]] More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering(https://arxiv.org/abs/2511.20086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的进步，它们在多项选择问题（MCQ）任务上的表现有了显着提高。然而，现有的方法面临着关键的局限性：答案选择通常是在没有上下文背景或解释的情况下呈现给法学硕士的。缺乏上下文可能会导致对所有可能答案的探索不完整，最终降低模型的推理能力。为了应对这些挑战，我们引入了 BiasPrompting，这是一种新颖的推理框架，可指导法学硕士在得出最终预测之前生成并批判性地评估所有看似合理的答案选项的推理。它由两个组成部分组成：首先是推理生成阶段，提示模型为每个答案选项生成支持性推理；然后是推理引导一致阶段，综合生成的推理以选择最合理的答案。通过综合评估，BiasPrompting 在五个广泛使用的多项选择题回答基准测试中展示了显着的改进。我们的实验表明，BiasPrompting 增强了法学硕士的推理能力，并为解决复杂和具有挑战性的问题提供了坚实的基础，特别是在现有方法表现不佳的情况下。</li>
</ul>

<h3>Title: SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space</h3>
<ul>
<li><strong>Authors: </strong>Zhenyi Shen, Junru Lu, Lin Gui, Jiazheng Li, Yulan He, Di Yin, Xing Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20102">https://arxiv.org/abs/2511.20102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20102">https://arxiv.org/pdf/2511.20102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20102]] SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space(https://arxiv.org/abs/2511.20102)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.</li>
<li><strong>摘要：</strong>完全注意力的二次复杂度限制了大型语言模型（LLM）中有效的长上下文处理。稀疏注意力通过限制每个查询只关注先前标记的子集来减轻这种成本；然而，免培训方法通常会导致性能严重下降。原生稀疏注意力方法（例如 NSA、MoBA）缓解了这个问题，但表现出一个关键的悖论：尽管其目标是接近完全注意力，但它们产生的注意力稀疏性低于全注意力模型，这可能会限制其有效性。我们将这个悖论归因于梯度更新缺陷：在稀疏训练期间排除的低排名键值对既不接收前向贡献也不接收后向梯度，因此永远不会学习适当的抑制。为了克服这个限制，我们提出了 SSA（稀疏注意力），这是一个统一的训练框架，它同时考虑稀疏和充分注意力，并在每一层强制双向对齐。这种设计保留了所有令牌的梯度流，同时明确鼓励稀疏注意力输出与其全注意力对应输出保持一致，从而促进更强的稀疏性。因此，SSA 在跨多个常识基准的稀疏和全注意力推理下都实现了最先进的性能。此外，SSA 使模型能够平滑地适应不同的稀疏预算；随着更多代币被允许参与，性能持续提高，支持推理时灵活的计算性能权衡。最后，我们表明，原生稀疏注意力训练通过减轻汇区注意力值的过度分配，令人惊讶地改善了长上下文外推，其中 SSA 表现出了最强的外推能力。</li>
</ul>

<h3>Title: "When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings</h3>
<ul>
<li><strong>Authors: </strong>Somsubhra De, Harsh Kumar, Arun Prakash A</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20120">https://arxiv.org/abs/2511.20120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20120">https://arxiv.org/pdf/2511.20120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20120]] "When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings(https://arxiv.org/abs/2511.20120)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.</li>
<li><strong>摘要：</strong>语法错误纠正（GEC）是自然语言处理中的一项重要任务，旨在自动检测和纠正文本中的语法错误。虽然基于 Transformer 的模型和大型注释数据集的最新进展极大地提高了英语等高资源语言的 GEC 性能，但进展并没有得到同等的扩展。对于大多数印度语言来说，由于资源有限、语言多样性和复杂的形态，GEC 仍然是一项具有挑战性的任务。在这项工作中，我们使用最先进的大型语言模型（LLM）（例如 GPT-4.1、Gemini-2.5 和 LLaMA-4）探索基于提示的方法，并结合少量策略以使其适应低资源设置。我们观察到，即使是基本的提示策略，例如零样本和少样本方法，也使这些 LLM 能够大大优于 Sarvam-22B 等经过微调的印度语言模型，从而说明了当代 GEC 的 LLM 卓越的多语言泛化能力。我们的实验表明，精心设计的提示和轻量级的适应显着提高了多种印度语言的纠正质量。我们在共同任务中取得了领先成果——泰米尔语（GLEU：91.57）和印地语（GLEU：85.69）排名第一，泰卢固语（GLEU：85.22）排名第二，孟加拉语（GLEU：92.86）排名第四，马拉雅拉姆语（GLEU：92.97）排名第五。这些发现凸显了即时驱动的 NLP 技术的有效性，并强调了大规模法学硕士在弥合多语言 GEC 资源差距方面的潜力。</li>
</ul>

<h3>Title: KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP</h3>
<ul>
<li><strong>Authors: </strong>Adilet Metinov, Gulida M. Kudakeeva, Gulnara D. Kabaeva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20182">https://arxiv.org/abs/2511.20182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20182">https://arxiv.org/pdf/2511.20182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20182]] KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP(https://arxiv.org/abs/2511.20182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.</li>
<li><strong>摘要：</strong>吉尔吉斯语仍然是一种资源匮乏的语言，基础 NLP 工具有限。为了解决这一差距，我们引入了 KyrgyzBERT，这是吉尔吉斯语第一个公开的基于 BERT 的单语语言模型。该模型有 3590 万个参数，并使用专为该语言的形态结构设计的自定义分词器。为了评估性能，我们创建了 kyrgyz-sst2，这是一个通过翻译斯坦福情绪树库并手动注释完整测试集而构建的情绪分析基准。在此数据集上进行微调的 KyrgyzBERT 达到了 0.8280 的 F1 分数，与大五倍的微调 mBERT 模型相媲美。所有模型、数据和代码的发布都是为了支持吉尔吉斯斯坦 NLP 的未来研究。</li>
</ul>

<h3>Title: REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance</h3>
<ul>
<li><strong>Authors: </strong>Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, Zhiyuan Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20233">https://arxiv.org/abs/2511.20233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20233">https://arxiv.org/pdf/2511.20233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20233]] REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance(https://arxiv.org/abs/2511.20233)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.</li>
<li><strong>摘要：</strong>社交媒体上错误信息的盛行威胁着公众的信任，需要自动事实核查系统提供准确的判决和可解释的解释。然而，现有的基于大语言模型（基于LLM）的方法通常严重依赖外部知识源，引入大量延迟甚至幻觉，从而破坏可靠性、可解释性和响应性，而这对于实时使用至关重要。为了应对这些挑战，我们提出了使用潜在解释 REFLEX 范式进行 REason 引导的事实检查，这是一种即插即用、自我完善的范式，利用主干模型中的内部知识来提高判决准确性和解释质量。 REFLEX 将事实检查重新表述为角色扮演对话，并联合训练判决预测和解释生成。它自适应地提取主干模型及其微调变体之间的对比激活对，以构建引导向量，将真理自然地分解为风格和实质。这些激活水平信号指导推理并抑制噪声解释，从而实现更忠实和更有效的推理。对现实世界数据集的实验表明，REFLEX 优于以前转向单一真相方向的方法，并强调了传统方法在事实检查任务中处理微妙的、人类未知的真相时面临的挑战。值得注意的是，仅用 465 个自提训练样本，RELFEX 就实现了最先进的性能。此外，经过解释性目标训练的模型可以有效地指导那些没有解释性目标的模型，提高了 7.57%，这凸显了内部解释信号在解释和增强事实推理方面发挥着双重作用。</li>
</ul>

<h3>Title: Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Luohe Shi, Zuchao Li, Lefei Zhang, Baoyuan Qi, Guoming Liu, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20340">https://arxiv.org/abs/2511.20340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20340">https://arxiv.org/pdf/2511.20340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20340]] Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios(https://arxiv.org/abs/2511.20340)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.</li>
<li><strong>摘要：</strong>推测性解码通过在内存到芯片数据传输期间利用空闲的计算资源来加速 LLM 推理。当前的推测性解码方法通常假设有大量可用计算能力，然后使用小型自回归语言模型生成复杂且庞大的草图树，以提高整体预测精度。然而，批处理等方法已广泛应用于主流模型推理系统，作为推测解码的更好替代方案，因为它们压缩了可用的空闲计算能力。因此，以低验证资源和低调度成本执行推测解码已成为重要的研究问题。我们相信，我们真正需要的是允许并行生成草稿序列的更强大的模型。认识到草稿模型仅生成有限长度序列的基本性质，我们提出了 SpecFormer，这是一种集成了单向和双向注意机制的新颖架构。 SpecFormer 将自回归模型从整个输入序列中提取信息的能力与非自回归模型的并行生成优势相结合。这种设计消除了对大型前缀树的依赖，即使在大批量场景下也能实现一致的加速。通过跨各种规模模型的无损推测解码实验，我们证明 SpecFormer 为扩展 LLM 推理设定了新标准，同时降低了训练需求和计算成本。</li>
</ul>

<h3>Title: The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Taewhoo Lee, Minju Song, Chanwoong Yoon, Jungwoo Park, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20344">https://arxiv.org/abs/2511.20344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20344">https://arxiv.org/pdf/2511.20344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20344]] The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models(https://arxiv.org/abs/2511.20344)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.</li>
<li><strong>摘要：</strong>类比推理是人类认知的核心，是各种智力活动的重要基础。虽然之前的工作表明法学硕士可以表示任务模式和表面概念，但目前尚不清楚这些模型是否可以编码高级关系概念并通过结构化比较将其应用于新情况。在这项工作中，我们使用比例和故事类比来探索这个基本方面，并确定三个关键发现。首先，法学硕士有效地编码了类似实体之间的底层关系；在正确的情况下，属性信息和关系信息都会通过中上层传播，而推理失败则反映了这些层中缺失的关系信息。其次，与人类不同的是，法学硕士不仅在关系信息缺失时常常陷入困境，而且在尝试将其应用于新实体时也常常陷入困境。在这种情况下，策略性地修补关键令牌位置的隐藏表示可以在一定程度上促进信息传输。最后，法学硕士中成功的类比推理的标志是类似情况之间的强结构对齐，而失败通常反映了对齐的退化或错误。总体而言，我们的研究结果表明，法学硕士在编码和应用高级关系概念方面表现出新兴但有限的能力，凸显了与人类认知的相似之处和差距。</li>
</ul>

<h3>Title: BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Al Sefat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20399">https://arxiv.org/abs/2511.20399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20399">https://arxiv.org/pdf/2511.20399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20399]] BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali(https://arxiv.org/abs/2511.20399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.</li>
<li><strong>摘要：</strong>大型语言模型在广泛的多语言基准上表现出色，但在比喻和文化推理方面仍有待广泛评估，特别是在资源匮乏的情况下。我们推出了 BengaliFig，这是一个紧凑但注释丰富的挑战集，旨在针对孟加拉语（一种广泛使用的资源匮乏语言）的这一差距。该数据集包含 435 个来自孟加拉口头和文学传统的独特谜语。每个项目都沿着五个正交维度进行注释，捕获推理类型、陷阱类型、文化深度、答案类别和难度，并通过约束感知、人工智能辅助管道自动转换为多项选择格式。我们在零样本和少样本思维链提示下评估了来自主要提供商的八个前沿法学硕士，揭示了隐喻和文化特定推理方面的一贯弱点。因此，BengaliFig 既为评估 LLM 在资源匮乏的文化背景下的稳健性提供了诊断探针，又向包容性和传统意识的 NLP 评估迈出了一步。</li>
</ul>

<h3>Title: A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Md Abdullah Al Kafi, Raka Moni, Sumit Kumar Banshal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20409">https://arxiv.org/abs/2511.20409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20409">https://arxiv.org/pdf/2511.20409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20409]] A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines(https://arxiv.org/abs/2511.20409)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream this http URL contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).</li>
<li><strong>摘要：</strong>文本规范化是许多自然语言处理 (NLP) 任务中必不可少的预处理步骤，而词干提取就是一种此类规范化技术，可将单词还原为其基本形式或词根形式。然而，评估词干提取方法具有挑战性，因为当前的评估方法有限，并且没有捕捉到过度词干提取带来的潜在危害；因此，有必要开发新的方法来评估词干提取方法。为了解决这个问题，本研究提出了一种新颖的、面向任务的方法来评估词干提取方法，该方法考虑了三个方面：（1）使用词干有效性得分（SES）进行词干提取的效用，（2）使用模型性能增量（MPD）进行词干提取对下游任务的影响，以及（3）使用平均归一化编辑距离（ANLD）进行词干提取和原始单词之间的语义相似度，从而提供了一个全面的评估框架。我们应用我们的评估框架来比较孟加拉语 (BNLTK) 和英语 (Snowball) 的两个词干分析器，我们的结果揭示了一个重大问题，促使我们详细分析它们的性能。虽然孟加拉语词干分析器由于有效的单词减少 (CR = 1.90) 而达到了最高的 SES (1.67)，但仅靠 SES 是不够的，因为我们提出的安全措施 ANLD 表明，这种高 SES 是由于有害的过度词干提取 (ANLD = 0.26)，这与观察到的下游此 http URL 对比度的下降相关，而英语词干分析器则实现了中等 SES (1.31)，具有安全的含义距离 (ANLD = 0.14)，使其字数减少对下游绩效做出积极贡献；因此，它是一个更可靠的词干分析器。我们的研究为区分潜在效率增益（高 SES）和意义保留（低 ANLD）提供了一个有价值的工具。</li>
</ul>

<h3>Title: Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts</h3>
<ul>
<li><strong>Authors: </strong>Mosab Rezaei, Mina Rajaei Moghadam, Abdul Rahman Shaikh, Hamed Alhoori, Reva Freedman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20459">https://arxiv.org/abs/2511.20459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20459">https://arxiv.org/pdf/2511.20459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20459]] Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts(https://arxiv.org/abs/2511.20459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.</li>
<li><strong>摘要：</strong>大型语言模型的最新进展为文体测量、写作风格和作者身份的研究创造了新的机会。然而，两个挑战仍然是核心：在不存在配对数据的情况下训练生成模型，以及在不仅依赖人类判断的情况下评估文体文本。在这项工作中，我们提出了一个以 19 世纪小说家的风格生成和评估句子的框架。大型语言模型通过最少的单标记提示进行微调，以狄更斯、奥斯汀、吐温、奥尔科特和梅尔维尔等作家的声音生成文本。为了评估这些生成模型，我们采用了基于变压器的检测器，该检测器在真实句子上进行了训练，将其用作分类器和文体解释的工具。我们通过句法比较和可解释的人工智能方法来补充这一点，包括基于注意力和基于梯度的分析，以识别驱动风格模仿的语言线索。我们的研究结果表明，生成的文本反映了作者独特的模式，并且基于人工智能的评估为人类评估提供了可靠的替代方案。这项工作的所有作品均在线发布。</li>
</ul>

<h3>Title: Adversarial Confusion Attack: Disrupting Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jakub Hoscilowicz, Artur Janicki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20494">https://arxiv.org/abs/2511.20494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20494">https://arxiv.org/pdf/2511.20494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20494]] Adversarial Confusion Attack: Disrupting Multimodal Large Language Models(https://arxiv.org/abs/2511.20494)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.</li>
<li><strong>摘要：</strong>我们介绍了对抗性混淆攻击，这是针对多模式大语言模型 (MLLM) 的一类新威胁。与越狱或有针对性的错误分类不同，其目标是引起系统性破坏，使模型生成不连贯或确信不正确的输出。应用程序包括将对抗性图像嵌入网站，以防止 MLLM 支持的代理可靠运行。所提出的攻击使用小型开源 MLLM 集合来最大化下一个令牌熵。在白盒设置中，我们表明单个对抗性图像可以破坏整体中的所有模型，无论是在全图像还是对抗性验证码设置中。尽管依赖基本的对抗技术 (PGD)，但该攻击会产生扰动，并转移到看不见的开源（例如 Qwen3-VL）和专有（例如 GPT-5.1）模型。</li>
</ul>

<h3>Title: The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nathan Roll, Jill Kries, Flora Jin, Catherine Wang, Ann Marie Finley, Meghan Sumner, Cory Shain, Laura Gwilliams</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20507">https://arxiv.org/abs/2511.20507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20507">https://arxiv.org/pdf/2511.20507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20507]] The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models(https://arxiv.org/abs/2511.20507)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as a candidate "model organism" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.</li>
<li><strong>摘要：</strong>大语言模型（LLM）已成为人类语言的候选“模型有机体”，为研究失语症等语言障碍的计算基础提供了前所未有的机会。然而，传统的临床评估并不适合法学硕士，因为它们预设了类人的实用压力，并探讨了人工架构所固有的认知过程。我们引入了文本失语症电池 (TAB)，这是一种从快速失语症电池 (QAB) 改编而来的纯文本基准，用于评估法学硕士的失语症样缺陷。 TAB 包括四个子测试：连接文本、单词理解、句子理解和重复。本文详细介绍了 TAB 的设计、子测试和评分标准。为了促进大规模使用，我们使用 Gemini 2.5 Flash 验证了自动评估协议，该协议的可靠性可与人类专家评估者相媲美（模型-共识协议的流行度加权 Cohen's kappa = 0.255，而人类-人类协议的流行度加权 Cohen's kappa = 0.286）。我们发布 TAB 作为一个基于临床的、可扩展的框架，用于分析人工系统中的语言缺陷。</li>
</ul>

<h3>Title: From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding</h3>
<ul>
<li><strong>Authors: </strong>Farjana Sultana Mim, Shuchin Aeron, Eric Miller, Kristen Wendell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20547">https://arxiv.org/abs/2511.20547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20547">https://arxiv.org/pdf/2511.20547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20547]] From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding(https://arxiv.org/abs/2511.20547)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.</li>
<li><strong>摘要：</strong>识别学生对话中的话语特征对于教育研究人员认识到导致学生参与知识构建而不仅仅是完成任务的课程和教学变量非常重要。对学生对话进行手动分析以识别这些话语特征既耗时又费力，这限制了研究的规模和范围。利用自然语言处理（NLP）技术可以促进这些话语特征的自动检测，为教育研究人员提供可扩展和数据驱动的见解。然而，现有的 NLP 研究重点关注对话中的话语，很少涉及教育数据。在这项工作中，我们通过引入一个带注释的学生对话教育对话数据集来解决这一差距，该数据集以知识构建和任务生产话语为特色。我们还使用预先训练的大型语言模型 GPT-3.5 和 Llama-3.1 建立基线模型，用于自动预测对话中每次谈话的这些话语属性。实验结果表明，这些最先进的模型在这项任务上表现不佳，表明了未来研究的潜力。</li>
</ul>

<h3>Title: On Evaluating LLM Alignment by Evaluating LLMs as Judges</h3>
<ul>
<li><strong>Authors: </strong>Yixin Liu, Pengfei Liu, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20604">https://arxiv.org/abs/2511.20604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20604">https://arxiv.org/pdf/2511.20604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20604]] On Evaluating LLM Alignment by Evaluating LLMs as Judges(https://arxiv.org/abs/2511.20604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.</li>
<li><strong>摘要：</strong>与人类偏好的一致性是法学硕士的一个重要评估方面，要求他们乐于助人、诚实、安全，并精确遵循人类的指示。评估大型语言模型 (LLM) 的一致性通常涉及直接评估其开放式响应，需要人工注释者或强大的 LLM 法官。相反，法学硕士本身也被广泛评估为评估一致性的法官。在这项工作中，我们研究了法学硕士的生成和评估能力之间的关系，以符合人类的偏好。为此，我们首先对各个LLM之间的生成-评估一致性（GE-consistency）进行了全面分析，揭示了在通过强大的LLM偏好预言机评估时，它们的生成和评估能力之间存在很强的相关性。利用这一发现，我们提出了一种基准范式，可以衡量法学硕士与人类偏好的一致性，而不直接评估其生成的输出，而是评估法学硕士作为评估者的角色。我们的评估表明，我们提出的基准 AlignEval 在对 LLM 进行排名时在捕捉人类偏好方面匹配或超越了广泛使用的自动 LLM 评估基准，例如 AlpacaEval 和 Arena-Hard。我们的研究为法学硕士的生成和评估能力之间的联系提供了宝贵的见解，并引入了一个无需直接评估模型输出即可评估一致性的基准。</li>
</ul>

<h3>Title: Latent Collaboration in Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, Ling Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.20639">https://arxiv.org/abs/2511.20639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.20639">https://arxiv.org/pdf/2511.20639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.20639]] Latent Collaboration in Multi-Agent Systems(https://arxiv.org/abs/2511.20639)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at this https URL.</li>
<li><strong>摘要：</strong>多智能体系统 (MAS) 将大型语言模型 (LLM) 从独立的单模型推理扩展到协调的系统级智能。虽然现有的 LLM 代理依赖基于文本的中介进行推理和通信，但我们通过使模型能够在连续潜在空间内直接协作而向前迈出了一步。我们引入 LatentMAS，这是一个端到端的免培训框架，可实现 LLM 代理之间的纯粹潜在协作。在 LatentMAS 中，每个智能体首先通过最后一层隐藏嵌入执行自回归潜在想法生成。然后，共享的潜在工作记忆保存并传输每个代理的内部表示，确保无损的信息交换。我们提供的理论分析表明，与基于普通文本的 MAS 相比，LatentMAS 具有更高的表现力和无损信息保存，且复杂性大大降低。此外，对数学和科学推理、常识理解和代码生成等 9 个综合基准的实证评估表明，LatentMAS 的性能始终优于强大的单模型和基于文本的 MAS 基线，准确率提高了 14.6%，输出令牌使用量减少了 70.8%-83.7%，并提供了 4 倍到 4.3 倍的更快端到端推理速度。这些结果表明，我们新的潜在协作框架增强了系统级推理质量，同时无需任何额外培训即可显着提高效率。代码和数据在此 https URL 上完全开源。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
