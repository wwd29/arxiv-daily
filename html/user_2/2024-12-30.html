<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-30</h1>
<h3>Title: Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Liang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge Zhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen, Junyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Minjia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, Baobao Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MM, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18619">https://arxiv.org/abs/2412.18619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18619">https://arxiv.org/pdf/2412.18619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18619]] Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey(https://arxiv.org/abs/2412.18619)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \& evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at this https URL</li>
<li><strong>摘要：</strong>下一个标记预测 (NTP) 以自然语言处理中的语言建模为基础，已发展成为跨各种模态的机器学习任务的多功能训练目标，并取得了相当大的成功。随着大型语言模型 (LLM) 的发展，统一了文本模态中的理解和生成任务，最近的研究表明，不同模态的任务也可以有效地封装在 NTP 框架中，将多模态信息转换为标记并根据上下文预测下一个标记。本综述介绍了一种全面的分类法，通过 NTP 的视角统一了多模态学习中的理解和生成。提议的分类法涵盖五个关键方面：多模态标记化、MMNTP 模型架构、统一任务表示、数据集和评估以及开放挑战。这种新的分类法旨在帮助研究人员探索多模态智能。收集最新论文和存储库的相关 GitHub 存储库可在此 https URL 上找到</li>
</ul>

<h3>Title: Investigating the Feasibility of Mitigating Potential Copyright Infringement via Large Language Model Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Guangyao Dou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18621">https://arxiv.org/abs/2412.18621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18621">https://arxiv.org/pdf/2412.18621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18621]] Investigating the Feasibility of Mitigating Potential Copyright Infringement via Large Language Model Unlearning(https://arxiv.org/abs/2412.18621)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities but also pose risks by learning and generating copyrighted material, leading to significant legal and ethical concerns. In a potential real-world scenario, model owners may need to continuously address copyright infringement in order to address requests for content removal that emerge at different time points. One potential way of addressing this is via sequential unlearning, where copyrighted content is removed sequentially as new requests arise. Despite its practical relevance, sequential unlearning in the context of copyright infringement has not been rigorously explored in existing literature. To address this gap, we propose Stable Sequential Unlearning (SSU), a novel framework designed to unlearn copyrighted content from LLMs over multiple time steps. Our approach works by identifying and removing specific weight updates in the model's parameters that correspond to copyrighted content using task vectors. We improve unlearning efficacy by introducing random labeling loss and ensuring the model retains its general-purpose knowledge by adjusting targeted parameters with gradient-based weight saliency. Extensive experimental results show that SSU sometimes achieves an effective trade-off between unlearning efficacy and general-purpose language abilities, outperforming existing baselines, but it's not a cure-all for unlearning copyrighted material.</li>
<li><strong>摘要：</strong>预训练的大型语言模型 (LLM) 表现出了卓越的能力，但也因学习和生成受版权保护的材料而带来风险，从而引发重大的法律和道德问题。在潜在的现实场景中，模型所有者可能需要不断解决版权侵权问题，以处理在不同时间点出现的内容删除请求。解决此问题的一种潜在方法是通过顺序反学习，即随着新请求的出现，按顺序删除受版权保护的内容。尽管顺序反学习具有实际意义，但现有文献尚未严格探索版权侵权背景下的顺序反学习。为了解决这一差距，我们提出了稳定顺序反学习 (SSU)，这是一个新颖的框架，旨在在多个时间步骤中从 LLM 中反学习受版权保护的内容。我们的方法通过使用任务向量识别和删除模型参数中与受版权保护的内容相对应的特定权重更新来工作。我们通过引入随机标记损失来提高反学习效率，并通过基于梯度的权重显着性调整目标参数来确保模型保留其通用知识。大量实验结果表明，SSU 有时能够在遗忘功效和通用语言能力之间实现有效平衡，表现优于现有基线，但它并不是遗忘受版权保护材料的万能药。</li>
</ul>

<h3>Title: Why Do Large Language Models (LLMs) Struggle to Count Letters?</h3>
<ul>
<li><strong>Authors: </strong>Tairan Fu, Raquel Ferrando, Javier Conde, Carlos Arriaga, Pedro Reviriego</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18626">https://arxiv.org/abs/2412.18626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18626">https://arxiv.org/pdf/2412.18626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18626]] Why Do Large Language Models (LLMs) Struggle to Count Letters?(https://arxiv.org/abs/2412.18626)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved unprecedented performance on many complex tasks, being able, for example, to answer questions on almost any topic. However, they struggle with other simple tasks, such as counting the occurrences of letters in a word, as illustrated by the inability of many LLMs to count the number of "r" letters in "strawberry". Several works have studied this problem and linked it to the tokenization used by LLMs, to the intrinsic limitations of the attention mechanism, or to the lack of character-level training data. In this paper, we conduct an experimental study to evaluate the relations between the LLM errors when counting letters with 1) the frequency of the word and its components in the training dataset and 2) the complexity of the counting operation. We present a comprehensive analysis of the errors of LLMs when counting letter occurrences by evaluating a representative group of models over a large number of words. The results show a number of consistent trends in the models evaluated: 1) models are capable of recognizing the letters but not counting them; 2) the frequency of the word and tokens in the word does not have a significant impact on the LLM errors; 3) there is a positive correlation of letter frequency with errors, more frequent letters tend to have more counting errors, 4) the errors show a strong correlation with the number of letters or tokens in a word and 5) the strongest correlation occurs with the number of letters with counts larger than one, with most models being unable to correctly count words in which letters appear more than twice.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在许多复杂任务上取得了前所未有的表现，例如，能够回答几乎所有主题的问题。然而，它们在其他简单任务上却举步维艰，例如计算单词中字母的出现次数，许多 LLM 无法计算“strawberry”中“r”字母的数量就说明了这一点。有几篇论文研究了这个问题，并将其与 LLM 使用的标记化、注意力机制的内在局限性或缺乏字符级训练数据联系起来。在本文中，我们进行了一项实验研究，以评估 LLM 在计数字母时的错误与 1) 单词及其成分在训练数据集中的频率和 2) 计数操作的复杂性之间的关系。我们通过评估大量单词的代表性模型组，对 LLM 在计数字母出现次数时的错误进行了全面分析。结果显示，所评估的模型存在许多一致的趋势：1) 模型能够识别字母，但不能计数； 2）单词和单词中标记的频率对 LLM 错误没有显著影响；3）字母频率与错误呈正相关，字母频率越高，计数错误就越多；4）错误与单词中字母或标记的数量呈很强的相关性；5）最强的相关性与计数大于 1 的字母数量有关，大多数模型无法正确计算字母出现两次以上的单词。</li>
</ul>

<h3>Title: KRAIL: A Knowledge-Driven Framework for Base Human Reliability Analysis Integrating IDHEAS and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Xiao, Peng Chen, Ben Qi, Hongru Zhao, Jingang Liang, Jiejuan Tong, Haitao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18627">https://arxiv.org/abs/2412.18627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18627">https://arxiv.org/pdf/2412.18627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18627]] KRAIL: A Knowledge-Driven Framework for Base Human Reliability Analysis Integrating IDHEAS and Large Language Models(https://arxiv.org/abs/2412.18627)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Human reliability analysis (HRA) is crucial for evaluating and improving the safety of complex systems. Recent efforts have focused on estimating human error probability (HEP), but existing methods often rely heavily on expert knowledge,which can be subjective and time-consuming. Inspired by the success of large language models (LLMs) in natural language processing, this paper introduces a novel two-stage framework for knowledge-driven reliability analysis, integrating IDHEAS and LLMs (KRAIL). This innovative framework enables the semi-automated computation of base HEP values. Additionally, knowledge graphs are utilized as a form of retrieval-augmented generation (RAG) for enhancing the framework' s capability to retrieve and process relevant data efficiently. Experiments are systematically conducted and evaluated on authoritative datasets of human reliability. The experimental results of the proposed methodology demonstrate its superior performance on base HEP estimation under partial information for reliability assessment.</li>
<li><strong>摘要：</strong>人为可靠性分析 (HRA) 对于评估和提高复杂系统的安全性至关重要。最近的努力集中在估计人为错误概率 (HEP)，但现有方法往往严重依赖专家知识，而这些知识可能具有主观性且耗时。受大型语言模型 (LLM) 在自然语言处理中的成功的启发，本文介绍了一种新颖的知识驱动可靠性分析两阶段框架，该框架集成了 IDHEAS 和 LLM (KRAIL)。这个创新框架可以半自动计算基本 HEP 值。此外，知识图谱被用作检索增强生成 (RAG) 的一种形式，以增强框架有效检索和处理相关数据的能力。在权威的人为可靠性数据集上系统地进行实验并进行评估。所提出方法的实验结果表明，它在部分信息下对可靠性评估的基本 HEP 估计具有优异的性能。</li>
</ul>

<h3>Title: DynaGRAG: Improving Language Understanding and Generation through Dynamic Subgraph Representation in Graph Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Karishma Thakrar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18644">https://arxiv.org/abs/2412.18644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18644">https://arxiv.org/pdf/2412.18644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18644]] DynaGRAG: Improving Language Understanding and Generation through Dynamic Subgraph Representation in Graph Retrieval-Augmented Generation(https://arxiv.org/abs/2412.18644)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to enhance language understanding and generation by leveraging external knowledge. However, effectively capturing and integrating the rich semantic information present in textual and structured data remains a challenge. To address this, a novel GRAG framework is proposed to focus on enhancing subgraph representation and diversity within the knowledge graph. By improving graph density, capturing entity and relation information more effectively, and dynamically prioritizing relevant and diverse subgraphs, the proposed approach enables a more comprehensive understanding of the underlying semantic structure. This is achieved through a combination of de-duplication processes, two-step mean pooling of embeddings, query-aware retrieval considering unique nodes, and a Dynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph Convolutional Networks (GCNs) and Large Language Models (LLMs) through hard prompting further enhances the learning of rich node and edge representations while preserving the hierarchical subgraph structure. Experimental results on multiple benchmark datasets demonstrate the effectiveness of the proposed GRAG framework, showcasing the significance of enhanced subgraph representation and diversity for improved language understanding and generation.</li>
<li><strong>摘要：</strong>图检索增强生成 (GRAG 或 Graph RAG) 架构旨在通过利用外部知识来增强语言理解和生成。然而，有效地捕获和集成文本和结构化数据中存在的丰富语义信息仍然是一个挑战。为了解决这个问题，提出了一种新颖的 GRAG 框架，专注于增强知识图谱中的子图表示和多样性。通过提高图密度、更有效地捕获实体和关系信息以及动态优先考虑相关和多样化的子图，所提出的方法可以更全面地理解底层语义结构。这是通过重复数据删除过程、嵌入的两步均值池化、考虑唯一节点的查询感知检索和动态相似性感知 BFS (DSA-BFS) 遍历算法的组合实现的。通过硬提示集成图卷积网络 (GCN) 和大型语言模型 (LLM) 进一步增强了对丰富节点和边缘表示的学习，同时保留了分层子图结构。在多个基准数据集上的实验结果证明了所提出的 GRAG 框架的有效性，展示了增强子图表示和多样性对于提高语言理解和生成的重要性。</li>
</ul>

<h3>Title: From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Ratnesh Kumar Joshi, Sagnik Sengupta, Asif Ekbal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18672">https://arxiv.org/abs/2412.18672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18672">https://arxiv.org/pdf/2412.18672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18672]] From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge Graphs(https://arxiv.org/abs/2412.18672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Hallucination, a persistent challenge plaguing language models, undermines their efficacy and trustworthiness in various natural language processing endeavors by generating responses that deviate from factual accuracy or coherence. This paper addresses language model hallucination by integrating curated knowledge graph (KG) triples to anchor responses in empirical data. We meticulously select and integrate relevant KG triples tailored to specific contexts, enhancing factual grounding and alignment with input. Our contribution involves constructing a comprehensive KG repository from Wikipedia and refining data to spotlight essential information for model training. By imbuing language models with access to this curated knowledge, we aim to generate both linguistically fluent responses and deeply rooted in factual accuracy and context relevance. This integration mitigates hallucinations by providing a robust foundation of information, enabling models to draw upon a rich reservoir of factual data during response generation. Experimental evaluations demonstrate the effectiveness of multiple approaches in reducing hallucinatory responses, underscoring the role of curated knowledge graphs in improving the reliability and trustworthiness of language model outputs.</li>
<li><strong>摘要：</strong>幻觉是困扰语言模型的一个长期挑战，它通过生成偏离事实准确性或连贯性的响应来破坏语言模型在各种自然语言处理工作中的有效性和可信度。本文通过集成精选的知识图谱 (KG) 三元组来锚定经验数据中的响应，解决了语言模型幻觉问题。我们精心选择并集成针对特定情境的相关 KG 三元组，增强了事实依据和与输入的一致性。我们的贡献包括从维基百科构建一个全面的 KG 存储库并优化数据以突出显示模型训练的重要信息。通过让语言模型能够访问这些精选知识，我们旨在生成既语言流畅又深深植根于事实准确性和情境相关性的响应。这种集成通过提供强大的信息基础来缓解幻觉，使模型能够在响应生成过程中利用丰富的事实数据。实验评估证明了多种方法在减少幻觉反应方面的有效性，强调了精选知识图谱在提高语言模型输出的可靠性和可信度方面的作用。</li>
</ul>

<h3>Title: AgreeMate: Teaching LLMs to Haggle</h3>
<ul>
<li><strong>Authors: </strong>Ainesh Chatterjee, Samuel Miller, Nithin Parepally</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18690">https://arxiv.org/abs/2412.18690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18690">https://arxiv.org/pdf/2412.18690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18690]] AgreeMate: Teaching LLMs to Haggle(https://arxiv.org/abs/2412.18690)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>We introduce AgreeMate, a framework for training Large Language Models (LLMs) to perform strategic price negotiations through natural language. We apply recent advances to a negotiation setting where two agents (i.e. buyer or seller) use natural language to bargain on goods using coarse actions. Specifically, we present the performance of Large Language Models when used as agents within a decoupled (modular) bargaining architecture. We demonstrate that using prompt engineering, fine-tuning, and chain-of-thought prompting enhances model performance, as defined by novel metrics. We use attention probing to show model attention to semantic relationships between tokens during negotiations.</li>
<li><strong>摘要：</strong>我们引入了 AgreeMate，这是一个用于训练大型语言模型 (LLM) 的框架，用于通过自然语言进行战略价格谈判。我们将最新进展应用于谈判环境，其中两个代理（即买方或卖方）使用自然语言通过粗略操作就商品进行讨价还价。具体来说，我们展示了大型语言模型在解耦（模块化）讨价还价架构中用作代理时的性能。我们证明，使用提示工程、微调和思路链提示可以提高模型性能，如新指标所定义。我们使用注意力探测来显示模型在谈判期间对标记之间语义关系的关注。</li>
</ul>

<h3>Title: CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era</h3>
<ul>
<li><strong>Authors: </strong>Yanlin Feng, Simone Papicchio, Sajjadur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18702">https://arxiv.org/abs/2412.18702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18702">https://arxiv.org/pdf/2412.18702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18702]] CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era(https://arxiv.org/abs/2412.18702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system (edge et al., 2024). Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping relation types and lack of normalization. As a solution, we propose property graph views on top of the underlying RDF graph that can be efficiently queried by LLMs using Cypher. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.</li>
<li><strong>摘要：</strong>从图数据中检索对于使用开放领域知识和私有企业数据扩充大型语言模型 (LLM) 至关重要，它也是最近的 GraphRAG 系统 (edge et al., 2024) 中的一个关键组件。尽管对知识图谱和知识库问答进行了数十年的研究，但领先的 LLM 框架（例如 Langchain 和 LlamaIndex）对从 Wikidata 等现代百科全书知识图谱中检索的支持却非常有限。在本文中，我们分析了根本原因，并提出现代 RDF 知识图谱（例如 Wikidata、Freebase）对于 LLM 来说效率较低，因为其模式过大，远远超出了典型的 LLM 上下文窗口、使用了资源标识符、关系类型重叠以及缺乏规范化。作为一种解决方案，我们提出了在底层 RDF 图之上的属性图视图，LLM 可以使用 Cypher 有效地查询这些视图。我们在 Wikidata 上实现了这个想法，并推出了 CypherBench，这是第一个基准测试，包含 11 个大规模、多领域属性图，包含 780 万个实体和 10,000 多个问题。为了实现这一目标，我们解决了几个关键挑战，包括开发 RDF 到属性图的转换引擎、创建文本到 Cypher 任务生成的系统管道以及设计新的评估指标。</li>
</ul>

<h3>Title: Multiple References with Meaningful Variations Improve Literary Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Si Wu, John Wieting, David A. Smith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18707">https://arxiv.org/abs/2412.18707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18707">https://arxiv.org/pdf/2412.18707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18707]] Multiple References with Meaningful Variations Improve Literary Machine Translation(https://arxiv.org/abs/2412.18707)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>While a source sentence can be translated in many ways, most machine translation (MT) models are trained with only a single reference. Previous work has shown that using synthetic paraphrases can improve MT. This paper investigates best practices for employing multiple references by analyzing the semantic similarity among different English translations of world literature in the Par3 dataset. We classify the semantic similarity between paraphrases into three groups: low, medium, and high, and fine-tune two different LLMs (mT5-large and LLaMA-2-7B) for downstream MT tasks. Across different models, holding the total training instances constant, single-reference but more source texts only marginally outperforms multiple-reference with half of the source texts. Moreover, using paraphrases of medium and high semantic similarity outperforms an unfiltered dataset (+BLEU 0.3-0.5, +COMET 0.2-0.9, +chrF++ 0.25-0.32). Our code is publicly available on GitHub.</li>
<li><strong>摘要：</strong>虽然源语句可以以多种方式翻译，但大多数机器翻译 (MT) 模型仅使用单个参考进行训练。先前的研究表明，使用合成释义可以改善机器翻译。本文通过分析 Par3 数据集中世界文学不同英文翻译之间的语义相似性，研究了使用多个参考的最佳实践。我们将释义之间的语义相似性分为三组：低、中、高，并对两个不同的 LLM（mT5-large 和 LLaMA-2-7B）进行微调以用于下游机器翻译任务。在不同的模型中，保持总训练实例不变，单参考但更多源文本仅略优于具有一半源文本的多参考。此外，使用中高语义相似度的释义优于未过滤的数据集（+BLEU 0.3-0.5、+COMET 0.2-0.9、+chrF++ 0.25-0.32）。我们的代码在 GitHub 上公开可用。</li>
</ul>

<h3>Title: Using Large Language Models for Automated Grading of Student Writing about Science</h3>
<ul>
<li><strong>Authors: </strong>Chris Impey, Matthew Wenger, Nikhil Garuda, Shahriar Golchin, Sarah Stamer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18719">https://arxiv.org/abs/2412.18719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18719">https://arxiv.org/pdf/2412.18719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18719]] Using Large Language Models for Automated Grading of Student Writing about Science(https://arxiv.org/abs/2412.18719)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.</li>
<li><strong>摘要：</strong>评估大班正式或非正式学习者的写作是一项重大挑战。因此，大多数大班，尤其是科学课，都依赖于客观评估工具，例如只有一个正确答案的多项选择测验。人工智能的快速发展使得使用大型语言模型 (LLM) 来评估学生写作成为可能。使用 GPT-4 进行了一项实验，以确定基于 LLM 的机器学习方法在评估天文学主题的简短写作作业时是否可以达到或超过教师评分的可靠性。观众包括 Coursera 提供的三门大规模开放在线课程 (MOOC) 的成人学习者。一门课程是天文学，第二门是天体生物学，第三门是天文学的历史和哲学。结果也适用于大学环境中的非科学专业学生，因为评估的内容和模式相似。数据包括三门课程中 120 名学生对 12 个问题的回答。 GPT-4 获得了这三门课程的总成绩、模范答案和评分标准。除了评估 LLM 再现教师成绩的可靠性之外，LLM 还负责生成自己的评分标准。总体而言，无论是总体还是个别学生，LLM 都比同行评分更可靠，并且与所有三门在线课程的教师成绩大致匹配。这意味着 LLM 可能很快就会用于对学生科学写作进行自动化、可靠和可扩展的评分。</li>
</ul>

<h3>Title: Optimizing Large Language Models with an Enhanced LoRA Fine-Tuning Algorithm for Efficiency and Robustness in NLP Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Hu, Xiaoxuan Liao, Jia Gao, Zhen Qi, Hongye Zheng, Chihang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18729">https://arxiv.org/abs/2412.18729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18729">https://arxiv.org/pdf/2412.18729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18729]] Optimizing Large Language Models with an Enhanced LoRA Fine-Tuning Algorithm for Efficiency and Robustness in NLP Tasks(https://arxiv.org/abs/2412.18729)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This study proposes a large language model optimization method based on the improved LoRA fine-tuning algorithm, aiming to improve the accuracy and computational efficiency of the model in natural language processing tasks. We fine-tune the large language model through a low-rank adaptation strategy, which significantly reduces the consumption of computing resources while maintaining the powerful capabilities of the pre-trained model. The experiment uses the QQP task as the evaluation scenario. The results show that the improved LoRA algorithm shows significant improvements in accuracy, F1 score, and MCC compared with traditional models such as BERT, Roberta, T5, and GPT-4. In particular, in terms of F1 score and MCC, our model shows stronger robustness and discrimination ability, which proves the potential of the improved LoRA algorithm in fine-tuning large-scale pre-trained models. In addition, this paper also discusses the application prospects of the improved LoRA algorithm in other natural language processing tasks, emphasizing its advantages in multi-task learning and scenarios with limited computing resources. Future research can further optimize the LoRA fine-tuning strategy and expand its application in larger-scale pre-trained models to improve the generalization ability and task adaptability of the model.</li>
<li><strong>摘要：</strong>本研究提出了一种基于改进的LoRA微调算法的大型语言模型优化方法，旨在提高模型在自然语言处理任务中的准确率和计算效率。我们通过低秩自适应策略对大型语言模型进行微调，在保持预训练模型强大能力的同时，显著降低了计算资源的消耗。实验以QQP任务为评估场景。结果表明，与BERT、Roberta、T5、GPT-4等传统模型相比，改进的LoRA算法在准确率、F1分数和MCC方面均有显著提升。特别是在F1分数和MCC方面，我们的模型表现出了更强的鲁棒性和判别能力，证明了改进的LoRA算法在微调大规模预训练模型方面的潜力。此外，本文还讨论了改进的LoRA算法在其他自然语言处理任务中的应用前景，强调其在多任务学习和计算资源有限的场景中的优势。未来的研究可以进一步优化LoRA微调策略并扩展其在更大规模预训练模型中的应用，以提高模型的泛化能力和任务适应性。</li>
</ul>

<h3>Title: Improving Generated and Retrieved Knowledge Combination Through Zero-shot Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinkai Du, Quanjie Han, Chao Lv, Yan Liu, Yalin Sun, Hao Shu, Hongbo Shan, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18800">https://arxiv.org/abs/2412.18800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18800">https://arxiv.org/pdf/2412.18800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18800]] Improving Generated and Retrieved Knowledge Combination Through Zero-shot Generation(https://arxiv.org/abs/2412.18800)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Open-domain Question Answering (QA) has garnered substantial interest by combining the advantages of faithfully retrieved passages and relevant passages generated through Large Language Models (LLMs). However, there is a lack of definitive labels available to pair these sources of knowledge. In order to address this issue, we propose an unsupervised and simple framework called Bi-Reranking for Merging Generated and Retrieved Knowledge (BRMGR), which utilizes re-ranking methods for both retrieved passages and LLM-generated passages. We pair the two types of passages using two separate re-ranking methods and then combine them through greedy matching. We demonstrate that BRMGR is equivalent to employing a bipartite matching loss when assigning each retrieved passage with a corresponding LLM-generated passage. The application of our model yielded experimental results from three datasets, improving their performance by +1.7 and +1.6 on NQ and WebQ datasets, respectively, and obtaining comparable result on TriviaQA dataset when compared to competitive baselines.</li>
<li><strong>摘要：</strong>开放域问答 (QA) 结合了忠实检索的段落和通过大型语言模型 (LLM) 生成的相关段落的优势，引起了广泛关注。然而，缺乏可用于将这些知识源配对的明确标签。为了解决这个问题，我们提出了一个无监督的简单框架，称为用于合并生成和检索知识的双向重排序 (BRMGR)，它对检索到的段落和 LLM 生成的段落都采用重排序方法。我们使用两种单独的重排序方法将这两种类型的段落配对，然后通过贪婪匹配将它们组合起来。我们证明，在将每个检索到的段落分配给相应的 LLM 生成的段落时，BRMGR 相当于使用二分匹配损失。我们的模型的应用从三个数据集产生了实验结果，在 NQ 和 WebQ 数据集上分别将它们的性能提高了 +1.7 和 +1.6，并且在 TriviaQA 数据集上获得了与竞争基线相当的结果。</li>
</ul>

<h3>Title: DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer Scaling Factor Search</h3>
<ul>
<li><strong>Authors: </strong>Lei Yang, Shaoyang Xu, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18811">https://arxiv.org/abs/2412.18811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18811">https://arxiv.org/pdf/2412.18811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18811]] DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer Scaling Factor Search(https://arxiv.org/abs/2412.18811)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) based on the Transformer architecture usually have their context length limited due to the high training cost. Recent advancements extend the context window by adjusting the scaling factors of RoPE and fine-tuning. However, suboptimal initialization of these factors results in increased fine-tuning costs and reduced performance at target length. To address these challenges, we propose an innovative RoPE-based fine-tuning framework that diverges from conventional scaling factors search. Specifically, we present a Divide-and-Conquer Incremental Search (DCIS) algorithm that strategically determines the better scaling factors. Further fine-tuning with the identified scaling factors effectively extends the context window of LLMs. Empirical results demonstrate that our methodology not only mitigates performance decay at extended target lengths but also allows the model to fine-tune on short contexts and generalize to long contexts, thereby reducing the cost of fine-tuning. The scaling factors obtained through DCIS can even perform effectively without fine-tuning. Further analysis of the search space reveals that DCIS achieves twice the search efficiency compared to other methods. We also examine the impact of the non-strictly increasing scaling factors utilized in DCIS and evaluate the general capabilities of LLMs across various context lengths.</li>
<li><strong>摘要：</strong>基于 Transformer 架构的大型语言模型 (LLM) 通常由于训练成本高而导致上下文长度受限。最近的进展通过调整 RoPE 和微调的缩放因子来扩展上下文窗口。然而，这些因子的次优初始化会导致微调成本增加，并降低目标长度下的性能。为了应对这些挑战，我们提出了一种创新的基于 RoPE 的微调框架，该框架不同于传统的缩放因子搜索。具体来说，我们提出了一种分而治之的增量搜索 (DCIS) 算法，该算法可以策略性地确定更好的缩放因子。使用确定的缩放因子进行进一步微调可以有效地扩展 LLM 的上下文窗口。实证结果表明，我们的方法不仅可以减轻扩展目标长度下的性能下降，而且还允许模型在短上下文上进行微调并推广到长上下文，从而降低微调成本。通过 DCIS 获得的缩放因子甚至可以在没有微调的情况下有效执行。对搜索空间的进一步分析表明，与其他方法相比，DCIS 实现了两倍的搜索效率。我们还研究了 DCIS 中使用的非严格增加的缩放因子的影响，并评估了不同上下文长度的 LLM 的一般能力。</li>
</ul>

<h3>Title: RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting</h3>
<ul>
<li><strong>Authors: </strong>Yilei Jiang, Yingshui Tan, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18826">https://arxiv.org/abs/2412.18826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18826">https://arxiv.org/pdf/2412.18826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18826]] RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting(https://arxiv.org/abs/2412.18826)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) have made remarkable progress in vision-language reasoning, they are also more susceptible to producing harmful content compared to models that focus solely on text. Existing defensive prompting techniques rely on a static, unified safety guideline that fails to account for the specific risks inherent in different multimodal contexts. To address these limitations, we propose RapGuard, a novel framework that uses multimodal chain-of-thought reasoning to dynamically generate scenario-specific safety prompts. RapGuard enhances safety by adapting its prompts to the unique risks of each input, effectively mitigating harmful outputs while maintaining high performance on benign tasks. Our experimental results across multiple MLLM benchmarks demonstrate that RapGuard achieves state-of-the-art safety performance, significantly reducing harmful content without degrading the quality of responses.</li>
<li><strong>摘要：</strong>虽然多模态大型语言模型 (MLLM) 在视觉语言推理方面取得了显著进展，但与仅关注文本的模型相比，它们也更容易产生有害内容。现有的防御性提示技术依赖于静态、统一的安全指南，而该指南未能考虑到不同多模态上下文中固有的特定风险。为了解决这些限制，我们提出了 RapGuard，这是一个新颖的框架，它使用多模态思维链推理来动态生成特定场景的安全提示。RapGuard 通过根据每个输入的独特风险调整提示来增强安全性，有效减轻有害输出，同时在良性任务上保持高性能。我们在多个 MLLM 基准上的实验结果表明，RapGuard 实现了最先进的安全性能，在不降低响应质量的情况下显着减少了有害内容。</li>
</ul>

<h3>Title: Bootstrap Your Own Context Length</h3>
<ul>
<li><strong>Authors: </strong>Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18860">https://arxiv.org/abs/2412.18860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18860">https://arxiv.org/pdf/2412.18860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18860]] Bootstrap Your Own Context Length(https://arxiv.org/abs/2412.18860)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.</li>
<li><strong>摘要：</strong>我们引入了一种引导方法，通过仅利用长上下文语言模型的短上下文功能来训练长上下文语言模型。我们的方法利用简单的代理工作流来合成各种长上下文指令调整数据，从而无需手动收集和注释数据。所提出的数据合成工作流只需要一个短上下文语言模型、一个文本检索器和一个文档集合，所有这些都可以在开源生态系统中轻松访问。随后，使用合成数据对语言模型进行微调以扩展其上下文长度。通过这种方式，我们通过引导过程有效地将语言模型的短上下文功能转移到长上下文场景。我们对开源 Llama-3 系列模型进行了实验，并证明我们的方法可以成功地将上下文长度扩展到最多 1M 个标记，从而在各种基准测试中实现卓越的性能。</li>
</ul>

<h3>Title: Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Meltem Aksoy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18863">https://arxiv.org/abs/2412.18863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18863">https://arxiv.org/pdf/2412.18863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18863]] Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models(https://arxiv.org/abs/2412.18863)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become integral tools in diverse domains, yet their moral reasoning capabilities across cultural and linguistic contexts remain underexplored. This study investigates whether multilingual LLMs, such as GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally specific moral values or impose dominant moral norms, particularly those rooted in English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight languages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and Russian, the study analyzes the models' adherence to six core moral foundations: care, equality, proportionality, loyalty, authority, and purity. The results reveal significant cultural and linguistic variability, challenging the assumption of universal moral consistency in LLMs. Although some models demonstrate adaptability to diverse contexts, others exhibit biases influenced by the composition of the training data. These findings underscore the need for culturally inclusive model development to improve fairness and trust in multilingual AI systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为不同领域不可或缺的工具，但它们在不同文化和语言背景下的道德推理能力仍未得到充分探索。本研究调查了多语言 LLM（例如 GPT-3.5-Turbo、GPT-4o-mini、Llama 3.1 和 MistralNeMo）是否反映了特定文化的道德价值观或强加了主导的道德规范，尤其是那些植根于英语的道德规范。该研究使用阿拉伯语、波斯语、英语、西班牙语、日语、中文、法语和俄语八种语言的最新道德基础问卷 (MFQ-2)，分析了这些模型对六个核心道德基础的遵守情况：关怀、平等、相称、忠诚、权威和纯洁。结果揭示了显著的文化和语言差异，挑战了 LLM 中普遍道德一致性的假设。尽管一些模型表现出对不同环境的适应性，但其他模型表现出受训练数据组成影响的偏见。这些发现强调了开发文化包容性模型的必要性，以提高多语言人工智能系统的公平性和信任度。</li>
</ul>

<h3>Title: HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18925">https://arxiv.org/abs/2412.18925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18925">https://arxiv.org/pdf/2412.18925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18925]] HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs(https://arxiv.org/abs/2412.18925)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.</li>
<li><strong>摘要：</strong>OpenAI o1 的突破凸显了通过增强推理能力来改进 LLM 的潜力。然而，大多数推理研究都集中在数学任务上，而医学等领域尚未得到充分探索。医学领域虽然不同于数学，但考虑到医疗保健的高标准，它也需要强大的推理能力来提供可靠的答案。然而，与数学不同，验证医学推理具有挑战性。为了解决这个问题，我们提出了可验证的医学问题，并使用医学验证器来检查模型输出的正确性。这种可验证的特性通过两阶段方法推动了医学推理的进步：（1）使用验证器指导搜索复杂的推理轨迹以微调 LLM，（2）应用基于验证器的奖励来进一步增强复杂推理。最后，我们推出了 HuatuoGPT-o1，这是一种能够进行复杂推理的医学 LLM，它仅使用 40K 个可验证问题就超越了一般和医学特定的基线。实验表明，复杂推理可以提高医学问题的解决能力，并且从强化学习中获益更多。我们希望我们的方法能够促进医学和其他专业领域的推理进步。</li>
</ul>

<h3>Title: Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference</h3>
<ul>
<li><strong>Authors: </strong>Libo Zhang, Zhaoning Zhang, Baizhou Xu, Songzhu Mei, Dongsheng Li (1) ((1) National University of Defense Technology, Changsha, China)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18934">https://arxiv.org/abs/2412.18934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18934">https://arxiv.org/pdf/2412.18934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18934]] Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference(https://arxiv.org/abs/2412.18934)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Due to the high resource demands of Large Language Models (LLMs), achieving widespread deployment on consumer-grade devices presents significant challenges. Typically, personal or consumer-grade devices, including servers configured prior to the era of large-scale models, generally have relatively weak GPUs and relatively strong CPUs. However, most current methods primarily depend on GPUs for computation. Therefore, we propose Dovetail, an approach that deploys the draft model on the GPU to generate draft tokens while allowing the target model to perform parallel verification on the CPU, thereby improving the utilization of all available hardware resources and occupying less inter-device communication bandwidth. Accordingly, we have redesigned the draft model to better align with heterogeneous hardware characteristics. To this end, we implemented several optimizations: reducing the number of draft tokens to mitigate latency in parallel verification, increasing the depth of the draft model to enhance its predictive capacity, and introducing DGF (Dynamic Gating Fusion) to improve the integration of features and token embeddings. In the HumanEval benchmark, Dovetail achieved an inference speed of 5.86 tokens per second for LLaMA2-Chat-7B using 3GB of VRAM, representing an approximately 2.77x improvement over CPU-only inference. Furthermore, the inference speed was increased to 8 tokens per second when utilizing 7GB of VRAM.</li>
<li><strong>摘要：</strong>由于大型语言模型（LLM）对资源的要求较高，在消费级设备上实现大规模部署面临巨大挑战。通常，个人或消费级设备（包括大规模模型时代之前配置的服务器）的 GPU 普遍较弱，而 CPU 相对较强。然而，目前大多数方法主要依赖 GPU 进行计算。因此，我们提出了 Dovetail 方法，该方法将草稿模型部署在 GPU 上生成草稿 token，同时让目标模型在 CPU 上进行并行验证，从而提高所有可用硬件资源的利用率并减少设备间通信带宽的占用。为此，我们重新设计了草稿模型，使其更好地适应异构硬件特性。为此，我们实现了多项优化：减少草稿 token 数量以缓解并行验证的延迟、增加草稿模型的深度以增强其预测能力、引入 DGF（动态门控融合）以改善特征和 token 嵌入的集成。在 HumanEval 基准测试中，Dovetail 使用 3GB VRAM 实现了 LLaMA2-Chat-7B 每秒 5.86 个令牌的推理速度，比仅使用 CPU 的推理速度提高了约 2.77 倍。此外，使用 7GB VRAM 时，推理速度提高到每秒 8 个令牌。</li>
</ul>

<h3>Title: MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Zuo, Yirui Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18947">https://arxiv.org/abs/2412.18947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18947">https://arxiv.org/pdf/2412.18947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18947]] MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models(https://arxiv.org/abs/2412.18947)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.</li>
<li><strong>摘要：</strong>医学大型语言模型 (MLLM) 已在医疗保健应用中展现出潜力，但它们容易产生幻觉——产生医学上难以置信或不准确的信息——给患者护理带来了巨大风险。本文介绍了 MedHallBench，这是一个用于评估和缓解 MLLM 中幻觉的综合基准框架。我们的方法将专家验证的医疗案例场景与已建立的医疗数据库相结合，以创建一个强大的评估数据集。该框架采用了一种复杂的测量系统，将自动 ACHMI（医学成像中的自动字幕幻觉测量）评分与严格的临床专家评估相结合，并利用强化学习方法实现自动注释。通过专门为医疗应用设计的优化的强化学习人类反馈 (RLHF) 训练管道，MedHallBench 能够在保持严格的准确性标准的同时，对不同临床环境中的 MLLM 进行全面评估。我们进行了涉及各种模型的比较实验，利用基准为广泛采用的大型语言模型 (LLM) 建立了基线。我们的研究结果表明，与传统指标相比，ACHMI 可以更细致地了解幻觉的影响，从而凸显其在幻觉评估方面的优势。这项研究建立了一个基础框架，以增强 MLLM 在医疗环境中的可靠性，并提出了可行的策略来应对医疗应用中 AI 幻觉的关键挑战。</li>
</ul>

<h3>Title: Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Ruixi Lin, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19018">https://arxiv.org/abs/2412.19018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19018">https://arxiv.org/pdf/2412.19018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19018]] Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability(https://arxiv.org/abs/2412.19018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In-context learning, which allows large language models to perform diverse tasks with a few demonstrations, is found to have imbalanced per-class prediction accuracy on multi-class text classification. Although notable output correction methods have been developed to tackle the issue and simultaneously improve downstream prediction accuracy, they may fail to answer the core interpretability challenges: why and which certain classes need corrections, and more importantly, a tailored correction for per-sample, per-class's probability. To address such interpretability gaps, we first find that the imbalance arises from certain classes consistently receiving high ICL output probabilities, whereas others receiving lower or mixed ranges, so the former is more frequently chosen, resulting in higher accuracy; more crucially, we find that these ranges have significantly varying degrees of influence on the accuracy bias, highlighting the need for precise, interpretable probability corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule Optimization based Debiasing method, that (1) detects which classes need corrections, and (2) for each correction-needed class, detects its probability ranges and applies asymmetric amplifications or reductions to correct them interpretably. Notably, across seven benchmark datasets, FuRud reduces the pairwise class accuracy bias (COBias) by more than half (56%), while achieving a relative increase of 21% in accuracy, outperforming state-of-the-art debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as 10 optimization examples. Furthermore, FuRud can work for prompt formats that lead to highly skewed predictions. For example, FuRud greatly improves ICL outputs which use letter options, with 44% relative accuracy increase and 54% relative COBias reduction.</li>
<li><strong>摘要：</strong>上下文学习允许大型语言模型通过少量演示执行各种任务，但在多类文本分类中，每个类别的预测准确率不平衡。尽管已经开发出值得注意的输出校正方法来解决这个问题，同时提高下游预测准确率，但它们可能无法回答核心的可解释性挑战：为什么以及哪些特定类别需要校正，更重要的是，针对每个样本、每个类别的概率进行量身定制的校正。为了解决这种可解释性差距，我们首先发现不平衡源于某些类别始终获得较高的 ICL 输出概率，而其他类别获得较低或混合的范围，因此前者被更频繁地选择，从而获得更高的准确率；更重要的是，我们发现这些范围对准确率偏差的影响程度差异很大，凸显了对按范围进行精确、可解释的概率校正的需求。受此启发，我们提出了一种基于模糊规则优化的去偏方法 FuRud，该方法 (1) 检测哪些类别需要校正，(2) 对于每个需要校正的类别，检测其概率范围并应用不对称放大或缩小以可解释地校正它们。值得注意的是，在七个基准数据集中，FuRud 将成对类别准确度偏差 (COBias) 降低了一半以上 (56%)，同时实现了准确度的相对提高 21%，优于最先进的去偏方法。此外，FuRud 只需 10 个优化示例即可优化下游任务。此外，FuRud 可以用于导致高度偏斜预测的提示格式。例如，FuRud 极大地改善了使用字母选项的 ICL 输出，相对准确度提高了 44%，相对 COBias 降低了 54%。</li>
</ul>

<h3>Title: Advancing LLM detection in the ALTA 2024 Shared Task: Techniques and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Dima Galat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19076">https://arxiv.org/abs/2412.19076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19076">https://arxiv.org/pdf/2412.19076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19076]] Advancing LLM detection in the ALTA 2024 Shared Task: Techniques and Analysis(https://arxiv.org/abs/2412.19076)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>The recent proliferation of AI-generated content has prompted significant interest in developing reliable detection methods. This study explores techniques for identifying AI-generated text through sentence-level evaluation within hybrid articles. Our findings indicate that ChatGPT-3.5 Turbo exhibits distinct, repetitive probability patterns that enable consistent in-domain detection. Empirical tests show that minor textual modifications, such as rewording, have minimal impact on detection accuracy. These results provide valuable insights for advancing AI detection methodologies, offering a pathway toward robust solutions to address the complexities of synthetic text identification.</li>
<li><strong>摘要：</strong>最近，人工智能生成内容的激增引起了人们对开发可靠检测方法的极大兴趣。本研究探索了通过混合文章中的句子级评估来识别人工智能生成文本的技术。我们的研究结果表明，ChatGPT-3.5 Turbo 表现出独特的重复概率模式，可实现一致的域内检测。实证测试表明，诸如改写之类的微小文本修改对检测准确性的影响微乎其微。这些结果为推进人工智能检测方法提供了宝贵的见解，为解决合成文本识别的复杂性提供了一条通往强大解决方案的途径。</li>
</ul>

<h3>Title: "I've Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen Entities</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19102">https://arxiv.org/abs/2412.19102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19102">https://arxiv.org/pdf/2412.19102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19102]] "I've Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen Entities(https://arxiv.org/abs/2412.19102)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Spoken named entity recognition (NER) aims to identify named entities from speech, playing an important role in speech processing. New named entities appear every day, however, annotating their Spoken NER data is costly. In this paper, we demonstrate that existing Spoken NER systems perform poorly when dealing with previously unseen named entities. To tackle this challenge, we propose a method for generating Spoken NER data based on a named entity dictionary (NED) to reduce costs. Specifically, we first use a large language model (LLM) to generate sentences from the sampled named entities and then use a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce a noise metric to filter out noisy data. To evaluate our approach, we release a novel Spoken NER benchmark along with a corresponding NED containing 8,853 entities. Experiment results show that our method achieves state-of-the-art (SOTA) performance in the in-domain, zero-shot domain adaptation, and fully zero-shot settings. Our data will be available at this https URL.</li>
<li><strong>摘要：</strong>口语命名实体识别 (NER) 旨在从语音中识别命名实体，在语音处理中起着重要作用。每天都会出现新的命名实体，但是，注释它们的口语 NER 数据成本高昂。在本文中，我们证明现有的口语 NER 系统在处理以前未见过的命名实体时表现不佳。为了应对这一挑战，我们提出了一种基于命名实体词典 (NED) 生成口语 NER 数据的方法，以降低成本。具体来说，我们首先使用大型语言模型 (LLM) 从采样的命名实体生成句子，然后使用文本转语音 (TTS) 系统生成语音。此外，我们引入了噪声度量来过滤掉噪声数据。为了评估我们的方法，我们发布了一个新颖的口语 NER 基准以及包含 8,853 个实体的相应 NED。实验结果表明，我们的方法在域内、零样本域自适应和完全零样本设置中实现了最佳 (SOTA) 性能。我们的数据将在此 https URL 上提供。</li>
</ul>

<h3>Title: SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values</h3>
<ul>
<li><strong>Authors: </strong>Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19113">https://arxiv.org/abs/2412.19113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19113">https://arxiv.org/pdf/2412.19113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19113]] SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values(https://arxiv.org/abs/2412.19113)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Missing value is a critical issue in data science, significantly impacting the reliability of analyses and predictions. Missing value imputation (MVI) is a longstanding problem because it highly relies on domain knowledge. Large language models (LLMs) have emerged as a promising tool for data cleaning, including MVI for tabular data, offering advanced capabilities for understanding and generating content. However, despite their promise, existing LLM techniques such as in-context learning and Chain-of-Thought (CoT) often fall short in guiding LLMs to perform complex reasoning for MVI, particularly when imputing derived missing values, which require mathematical formulas and data relationships across rows and columns. This gap underscores the need for further advancements in LLM methodologies to enhance their reasoning capabilities for more reliable imputation outcomes. To fill this gap, we propose SketchFill, a novel sketch-based method to guide LLMs in generating accurate formulas to impute missing numerical values. Our experimental results demonstrate that SketchFill significantly outperforms state-of-the-art methods, achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher accuracy than MetaGPT. This sets a new standard for automated data cleaning and advances the field of MVI for numerical values.</li>
<li><strong>摘要：</strong>缺失值是数据科学中的一个关键问题，对分析和预测的可靠性有重大影响。缺失值插补 (MVI) 是一个长期存在的问题，因为它高度依赖于领域知识。大型语言模型 (LLM) 已成为一种有前途的数据清理工具，包括用于表格数据的 MVI，可提供理解和生成内容的高级功能。然而，尽管前景光明，现有的 LLM 技术（例如上下文学习和思维链 (CoT)）往往无法指导 LLM 对 MVI 进行复杂的推理，尤其是在插补派生的缺失值时，这需要跨行和列的数学公式和数据关系。这一差距凸显了 LLM 方法需要进一步改进，以增强其推理能力，从而获得更可靠的插补结果。为了填补这一空白，我们提出了 SketchFill，这是一种基于草图的新颖方法，可指导 LLM 生成准确的公式来插补缺失的数值。我们的实验结果表明，SketchFill 的表现明显优于最先进的方法，其准确率比基于 CoT 的方法高出 56.2%，比 MetaGPT 高出 78.8%。这为自动数据清理树立了新标准，并推动了数值 MVI 领域的发展。</li>
</ul>

<h3>Title: SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19140">https://arxiv.org/abs/2412.19140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19140">https://arxiv.org/pdf/2412.19140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19140]] SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis(https://arxiv.org/abs/2412.19140)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In recent years, fine-grained sentiment analysis in finance has gained significant attention, but the scarcity of entity-level datasets remains a key challenge. To address this, we have constructed the largest English and Chinese financial entity-level sentiment analysis datasets to date. Building on this foundation, we propose a novel two-stage sentiment analysis approach called Self-aware In-context Learning Correction (SILC). The first stage involves fine-tuning a base large language model to generate pseudo-labeled data specific to our task. In the second stage, we train a correction model using a GNN-based example retriever, which is informed by the pseudo-labeled data. This two-stage strategy has allowed us to achieve state-of-the-art performance on the newly constructed datasets, advancing the field of financial sentiment analysis. In a case study, we demonstrate the enhanced practical utility of our data and methods in monitoring the cryptocurrency market. Our datasets and code are available at this https URL.</li>
<li><strong>摘要：</strong>近年来，金融领域的细粒度情绪分析备受关注，但实体级数据集的稀缺仍然是一个关键挑战。为了解决这个问题，我们构建了迄今为止最大的中英文金融实体级情绪分析数据集。在此基础上，我们提出了一种新颖的两阶段情绪分析方法，称为自我意识上下文学习校正 (SILC)。第一阶段涉及微调基础大型语言模型以生成特定于我们任务的伪标记数据。在第二阶段，我们使用基于 GNN 的示例检索器训练校正模型，该模型由伪标记数据提供信息。这种两阶段策略使我们能够在新构建的数据集上实现最先进的性能，推动了金融情绪分析领域的发展。在案例研究中，我们展示了我们的数据和方法在监控加密货币市场方面增强的实用性。我们的数据集和代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes</h3>
<ul>
<li><strong>Authors: </strong>Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19260">https://arxiv.org/abs/2412.19260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19260">https://arxiv.org/pdf/2412.19260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19260]] MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes(https://arxiv.org/abs/2412.19260)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Several studies showed that Large Language Models (LLMs) can answer medical questions correctly, even outperforming the average human score in some medical exams. However, to our knowledge, no study has been conducted to assess the ability of language models to validate existing or generated medical text for correctness and consistency. In this paper, we introduce MEDEC (this https URL), the first publicly available benchmark for medical error detection and correction in clinical notes, covering five types of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes from three US hospital systems that were not previously seen by any LLM. The dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen participating systems [Ben Abacha et al., 2024]. In this paper, we describe the data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4, Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and correcting medical errors requiring both medical knowledge and reasoning capabilities. We also conducted a comparative study where two medical doctors performed the same task on the MEDEC test set. The results showed that MEDEC is a sufficiently challenging benchmark to assess the ability of models to validate existing or generated notes and to correct medical errors. We also found that although recent LLMs have a good performance in error detection and correction, they are still outperformed by medical doctors in these tasks. We discuss the potential factors behind this gap, the insights from our experiments, the limitations of current evaluation metrics, and share potential pointers for future research.</li>
<li><strong>摘要：</strong>多项研究表明，大型语言模型 (LLM) 可以正确回答医学问题，甚至在某些医学考试中的表现优于人类的平均分数。但是，据我们所知，尚未进行任何研究来评估语言模型验证现有或生成的医学文本的正确性和一致性的能力。在本文中，我们介绍了 MEDEC（此 https URL），这是第一个公开可用的临床笔记医疗错误检测和纠正基准，涵盖五种类型的错误（诊断、管理、治疗、药物治疗和病原体）。MEDEC 包含 3,848 份临床文本，其中包括来自三个美国医院系统的 488 份临床笔记，这些笔记之前从未被任何 LLM 见过。该数据集已用于 MEDIQA-CORR 共享任务，以评估 17 个参与系统 [Ben Abacha 等人，2024 年]。在本文中，我们描述了数据创建方法，并评估了最近的 LLM（例如 o1-preview、GPT-4、Claude 3.5 Sonnet 和 Gemini 2.0 Flash）在检测和纠正需要医学知识和推理能力的医疗错误方面的任务。我们还进行了一项比较研究，其中两名医生在 MEDEC 测试集上执行了相同的任务。结果表明，MEDEC 是一个足够具有挑战性的基准，可以评估模型验证现有或生成的笔记以及纠正医疗错误的能力。我们还发现，尽管最近的 LLM 在错误检测和纠正方面表现良好，但它们在这些任务中的表现仍然不如医生。我们讨论了这一差距背后的潜在因素、从实验中获得的见解、当前评估指标的局限性，并分享了未来研究的潜在指针。</li>
</ul>

<h3>Title: Dynamic Skill Adaptation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaao Chen, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19361">https://arxiv.org/abs/2412.19361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19361">https://arxiv.org/pdf/2412.19361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19361]] Dynamic Skill Adaptation for Large Language Models(https://arxiv.org/abs/2412.19361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework to adapt novel and complex skills to Large Language Models (LLMs). Compared with previous work which learns from human-curated and static data in random orders, we propose to first automatically generate and organize the training data by mimicking the learning pathways of human and then dynamically tailor the training data based on the training dynamics. Specifically, inspired by the learning structures and teaching strategies in the human education system, we first construct a skill graph by decomposing complex skills into sub-skills and arranging them based on their dependencies in human syllables. For every skill, we utilize LLMs to generate both textbook-like data which contains detailed descriptions of skills for pre-training and exercise-like data which targets at explicitly utilizing the skills to solve problems for instruction-tuning. Furthermore, during the instruction-tuning, we dynamically update the training data which down-weight easy-to-learn examples, generate more complex examples, and filter out data with errors. Experiments on large language models such as LLAMA and Mistral demonstrate the effectiveness of our proposed methods in adapting math reasoning skills and social study skills.</li>
<li><strong>摘要：</strong>我们提出了动态技能适应 (DSA)，这是一种自适应动态框架，用于将新颖而复杂的技能适应大型语言模型 (LLM)。与以前以随机顺序从人工策划的静态数据中学习的工作相比，我们提出首先通过模仿人类的学习路径自动生成和组织训练数据，然后根据训练动态动态调整训练数据。具体来说，受到人类教育系统中学习结构和教学策略的启发，我们首先通过将复杂技能分解为子技能并根据它们在人类音节中的依赖关系进行排列来构建技能图。对于每一项技能，我们利用 LLM 生成类似教科书的数据（其中包含用于预训练的技能的详细描述）和类似练习的数据（旨在明确利用这些技能解决问题以进行指令调整）。此外，在指令调整期间，我们动态更新训练数据，降低易于学习的示例的权重，生成更复杂的示例，并过滤掉有错误的数据。在 LLAMA 和 Mistral 等大型语言模型上进行的实验证明了我们提出的方法在调整数学推理技能和社会学习技能方面的有效性。</li>
</ul>

<h3>Title: DeepSeek-V3 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W.L. Xiao, Wangding Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19437">https://arxiv.org/abs/2412.19437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19437">https://arxiv.org/pdf/2412.19437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19437]] DeepSeek-V3 Technical Report(https://arxiv.org/abs/2412.19437)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at this https URL.</li>
<li><strong>摘要：</strong>我们提出了 DeepSeek-V3，这是一个强大的混合专家 (MoE) 语言模型，总共有 671B 个参数，每个 token 激活 37B。为了实现高效的推理和经济高效的训练，DeepSeek-V3 采用了多头潜在注意力 (MLA) 和 DeepSeekMoE 架构，这些架构在 DeepSeek-V2 中得到了彻底的验证。此外，DeepSeek-V3 开创了一种无辅助损失的负载平衡策略，并设置了多 token 预测训练目标以获得更强大的性能。我们在 14.8 万亿个多样化和高质量的 token 上对 DeepSeek-V3 进行了预训练，然后进行监督微调和强化学习阶段，以充分利用其功能。综合评估表明，DeepSeek-V3 优于其他开源模型，并实现了与领先的闭源模型相当的性能。尽管性能出色，但 DeepSeek-V3 仅需要 2.788M H800 GPU 小时即可完成完整训练。此外，它的训练过程非常稳定。在整个训练过程中，我们没有遇到任何不可恢复的损失峰值或执行任何回滚。模型检查点可在此 https URL 上找到。</li>
</ul>

<h3>Title: Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19449">https://arxiv.org/abs/2412.19449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19449">https://arxiv.org/pdf/2412.19449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19449]] Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models(https://arxiv.org/abs/2412.19449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This study proposes a knowledge distillation algorithm based on large language models and feature alignment, aiming to effectively transfer the knowledge of large pre-trained models into lightweight student models, thereby reducing computational costs while maintaining high model performance. Different from the traditional soft label distillation method, this method introduces a multi-layer feature alignment strategy to deeply align the intermediate features and attention mechanisms of the teacher model and the student model, maximally retaining the semantic expression ability and context modeling ability of the teacher model. In terms of method design, a multi-task loss function is constructed, including feature matching loss, attention alignment loss, and output distribution matching loss, to ensure multi-level information transfer through joint optimization. The experiments were comprehensively evaluated on the GLUE data set and various natural language processing tasks. The results show that the proposed model performs very close to the state-of-the-art GPT-4 model in terms of evaluation indicators such as perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline models such as DeBERTa, XLNet, and GPT-3, showing significant performance improvements and computing efficiency advantages. Research results show that the feature alignment distillation strategy is an effective model compression method that can significantly reduce computational overhead and storage requirements while maintaining model capabilities. Future research can be further expanded in the directions of self-supervised learning, cross-modal feature alignment, and multi-task transfer learning to provide more flexible and efficient solutions for the deployment and optimization of deep learning models.</li>
<li><strong>摘要：</strong>本研究提出一种基于大型语言模型和特征对齐的知识蒸馏算法，旨在将大型预训练模型的知识有效地迁移到轻量级的学生模型中，从而在保持高模型性能的同时降低计算成本。不同于传统的软标签蒸馏方法，该方法引入了多层特征对齐策略，将教师模型和学生模型的中间特征和注意机制进行深度对齐，最大限度地保留教师模型的语义表达能力和上下文建模能力。在方法设计方面，构建了多任务损失函数，包括特征匹配损失、注意对齐损失和输出分布匹配损失，通过联合优化确保多层次的信息传递。实验在GLUE数据集和各项自然语言处理任务上进行了综合评估。结果表明，所提模型在困惑度、BLEU、ROUGE、CER等评测指标上的表现非常接近最先进的GPT-4模型。同时远超DeBERTa、XLNet、GPT-3等基线模型，表现出显著的性能提升和计算效率优势。研究结果表明，特征对齐蒸馏策略是一种有效的模型压缩方法，可以在保持模型能力的同时，显著降低计算开销和存储需求。未来的研究可以在自监督学习、跨模态特征对齐、多任务迁移学习等方向上进一步拓展，为深度学习模型的部署和优化提供更灵活、更高效的解决方案。</li>
</ul>

<h3>Title: Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19512">https://arxiv.org/abs/2412.19512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19512">https://arxiv.org/pdf/2412.19512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19512]] Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging(https://arxiv.org/abs/2412.19512)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) for downstream tasks is a widely adopted approach, but it often leads to safety degradation in safety-aligned LLMs. Currently, many solutions address this issue by incorporating additional safety data, which can be impractical in many cases. In this paper, we address the question: How can we improve downstream task performance while preserving safety in LLMs without relying on additional safety data? We propose a simple and effective method that maintains the inherent safety of LLMs while enhancing their downstream task performance: merging the weights of pre- and post-fine-tuned safety-aligned models. Experimental results across various downstream tasks, models, and merging methods demonstrate that this approach effectively mitigates safety degradation while improving downstream task performance, offering a practical solution for adapting safety-aligned LLMs.</li>
<li><strong>摘要：</strong>针对下游任务对大型语言模型 (LLM) 进行微调是一种广泛采用的方法，但它通常会导致安全一致的 LLM 的安全性下降。目前，许多解决方案通过合并额外的安全数据来解决此问题，但这在许多情况下是不切实际的。在本文中，我们讨论了以下问题：如何在不依赖额外安全数据的情况下，在保持 LLM 安全性的同时提高下游任务性能？我们提出了一种简单有效的方法，既能保持 LLM 的固有安全性，又能提高其下游任务性能：合并微调前后的安全一致模型的权重。各种下游任务、模型和合并方法的实验结果表明，这种方法有效地缓解了安全性下降，同时提高了下游任务性能，为采用安全一致的 LLM 提供了一种实用的解决方案。</li>
</ul>

<h3>Title: Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19513">https://arxiv.org/abs/2412.19513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19513">https://arxiv.org/pdf/2412.19513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19513]] Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs(https://arxiv.org/abs/2412.19513)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can correct their self-generated responses, but a decline in accuracy after self-correction is also witnessed. To have a deeper understanding of self-correction, we endeavor to decompose, evaluate, and analyze the self-correction behaviors of LLMs. By enumerating and analyzing answer correctness before and after self-correction, we decompose the self-correction capability into confidence (being confident to correct answers) and critique (turning wrong answers to correct) capabilities, and propose two metrics from a probabilistic perspective to measure these 2 capabilities, along with another metric for overall self-correction capability evaluation. Based on our decomposition and evaluation metrics, we conduct extensive experiments and draw some empirical conclusions. For example, we find different models can exhibit distinct behaviors: some models are confident while others are more critical. We also find the trade-off between the two capabilities (i.e. improving one can lead to a decline in the other) when manipulating model self-correction behavior by prompts or in-context learning. Further, we find a simple yet efficient strategy to improve self-correction capability by transforming Supervision Fine-Tuning (SFT) data format, and our strategy outperforms vanilla SFT in both capabilities and achieves much higher accuracy after self-correction. Our code will be publicly available on GitHub.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）可以纠正其自生成的响应，但自我纠正后准确率也会下降。为了更深入地理解自我纠正，我们努力分解、评估和分析LLM的自我纠正行为。通过枚举和分析自我纠正前后的答案正确性，我们将自我纠正能力分解为信心（有信心纠正答案）和批判（将错误答案转化为正确答案）能力，并从概率角度提出了两个指标来衡量这两个能力，以及另一个指标用于整体自我纠正能力的评估。基于我们的分解和评估指标，我们进行了大量实验并得出了一些实证结论。例如，我们发现不同的模型可以表现出不同的行为：一些模型很有信心，而另一些模型则更具批判性。我们还发现，当通过提示或情境学习来操纵模型自我修正行为时，两种能力之间存在权衡（即，提高一种能力可能会导致另一种能力下降）。此外，我们发现了一种简单而有效的策略，即通过转换监督微调 (SFT) 数据格式来提高自我修正能力，我们的策略在两种能力上都优于普通 SFT，并且在自我修正后实现了更高的准确率。我们的代码将在 GitHub 上公开发布。</li>
</ul>

<h3>Title: Exploiting Domain-Specific Parallel Data on Multilingual Language Models for Low-resource Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19522">https://arxiv.org/abs/2412.19522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19522">https://arxiv.org/pdf/2412.19522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19522]] Exploiting Domain-Specific Parallel Data on Multilingual Language Models for Low-resource Language Translation(https://arxiv.org/abs/2412.19522)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Neural Machine Translation (NMT) systems built on multilingual sequence-to-sequence Language Models (msLMs) fail to deliver expected results when the amount of parallel data for a language, as well as the language's representation in the model are limited. This restricts the capabilities of domain-specific NMT systems for low-resource languages (LRLs). As a solution, parallel data from auxiliary domains can be used either to fine-tune or to further pre-train the msLM. We present an evaluation of the effectiveness of these two techniques in the context of domain-specific LRL-NMT. We also explore the impact of domain divergence on NMT model performance. We recommend several strategies for utilizing auxiliary parallel data in building domain-specific NMT models for LRLs.</li>
<li><strong>摘要：</strong>当语言的并行数据量以及语言在模型中的表示有限时，基于多语言序列到序列语言模型 (msLM) 构建的神经机器翻译 (NMT) 系统无法提供预期结果。这限制了领域特定 NMT 系统针对低资源语言 (LRL) 的能力。作为一种解决方案，来自辅助域的并行数据可用于微调或进一步预训练 msLM。我们在领域特定 LRL-NMT 的背景下对这两种技术的有效性进行了评估。我们还探讨了领域分歧对 NMT 模型性能的影响。我们推荐了几种利用辅助并行数据为 LRL 构建领域特定 NMT 模型的策略。</li>
</ul>

<h3>Title: TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data</h3>
<ul>
<li><strong>Authors: </strong>Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19544">https://arxiv.org/abs/2412.19544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19544">https://arxiv.org/pdf/2412.19544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19544]] TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data(https://arxiv.org/abs/2412.19544)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Semantic parsing, which converts natural language questions into logic forms, plays a crucial role in reasoning within structured environments. However, existing methods encounter two significant challenges: reliance on extensive manually annotated datasets and limited generalization capability to unseen examples. To tackle these issues, we propose Targeted Synthetic Data Generation (TARGA), a practical framework that dynamically generates high-relevance synthetic data without manual annotation. Starting from the pertinent entities and relations of a given question, we probe for the potential relevant queries through layer-wise expansion and cross-layer combination. Then we generate corresponding natural language questions for these constructed queries to jointly serve as the synthetic demonstrations for in-context learning. Experiments on multiple knowledge base question answering (KBQA) datasets demonstrate that TARGA, using only a 7B-parameter model, substantially outperforms existing non-fine-tuned methods that utilize close-sourced model, achieving notable improvements in F1 scores on GrailQA(+7.7) and KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency, robustness, and generalization capabilities under non-I.I.D. settings.</li>
<li><strong>摘要：</strong>语义解析将自然语言问题转换为逻辑形式，在结构化环境中的推理中起着至关重要的作用。然而，现有的方法面临两个重大挑战：依赖大量手动注释的数据集和对未见过的示例的有限泛化能力。为了解决这些问题，我们提出了有针对性的合成数据生成 (TARGA)，这是一个实用的框架，可以在没有手动注释的情况下动态生成高相关性的合成数据。从给定问题的相关实体和关系开始，我们通过逐层扩展和跨层组合来探测潜在的相关查询。然后，我们为这些构造的查询生成相应的自然语言问题，以共同作为上下文学习的合成演示。在多个知识库问答 (KBQA) 数据集上的实验表明，仅使用 7B 参数模型的 TARGA 大大优于现有的使用闭源模型的非微调方法，在 GrailQA (+7.7) 和 KBQA-Agent (+12.2) 上的 F1 分数显着提高。此外，TARGA 在非独立同分布环境下还表现出卓越的样本效率、稳健性和泛化能力。</li>
</ul>

<h3>Title: Machine Generated Product Advertisements: Benchmarking LLMs Against Human Performance</h3>
<ul>
<li><strong>Authors: </strong>Sanjukta Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19610">https://arxiv.org/abs/2412.19610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19610">https://arxiv.org/pdf/2412.19610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19610]] Machine Generated Product Advertisements: Benchmarking LLMs Against Human Performance(https://arxiv.org/abs/2412.19610)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This study compares the performance of AI-generated and human-written product descriptions using a multifaceted evaluation model. We analyze descriptions for 100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4) with and without sample descriptions, against human-written descriptions. Our evaluation metrics include sentiment, readability, persuasiveness, Search Engine Optimization(SEO), clarity, emotional appeal, and call-to-action effectiveness. The results indicate that ChatGPT 4 performs the best. In contrast, other models demonstrate significant shortcomings, producing incoherent and illogical output that lacks logical structure and contextual relevance. These models struggle to maintain focus on the product being described, resulting in disjointed sentences that do not convey meaningful information. This research provides insights into the current capabilities and limitations of AI in the creation of content for e-Commerce.</li>
<li><strong>摘要：</strong>本研究使用多方面评估模型比较了人工智能生成和人工编写的产品描述的性能。我们分析了四种人工智能模型（Gemma 2B、LLAMA、GPT2 和 ChatGPT 4）生成的 100 种产品描述，这些产品有样本描述和没有样本描述，并与人工编写的描述进行了对比。我们的评估指标包括情感、可读性、说服力、搜索引擎优化 (SEO)、清晰度、情感吸引力和号召性用语的有效性。结果表明，ChatGPT 4 表现最佳。相比之下，其他模型表现出了明显的缺陷，产生的输出不连贯、不合逻辑，缺乏逻辑结构和上下文相关性。这些模型很难将注意力集中在所描述的产品上，导致句子脱节，无法传达有意义的信息。这项研究深入了解了人工智能在电子商务内容创建方面的当前能力和局限性。</li>
</ul>

<h3>Title: Enhancing Whisper's Accuracy and Speed for Indian Languages through Prompt-Tuning and Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Kumud Tripathi, Raj Gothi, Pankaj Wasnik</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19785">https://arxiv.org/abs/2412.19785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19785">https://arxiv.org/pdf/2412.19785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19785]] Enhancing Whisper's Accuracy and Speed for Indian Languages through Prompt-Tuning and Tokenization(https://arxiv.org/abs/2412.19785)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Automatic speech recognition has recently seen a significant advancement with large foundational models such as Whisper. However, these models often struggle to perform well in low-resource languages, such as Indian languages. This paper explores two novel approaches to enhance Whisper's multilingual speech recognition performance in Indian languages. First, we propose prompt-tuning with language family information, which enhances Whisper's accuracy in linguistically similar languages. Second, we introduce a novel tokenizer that reduces the number of generated tokens, thereby accelerating Whisper's inference speed. Our extensive experiments demonstrate that the tokenizer significantly reduces inference time, while prompt-tuning enhances accuracy across various Whisper model sizes, including Small, Medium, and Large. Together, these techniques achieve a balance between optimal WER and inference speed.</li>
<li><strong>摘要：</strong>最近，随着 Whisper 等大型基础模型的出现，自动语音识别取得了重大进展。然而，这些模型在资源匮乏的语言（如印度语）中往往表现不佳。本文探讨了两种新方法来提高 Whisper 在印度语中的多语言语音识别性能。首先，我们提出了使用语系信息进行快速调整的建议，这提高了 Whisper 在语言相似的语言中的准确性。其次，我们引入了一种新颖的标记器，它可以减少生成的标记数量，从而加快 Whisper 的推理速度。我们进行了大量的实验，结果表明，标记器显著缩短了推理时间，而快速调整则提高了各种 Whisper 模型大小（包括小型、中型和大型）的准确性。这些技术共同实现了最佳 WER 和推理速度之间的平衡。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
