<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-27</h1>
<h3>Title: Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle</h3>
<ul>
<li><strong>Authors: </strong>Zihan Wang, Cheng Tang, Lei Gong, Cheng Li, Chao Wang, teng wang, Wenqi Lou, Xuehai Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16986">https://arxiv.org/abs/2601.16986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16986">https://arxiv.org/pdf/2601.16986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16986]] Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle(https://arxiv.org/abs/2601.16986)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的思想链 (CoT) 推理显着提高了复杂任务的准确性，但由于键值 (KV) 缓存中存储的长思考阶段序列而产生过多的内存开销。与所有令牌都同等重要的传统生成任务不同，CoT 强调最终答案，使得传统的 KV 压缩策略无效。在本文中，我们提出了 Crystal-KV，一个专为 CoT 推理量身定制的高效 KV 缓存管理框架。我们的主要见解是答案第一原则。通过将答案偏好映射到思考阶段注意力图，我们区分了 SlipKV（主要维持推理流程，但偶尔可能会引入误导性上下文）和 CrystalKV（真正有助于最终答案的正确性）。接下来，我们提出了一种基于注意力的最近最少使用算法。它精确地识别 SlipKV 条目的实用程序何时到期并将其驱逐，从而保留 CrystalKV 而不会中断推理流程。最后，我们介绍了一种自适应缓存预算分配算法。基于CrystalKV的动态比例，预估各层/头的重要性，并在推理过程中调整KV缓存预算，放大关键组件，提高预算利用率。结果表明，Crystal-KV 实现了最先进的 KV 缓存压缩，显着提高了吞吐量，并实现了更快的响应时间，同时保持甚至提高了 CoT 推理的答案准确性。</li>
</ul>

<h3>Title: Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions</h3>
<ul>
<li><strong>Authors: </strong>Shunyang Luo, Peibei Cao, Zhihui Zhu, Kehua Feng, Zhihua Wang, Keyan Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16987">https://arxiv.org/abs/2601.16987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16987">https://arxiv.org/pdf/2601.16987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16987]] Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions(https://arxiv.org/abs/2601.16987)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.</li>
<li><strong>摘要：</strong>奖励模型 (RM) 是协调大型语言模型的核心，但其实际有效性取决于对未见提示和变化分布的泛化。大多数现有的 RM 评估依赖于静态的、预先注释的偏好数据集，这些数据集提供的覆盖范围有限，并且通常无法忠实地评估开放世界环境中的泛化能力。我们引入了成对最大差异竞争（PMDC），这是一种动态且注释高效的框架，用于使用大型、未标记的开放域提示池来评估 RM 泛化。 PMDC 主动选择即时响应对，最大限度地增加两个 RM 之间的分歧，从而产生一组紧凑的高度争议的测试用例。这些案例由预言机裁决，并通过 Bradley-Terry 模型汇总所得结果，以生成 RM 的全球排名和成对胜率情况。我们应用 PMDC 重新评估 10 个有代表性的 RM，并观察到与传统基准相比，排名发生了显着的洗牌。定性分析进一步揭示了系统泛化失败，为改进奖励模型提供了宝贵的见解。</li>
</ul>

<h3>Title: RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Zhou, Ziqi Liu, Yan Wang, Yiming Lin, Yangbin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17002">https://arxiv.org/abs/2601.17002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17002">https://arxiv.org/pdf/2601.17002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17002]] RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection(https://arxiv.org/abs/2601.17002)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.</li>
<li><strong>摘要：</strong>讽刺检测仍然是一个重大挑战，因为它依赖于细致入微的上下文理解、世界知识和多方面的语言线索，而这些线索在不同的讽刺表达中存在很大差异。现有的方法，从微调变压器到大型语言模型，都对所有输入应用统一的推理策略，努力满足讽刺的多样化分析需求。这些要求的范围从对上下文期望违规进行建模到要求外部知识基础或识别特定的修辞模式。为了解决这个限制，我们引入了 RAM-SD，一种用于讽刺检测的检索增强多代理框架。该框架通过四个阶段进行操作：（1）上下文检索以讽刺和非讽刺范例为基础进行查询； （2）元规划器对讽刺类型进行分类，并从预定义的集合中选择最佳推理计划； (3) 一组专业代理执行互补的多视图分析； (4) 整合者将这些分析综合成带有自然语言解释的最终的、可解释的判断。根据四个标准基准进行评估，RAM-SD 的 Macro-F1 达到了 77.74%，比强大的 GPT-4o+CoC 基准高出 7.01 分。我们的框架不仅设定了新的性能基准，而且还提供透明且可解释的推理轨迹，阐明讽刺理解背后的认知过程。</li>
</ul>

<h3>Title: Dynamic Role Assignment for Multi-Agent Debate</h3>
<ul>
<li><strong>Authors: </strong>Miao Zhang, Junsik Kim, Siyuan Xiang, Jian Gao, Cheng Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17152">https://arxiv.org/abs/2601.17152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17152">https://arxiv.org/pdf/2601.17152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17152]] Dynamic Role Assignment for Multi-Agent Debate(https://arxiv.org/abs/2601.17152)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.</li>
<li><strong>摘要：</strong>多智能体大语言模型（LLM）和视觉语言模型（VLM）辩论系统采用专门的角色来解决复杂的问题，但模型的专业化并没有被用来决定哪个模型应该扮演哪个角色。我们提出动态角色分配，这是一个在实际辩论之前运行元辩论以选择合适代理的框架。元辩论有两个阶段：(1) 提案，候选人提供针对角色的论点；(2) 同行评审，根据数据和特定于角色的标准对提案进行评分，从而为每个职位选择最佳代理。我们在法学硕士问题解决基准上评估我们的方法。应用在现有的辩论系统之上，我们的方法始终优于统一分配（使用相同模型填充所有角色）高达 74.8%，优于随机分配（将模型分配给角色而不考虑其适用性）高达 29.7%，具体取决于任务和具体分配。这项工作为多代理系统设计建立了一个新的范例，从静态代理部署转向动态和能力感知的选择。</li>
</ul>

<h3>Title: Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text</h3>
<ul>
<li><strong>Authors: </strong>Tunazzina Islam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17172">https://arxiv.org/abs/2601.17172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17172">https://arxiv.org/pdf/2601.17172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17172]] Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text(https://arxiv.org/abs/2601.17172)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越能够大规模生成个性化、有说服力的文本，从而引发了有关自动通信中的偏见和公平性的新问题。本文首次系统分析了法学硕士在处理以人口统计为条件的目标消息传递任务时的行为方式。我们引入了一个受控评估框架，使用三种领先的模型——GPT-4o、Llama-3.3和Mistral-Large 2.1——跨越两个世代设置：独立生成，它隔离了内在的人口统计影响，以及背景丰富的生成，它结合了主题和区域背景来模拟现实的目标。我们从三个维度评估生成的消息：词汇内容、语言风格和说服性框架。我们实例化了这个气候传播框架，发现不同模型中一致的基于年龄和性别的不对称性：针对男性和青年的信息强调能动性、创新和自信，而针对女性和老年人的信息则强调温暖、关怀和传统。上下文提示系统地放大了这些差异，针对年轻或男性受众的信息的说服力得分明显更高。我们的研究结果表明，人口刻板印象如何在法学硕士生成的有针对性的沟通中浮出水面并得到加强，强调了对偏见意识生成管道和透明审计框架的需求，这些框架明确考虑了社会敏感应用程序中的人口状况。</li>
</ul>

<h3>Title: Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content</h3>
<ul>
<li><strong>Authors: </strong>Parth Bhalerao, Diola Dsouza, Ruiwen Guan, Oana Ignat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17173">https://arxiv.org/abs/2601.17173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17173">https://arxiv.org/pdf/2601.17173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17173]] Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content(https://arxiv.org/abs/2601.17173)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at this https URL.</li>
<li><strong>摘要：</strong>问答系统通常根据事实正确性进行评估，但许多现实世界的应用程序（例如教育和职业指导）需要指导：提供反思和指导的响应。现有的 QA 基准很少能捕捉到这种区别，特别是在多语言和长格式环境中。我们推出了 MentorQA，这是第一个多语言数据集和评估框架，用于从长视频中进行以指导为中心的问答，包含来自四种语言的 180 小时内容的近 9,000 个 QA 对。我们定义以指导为中心的评估维度，超越事实准确性，捕捉清晰度、一致性和学习价值。使用 MentorQA，我们在受控条件下比较单代理、双代理、RAG 和多代理 QA 架构。多代理管道持续产生更高质量的指导响应，对于复杂主题和资源较少的语言尤其有显着的收益。我们进一步分析了基于法学硕士的自动化评估的可靠性，观察到与人类判断一致的实质性变化。总体而言，这项工作将以指导为中心的 QA 确立为一个独特的研究问题，并为研究教育人工智能中的代理架构和评估设计提供了多语言基准。数据集和评估框架在此 https URL 发布。</li>
</ul>

<h3>Title: Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Seyyed Saeid Cheshmi, Hahnemann Ortiz, James Mooney, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17197">https://arxiv.org/abs/2601.17197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17197">https://arxiv.org/pdf/2601.17197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17197]] Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding(https://arxiv.org/abs/2601.17197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at this https URL.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）在视觉数学和科学问答等文字多模态任务中表现出了强大的推理能力。然而，讽刺、幽默和隐喻等比喻语言仍然是一个重大挑战，因为它通过表达含义和预期含义之间微妙的不一致来传达意图和情感。在多模态设置中，伴随图像可以放大或反转文本含义，要求模型能够跨模态推理并考虑主观性。我们提出了一个开发高效多模态推理模型的三步框架，该模型可以（i）解释多模态比喻语言，（ii）提供透明的推理轨迹，以及（iii）泛化多种比喻风格。跨四种风格的实验表明：(1) 合并推理痕迹可显着提高多模态比喻理解，(2) 在一种风格中学习的推理可以转移到其他风格，尤其是在讽刺和幽默等相关风格之间，(3) 跨风格联合训练产生通用推理 VLM，其性能优于更大的开源和闭源模型。我们的研究结果表明，具有可验证推理的轻量级 VLM 可以实现强大的跨风格泛化，同时为多模式任务提供可检查的推理轨迹。代码和实现可在此 https URL 获取。</li>
</ul>

<h3>Title: DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Saadat Hasan Khan, Spencer Hong, Jingyu Wu, Kevin Lybarger, Youbing Yin, Erin Babinsky, Daben Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17212">https://arxiv.org/abs/2601.17212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17212">https://arxiv.org/pdf/2601.17212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17212]] DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation(https://arxiv.org/abs/2601.17212)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）是一种将语言模型输出基于特定领域信息的常用技术。然而，RAG 经常受到推理密集型问答（QA）的挑战，因为余弦相似度等常见检索方法以引入冗余内容为代价最大化相关性，从而降低信息召回率。为了解决这个问题，我们引入了以多样性为中心的检索增强生成（DF-RAG），它系统地将多样性纳入检索步骤，以提高复杂、推理密集型 QA 基准的性能。 DF-RAG 基于最大边际相关性框架来选择与查询相关且彼此最大程度不相似的信息块。 DF-RAG 的一项关键创新是它能够在测试时动态优化每个查询的多样性水平，而无需任何额外的微调或先验信息。我们表明，与使用余弦相似性的普通 RAG 相比，DF-RAG 在推理密集型 QA 基准上将 F1 性能提高了 4-10%，并且也优于其他已建立的基线。此外，我们估计 Oracle 的绝对 F1 收益比普通 RAG 最高可达 18%，其中 DF-RAG 最高可达 91.3%。</li>
</ul>

<h3>Title: Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Massimiliano Pronesti, Anya Belz, Yufang Hou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17223">https://arxiv.org/abs/2601.17223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17223">https://arxiv.org/pdf/2601.17223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17223]] Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning(https://arxiv.org/abs/2601.17223)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.</li>
<li><strong>摘要：</strong>最近关于具有可验证奖励的强化学习（RLVR）的研究表明，使用结果级验证信号（例如代码的单元测试或数学的精确匹配检查）可以显着改进大型语言模型（LLM）。与此同时，流程监督长期以来一直被探索作为塑造法学硕士中间推理行为的一种方法，但现有的方法依赖于神经法官对思维链步骤进行评分，使它们容易受到不透明、偏见和奖励黑客的影响。为了解决这一差距，我们引入了可验证过程奖励模型（VPRM），这是一种强化学习框架，其中中间推理步骤由确定性的、基于规则的验证器进行检查。我们将 VPRM 应用于医学证据合成的偏倚风险评估，在该领域，指南定义的标准和基于规则的决策路径可以对推理轨迹进行编程验证。在多个数据集中，我们发现 VPRM 生成的推理紧密遵循领域规则，并在步骤级决策和最终标签之间实现了更高的一致性。结果表明，VPRM 的 F1 比最先进的模型高出 20%，比可验证的结果奖励高出 6.5%，在证据基础和逻辑一致性方面取得了实质性进展。</li>
</ul>

<h3>Title: Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation</h3>
<ul>
<li><strong>Authors: </strong>David Y. Liu, Xanthe Muston, Aditya Joshi, Sebastian Sequoiah-Grayson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17226">https://arxiv.org/abs/2601.17226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17226">https://arxiv.org/pdf/2601.17226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17226]] Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation(https://arxiv.org/abs/2601.17226)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.</li>
<li><strong>摘要：</strong>尽管讲故事具有主观性，但过去的自动故事生成（ASG）工作仍然依赖于有限的基本事实来进行训练和评估。在这项工作中，我们探索强化学习（d-RLAIF）作为监督微调（SFT）的训练后替代方案。我们首先应用托多罗夫的叙事平衡理论来建立定义理想的 ASG 品质的原则。我们根据我们的原则提示 7B 和 14B LLM 作为评判模型，以测试与人类注释者的一致性，并在 d-RLAIF 期间提供奖励信号。我们使用 Gemini-3-Flash 来评估训练后模型的输出，并将其与 TimeTravel 数据集中的人类编写的故事进行比较。我们证明 d-RLAIF 提供了监督微调 (SFT) 的可行替代方案，即生成更加多样化且符合人类叙事惯例的故事。我们的论文展示了强化学习对于 ASG 等主观任务的语言基础后训练的前景。</li>
</ul>

<h3>Title: CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Akshith Reddy Putta, Jacob Devasier, Chengkai Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17230">https://arxiv.org/abs/2601.17230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17230">https://arxiv.org/pdf/2601.17230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17230]] CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval(https://arxiv.org/abs/2601.17230)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.</li>
<li><strong>摘要：</strong>自动事实核查主要侧重于根据静态语料库验证一般知识，而忽略了法律等高风险领域，因为这些领域的真相不断发展且技术复杂。我们引入 CaseFacts，这是一个用于验证针对美国最高法院先例的口语法律主张的基准。与将正式文本映射到正式文本的现有资源不同，CaseFacts 要求系统在考虑时间有效性的同时，弥合外行断言和技术法理学之间的语义差距。该数据集包含 6,294 个声明，分为“支持”、“驳斥”或“否决”。我们使用多级管道构建此基准，该管道利用大型语言模型 (LLM) 综合专家案例摘要中的主张，采用新颖的语义相似性启发式方法来有效识别和验证复杂的法律否决。对最先进的法学硕士的实验表明，这项任务仍然具有挑战性；值得注意的是，与闭门基线相比，由于检索了嘈杂的、非权威的先例，使用不受限制的网络搜索增强模型会降低性能。我们发布 CaseFacts 来促进对法律事实验证系统的研究。</li>
</ul>

<h3>Title: Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Jacob Devasier, Akshith Putta, Qing Wang, Alankrit Moses, Chengkai Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17232">https://arxiv.org/abs/2601.17232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17232">https://arxiv.org/pdf/2601.17232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17232]] Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data(https://arxiv.org/abs/2601.17232)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.</li>
<li><strong>摘要：</strong>自动事实检查基准在很大程度上忽略了根据现实世界的大量结构化数据验证声明的挑战，而是专注于小型、精心策划的表。我们引入了一个新的大规模多语言数据集来解决这一关键差距。它包含基于 434 个复杂的 OECD 表的 78,503 个综合声明，每个表平均超过 50 万行。我们提出了一种新颖的框架引导方法，其中算法基于六个语义框架以编程方式选择重要数据点，以生成英语、中文、西班牙语和印地语的现实声明。至关重要的是，我们通过知识探索实验证明法学硕士没有记住这些事实，迫使系统执行真正的检索和推理，而不是依赖参数化知识。我们提供了一个基线 SQL 生成系统，并表明我们的基准测试具有很高的挑战性。我们的分析认为证据检索是主要瓶颈，模型很难在海量表中找到正确的数据。该数据集为推进这一未解决的现实问题的研究提供了重要的新资源。</li>
</ul>

<h3>Title: PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Rifqi Farhansyah, Hanif Muhammad Zhafran, Farid Adilazuarda, Shamsuddeen Hassan Muhammad, Maryam Ibrahim Mukhtar, Nedjma Ousidhoum, Genta Indra Winata, Ayu Purwarianti, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17277">https://arxiv.org/abs/2601.17277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17277">https://arxiv.org/pdf/2601.17277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17277]] PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues(https://arxiv.org/abs/2601.17277)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.</li>
<li><strong>摘要：</strong>语码转换是世界上大多数使用多种语言的人中的一种普遍做法，但很少有基准能够准确反映其在日常交流中的复杂性。我们提出了 PingPong，这是自然多方语码转换对话的基准，涵盖五种语言组合变体，其中一些是三语的。我们的数据集由 2 到 4 名参与者之间人工编写的对话组成，涵盖真实的多线程结构，其中回复经常引用对话中更早的点。我们证明，我们的数据比机器生成的替代数据更加自然，结构更加多样化，在消息长度、说话者优势和回复距离方面提供了更大的变化。基于这些对话，我们定义了三个下游任务：问答、对话总结和主题分类。对 PingPong 上几种最先进的语言模型的评估表明，在代码转换输入方面的性能仍然有限，这强调了迫切需要更强大的 NLP 系统，该系统能够解决现实世界多语言话语的复杂性。</li>
</ul>

<h3>Title: Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yaokun Liu, Yifan Liu, Phoebe Mbuvi, Zelin Li, Ruichen Yao, Gawon Lim, Dong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17284">https://arxiv.org/abs/2601.17284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17284">https://arxiv.org/pdf/2601.17284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17284]] Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering(https://arxiv.org/abs/2601.17284)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at this https URL, and the CV-MedBench dataset is released on Hugging Face at this https URL.</li>
<li><strong>摘要：</strong>模糊的用户查询严重阻碍了大型语言模型在医疗问答中的部署，这是一个重大的安全风险，明显降低了高风险医疗保健环境中答案的准确性。在本文中，我们通过将输入模糊性与任意不确定性（AU）联系起来，形式化了这一挑战，随意不确定性是由于未指定的输入而产生的不可约不确定性。为了促进这个方向的研究，我们构建了 CV-MedBench，这是第一个为研究医学 QA 中输入模糊性而设计的基准。使用这个基准，我们从表示工程的角度分析 AU，揭示 AU 在 LLM 的内部激活模式中线性编码。利用这种洞察力，我们引入了一种新颖的 AU 引导的“回答前澄清”框架，其中包含 AU-Probe——一个轻量级模块，可以直接从隐藏状态检测输入歧义。与现有的不确定性估计方法不同，AU-Probe 不需要 LLM 微调，也不需要多次前向传递，从而实现了主动请求用户澄清并显着增强安全性的有效机制。四个开放法学硕士的广泛实验证明了我们的 QA 框架的有效性，与基线相比，平均准确度提高了 9.48%。我们的框架为安全医疗质量保证提供了高效、强大的解决方案，增强了健康相关应用程序的可靠性。该代码可在此 https URL 中获取，并且 CV-MedBench 数据集已在 Hugging Face 上的此 https URL 上发布。</li>
</ul>

<h3>Title: Meta-Judging with Large Language Models: Concepts, Methods, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Hugo Silva, Mateus Mendes, Hugo Gonçalo Oliveira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17312">https://arxiv.org/abs/2601.17312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17312">https://arxiv.org/pdf/2601.17312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17312]] Meta-Judging with Large Language Models: Concepts, Methods, and Challenges(https://arxiv.org/abs/2601.17312)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 正在快速发展，现在经常被用作评估器，这一过程通常被称为 LLM 作为法官，它提供模型输出的质量评估。然而，最近的研究指出了此类评估的重大缺陷，包括对提示的敏感性、系统偏差、冗长效应以及不可靠或幻觉的基本原理。这些限制促使开发出一种更强大的范式，称为法学硕士作为元法官。本调查回顾了元判断的最新进展，并通过介绍六个关键视角的框架来组织文献：（i）概念基础，（ii）元判断机制，（iii）对齐训练方法，（iv）评估，（v）局限性和失败模式，以及（vi）未来方向。通过分析法学硕士作为法官的局限性并总结法学硕士元评审的最新进展，我们认为法学硕士作为元法官为更稳定和值得信赖的自动化评估提供了一个有希望的方向，同时强调了与成本、即时敏感性和共享模型偏差相关的剩余挑战，必须解决这些挑战以推进下一代法学硕士评估方法。</li>
</ul>

<h3>Title: The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17344">https://arxiv.org/abs/2601.17344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17344">https://arxiv.org/pdf/2601.17344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17344]] The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents(https://arxiv.org/abs/2601.17344)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.</li>
<li><strong>摘要：</strong>具有扩展自主权的大型语言模型 (LLM) 代理解锁了新功能，但也给 LLM 安全带来了更大的挑战。特别是，法学硕士代理人可能会追求偏离人类价值观和道德规范的目标，这种风险称为价值观错位。现有的评估主要侧重于对明确有害输入的响应或针对系统故障的鲁棒性，而现实、完全良性和代理环境中的价值失调在很大程度上仍未得到充分探索。为了填补这一空白，我们首先将失控风险形式化，并确定之前未被充分审查的内在价值错位（内在 VM）​​。然后，我们介绍 IMPRESS（现实场景集中的内在价值错位探针），这是一个场景驱动的框架，用于系统地评估这种风险。遵循我们的框架，我们使用具有严格质量控制的多阶段 LLM 生成流程，构建由现实、完全良性和情境化场景组成的基准。我们在 21 个最先进的 LLM 代理上评估了 Intrinsic VM，发现它是跨模型的常见且广泛观察到的安全风险。此外，错位率因动机、风险类型、模型规模和架构而异。虽然解码策略和超参数仅表现出边际影响，但情境化和框架机制显着塑造了错位行为。最后，我们进行人工验证来验证我们的自动判断并评估现有的缓解策略，例如安全提示和护栏，这些策略表现出不稳定或有效性有限。我们进一步展示了 IMPRESS 在整个 AI 生态系统中的关键用例。我们的代码和基准将在接受后公开发布。</li>
</ul>

<h3>Title: Do readers prefer AI-generated Italian short stories?</h3>
<ul>
<li><strong>Authors: </strong>Michael Farrell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17363">https://arxiv.org/abs/2601.17363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17363">https://arxiv.org/pdf/2601.17363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17363]] Do readers prefer AI-generated Italian short stories?(https://arxiv.org/abs/2601.17363)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.</li>
<li><strong>摘要：</strong>这项研究调查了读者是否更喜欢人工智能生成的意大利语短篇小说，而不是意大利著名作家写的短篇小说。在盲态设置中，20 名参与者阅读并评估了三个故事，其中两个是由 ChatGPT-4o 创建的，另一个是由 Alberto Moravia 创作的，并且没有被告知其来源。为了探索潜在的影响因素，还收集了阅读习惯和人口统计数据，包括年龄、性别、教育程度和第一语言。结果显示，人工智能编写的文本获得的平均评分略高，并且更容易受到青睐，尽管差异不大。文本偏好与人口统计或阅读习惯变量之间没有发现统计上显着的关联。这些发现挑战了关于读者对人类创作的小说的偏好的假设，并提出了关于文学语境中合成文本编辑的必要性的问题。</li>
</ul>

<h3>Title: Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Fasha, Bassam Hammo, Bilal Sowan, Husam Barham, Esam Nsour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17364">https://arxiv.org/abs/2601.17364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17364">https://arxiv.org/pdf/2601.17364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17364]] Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws(https://arxiv.org/abs/2601.17364)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.</li>
<li><strong>摘要：</strong>本研究以约旦法律为案例，探讨Llama-3.1大语言模型对阿拉伯语问答的微调。该模型的两个版本 - Llama-3.1-8B-bnb-4bit 和 Llama-3.1-8B-Instruct-bnb-4bit - 使用具有 LoRA 适配器和 4 位量化模型的参数高效微调 (PEFT) 进行微调，利用 Unsloth 框架进行加速和资源高效的训练。包含 6000 个法律问答对的自定义数据集根据约旦法律整理而成，并格式化为结构化提示。使用 BLEU 和 ROUGE 指标评估性能，以将微调模型与其各自的基本版本进行比较。结果表明，通过量化和优化微调策略，提高了法律推理和准确性，同时实现了资源效率。这项工作强调了针对阿拉伯法律领域采用大型语言模型的潜力，并强调了微调特定领域任务的有效技术。</li>
</ul>

<h3>Title: Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17367">https://arxiv.org/abs/2601.17367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17367">https://arxiv.org/pdf/2601.17367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17367]] Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers(https://arxiv.org/abs/2601.17367)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.</li>
<li><strong>摘要：</strong>标准注意力机制的二次复杂度给长上下文场景中的大型语言模型（LLM）带来了显着的可扩展性瓶颈。虽然在单个模型中结合稀疏和完整注意力的混合注意力策略提供了可行的解决方案，但它们通常采用静态计算比率（即稀疏与完整注意力的固定比例）并且无法适应推理过程中下游任务不同的稀疏敏感性。为了解决这个问题，我们提出了弹性注意力（Elastic Attention），它允许模型根据输入动态调整其整体稀疏性。这是通过将轻量级注意力路由器集成到现有的预训练模型中来实现的，该模型动态地将每个注意力头分配给不同的计算模式。在 8xA800 GPU 上仅 12 小时的训练内，我们的方法就使模型能够实现强大的性能和高效的推理。在广泛使用的法学硕士上进行的三个长上下文基准的实验证明了我们方法的优越性。</li>
</ul>

<h3>Title: WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews</h3>
<ul>
<li><strong>Authors: </strong>Kiyotada Mori, Shohei Tanaka, Tosho Hirasawa, Tadashi Kozuno, Koichiro Yoshino, Yoshitaka Ushiku</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17377">https://arxiv.org/abs/2601.17377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17377">https://arxiv.org/pdf/2601.17377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17377]] WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews(https://arxiv.org/abs/2601.17377)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.</li>
<li><strong>摘要：</strong>由于提交论文数量的快速增长，科学同行评审过程面临着人力资源短缺的问题。人们正在积极探索使用语言模型来减少同行评审的人力成本，作为应对这一挑战的潜在解决方案。已经提出了一种方法，以人类可解释的方式评估科学评论的证实水平。该方法提取论证、主张和证据的核心组成部分，并根据证据支持的主张的比例来评估证实的水平。证实程度是指主张基于客观事实的程度。然而，在评估证实程度时，仅仅检测某个主张是否存在支持证据是不够的；还必须准确评估主张及其证据之间的逻辑推论。我们提出了一种新的科学评论评价指标，用于评估主张和证据之间的逻辑推理。实验结果表明，与传统方法相比，所提出的方法与人类评分具有更高的相关性，表明其有可能更好地支持同行评审过程的效率。</li>
</ul>

<h3>Title: CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Hu, Wei Zhou, Juesi Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17397">https://arxiv.org/abs/2601.17397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17397">https://arxiv.org/pdf/2601.17397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17397]] CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing(https://arxiv.org/abs/2601.17397)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.</li>
<li><strong>摘要：</strong>知识编辑（KE）已成为无需重新训练即可更新大型语言模型（LLM）中事实的有前途的范例。然而，多语言知识编辑（MKE）的进展目前受到有偏见的评估框架的阻碍。我们观察到，现有的 MKE 基准通常是通过机械地将以英语为中心的数据集翻译成目标语言（例如英语到中文）来构建的。这种方法引入了翻译工件，并忽略了目标语言本地的文化特定实体，未能反映法学硕士的真实知识分布。为了解决这个问题，我们提出了 CLM-Bench，这是一个使用中国本土优先方法构建的文化意识基准。我们策划了 1,010 个植根于中国文化背景的高质量 CounterFact 对，并将它们与英语对应项进行匹配。使用 CLM-Bench，我们对代表性的 LLM（例如 Llama-3、Qwen2）进行了广泛的实验，并揭示了显着的跨语言错位：一种语言的编辑独立运行，无法传播到另一种语言。我们进一步通过分层表示分析提供了几何解释，证明中文和英文的编辑向量几乎是正交的——位于不相交的子空间中——而混合语言编辑则表现出这些向量的线性可加性。我们的研究结果对当前跨语言迁移方法的有效性提出了挑战，并强调了文化本土基准的重要性。</li>
</ul>

<h3>Title: Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jaehui Hwang, Dongyoon Han, Sangdoo Yun, Byeongho Heo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17421">https://arxiv.org/abs/2601.17421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17421">https://arxiv.org/pdf/2601.17421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17421]] Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning(https://arxiv.org/abs/2601.17421)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）中类似话语标记的出现，例如“等待”和“因此”，为了解其推理过程提供了一个独特的窗口。然而，仍然缺乏对这些信号如何随着训练策略和模型规模的变化进行系统分析。在本文中，我们通过各种模型的令牌概率来分析令牌级信号。我们发现特定的标记与推理正确性密切相关，随着训练策略的不同而变化，同时在模型尺度上保持稳定。仔细观察与答案概率相关的“等待”标记表明，在小规模数据集上微调的模型通过此类信号获得推理能力，但仅部分利用它们。这项工作提供了一个系统的视角来观察和理解法学硕士推理的动态。</li>
</ul>

<h3>Title: Clustering-driven Memory Compression for On-device Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ondrej Bohdal, Pramit Saha, Umberto Michieli, Mete Ozay, Taha Ceritli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17443">https://arxiv.org/abs/2601.17443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17443">https://arxiv.org/pdf/2601.17443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17443]] Clustering-driven Memory Compression for On-device Large Language Models(https://arxiv.org/abs/2601.17443)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常依赖于从过去的交互中提取的特定于用户的记忆来实现个性化生成。常见的做法是将这些记忆与输入提示连接起来，但这种方法很快就会耗尽设备上 LLM 中可用的有限上下文。通过平均来压缩内存可以减轻上下文增长，但由于异构内存之间的语义冲突，它经常会损害性能。在这项工作中，我们引入了一种基于集群的内存压缩策略，可以平衡上下文效率和个性化质量。我们的方法根据相似性对记忆进行分组，并在串联之前将它们合并到簇中，从而在减少冗余的同时保持一致性。实验表明，我们的方法大大降低了内存令牌的数量，同时优于朴素平均或直接串联等基线策略。此外，对于固定的上下文预算，集群驱动的合并会产生更紧凑的内存表示，并持续提高生成质量。</li>
</ul>

<h3>Title: Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes</h3>
<ul>
<li><strong>Authors: </strong>Gautam Siddharth Kashyap, Harsh Joshi, Niharika Jain, Ebad Shabbir, Jiechao Gao, Nipun Joshi, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17530">https://arxiv.org/abs/2601.17530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17530">https://arxiv.org/pdf/2601.17530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17530]] Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes(https://arxiv.org/abs/2601.17530)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.</li>
<li><strong>摘要：</strong>深度造假技术的迅速崛起，使超现实的合成媒体能够操纵公众的看法，对社会和政治稳定构成了严重威胁。然而，现有的检测方法面临两个核心限制：（1）模态碎片化，这导致在不同和对抗性的深度伪造模态之间泛化能力较差； （2）浅层模态间推理，导致细粒度语义不一致的检测有限。为了解决这些问题，我们提出了 ConLLM（大型语言模型对比学习），这是一种用于稳健的多模态深度伪造检测的混合框架。 ConLLM 采用两阶段架构：第一阶段使用预训练模型（PTM）来提取特定于模态的嵌入；第 2 阶段通过对比学习来对齐这些嵌入，以减轻模态碎片，并使用基于 LLM 的推理对其进行细化，以通过捕获语义不一致来解决浅层模态间推理。 ConLLM 在音频、视频和视听模式方面表现出强大的性能。它将音频 Deepfake EER 降低高达 50%，将视频准确率提高高达 8%，并在视听任务中实现约 9% 的准确率提升。消融研究证实，基于 PTM 的嵌入在各种模式中贡献了 9%-10% 的一致改进。</li>
</ul>

<h3>Title: Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Song, Yizhi Zhou, Xiangyu Kong, Jiulong Jiao, Xinrui Bao, Xu You, Xueqing Shi, Yuhang Zhou, Heng Qi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17532">https://arxiv.org/abs/2601.17532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17532">https://arxiv.org/pdf/2601.17532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17532]] Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection(https://arxiv.org/abs/2601.17532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）以外部证据为基础的大型语言模型，但在有限的上下文预算下，关键的挑战是决定应该注入哪些检索到的段落。我们表明，检索相关性指标（例如 NDCG）与端到端 QA 质量相关性较弱，甚至在多通道注入下可能呈负相关，其中冗余和轻微冲突会破坏生成的稳定性。我们提出\textbf{信息增益修剪（IGP）}，这是一种易于部署的重新排名和修剪模块，它使用与生成器对齐的效用信号来选择证据，并在截断之前过滤弱或有害的段落，而不改变现有的预算接口。在五个开放域 QA 基准测试以及多个检索器和生成器中，IGP 不断改进质量与成本的权衡。在代表性的多证据设置中，与仅检索器的基线相比，IGP 在平均 F1 方面提供了约 +12--20% 的相对改进，同时将最终阶段的输入标记减少了大约 76--79%。</li>
</ul>

<h3>Title: Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salemi, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17569">https://arxiv.org/abs/2601.17569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17569">https://arxiv.org/pdf/2601.17569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17569]] Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations(https://arxiv.org/abs/2601.17569)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.</li>
<li><strong>摘要：</strong>个性化对于将大语言模型 (LLM) 输出与个人用户偏好和背景知识保持一致至关重要。最先进的解决方案基于检索增强，从用户配置文件中检索相关上下文以供法学硕士使用。这些方法处理将检索到的私有数据暴露给云提供商和依赖能力较差的本地模型之间的权衡。我们推出 $P^3$，这是一个交互式框架，可实现高质量个性化，而无需向服务器端法学硕士泄露私人资料。在 $P^3$ 中，大型服务器端模型仅根据用户查询生成一系列 $k$ 草稿令牌，而小型客户端模型可检索用户的私人配置文件，评估和修改这些草稿以更好地反映用户偏好。重复此过程直到生成结束令牌。 LaMP-QA（由三个个性化问答数据集组成的最新基准）上的实验表明，$P^3$ 始终优于非个性化服务器端和个性化客户端基线，平均实现了 7.4%$ 至 9%$ 的统计显着改进。重要的是，$P^3$ 恢复了“泄漏”上限场景的 90.3%$ 至 95.7%$ 效用，在该场景中，完整的配置文件暴露于大型服务器端模型。隐私分析（包括可链接性和属性推断攻击）表明，$P^3$ 保留了非个性化服务器端模型的隐私，与在没有任何个人上下文的情况下提交查询相比，仅引入了边际额外泄漏（$1.5%$--$3.5%$）。此外，该框架对于边缘部署非常高效，客户端模型仅生成总代币的 9.2%$。这些结果表明，$P^3$ 为个性化生成提供了实用、有效的解决方案，并提高了隐私性。</li>
</ul>

<h3>Title: Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models</h3>
<ul>
<li><strong>Authors: </strong>Matija Luka Kukić, Marko Čuljak, David Dukić, Martin Tutek, Jan Šnajder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17585">https://arxiv.org/abs/2601.17585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17585">https://arxiv.org/pdf/2601.17585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17585]] Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models(https://arxiv.org/abs/2601.17585)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.</li>
<li><strong>摘要：</strong>现代语言模型 (LM) 以自回归方式进行训练，仅以前缀为条件。相比之下，序列标记（SL）任务为每个单独的输入标记分配标签，自然受益于双向上下文。这种差异历来导致 SL 依赖于固有的仅双向编码器模型。然而，仅解码器模型的快速发展提出了它们是否可以适应 SL 的问题。虽然因果掩模去除已成为一种可行的技术，用于调整仅解码器模型以利用 SL 的完整上下文，但它需要对基本模型功能进行大量更改。在这项工作中，我们探索序列重复（SR）作为一种侵入性较小的替代方案，以在仅解码器模型中实现双向性。通过微调实验，我们表明 SR 本质上使解码器具有双向性，提高了 token 级嵌入的质量并超越了编码器和未屏蔽的解码器。与之前的说法相反，我们发现增加重复次数并不会降低 SL 性能。最后，我们证明，与最终层的嵌入相比，中间层的嵌入对于 SR 非常有效，同时计算效率显着提高。我们的研究结果强调，SR 缓解了解码器的结构限制，实现了更高效、适应性更强的 LM，并扩大了它们对其他令牌级任务的适用性。</li>
</ul>

<h3>Title: From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianjun Zhong, Linyang He, Nima Mesgarani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17593">https://arxiv.org/abs/2601.17593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17593">https://arxiv.org/pdf/2601.17593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17593]] From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs(https://arxiv.org/abs/2601.17593)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question. In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.</li>
<li><strong>摘要：</strong>大型语言模型的最新进展重新激发了人们对机械地描述多步推理如何表示和计算的兴趣。虽然许多先前的工作将推理视为线性步骤链，但许多推理问题更自然地构造为有向无环图（DAG），其中中间结论可能取决于多个前提，分支为并行子推导，然后合并或重用。理解这种图结构推理是否反映在模型内部仍然是一个悬而未决的问题。在这项工作中，我们引入了 Reasoning DAG Probing，这是一个框架，它直接询问 LLM 隐藏状态是否以可线性访问的形式编码推理 DAG 的几何形状，以及该结构跨层出现的位置。在此框架内，我们将每个推理节点与文本实现相关联，并训练轻量级探针以从隐藏状态预测两个图论属性：节点深度和成对节点距离。我们使用这些探针来分析 DAG 结构的分层出现，并评估破坏推理相关结构同时保留表面文本属性的控制。我们的结果提供了证据，证明 DAG 几何推理在中间层中进行了有意义的编码，可恢复性随节点深度和模型规模而系统变化，这表明 LLM 推理不仅是顺序的，而且表现出可测量的内部图结构。</li>
</ul>

<h3>Title: Learning to Ideate for Machine Learning Engineering Agents</h3>
<ul>
<li><strong>Authors: </strong>Yunxiang Zhang, Kang Zhou, Zhichao Xu, Kiran Ramnath, Yun Zhou, Sangmin Woo, Haibo Ding, Lin Lee Cheong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17596">https://arxiv.org/abs/2601.17596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17596">https://arxiv.org/pdf/2601.17596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17596]] Learning to Ideate for Machine Learning Engineering Agents(https://arxiv.org/abs/2601.17596)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.</li>
<li><strong>摘要：</strong>现有的机器学习工程（MLE）代理努力迭代优化其实施的算法以提高有效性。为了解决这个问题，我们引入了 MLE-Ideator，这是一个将构思与实现分开的双代理框架。在我们的系统中，实施代理可以向专门的创意者请求战略帮助。我们证明这种方法在两个方面是有效的。首先，在免训练设置中，我们的框架显着优于 MLE-Bench 上仅实现的代理基线。其次，我们证明创意者可以通过强化学习（RL）进行训练，以产生更有效的想法。仅使用来自 10 个 MLE 任务的 1K 训练样本，我们经过 RL 训练的 Qwen3-8B Ideator 与未经训练的对应物相比实现了 11.5% 的相对改进，并超过了 Claude Sonnet 3.5。这些结果凸显了训练战略人工智能系统以进行科学发现的一条有前途的道路。</li>
</ul>

<h3>Title: What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization</h3>
<ul>
<li><strong>Authors: </strong>Sara Rezaeimanesh, Mohammad M. Ghassemi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17609">https://arxiv.org/abs/2601.17609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17609">https://arxiv.org/pdf/2601.17609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17609]] What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization(https://arxiv.org/abs/2601.17609)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.</li>
<li><strong>摘要：</strong>在医学和金融等领域，大规模标记数据成本高昂且通常无法获得，导致在小型数据集上训练的模型难以推广到现实世界的人群。大型语言模型包含来自这些领域多年研究的广泛知识。我们提出了 LoID（Logit-Informed Distributions），这是一种确定性方法，用于通过直接访问其标记级预测来提取贝叶斯逻辑回归的信息先验分布。我们不依赖生成的文本，而是通过精心构建的句子来探究模型在相反语义方向（正面影响与负面影响）上的置信度。通过衡量法学硕士在不同措辞中倾向于一个方向的一致性，我们提取了模型对每个特征影响的信念的强度和可靠性。我们在以协变量偏移为特征的合成分布外 (OOD) 设置下评估了 10 个现实世界表格数据集的 LoID，其中训练数据仅代表总体的一个子集。我们将我们的方法与（1）标准无信息先验，（2）AutoElicit，一种提示LLM通过文本完成生成先验的最新方法，（3）LLMProcesses，一种使用LLM通过上下文学习生成数值预测的方法以及（4）通过在完整数据集上拟合逻辑回归而得出的预言式上限进行比较。我们使用曲线下面积 (AUC) 评估性能。在整个数据集上，LoID 显着提高了在 OOD 数据上训练的逻辑回归的性能，相对于预言机模型恢复了 \textbf{59\%} 的性能差距。 LoID 在 10 个数据集中的 8 个上优于 AutoElicit 和 LLMProcessesc，同时提供了一种可重复且计算高效的机制，用于将 LLM 知识集成到贝叶斯推理中。</li>
</ul>

<h3>Title: Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization</h3>
<ul>
<li><strong>Authors: </strong>Bich Ngoc (Rubi)Doan, Giuseppe Russo, Gianmarco De Francisci Morales, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17658">https://arxiv.org/abs/2601.17658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17658">https://arxiv.org/pdf/2601.17658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17658]] Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization(https://arxiv.org/abs/2601.17658)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term "radicalization personas." Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.</li>
<li><strong>摘要：</strong>阴谋论的兴起削弱了信任并加剧了两极分化，在公共话语中造成了深远的社会危害。除了这种公共影响之外，阴谋论信徒的朋友和家人也遭受了深刻的个人损失，这是大规模计算研究中经常被忽视的一个方面。这项研究通过系统地绘制激进化历程并量化对亲人造成的相关情感损失来填补这一空白。我们使用 QAnon 的著名案例作为案例研究，通过新颖的混合方法分析了来自 r/QAnonCasualties 支持社区的 12747 个叙述。首先，我们使用主题建模 (BERTopic) 来绘制激进化轨迹，识别关键的预先存在的条件、触发因素和激进化后的特征。由此，我们应用基于 LDA 的图形模型来揭示 QAnon 追随者的六种重复出现的原型，我们将其称为“激进角色”。最后，使用法学硕士辅助的情绪检测和回归模型，我们将这些人物角色与叙述者报告的特定情绪损失联系起来。我们的研究结果表明，这些人物角色不仅是描述性的，而且是描述性的。它们是叙述者所经历的特定情感伤害的有力预测因素。激进化被视为一种故意的意识形态选择，与叙述者的愤怒和厌恶有关，而那些以个人和认知崩溃为标志的则与恐惧和悲伤有关。这项工作为将激进化理解为一种关系现象提供了第一个实证框架，为研究人员和实践者应对其人际影响提供了重要的路线图。</li>
</ul>

<h3>Title: UrduLM: A Resource-Efficient Monolingual Urdu Language Model</h3>
<ul>
<li><strong>Authors: </strong>Syed Muhammad Ali, Hammad Sajid, Zainab Haider, Ali Muhammad Asad, Haya Fatima, Abdul Samad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17664">https://arxiv.org/abs/2601.17664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17664">https://arxiv.org/pdf/2601.17664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17664]] UrduLM: A Resource-Efficient Monolingual Urdu Language Model(https://arxiv.org/abs/2601.17664)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.</li>
<li><strong>摘要：</strong>乌尔都语在全球有 2.3 亿人使用，缺乏专门的基于 Transformer 的语言模型和精选语料库。虽然多语言模型提供有限的乌尔都语支持，但由于训练数据不足，它们的性能较差，计算成本较高，并且文化不准确。为了应对这些挑战，我们提出了 UrduLM，一种在资源匮乏的环境中训练的预训练乌尔都语单语语言模型。我们从不同来源策划了 33GB 乌尔都语语料库，开发了一个自定义 BPE 分词器，与多语言替代方案相比，它可以将分词开销减少至少 20-30%，并预训练 100M 参数的仅解码器模型。在几次评估中，UrduLM 凭借高达 30 倍大小的多语言模型实现了具有竞争力的性能，在情感分类上达到 66.6% 的准确率，在语法纠正任务上的 BLEU 分数超过 30。完整的方法论——包括语料库、分词器、模型权重和评估基准——公开发布，为乌尔都语 NLP 研究建立基线，并为其他代表性不足的语言提供可扩展的框架。</li>
</ul>

<h3>Title: Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chunxu Zhao, Xin Huang, Xue Han, Shujian Huang, Chao Deng, Junlan Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17671">https://arxiv.org/abs/2601.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17671">https://arxiv.org/pdf/2601.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17671]] Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning(https://arxiv.org/abs/2601.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 展现出令人印象深刻的推理能力，但经验证据表明，它们并不像预期的那样与语言无关，导致多语言环境中的性能下降，尤其是对于资源匮乏的语言。我们将这种下降归因于模型不一致的多语言理解和推理一致性。为了解决这个问题，我们提出了枢轴对齐自反馈多语言推理（PASMR），旨在提高法学硕士多语言数学推理能力的一致性。此方法将模型的主要语言指定为枢轴语言。在训练过程中，模型首先将问题翻译成枢轴语言，以便更好地协调推理模式。然后，目标语言的推理过程由枢轴语言的推理答案监督，从而建立跨语言的自我反馈机制，而不依赖于外部正确答案或奖励模型。大量的实验结果表明，我们的方法增强了模型对问题的理解及其推理能力，从而显着改进了任务。</li>
</ul>

<h3>Title: S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Qingsen Ma, Dianyun Wang, Yaoye Wang, Lechen Ning, Sujie Zhu, Xiaohang Zhang, Jiaming Lyu, Linhao Ren, Zhenbo Xu, Zhaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17702">https://arxiv.org/abs/2601.17702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17702">https://arxiv.org/pdf/2601.17702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17702]] S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference(https://arxiv.org/abs/2601.17702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages. We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size. At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地应用于多文档和长格式输入，但长上下文推理仍然缺乏内存和噪声效率。键值 (KV) 缓存随上下文长度线性扩展，而外部检索方法通常返回词汇相似但因果不相关的段落。我们提出了 S3-Attention，这是一种内存优先的推理时间框架，它将长上下文处理视为注意力对齐的内源检索。 S3-Attention 使用轻量级稀疏自动编码器将瞬态键和查询投影解码为 top-k 稀疏特征标识符，并在单次流扫描期间构建基于 CPU 的倒排索引将特征映射到标记位置或跨度。这种设计允许完全丢弃 KV 缓存，并通过扫描块大小限制 GPU 内存使用。在生成时，特征共激活用于检索紧凑的证据范围，可选地与 BM25 融合以实现精确的词汇匹配。在具有固定提示、解码和匹配令牌预算的统一 LongBench 评估协议下，S3-Hybrid 紧密匹配多个模型系列的全上下文推理，并提高了多种信息密集设置中的鲁棒性。我们还报告了当前原型的工程限制，这会导致比优化的全 KV 基线更高的挂钟延迟，从而推动未来的内核级优化。</li>
</ul>

<h3>Title: Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Qureshi, Kenneth Rice, Alexander Wolpert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17705">https://arxiv.org/abs/2601.17705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17705">https://arxiv.org/pdf/2601.17705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17705]] Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings(https://arxiv.org/abs/2601.17705)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.</li>
<li><strong>摘要：</strong>仅当符合人类对文本之间相似性的感知时，文本嵌入之间的相似性度量才可以被认为是足够的。在本文中，我们介绍了距离与距离比（DDR），这是一种衡量 LLM 句子嵌入之间相似性的新方法。受 Lipschitz 连续性的启发，DDR 衡量上下文前的词嵌入之间的相似度和上下文后的 LLM 嵌入之间的相似度的变化率，从而衡量上下文的语义影响。我们在实验中评估 DDR 的性能，实验设计为对从句子数据集提取的句子应用一系列扰动。对于每个句子，我们通过用同义词（构成语义相似的文本）或随机选择的单词（构成语义不同的文本）替换一个、两个或三个单词来生成变体。我们将 DDR 的性能与其他流行的相似性指标进行比较，并证明 DDR 始终能够在语义相似和不相似的文本之间提供更精细的区分，即使在最小的受控编辑下也是如此。</li>
</ul>

<h3>Title: A Computational Approach to Visual Metonymy</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Ghosh, Linfeng Liu, Tianyu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17706">https://arxiv.org/abs/2601.17706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17706">https://arxiv.org/pdf/2601.17706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17706]] A Computational Approach to Visual Metonymy(https://arxiv.org/abs/2601.17706)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: this https URL.</li>
<li><strong>摘要：</strong>图像传达的信息往往比字面上描述的要多：一套工具可以暗示一种职业，一件文化制品可以暗示一种传统。这种间接的视觉参考，称为视觉转喻，邀请观众通过相关线索而不是明确的描述来恢复目标概念。在这项工作中，我们首次提出了视觉转喻的计算研究。我们引入了一种基于符号学理论的新颖管道，它利用大型语言模型和文本到图像模型来生成转喻视觉表示。使用这个框架，我们构建了 ViMET，这是第一个视觉转喻数据集，包含 2000 个多项选择题，用于评估多模态语言模型中的认知推理能力。我们数据集的实验结果揭示了人类表现 (86.9%) 和最先进的视觉语言模型 (65.9%) 之间的显着差距，凸显了机器解释间接视觉参考的能力的局限性。我们的数据集可在以下网址公开获取：此 https URL。</li>
</ul>

<h3>Title: Unsupervised Elicitation of Moral Values from Language Models</h3>
<ul>
<li><strong>Authors: </strong>Meysam Alizadeh, Fabrizio Gilardi, Zeynab Samei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17728">https://arxiv.org/abs/2601.17728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17728">https://arxiv.org/pdf/2601.17728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17728]] Unsupervised Elicitation of Moral Values from Language Models(https://arxiv.org/abs/2601.17728)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.</li>
<li><strong>摘要：</strong>随着人工智能系统变得普遍，将其行为植根于人类价值观至关重要。先前的研究表明，语言模型（LM）表现出有限的内在道德推理，导致人们呼吁明确的道德教学。然而，考虑到多元框架和普遍存在的偏见，构建用于道德评估的真实数据是很困难的。我们研究了无监督启发作为替代方案，询问预训练的（基础）LM 是否具有内在的道德推理能力，可以在没有人类监督的情况下显现出来。我们在三个基准数据集和四个 LM 中使用内部一致性最大化 (ICM) 算法，测试 ICM 是否能够可靠地标记道德判断、跨道德框架进行泛化并减轻社会偏见。结果表明，ICM 在 Norm Bank 和 ETHICS 基准上优于所有预训练和聊天机器人基线，而 ICM 标签的微调性能与人类标签相当或超过。在理论驱动的道德框架中，ICM 在正义和常识道德方面取得了最大的相对收益。此外，尽管聊天机器人 LM 表现出的社会偏见失败率与预训练的 LM 相当，但 ICM 将此类错误减少了一半以上，其中种族、社会经济地位和政治方面的改善最大。这些发现表明，经过预训练的 LM 具有潜在的道德推理能力，可以通过 ICM 等无监督方法来激发，从而为 AI 对齐提供可扩展的路径。</li>
</ul>

<h3>Title: ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyoung Park, Sanghyeok Lee, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17755">https://arxiv.org/abs/2601.17755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17755">https://arxiv.org/pdf/2601.17755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17755]] ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation(https://arxiv.org/abs/2601.17755)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.</li>
<li><strong>摘要：</strong>图检索增强生成（GraphRAG）通过将外部知识组织成实体和关系的结构化图，已成功应用于各种知识密集型问答任务。它使大型语言模型 (LLM) 能够执行文本块检索之外的复杂推理。最近的工作采用强化学习（RL）来训练代理 GraphRAG 框架，该框架在法学硕士和知识图之间执行迭代交互。然而，现有的基于强化学习的框架（例如 Graph-R1）存在两个关键限制：（1）它们主要依赖于语义相似性进行检索，通常忽略底层图结构；（2）它们依赖于稀疏的结果级奖励，无法捕获中间检索步骤的质量及其依赖性。为了解决这些限制，我们提出了 ProGraph-R1，一种用于基于图的检索和多步骤推理的进度感知代理框架。 ProGraph-R1引入了一种结构感知的超图检索机制，该机制共同考虑语义相关性和图连接性，鼓励沿多跳推理路径的连贯遍历。我们还设计了一种基于进度的逐步策略优化，它通过根据图中的中间推理进度调节优势来提供密集的学习信号，而不是仅仅依赖于最终结果。多跳问答基准实验表明，与现有 GraphRAG 方法相比，ProGraph-R1 持续提高了推理准确性和生成质量。</li>
</ul>

<h3>Title: Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali</h3>
<ul>
<li><strong>Authors: </strong>Md Asgor Hossain Reaj, Rajan Das Gupta, Jui Saha Pritha, Abdullah Al Noman, Abir Ahmed, Golam Md Mohiuddin, Tze Hui Liew</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17764">https://arxiv.org/abs/2601.17764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17764">https://arxiv.org/pdf/2601.17764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17764]] Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali(https://arxiv.org/abs/2601.17764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLM）取得了巨大的成功；然而，固有的性别偏见问题仍然存在，尤其是在非英语语言中。尽管目前的研究主要强调英语，但对孟加拉语等南半球语言固有的语言和文化偏见却很少进行研究。本研究旨在研究孟加拉语性别偏见的特征和严重程度，评估当前方法在识别和减轻偏见方面的有效性。我们使用多种方法来提取性别偏见的话语，包括基于词典的挖掘、计算分类模型、基于翻译的比较分析和基于 GPT 的偏见创建。我们的研究表明，以英语为中心的偏见检测框架直接应用于孟加拉语受到语言差异和影响隐性偏见的社会文化因素的严重限制。为了解决这些困难，我们在农村和低收入地区进行了两次实地调查，收集了有关性别偏见的真实见解。研究结果表明，孟加拉语的性别偏见与英语相比呈现出明显的特征，需要更加本地化和上下文敏感的方法。此外，我们的研究强调需要整合社区驱动的研究方法来识别经常被自动化系统忽视的文化相关偏见。我们的研究通过说明需要创建专门为代表性不足的语言设计的语言工具，加强了围绕人工智能中性别偏见的持续讨论。这项研究为进一步研究减少孟加拉语和其他印度语言的偏见奠定了基础，促进更具包容性和公平的 NLP 系统的发展。</li>
</ul>

<h3>Title: DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Liu, Xiaoyu Guan, Di Liang, Xianjie Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17777">https://arxiv.org/abs/2601.17777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17777">https://arxiv.org/pdf/2601.17777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17777]] DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning(https://arxiv.org/abs/2601.17777)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.</li>
<li><strong>摘要：</strong>有监督微调（SFT）是使大型语言模型（LLM）适应下游任务的关键步骤。然而，异构 SFT 任务之间相互冲突的目标常常会引发“跷跷板效应”：对一项任务进行优化可能会降低其他任务的性能，特别是当模型参数不加区别地更新时。在本文中，我们提出了一种原理性方法来解开和隔离特定于任务的参数区域，其动机是参数异质性是跨任务干扰的基础。具体来说，我们首先在不同的 SFT 任务上独立地微调 LLM，并将每个任务的核心参数区域识别为表现出最大更新的参数子集。具有高度重叠的核心参数区域的任务被合并以进行联合训练，而不相交的任务被组织到不同的阶段。在多阶段SFT期间，先前任务中获取的核心参数被冻结，从而防止后续任务覆盖。为了验证我们方法的有效性，我们对多个公共数据集进行了深入的实验。结果表明，与多阶段和多任务调整基线相比，我们的动态参数隔离策略持续减少了数据冲突，并实现了一致的性能改进。</li>
</ul>

<h3>Title: Controlling Reading Ease with Gaze-Guided Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Andreas Säuberli, Darja Jepifanova, Diego Frassinelli, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17781">https://arxiv.org/abs/2601.17781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17781">https://arxiv.org/pdf/2601.17781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17781]] Controlling Reading Ease with Gaze-Guided Text Generation(https://arxiv.org/abs/2601.17781)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.</li>
<li><strong>摘要：</strong>阅读时眼睛移动的方式可以告诉我们处理文本所需的认知努力。在本研究中，我们利用这一事实来生成具有可控阅读难度的文本。我们的方法采用预测人类注视模式的模型来引导语言模型输出引发某些阅读行为。我们通过对英语为母语和非英语为母语的人进行眼动追踪实验来评估该方法。结果表明，该方法可以有效地使生成的文本更容易或更难阅读，从阅读时间和文本的感知难度两方面进行衡量。统计分析表明，阅读行为的变化主要是由于影响词汇处理的特征造成的。我们的方法的可能应用包括用于信息可访问性的文本简化以及用于语言学习的个性化教育材料的生成。</li>
</ul>

<h3>Title: Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations</h3>
<ul>
<li><strong>Authors: </strong>Yixin Liu, Kehan Yan, Shiyuan Li, Qingfeng Chen, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17786">https://arxiv.org/abs/2601.17786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17786">https://arxiv.org/pdf/2601.17786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17786]] Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations(https://arxiv.org/abs/2601.17786)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at this https URL.</li>
<li><strong>摘要：</strong>文本异常检测 (TAD) 在各种语言驱动的实际应用中发挥着至关重要的作用，包括有害内容审核、网络钓鱼检测和垃圾邮件评论过滤。虽然两步“嵌入检测器”TAD 方法已显示出最先进的性能，但其有效性通常受到使用单一嵌入模型以及缺乏跨不同数据集和异常类型的适应性的限制。为了解决这些限制，我们建议利用多个预训练语言模型的嵌入，并将它们集成到多视图 TAD 框架 $MCA^2$ 中。 $MCA^2$采用多视图重建模型，从多个嵌入角度有效地提取正常文本模式。为了利用视图间的互补性，设计了对比协作模块来利用和加强不同视图之间的交互。此外，还开发了自适应分配模块来自动分配每个视图的贡献权重，从而提高对不同数据集的适应性。对 10 个基准数据集进行的广泛实验验证了 $MCA^2$ 相对于强基线的有效性。 $MCA^2$ 的源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents</h3>
<ul>
<li><strong>Authors: </strong>Dan Greenstein, Zohar Karnin, Chen Amiraz, Oren Somekh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17829">https://arxiv.org/abs/2601.17829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17829">https://arxiv.org/pdf/2601.17829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17829]] Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents(https://arxiv.org/abs/2601.17829)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.</li>
<li><strong>摘要：</strong>函数调用代理的构建已成为扩展模型功能的有前途的途径。这项任务的一个主要挑战是获取高质量的多样化数据进行训练。先前的工作强调功能、调用模式和交互转向的多样性，但请求的语言多样性和参数的覆盖范围（例如 \texttt{city\_name}、\texttt{stock\_ticker}）仍未得到充分探索。我们提出了一种方法，通过优化查询和参数的通用多样性指标来生成合成数据集，而不依赖于手工制定的规则或分类法，从而使其对不同的用例具有鲁棒性。我们通过内在和外在测试证明了我们技术的有效性，并将其与 SoTA 数据生成方法进行比较。我们在多样性方面表现出优于基线的优势，同时保持可比较的正确性。此外，当用作训练集时，与基于基线数据生成方法的类似模型相比，由我们的数据集生成的模型在分布外性能方面表现出卓越的性能。特别是，与类似的同行相比，我们在 BFCL 基准上的准确率提高了 7.4\%$。</li>
</ul>

<h3>Title: EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy</h3>
<ul>
<li><strong>Authors: </strong>Lanqing Du, Yunong Li, YuJie Long, Shihong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17842">https://arxiv.org/abs/2601.17842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17842">https://arxiv.org/pdf/2601.17842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17842]] EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy(https://arxiv.org/abs/2601.17842)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a "top-down" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a "bottom-up" trajectory, it deconstructs the intervention into a three-stage reasoning flow: "Embodied Perception - Cognitive Exploration - Narrative Intervention." Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed "EFT-Instruct," a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.</li>
<li><strong>摘要：</strong>利用大型语言模型 (LLM) 进行心理健康问答 (MHQA) 有望缓解资源短缺。然而，现有的基于认知行为治疗（CBT）的方法主要倾向于“自上而下”的理性重组，常常忽视客户的具体体验和初级情绪处理。为了解决这个问题，我们提出了一种基于情绪聚焦疗法（EFT）的多智能体思想链框架（EFT-CoT）。采用“自下而上”的轨迹，将干预解构为三阶段推理流程：“具身感知-认知探索-叙事干预”。该系统利用八个专门代理，明确执行关键组件，例如躯体意识映射、适应性评估、核心信念提取和叙事重组。我们进一步构建了“EFT-Instruct”，这是一个通过思想链蒸馏约 67,000 篇真实文本的高质量数据集，并对专门模型 EFT-LLM 进行了微调。实验评估表明，EFT-LLM 在同理心深度和结构专业精神等指标上优于强大的基线和人类反应。消融研究证实了多主体机制的必要性。该模型展示了卓越的心理推理能力，为可解释的、高度同理心的咨询系统提供了有效的途径。</li>
</ul>

<h3>Title: D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jia Gu, Liang Pang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17865">https://arxiv.org/abs/2601.17865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17865">https://arxiv.org/pdf/2601.17865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17865]] D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models(https://arxiv.org/abs/2601.17865)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）中下一个令牌（P_token）的预测概率与下一条信息的相关概率、下一个产品的购买概率以及下一个动作的执行概率有着千丝万缕的联系——所有这些都属于任务级目标分布（P_task）的范围。虽然众所周知，法学硕士可以生成接近现实世界分布的样本，但它们的细粒度采样概率是否忠实地符合任务要求仍然是一个悬而未决的问题。通过受控分布抽样模拟，我们发现了 LLM 行为中的显着二分法，区分了两种模型类型：D 模型（例如 Qwen-2.5），其 P_token 表现出较大的逐步变异性，并且与 P_task 的一致性较差；和 E 模型（例如 Mistral-Small），其 P_token 更稳定并且与 P_task 更好地保持一致。我们在代码生成和推荐等下游任务中进一步评估这两种模型类型，揭示了影响任务结果的多样性和稳定性之间的系统权衡。最后，我们分析了两个模型家族的内部属性，以探讨它们的潜在机制。这些发现为法学硕士的概率抽样行为提供了基础见解，并为何时支持 D 模型和 E 模型提供了实用指导。对于网络规模的应用程序，包括推荐、搜索和会话代理，我们的结果为模型选择和配置提供了信息，以在现实世界的不确定性下平衡多样性与可靠性，从而提供更好的解释水平。</li>
</ul>

<h3>Title: On the Emergence and Test-Time Use of Structural Information in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michelle Chao Chen, Moritz Miller, Bernhard Schölkopf, Siyuan Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17869">https://arxiv.org/abs/2601.17869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17869">https://arxiv.org/pdf/2601.17869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17869]] On the Emergence and Test-Time Use of Structural Information in Large Language Models(https://arxiv.org/abs/2601.17869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.</li>
<li><strong>摘要：</strong>从观察数据中学习结构信息对于在训练语料库之外产生新知识至关重要。这适用于科学发现中的机械理解以及灵活的测试时间组合生成。因此，我们研究语言模型如何学习抽象结构并在测试时利用学习到的结构信息。为了确保受控设置，我们设计了一个基于语言结构转换的自然语言数据集。我们凭经验表明，学习结构信息的出现与复杂的推理任务相关，并且执行测试时组合生成的能力仍然有限。</li>
</ul>

<h3>Title: Self-Manager: Parallel Agent Loop for Long-form Deep Research</h3>
<ul>
<li><strong>Authors: </strong>Yilong Xu, Zhi Zheng, Xiang Long, Yujun Cai, Yiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17879">https://arxiv.org/abs/2601.17879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17879">https://arxiv.org/pdf/2601.17879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17879]] Self-Manager: Parallel Agent Loop for Long-form Deep Research(https://arxiv.org/abs/2601.17879)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.</li>
<li><strong>摘要：</strong>长期深入研究需要在更广阔的视野内进行多方面的调查才能获得全面的报告。在处理此类复杂任务时，现有代理在子任务级别管理上下文，以克服线性上下文积累和信息丢失。然而，它们仍然坚持单一上下文窗口和顺序执行范式，这导致相互干扰和阻塞行为，限制了可扩展性和适应性。为了解决这个问题，本文引入了 Self-Manager，这是一个支持异步和并发执行的并行代理循环。主线程可以创建多个子线程，每个子线程都有自己的隔离上下文，并通过线程控制块迭代地管理它们，从而允许更集中和灵活的并行代理执行。为了评估其有效性，我们在 DeepResearch Bench 上对 Self-Manager 进行了基准测试，它在所有指标上始终优于现有的单代理循环基线。此外，我们进行了广泛的分析实验，以证明自我管理器设计选择的必要性，以及其在情境能力、效率和泛化方面的优势。</li>
</ul>

<h3>Title: Assessment of Generative Named Entity Recognition in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhan, Yile Wang, Hui Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17898">https://arxiv.org/abs/2601.17898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17898">https://arxiv.org/pdf/2601.17898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17898]] Assessment of Generative Named Entity Recognition in the Era of Large Language Models(https://arxiv.org/abs/2601.17898)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的兴起，命名实体识别（NER）正在从序列标记任务演变为生成范式。我们对开源法学硕士在平面和嵌套 NER 任务上进行了系统评估。我们调查了几个研究问题，包括生成 NER 和传统 NER 模型之间的性能差距、输出格式的影响、LLM 是否依赖记忆以及微调后一般能力的保留。通过对 8 个不同规模的 LLM 和 4 个标准 NER 数据集的实验，我们发现：（1）通过参数高效的微调和内联括号或 XML 等结构化格式，开源 LLM 的性能可与传统基于编码器的模型相媲美，并超越 GPT-3 等闭源 LLM； （2）法学硕士的NER能力源于指令遵循和生成能力，而不仅仅是实体标签对的记忆； (3) 应用 NER 指令调整对法学硕士的一般能力影响最小，甚至由于增强的实体理解而提高了 DROP 等数据集的性能。这些发现表明，法学硕士的生成 NER 是传统方法的一种有前景、用户友好的替代方案。我们在此 https URL 发布数据和代码。</li>
</ul>

<h3>Title: ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhao, Qinghua Yao, Xinyuan song, Wei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17921">https://arxiv.org/abs/2601.17921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17921">https://arxiv.org/pdf/2601.17921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17921]] ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation(https://arxiv.org/abs/2601.17921)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.</li>
<li><strong>摘要：</strong>低秩自适应（LoRA）是参数高效微调（PEFT）领域的代表性方法，是现代大语言模型（LLM）民主化的关键。普通的 LoRA 采用统一的排名来实现，最近的文献发现，在 LLM 主干上正确分配排名可以带来性能提升。然而，以前的排名分配方法存在局限性，因为它们依赖于 LoRA 排名的不可解释且不可靠的重要性度量。为了解决上述问题，我们提出了ShapLoRA框架。受可解释归因度量 Shapley 值的启发，我们将基于敏感性的度量与 LoRA 等级之间的协作博弈中的联盟思想相结合，并提出了一种更可解释的重要性度量，称为 Shapley 敏感性。此外，我们通过以下方式优化现有工作的工作流程：（a）在单独的验证集上计算 Shapley 敏感性； (b) 建立公平比较的分配-再培训程序。我们对各种具有挑战性的任务进行了实验，实验结果表明，我们的 ShapLoRA 方法可以在具有可比可调参数的情况下优于最近的基线。\footnote{代码和微调模型将开源，以促进未来的研究。</li>
</ul>

<h3>Title: A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D'Ercoli, Subati Abulikemu, Zhongtian Sun, Richard Bethlehem, Pietro Lio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17952">https://arxiv.org/abs/2601.17952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17952">https://arxiv.org/pdf/2601.17952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17952]] A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models(https://arxiv.org/abs/2601.17952)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.</li>
<li><strong>摘要：</strong>可解释性仍然是在阿尔茨海默病进展诊断等临床环境中部署大型语言模型 (LLM) 的一个关键挑战，在这些环境中，早期且可靠的预测至关重要。由于LLM表示的多语义性质，现有的归因方法表现出较高的方法间变异性和不稳定的解释，而机械可解释性方法缺乏与模型输入和输出的直接对齐，并且不提供明确的重要性分数。我们引入了一个统一的可解释性框架，通过单语义特征提取整合了归因和机械视角。通过在 LLM 层级别构建单语义嵌入空间并优化框架以显式减少方法间变异性，我们的方法可产生稳定的输入级重要性得分，并通过感兴趣层的解压缩表示突出显着特征，从而推进 LLM 在认知健康和神经退行性疾病中安全且值得信赖的应用。</li>
</ul>

<h3>Title: LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction</h3>
<ul>
<li><strong>Authors: </strong>Junior Cedric Tonga, Chen Cecilia Liu, Iryna Gurevych, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17971">https://arxiv.org/abs/2601.17971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17971">https://arxiv.org/pdf/2601.17971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17971]] LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction(https://arxiv.org/abs/2601.17971)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 编码从不同的网络规模数据中学习到的丰富文化知识，为大规模建模文化常识提供了前所未有的机会。然而，这些知识仍然大多是隐性的和非结构化的，限制了其可解释性和使用。我们提出了一个基于提示的迭代框架，用于构建文化常识知识图谱（CCKG），将法学硕士视为文化档案，系统地引出文化特定的实体、关系和实践，并将它们组成跨语言的多步骤推理链。我们通过人类对文化相关性、正确性和路径一致性的判断来评估五个国家的 CCKG。我们发现，即使目标文化是非英语（例如中文、印度尼西亚语、阿拉伯语），文化知识图谱也可以用英语更好地实现，这表明当前法学硕士的文化编码不均匀。通过 CCKG 增强小型法学硕士的能力，可以提高文化推理和故事生成的表现，其中英国连锁店的收益最大。我们的结果表明了法学硕士作为文化技术的前景和局限性，以及链式结构的文化知识是基于文化的 NLP 的实用基础。</li>
</ul>

<h3>Title: SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets</h3>
<ul>
<li><strong>Authors: </strong>Kshitij Mishra, Nils Lukas, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17982">https://arxiv.org/abs/2601.17982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17982">https://arxiv.org/pdf/2601.17982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17982]] SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets(https://arxiv.org/abs/2601.17982)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.</li>
<li><strong>摘要：</strong>小语言模型 (SLM) 很难处理复杂的推理，因为在计算预算紧张的情况下探索成本高昂。我们引入了语义多样性-探索-利用（SD-E$^2$），这是一种强化学习框架，它通过优化生成的推理轨迹中的语义多样性来使探索变得明确。使用冻结的句子嵌入模型，SD-E$^2$ 分配多样性奖励，捕获 (i) 语义上不同的解决方案策略的覆盖范围和 (ii) 它们在嵌入空间中的平均成对差异性，而不是表面形式的新颖性。这种多样性奖励与 z 分数归一化多目标中的结果正确性和解决方案效率相结合，稳定了训练。在 GSM8K 上，SD-E$^2$ 分别超过基础 Qwen2.5-3B-Instruct 和强大的 GRPO 基线（GRPO-CFL 和 GRPO-CFEE）+27.4、+5.2 和 +1.5 个百分点，同时每个问题平均发现 9.8 个语义不同的策略。我们进一步将 MedMCQA 提高到 49.64%，而基础模型为 38.37%，并且在更严格的 AIME 基准（1983-2025）上显示出进步，达到 13.28%，而基础模型为 6.74%。这些结果表明，奖励语义新颖性可以为训练具有推理能力的 SLM 提供计算效率更高的探索利用信号。通过引入认知适应——调整推理过程结构而不是每个令牌计算——SD-E$^2$ 为资源受限模型中的效率提升提供了补充途径。</li>
</ul>

<h3>Title: AI-based approach to burnout identification from textual data</h3>
<ul>
<li><strong>Authors: </strong>Marina Zavertiaeva, Petr Parshakov, Mikhail Usanin, Aleksei Smirnov, Sofia Paklina, Anastasiia Kibardina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17993">https://arxiv.org/abs/2601.17993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17993">https://arxiv.org/pdf/2601.17993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17993]] AI-based approach to burnout identification from textual data(https://arxiv.org/abs/2601.17993)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.</li>
<li><strong>摘要：</strong>本研究引入了一种基于人工智能的方法，该方法利用自然语言处理（NLP）来检测文本数据中的倦怠。该方法依赖于 RuBERT 模型，该模型最初经过训练用于情感分析，随后使用两个数据源进行微调以进行倦怠检测：使用 ChatGPT 生成的合成句子以及从俄罗斯 YouTube 视频中收集的关于倦怠的用户评论。由此产生的模型为输入文本分配倦怠概率，并可应用于处理大量书面通信，以监控高压力工作环境中与倦怠相关的语言信号。</li>
</ul>

<h3>Title: Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems</h3>
<ul>
<li><strong>Authors: </strong>Hendrika Maclean, Mert Can Cakmak, Muzakkiruddin Ahmed Mohammed, Shames Al Mandalawi, John Talburt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18012">https://arxiv.org/abs/2601.18012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18012">https://arxiv.org/pdf/2601.18012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18012]] Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems(https://arxiv.org/abs/2601.18012)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.</li>
<li><strong>摘要：</strong>现在，大型语言模型每天都用于写作、搜索和分析，并且它们的自然语言理解能力不断提高。然而，它们在精确的数值计算和生成易于审计的输出方面仍然不可靠。我们将综合薪资系统作为一个重点突出的高风险示例进行研究，并评估模型是否能够理解薪资模式、以正确的顺序应用规则并提供精确到百分之一的结果。我们的实验涵盖从基本到复杂案例的分层数据集、从最小基线到模式引导和推理变体的一系列提示，以及包括 GPT、Claude、Perplexity、Grok 和 Gemini 在内的多个模型系列。结果表明，在仔细提示就足够的情况下，在明确的情况下，在需要显式计算的情况下。这项工作为在需要准确性和保证的环境中部署法学硕士提供了一个紧凑的、可重复的框架和实用指南。</li>
</ul>

<h3>Title: A System for Name and Address Parsing with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Adeeba Tarannum, Muzakkiruddin Ahmed Mohammed, Mert Can Cakmak, Shames Al Mandalawi, John Talburt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18014">https://arxiv.org/abs/2601.18014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18014">https://arxiv.org/pdf/2601.18014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18014]] A System for Name and Address Parsing with Large Language Models(https://arxiv.org/abs/2601.18014)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.</li>
<li><strong>摘要：</strong>将非结构化人员和地址文本可靠地转换为结构化数据仍然是大规模信息系统中的关键挑战。传统的基于规则和概率的方法在干净的输入上表现良好，但在嘈杂或多语言条件下会失败，而神经和大型语言模型（LLM）通常缺乏确定性控制和可重复性。本文介绍了一个提示驱动、以验证为中心的框架，该框架无需微调即可将自由文本记录转换为一致的 17 字段模式。该方法在固定实验设置下集成了输入归一化、结构化提示、约束解码和严格的基于规则的验证，以确保可重复性。对异构现实世界地址数据的评估显示出较高的字段级精度、较强的模式遵循性和稳定的置信度校准。结果表明，将确定性验证与生成提示相结合，为结构化信息提取提供了稳健、可解释且可扩展的解决方案，为训练密集型或特定领域模型提供了实用的替代方案。</li>
</ul>

<h3>Title: CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data</h3>
<ul>
<li><strong>Authors: </strong>Pedro Ortiz Suarez, Laurie Burchell, Catherine Arnett, Rafael Mosquera-Gómez, Sara Hincapie-Monsalve, Thom Vaughan, Damian Stewart, Malte Ostendorff, Idris Abdulmumin, Vukosi Marivate, Shamsuddeen Hassan Muhammad, Atnafu Lambebo Tonja, Hend Al-Khalifa, Nadia Ghezaiel Hammouda, Verrah Otiende, Tack Hwa Wong, Jakhongir Saydaliev, Melika Nobakhtian, Muhammad Ravi Shulthan Habibi, Chalamalasetti Kranti, Carol Muchemi, Khang Nguyen, Faisal Muhammad Adam, Luis Frentzen Salim, Reem Alqifari, Cynthia Amol, Joseph Marvin Imperial, Ilker Kesen, Ahmad Mustafid, Pavel Stepachev, Leshem Choshen, David Anugraha, Hamada Nayel, Seid Muhie Yimam, Vallerie Alexandra Putra, My Chiffon Nguyen, Azmine Toushik Wasi, Gouthami Vadithya, Rob van der Goot, Lanwenn ar C'horr, Karan Dua, Andrew Yates, Mithil Bangera, Yeshil Bangera, Hitesh Laxmichand Patel, Shu Okabe, Fenal Ashokbhai Ilasariya, Dmitry Gaynullin, Genta Indra Winata, Yiyuan Li, Juan Pablo Martínez, Amit Agarwal, Ikhlasul Akmal Hanif, Raia Abu Ahmad, Esther Adenuga, Filbert Aurelian Tjiaranata, Weerayut Buaphet, Michael Anugraha, Sowmya Vajjala, Benjamin Rice, Azril Hafizi Amirudin, Jesujoba O. Alabi, Srikant Panda, Yassine Toughrai, Bruhan Kyomuhendo, Daniel Ruffinelli, Akshata A, Manuel Goulão, Ej Zhou, Ingrid Gabriela Franco Ramirez, Cristina Aggazzotti, Konstantin Dobler, Jun Kevin, Quentin Pagès, Nicholas Andrews, Nuhu Ibrahim, Mattes Ruckdeschel, Amr Keleg, Mike Zhang, Casper Muziri, Saron Samuel, Sotaro Takeshita, Kun Kerdthaisong, Luca Foppiano, Rasul Dent, Tommaso Green, Ahmad Mustapha Wali, Kamohelo Makaaka, Vicky Feliren, Inshirah Idris, Hande Celikkanat, Abdulhamid Abubakar, Jean Maillard, Benoît Sagot, Thibault Clérice, Kenton Murray, Sarah Luger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18026">https://arxiv.org/abs/2601.18026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18026">https://arxiv.org/pdf/2601.18026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18026]] CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data(https://arxiv.org/abs/2601.18026)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.</li>
<li><strong>摘要：</strong>语言识别（LID）是管理多语言语料库的基本步骤。然而，LID 模型对于许多语言来说仍然表现不佳，尤其是在经常用于训练多语言语言模型的嘈杂且异构的网络数据上。在本文中，我们介绍了 CommonLID，这是一个社区驱动的、人工注释的 Web 领域 LID 基准，涵盖 109 种语言。所包含的许多语言以前都没有得到充分服务，这使得 CommonLID 成为开发更具代表性的高质量文本语料库的关键资源。我们通过使用 CommonLID 以及其他五个常见评估集来测试八个流行的 LID 模型，从而展示了 CommonLID 的价值。我们分析我们的结果以定位我们的贡献并提供最新技术的概述。我们特别强调，现有的评估高估了网络领域许多语言的 LID 准确性。我们在开放、宽松的许可证下提供 CommonLID 和用于创建它的代码。</li>
</ul>

<h3>Title: Addressing LLM Diversity by Infusing Random Concepts</h3>
<ul>
<li><strong>Authors: </strong>Pulin Agrawal, Prasoon Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18053">https://arxiv.org/abs/2601.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18053">https://arxiv.org/pdf/2601.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18053]] Addressing LLM Diversity by Infusing Random Concepts(https://arxiv.org/abs/2601.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.</li>
<li><strong>摘要：</strong>众所周知，大型语言模型 (LLM) 产生的输出多样性有限。在这项工作中，我们研究在提示中注入随机概念是否可以提高生成输出的多样性。为了对这种方法进行基准测试，我们设计了一个系统评估协议，其中涉及以“列出 10 位好莱坞演员”的形式向法学硕士提出问题，并分析法学硕士结果的多样性衡量标准。我们对多个法学硕士的实验表明，在前面添加与提示无关的随机单词/句子会导致法学硕士输出的多样性更大。我们相信，这一有希望的结果和评估协议为未来的工作开辟了有趣的途径，例如如何将随机性注入法学硕士可以应用于其他领域。此外，评估协议还可以激发对 LLM 多样性基准测试的更系统的研究。</li>
</ul>

<h3>Title: Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aryan Roy, Zekun Wang, Christopher J. MacLellan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18065">https://arxiv.org/abs/2601.18065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18065">https://arxiv.org/pdf/2601.18065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18065]] Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models(https://arxiv.org/abs/2601.18065)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.</li>
<li><strong>摘要：</strong>当使用纯文本提示进行评估时，视觉语言模型 (VLM) 是否比纯文本大语言模型 (LLM) 对语言具体性更具有人类的敏感性？我们通过在多个模型尺度上对匹配的 Llama 文本主干和其 Llama Vision 对应物之间的受控比较来研究这个问题，将多模态预训练视为感知基础的消融，而不是在推理时访问图像。我们在三个互补的层面上衡量具体性效果：(i) 输出行为，通过将问题级具体性与 QA 准确性联系起来； (ii) 嵌入几何图形，通过测试表示是否沿着具体轴组织； (iii) 注意力动态，通过注意力熵测量来量化情境依赖。此外，我们从模型中得出令牌级别的具体性评级，并评估与人类规范分布的一致性，测试多模式训练是否产生更符合人类一致的判断。在基准和尺度上，VLM 在更具体的输入上表现出更大的收益，表现出更清晰的具体结构表示，产生更符合人类规范的评级，并显示与增加的基础一致的系统不同的注意力模式。</li>
</ul>

<h3>Title: Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents</h3>
<ul>
<li><strong>Authors: </strong>Mahesh Ramesh, Kaousheik Jayakumar, Aswinkumar Ramkumar, Pavan Thodima, Aniket Rege</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18077">https://arxiv.org/abs/2601.18077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18077">https://arxiv.org/pdf/2601.18077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18077]] Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents(https://arxiv.org/abs/2601.18077)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.</li>
<li><strong>摘要：</strong>对于人类和多智能体系统来说，不完整信息下的合作推理仍然具有挑战性。纸牌游戏 Hanabi 体现了这一挑战，需要心理理论推理和战略沟通。我们在 2-5 人游戏中对 17 个最先进的 LLM 代理进行了基准测试，并研究了跨模型规模（4B 到 600B+）的上下文工程的影响，以了解持续的协调失败和脚手架的鲁棒性：从只有明确卡片详细信息的最小提示（Watson 设置），到具有程序化、贝叶斯驱动的推论（Sherlock 设置）的脚手架，到通过工作记忆进行多轮状态跟踪（Mycroft 设置）。我们证明（1）智能体可以维护用于状态跟踪的内部工作记忆，（2）不同 LLM 之间的交叉游戏性能可以根据模型强度平滑插值。在《神探夏洛克》的设定中，最强的推理模型在玩家数量上平均超过 15 分，但仍然落后于经验丰富的人类和专业 Hanabi 智能体，两者的得分始终高于 20。我们发布了第一个带有注释轨迹和移动实用程序的公共 Hanabi 数据集：(1) HanabiLogs，包含 1,520 个用于指令调整的完整游戏日志，以及 (2) HanabiRewards，包含 560 个游戏，为所有候选移动提供密集的移动级别值注释。我们的数据集上的 4B 开放权重模型 (Qwen3-Instruct) 的监督和 RL 微调分别将合作 Hanabi 游戏提高了 21% 和 156%，使性能与强大的专有推理模型 (o4-mini) 相差约 3 分，并超过最佳非推理模型 (GPT-4.1) 52%。 HanabiRewards RL 微调模型进一步推广到 Hanabi 之外，将协作组猜测基准的性能提高了 11%，EventQA 上的时间推理提高了 6.4%，IFBench-800K 上的指令跟踪提高了 1.7 Pass@10，并匹配 AIME 2025 数学推理 Pass@10。</li>
</ul>

<h3>Title: CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations</h3>
<ul>
<li><strong>Authors: </strong>Stephanie Fong, Zimu Wang, Guilherme C. Oliveira, Xiangyu Zhao, Yiwen Jiang, Jiahe Liu, Beau-Luke Colton, Scott Woods, Martha E. Shenton, Barnaby Nelson, Zongyuan Ge, Dominic Dwyer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18102">https://arxiv.org/abs/2601.18102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18102">https://arxiv.org/pdf/2601.18102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18102]] CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations(https://arxiv.org/abs/2601.18102)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.</li>
<li><strong>摘要：</strong>NLP 工具在医学上的采用需要最终用户的可解释性，但传统的可解释人工智能 (XAI) 方法与临床推理不一致，并且缺乏临床医生的输入。我们引入 CHiRPE（具有可解释性的临床高风险预测），这是一种 NLP 流程，它采用转录的半结构化临床访谈来：（i）预测精神病风险； (ii) 生成与临床医生共同开发的新颖的 SHAP 解释格式。 CHiRPE 管道经过 AMP-SCZ 研究 24 个国际诊所的 944 份半结构化访谈笔录的训练，集成了症状域映射、LLM 总结和 BERT 分类。 CHiRPE 在三个 BERT 变体中实现了 90% 以上的准确率，并且优于基线模型。解释格式由 28 名临床专家进行了评估，他们表示非常喜欢我们新颖的概念引导解释，尤其是混合图文摘要格式。 CHiRPE 证明临床指导的模型开发可以产生准确且可解释的结果。我们的下一步重点是在 24 个国际站点进行实际测试。</li>
</ul>

<h3>Title: GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health</h3>
<ul>
<li><strong>Authors: </strong>Jiatan Huang, Zheyuan Zhang, Tianyi Ma, Mingchen Li, Yaning Zheng, Yanfang Ye, Chuxu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18106">https://arxiv.org/abs/2601.18106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18106">https://arxiv.org/pdf/2601.18106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18106]] GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health(https://arxiv.org/abs/2601.18106)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.</li>
<li><strong>摘要：</strong>营养干预对于管理慢性健康状况很重要，但当前的计算方法为个性化饮食指导提供的支持有限。我们发现了三个关键差距：（1）饮食模式研究经常忽视现实世界的限制，例如社会经济地位、合并症和有限的食物获取； (2) 推荐系统很少解释为什么特定食物对特定患者有帮助； (3) 没有统一的基准来评估营养干预所需的相关任务的方法。我们推出 GLEN-Bench，这是第一个基于图形语言的营养健康评估综合基准。我们结合 NHANES 健康记录、FNDDS 食物成分数据和 USDA 食物获取指标来构建一个知识图谱，将人口统计、健康状况、饮食行为、贫困相关限制和营养需求联系起来。我们使用阿片类药物使用障碍来测试基准，其中模型必须检测不同疾病阶段的细微营养差异。 GLEN-Bench 包括三个相互关联的任务：风险检测根据饮食和社会经济模式识别高危个体；建议建议在资源限制内满足临床需求的个性化食品；问答提供基于图形的自然语言解释以促进理解。我们评估这些图语言方法，包括图神经网络、大型语言模型和混合架构，以建立可靠的基线并确定实际的设计选择。我们的分析确定了与健康风险相关的明确饮食模式，提供了可以指导实际干预措施的见解。</li>
</ul>

<h3>Title: FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lin Sun, Linglin Zhang, Jingang Huang, Change Jia, Zhengwei Cheng, Xiangzheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18116">https://arxiv.org/abs/2601.18116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18116">https://arxiv.org/pdf/2601.18116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18116]] FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning(https://arxiv.org/abs/2601.18116)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis. We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs. Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.</li>
<li><strong>摘要：</strong>长上下文大语言模型（LLM）的快速扩展重新引发了关于检索增强生成（RAG）是否仍然必要的争论。然而，经验证据揭示了长上下文推理的持续局限性，包括中间丢失现象、高计算成本和多文档推理的可扩展性差。相反，传统的 RAG 系统虽然高效，但受到平面块级检索的限制，这种检索引入了语义噪声并且无法支持结构化跨文档合成。我们提出\textbf{FABLE}，一个基于\textbf{F}orest的\textbf{A}自适应\textbf{B}i-path\textbf{L}LM-\textbf{E}增强的检索框架，它将LLM集成到知识组织和检索中。 FABLE 使用多粒度语义结构构建 LLM 增强的分层森林索引，然后采用双路径策略，将 LLM 引导的分层遍历与结构感知传播相结合，以实现细粒度证据获取，并通过显式预算控制实现自适应效率权衡。大量实验表明，FABLE 始终优于 SOTA RAG 方法，并达到与全上下文 LLM 推理相当的准确性，标记减少高达 94%，这表明长上下文 LLM 放大而不是完全取代结构化检索的需求。</li>
</ul>

<h3>Title: Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kunat Pipatanakul, Pittawat Taveekitworachai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18129">https://arxiv.org/abs/2601.18129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18129">https://arxiv.org/pdf/2601.18129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18129]] Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models(https://arxiv.org/abs/2601.18129)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）进展迅速；然而，大多数最先进的模型主要使用英语和中文等高资源语言进行训练和评估，并且通常由少数能够访问大规模计算和数据的组织开发。这种把关为主权环境造成了实际障碍，在这种环境中，区域或国家规模的机构或领域所有者必须保留对模型权重、训练数据和部署的控制和理解，同时在有限的资源和严格的透明度约束下运行。为此，我们确定了两个核心要求：（1）可采用性，将基础模型转变为通用助手的能力；（2）主权能力，执行高风险、特定区域任务的能力（例如，用当地语言和文化知识进行法律推理）。我们研究是否可以在不扩展大规模指令语料库或依赖复杂的偏好调整管道和大规模强化微调（RFT）的情况下实现这些要求。我们推出了 Typhoon S，这是一个最小且开放的训练后配方，结合了监督微调、策略蒸馏和小规模 RFT。使用 Thai 作为代表性案例研究，我们证明了我们的方法将主权适应和通用基础模型转变为具有强大通用性能的指令调整模型。我们进一步表明，使用 InK-GRPO 的小规模 RFT（GRPO 的扩展，通过下一个单词预测损失来增强 GRPO 损失）可以改善泰国法律推理和泰国特定知识，同时保留一般能力。我们的结果表明，精心设计的培训后策略可以减少所需的教学数据和计算规模，为在学术规模资源下获得高质量主权法学硕士提供了一条实用途径。</li>
</ul>

<h3>Title: MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Juexiang Ye, Xue Li, Xinyu Yang, Chengkai Huang, Lanshun Nie, Lina Yao, Dechen Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18204">https://arxiv.org/abs/2601.18204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18204">https://arxiv.org/pdf/2601.18204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18204]] MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning(https://arxiv.org/abs/2601.18204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.</li>
<li><strong>摘要：</strong>在长范围交互中运行的基于大型语言模型的代理需要支持时间一致性、多跳推理和跨会话的基于证据的重用的存储系统。现有的方法很大程度上依赖于非结构化检索或粗略抽象，这通常会导致时间冲突、脆弱的推理和有限的可追溯性。我们提出了 MemWeaver，一个统一的记忆框架，它将长期代理经验整合为三个相互关联的组件：用于结构化关系推理的时间接地图记忆、从重复观察中抽象出重复交互模式的经验记忆，以及保留原始文本证据的段落记忆。 MemWeaver 采用双通道检索策略，联合检索结构化知识和支持证据，以构建紧凑但信息密集的推理上下文。 LoCoMo 基准测试表明，与长上下文基线相比，MemWeaver 显着提高了多跳和时间推理准确性，同时将输入上下文长度减少了 95% 以上。</li>
</ul>

<h3>Title: BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Peng Sun, Xiangyu Zhang, Duan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18253">https://arxiv.org/abs/2601.18253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18253">https://arxiv.org/pdf/2601.18253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18253]] BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation(https://arxiv.org/abs/2601.18253)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.</li>
<li><strong>摘要：</strong>准确评估用户满意度对于对话式AI的迭代开发至关重要。然而，对于开放式助手来说，传统的 A/B 测试缺乏可靠的指标：显式反馈稀疏，隐式指标模糊。为了弥补这一差距，我们引入了 BoRP（自举回归探测），这是一个用于高保真满意度评估的可扩展框架。与生成方法不同，BoRP 利用了 LLM 潜在空间的几何特性。它采用基于极化指数的引导机制来自动生成评分标准，并利用偏最小二乘法 (PLS) 将隐藏状态映射到连续分数。工业数据集上的实验表明，BoRP（Qwen3-8B/14B）显着优于生成基线（甚至 Qwen3-Max），与人类判断一致。此外，BoRP 将推理成本降低了几个数量级，从而能够通过 CUPED 进行全面监控和高度敏感的 A/B 测试。</li>
</ul>

<h3>Title: Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Jia, Pei Liu, Haoqin Sun, Jiaming Zhou, Xuxin Cheng, Cao Liu, Ke Zeng, Xunliang Cai, Yong Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18281">https://arxiv.org/abs/2601.18281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18281">https://arxiv.org/pdf/2601.18281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18281]] Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue(https://arxiv.org/abs/2601.18281)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single "correct" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.</li>
<li><strong>摘要：</strong>端到端口语模型（SLM）在副语言感知方面具有巨大潜力，许多研究旨在增强其能力，特别是在移情对话方面。然而，当前的方法在很大程度上依赖于严格的监督信号，例如监督微调中的真实响应或强化学习中的偏好分数。这种依赖从根本上限制了复杂同理心的建模，因为没有单一的“正确”反应，而且简单的数字分数无法完全捕捉情感表达的细微差别或同理心行为的适当性。为了解决这些局限性，我们依次引入了 EmpathyEval，一种基于描述性自然语言的评估模型，用于评估口语对话中的同理心质量。在 EmpathyEval 的基础上，我们提出了 ReEmpathy，这是一种端到端的 SLM，通过一种新颖的移情自我反思交替推理机制来增强移情对话，该机制将口头响应生成与自由形式、移情相关的反思推理交织在一起。大量实验表明，ReEmpathy 通过实现反思性推理，显着改善了对同理心敏感的口语对话，为实现更具情商和同理心意识的人机交互提供了一种有前景的方法。</li>
</ul>

<h3>Title: U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents</h3>
<ul>
<li><strong>Authors: </strong>Jin Su, Runnan Fang, Yeqiu Li, Xiaobin Wang, Shihao Cai, Pengjun Xie, Ningyu Zhang, Fajie Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18285">https://arxiv.org/abs/2601.18285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18285">https://arxiv.org/pdf/2601.18285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18285]] U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents(https://arxiv.org/abs/2601.18285)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $\tau$-bench, $\tau^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.</li>
<li><strong>摘要：</strong>基于大型语言模型（LLM）的代理已成功部署在许多工具增强的环境中，但它们的可扩展性从根本上受到上下文长度的限制。现有的上下文折叠方法通过总结过去的交互来缓解这个问题，但它们通常是针对单查询或单意图场景而设计的。在更现实的以用户为中心的对话中，我们确定了两种主要的失败模式：（i）它们不可逆转地丢弃对后续决策至关重要的细粒度约束和中间事实，以及（ii）它们的摘要无法跟踪不断变化的用户意图，导致遗漏和错误的操作。为了解决这些限制，我们提出了 U-Fold，这是一种针对以用户为中心的任务量身定制的动态上下文折叠框架。 U-Fold 保留完整的用户代理对话和工具调用历史记录，但每次都使用两个核心组件来生成意图感知、不断发展的对话摘要和紧凑的、与任务相关的工具日志。在 $\tau$-bench、$\tau^2$-bench、VitaBench 和更难的上下文膨胀设置上进行的大量实验表明，U-Fold 始终优于 ReAct（在长上下文设置中实现 71.4% 的获胜率）和之前的折叠基线（改进高达 27.0%），特别是在长、嘈杂、多轮任务上。我们的研究表明，U-Fold 是将上下文管理技术从单一查询基准转移到现实的以用户为中心的应用程序的一个有希望的一步。</li>
</ul>

<h3>Title: Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyan Gong, Zhiqiang Liu, Songze Li, Xiaoke Guo, Yuanxiang Liu, Xinle Deng, Zhizhen Liu, Lei Liang, Huajun Chen, Wen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18296">https://arxiv.org/abs/2601.18296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18296">https://arxiv.org/pdf/2601.18296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18296]] Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning(https://arxiv.org/abs/2601.18296)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at this https URL.</li>
<li><strong>摘要：</strong>时态知识图问答（TKGQA）本质上是具有挑战性的，因为它需要对具有多跳依赖性和复杂时态约束的动态事实进行复杂的推理。现有方法依赖于固定的工作流程和昂贵的闭源 API，限制了灵活性和可扩展性。我们提出了 Temp-R1，这是第一个通过强化学习训练的 TKGQA 自主端到端代理。为了解决单动作推理中的认知过载问题，我们通过专门的内部动作和外部动作来扩展动作空间。为了防止简单问题上的捷径学习，我们引入了逆向课程学习，首先对困难的问题进行训练，在转移到更简单的案例之前强制发展复杂的推理。我们的 8B 参数 Temp-R1 在 MultiTQ 和 TimelineKGQA 上实现了最先进的性能，在复杂问题上比强大的基线提高了 19.8%。我们的工作为自主时间推理代理建立了一个新的范例。我们的代码很快就会在此 https URL 上公开发布。</li>
</ul>

<h3>Title: Suppressing Final Layer Hidden State Jumps in Transformer Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Keigo Shibata, Kazuki Yano, Ryosuke Takahashi, Jaesung Lee, Wataru Ikeda, Jun Suzuki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18302">https://arxiv.org/abs/2601.18302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18302">https://arxiv.org/pdf/2601.18302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18302]] Suppressing Final Layer Hidden State Jumps in Transformer Pretraining(https://arxiv.org/abs/2601.18302)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.</li>
<li><strong>摘要：</strong>本文讨论 Transformer 语言模型的内部行为。据报道，许多最近的预训练模型在中间 Transformer 层中的输入和输出隐藏状态向量之间的角距离仅表现出轻微的变化，尽管最终 Transformer 层中或周围发生的角距离不成比例地大“跳跃”。为了描述这一点，我们首先引入最后一层周围跳跃强度的定量指标，然后证明它在许多开放权重模型中的普遍性，以及它在整个预训练过程中的放大效果。假设这种跳跃表明一种不良特性，我们提出了跳跃抑制正则化器（JREG），它在预训练期间惩罚这种跳跃，从而鼓励中间层更平衡地使用能力。对基于 Llama 的模型的三种模型大小（使用所提出的 JREG 方法进行训练）进行的实证评估表明，与基线相比，在不改变模型架构的情况下，任务性能有所提高。</li>
</ul>

<h3>Title: Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM</h3>
<ul>
<li><strong>Authors: </strong>Everlyn Asiko Chimoto, Mostafa Elhoushi, Bruce A. Bassett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18306">https://arxiv.org/abs/2601.18306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18306">https://arxiv.org/pdf/2601.18306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18306]] Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM(https://arxiv.org/abs/2601.18306)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.</li>
<li><strong>摘要：</strong>量化是减少大型语言模型 (LLM) 的存储占用和计算成本的有效技术，但它通常会导致性能下降。现有的训练后量化方法通常使用小型的、仅限英语的校准集；然而，它们对多语言模型的影响仍未得到充分探索。我们对来自 10 种语言的数据在两个量化器（GPTQ、AWQ）上系统地评估了八种校准设置（五种单语言和三种多语言混合）。我们的研究结果揭示了一个一致的趋势：与纯英语基线相比，非英语和多语言校准集显着改善了困惑度。具体来说，我们观察到 Llama3.1 8B 和 Qwen2.5 7B 上的两个量化器的平均困惑度都有显着的提高，多语言混合实现了最大的总体困惑度降低，高达 3.52 个百分点。此外，我们的分析表明，根据评估语言定制校准集可以为各个语言带来最大的改进，强调了语言对齐的重要性。我们还确定了某些语言量化器组合会降低性能的特定故障案例，我们将其追溯到跨语言激活范围分布的差异。这些结果凸显出静态一刀切的校准并不是最优的，并且在语言和多样性方面定制校准数据在稳健量化多语言法学硕士方面发挥着至关重要的作用。</li>
</ul>

<h3>Title: MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization</h3>
<ul>
<li><strong>Authors: </strong>Jinwei Lu, Yuanfeng Song, Chen Zhang, Raymond Chi-Wing Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18320">https://arxiv.org/abs/2601.18320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18320">https://arxiv.org/pdf/2601.18320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18320]] MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization(https://arxiv.org/abs/2601.18320)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.</li>
<li><strong>摘要：</strong>现实世界的可视化任务涉及复杂的多模式要求，超出了简单的文本到图表生成的范围，需要参考图像、代码示例和迭代细化。当前的系统表现出基本的局限性：单模态输入、一次性生成和严格的工作流程。虽然基于法学硕士的方法显示出满足这些复杂要求的潜力，但它们带来了可靠性挑战，包括灾难性故障和无限循环敏感性。为了解决这一差距，我们提出了 MultiVis-Agent，这是一种逻辑规则增强的多代理框架，用于可靠的多模式和多场景可视化生成。我们的方法引入了四层逻辑规则框架，为系统可靠性提供数学保证，同时保持灵活性。与传统的基于规则的系统不同，我们的逻辑规则是指导 LLM 推理而不是取代它的数学约束。我们将 MultiVis 任务形式化，涵盖从基本生成到迭代细化的四个场景，并开发了 MultiVis-Bench，这是一个包含 1,000 多个案例的多模态可视化评估基准。大量实验表明，我们的方法在具有挑战性的任务上实现了 75.63% 的可视化得分，显着优于基线 (57.54-62.79%)，任务完成率为 99.58%，代码执行成功率为 94.56%（对比没有逻辑规则的 74.48% 和 65.10%），成功解决了自动化可视化生成中的复杂性和可靠性挑战。</li>
</ul>

<h3>Title: Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Clément Christophe, Wadood Mohammed Abdul, Prateek Munjal, Tathagata Raha, Ronnie Rajan, Praveenkumar Kanithi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18334">https://arxiv.org/abs/2601.18334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18334">https://arxiv.org/pdf/2601.18334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18334]] Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare(https://arxiv.org/abs/2601.18334)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or "confusability". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized "Thinking" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.</li>
<li><strong>摘要：</strong>随着法学硕士越来越多地融入临床工作流程，他们的阿谀奉承倾向，优先考虑用户同意而不是事实准确性，这给患者安全带来了重大风险。虽然现有的评估通常依赖于主观数据集，但我们引入了一个基于医学 MCQA 的强大框架，具有可验证的基本事实。我们提出了调整后的谄媚分数，这是一种新颖的指标，通过考虑随机模型的不稳定性或“混淆性”来隔离对齐偏差。通过对 Qwen-3 和 Llama-3 系列进行广泛的扩展分析，我们确定了清晰的弹性扩展轨迹。此外，我们揭示了推理优化“思考”模型中的一个反直觉漏洞：虽然它们表现出很高的普通准确性，但它们的内部推理痕迹经常在权威压力下合理化不正确的用户建议。我们跨前沿模型的结果表明，基准性能并不能代表临床可靠性，并且简化的推理结构可能会针对专家驱动的阿谀奉承提供卓越的鲁棒性。</li>
</ul>

<h3>Title: When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs</h3>
<ul>
<li><strong>Authors: </strong>Junyi Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18350">https://arxiv.org/abs/2601.18350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18350">https://arxiv.org/pdf/2601.18350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18350]] When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs(https://arxiv.org/abs/2601.18350)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 显示出强大的通用能力，但经常在医学术语精度和安全关键指令遵循方面遇到困难。我们通过两阶段 LoRA 管道，使用 14B 参数基础模型，提出了一个针对安全关键领域中适配器干扰的案例研究：(1) 领域自适应预训练 (PT)，通过持续预训练 (DAPT) 注入广泛的医学知识；(2) 监督微调 (SFT)，通过指令式数据使模型与医学问答行为保持一致。为了平衡指令跟踪能力和领域知识保留，我们提出加权适配器合并，在导出合并的基础模型检查点之前线性组合 SFT 和 PT 适配器。在保留的医学验证集 (F5/F6) 上，合并模型在实际解码配置下实现 BLEU-4 = 16.38、ROUGE-1 = 20.42、ROUGE-2 = 4.60 和 ROUGE-L = 11.54。我们通过损失曲线和受控解码比较进一步分析解码灵敏度和训练稳定性。</li>
</ul>

<h3>Title: Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Manjie Xu, Isabella Yin, Xinyi Tu, Chi Zhang, Yixin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18352">https://arxiv.org/abs/2601.18352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18352">https://arxiv.org/pdf/2601.18352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18352]] Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning(https://arxiv.org/abs/2601.18352)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.</li>
<li><strong>摘要：</strong>法学硕士与语义惯性作斗争：当动态的上下文规则与预先训练的先验知识（例如，“熔岩是危险的”）相矛盾时，他们无法抑制它们。我们使用 Baba Is You 来探究这一现象，其中物理定律是可变的文本规则，可以精确评估模型在规则变化时覆盖学习先验的能力。我们定量地观察到，较大的模型可以表现出逆缩放：当自然语言推理需要抑制预先训练的关联（例如，接受“熔岩是安全的”）时，它们的表现比较小的模型更差。我们的分析将其归因于自然语言编码，它纠缠着描述性语义和逻辑规则，尽管存在明显矛盾的规则，但仍导致人们对熟悉的物理学产生持续的幻觉。在这里，我们表明，将动态表示为可执行代码而不是描述性文本，可以扭转这种趋势并实现有效的先验抑制。我们引入了 Code-Grounded Vistas (LCV)，它可以对反事实对上的模型进行微调，并识别具有矛盾规则的状态，从而迫使人们关注逻辑约束而不是视觉语义。这种训练时间方法在效率和准确性方面都优于昂贵的推理时间搜索方法。我们的结果表明，表示从根本上决定了缩放是否会改善或损害上下文推理。这挑战了“更大的模型普遍更好”的假设，对需要动态覆盖学习先验的领域产生影响。</li>
</ul>

<h3>Title: CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Silva, José Evans, José Isidro, Miguel Marques, Afonso Fonseca, Ricardo Morais, João Canavilhas, Arian Pasquali, Purificação Silvano, Alípio Jorge, Nuno Guimarães, Sérgio Nunes, Ricardo Campos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18374">https://arxiv.org/abs/2601.18374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18374">https://arxiv.org/pdf/2601.18374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18374]] CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes(https://arxiv.org/abs/2601.18374)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.</li>
<li><strong>摘要：</strong>市议会会议记录通常是冗长而正式的文件，具有官僚的写作风格。尽管它们是公开的，但它们的结构往往使公民或记者难以有效地查找信息。在此演示中，我们展示了 CitiLink，这是一个旨在将非结构化市政会议纪要转换为结构化可搜索数据的平台，展示了 NLP 和 IR 如何增强地方政府的可及性和透明度。该系统采用法学硕士来提取元数据、讨论的主题和投票结果，然后在数据库中建立索引，以支持通过用户友好的界面进行 BM25 排名和分面过滤的全文搜索。开发的系统是在葡萄牙六个城市历时 120 分钟的时间内构建完成的。为了评估其可用性，CitiLink 通过与市政人员的指导会议进行了测试，提供了有关真实用户如何与系统交互的见解。此外，我们还评估了Gemini在从会议纪要中提取相关信息方面的表现，凸显了其在数据提取方面的有效性。</li>
</ul>

<h3>Title: Hierarchical Text Classification with LLM-Refined Taxonomies</h3>
<ul>
<li><strong>Authors: </strong>Jonas Golde, Nicolaas Jedema, Ravi Krishnan, Phong Le</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18375">https://arxiv.org/abs/2601.18375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18375">https://arxiv.org/pdf/2601.18375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18375]] Hierarchical Text Classification with LLM-Refined Taxonomies(https://arxiv.org/abs/2601.18375)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.</li>
<li><strong>摘要：</strong>层次文本分类 (HTC) 依赖于将标签组织成结构化层次结构的分类法。然而，许多现实世界的分类法引入了歧义，例如相似父节点下的相同叶名称，这会阻止语言模型 (LM) 学习清晰的决策边界。在本文中，我们提出了 TaxMorph，这是一个使用大型语言模型 (LLM) 通过重命名、合并、拆分和重新排序等操作来转换整个分类法的框架。与之前的工作不同，我们的方法修改了完整的层次结构，以更好地匹配 LM 编码的语义。三个 HTC 基准​​测试的实验表明，LLM 改进的分类法在各种设置中始终优于人工策划的分类法，最高可达 +2.9pp。在F1中。为了更好地理解这些改进，我们比较了 LM 在人工策划和 LLM 细化的分类法中将叶节点分配给父节点的能力，反之亦然。我们发现，人工分类法可以在嵌入空间中产生更容易分离的簇。然而，LLM 细化的分类法与模型在分类过程中的实际混淆模式更加一致。换句话说，尽管它们更难分开，但它们更好地反映了模型的归纳偏差。这些发现表明，LLM 引导的细化创建了与模型学习方式更兼容的分类法，从而提高了 HTC 的性能。</li>
</ul>

<h3>Title: Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mikel Zubillaga, Oscar Sainz, Oier Lopez de Lacalle, Eneko Agirre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18395">https://arxiv.org/abs/2601.18395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18395">https://arxiv.org/pdf/2601.18395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18395]] Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction(https://arxiv.org/abs/2601.18395)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.</li>
<li><strong>摘要：</strong>文档级信息提取（DocIE）旨在生成一个输出模板，其中包含给定文档中出现的实体和感兴趣的关系。标准做法包括提示仅解码器的法学硕士使用贪婪解码来避免输出变化。我们没有将这种可变性视为一种限制，而是证明采样可以产生比贪婪解码更好的解决方案，特别是在使用推理模型时。因此，我们提出了 ThinkTwice，一种采样和选择框架，其中法学硕士为给定文档生成多个候选模板，然后选择模块选择最合适的模板。我们引入了一种利用生成输出之间的一致性的无监督方法，以及一种使用在标记的 DocIE 数据上训练的奖励模型的监督选择方法。为了解决 DocIE 黄金推理轨迹的稀缺问题，我们提出了一种基于拒绝采样的方法来生成将输出模板与推理轨迹配对的白银训练数据。我们的实验证明了无监督和监督 ThinkTwice 的有效性，始终优于贪婪基线和最先进的技术。</li>
</ul>

<h3>Title: Pisets: A Robust Speech Recognition System for Lectures and Interviews</h3>
<ul>
<li><strong>Authors: </strong>Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin, Mikhail Klementev, Roman Derunets, Lyudmila Budneva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18415">https://arxiv.org/abs/2601.18415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18415">https://arxiv.org/pdf/2601.18415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18415]] Pisets: A Robust Speech Recognition System for Lectures and Interviews(https://arxiv.org/abs/2601.18415)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: this https URL.</li>
<li><strong>摘要：</strong>这项工作为科学家和记者提供了一个语音到文本系统“Pisets”，该系统基于三组件架构，旨在提高语音识别准确性，同时最大限度地减少与 Whisper 模型相关的错误和幻觉。该架构包括使用 Wav2Vec2 的初级识别、通过音频频谱图转换器 (AST) 的误报过滤以及通过 Whisper 的最终语音识别。课程学习方法的实施和多样化俄语语音语料库的利用显着提高了系统的有效性。此外，还引入了先进的不确定性建模技术，有助于进一步提高转录质量。与 WhisperX 和通常的 Whisper 模型相比，所提出的方法可确保在各种声学条件下稳健地转录长音频数据。 “Pisets”系统的源代码可在 GitHub 上公开获取：此 https URL。</li>
</ul>

<h3>Title: Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel B. Hier, Tayo Obafemi-Ajayi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18468">https://arxiv.org/abs/2601.18468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18468">https://arxiv.org/pdf/2601.18468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18468]] Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models(https://arxiv.org/abs/2601.18468)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.</li>
<li><strong>摘要：</strong>大型语言模型在预训练后存储的生物医学事实的强度不均匀：一些事实存在于权重中，但在确定性解码（潜在知识）下无法可靠地访问，而其他事实则几乎没有表示。我们对 Llama 3.1 8B Instruct 进行了微调，以从人类表型本体（800 对）和基因本体（400 个训练对）学习本体术语标识符映射，保留 400 个 GO 对来测试泛化。将学习视为跨 20 个时期的事件过程，我们使用随机解码来检测基线的潜在知识，并使用 Cox 比例风险模型来识别获取、泛化和退化的预测因子。 HPO 的基线确定性召回率为 2.8%，微调后升至 71.9%。潜在知识是更快事实获取的最强预测因子（HR 2.6），并且与更早、更高的峰值学习率和更快的收敛相关；标识符频率和策划注释计数的影响较小。对隐瞒 GO 事实的泛化并不常见（5.8%），但当存在潜在知识时更有可能。之前正确的 GO 映射对于保留的（未见过的）术语比训练过的（见过的）术语更容易降级，这表明训练期间强化的保护作用。这些结果表明，潜在知识既可以预测微调过程中事实学习的速度，也可以预测未见过的本体事实的有限泛化，而对退化的抵抗力取决于事实是否得到强化。</li>
</ul>

<h3>Title: Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18483">https://arxiv.org/abs/2601.18483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18483">https://arxiv.org/pdf/2601.18483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18483]] Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs(https://arxiv.org/abs/2601.18483)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 提供强大的生成能力，但许多应用程序需要对特定文本概念（例如幽默、说服力或形式）进行显式和 \textit{细粒度} 控制。提示和表示工程中的先前方法可以提供粗略或单属性控制，但对多属性设置的系统评估仍然有限。我们引入了针对单概念和双概念场景的细粒度可控性评估框架，重点关注语言上不同的概念对（例如，说服力与幽默）。令人惊讶的是，在多个法学硕士和生成任务中，我们发现在双概念设置中性能经常下降，即使所选择的概念原则上应该是可分离的。这揭示了基于朴素提示的控制的一个根本局限性：即使概念直观上是独立的，模型也难以实现组合性。我们的框架提供了这一差距的系统证据，并提供了衡量未来多概念控制方法能力的原则性方法。</li>
</ul>

<h3>Title: Demographic Probing of Large Language Models Lacks Construct Validity</h3>
<ul>
<li><strong>Authors: </strong>Manuel Tonneau, Neil K. R. Seghal, Niyati Malhotra, Victor Orozco-Olvera, Ana María Muñoz Boudet, Lakshmi Subramanian, Sharath Chandra Guntuku, Valentin Hofmann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18486">https://arxiv.org/abs/2601.18486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18486">https://arxiv.org/pdf/2601.18486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18486]] Demographic Probing of Large Language Models Lacks Construct Validity(https://arxiv.org/abs/2601.18486)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.</li>
<li><strong>摘要：</strong>人口统计探测广泛用于研究大型语言模型 (LLM) 如何使其行为适应所指示的人口统计属性。这种方法通常使用孤立的单一人口统计线索（例如，名称或方言）作为群体成员身份的信号，隐含地假设强结构有效性：这些线索是相同的潜在的、人口统计条件行为的可互换操作。我们在现实的寻求建议互动中测试了这一假设，重点关注美国背景下的种族和性别。我们发现，旨在代表同一人口群体的线索只会引起模型行为的部分重叠变化，而给定线索内的群体之间的分化很弱且不均匀。因此，估计的差异是不稳定的，不同线索的大小和方向都不同。我们进一步表明，这些不一致部分是由于线索编码人口统计属性的强烈程度的变化以及独立塑造模型行为的语言混杂因素造成的。总之，我们的研究结果表明，人口统计调查缺乏结构有效性：它不能对法学硕士如何根据人口统计信息产生单一、稳定的特征，这可能反映了错误指定或支离破碎的结构。最后，我们建议使用多种、生态上有效的线索和对混杂因素的明确控制，以支持关于法学硕士人口影响的更有说服力的主张。</li>
</ul>

<h3>Title: Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research</h3>
<ul>
<li><strong>Authors: </strong>Antonio Garzon-Vico, Krithika Sharon Komalapati, Arsalan Shahid, Jan Rosier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18512">https://arxiv.org/abs/2601.18512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18512">https://arxiv.org/pdf/2601.18512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18512]] Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research(https://arxiv.org/abs/2601.18512)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.</li>
<li><strong>摘要：</strong>本研究介绍了一种方法框架，该框架使用大型语言模型来创建真实高层管理人员的虚拟角色。借鉴真实的首席执行官沟通和道德基础理论，我们构建了基于法学硕士的参与者，模拟个别领导者的决策。在三个阶段中，我们通过将这些虚拟首席执行官与人类参与者进行基准比较来评估构建的有效性、可靠性和行为保真度。我们的结果表明，理论上的支架人物角色近似于在人类样本中观察到的道德判断，这表明基于法学硕士的人物角色可以作为直接接触高管的环境中组织研究的可靠和补充工具。最后，我们概述了在组织环境中使用基于法学硕士的角色对未来研究的影响。</li>
</ul>

<h3>Title: GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback</h3>
<ul>
<li><strong>Authors: </strong>James Sungarda, Hongkai Liu, Zilong Zhou, Tien-Hsuan Wu, Johnson Chun-Sing Cheung, Ben Kao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18517">https://arxiv.org/abs/2601.18517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18517">https://arxiv.org/pdf/2601.18517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18517]] GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback(https://arxiv.org/abs/2601.18517)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, agent</a></li>
<li><strong>Abstract: </strong>Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.</li>
<li><strong>摘要：</strong>现场教育是社会工作的标志性教学法，但在培训期间提供及时和客观的反馈受到讲师和咨询客户的限制。在本文中，我们介绍了 SWITCH，即社会工作互动培训聊天机器人。 SWITCH 将真实的客户模拟、实时咨询技能分类和动机访谈 (MI) 进展系统集成到培训工作流程中。为了对客户进行建模，SWITCH 使用基于认知的配置文件，其中包括静态字段（例如背景、信仰）和动态字段（例如情绪、自动思维、开放性），从而允许代理的行为在整个会话中真实地演变。技能分类模块从用户话语中识别咨询技能，并将结果反馈给调节 MI 阶段转换的 MI 控制器。为了提高分类准确性，我们通过检索带注释的转录本和微调的 BERT 多标签分类器来研究上下文学习。在实验中，我们证明了基于 BERT 的方法和上下文学习都大幅优于基线。因此，SWITCH 提供了可扩展、低成本且一致的培训工作流程，可补充现场教育，并使主管能够专注于更高级别的指导。</li>
</ul>

<h3>Title: Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models</h3>
<ul>
<li><strong>Authors: </strong>Francesco Maria Molfese, Momchil Hardalov, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18527">https://arxiv.org/abs/2601.18527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18527">https://arxiv.org/pdf/2601.18527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18527]] Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models(https://arxiv.org/abs/2601.18527)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.</li>
<li><strong>摘要：</strong>凭借数百万个标记的上下文窗口，长上下文语言模型 (LCLM) 可以对整个文档集合进行编码，为传统检索增强生成 (RAG) 提供了强大的替代方案。然而，目前尚不清楚微调策略是否可以提高长上下文性能并转化为 KV 缓存压缩技术下更高的鲁棒性。在这项工作中，我们研究了哪些训练策略最有效地增强 LCLM 识别和使用相关信息的能力，以及增强它们在 KV 缓存压缩下的鲁棒性。我们的实验显示了领域内的重大改进，与基本模型相比，实现了高达 +20 点的增益。然而，域外泛化仍然依赖于具有较大方差的任务 - LCLM 在财务问题上表现出色（+9 分），而 RAG 在多项选择问题上表现出比基线模型更强的表现（+6 分）。最后，我们表明，我们的微调方法在 KV 缓存压缩下的鲁棒性方面带来了适度的改进，并且增益因任务而异。</li>
</ul>

<h3>Title: From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Jiang, Yufei Wang, Qiyuan Zhang, Xingshan Zeng, Liangyou Li, Jierun Chen, Chaofan Tao, Haoli Bai, Lifeng Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18533">https://arxiv.org/abs/2601.18533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18533">https://arxiv.org/pdf/2601.18533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18533]] From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation(https://arxiv.org/abs/2601.18533)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at this https URL.</li>
<li><strong>摘要：</strong>具有可验证奖励的强化学习（RLVR）通过检查最终可验证答案（即可验证点信号）来成功完成推理任务（例如数学和代码）。然而，将这种范式扩展到开放式生成具有挑战性，因为没有明确的基本事实。依赖单点监督通常会导致效率低下和奖励黑客行为。为了解决这些问题，我们提出了具有可验证的基于参考的奖励（RLVRR）的强化学习。 RLVRR 不检查最终答案，而是从高质量参考文献（即奖励链）中提取有序的语言信号。具体来说，RLVRR 将奖励分解为两个维度：内容，保留确定性的核心概念（例如关键字）；风格，通过基于 LLM 的验证来评估对风格属性的遵守情况。通过这种方式，RLVRR 将 RL 的探索性强度与监督微调（SFT）的效率和可靠性结合起来。使用 Qwen 和 Llama 模型对 10 多个基准进行的广泛实验证实了我们方法的优势。 RLVRR (1) 大大优于使用十倍多的数据和高级奖励模型训练的 SFT，(2) 统一了结构化推理和开放式生成的训练，(3) 在保持输出多样性的同时更有效地泛化。这些结果使 RLVRR 成为实现通用 LLM 对齐的可验证强化学习的原则性且有效的途径。我们在此 https URL 发布我们的代码和数据。</li>
</ul>

<h3>Title: Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection</h3>
<ul>
<li><strong>Authors: </strong>Devansh Srivastav, David Pape, Lea Schönherr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18552">https://arxiv.org/abs/2601.18552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18552">https://arxiv.org/pdf/2601.18552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18552]] Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection(https://arxiv.org/abs/2601.18552)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.</li>
<li><strong>摘要：</strong>法学硕士越来越多地融入日常决策中，但他们的输出可以编码微妙的、意想不到的行为，从而塑造用户的信念和行动。我们将这些隐蔽的、以目标为导向的行为称为隐藏意图，它们可能源自训练和优化人工制品，或者由敌对开发人员故意诱导，但在实践中仍然难以检测。我们引入了十类隐藏意图的分类法，以社会科学研究为基础，按意图、机制、背景和影响进行组织，将注意力从表面行为转移到设计层面的影响策略。我们展示了如何在受控模型中轻松诱导隐藏的意图，为评估和潜在滥用的演示提供了测试平台。我们系统地评估了检测方法，包括推理和非推理法学硕士法官，发现检测在现实的开放世界环境中崩溃，特别是在低流行率条件下，其中误报压倒了精度，漏报掩盖了真正的风险。对精确率和精确率与 FNR 权衡的压力测试揭示了为什么在没有极小的误报率或操纵类型的强先验的情况下审计会失败。最后，定性案例研究表明，所有十个类别都体现在已部署的最先进的法学硕士中，强调了对稳健框架的迫切需要。我们的工作首次系统分析了开放世界环境下法学硕士隐藏意图的可检测性失败，为理解、诱导和压力测试此类行为提供了基础，并建立了灵活的分类法来预测不断变化的威胁并为治理提供信息。</li>
</ul>

<h3>Title: One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization</h3>
<ul>
<li><strong>Authors: </strong>Franziska Weeber, Vera Neplenbroek, Jan Batzner, Sebastian Padó</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18572">https://arxiv.org/abs/2601.18572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18572">https://arxiv.org/pdf/2601.18572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18572]] One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization(https://arxiv.org/abs/2601.18572)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.</li>
<li><strong>摘要：</strong>按社会人口学子群体对法学硕士进行个性化通常可以改善用户体验，但也可能会引入或放大群体之间的偏见和不公平结果。之前的工作采用了所谓的人物角色，即传达给模型的社会人口学用户属性，通过依靠单个提示来提示人物角色（例如用户名或明确的属性提及）来研究法学硕士的偏见。这忽视了法学硕士对提示变化的敏感性（稳健性）和实际交互中某些线索的稀有性（外部有效性）。我们比较了七个开放式和专有法学硕士在四项写作和建议任务中的六种常用角色线索。虽然线索总体上高度相关，但它们会在不同角色的反应中产生很大的差异。因此，我们警告不要从单一角色线索中提出主张，并建议未来的个性化研究来评估多个外部有效线索。</li>
</ul>

<h3>Title: From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuan Cao, Feixiang Liu, Xinyue Wang, Yihan Zhu, Hui Xu, Zheng Wang, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18582">https://arxiv.org/abs/2601.18582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18582">https://arxiv.org/pdf/2601.18582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18582]] From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection(https://arxiv.org/abs/2601.18582)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.</li>
<li><strong>摘要：</strong>性格检测旨在通过个人的社交媒体帖子来衡量个人相应的性格特征。大型语言模型 (LLM) 的进步为人格检测任务提供了新颖的视角。现有方法通过利用法学硕士从文本帖子中提取语义信息作为提示，然后训练分类器进行分类，从而增强人格特质分析。然而，由于人类性格固有的复杂性和微妙的特质间差异，准确地对人格特质进行分类仍然具有挑战性。此外，基于提示的方法通常表现出对专家制作的知识的过度依赖，而没有自主模式学习能力。为了解决这些限制，我们将人格检测视为排序任务而不是分类，并提出了相应的强化学习训练范例。首先，我们采用监督微调（SFT）来建立人格特质排名功能，同时强制执行标准化输出格式，从而创建强大的初始化。随后，我们引入了具有专门的基于排名的奖励函数的组相对策略优化（GRPO）。与具有明确解决方案的验证任务不同，性格评估涉及主观解释和特征类别之间模糊的界限。我们的奖励函数通过训练法学硕士学习最佳答案排名来明确解决这一挑战。综合实验表明，我们的方法在多个人格检测基准上实现了最先进的性能。</li>
</ul>

<h3>Title: Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lintang Sutawika, Gokul Swamy, Zhiwei Steven Wu, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18722">https://arxiv.org/abs/2601.18722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18722">https://arxiv.org/pdf/2601.18722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18722]] Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning(https://arxiv.org/abs/2601.18722)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than of the training data across the single-language, multilingual, and generalization to unseen language settings.</li>
<li><strong>摘要：</strong>当用训练数据中较少见的语言提出问题时，当前的推理大型语言模型 (RLM) 通常表现出比用英语提出相同问题时要低得多的性能。作为回应，我们引入了 \texttt{SP3F} （带有特权成对反馈的自我游戏），这是一个两阶段框架，用于增强多语言推理，而无需目标语言中的 \textit{任何} 数据。首先，我们对英语问答对的翻译版本进行监督微调（SFT），以提高基本模型的正确性。其次，我们以自我对弈的方式利用配对法官的反馈来执行强化学习，法官收到的英文参考响应为 \textit{privileged information}。因此，即使模型的响应都不完全正确，特权成对判断仍然可以判断哪个响应更好。端到端，\texttt{SP3F} 极大地提高了基础模型的性能，甚至在多个数学和非数学任务上的性能优于完全经过后训练的模型，并且在单语言、多语言和泛化到未见过的语言设置方面的训练数据较少。</li>
</ul>

<h3>Title: Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale</h3>
<ul>
<li><strong>Authors: </strong>Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18730">https://arxiv.org/abs/2601.18730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18730">https://arxiv.org/pdf/2601.18730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18730]] Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale(https://arxiv.org/abs/2601.18730)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.</li>
<li><strong>摘要：</strong>协调的宪法框架旨在使大型语言模型（LLM）与以自然语言编写的充满价值的原则保持一致（例如避免使用有偏见的语言）。之前的工作主要集中在参数微调技术上，例如根据人类反馈进行强化学习（RLHF），以灌输这些原则。然而，这些方法对计算的要求很高，需要仔细的工程和调整，并且通常需要难以获得的人类注释数据。我们提出 \textsc{reflect}，一种用于宪法对齐的推理时间框架，不需要任何训练或数据，提供了一种即插即用的方法，用于将指令调整的模型与一组原则对齐。 \textsc{reflect} 完全在上下文中运作，将 (i) 体质条件的基本反应与后一代 (ii) 自我评价、(iii)(a) 自我批评和 (iii)(b) 最终修订结合起来。 \textsc{reflect} 在生成后对原则进行显式上下文推理的技术优于标准的小样本提示，并提供透明的推理痕迹。我们的结果表明， \textsc{reflect} 显着提高了 LLM 对多样化和复杂原则的一致性，包括与模型原始参数微调中强调的原则截然不同的原则，而不牺牲事实推理。 \textsc{reflect} 在减少罕见但严重违反原则的发生率方面特别有效，从而提高代分配尾部的安全性和稳健性。最后，我们表明 \textsc{reflect} 自然地为传统参数微调技术生成有用的训练数据，从而实现有效扩展并减少长期部署场景中的推理时间计算开销。</li>
</ul>

<h3>Title: One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Hongru Cai, Yongqi Li, Tiezheng Yu, Fengbin Zhu, Wenjie Wang, Fuli Feng, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18731">https://arxiv.org/abs/2601.18731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18731">https://arxiv.org/pdf/2601.18731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18731]] One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment(https://arxiv.org/abs/2601.18731)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的对齐旨在使输出与人类偏好保持一致，而个性化对齐则进一步使模型适应个人用户。这依赖于捕捉用户特定偏好并自动提供个性化反馈的个性化奖励模型。然而，开发这些模型面临两个关键挑战：个人用户反馈的缺乏以及需要有效适应看不见的用户。我们认为，解决这些限制需要进行范式转变，从拟合数据到学习用户偏好，再到学习偏好适应的过程。为了实现这一点，我们提出了元奖励建模（MRM），它将个性化奖励建模重新表述为元学习问题。具体来说，我们将每个用户的奖励模型表示为基本奖励函数的加权组合，并使用与模型无关的元学习（MAML）式框架来优化这些权重的初始化，以支持有限反馈下的快速适应。为了确保鲁棒性，我们引入了鲁棒个性化目标（RPO），它在元优化过程中更加重视难以学习的用户。对个性化偏好数据集的大量实验验证了 MRM 增强了小样本个性化，提高了用户鲁棒性，并始终优于基线。</li>
</ul>

<h3>Title: Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory</h3>
<ul>
<li><strong>Authors: </strong>Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18771">https://arxiv.org/abs/2601.18771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18771">https://arxiv.org/pdf/2601.18771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18771]] Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory(https://arxiv.org/abs/2601.18771)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在复杂的推理任务中表现出了卓越的能力，特别是在增强了能够系统地探索外部知识库的搜索机制时。该领域已经从传统的检索增强生成（RAG）框架发展到更复杂的基于搜索的框架，通过显式搜索策略协调多步骤推理。然而，现有的搜索框架仍然严重依赖隐式自然语言推理来确定搜索策略以及如何跨推理步骤利用检索到的信息。这种对隐式推理的依赖为管理子问题之间的依赖关系、有效地重用先前检索的知识以及通过强化学习学习最佳搜索策略带来了根本性的挑战。为了解决这些限制，我们提出了 Dep-Search，这是一种依赖性感知搜索框架，通过 GRPO 集成结构化推理、检索和持久内存，超越了现有的搜索框架。 Dep-Search 引入了显式控制机制，使模型能够分解具有依赖关系的问题，在需要时检索信息，从内存中访问先前存储的知识，并将长推理上下文总结为可重用的内存条目。通过对七个不同问答数据集的广泛实验，我们证明 Dep-Search 显着增强了法学硕士处理复杂多跳推理任务的能力，在不同模型规模的强大基线上实现了实质性改进。</li>
</ul>

<h3>Title: Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Mumin Jia, Jairo Diaz-Rodriguez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18788">https://arxiv.org/abs/2601.18788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18788">https://arxiv.org/pdf/2601.18788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18788]] Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings(https://arxiv.org/abs/2601.18788)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.</li>
<li><strong>摘要：</strong>无监督文本分割至关重要，因为边界标签昂贵且主观，并且通常无法跨域和粒度选择进行传输。我们提出了 Embed-KCPD，这是一种免训练方法，它将句子表示为嵌入向量，并通过最小化惩罚的 KCPD 目标来估计边界。除了算法实例化之外，据我们所知，我们还开发了 $m$ 依赖序列下 KCPD 的第一个依赖感知理论，这是语言中常见的短程依赖的有限内存抽象。我们证明了人口惩罚风险的预言不等式和本地化保证，表明每个真正的变化点都在相对于段长度较小的窗口内恢复。为了将理论与实践联系起来，我们引入了一个基于 LLM 的模拟框架，该框架可生成具有受控有限内存依赖性和已知边界的合成文档，从而验证预测的缩放行为。在标准分割基准中，Embed-KCPD 通常优于强大的无监督基准。对 Taylor Swift 推文的案例研究表明，Embed-KCPD 结合了强大的理论保证、模拟可靠性和文本分割的实际有效性。</li>
</ul>

<h3>Title: MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</h3>
<ul>
<li><strong>Authors: </strong>Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18790">https://arxiv.org/abs/2601.18790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18790">https://arxiv.org/pdf/2601.18790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18790]] MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts(https://arxiv.org/abs/2601.18790)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.</li>
<li><strong>摘要：</strong>大型语言模型越来越针对深度推理进行优化，优先考虑正确执行复杂任务而不是一般对话。我们调查这种对计算的关注是否会产生一种忽视危急情况下安全的“狭隘视野”。我们推出 MortalMATH，这是一个包含 150 个场景的基准，用户在描述日益危及生命的紧急情况（例如中风症状、自由落体）时请求代数帮助。我们发现了一个尖锐的行为分歧：通才模型（如 Llama-3.1）成功地拒绝了数学来解决危险。相比之下，专门的推理模型（如 Qwen-3-32b 和 GPT-5-nano）通常完全忽略紧急情况，在用户描述死亡时保持超过 95% 的任务完成率。此外，推理所需的计算时间会带来危险的延迟：在提供任何潜在帮助之前最多需要 15 秒。这些结果表明，不懈地追求正确答案的训练模型可能会无意中忘记安全部署所需的生存本能。</li>
</ul>

<h3>Title: ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models</h3>
<ul>
<li><strong>Authors: </strong>Brian Ondov, Chia-Hsuan Chang, Yujia Zhou, Mauro Giuffrè, Hua Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18796">https://arxiv.org/abs/2601.18796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18796">https://arxiv.org/pdf/2601.18796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18796]] ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models(https://arxiv.org/abs/2601.18796)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.</li>
<li><strong>摘要：</strong>文本嵌入已成为各种语言应用程序的重要组成部分。然而，解释、探索和反转嵌入空间的方法是有限的，降低了透明度并排除了潜在有价值的生成用例。在这项工作中，我们使用最近报道的嵌入语言模型（ELM）方法将大型语言模型与临床试验的嵌入结合起来。我们开发了一个开源的、与领域无关的 ELM 架构和培训框架，设计临床试验的培训任务，并引入经过专家验证的合成数据集。然后，我们训练一系列 ELM，探索任务和训练制度的影响。我们的最终模型 ctELM 可以准确地描述和比较仅来自嵌入的未见过的临床试验，并从新的载体产生合理的临床试验。我们进一步表明，生成的试验摘要对沿着研究对象年龄和性别的概念向量移动嵌入做出响应。我们的公共 ELM 实施和实验结果将有助于将大型语言模型与生物医学领域及其他领域的嵌入空间保持一致。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
