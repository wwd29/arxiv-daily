<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-01</h1>
<h3>Title: Title:
          An Analysis of Multilingual FActScore</h3>
<ul>
<li><strong>Authors: </strong>Kim Trong Vu, Michael Krumdick, Varshini Reddy, Franck Dernoncourt, Viet Dac Lai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          An Analysis of Multilingual FActScore(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>FActScore has gained popularity as a metric to estimate the factuality of long-form texts generated by Large Language Models (LLMs) in English. However, there has not been any work in studying the behavior of FActScore in other languages. This paper studies the limitations of each component in the four-component pipeline of FActScore in the multilingual setting. We introduce a new dataset for FActScore on texts generated by strong multilingual LLMs. Our evaluation shows that LLMs exhibit distinct behaviors in both fact extraction and fact scoring tasks. No LLM produces consistent and reliable FActScore across languages with varying levels of resources. We also find that the knowledge source plays an important role in the quality of the estimated FActScore. Using Wikipedia as the knowledge source may hinder the true FActScore of long-form text due to its limited coverage in medium- and low-resource languages. We also incorporate three mitigations to our knowledge source that ultimately improve FActScore estimation across all languages.</li>
<li><strong>摘要：</strong>FActScore 已成为一种衡量英语大型语言模型 (LLM) 生成的长文本真实性的指标。然而，目前还没有研究 FActScore 在其他语言中的行为的研究。本文研究了多语言环境中 FActScore 四组件管道中每个组件的局限性。我们为强大的多语言 LLM 生成的文本引入了一个新的 FActScore 数据集。我们的评估表明，LLM 在事实提取和事实评分任务中表现出不同的行为。没有一个 LLM 能够在具有不同资源水平的语言中产生一致且可靠的 FActScore。我们还发现知识源在估计的 FActScore 的质量中起着重要作用。使用 Wikipedia 作为知识源可能会妨碍长文本的真实 FActScore，因为它在中低资源语言中的覆盖范围有限。我们还将三种缓解措施纳入我们的知识源，最终改善所有语言的 FActScore 估计。</li>
</ul>

<h3>Title: Title:
          Can Large Language Models Generate High-quality Patent Claims?</h3>
<ul>
<li><strong>Authors: </strong>Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can Large Language Models Generate High-quality Patent Claims?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions' features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种文本生成任务中表现出色，但在提供高度结构化和精确语言的专利领域仍未得到充分探索。本文构建了一个数据集来研究当前 LLM 在专利权利要求生成中的表现。我们的结果表明，基于专利描述生成权利要求的效果优于以前依赖摘要的研究。有趣的是，当前针对专利的 LLM 表现比最先进的通用 LLM 差得多，这凸显了未来研究领域内 LLM 的必要性。我们还发现 LLM 可以生成高质量的第一个独立权利要求，但其对后续从属权利要求的性能明显下降。此外，微调可以提高发明特征的完整性、概念清晰度和特征链接。在测试的 LLM 中，GPT-4 在专利专家的全面人工评估中表现出色，具有更好的特征覆盖率、概念清晰度和技术连贯性。尽管具有这些能力，但仍然需要进行全面的修订和修改才能通过严格的专利审查并确保法律稳健性。</li>
</ul>

<h3>Title: Title:
          Changing Answer Order Can Decrease MMLU Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, Megan Ung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Changing Answer Order Can Decrease MMLU Accuracy(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) have grown in prevalence, particular benchmarks have become essential for the evaluation of these models and for understanding model capabilities. Most commonly, we use test accuracy averaged across multiple subtasks in order to rank models on leaderboards, to determine which model is best for our purposes. In this paper, we investigate the robustness of the accuracy measurement on a widely used multiple choice question answering dataset, MMLU. When shuffling the answer label contents, we find that all explored models decrease in accuracy on MMLU, but not every model is equally sensitive. These findings suggest a possible adjustment to the standard practice of leaderboard testing, where we additionally consider the percentage of examples each model answers correctly by random chance.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的普及，特定的基准对于评估这些模型和了解模型功能变得至关重要。最常见的是，我们使用跨多个子任务的平均测试准确率来对排行榜上的模型进行排名，以确定哪种模型最适合我们的目的。在本文中，我们研究了广泛使用的多项选择题回答数据集 MMLU 的准确率测量的稳健性。当打乱答案标签内容时，我们发现所有探索的模型在 MMLU 上的准确率都会下降，但并非每个模型都同样敏感。这些发现表明可以对排行榜测试的标准做法进行调整，我们还会考虑每个模型随机正确回答的示例百分比。</li>
</ul>

<h3>Title: Title:
          xTower: A Multilingual LLM for Explaining and Correcting Translation Errors</h3>
<ul>
<li><strong>Authors: </strong>Marcos Treviso, Nuno M. Guerreiro, Sweta Agrawal, Ricardo Rei, José Pombal, Tania Vaz, Helena Wu, Beatriz Silva, Daan van Stigt, André F. T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          xTower: A Multilingual LLM for Explaining and Correcting Translation Errors(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While machine translation (MT) systems are achieving increasingly strong performance on benchmarks, they often produce translations with errors and anomalies. Understanding these errors can potentially help improve the translation quality and user experience. This paper introduces xTower, an open large language model (LLM) built on top of TowerBase designed to provide free-text explanations for translation errors in order to guide the generation of a corrected translation. The quality of the generated explanations by xTower are assessed via both intrinsic and extrinsic evaluation. We ask expert translators to evaluate the quality of the explanations across two dimensions: relatedness towards the error span being explained and helpfulness in error understanding and improving translation quality. Extrinsically, we test xTower across various experimental setups in generating translation corrections, demonstrating significant improvements in translation quality. Our findings highlight xTower's potential towards not only producing plausible and helpful explanations of automatic translations, but also leveraging them to suggest corrected translations.</li>
<li><strong>摘要：</strong>虽然机器翻译 (MT) 系统在基准测试中的表现越来越好，但它们经常会产生有错误和异常的翻译。了解这些错误可能有助于提高翻译质量和用户体验。本文介绍了 xTower，这是一个建立在 TowerBase 之上的开放式大型语言模型 (LLM)，旨在为翻译错误提供自由文本解释，以指导生成更正的翻译。xTower 生成的解释的质量通过内在和外在评估来评估。我们要求专业翻译人员从两个维度评估解释的质量：与所解释的错误范围的相关性以及对理解错误和提高翻译质量的帮助性。在外部，我们在各种实验设置中测试了 xTower 生成翻译更正的效果，结果显示翻译质量有显著提高。我们的研究结果强调了 xTower 不仅可以为自动翻译提供合理且有用的解释，还可以利用它们来建议更正的翻译。</li>
</ul>

<h3>Title: Title:
          LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shouchang Guo, Sonam Damani, Keng-hao Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In prompt tuning, a prefix or suffix text is added to the prompt, and the embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix are optimized to gain more control over language models for specific tasks. This approach eliminates the need for hand-crafted prompt engineering or explicit model fine-tuning. Prompt tuning is significantly more parameter-efficient than model fine-tuning, as it involves optimizing partial inputs of language models to produce desired outputs. In this work, we aim to further reduce the amount of trainable parameters required for a language model to perform well on specific tasks. We propose Low-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves efficient prompt optimization. The proposed method demonstrates similar outcomes to full parameter prompt tuning while reducing the number of trainable parameters by a factor of 5. It also provides promising results compared to the state-of-the-art methods that would require 10 to 20 times more parameters.</li>
<li><strong>摘要：</strong>在提示调优中，会将前缀或后缀文本添加到提示中，并优化前缀/后缀的嵌入（软提示）或标记索引（硬提示），以更好地控制特定任务的语言模型。这种方法无需手动设计提示工程或显式模型微调。提示调优比模型微调具有更高的参数效率，因为它涉及优化语言模型的部分输入以产生所需的输出。在这项工作中，我们旨在进一步减少语言模型在特定任务上表现良好所需的可训练参数数量。我们提出了低秩提示调优 (LoPT)，这是一种实现高效提示优化的低秩提示模型。所提出的方法展示了与全参数提示调优类似的结果，同时将可训练参数的数量减少了 5 倍。与需要 10 到 20 倍参数的最先进的方法相比，它还提供了有希望的结果。</li>
</ul>

<h3>Title: Title:
          Development and Evaluation of a Retrieval-Augmented Generation Tool for Creating SAPPhIRE Models of Artificial Systems</h3>
<ul>
<li><strong>Authors: </strong>Anubhab Majumder, Kausik Bhattacharya, Amaresh Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Development and Evaluation of a Retrieval-Augmented Generation Tool for Creating SAPPhIRE Models of Artificial Systems(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Representing systems using the SAPPhIRE causality model is found useful in supporting design-by-analogy. However, creating a SAPPhIRE model of artificial or biological systems is an effort-intensive process that requires human experts to source technical knowledge from multiple technical documents regarding how the system works. This research investigates how to leverage Large Language Models (LLMs) in creating structured descriptions of systems using the SAPPhIRE model of causality. This paper, the second part of the two-part research, presents a new Retrieval-Augmented Generation (RAG) tool for generating information related to SAPPhIRE constructs of artificial systems and reports the results from a preliminary evaluation of the tool's success - focusing on the factual accuracy and reliability of outcomes.</li>
<li><strong>摘要：</strong>使用 SAPPhIRE 因果关系模型表示系统有助于支持类比设计。但是，创建人工或生物系统的 SAPPhIRE 模型是一个耗费大量精力的过程，需要人类专家从多个技术文档中获取有关系统工作原理的技术知识。本研究调查了如何利用大型语言模型 (LLM) 使用 SAPPhIRE 因果关系模型创建系统的结构化描述。本文是两部分研究的第二部分，介绍了一种新的检索增强生成 (RAG) 工具，用于生成与人工系统的 SAPPhIRE 构造相关的信息，并报告了对该工具成功的初步评估结果——重点关注结果的事实准确性和可靠性。</li>
</ul>

<h3>Title: Title:
          Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts</h3>
<ul>
<li><strong>Authors: </strong>Naseela Pervez, Alexander J. Titus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly utilized to assist in scientific and academic writing, helping authors enhance the coherence of their articles. Previous studies have highlighted stereotypes and biases present in LLM outputs, emphasizing the need to evaluate these models for their alignment with human narrative styles and potential gender biases. In this study, we assess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large, and Gemini 1.5 Flash - by analyzing their performance on benchmark text-generation tasks for scientific abstracts. We employ the Linguistic Inquiry and Word Count (LIWC) framework to extract lexical, psychological, and social features from the generated texts. Our findings indicate that, while these models generally produce text closely resembling human authored content, variations in stylistic features suggest significant gender biases. This research highlights the importance of developing LLMs that maintain a diversity of writing styles to promote inclusivity in academic discourse.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于协助科学和学术写作，帮助作者提高文章的连贯性。先前的研究强调了 LLM 输出中存在的刻板印象和偏见，强调需要评估这些模型是否与人类叙事风格和潜在的性别偏见保持一致。在本研究中，我们通过分析三个著名的 LLM（Claude 3 Opus、Mistral AI Large 和 Gemini 1.5 Flash）在科学摘要的基准文本生成任务中的表现来评估它们的一致性。我们使用语言查询和字数统计 (LIWC) 框架从生成的文本中提取词汇、心理和社会特征。我们的研究结果表明，虽然这些模型通常会生成与人类创作内容非常相似的文本，但风格特征的变化表明存在明显的性别偏见。这项研究强调了开发保持多样化写作风格的 LLM 以促进学术话语包容性的重要性。</li>
</ul>

<h3>Title: Title:
          Monitoring Latent World States in Language Models with Propositional Probes</h3>
<ul>
<li><strong>Authors: </strong>Jiahai Feng, Stuart Russell, Jacob Steinhardt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Monitoring Latent World States in Language Models with Propositional Probes(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Language models are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of language models could help monitor and correct unfaithful behavior. We hypothesize that language models represent their input contexts in a latent world model, and seek to extract this latent world state from the activations. We do so with 'propositional probes', which compositionally probe tokens for lexical information and bind them into logical propositions representing the world state. For example, given the input context ''Greg is a nurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg, nurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to this is identifying a 'binding subspace' in which bound tokens have high similarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and ''physicist''). We validate propositional probes in a closed-world setting with finitely many predicates and properties. Despite being trained on simple templated contexts, propositional probes generalize to contexts rewritten as short stories and translated to Spanish. Moreover, we find that in three settings where language models respond unfaithfully to the input context -- prompt injections, backdoor attacks, and gender bias -- the decoded propositions remain faithful. This suggests that language models often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.</li>
<li><strong>摘要：</strong>语言模型容易受到偏见、谄媚、后门和其他导致对输入上下文做出不忠实反应的倾向的影响。解释语言模型的内部状态可以帮助监控和纠正不忠实的行为。我们假设语言模型在潜在世界模型中表示其输入上下文，并试图从激活中提取这种潜在世界状态。我们使用“命题探测”来实现这一点，它组合探测标记以获取词汇信息，并将它们绑定到代表世界状态的逻辑命题中。例如，给定输入上下文“Greg 是一名护士。Laura 是一名物理学家。”，我们从模型的激活中解码命题“WorksAs(Greg, 护士)”和“WorksAs(Laura, 物理学家)”。关键在于确定一个“绑定子空间”，其中绑定标记具有高度相似性（“Greg”和“护士”），但未绑定标记则不相似（“Greg”和“物理学家”）。我们在具有有限多个谓词和属性的封闭世界环境中验证命题探测。尽管在简单的模板化上下文中进行训练，但命题探测可以推广到重写为短篇小说并翻译成西班牙语的上下文。此外，我们发现在语言模型对输入上下文反应不忠实的三种环境中——提示注入、后门攻击和性别偏见——解码后的命题仍然忠实。这表明语言模型通常会对忠实的世界模型进行编码，但对其进行解码却不忠实，这促使人们寻找更好的可解释性工具来监控 LM。</li>
</ul>

<h3>Title: Title:
          Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Miyoung Ko, Sue Hyun Park, Joonsuk Park, Minjoon Seo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite significant advancements, there is a limited understanding of how large language models (LLMs) utilize knowledge for reasoning. To address this, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with parent nodes of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, discrepancies in LLMs' performance on simpler sub-problems versus complex questions. We also measure backward discrepancy, where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models have more discrepancies than larger models. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.</li>
<li><strong>摘要：</strong>尽管取得了重大进展，但人们对大型语言模型 (LLM) 如何利用知识进行推理的理解有限。为了解决这个问题，我们提出了一种方法，将复杂的现实世界问题解构为一个图，将每个问题表示为一个节点，其父节点是解决问题所需的背景知识。我们开发了 DepthQA 数据集，将问题解构为三个深度：(i) 回忆概念知识，(ii) 应用程序知识，(iii) 分析战略知识。基于分层图，我们量化了前向差异，即 LLM 在较简单子问题和复杂问题上的表现差异。我们还测量了后向差异，其中 LLM 可以回答复杂问题，但在解决简单问题时会遇到困难。我们的分析表明，较小的模型比较大的模型有更大的差异。此外，通过多轮交互引导模型从简单问题到复杂问题可以提高不同模型大小的性能，这凸显了结构化中间步骤在知识推理中的重要性。这项工作增进了我们对 LLM 推理的理解，并提出了提高他们解决问题的能力的方法。</li>
</ul>

<h3>Title: Title:
          Are Generative Language Models Multicultural? A Study on Hausa Culture and Emotions using ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Said Ahmad, Shiran Dudy, Resmi Ramachandranpillai, Kenneth Church</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Are Generative Language Models Multicultural? A Study on Hausa Culture and Emotions using ChatGPT(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), such as ChatGPT, are widely used to generate content for various purposes and audiences. However, these models may not reflect the cultural and emotional diversity of their users, especially for low-resource languages. In this paper, we investigate how ChatGPT represents Hausa's culture and emotions. We compare responses generated by ChatGPT with those provided by native Hausa speakers on 37 culturally relevant questions. We conducted experiments using emotion analysis and applied two similarity metrics to measure the alignment between human and ChatGPT responses. We also collected human participants ratings and feedback on ChatGPT responses. Our results show that ChatGPT has some level of similarity to human responses, but also exhibits some gaps and biases in its knowledge and awareness of the Hausa culture and emotions. We discuss the implications and limitations of our methodology and analysis and suggest ways to improve the performance and evaluation of LLMs for low-resource languages.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM)，例如 ChatGPT，被广泛用于为各种目的和受众生成内容。然而，这些模型可能无法反映其用户的文化和情感多样性，尤其是对于资源匮乏的语言。在本文中，我们研究了 ChatGPT 如何代表豪萨人的文化和情感。我们将 ChatGPT 生成的回答与豪萨语母语人士对 37 个文化相关问题的回答进行了比较。我们使用情感分析进行了实验，并应用了两个相似性指标来衡量人类和 ChatGPT 反应之间的一致性。我们还收集了人类参与者对 ChatGPT 反应的评分和反馈。我们的结果表明，ChatGPT 与人类反应有一定程度的相似性，但在对豪萨文化和情感的知识和认识方面也表现出一些差距和偏见。我们讨论了我们的方法和分析的含义和局限性，并提出了改进资源匮乏语言的 LLM 性能和评估的方法。</li>
</ul>

<h3>Title: Title:
          Captioning Visualizations with Large Language Models (CVLLM): A Tutorial</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Carenini, Jordon Johnson, Ali Salamatian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Captioning Visualizations with Large Language Models (CVLLM): A Tutorial(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Automatically captioning visualizations is not new, but recent advances in large language models(LLMs) open exciting new possibilities. In this tutorial, after providing a brief review of Information Visualization (InfoVis) principles and past work in captioning, we introduce neural models and the transformer architecture used in generic LLMs. We then discuss their recent applications in InfoVis, with a focus on captioning. Additionally, we explore promising future directions in this field.</li>
<li><strong>摘要：</strong>自动为可视化添加字幕并非新鲜事，但大型语言模型 (LLM) 的最新进展开辟了令人兴奋的新可能性。在本教程中，在简要回顾信息可视化 (InfoVis) 原理和过去的字幕工作之后，我们将介绍神经模型和通用 LLM 中使用的转换器架构。然后，我们将讨论它们在 InfoVis 中的最新应用，重点是字幕。此外，我们还探索了该领域有希望的未来方向。</li>
</ul>

<h3>Title: Title:
          Handling Ontology Gaps in Semantic Parsing</h3>
<ul>
<li><strong>Authors: </strong>Andrea Bacciu, Marco Damonte, Marco Basaldella, Emilio Monti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Handling Ontology Gaps in Semantic Parsing(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination, agent</a></li>
<li><strong>Abstract: </strong>The majority of Neural Semantic Parsing (NSP) models are developed with the assumption that there are no concepts outside the ones such models can represent with their target symbols (closed-world assumption). This assumption leads to generate hallucinated outputs rather than admitting their lack of knowledge. Hallucinations can lead to wrong or potentially offensive responses to users. Hence, a mechanism to prevent this behavior is crucial to build trusted NSP-based Question Answering agents. To that end, we propose the Hallucination Simulation Framework (HSF), a general setting for stimulating and analyzing NSP model hallucinations. The framework can be applied to any NSP task with a closed-ontology. Using the proposed framework and KQA Pro as the benchmark dataset, we assess state-of-the-art techniques for hallucination detection. We then present a novel hallucination detection strategy that exploits the computational graph of the NSP model to detect the NSP hallucinations in the presence of ontology gaps, out-of-domain utterances, and to recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and ~1%. This is the first work in closed-ontology NSP that addresses the problem of recognizing ontology gaps. We release our code and checkpoints at this https URL.</li>
<li><strong>摘要：</strong>大多数神经语义解析 (NSP) 模型都是基于这样的假设开发的：除了这些模型可以用目标符号表示的概念之外，没有其他概念（封闭世界假设）。这种假设导致产生幻觉输出，而不是承认他们缺乏知识。幻觉可能会导致对用户做出错误或潜在的冒犯性反应。因此，防止这种行为的机制对于构建基于 NSP 的可信问答代理至关重要。为此，我们提出了幻觉模拟框架 (HSF)，这是一种用于刺激和分析 NSP 模型幻觉的通用设置。该框架可以应用于任何具有封闭本体的 NSP 任务。使用提出的框架和 KQA Pro 作为基准数据集，我们评估了最先进的幻觉检测技术。然后，我们提出了一种新颖的幻觉检测策略，该策略利用 NSP 模型的计算图来检测存在本体差距、域外话语的情况下的 NSP 幻觉，并识别 NSP 错误，分别将 F1 分数提高约 21%、约 24% 和约 1%。这是封闭本体 NSP 中第一项解决识别本体差距问题的工作。我们在此 https URL 上发布了我们的代码和检查点。</li>
</ul>

<h3>Title: Title:
          Context Matters: An Empirical Study of the Impact of Contextual Information in Temporal Question Answering Systems</h3>
<ul>
<li><strong>Authors: </strong>Dan Schumacher, Fatemeh Haji, Tara Grey, Niharika Bandlamudi, Nupoor Karnik, Gagana Uday Kumar, Jason Cho-Yu Chiang, Paul Rad, Nishant Vishwamitra, Anthony Rios</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Context Matters: An Empirical Study of the Impact of Contextual Information in Temporal Question Answering Systems(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often struggle with temporal reasoning, crucial for tasks like historical event analysis and time-sensitive information retrieval. Despite advancements, state-of-the-art models falter in handling temporal information, especially when faced with irrelevant or noisy contexts. This paper addresses this gap by empirically examining the robustness of temporal question-answering (TQA) systems trained on various context types, including relevant, irrelevant, slightly altered, and no context. Our findings indicate that training with a mix of these contexts enhances model robustness and accuracy. Additionally, we show that the position of context relative to the question significantly impacts performance, with question-first positioning yielding better results. We introduce two new context-rich TQA datasets, ContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines for training robust TQA models. Our work lays the foundation for developing reliable and context-aware temporal QA systems, with broader implications for enhancing LLM robustness against diverse and potentially adversarial information.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常在时间推理方面存在困难，而时间推理对于历史事件分析和时间敏感信息检索等任务至关重要。尽管取得了进展，但最先进的模型在处理时间信息方面仍存在问题，尤其是在面对不相关或嘈杂的上下文时。本文通过实证检验在各种上下文类型（包括相关、不相关、略有改变和无上下文）上训练的时间问答 (TQA) 系统的稳健性来解决这一差距。我们的研究结果表明，使用这些上下文的混合进行训练可以提高模型的稳健性和准确性。此外，我们还表明，上下文相对于问题的位置会显著影响性能，问题优先的定位会产生更好的结果。我们引入了两个新的上下文丰富的 TQA 数据集 ContextAQA 和 ContextTQE，并为训练稳健的 TQA 模型提供了全面的评估和指导。我们的工作为开发可靠且具有上下文感知的时间 QA 系统奠定了基础，对增强 LLM 对各种潜在对抗信息的稳健性具有更广泛的意义。</li>
</ul>

<h3>Title: Title:
          Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations</h3>
<ul>
<li><strong>Authors: </strong>Ritam Dutt, Zhen Wu, Kelly Shi, Divyanshu Sheth, Prakhar Gupta, Carolyn Penstein Rose</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2,340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification, zero-shot, and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora.</li>
<li><strong>摘要：</strong>我们提出了一种可推广的分类方法，该方法利用大型语言模型 (LLM) 来促进对对话中隐含编码的社交意义的检测。我们设计了一个多方面的提示来提取将可见线索与潜在社交意义联系起来的推理的文本解释。这些提取的解释或理由可作为对对话文本的增强，以促进对话理解和迁移。我们在 2,340 个实验设置中的实证结果表明，添加这些理由具有显著的积极影响。我们的研究结果适用于两个不同的社交意义检测任务的域内分类、零样本和少样本域迁移，每个任务都涵盖两个不同的语料库。</li>
</ul>

<h3>Title: Title:
          Rethinking harmless refusals when fine-tuning foundation models</h3>
<ul>
<li><strong>Authors: </strong>Florin Pop, Judd Rosenblatt, Diogo Schwerz de Lucena, Michael Vaiana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Rethinking harmless refusals when fine-tuning foundation models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the degree to which fine-tuning in Large Language Models (LLMs) effectively mitigates versus merely conceals undesirable behavior. Through the lens of semi-realistic role-playing exercises designed to elicit such behaviors, we explore the response dynamics of LLMs post fine-tuning interventions. Our methodology involves prompting models for Chain-of-Thought (CoT) reasoning and analyzing the coherence between the reasoning traces and the resultant outputs. Notably, we identify a pervasive phenomenon we term \emph{reason-based deception}, where models either stop producing reasoning traces or produce seemingly ethical reasoning traces that belie the unethical nature of their final outputs. We further examine the efficacy of response strategies (polite refusal versus explicit rebuttal) in curbing the occurrence of undesired behavior in subsequent outputs of multi-turn interactions. Our findings reveal that explicit rebuttals significantly outperform polite refusals in preventing the continuation of undesired outputs and nearly eliminate reason-based deception, challenging current practices in model fine-tuning. Accordingly, the two key contributions of this paper are (1) defining and studying reason-based deception, a new type of hidden behavior, and (2) demonstrating that rebuttals provide a more robust response model to harmful requests than refusals, thereby highlighting the need to reconsider the response strategies in fine-tuning approaches.</li>
<li><strong>摘要：</strong>在本文中，我们研究了大型语言模型 (LLM) 中的微调在多大程度上有效缓解了不良行为，而不是仅仅掩盖了不良行为。通过旨在引发此类行为的半现实角色扮演练习，我们探索了 LLM 在微调干预后的响应动态。我们的方法包括提示模型进行思路链 (CoT) 推理，并分析推理轨迹与结果输出之间的一致性。值得注意的是，我们发现了一种普遍存在的现象，我们称之为 \emph{基于理由的欺骗}，其中模型要么停止产生推理轨迹，要么产生看似合乎道德的推理轨迹，掩盖了其最终输出的不道德性质。我们进一步研究了响应策略（礼貌拒绝与明确反驳）在抑制多轮交互后续输出中出现不良行为方面的有效性。我们的研究结果表明，在防止不良结果继续发生方面，明确的反驳明显优于礼貌的拒绝，并且几乎消除了基于理由的欺骗，这对模型微调的现行做法提出了挑战。因此，本文的两个主要贡献是：（1）定义和研究基于理由的欺骗，这是一种新型的隐藏行为；（2）证明反驳比拒绝为有害请求提供了更强大的响应模型，从而强调了重新考虑微调方法中的响应策略的必要性。</li>
</ul>

<h3>Title: Title:
          SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Synthetic data generation has gained significant attention recently for its utility in training large vision and language models. However, the application of synthetic data to the training of multimodal context-augmented generation systems has been relatively unexplored. This gap in existing work is important because existing vision and language models (VLMs) are not trained specifically for context-augmented generation. Resources for adapting such models are therefore crucial for enabling their use in retrieval-augmented generation (RAG) settings, where a retriever is used to gather relevant information that is then subsequently provided to a generative model via context augmentation. To address this challenging problem, we generate SK-VQA: a large synthetic multimodal dataset containing over 2 million question-answer pairs which require external knowledge to determine the final answer. Our dataset is both larger and significantly more diverse than existing resources of its kind, possessing over 11x more unique questions and containing images from a greater variety of sources than previously-proposed datasets. Through extensive experiments, we demonstrate that our synthetic dataset can not only serve as a challenging benchmark, but is also highly effective for adapting existing generative multimodal models for context-augmented generation.</li>
<li><strong>摘要：</strong>合成数据生成最近因其在训练大型视觉和语言模型中的实用性而备受关注。然而，合成数据在多模态上下文增强生成系统训练中的应用相对较少。现有工作中的这一差距很重要，因为现有的视觉和语言模型 (VLM) 并非专门针对上下文增强生成进行训练。因此，调整此类模型的资源对于使其在检索增强生成 (RAG) 设置中的使用至关重要，在这种设置中，检索器用于收集相关信息，然后通过上下文增强将其提供给生成模型。为了解决这个具有挑战性的问题，我们生成了 SK-VQA：一个大型合成多模态数据集，包含超过 200 万个问答对，需要外部知识来确定最终答案。我们的数据集比现有的同类资源更大，也更加多样化，拥有超过 11 倍的独特问题，并且包含来自比以前提出的数据集更多样化来源的图像。通过大量实验，我们证明我们的合成数据集不仅可以作为具有挑战性的基准，而且还能够非常有效地调整现有的生成多模态模型以进行上下文增强生成。</li>
</ul>

<h3>Title: Title:
          Mixture of In-Context Experts Enhance LLMs' Long Context Awareness</h3>
<ul>
<li><strong>Authors: </strong>Hongzhan Lin, Ang Lv, Yuhan Chen, Chen Zhu, Yang Song, Hengshu Zhu, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mixture of In-Context Experts Enhance LLMs' Long Context Awareness(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Many studies have revealed that large language models (LLMs) exhibit uneven awareness of different contextual positions.Their limited context awareness can lead to overlooking critical information and subsequent task failures. While several approaches have been proposed to enhance LLMs' context awareness, achieving both effectiveness and efficiency remains this http URL this paper, for LLMs utilizing RoPE as position embeddings, we introduce a novel method called ``Mixture of In-Context Experts'' (MoICE) to address this challenge. MoICE comprises two key components: a router integrated into each attention head within LLMs and a lightweight router-only training optimization strategy: (1) MoICE views each RoPE angle as an `in-context' expert, demonstrated to be capable of directing the attention of a head to specific contextual positions. Consequently, each attention head flexibly processes tokens using multiple RoPE angles dynamically selected by the router to attend to the needed positions. This approach mitigates the risk of overlooking essential contextual information. (2) The router-only training strategy entails freezing LLM parameters and exclusively updating routers for only a few steps. When applied to open-source LLMs including Llama and Mistral, MoICE surpasses prior methods across multiple tasks on long context understanding and generation, all while maintaining commendable inference efficiency.</li>
<li><strong>摘要：</strong>许多研究表明，大型语言模型 (LLM) 对不同上下文位置的意识不均衡。它们有限的上下文意识可能导致忽略关键信息并导致随后的任务失败。虽然已经提出了几种方法来增强 LLM 的上下文意识，但同时实现有效性和效率仍然是本文要解决的问题。对于使用 RoPE 作为位置嵌入的 LLM，我们引入了一种称为“上下文专家混合”（MoICE）的新方法来应对这一挑战。MoICE 包含两个关键组件：集成到 LLM 中每个注意头的路由器和轻量级的仅路由器训练优化策略：（1）MoICE 将每个 RoPE 角度视为“上下文”专家，被证明能够将头部的注意力引导到特定的上下文位置。因此，每个注意头使用路由器动态选择的多个 RoPE 角度灵活地处理标记以关注所需的位置。这种方法减轻了忽略重要上下文信息的风险。（2）仅路由器训练策略需要冻结 LLM 参数并仅更新路由器几个步骤。当应用于包括 Llama 和 Mistral 在内的开源 LLM 时，MoICE 在长上下文理解和生成的多个任务中超越了之前的方法，同时保持了令人称赞的推理效率。</li>
</ul>

<h3>Title: Title:
          Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework with Debate-Driven Text Planning for Argument Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhe Hu, Hou Pong Chan, Jing Li, Yu Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework with Debate-Driven Text Planning for Argument Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Writing persuasive arguments is a challenging task for both humans and machines. It entails incorporating high-level beliefs from various perspectives on the topic, along with deliberate reasoning and planning to construct a coherent narrative. Current language models often generate surface tokens autoregressively, lacking explicit integration of these underlying controls, resulting in limited output diversity and coherence. In this work, we propose a persona-based multi-agent framework for argument writing. Inspired by the human debate, we first assign each agent a persona representing its high-level beliefs from a unique perspective, and then design an agent interaction process so that the agents can collaboratively debate and discuss the idea to form an overall plan for argument writing. Such debate process enables fluid and nonlinear development of ideas. We evaluate our framework on argumentative essay writing. The results show that our framework can generate more diverse and persuasive arguments through both automatic and human evaluations.</li>
<li><strong>摘要：</strong>撰写有说​​服力的论据对人类和机器来说都是一项具有挑战性的任务。它需要结合来自不同角度的关于该主题的高级信念，以及深思熟虑的推理和计划来构建连贯的叙述。当前的语言模型通常自回归地生成表面标记，缺乏对这些底层控制的明确集成，导致输出的多样性和连贯性有限。在这项工作中，我们提出了一个基于角色的多智能体论据写作框架。受人类辩论的启发，我们首先为每个智能体分配一个从独特视角代表其高级信念的角色，然后设计一个智能体交互过程，以便智能体可以协作辩论和讨论该想法，以形成论据写作的总体计划。这样的辩论过程使思想的发展变得流畅和非线性。我们在论证性文章写作上评估了我们的框架。结果表明，我们的框架可以通过自动和人工评估生成更多样化和更有说服力的论点。</li>
</ul>

<h3>Title: Title:
          Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment</h3>
<ul>
<li><strong>Authors: </strong>Orgest Xhelili, Yihong Liu, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Multilingual pre-trained models (mPLMs) have shown impressive performance on cross-lingual transfer tasks. However, the transfer performance is often hindered when a low-resource target language is written in a different script than the high-resource source language, even though the two languages may be related or share parts of their vocabularies. Inspired by recent work that uses transliteration to address this problem, our paper proposes a transliteration-based post-pretraining alignment (PPA) method aiming to improve the cross-lingual alignment between languages using diverse scripts. We select two areal language groups, $\textbf{Mediterranean-Amharic-Farsi}$ and $\textbf{South+East Asian Languages}$, wherein the languages are mutually influenced but use different scripts. We apply our method to these language groups and conduct extensive experiments on a spectrum of downstream tasks. The results show that after PPA, models consistently outperform the original model (up to 50% for some tasks) in English-centric transfer. In addition, when we use languages other than English as sources in transfer, our method obtains even larger improvements. We will make our code and models publicly available at \url{this https URL}.</li>
<li><strong>摘要：</strong>多语言预训练模型 (mPLM) 在跨语言迁移任务上表现出色。然而，当资源较少的目标语言与资源丰富的源语言使用不同的文字书写时，迁移性能通常会受到阻碍，即使这两种语言可能相关或共享部分词汇。受最近使用音译解决此问题的研究的启发，我们的论文提出了一种基于音译的后预训练对齐 (PPA) 方法，旨在改善使用不同文字的语言之间的跨语言对齐。我们选择了两个区域语言组，地中海-阿姆哈拉语-波斯语和东南亚语言，其中语言相互影响但使用不同的文字。我们将我们的方法应用于这些语言组，并对一系列下游任务进行了广泛的实验。结果表明，经过 PPA 后，模型在以英语为中心的迁移中始终优于原始模型（在某些任务中高达 50%）。此外，当我们使用英语以外的语言作为迁移源时，我们的方法会获得更大的改进。我们将在 \url{此 https URL} 上公开我们的代码和模型。</li>
</ul>

<h3>Title: Title:
          Belief Revision: The Adaptability of Large Language Models Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Belief Revision: The Adaptability of Large Language Models Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The capability to reason from text is crucial for real-world NLP applications. Real-world scenarios often involve incomplete or evolving data. In response, individuals update their beliefs and understandings accordingly. However, most existing evaluations assume that language models (LMs) operate with consistent information. We introduce Belief-R, a new dataset designed to test LMs' belief revision ability when presented with new evidence. Inspired by how humans suppress prior inferences, this task assesses LMs within the newly proposed delta reasoning ($\Delta R$) framework. Belief-R features sequences of premises designed to simulate scenarios where additional information could necessitate prior conclusions drawn by LMs. We evaluate $\sim$30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information. Further, models adept at updating often underperformed in scenarios without necessary updates, highlighting a critical trade-off. These insights underscore the importance of improving LMs' adaptiveness to changing information, a step toward more reliable AI systems.</li>
<li><strong>摘要：</strong>从文本推理的能力对于现实世界的 NLP 应用至关重要。现实世界的场景通常涉及不完整或不断发展的数据。作为回应，个人会相应地更新他们的信念和理解。然而，大多数现有的评估都假设语言模型 (LM) 使用一致的信息进行操作。我们引入了 Belief-R，这是一个新的数据集，旨在测试 LM 在面对新证据时的信念修正能力。受人类如何抑制先前推断的启发，这项任务在新提出的 delta 推理 ($\Delta R$) 框架内评估 LM。Belief-R 具有一系列前提，旨在模拟额外信息可能需要 LM 得出先前结论的场景。我们评估了不同提示策略中的 $\sim$30 个 LM，发现 LM 通常难以根据新信息适当地修改其信念。此外，擅长更新的模型在没有必要更新的场景中往往表现不佳，这突显了一个关键的权衡。这些见解强调了提高 LM 对不断变化的信息的适应性的重要性，这是朝着更可靠的 AI 系统迈出的一步。</li>
</ul>

<h3>Title: Title:
          Direct Preference Knowledge Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yixing Li, Yuxian Gu, Li Dong, Dequan Wang, Yu Cheng, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Direct Preference Knowledge Distillation for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the field of large language models (LLMs), Knowledge Distillation (KD) is a critical technique for transferring capabilities from teacher models to student models. However, existing KD methods face limitations and challenges in distillation of LLMs, including efficiency and insufficient measurement capabilities of traditional KL divergence. It is shown that LLMs can serve as an implicit reward function, which we define as a supplement to KL divergence. In this work, we propose Direct Preference Knowledge Distillation (DPKD) for LLMs. DPKD utilizes distribution divergence to represent the preference loss and implicit reward function. We re-formulate KD of LLMs into two stages: first optimizing and objective consisting of implicit reward and reverse KL divergence and then improving the preference probability of teacher outputs over student outputs. We conducted experiments and analysis on various datasets with LLM parameters ranging from 120M to 13B and demonstrate the broad applicability and effectiveness of our DPKD approach. Meanwhile, we prove the value and effectiveness of the introduced implicit reward and output preference in KD through experiments and theoretical analysis. The DPKD method outperforms the baseline method in both output response precision and exact match percentage. Code and data are available at this https URL.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 领域，知识蒸馏 (KD) 是将能力从教师模型转移到学生模型的关键技术。然而，现有的 KD 方法在 LLM 蒸馏方面面临限制和挑战，包括传统 KL 散度的效率和测量能力不足。结果表明，LLM 可以作为隐式奖励函数，我们将其定义为 KL 散度的补充。在这项工作中，我们提出了用于 LLM 的直接偏好知识蒸馏 (DPKD)。DPKD 利用分布散度来表示偏好损失和隐式奖励函数。我们将 LLM 的 KD 重新划分为两个阶段：首先优化由隐式奖励和反向 KL 散度组成的目标，然后提高教师输出相对于学生输出的偏好概率。我们在 LLM 参数从 120M 到 13B 的各种数据集上进行了实验和分析，并证明了我们的 DPKD 方法的广泛适用性和有效性。同时，我们通过实验和理论分析证明了在 KD 中引入隐性奖励和输出偏好的价值和有效性。DPKD 方法在输出响应精度和精确匹配百分比方面均优于基线方法。代码和数据可在此 https URL 上获取。</li>
</ul>

<h3>Title: Title:
          Scalable and Domain-General Abstractive Proposition Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Javad Hosseini, Yang Gao, Tim Baumgärtner, Alex Fabrikant, Reinald Kim Amplayo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Scalable and Domain-General Abstractive Proposition Segmentation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Segmenting text into fine-grained units of meaning is important to a wide range of NLP applications. The default approach of segmenting text into sentences is often insufficient, especially since sentences are usually complex enough to include multiple units of meaning that merit separate treatment in the downstream task. We focus on the task of abstractive proposition segmentation: transforming text into simple, self-contained, well-formed sentences. Several recent works have demonstrated the utility of proposition segmentation with few-shot prompted LLMs for downstream tasks such as retrieval-augmented grounding and fact verification. However, this approach does not scale to large amounts of text and may not always extract all the facts from the input text. In this paper, we first introduce evaluation metrics for the task to measure several dimensions of quality. We then propose a scalable, yet accurate, proposition segmentation model. We model proposition segmentation as a supervised task by training LLMs on existing annotated datasets and show that training yields significantly improved results. We further show that by using the fine-tuned LLMs as teachers for annotating large amounts of multi-domain synthetic distillation data, we can train smaller student models with results similar to the teacher LLMs. We then demonstrate that our technique leads to effective domain generalization, by annotating data in two domains outside the original training data and evaluating on them. Finally, as a key contribution of the paper, we share an easy-to-use API for NLP practitioners to use.</li>
<li><strong>摘要：</strong>将文本分割成细粒度的意义单元对于广泛的 NLP 应用非常重要。将文本分割成句子的默认方法通常是不够的，特别是因为句子通常足够复杂，包含多个意义单元，值得在下游任务中单独处理。我们专注于抽象命题分割的任务：将文本转换为简单、自足、格式良好的句子。最近的几项研究已经证明了使用少样本提示的 LLM 进行命题分割对于下游任务（例如检索增强基础和事实验证）的实用性。然而，这种方法不能扩展到大量文本，并且可能并不总是从输入文本中提取所有事实。在本文中，我们首先引入了该任务的评估指标来衡量质量的几个维度。然后，我们提出了一个可扩展但准确的命题分割模型。我们通过在现有带注释的数据集上训练 LLM，将命题分割建模为监督任务，并表明训练可以显着提高结果。我们进一步表明，通过使用经过微调的 LLM 作为教师模型来注释大量多领域合成蒸馏数据，我们可以训练较小的学生模型，其结果与教师 LLM 相似。然后，我们证明我们的技术可以实现有效的领域泛​​化，方法是注释原始训练数据之外的两个领域的数据并对其进行评估。最后，作为本文的一项重要贡献，我们分享了一个易于使用的 API，供 NLP 从业者使用。</li>
</ul>

<h3>Title: Title:
          BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong reasoning capabilities. Nevertheless, they still suffer from factual errors when tackling knowledge-intensive tasks. Retrieval-augmented reasoning represents a promising approach. However, significant challenges still persist, including inaccurate and insufficient retrieval for complex questions, as well as difficulty in integrating multi-source knowledge. To address this, we propose Beam Aggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive multi-hop QA. BeamAggR explores and prioritizes promising answers at each hop of question. Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning. For atomic questions, the LLM conducts reasoning on multi-source knowledge to get answer candidates. For composite questions, the LLM combines beam candidates, explores multiple reasoning paths through probabilistic aggregation, and prioritizes the most promising trajectory. Extensive experiments on four open-domain multi-hop reasoning datasets show that our method significantly outperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that BeamAggR elicits better knowledge collaboration and answer aggregation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展示出强大的推理能力。然而，它们在处理知识密集型任务时仍然存在事实错误。检索增强推理代表了一种有前途的方法。然而，仍然存在重大挑战，包括复杂问题的检索不准确和不足，以及整合多源知识的困难。为了解决这个问题，我们提出了束聚合推理，BeamAggR，一个用于知识密集型多跳问答的推理框架。BeamAggR 在问题的每一跳上探索并优先考虑有希望的答案。具体来说，我们将复杂问题解析为树，其中包括原子问题和复合问题，然后自下而上进行推理。对于原子问题，LLM 对多源知识进行推理以获得答案候选。对于复合问题，LLM 组合束候选，通过概率聚合探索多条推理路径，并优先考虑最有希望的轨迹。在四个开放域多跳推理数据集上进行的大量实验表明，我们的方法比 SOTA 方法高出 8.5%。此外，我们的分析表明，BeamAggR 可以实现更好的知识协作和答案聚合。</li>
</ul>

<h3>Title: Title:
          AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through low-confidence single-token predictions</h3>
<ul>
<li><strong>Authors: </strong>Waligóra Witold</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through low-confidence single-token predictions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces AnomaLLMy, a novel technique for the automatic detection of anomalous tokens in black-box Large Language Models (LLMs) with API-only access. Utilizing low-confidence single-token predictions as a cost-effective indicator, AnomaLLMy identifies irregularities in model behavior, addressing the issue of anomalous tokens degrading the quality and reliability of models. Validated on the cl100k_base dataset, the token set of GPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the method's efficiency with just \$24.39 spent in API credits. The insights from this research are expected to be beneficial for enhancing the robustness of and accuracy of LLMs, particularly in the development and assessment of tokenizers.</li>
<li><strong>摘要：</strong>本文介绍了一种新技术 AnomaLLMy，用于自动检测黑盒大型语言模型 (LLM) 中的异常标记，且仅通过 API 访问。AnomaLLMy 利用低置信度单标记预测作为经济高效的指标，识别模型行为中的异常，解决异常标记降低模型质量和可靠性的问题。在 cl100k_base 数据集（GPT-4 的标记集）上进行验证后，AnomaLLMy 检测到 413 个主要异常和 65 个次要异常，证明了该方法的有效性，并且仅花费 24.39 美元 API 积分。这项研究的见解有望有助于提高 LLM 的稳健性和准确性，特别是在标记器的开发和评估方面。</li>
</ul>

<h3>Title: Title:
          YuLan: An Open-source Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, Lei Zhang, Junyi Li, Xiaolei Wang, Lei Wang, Beichen Zhang, Zican Dong, Xiaoxue Cheng, Yuhan Chen, Xinyu Tang, Yupeng Hou, Qiangqiang Ren, Xincheng Pang, Shufang Xie, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ze-Feng Gao, Yueguo Chen, Weizheng Lu, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          YuLan: An Open-source Large Language Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become the foundation of many applications, leveraging their extensive capabilities in processing and understanding natural language. While many open-source LLMs have been released with technical reports, the lack of training details hinders further research and development. This paper presents the development of YuLan, a series of open-source LLMs with $12$ billion parameters. The base model of YuLan is pre-trained on approximately $1.7$T tokens derived from a diverse corpus, including massive English, Chinese, and multilingual texts. We design a three-stage pre-training method to enhance YuLan's overall capabilities. Subsequent phases of training incorporate instruction-tuning and human alignment, employing a substantial volume of high-quality synthesized data. To facilitate the learning of complex and long-tail knowledge, we devise a curriculum-learning framework throughout across these stages, which helps LLMs learn knowledge in an easy-to-hard manner. YuLan's training is finished on Jan, 2024 and has achieved performance on par with state-of-the-art LLMs across various English and Chinese benchmarks. This paper outlines a comprehensive technical roadmap for developing LLMs from scratch. Our model and codes are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为许多应用程序的基础，利用其在处理和理解自然语言方面的广泛能力。虽然许多开源 LLM 都已发布技术报告，但缺乏训练细节阻碍了进一步的研究和开发。本文介绍了 YuLan 的开发过程，YuLan 是一系列具有 120 亿美元参数的开源 LLM。YuLan 的基础模型在来自多样化语料库的约 1.7T 个 token 上进行了预训练，包括大量英语、中文和多语言文本。我们设计了一种三阶段预训练方法来增强 YuLan 的整体能力。后续的训练阶段包括指令调整和人工对齐，使用大量高质量的合成数据。为了促进复杂和长尾知识的学习，我们在这些阶段设计了一个课程学习框架，帮助 LLM 以由易到难的方式学习知识。 YuLan 的培训于 2024 年 1 月结束，在各种英语和中文基准上的表现与最先进的 LLM 相当。本文概述了从头开始开发 LLM 的全面技术路线图。我们的模型和代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood</h3>
<ul>
<li><strong>Authors: </strong>Yang Xu, Yu Wang, Hao An, Zhichen Liu, Yongyuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Human and model-generated texts can be distinguished by examining the magnitude of likelihood in language. However, it is becoming increasingly difficult as language model's capabilities of generating human-like texts keep evolving. This study provides a new perspective by using the relative likelihood values instead of absolute ones, and extracting useful features from the spectrum-view of likelihood for the human-model text detection task. We propose a detection procedure with two classification methods, supervised and heuristic-based, respectively, which results in competitive performances with previous zero-shot detection methods and a new state-of-the-art on short-text detection. Our method can also reveal subtle differences between human and model languages, which find theoretical roots in psycholinguistics studies. Our code is available at this https URL</li>
<li><strong>摘要：</strong>通过检查语言中似然度的大小，可以区分人类和模型生成的文本。然而，随着语言模型生成类似人类文本的能力不断发展，这变得越来越困难。这项研究通过使用相对似然值而不是绝对似然值，并从似然谱视图中提取有用的特征，为人类模型文本检测任务提供了一个新的视角。我们提出了一种检测程序，分别采用两种分类方法，监督和基于启发式，其性能与以前的零样本检测方法相媲美，并在短文本检测方面取得了新的进展。我们的方法还可以揭示人类语言和模型语言之间的细微差别，这在心理语言学研究中找到了理论根源。我们的代码可在此 https URL 上找到</li>
</ul>

<h3>Title: Title:
          Investigating the Timescales of Language Processing with EEG and Language Models</h3>
<ul>
<li><strong>Authors: </strong>Davide Turco, Conor Houghton</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Investigating the Timescales of Language Processing with EEG and Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study explores the temporal dynamics of language processing by examining the alignment between word representations from a pre-trained transformer-based language model, and EEG data. Using a Temporal Response Function (TRF) model, we investigate how neural activity corresponds to model representations across different layers, revealing insights into the interaction between artificial language models and brain responses during language comprehension. Our analysis reveals patterns in TRFs from distinct layers, highlighting varying contributions to lexical and compositional processing. Additionally, we used linear discriminant analysis (LDA) to isolate part-of-speech (POS) representations, offering insights into their influence on neural responses and the underlying mechanisms of syntactic processing. These findings underscore EEG's utility for probing language processing dynamics with high temporal resolution. By bridging artificial language models and neural activity, this study advances our understanding of their interaction at fine timescales.</li>
<li><strong>摘要：</strong>本研究通过检查预训练的基于 Transformer 的语言模型中的单词表示与 EEG 数据之间的对齐情况，探索了语言处理的时间动态。使用时间响应函数 (TRF) 模型，我们研究了神经活动如何与不同层的模型表示相对应，从而揭示了语言理解过程中人工语言模型与大脑反应之间相互作用的见解。我们的分析揭示了不同层 TRF 中的模式，突出了对词汇和构图处理的不同贡献。此外，我们使用线性判别分析 (LDA) 来分离词性 (POS) 表示，从而深入了解它们对神经反应的影响以及句法处理的潜在机制。这些发现强调了 EEG 在高时间分辨率下探测语言处理动态方面的实用性。通过将人工语言模型与神经活动联系起来，本研究加深了我们对它们在精细时间尺度上的相互作用的理解。</li>
</ul>

<h3>Title: Title:
          Paraphrase Types Elicit Prompt Engineering Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Paraphrase Types Elicit Prompt Engineering Capabilities(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Much of the success of modern language models depends on finding a suitable prompt to instruct the model. Until now, it has been largely unknown how variations in the linguistic expression of prompts affect these models. This study systematically and empirically evaluates which linguistic features influence models through paraphrase types, i.e., different linguistic changes at particular positions. We measure behavioral changes for five models across 120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon, lexico-syntax, discourse, and others). We also control for other prompt engineering factors (e.g., prompt length, lexical diversity, and proximity to training data). Our results show a potential for language models to improve tasks when their prompts are adapted in specific paraphrase types (e.g., 6.7% median gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in morphology and lexicon, i.e., the vocabulary used, showed promise in improving prompts. These findings contribute to developing more robust language models capable of handling variability in linguistic expression.</li>
<li><strong>摘要：</strong>现代语言模型的成功很大程度上取决于找到合适的提示来指导模型。到目前为止，人们在很大程度上还不知道提示的语言表达变化如何影响这些模型。这项研究系统地、实证地评估了哪些语言特征通过释义类型（即特定位置的不同语言变化）影响模型。我们测量了 120 个任务和 6 个释义系列（即形态、句法、词汇、词汇句法、话语等）中 5 个模型的行为变化。我们还控制了其他提示工程因素（例如提示长度、词汇多样性和与训练数据的接近度）。我们的结果表明，当语言模型的提示适应特定的释义类型时，它们有可能改善任务（例如，Mixtral 8x7B 中的中位数增益为 6.7%；LLaMA 3 8B 中的中位数增益为 5.5%）。具体而言，形态和词汇的变化（即所使用的词汇）有望改善提示。这些发现有助于开发更强大的语言模型，能够处理语言表达的变化。</li>
</ul>

<h3>Title: Title:
          Interactive Topic Models with Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Garima Dhanania, Sheshera Mysore, Chau Minh Pham, Mohit Iyyer, Hamed Zamani, Andrew McCallum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Interactive Topic Models with Optimal Transport(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Topic models are widely used to analyze document collections. While they are valuable for discovering latent topics in a corpus when analysts are unfamiliar with the corpus, analysts also commonly start with an understanding of the content present in a corpus. This may be through categories obtained from an initial pass over the corpus or a desire to analyze the corpus through a predefined set of categories derived from a high level theoretical framework (e.g. political ideology). In these scenarios analysts desire a topic modeling approach which incorporates their understanding of the corpus while supporting various forms of interaction with the model. In this work, we present EdTM, as an approach for label name supervised topic modeling. EdTM models topic modeling as an assignment problem while leveraging LM/LLM based document-topic affinities and using optimal transport for making globally coherent topic-assignments. In experiments, we show the efficacy of our framework compared to few-shot LLM classifiers, and topic models based on clustering and LDA. Further, we show EdTM's ability to incorporate various forms of analyst feedback and while remaining robust to noisy analyst inputs.</li>
<li><strong>摘要：</strong>主题模型被广泛用于分析文档集合。虽然当分析师不熟悉语料库时，它们对于发现语料库中的潜在主题非常有用，但分析师通常也从了解语料库中的内容开始。这可能是通过从初次浏览语料库中获得的类别，或者希望通过从高级理论框架（例如政治意识形态）派生的预定义类别集来分析语料库。在这些情况下，分析师希望采用一种主题建模方法，该方法结合了他们对语料库的理解，同时支持与模型的各种形式的交互。在这项工作中，我们提出了 EdTM，作为一种标签名称监督主题建模的方法。EdTM 将主题建模建模为一个分配问题，同时利用基于 LM/LLM 的文档主题亲和力并使用最佳传输进行全局一致的主题分配。在实验中，我们展示了我们的框架与少样本 LLM 分类器以及基于聚类和 LDA 的主题模型相比的有效性。此外，我们展示了 EdTM 能够整合各种形式的分析师反馈，同时还能抵御嘈杂的分析师输入。</li>
</ul>

<h3>Title: Title:
          From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct $50$k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks. Our code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>我们探索视觉语言模型 (VLM) 中的多步骤推理。这个问题很有挑战性，因为由视觉和语言处理多个步骤组成的推理数据几乎不可用。为了克服这一挑战，我们首先引入了一种从最少到最多的视觉推理范式，该范式交错将问题分解为子问题并调用外部工具来解决子问题。基于该范式，我们进一步提出了一种新颖的数据合成方法，可以自下而上地自动为图像创建问题和多步骤推理路径。我们的方法将复杂的合成任务划分为几个简单的子任务，并且（几乎完全）依赖开源模型来完成子任务。因此，整个合成过程是可重复且经济高效的，并且合成数据的质量有保证。通过该方法，我们构建了 50$k 个视觉推理示例。然后，我们通过监督微调开发了一个视觉推理器，它能够以即插即用的方式普遍增强各种现有 VLM 的推理能力。大量实验表明，视觉推理器可以在四个 VQA 基准上持续显著地改进四个 VLM。我们的代码和数据集可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring</h3>
<ul>
<li><strong>Authors: </strong>Jiazheng Li, Hainiu Xu, Zhaoyue Sun, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Generating rationales that justify scoring decisions has been a promising way to facilitate explainability in automated scoring systems. However, existing methods do not match the accuracy of classifier-based methods. Plus, the generated rationales often contain hallucinated information. To address these issues, we propose a novel framework capable of generating more faithful rationales and, more importantly, matching performance with classifier-based black-box scoring systems. We first mimic the human assessment process by querying Large Language Models (LLMs) to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data. Finally, we utilise the generated synthetic data to calibrate LLMs through a two-step training process: supervised fine-tuning and preference optimization. Extensive experimental results demonstrate that our framework achieves a 38% assessment performance improvement in the QWK score compared to prior work while producing higher-quality rationales, as recognised by human evaluators and LLMs. Our work sheds light on the effectiveness of performing preference optimization using synthetic preference data obtained from thought tree paths.</li>
<li><strong>摘要：</strong>生成合理性理由来证明评分决策是促进自动评分系统可解释性的一种有前途的方法。然而，现有方法的准确性无法与基于分类器的方法相媲美。此外，生成的合理性理由通常包含幻觉信息。为了解决这些问题，我们提出了一个新颖的框架，该框架能够生成更忠实的合理性理由，更重要的是，能够与基于分类器的黑盒评分系统的性能相匹配。我们首先通过查询大型语言模型 (LLM) 来模拟人类评估过程以生成思维树。然后，我们从每个思维树路径中总结中间评估决策，以创建合成合理性数据和合理性偏好数据。最后，我们利用生成的合成数据通过两步训练过程校准 LLM：监督微调和偏好优化。大量实验结果表明，与之前的工作相比，我们的框架在 QWK 分数的评估性能提高了 38%，同时产生了更高质量的合理性理由，得到了人类评估者和 LLM 的认可。我们的工作阐明了使用从思维树路径获得的合成偏好数据执行偏好优化的有效性。</li>
</ul>

<h3>Title: Title:
          Mining Reasons For And Against Vaccination From Unstructured Data Using Nichesourcing and AI Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Damián Ariel Furman, Juan Junqueras, Z. Burçe Gümüslü, Edgar Altszyler, Joaquin Navajas, Ophelia Deroy, Justin Sulik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mining Reasons For And Against Vaccination From Unstructured Data Using Nichesourcing and AI Data Augmentation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>We present Reasons For and Against Vaccination (RFAV), a dataset for predicting reasons for and against vaccination, and scientific authorities used to justify them, annotated through nichesourcing and augmented using GPT4 and GPT3.5-Turbo. We show how it is possible to mine these reasons in non-structured text, under different task definitions, despite the high level of subjectivity involved and explore the impact of artificially augmented data using in-context learning with GPT4 and GPT3.5-Turbo. We publish the dataset and the trained models along with the annotation manual used to train annotators and define the task.</li>
<li><strong>摘要：</strong>我们介绍了“接种疫苗的理由和反对理由”（RFAV），这是一个用于预测接种疫苗的理由和反对理由的数据集，以及用于证明这些理由的科学权威，这些理由通过利基来源进行注释，并使用 GPT4 和 GPT3.5-Turbo 进行了增强。我们展示了如何在不同的任务定义下在非结构化文本中挖掘这些理由，尽管这涉及高度的主观性，并探索了使用 GPT4 和 GPT3.5-Turbo 进行上下文学习对人工增强数据的影响。我们发布了数据集和经过训练的模型，以及用于训练注释者和定义任务的注释手册。</li>
</ul>

<h3>Title: Title:
          BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5</h3>
<ul>
<li><strong>Authors: </strong>Zhehuai Chen, He Huang, Oleksii Hrinchuk, Krishna C. Puvvada, Nithin Rao Koluguri, Piotr Żelasko, Jagadeesh Balam, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Incorporating speech understanding capabilities into pretrained large-language models has become a vital research direction (SpeechLLM). The previous architectures can be categorized as: i) GPT-style, prepend speech prompts to the text prompts as a sequence of LLM inputs like a decoder-only model; ii) T5-style, introduce speech cross-attention to each layer of the pretrained LLMs. We propose BESTOW architecture to bring the BESt features from TwO Worlds into a single model that is highly efficient and has strong multitask capabilities. Moreover, there is no clear streaming solution for either style, especially considering the solution should generalize to speech multitask. We reformulate streamable SpeechLLM as a read-write policy problem and unifies the offline and streaming research with BESTOW architecture. Hence we demonstrate the first open-source SpeechLLM solution that enables Streaming and Multitask at scale (beyond ASR) at the same time. This streamable solution achieves very strong performance on a wide range of speech tasks (ASR, AST, SQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower training/inference cost, and demonstrates LLM knowledge transferability to speech.</li>
<li><strong>摘要：</strong>将语音理解功能融入预训练的大型语言模型已成为一个重要的研究方向（SpeechLLM）。先前的架构可分为：i）GPT 风格，将语音提示作为 LLM 输入序列添加到文本提示中，就像仅解码器模型一样；ii）T5 风格，将语音交叉注意力引入预训练 LLM 的每一层。我们提出了 BESTOW 架构，将 TwO Worlds 中的 BESt 特征带入一个高效且具有强大多任务能力的单一模型中。此外，这两种风格都没有明确的流式传输解决方案，尤其是考虑到该解决方案应该推广到语音多任务。我们将可流式传输的 SpeechLLM 重新表述为读写策略问题，并将离线和流式传输研究与 BESTOW 架构统一起来。因此，我们展示了第一个开源 SpeechLLM 解决方案，可同时实现大规模（超越 ASR）的流式传输和多任务处理。这种可流式传输的解决方案在各种语音任务（ASR、AST、SQA、未见过的 DynamicSuperb）上实现了非常强大的性能。它具有端到端优化功能，训练/推理成本较低，并展示了 LLM 知识向语音的可迁移性。</li>
</ul>

<h3>Title: Title:
          Simulating Financial Market via Large Language Model based Agents</h3>
<ul>
<li><strong>Authors: </strong>Shen Gao, Yuntao Wen, Minghang Zhu, Jianing Wei, Yuhan Cheng, Qunzi Zhang, Shuo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Simulating Financial Market via Large Language Model based Agents(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Most economic theories typically assume that financial market participants are fully rational individuals and use mathematical models to simulate human behavior in financial markets. However, human behavior is often not entirely rational and is challenging to predict accurately with mathematical models. In this paper, we propose \textbf{A}gent-based \textbf{S}imulated \textbf{F}inancial \textbf{M}arket (ASFM), which first constructs a simulated stock market with a real order matching system. Then, we propose a large language model based agent as the stock trader, which contains the profile, observation, and tool-learning based action module. The trading agent can comprehensively understand current market dynamics and financial policy information, and make decisions that align with their trading strategy. In the experiments, we first verify that the reactions of our ASFM are consistent with the real stock market in two controllable scenarios. In addition, we also conduct experiments in two popular economics research directions, and we find that conclusions drawn in our \model align with the preliminary findings in economics research. Based on these observations, we believe our proposed ASFM provides a new paradigm for economic research.</li>
<li><strong>摘要：</strong>大多数经济理论通常假设金融市场参与者是完全理性的个体，并使用数学模型来模拟人类在金融市场中的行为。然而，人类行为往往不是完全理性的，很难用数学模型准确预测。在本文中，我们提出了基于代理的模拟金融市场（ASFM），首先构建一个具有真实订单匹配系统的模拟股票市场。然后，我们提出了一个基于大型语言模型的代理作为股票交易员，其中包含配置文件、观察和基于工具学习的动作模块。交易代理可以全面了解当前的市场动态和金融政策信息，并做出符合其交易策略的决策。在实验中，我们首先验证了我们的 ASFM 在两个可控场景中的反应与真实股票市场一致。此外，我们还在两个流行的经济学研究方向进行了实验，我们发现我们的模型得出的结论与经济学研究的初步发现一致。基于这些观察，我们相信我们提出的 ASFM 为经济研究提供了一个新的范式。</li>
</ul>

<h3>Title: Title:
          Into the Unknown: Generating Geospatial Descriptions for New Environments</h3>
<ul>
<li><strong>Authors: </strong>Tzuf Paz-Argaman, John Palowitch, Sayali Kulkarni, Reut Tsarfaty, Jason Baldridge</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Into the Unknown: Generating Geospatial Descriptions for New Environments(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.</li>
<li><strong>摘要：</strong>与专注于弥合视觉与语言之间差距以实现具身导航的视觉和语言导航 (VLN) 任务类似，新的 Rendezvous (RVS) 任务需要使用非连续导航指令和地图对非中心空间关系（独立于观察者的视点）进行推理。然而，在没有训练数据的新环境中，性能会大幅下降。使用与坐标配对的开源描述（例如 Wikipedia）可以提供训练数据，但空间导向文本有限，导致地理位置分辨率低。我们提出了一种大规模增强方法，使用现成的地理空间数据为新环境生成高质量的合成数据。我们的方法构建了一个扎实的知识图谱，捕捉实体关系。采样的实体和关系（“学校北边的商店”）通过 (i) 使用上下文无关语法 (CFG) 生成大量模板以嵌入特定实体和关系；(ii) 将实体和关系输入大型语言模型 (LLM) 以生成指令来生成导航指令。对 RVS 的全面评估表明，我们的方法在未见过的环境中将 100 米精度提高了 45.83%。此外，我们证明，无论是在未见过的环境中，使用基于 CFG 的增强训练的模型都比使用基于 LLM 的增强训练的模型取得了更好的性能。这些发现表明，在以前未知的环境中，为基于文本的地理空间推理明确构建空间信息的潜在优势可以解锁数据稀缺的场景。</li>
</ul>

<h3>Title: Title:
          Single Parent Family: A Spectrum of Family Members from a Single Pre-Trained Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Habib Hajimolahoseini, Mohammad Hassanpour, Foozhan Ataiefard, Boxing Chen, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Single Parent Family: A Spectrum of Family Members from a Single Pre-Trained Foundation Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel method of Progressive Low Rank Decomposition (PLRD) tailored for the compression of large language models. Our approach leverages a pre-trained model, which is then incrementally decompressed to smaller sizes using progressively lower ranks. This method allows for significant reductions in computational overhead and energy consumption, as subsequent models are derived from the original without the need for retraining from scratch. We detail the implementation of PLRD, which strategically decreases the tensor ranks, thus optimizing the trade-off between model performance and resource usage. The efficacy of PLRD is demonstrated through extensive experiments showing that models trained with PLRD method on only 1B tokens maintain comparable performance with traditionally trained models while using 0.1% of the tokens. The versatility of PLRD is highlighted by its ability to generate multiple model sizes from a single foundational model, adapting fluidly to varying computational and memory budgets. Our findings suggest that PLRD could set a new standard for the efficient scaling of LLMs, making advanced AI more feasible on diverse platforms.</li>
<li><strong>摘要：</strong>本文介绍了一种专为大型语言模型压缩而量身定制的渐进低秩分解 (PLRD) 新方法。我们的方法利用预先训练的模型，然后使用逐渐降低的秩逐步将其解压缩为较小的尺寸。这种方法可以显著减少计算开销和能耗，因为后续模型是从原始模型派生而来的，而无需从头开始重新训练。我们详细介绍了 PLRD 的实现，它策略性地降低了张量秩，从而优化了模型性能和资源使用之间的权衡。通过大量实验证明了 PLRD 的有效性，结果表明，仅使用 1B 个 token 用 PLRD 方法训练的模型在使用 0.1% 的 token 的情况下保持与传统训练模型相当的性能。PLRD 的多功能性突出表现在其能够从单个基础模型生成多种模型大小，灵活适应不同的计算和内存预算。我们的研究结果表明，PLRD 可以为 LLM 的高效扩展树立新标准，使高级 AI 在不同平台上更加可行。</li>
</ul>

<h3>Title: Title:
          The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Chen, Baohao Liao, Jirui Qi, Panagiotis Eustratiadis, Christof Monz, Arianna Bisazza, Maarten de Rijke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Following multiple instructions is a crucial ability for large language models (LLMs). Evaluating this ability comes with significant challenges: (i) limited coherence between multiple instructions, (ii) positional bias where the order of instructions affects model performance, and (iii) a lack of objectively verifiable tasks. To address these issues, we introduce a benchmark designed to evaluate models' abilities to follow multiple instructions through sequential instruction following (SIFo) tasks. In SIFo, the successful completion of multiple instructions is verifiable by examining only the final instruction. Our benchmark evaluates instruction following using four tasks (text modification, question answering, mathematics, and security rule following), each assessing different aspects of sequential instruction following. Our evaluation of popular LLMs, both closed-source and open-source, shows that more recent and larger models significantly outperform their older and smaller counterparts on the SIFo tasks, validating the benchmark's effectiveness. All models struggle with following sequences of instructions, hinting at an important lack of robustness of today's language models.</li>
<li><strong>摘要：</strong>遵循多条指令是大型语言模型 (LLM) 的一项关键能力。评估这种能力面临着重大挑战：(i) 多条指令之间的连贯性有限，(ii) 指令顺序影响模型性能的位置偏差，以及 (iii) 缺乏客观可验证的任务。为了解决这些问题，我们引入了一个基准，旨在通过顺序指令遵循 (SIFo) 任务评估模型遵循多条指令的能力。在 SIFo 中，只需检查最后一条指令即可验证多条指令是否成功完成。我们的基准使用四个任务（文本修改、问答、数学和安全规则遵循）来评估指令遵循，每个任务评估顺序指令遵循的不同方面。我们对流行的 LLM（包括闭源和开源）的评估表明，较新和较大的模型在 SIFo 任务上的表现明显优于较旧和较小的模型，从而验证了基准的有效性。所有模型都难以遵循指令序列，这表明当今的语言模型存在严重的鲁棒性不足。</li>
</ul>

<h3>Title: Title:
          ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, Hayato Yamana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community still needs to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM's hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a total score of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play a crucial role in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.</li>
<li><strong>摘要：</strong>工具增强的大型语言模型 (LLM) 正在迅速融入到实际应用中。由于缺乏基准测试，社区仍然需要充分了解这些模型中的幻觉问题。为了应对这一挑战，我们引入了一个全面的诊断基准 ToolBH。具体来说，我们从两个角度评估 LLM 的幻觉：深度和广度。在深度方面，我们提出了一个多层次的诊断过程，包括 (1) 可解性检测、(2) 解决方案规划和 (3) 缺失工具分析。对于广度，我们根据工具集的特点考虑了三种情况：缺少必要的工具、潜在的工具和功能有限的工具。此外，我们开发了七个任务，并通过多轮手动注释收集了 700 个评估样本。结果显示了 ToolBH 基准测试带来的重大挑战。当前先进的模型 Gemini-1.5-Pro 和 GPT-4o 在满分为 100 的评分标准下，总分分别仅为 45.3 和 37.0。在这个基准测试中，更大的模型参数并不能保证更好的性能；训练数据和响应策略在工具增强的 LLM 场景中也起着至关重要的作用。我们的诊断分析表明，模型错误的主要原因在于评估任务的可解性。此外，开放权重模型在冗长的回复中性能会下降，而专有模型在较长的推理中表现出色。</li>
</ul>

<h3>Title: Title:
          LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Renzhi Wang, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) require continual knowledge updates to stay abreast of the ever-changing world facts, prompting the formulation of lifelong model editing task. While recent years have witnessed the development of various techniques for single and batch editing, these methods either fail to apply or perform sub-optimally when faced with lifelong editing. In this paper, we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong model editing. We first analyze the factors influencing the effectiveness of conventional MoE adaptor in lifelong editing, including catastrophic forgetting, inconsistent routing and order sensitivity. Based on these insights, we propose a tailored module insertion method to achieve lifelong editing, incorporating a novel KV anchor routing to enhance routing consistency between training and inference stage, along with a concise yet effective clustering-based editing order planning. Experimental results demonstrate the effectiveness of our method in lifelong editing, surpassing previous model editing techniques while maintaining outstanding performance in batch editing task. Our code will be available.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 需要不断更新知识以跟上不断变化的世界事实，从而促使制定终身模型编辑任务。虽然近年来见证了各种单次和批量编辑技术的发展，但这些方法在面对终身编辑时要么无法应用，要么表现不佳。在本文中，我们介绍了 LEMoE，一种用于终身模型编辑的高级混合专家 (MoE) 适配器。我们首先分析影响传统 MoE 适配器在终身编辑中有效性的因素，包括灾难性遗忘、不一致的路由和顺序敏感性。基于这些见解，我们提出了一种定制的模块插入方法来实现终身编辑，结合了一种新颖的 KV 锚路由来增强训练和推理阶段之间的路由一致性，以及一种简洁而有效的基于聚类的编辑顺序规划。实验结果证明了我们的方法在终身编辑中的有效性，超越了以前的模型编辑技术，同时在批量编辑任务中保持了出色的性能。我们的代码将可用。</li>
</ul>

<h3>Title: Title:
          BioMNER: A Dataset for Biomedical Method Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Chen Tang, Bohao Yang, Kun Zhao, Bo Lv, Chenghao Xiao, Frank Guerin, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BioMNER: A Dataset for Biomedical Method Entity Recognition(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Named entity recognition (NER) stands as a fundamental and pivotal task within the realm of Natural Language Processing. Particularly within the domain of Biomedical Method NER, this task presents notable challenges, stemming from the continual influx of domain-specific terminologies in scholarly literature. Current research in Biomedical Method (BioMethod) NER suffers from a scarcity of resources, primarily attributed to the intricate nature of methodological concepts, which necessitate a profound understanding for precise delineation. In this study, we propose a novel dataset for biomedical method entity recognition, employing an automated BioMethod entity recognition and information retrieval system to assist human annotation. Furthermore, we comprehensively explore a range of conventional and contemporary open-domain NER methodologies, including the utilization of cutting-edge large-scale language models (LLMs) customised to our dataset. Our empirical findings reveal that the large parameter counts of language models surprisingly inhibit the effective assimilation of entity extraction patterns pertaining to biomedical methods. Remarkably, the approach, leveraging the modestly sized ALBERT model (only 11MB), in conjunction with conditional random fields (CRF), achieves state-of-the-art (SOTA) performance.</li>
<li><strong>摘要：</strong>命名实体识别 (NER) 是自然语言处理领域中一项基本且关键的任务。特别是在生物医学方法 NER 领域，这项任务带来了显著的挑战，因为学术文献中不断涌入领域特定术语。生物医学方法 (BioMethod) NER 的当前研究面临资源匮乏的问题，这主要归因于方法概念的复杂性，需要深刻理解才能准确描述。在本研究中，我们提出了一种用于生物医学方法实体识别的新数据集，采用自动化 BioMethod 实体识别和信息检索系统来协助人工注释。此外，我们全面探索了一系列传统和当代开放域 NER 方法，包括利用针对我们的数据集定制的尖端大规模语言模型 (LLM)。我们的实证结果表明，语言模型的大量参数数量令人惊讶地抑制了与生物医学方法相关的实体提取模式的有效吸收。值得注意的是，该方法利用适度大小的 ALBERT 模型（仅 11MB）结合条件随机场 (CRF) 实现了最先进 (SOTA) 的性能。</li>
</ul>

<h3>Title: Title:
          Understanding and Mitigating Language Confusion in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo Dehaze, Sebastian Ruder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Understanding and Mitigating Language Confusion in LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user's desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation at this https URL.</li>
<li><strong>摘要：</strong>我们研究了 LLM 的一个令人惊讶的局限性：它们无法以用户所需的语言一致地生成文本。我们创建了语言混淆基准 (LCB) 来评估此类故障，涵盖 15 种类型多样的语言，包括现有和新创建的英语和多语言提示。我们评估了一系列反映实际用例的单语和跨语言生成的 LLM，发现 Llama Instruct 和 Mistral 模型表现出高度的语言混淆，即使是最强大的模型也无法以正确的语言始终如一地做出响应。我们观察到，基础和以英语为中心的 instruct 模型更容易出现语言混淆，而复杂的提示和高采样温度会加剧这种情况。我们发现，可以通过少量提示、多语言 SFT 和偏好调整来部分缓解语言混淆。我们在此 https URL 上发布了我们的语言混淆基准，它是高效、可扩展的多语言评估的第一层。</li>
</ul>

<h3>Title: Title:
          To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bastien Liétard, Pascal Denis, Mikaella Keller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Polysemy and synonymy are two crucial interrelated facets of lexical ambiguity. While both phenomena have been studied extensively in NLP, leading to dedicated systems, they are often been considered independently. While many tasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction) highlight the role of a word's senses, the study of synonymy is rooted in the study of concepts, i.e. meaning shared across the lexicon. In this paper, we introduce Concept Induction, the unsupervised task of learning a soft clustering among words that defines a set of concepts directly from data. This task generalizes that of Word Sense Induction. We propose a bi-level approach to Concept Induction that leverages both a local lemma-centric view and a global cross-lexicon perspective to induce concepts. We evaluate the obtained clustering on SemCor's annotated data and obtain good performances (BCubed F1 above 0.60). We find that the local and the global levels are mutually beneficial to induce concepts and also senses in our setting. Finally, we create static embeddings representing our induced concepts and use them on the Word-in-Context task, obtaining competitive performances with the State-of-the-Art.</li>
<li><strong>摘要：</strong>多义性和同义词是词汇歧义的两个相互关联的关键方面。虽然这两种现象在 NLP 中都得到了广泛的研究，并产生了专门的系统，但它们通常被独立考虑。虽然许多处理多义性的任务（例如词义消歧或归纳）强调了词义的作用，但同义词的研究植根于概念的研究，即词典中共享的意义。在本文中，我们介绍了概念归纳，这是一种无监督任务，即学习单词之间的软聚类，直接从数据中定义一组概念。此任务概括了词义归纳的任务。我们提出了一种双层概念归纳方法，该方法利用局部词义中心视图和全局跨词典视角来归纳概念。我们在 SemCor 的注释数据上评估了获得的聚类，并获得了良好的性能（BCubed F1 高于 0.60）。我们发现，局部和全局层面对于在我们的环境中诱导概念和感觉是互利的。最后，我们创建代表我们诱导概念的静态嵌入，并将其用于 Word-in-Context 任务，从而获得与最先进技术相媲美的性能。</li>
</ul>

<h3>Title: Title:
          Applying RLAIF for Code Generation with API-usage in Lightweight LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sujan Dutta, Sayantan Mahinder, Raviteja Anantha, Bortik Bandyopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Applying RLAIF for Code Generation with API-usage in Lightweight LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant potential across various domains, including mitigating harm in LLM outputs, enhancing text summarization, and mathematical reasoning. This paper introduces an RLAIF framework for improving the code generation abilities of lightweight (<1B parameters) LLMs. We specifically focus on code generation tasks that require writing appropriate API calls, which is challenging due to the well-known issue of hallucination in LLMs. Our framework extracts AI feedback from a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and uses this data to train a reward model towards better alignment from smaller LLMs. We run our experiments on the Gorilla dataset and meticulously assess the quality of the model-generated code across various metrics, including AST, ROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate accurately. Our approach significantly enhances the fine-tuned LLM baseline's performance, achieving a 4.5% improvement in executability rate. Notably, a smaller LLM model (780M parameters) trained with RLAIF surpasses a much larger fine-tuned baseline with 7B parameters, achieving a 1.0% higher code executability rate.</li>
<li><strong>摘要：</strong>基于 AI 反馈的强化学习 (RLAIF) 已在各个领域展现出巨大潜力，包括减轻 LLM 输出中的危害、增强文本摘要和数学推理。本文介绍了一种 RLAIF 框架，用于提高轻量级（<1B 个参数）LLM 的代码生成能力。我们特别关注需要编写适当 API 调用的代码生成任务，这很有挑战性，因为 LLM 中存在众所周知的幻觉问题。我们的框架通过专门的提示策略从较大的 LLM（例如 GPT-3.5）中提取 AI 反馈，并使用这些数据训练奖励模型，以实现与较小 LLM 的更好对齐。我们在 Gorilla 数据集上运行实验，并根据各种指标（包括 AST、ROUGE 和 Code-BLEU）仔细评估模型生成的代码的质量，并开发一个管道来准确计算其可执行率。我们的方法显著提高了微调后的 LLM 基线的性能，可执行率提高了 4.5%。值得注意的是，使用 RLAIF 训练的较小的 LLM 模型（780M 个参数）超越了具有 7B 参数的更大的微调基线，实现了 1.0% 更高的代码可执行率。</li>
</ul>

<h3>Title: Title:
          Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification</h3>
<ul>
<li><strong>Authors: </strong>Anisha Gunjal, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Automatic factuality verification of large language model (LLM) generations is becoming more and more widely used to combat hallucinations. A major point of tension in the literature is the granularity of this fact-checking: larger chunks of text are hard to fact-check, but more atomic facts like propositions may lack context to interpret correctly. In this work, we assess the role of context in these atomic facts. We argue that fully atomic facts are not the right representation, and define two criteria for molecular facts: decontextuality, or how well they can stand alone, and minimality, or how little extra information is added to achieve decontexuality. We quantify the impact of decontextualization on minimality, then present a baseline methodology for generating molecular facts automatically, aiming to add the right amount of information. We compare against various methods of decontextualization and find that molecular facts balance minimality with fact verification accuracy in ambiguous settings.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 生成的自动事实性验证正越来越广泛地用于对抗幻觉。文献中一个主要的争论点是这种事实核查的粒度：较大的文本块很难进行事实核查，但像命题这样的更原子的事实可能缺乏正确解释的上下文。在这项工作中，我们评估了上下文在这些原子事实中的作用。我们认为完全原子的事实并不是正确的表示，并为分子事实定义了两个标准：去语境性，即它们独立存在的程度，以及最小性，即为实现去语境性而添加的额外信息的数量。我们量化了去语境化对最小性的影响，然后提出了一种自动生成分子事实的基线方法，旨在添加适量的信息。我们与各种去语境化方法进行了比较，发现分子事实在模糊设置中平衡了最小性和事实验证准确性。</li>
</ul>

<h3>Title: Title:
          Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sheridan Feucht, David Atkinson, Byron Wallace, David Bau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>LLMs process text as sequences of tokens that roughly correspond to words, where less common words are represented by multiple tokens. However, individual tokens are often semantically unrelated to the meanings of the words/concepts they comprise. For example, Llama-2-7b's tokenizer splits the word "northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of which correspond to semantically meaningful units like "north" or "east." Similarly, the overall meanings of named entities like "Neil Young" and multi-word expressions like "break a leg" cannot be directly inferred from their constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups of tokens into useful higher-level representations? In this work, we find that last token representations of named entities and multi-token words exhibit a pronounced "erasure" effect, where information about previous and current tokens is rapidly forgotten in early layers. Using this observation, we propose a method to "read out" the implicit vocabulary of an autoregressive LLM by examining differences in token representations across layers, and present results of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is the first attempt to probe the implicit vocabulary of an LLM.</li>
<li><strong>摘要：</strong>LLM 将文本处理为大致与单词相对应的标记序列，其中不太常见的单词由多个标记表示。但是，单个标记通常在语义上与它们组成的单词/概念的含义无关。例如，Llama-2-7b 的标记器将单词“northeastern”拆分为标记 ['_n', 'ort', 'he', 'astern']，其中没有一个对应于语义上有意义的单位，如“north”或“east”。同样，命名实体（如“Neil Young”）和多词表达（如“break a leg”）的整体含义不能直接从其组成标记中推断出来。从机制上讲，LLM 如何将这些任意的标记组转换为有用的高级表示？在这项工作中，我们发现命名实体和多标记单词的最后标记表示表现出明显的“擦除”效果，其中有关先前和当前标记的信息在早期层中被迅速遗忘。利用这一观察，我们提出了一种通过检查各层标记表示的差异来“读出”自回归 LLM 的隐性词汇的方法，并展示了该方法对 Llama-2-7b 和 Llama-3-8B 的结果。据我们所知，这是首次尝试探究 LLM 的隐性词汇。</li>
</ul>

<h3>Title: Title:
          Scaling Synthetic Data Creation with 1,000,000,000 Personas</h3>
<ul>
<li><strong>Authors: </strong>Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Scaling Synthetic Data Creation with 1,000,000,000 Personas(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的角色驱动数据合成方法，该方法利用大型语言模型 (LLM) 中的各种视角来创建多样化的合成数据。为了充分利用这种方法，我们引入了 Persona Hub——一个从网络数据中自动整理出来的 10 亿个不同角色的集合。这 10 亿角色（约占世界总人口的 13%）作为世界知识的分布式载体，可以利用 LLM 中封装的几乎所有视角，从而促进为各种场景大规模创建多样化的合成数据。通过展示 Persona Hub 在大规模合成高质量数学和逻辑推理问题、指令（即用户提示）、知识丰富的文本、游戏 NPC 和工具（功能）方面的用例，我们证明了角色驱动的数据合成是多功能、可扩展、灵活且易于使用的，可能会推动合成数据创建和实践应用的范式转变，这可能会对 LLM 的研究和开发产生深远影响。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
