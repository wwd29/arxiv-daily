<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-19</h1>
<h3>Title: The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts</h3>
<ul>
<li><strong>Authors: </strong>Warren Johnson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15843">https://arxiv.org/abs/2602.15843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15843">https://arxiv.org/pdf/2602.15843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15843]] The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts(https://arxiv.org/abs/2602.15843)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In "Compress or Route?" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the "perplexity paradox" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a "perplexity paradox": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.</li>
<li><strong>摘要：</strong>在“压缩还是路由？”中(Johnson, 2026)，我们发现代码生成可以容忍激进的提示压缩 (r >= 0.6)，而思想链推理会逐渐退化。该研究仅限于 HumanEval（164 个问题），未验证“困惑悖论”机制，并且没有提供自适应算法。本文解决了所有三个差距。首先，我们验证了六个代码基准（HumanEval、MBPP、HumanEval+、MultiPL-E）和四个推理基准（GSM8K、MATH、ARC-Challenge、MMLU-STEM），确认压缩阈值可以跨语言和困难进行推广。其次，我们进行了第一次每个标记的困惑度分析（n=723 个标记），揭示了一个“困惑度悖论”：代码语法标记被保留（高困惑度），而数学问题中的数值尽管是任务关键型的（低困惑度）却被修剪。签名注入将通过率恢复了 +34 个百分点（5.3% 至 39.3%；Cohen's h=0.890）。第三，我们提出TAAC（任务感知自适应压缩），实现了22%的成本降低和96%的质量保留，比固定比率压缩高7%。 MBPP 验证（n=1,800 项试验）证实了系统变异：r=0.3 时为 3.6%，r=1.0 时为 54.6%。</li>
</ul>

<h3>Title: Language Model Representations for Efficient Few-Shot Tabular Classification</h3>
<ul>
<li><strong>Authors: </strong>Inwon Kang, Parikshit Ram, Yi Zhou, Horst Samulowitz, Oshani Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15844">https://arxiv.org/abs/2602.15844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15844">https://arxiv.org/pdf/2602.15844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15844]] Language Model Representations for Efficient Few-Shot Tabular Classification(https://arxiv.org/abs/2602.15844)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\textbf{Ta}$ble $\textbf{R}$epresentation with $\textbf{L}$anguage Model~($\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.</li>
<li><strong>摘要：</strong>网络是表格形式的结构化数据的丰富来源，从产品目录和知识库到科学数据集。然而，这些表的结构和语义的异构性使得构建能够有效利用它们所包含的信息的统一方法具有挑战性。与此同时，大型语言模型 (LLM) 正在成为语义搜索等任务的网络基础设施中日益不可或缺的组成部分。这就提出了一个关键问题：我们能否利用这些已经部署的法学硕士对网络原生表中的结构化数据（例如产品目录、知识库导出、科学数据门户）进行分类，从而避免对专门模型或广泛再培训的需要？这项工作研究了一种轻量级范例，即 $\textbf{Ta}$ble $\textbf{R}$ 表示与 $\textbf{L}$anguage Model~($\textbf{TaRL}$)，用于直接利用各个表行的语义嵌入的少量表格分类。我们首先表明，与专门的表格模型相比，这些嵌入的简单应用表现不佳。然后，我们证明可以通过两项关键技术来释放它们的潜力：从所有嵌入中删除公共组件并校准 softmax 温度。我们证明，一个简单的元学习器经过手工特征的训练，可以学习预测合适的温度。这种方法在语义丰富的表的低数据机制 ($k \leq 32$) 中实现了与最先进模型相当的性能。我们的研究结果证明了重用现有的 LLM 基础设施以实现有效的语义驱动途径的可行性，以重用现有的 LLM 基础设施来理解 Web 表。</li>
</ul>

<h3>Title: KD4MT: A Survey of Knowledge Distillation for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Ona de Gibert, Joseph Attieh, Timothee Mickus, Yves Scherrer, Jörg Tiedemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15845">https://arxiv.org/abs/2602.15845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15845">https://arxiv.org/pdf/2602.15845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15845]] KD4MT: A Survey of Knowledge Distillation for Machine Translation(https://arxiv.org/abs/2602.15845)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this narrative: in MT, KD also functions as a general-purpose knowledge transfer mechanism that shapes supervision and translation quality as well as efficiency. This survey synthesizes KD for MT (KD4MT) across 105 papers (through October 1, 2025). We begin by introducing both MT and KD for non-experts, followed by an overview of the standard KD approaches relevant to MT applications. Subsequently, we categorize advances in the KD4MT literature based on (i) their methodological contributions and (ii) their practical applications. Our qualitative and quantitative analyses identify common trends in the field and highlight key research gaps as well as the absence of unified evaluation practice for KD methods in MT. We further provide practical guidelines for selecting a KD method in concrete settings and highlight potential risks associated with the application of KD to MT such as increased hallucination and bias amplification. Finally, we discuss the role of LLMs in re-shaping the KD4MT field. To support further research, we complement our survey with a publicly available database summarizing the main characteristics of the surveyed KD methods and a glossary of key terms.</li>
<li><strong>摘要：</strong>近年来，知识蒸馏（KD）作为一个研究领域作为一种压缩工具获得了很大的关注，以解决与 NLP 中越来越大的模型相关的挑战。值得注意的是，机器翻译 (MT) 对这一叙述提供了更加细致入微的解读：在 MT 中，KD 还充当通用知识转移机制，塑造监督和翻译质量以及效率。该调查综合了 105 篇论文中的 KD for MT (KD4MT)（截至 2025 年 10 月 1 日）。我们首先为非专家介绍 MT 和 KD，然后概述与 MT 应用相关的标准 KD 方法。随后，我们根据 (i) 方法论贡献和 (ii) 实际应用对 KD4MT 文献中的进展进行分类。我们的定性和定量分析确定了该领域的共同趋势，并强调了关键的研究差距以及机器翻译中 KD 方法缺乏统一评估实践的情况。我们进一步提供了在具体环境中选择 KD 方法的实用指南，并强调了将 KD 应用于 MT 相关的潜在风险，例如幻觉增加和偏差放大。最后，我们讨论了法学硕士在重塑 KD4MT 领域中的作用。为了支持进一步的研究，我们用一个公开的数据库来补充我们的调查，该数据库总结了所调查的 KD 方法的主要特征和关键术语词汇表。</li>
</ul>

<h3>Title: Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Gao, Shaonan Wang, Nai Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15846">https://arxiv.org/abs/2602.15846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15846">https://arxiv.org/pdf/2602.15846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15846]] Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs(https://arxiv.org/abs/2602.15846)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Decoder-only large language models achieve strong broad performance but are brittle to minor grammatical perturbations, undermining reliability for downstream reasoning. However, directly injecting explicit syntactic structure into an existing checkpoint can interfere with its pretrained competence. We introduce a checkpoint-compatible gated tree cross-attention (GTCA) branch that reads precomputed constituency chunk memory while leaving backbone architecture unchanged. Our design uses a token update mask and staged training to control the scope and timing of structural updates. Across benchmarks and Transformer backbones, GTCA strengthens syntactic robustness beyond continued-training baselines without compromising Multiple-Choice QA performance or commonsense reasoning, providing a practical checkpoint-compatible route to more syntax-robust decoder-only LLMs.</li>
<li><strong>摘要：</strong>仅解码器的大型语言模型可以实现强大的广泛性能，但对于轻微的语法扰动却很脆弱，从而损害了下游推理的可靠性。然而，直接将显式语法结构注入现有检查点可能会干扰其预训练的能力。我们引入了一个与检查点兼容的门控树交叉注意（GTCA）分支，该分支读取预先计算的选区块内存，同时保持主干架构不变。我们的设计使用令牌更新掩码和分阶段训练来控制结构更新的范围和时间。在基准测试和 Transformer 主干上，GTCA 增强了超出持续训练基线的语法稳健性，而不会影响多项选择 QA 性能或常识推理，从而为语法更稳健的纯解码器 LLM 提供了一条实用的检查点兼容途径。</li>
</ul>

<h3>Title: Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pranav Bhandari, Usman Naseem, Mehwish Nasim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15847">https://arxiv.org/abs/2602.15847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15847">https://arxiv.org/pdf/2602.15847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15847]] Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models(https://arxiv.org/abs/2602.15847)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 中的人格控制通常依赖于注入特定特质的控制向量，隐含地假设人格特质可以独立控制。在这项工作中，我们通过分析大五人格转向方向之间的几何关系来检验这一假设是否成立。我们研究从两个模型系列（LLaMA-3-8B 和 Mistral-8B）中提取的转向向量，并应用一系列几何条件方案，从无约束方向到软和硬正交化。我们的结果表明，个性转向表现出显着的几何依赖性：即使线性重叠被明确消除，转向一种特征也会持续引起其他特征的变化。虽然硬正交化强制了几何独立性，但它并不能消除跨特征行为效应，并且会降低转向强度。这些发现表明法学硕士的人格特质占据了一个稍微耦合的子空间，限制了完全独立的特质控制。</li>
</ul>

<h3>Title: Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling</h3>
<ul>
<li><strong>Authors: </strong>Andrius Matšenas, Anet Lello, Tõnis Lees, Hans Peep, Kim Lilii Tamm</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15848">https://arxiv.org/abs/2602.15848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15848">https://arxiv.org/pdf/2602.15848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15848]] Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling(https://arxiv.org/abs/2602.15848)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.</li>
<li><strong>摘要：</strong>这项研究验证了大型语言模型 (LLM) 作为基于问卷的人格评估的动态替代方案。通过受试者内部实验 (N=33)，我们将 LLM 指导对话中得出的“大五人格”得分与黄金标准 IPIP-50 问卷进行了比较，同时还测量了用户感知的准确性。结果表明收敛效度中等（r=0.38-0.58），方法之间的责任心、开放性和神经质得分在统计上相当。宜人性和外向性表现出显着差异，表明需要针对特定​​特质进行校准。值得注意的是，参与者认为法学硕士生成的档案与传统问卷结果同样准确。这些发现表明对话式人工智能为传统心理测量学提供了一种有前途的新方法。</li>
</ul>

<h3>Title: Preference Optimization for Review Question Generation Improves Writing Quality</h3>
<ul>
<li><strong>Authors: </strong>Karun Sharma, Vidushee Vats, Shengzhi Li, Yuxiang Wang, Zhongtian Sun, Prayag Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15849">https://arxiv.org/abs/2602.15849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15849">https://arxiv.org/pdf/2602.15849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15849]] Preference Optimization for Review Question Generation Improves Writing Quality(https://arxiv.org/abs/2602.15849)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.</li>
<li><strong>摘要：</strong>同行评审依赖于实质性的、基于证据的问题，但现有的基于 LLM 的方法通常会生成表面级别的查询，从论文的首页提取超过 50% 的问题标记。为了弥补这一差距，我们开发了 IntelliReward，这是一种新颖的奖励模型，由冻结的自回归 LLM 构建而成，并在最后 50 个令牌状态上具有可训练的多头变压器，该模型在预测专家级人类偏好方面优于基于 API 的 SFT 基线。通过将解耦剪辑和动态采样策略优化 (DAPO) 与 IntelliReward 结合使用，我们训练了 IntelliAsk，这是一种符合人类努力、证据和基础标准的问题生成模型。我们发现推理和写作基准持续改进，这表明审稿人的问题质量与更广泛的能力相关。与 Qwen3-32B 基本模型相比，IntelliAsk 在各种基准测试中显示出可测量的增益，特别是提高了 MuSR（68.3 vs 64.7 Acc）等推理任务和 WriteBench（8.31 vs 8.07）等复杂写作评估的性能。我们发布了我们的实现、专家偏好注释和 IntelliReward 模型，为 LLM 生成的评审问题中的基础、努力和证据提供自动评估基准。</li>
</ul>

<h3>Title: Large Language Models for Assisting American College Applications</h3>
<ul>
<li><strong>Authors: </strong>Zhengliang Liu, Weihang You, Peng Shu, Junhao Chen, Yi Pan, Hanqi Jiang, Yiwei Li, Zhaojun Ding, Chao Cao, Xinliang Li, Yifan Zhou, Ruidong Zhang, Shaochen Xu, Wei Ruan, Huaqin Zhao, Dajiang Zhu, Tianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15850">https://arxiv.org/abs/2602.15850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15850">https://arxiv.org/pdf/2602.15850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15850]] Large Language Models for Assisting American College Applications(https://arxiv.org/abs/2602.15850)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>American college applications require students to navigate fragmented admissions policies, repetitive and conditional forms, and ambiguous questions that often demand cross-referencing multiple sources. We present EZCollegeApp, a large language model (LLM)-powered system that assists high-school students by structuring application forms, grounding suggested answers in authoritative admissions documents, and maintaining full human control over final responses. The system introduces a mapping-first paradigm that separates form understanding from answer generation, enabling consistent reasoning across heterogeneous application portals. EZCollegeApp integrates document ingestion from official admissions websites, retrieval-augmented question answering, and a human-in-the-loop chatbot interface that presents suggestions alongside application fields without automated submission. We describe the system architecture, data pipeline, internal representations, security and privacy measures, and evaluation through automated testing and human quality assessment. Our source code is released on GitHub (this https URL) to facilitate the broader impact of this work.</li>
<li><strong>摘要：</strong>美国大学申请要求学生应对分散的招生政策、重复和有条件的表格以及往往需要交叉引用多个来源的模糊问题。我们推出 EZCollegeApp，这是一个基于大型语言模型 (LLM) 的系统，可通过构建申请表、将建议答案纳入权威招生文件以及保持对最终答案的完全人为控制来帮助高中生。该系统引入了映射优先范例，将形式理解与答案生成分开，从而实现跨异构应用程序门户的一致推理。 EZCollegeApp 集成了来自官方招生网站的文档提取、检索增强问答以及人机交互聊天机器人界面，该界面可在申请字段旁边提供建议，而无需自动提交。我们描述了系统架构、数据管道、内部表示、安全和隐私措施，以及通过自动化测试和人工质量评估进行的评估。我们的源代码在 GitHub（此 https URL）上发布，以促进这项工作产生更广泛的影响。</li>
</ul>

<h3>Title: Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey</h3>
<ul>
<li><strong>Authors: </strong>David Y. Liu, Aditya Joshi, Paul Dawson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15851">https://arxiv.org/abs/2602.15851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15851">https://arxiv.org/pdf/2602.15851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15851]] Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey(https://arxiv.org/abs/2602.15851)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing efforts that reflect established distinctions in narratology. We discover patterns in the following: narrative datasets and tasks, narrative theories and NLP pipeline and methodological trends in prompting and fine-tuning. We highlight how LLMs enable easy connections of NLP pipelines with abstract narrative concepts and opportunities for interdisciplinary collaboration. Challenges remain in attempts to work towards any unified definition or benchmark of narrative related tasks, making model comparison difficult. For future directions, instead of the pursuit of a single, generalised benchmark for 'narrative quality', we believe that progress benefits more from efforts that focus on the following: defining and improving theory-based metrics for individual narrative attributes to incrementally improve model performance; conducting large-scale, theory-driven literary/social/cultural analysis; and creating experiments where outputs can be used to validate or refine narrative theories. This work provides a contextual foundation for more systematic and theoretically informed narrative research in NLP by providing an overview to ongoing research efforts and the broader narrative studies landscape.</li>
<li><strong>摘要：</strong>使用大语言模型（LLM）的叙事理论的应用在自动故事生成和理解任务中提供了有前景的用例。我们的调查研究了自然语言处理（NLP）研究如何与叙事研究领域相结合，并为反映叙事学中既定区别的正在进行的努力提出了分类法。我们发现以下模式：叙事数据集和任务、叙事理论和 NLP 流程以及提示和微调的方法论趋势。我们重点介绍法学硕士如何轻松连接 NLP 流程与抽象叙事概念以及跨学科合作的机会。尝试对叙事相关任务进行统一定义或基准仍然存在挑战，这使得模型比较变得困难。对于未来的方向，我们认为，进展更多地受益于关注以下方面的努力，而不是追求单一的、通用的“叙事质量”基准：定义和改进个体叙事属性的基于理论的指标，以逐步提高模型性能；进行大规模的、理论驱动的文学/社会/文化分析；并创建实验，其中的输出可用于验证或完善叙事理论。这项工作通过概述正在进行的研究工作和更广泛的叙事研究领域，为 NLP 中更系统、更有理论依据的叙事研究提供了背景基础。</li>
</ul>

<h3>Title: A Lightweight Explainable Guardrail for Prompt Safety</h3>
<ul>
<li><strong>Authors: </strong>Md Asiful Islam, Mihai Surdeanu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15853">https://arxiv.org/abs/2602.15853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15853">https://arxiv.org/pdf/2602.15853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15853]] A Lightweight Explainable Guardrail for Prompt Safety(https://arxiv.org/abs/2602.15853)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.</li>
<li><strong>摘要：</strong>我们提出了一种轻量级可解释护栏（LEG）方法来对不安全提示进行分类。 LEG 使用多任务学习架构来共同学习提示分类器和解释分类器，其中后者标记解释安全/不安全整体决策的提示词。 LEG 使用合成数据进行训练以实现可解释性，这些数据是使用抵消法学硕士确认偏差的新颖策略生成的。最后，LEG 的训练过程使用了一种新颖的损失，可以捕获全局解释信号，并将交叉熵和焦点损失与基于不确定性的加权相结合。尽管 LEG 的模型大小比当前方法小得多，但在域内和域外的三个数据集上，LEG 在即时分类和可解释性方面都获得了与最先进技术相当或更好的性能。如果接受，我们将公开发布所有模型和带注释的数据集。</li>
</ul>

<h3>Title: Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Xu, Xingyu Ren, Zhiqiang You, Yumeng Zhang, Zhoupeng Shou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15854">https://arxiv.org/abs/2602.15854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15854">https://arxiv.org/pdf/2602.15854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15854]] Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization(https://arxiv.org/abs/2602.15854)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.</li>
<li><strong>摘要：</strong>大型语言模型在面向任务的对话系统中显示出潜力，但现有的训练方法通常依赖于令牌级别的可能性或偏好优化，这与长期任务的成功不太相符。为了解决这个问题，我们提出了面向目标的偏好优化（GOPO），这是一种分层强化学习框架，可通过专家代理和客户服务代理将策略规划与响应生成分离。专家代理在对话轨迹级别优化多轮目标偏好，而客户服务代理则生成与所选策略严格一致的响应。我们在公共基准和电子商务客户服务数据集上评估 GOPO，并引入以任务为中心的顺序参与（TSE），这是一种源自真实电子商务交互数据的序列级指标。在 Mgshop 数据集上，GOPO 比 PPO 和 Memento 使 TSE 提高了 7.7% 和 10.3%，并且在序列级奖励和生成质量方面具有一致的增益。此外，使用 GOPO 训练的 14B 模型的 TSE 分别比 Qwen-235B 和 GPT-5.2 高 2.7% 和 1.5%。消融研究证实了专家代理在长期优化中的关键作用。 GOPO 在其他数据集上也展示了一致的改进。这项工作为商业场景中面向任务的对话系统建立了新的范式，并且代码和数据集将公开。</li>
</ul>

<h3>Title: Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Liu, Zian Jia, Xinyu Gao, Kanjun Xu, Yun Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15856">https://arxiv.org/abs/2602.15856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15856">https://arxiv.org/pdf/2602.15856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15856]] Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective(https://arxiv.org/abs/2602.15856)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to address this by encoding long documents into compact embeddings, yet they often underperform non-compressed RAG due to their reliance on auto-encoder-like full-compression that forces the encoder to compress all document information regardless of relevance to the input query. In this work, we conduct an analysis on this paradigm and reveal two fundamental limitations: (I) Infeasibility, full-compression conflicts with the LLM's downstream generation behavior; and (II) Non-necessity: full-compression is unnecessary and dilutes task-relevant information density. Motivated by these insights, we introduce SeleCom, a selector-based soft compression framework for RAG that redefines the encoder's role as query-conditioned information selector. The selector is decoder-only and is trained with a massive, diverse and difficulty-graded synthetic QA dataset with curriculum learning. Extensive experiments show that SeleCom significantly outperforms existing soft compression approaches and achieves competitive or superior performance to non-compression baselines, while reducing computation and latency by 33.8%~84.6%.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 有效地将大型语言模型 (LLM) 与外部知识结合起来，并广泛应用于与 Web 相关的任务。然而，其可扩展性受到上下文长度过大和冗余检索的阻碍。最近关于软上下文压缩的研究旨在通过将长文档编码为紧凑的嵌入来解决这个问题，但它们通常表现不佳，因为它们依赖于类似自动编码器的完全压缩，强制编码器压缩所有文档信息，而不管与输入查询的相关性。在这项工作中，我们对该范式进行了分析，并揭示了两个基本局限性：（I）不可行性，完全压缩与LLM的下游生成行为相冲突； (II)非必要性：完全压缩是不必要的，并且稀释了与任务相关的信息密度。受这些见解的启发，我们引入了 SeleCom，这是一种基于选择器的 RAG 软压缩框架，它重新定义了编码器作为查询条件信息选择器的角色。选择器仅是解码器，并使用大量、多样化且按难度分级的综合 QA 数据集和课程学习进行训练。大量实验表明，SeleCom 显着优于现有的软压缩方法，并实现了与非压缩基线相比具有竞争力或优越的性能，同时减少了 33.8%~84.6% 的计算和延迟。</li>
</ul>

<h3>Title: Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15857">https://arxiv.org/abs/2602.15857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15857">https://arxiv.org/pdf/2602.15857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15857]] Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach(https://arxiv.org/abs/2602.15857)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The analysis of public opinion from multiple heterogeneous sources presents significant challenges due to structural differences, semantic variations, and platform-specific biases. This paper introduces a novel Collaborative Reasoning and Adaptive Fusion (CRAF) framework that systematically integrates traditional feature-based methods with large language models (LLMs) through a structured multi-stage reasoning mechanism. Our approach features four key innovations: (1) a cross-platform collaborative attention module that aligns semantic representations while preserving source-specific characteristics, (2) a hierarchical adaptive fusion mechanism that dynamically weights features based on both data quality and task requirements, (3) a joint optimization strategy that simultaneously learns topic representations and sentiment distributions through shared latent spaces, and (4) a novel multimodal extraction capability that processes video content from platforms like Douyin and Kuaishou by integrating OCR, ASR, and visual sentiment analysis. Theoretical analysis demonstrates that CRAF achieves a tighter generalization bound with a reduction of O(sqrt(d log K / m)) compared to independent source modeling, where d is feature dimensionality, K is the number of sources, and m is sample size. Comprehensive experiments on three multi-platform datasets (Weibo-12, CrossPlatform-15, NewsForum-8) show that CRAF achieves an average topic clustering ARI of 0.76 (4.1% improvement over best baseline) and sentiment analysis F1-score of 0.84 (3.8% improvement). The framework exhibits strong cross-platform adaptability, reducing the labeled data requirement for new platforms by 75%.</li>
<li><strong>摘要：</strong>由于结构差异、语义变化和特定于平台的偏见，对多个异构来源的舆论分析提出了重大挑战。本文介绍了一种新颖的协作推理和自适应融合（CRAF）框架，该框架通过结构化的多阶段推理机制将传统的基于特征的方法与大型语言模型（LLM）系统地集成在一起。我们的方法具有四个关键创新：(1) 跨平台协作注意力模块，在保留特定源特征的同时对齐语义表示；(2) 分层自适应融合机制，根据数据质量和任务要求动态加权特征；(3) 联合优化策略，通过共享潜在空间同时学习主题表示和情感分布；(4) 新颖的多模态提取功能，通过集成 OCR、ASR 和视觉情感分析来处理来自抖音和快手等平台的视频内容。理论分析表明，与独立源建模相比，CRAF 实现了更严格的泛化界限，降低了 O(sqrt(d log K / m))，其中 d 是特征维度，K 是源数量，m 是样本大小。对三个多平台数据集（Weibo-12、CrossPlatform-15、NewsForum-8）的综合实验表明，CRAF 的平均主题聚类 ARI 为 0.76（较最佳基线提高 4.1%），情感分析 F1 得分为 0.84（提高 3.8%）。该框架具有很强的跨平台适应性，将新平台的标签数据需求减少了75%。</li>
</ul>

<h3>Title: State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Annie Wong, Aske Plaat, Thomas Bäck, Niki van Stein, Anna V. Kononova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15858">https://arxiv.org/abs/2602.15858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15858">https://arxiv.org/pdf/2602.15858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15858]] State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models(https://arxiv.org/abs/2602.15858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. Holding model parameters fixed, we systematically vary three key aspects: (1) state granularity (long form versus summary), (2) structure (natural language versus symbolic), and (3) spatial grounding (text-only versus images or textual map encodings) across sequential decision-making benchmarks. We find that trajectory summarisation improves performance by reducing noise and stabilising long-horizon reasoning. Second, natural language representations are the most robust across models, whereas structured encodings help mainly for models with strong code or structured output priors, such as JSON schemas. Third, while image-inputs show some benefit, text-based spatial encodings prove most effective. This advantage stems not from the spatial information itself, but from the act of construction, which compels the model to perform the spatial reasoning that static input does not elicit. Overall, we demonstrate that design choices for representing state are a decisive factor in performance, distinct from the availability of information itself. We note, however, that even with improved representations, current LLMs and VLMs remain brittle over long horizons, particularly when they must synthesise information to manage multiple subtasks to reach a goal.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 从静态推理任务转向动态环境，它们的成功取决于导航和响应在推理时交互时发生变化的环境的能力。这些环境中一个未被充分探索的因素是国家的代表。保持模型参数固定，我们系统地改变三个关键方面：（1）状态粒度（长格式与摘要），（2）结构（自然语言与符号），以及（3）跨顺序决策基准的空间基础（纯文本与图像或文本映射编码）。我们发现轨迹总结通过减少噪声和稳定长视野推理来提高性能。其次，自然语言表示在模型中是最稳健的，而结构化编码主要有助于具有强大代码或结构化输出先验的模型，例如 JSON 模式。第三，虽然图像输入显示出一些好处，但基于文本的空间编码被证明是最有效的。这种优势并非源于空间信息本身，而是源于构造行为，它迫使模型执行静态输入无法引发的空间推理。总的来说，我们证明了表示状态的设计选择是性能的决定性因素，与信息本身的可用性不同。然而，我们注意到，即使改进了表征，当前的 LLM 和 VLM 从长远来看仍然很脆弱，特别是当它们必须综合信息来管理多个子任务以实现目标时。</li>
</ul>

<h3>Title: From Transcripts to AI Agents: Knowledge Extraction, RAG Integration, and Robust Evaluation of Conversational AI Assistants</h3>
<ul>
<li><strong>Authors: </strong>Krittin Pachtrachai, Petmongkon Pornpichitsuwan, Wachiravit Modecrua, Touchapon Kraisingkorn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15859">https://arxiv.org/abs/2602.15859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15859">https://arxiv.org/pdf/2602.15859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15859]] From Transcripts to AI Agents: Knowledge Extraction, RAG Integration, and Robust Evaluation of Conversational AI Assistants(https://arxiv.org/abs/2602.15859)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Building reliable conversational AI assistants for customer-facing industries remains challenging due to noisy conversational data, fragmented knowledge, and the requirement for accurate human hand-off - particularly in domains that depend heavily on real-time information. This paper presents an end-to-end framework for constructing and evaluating a conversational AI assistant directly from historical call transcripts. Incoming transcripts are first graded using a simplified adaptation of the PIPA framework, focusing on observation alignment and appropriate response behavior, and are filtered to retain only high-quality interactions exhibiting coherent flow and effective human agent responses. Structured knowledge is then extracted from curated transcripts using large language models (LLMs) and deployed as the sole grounding source in a Retrieval-Augmented Generation (RAG) pipeline. Assistant behavior is governed through systematic prompt tuning, progressing from monolithic prompts to lean, modular, and governed designs that ensure consistency, safety, and controllable execution. Evaluation is conducted using a transcript-grounded user simulator, enabling quantitative measurement of call coverage, factual accuracy, and human escalation behavior. Additional red teaming assesses robustness against prompt injection, out-of-scope, and out-of-context attacks. Experiments are conducted in the Real Estate and Specialist Recruitment domains, which are intentionally challenging and currently suboptimal for automation due to their reliance on real-time data. Despite these constraints, the assistant autonomously handles approximately 30 percents of calls, achieves near-perfect factual accuracy and rejection behavior, and demonstrates strong robustness under adversarial testing.</li>
<li><strong>摘要：</strong>由于嘈杂的对话数据、碎片化的知识以及准确的人工交接的要求，为面向客户的行业构建可靠的对话式人工智能助手仍然具有挑战性——特别是在严重依赖实时信息的领域。本文提出了一个端到端框架，用于直接根据历史通话记录构建和评估对话式人工智能助手。传入的转录本首先使用 PIPA 框架的简化改编进行分级，重点关注观察对齐和适当的响应行为，并进行过滤以仅保留表现出连贯流程和有效人类代理响应的高质量交互。然后使用大型语言模型 (LLM) 从整理的记录中提取结构化知识，并将其部署为检索增强生成 (RAG) 管道中的唯一基础源。助理行为通过系统提示调整进行管理，从整体提示发展到精益、模块化和受管理的设计，以确保一致性、安全性和可控执行。使用基于记录的用户模拟器进行评估，从而能够定量测量呼叫覆盖范围、事实准确性和人为升级行为。额外的红队评估针对即时注入、超出范围和脱离上下文的攻击的稳健性。实验是在房地产和专家招聘领域进行的，这些领域故意具有挑战性，并且由于依赖实时数据，目前对于自动化来说不是最佳选择。尽管存在这些限制，助手仍能自主处理大约 30% 的呼叫，实现近乎完美的事实准确性和拒绝行为，并在对抗性测试下表现出强大的鲁棒性。</li>
</ul>

<h3>Title: Reranker Optimization via Geodesic Distances on k-NN Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Wen G. Gong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15860">https://arxiv.org/abs/2602.15860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15860">https://arxiv.org/pdf/2602.15860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15860]] Reranker Optimization via Geodesic Distances on k-NN Manifolds(https://arxiv.org/abs/2602.15860)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Current neural reranking approaches for retrieval-augmented generation (RAG) rely on cross-encoders or large language models (LLMs), requiring substantial computational resources and exhibiting latencies of 3-5 seconds per query. We propose Maniscope, a geometric reranking method that computes geodesic distances on k-nearest neighbor (k-NN) manifolds constructed over retrieved document candidates. This approach combines global cosine similarity with local manifold geometry to capture semantic structure that flat Euclidean metrics miss. Evaluating on eight BEIR benchmark datasets (1,233 queries), Maniscope outperforms HNSW graph-based baseline on the three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2x faster (4.7 ms vs 14.8 ms average). Compared to cross-encoder rerankers, Maniscope achieves within 2% accuracy at 10-45x lower latency. On TREC-COVID, LLM-Reranker provides only +0.5% NDCG@3 improvement over Maniscope at 840x higher latency, positioning Maniscope as a practical alternative for real-time RAG deployment. The method requires O(N D + M^2 D + M k log k) complexity where M << N , enabling sub-10 ms latency. We plan to release Maniscope as open-source software.</li>
<li><strong>摘要：</strong>当前用于检索增强生成 (RAG) 的神经重新排序方法依赖于交叉编码器或大型语言模型 (LLM)，需要大量计算资源，并且每个查询的延迟为 3-5 秒。我们提出了 Maniscope，一种几何重新排序方法，用于计算在检索到的候选文档上构建的 k 最近邻 (k-NN) 流形上的测地距离。这种方法将全局余弦相似性与局部流形几何相结合，以捕获平面欧几里得度量所遗漏的语义结构。通过对 8 个 BEIR 基准数据集（1,233 个查询）进行评估，Maniscope 在三个最难的数据集（NFCorpus：+7.0%、TREC-COVID：+1.6%、AorB：+2.8% NDCG@3）上的性能优于基于 HNSW 图形的基线，同时速度提高了 3.2 倍（4.7 毫秒 vs 平均 14.8 毫秒）。与跨编码器重新排序器相比，Maniscope 的准确度在 2% 以内，延迟降低了 10-45 倍。在 TREC-COVID 上，LLM-Reranker 的 NDCG@3 仅比 Maniscope 提高了 0.5%，延迟提高了 840 倍，将 Maniscope 定位为实时 RAG 部署的实用替代方案。该方法需要 O(N D + M^2 D + M k log k) 复杂度，其中 M << N ，从而实现低于 10 毫秒的延迟。我们计划将 Maniscope 作为开源软件发布。</li>
</ul>

<h3>Title: CAST: Achieving Stable LLM-based Text Analysis for Data Analytics</h3>
<ul>
<li><strong>Authors: </strong>Jinxiang Xie, Zihao Li, Wei He, Rui Ding, Shi Han, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15861">https://arxiv.org/abs/2602.15861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15861">https://arxiv.org/pdf/2602.15861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15861]] CAST: Achieving Stable LLM-based Text Analysis for Data Analytics(https://arxiv.org/abs/2602.15861)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Text analysis of tabular data relies on two core operations: \emph{summarization} for corpus-level theme extraction and \emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stability demanded by data analytics. To address this challenge, we introduce \textbf{CAST} (\textbf{C}onsistency via \textbf{A}lgorithmic Prompting and \textbf{S}table \textbf{T}hinking), a framework that enhances output stability by constraining the model's latent reasoning path. CAST combines (i) Algorithmic Prompting to impose a procedural scaffold over valid reasoning transitions and (ii) Thinking-before-Speaking to enforce explicit intermediate commitments before final generation. To measure progress, we introduce \textbf{CAST-S} and \textbf{CAST-T}, stability metrics for bulleted summarization and tagging, and validate their alignment with human judgments. Experiments across publicly available benchmarks on multiple LLM backbones show that CAST consistently achieves the best stability among all baselines, improving Stability Score by up to 16.2\%, while maintaining or improving output quality.</li>
<li><strong>摘要：</strong>表格数据的文本分析依赖于两个核心操作：用于语料库级主题提取的 \emph{summarization} 和用于行级标记的 \emph{tagging}。使用大型语言模型 (LLM) 执行这些任务的一个关键限制是它们无法满足数据分析所需的输出稳定性的高标准。为了应对这一挑战，我们引入了 \textbf{CAST} （通过 \textbf{A}lgorithmic Prompting 和 \textbf{S}table \textbf{T}hinking 实现 \textbf{C}consistency），这是一个通过约束模型的潜在推理路径来增强输出稳定性的框架。 CAST 结合了 (i) 算法提示，在有效的推理转换上强加程序支架，以及 (ii) 先思考后说话，在最终生成之前强制执行明确的中间承诺。为了衡量进展，我们引入了 \textbf{CAST-S} 和 \textbf{CAST-T}，用于项目符号摘要和标记的稳定性指标，并验证它们与人类判断的一致性。在多个 LLM 主干上的公开基准测试中进行的实验表明，CAST 在所有基准中始终实现了最佳稳定性，将稳定性得分提高了高达 16.2%，同时保持或提高了输出质量。</li>
</ul>

<h3>Title: Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation</h3>
<ul>
<li><strong>Authors: </strong>Guoshan Liu, Bin Zhu, Yian Li, Jingjing Chen, Chong-Wah Ngo, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15862">https://arxiv.org/abs/2602.15862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15862">https://arxiv.org/pdf/2602.15862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15862]] Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation(https://arxiv.org/abs/2602.15862)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that predicts and validates actions and ingredients as internal context for instruction generation. Our two-stage pipeline combines supervised fine-tuning (SFT) with reinforcement fine-tuning (RFT): SFT builds foundational accuracy using an Action-Reasoning dataset and ingredient corpus, while RFT employs frequency-aware rewards to improve long-tail action prediction and ingredient generalization. A Semantic Confidence Scoring and Rectification (SCSR) module further filters and corrects predictions. Experiments on Recipe1M show state-of-the-art performance and markedly improved semantic fidelity.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLMM) 的最新进展使得能够从食物图像生成菜谱，但尽管词汇得分很高（例如 BLEU、ROUGE），但输出通常包含语义上不正确的操作或成分。为了解决这一差距，我们提出了一个基于语义的框架，该框架可以预测和验证动作和成分作为指令生成的内部上下文。我们的两阶段流程将监督微调 (SFT) 与强化微调 (RFT) 相结合：SFT 使用动作推理数据集和成分语料库构建基础准确性，而 RFT 采用频率感知奖励来改进长尾动作预测和成分泛化。语义置信度评分和纠正 (SCSR) 模块进一步过滤和纠正预测。 Recipe1M 上的实验显示了最先进的性能并显着提高了语义保真度。</li>
</ul>

<h3>Title: Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Daehoon Gwak, Minseo Jung, Junwoo Park, Minho Park, ChaeHun Park, Junha Hyung, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15863">https://arxiv.org/abs/2602.15863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15863">https://arxiv.org/pdf/2602.15863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15863]] Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning(https://arxiv.org/abs/2602.15863)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.</li>
<li><strong>摘要：</strong>最近的研究表明，大型语言模型（LLM）可以通过自行生成的少量示例来提高其推理性能，达到与手动策划的上下文示例相当的结果。然而，这些成果背后的根本机制仍不清楚，因此很难决定何时以及如何有效地应用该技术。在这项工作中，我们认为关键的好处不是来自生成的示例本身，而是来自创建它们的行为。为了验证这一点，针对不同 LLM 架构的推理密集型任务，我们系统地评估了上下文学习的三种提示策略：（1）零样本提示； (2) 综合提示，法学硕士在单一、统一的提示中创建和解决问题； (3) 解耦提示，其中自行生成的示例被重用为上下文示例，但其创建本身的上下文被排除在外。我们对五种广泛使用的模型架构进行了实验，证明集成提示始终优于零样本提示和解耦提示。相比之下，解耦提示仅比零样本提供边际收益。此外，为了进行更深入的分析，我们进行了注意力分析，并观察了集成提示和解耦提示之间注意力模式的显着差异。这些发现表明，自生成提示的优势来自于问题创建的过程，而不是例子本身，为设计更有效的提示策略提供了宝贵的见解。</li>
</ul>

<h3>Title: Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?</h3>
<ul>
<li><strong>Authors: </strong>Berry Gerrits</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15867">https://arxiv.org/abs/2602.15867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15867">https://arxiv.org/pdf/2602.15867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15867]] Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?(https://arxiv.org/abs/2602.15867)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment for assessing how LLM-based chatbots interpret natural language descriptions and generate appropriate action sequences to succeed in the game. We test the performance of leading proprietary models - ChatGPT, Claude, and Gemini - under both minimal and detailed instructions, measuring game progress through achieved scores as the primary metric. Our results reveal that all tested models achieve less than 10% completion on average, with even the best-performing model (Claude Opus 4.5) reaching only approximately 75 out of 350 possible points. Notably, providing detailed game instructions offers no improvement, nor does enabling ''extended thinking''. Qualitative analysis of the models' reasoning processes reveals fundamental limitations: repeated unsuccessful actions suggesting an inability to reflect on one's own thinking, inconsistent persistence of strategies, and failure to learn from previous attempts despite access to conversation history. These findings suggest substantial limitations in current LLMs' metacognitive abilities and problem-solving capabilities within the domain of text-based games, raising questions about the nature and extent of their reasoning capabilities.</li>
<li><strong>摘要：</strong>在这篇定位论文中，我们通过当代大型语言模型 (LLM) 在 Zork 中的表现来评估其解决问题和推理的能力。Zork 是一款开创性的基于文本的冒险游戏，于 1977 年首次发布。该游戏基于对话的结构提供了一个受控环境，用于评估基于 LLM 的聊天机器人如何解释自然语言描述并生成适当的动作序列以在游戏中取得成功。我们在最少和详细的说明下测试领先专有模型（ChatGPT、Claude 和 Gemini）的性能，通过获得的分数作为主要指标来衡量游戏进度。我们的结果显示，所有测试模型的平均完成度不到 10%，即使是性能最好的模型 (Claude Opus 4.5)，在 350 个可能的分数中也只能达到大约 75 分。值得注意的是，提供详细的游戏说明并不能带来任何改进，启用“扩展思维”也不会带来任何改进。对模型推理过程的定性分析揭示了根本的局限性：重复的不成功的行动表明无法反思自己的思维，策略的持续性不一致，以及尽管可以访问对话历史但仍无法从以前的尝试中学习。这些发现表明，当前法学硕士在基于文本的游戏领域中的元认知能力和解决问题的能力存在很大的局限性，引发了关于他们推理能力的性质和程度的问题。</li>
</ul>

<h3>Title: Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Magnus Boman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15868">https://arxiv.org/abs/2602.15868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15868">https://arxiv.org/pdf/2602.15868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15868]] Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning(https://arxiv.org/abs/2602.15868)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在看似微不足道的任务上表现出失败模式。我们提出使用确定性多磁带图灵机来形式化 LLM 交互，其中每个磁带代表一个不同的组件：输入字符、标记、词汇、模型参数、激活、概率分布和输出文本。该模型能够将故障模式精确定位到特定的管道阶段，揭示例如标记化如何模糊计数任务所需的字符级结构。该模型通过将输出磁带上的计算外部化，阐明了为什么思想链提示等技术会有所帮助，同时也揭示了它们的基本局限性。这种方法为几何隐喻提供了严格的、可证伪的替代方案，并通过原则性误差分析补充了经验尺度定律。</li>
</ul>

<h3>Title: Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches</h3>
<ul>
<li><strong>Authors: </strong>Noopur Zambare, Kiana Aghakasiri, Carissa Lin, Carrie Ye, J. Ross Mitchell, Mohamed Abdalla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15869">https://arxiv.org/abs/2602.15869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15869">https://arxiv.org/pdf/2602.15869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15869]] Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches(https://arxiv.org/abs/2602.15869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown strong performance on clinical de-identification, the task of identifying sensitive identifiers to protect privacy. However, previous work has not examined their generalizability between formats, cultures, and genders. In this work, we systematically evaluate fine-tuned transformer models (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B) at de-identification. We show that smaller models achieve comparable performance while substantially reducing inference cost, making them more practical for deployment. Moreover, we demonstrate that smaller models can be fine-tuned with limited data to outperform larger models in de-identifying identifiers drawn from Mandarin, Hindi, Spanish, French, Bengali, and regional variations of English, in addition to gendered names. To improve robustness in multi-cultural contexts, we introduce and publicly release BERT-MultiCulture-DEID, a set of de-identification models based on BERT, ClinicalBERT, and ModernBERT, fine-tuned on MIMIC with identifiers from multiple language variants. Our findings provide the first comprehensive quantification of the efficiency-generalizability trade-off in de-identification and establish practical pathways for fair and efficient clinical de-identification. Details on accessing the models are available at: this https URL</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在临床去识别（识别敏感标识符以保护隐私的任务）方面表现出了强大的性能。然而，之前的工作并没有检验它们在格式、文化和性别之间的普遍性。在这项工作中，我们在去识别方面系统地评估了微调的 Transformer 模型（BERT、ClinicalBERT、ModernBERT）、小型 LLM（Llama 1-8B、Qwen 1.5-7B）和大型 LLM（Llama-70B、Qwen-72B）。我们证明，较小的模型可以实现相当的性能，同时大幅降低推理成本，使它们更适合部署。此外，我们证明，可以使用有限的数据对较小的模型进行微调，以在去除来自普通话、印地语、西班牙语、法语、孟加拉语和英语区域变体以及性别名称的标识符方面优于较大的模型。为了提高多文化背景下的鲁棒性，我们引入并公开发布了 BERT-MultiCulture-DEID，这是一套基于 BERT、ClinicalBERT 和 ModernBERT 的去识别模型，并在 MIMIC 上使用来自多种语言变体的标识符进行了微调。我们的研究结果首次全面量化了去身份验证的效率与通用性权衡，并为公平有效的临床去身份验证建立了实用途径。有关访问模型的详细信息，请访问：此 https URL</li>
</ul>

<h3>Title: VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering</h3>
<ul>
<li><strong>Authors: </strong>Shuhui Qu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15870">https://arxiv.org/abs/2602.15870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15870">https://arxiv.org/pdf/2602.15870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15870]] VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering(https://arxiv.org/abs/2602.15870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Autoregressive language models decode left-to-right with irreversible commitments, limiting revision during multi-step reasoning. We propose \textbf{VDLM}, a modular variable diffusion language model that separates semantic planning from text rendering. VDLM applies LLaDA-style masked diffusion over semantic variable embeddings to enable iterative refinement in latent space, then post-trains the planner with trajectory-aware optimization using embedding-space rewards and values, avoiding text decoding inside the RL loop. To convert planned embeddings back to text, we use a \textbf{Vec2Text} renderer and introduce \textbf{embedding perturbations} to robustify decoding under planner noise. Across nine benchmarks spanning general reasoning, math, and code, VDLM is competitive in pre-training and yields substantial post-training improvements on long-form generation tasks, outperforming other baselines. These results highlight the effectiveness of embedding-space post-training and robust latent-to-text rendering for diffusion language modeling.</li>
<li><strong>摘要：</strong>自回归语言模型以不可逆的承诺从左到右解码，限制多步推理期间的修改。我们提出了 \textbf{VDLM}，一种模块化变量扩散语言模型，它将语义规划与文本渲染分开。 VDLM 在语义变量嵌入上应用 LLaDA 风格的屏蔽扩散，以实现潜在空间中的迭代细化，然后使用嵌入空间奖励和值通过轨迹感知优化对规划器进行后训练，避免 RL 循环内的文本解码。为了将计划的嵌入转换回文本，我们使用 \textbf{Vec2Text} 渲染器并引入 \textbf{embedding perturbations} 来增强规划噪声下的解码。在涵盖一般推理、数学和代码的九个基准中，VDLM 在预训练方面具有竞争力，并且在长格式生成任务上产生了显着的训练后改进，优于其他基准。这些结果凸显了嵌入空间后训练和强大的潜在文本渲染对于扩散语言建模的有效性。</li>
</ul>

<h3>Title: CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content</h3>
<ul>
<li><strong>Authors: </strong>Diletta Abbonato</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15871">https://arxiv.org/abs/2602.15871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15871">https://arxiv.org/pdf/2602.15871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15871]] CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content(https://arxiv.org/abs/2602.15871)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The proliferation of large language models (LLMs) in academic workflows has introduced unprecedented challenges to bibliographic integrity, particularly through reference hallucination -- the generation of plausible but non-existent citations. Recent investigations have documented the presence of AI-hallucinated citations even in papers accepted at premier machine learning conferences such as NeurIPS and ICLR, underscoring the urgency of automated verification mechanisms. This paper presents "CheckIfExist", an open-source web-based tool designed to provide immediate verification of bibliographic references through multi-source validation against CrossRef, Semantic Scholar, and OpenAlex scholarly databases. While existing reference management tools offer bibliographic organization capabilities, they do not provide real-time validation of citation authenticity. Commercial hallucination detection services, though increasingly available, often impose restrictive usage limits on free tiers or require substantial subscription fees. The proposed tool fills this gap by employing a cascading validation architecture with string similarity algorithms to compute multi-dimensional match confidence scores, delivering instant feedback on reference authenticity. The system supports both single-reference verification and batch processing of BibTeX entries through a unified interface, returning validated APA citations and exportable BibTeX records within seconds.</li>
<li><strong>摘要：</strong>学术工作流程中大型语言模型（LLM）的激增给书目完整性带来了前所未有的挑战，特别是通过参考幻觉——生成看似合理但不存在的引文。最近的调查记录了即使在 NeurIPS 和 ICLR 等顶级机器学习会议上接受的论文中也存在人工智能幻觉引用，这凸显了自动验证机制的紧迫性。本文介绍了“CheckIfExist”，这是一个基于网络的开源工具，旨在通过针对 CrossRef、Semantic Sc​​holar 和 OpenAlex 学术数据库的多源验证来提供书目参考的即时验证。虽然现有的参考管理工具提供书目组织功能，但它们不提供引文真实性的实时验证。商业幻觉检测服务虽然越来越可用，但通常对免费套餐施加限制性使用限制或需要大量订阅费。所提出的工具通过采用级联验证架构和字符串相似性算法来计算多维匹配置信度分数，提供有关参考真实性的即时反馈，从而填补了这一空白。该系统通过统一的界面支持 BibTeX 条目的单引用验证和批量处理，在几秒钟内返回经过验证的 APA 引文和可导出的 BibTeX 记录。</li>
</ul>

<h3>Title: P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA</h3>
<ul>
<li><strong>Authors: </strong>Xingda Lyu, Gongfu Lyu, Zitai Yan, Yuxin Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15874">https://arxiv.org/abs/2602.15874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15874">https://arxiv.org/pdf/2602.15874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15874]] P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA(https://arxiv.org/abs/2602.15874)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate remarkable capabilities but remain limited by their reliance on static training data. Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets. Using LLaMA-3.2-1B-Instruct fine-tuned via LoRA, we evaluate on PubMedQA and 2WikiMultihopQA. P-RAG outperforms Standard RAG on PubMedQA by 10.47 percentage points in F1 (93.33% vs. 82.86%; 12.64% relative). On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT prompting substantially improves multi-hop reasoning but yields mixed results for simpler, single-hop queries. These findings underscore P-RAG's potential for accurate, scalable, and contextually adaptive biomedical question answering. Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 展示了卓越的功能，但仍因其对静态训练数据的依赖而受到限制。 Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets.使用通过 LoRA 微调的 LLaMA-3.2-1B-Instruct，我们对 PubMedQA 和 2WikiMultihopQA 进行评估。在 F1 中，P-RAG 在 PubMedQA 上的表现比标准 RAG 高出 10.47 个百分点（93.33% vs. 82.86%；相对 12.64%）。 On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT 提示极大地改进了多跳推理，但对于更简单的单跳查询会产生混合结果。这些发现强调了 P-RAG 在准确、可扩展和上下文自适应生物医学问答方面的潜力。 Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.</li>
</ul>

<h3>Title: Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity</h3>
<ul>
<li><strong>Authors: </strong>Haihui Pan, Yuzhong Hong, Shaoke Lv, Junwei Bao, Hongfei Jiang, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15894">https://arxiv.org/abs/2602.15894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15894">https://arxiv.org/pdf/2602.15894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15894]] Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity(https://arxiv.org/abs/2602.15894)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.</li>
<li><strong>摘要：</strong>最近的研究表明，虽然对齐方法显着提高了大型语言模型（LLM）输出的质量，但它们同时降低了模型输出的多样性。尽管已经提出了一些方法来增强法学硕士输出的多样性，但它们往往以降低性能为代价。在这项工作中，我们首先从理论上证明对齐任务可以分解为两个分布：质量和多样性。为了在保证质量的同时增强LLM输出的多样性，我们提出了质量约束熵最大化策略优化（QEMPO）。 QEMPO 的目标是在保证输出质量的同时最大化策略的输出熵。通过给QEMPO添加不同的约束，我们得到不同的策略。为了优化策略，我们提出了在线和离线培训方法。实验验证了QEMPO在提高输出多样性的同时，实现了与RLHF相当甚至更好的性能。</li>
</ul>

<h3>Title: Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Zhou, Haochen Li, Zhiqiang Nie, JiaLe Chen, Qing Gong, Weizhen Zhang, Chun Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15895">https://arxiv.org/abs/2602.15895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15895">https://arxiv.org/pdf/2602.15895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15895]] Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion(https://arxiv.org/abs/2602.15895)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）通过整合外部知识，有效减轻法学硕士的幻觉。然而，现有框架中文本固有的离散表示往往会导致语义完整性的丧失，从而导致检索偏差。受人类情景记忆机制的启发，我们提出了 CogitoRAG，一个模拟人类认知记忆过程的 RAG 框架。该框架的核心在于语义要点的提取和演化。在离线索引阶段，CogitoRAG首先将非结构化语料推导为要点记忆语料，然后将其转化为整合实体、关系事实和记忆节点的多维知识图谱。在在线检索阶段，该框架通过查询分解模块处理复杂查询，将其分解为综合子查询，模仿人类对复杂信息所采用的认知分解。随后，实体扩散模块在结构相关性和实体频率奖励机制的指导下，在整个图中执行关联检索。此外，我们提出了 CogniRank 算法，该算法通过将扩散导出的分数与语义相似性融合来精确地对候选段落进行重新排序。最终证据以段落记忆配对格式传递给生成器，提供高密度信息支持。五个主流 QA 基准测试和 GraphBench 上的多任务生成的实验结果表明，CogitoRAG 显着优于最先进的 RAG 方法，展示了复杂知识集成和推理的卓越能力。</li>
</ul>

<h3>Title: Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation</h3>
<ul>
<li><strong>Authors: </strong>Xinguo Feng, Zhongkui Ma, Zihan Wang, Alsharif Abuadbba, Guangdong Bai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15897">https://arxiv.org/abs/2602.15897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15897">https://arxiv.org/pdf/2602.15897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15897]] Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation(https://arxiv.org/abs/2602.15897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Training and fine-tuning large-scale language models largely benefit from collaborative learning, but the approach has been proven vulnerable to gradient inversion attacks (GIAs), which allow adversaries to reconstruct private training data from shared gradients. Existing defenses mainly employ gradient perturbation techniques, e.g., noise injection or gradient pruning, to disrupt GIAs' direct mapping from gradient space to token space. However, these methods often fall short due to the retention of semantics similarity across gradient, embedding, and token spaces. In this work, we propose a novel defense mechanism named GHOST (gradient shield with obfuscated tokens), a token-level obfuscation mechanism that neutralizes GIAs by decoupling the inherent connections across gradient, embedding, and token spaces. GHOST is built upon an important insight: due to the large scale of the token space, there exist semantically distinct yet embedding-proximate tokens that can serve as the shadow substitutes of the original tokens, which enables a semantic disconnection in the token space while preserving the connection in the embedding and gradient spaces. GHOST comprises a searching step, which identifies semantically distinct candidate tokens using a multi-criteria searching process, and a selection step, which selects optimal shadow tokens to ensure minimal disruption to features critical for training by preserving alignment with the internal outputs produced by original tokens. Evaluation across diverse model architectures (from BERT to Llama) and datasets demonstrates the remarkable effectiveness of GHOST in protecting privacy (as low as 1% in recovery rate) and preserving utility (up to 0.92 in classification F1 and 5.45 in perplexity), in both classification and generation tasks against state-of-the-art GIAs and adaptive attack scenarios.</li>
<li><strong>摘要：</strong>训练和微调大规模语言模型在很大程度上受益于协作学习，但该方法已被证明容易受到梯度反转攻击（GIA）的攻击，这使得对手可以从共享梯度重建私人训练数据。现有的防御主要采用梯度扰动技术，例如噪声注入或梯度修剪，来破坏 GIA 从梯度空间到令牌空间的直接映射。然而，由于梯度、嵌入和标记空间之间语义相似性的保留，这些方法常常存在不足。在这项工作中，我们提出了一种名为 GHOST（带有混淆令牌的梯度屏蔽）的新型防御机制，这是一种令牌级混淆机制，通过解耦梯度、嵌入和令牌空间之间的固有连接来中和 GIA。 GHOST 建立在一个重要的洞察之上：由于令牌空间的规模很大，存在语义上不同但嵌入接近的令牌，可以充当原始令牌的影子替代品，这使得令牌空间中的语义断开，同时保留嵌入和梯度空间中的连接。 GHOST 包括一个搜索步骤和一个选择步骤，该步骤使用多标准搜索过程识别语义上不同的候选标记，该步骤选择最佳影子标记，以通过保持与原始标记生成的内部输出的对齐来确保对训练关键特征的干扰最小。对不同模型架构（从 BERT 到 Llama）和数据集的评估表明，在针对最先进的 GIA 和自适应攻击场景的分类和生成任务中，GHOST 在保护隐私（恢复率低至 1%）和保留实用性（分类 F1 中高达 0.92，困惑度高达 5.45）方面具有显着的有效性。</li>
</ul>

<h3>Title: MultiCube-RAG for Multi-hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jimeng Shi, Wei Hu, Runchu Tian, Bowen Jin, Wonbin Kweon, SeongKu Kang, Yunfan Kang, Dingqi Ye, Sizhe Zhou, Shaowen Wang, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15898">https://arxiv.org/abs/2602.15898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15898">https://arxiv.org/pdf/2602.15898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15898]] MultiCube-RAG for Multi-hop Question Answering(https://arxiv.org/abs/2602.15898)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Multi-hop question answering (QA) necessitates multi-step reasoning and retrieval across interconnected subjects, attributes, and relations. Existing retrieval-augmented generation (RAG) methods struggle to capture these structural semantics accurately, resulting in suboptimal performance. Graph-based RAGs structure such information in graphs, but the resulting graphs are often noisy and computationally expensive. Moreover, most methods rely on single-step retrieval, neglecting the need for multi-hop reasoning processes. Recent training-based approaches attempt to incentivize the large language models (LLMs) for iterative reasoning and retrieval, but their training processes are prone to unstable convergence and high computational overhead. To address these limitations, we devise an ontology-based cube structure with multiple and orthogonal dimensions to model structural subjects, attributes, and relations. Built on the cube structure, we propose MultiCube-RAG, a training-free method consisting of multiple cubes for multi-step reasoning and retrieval. Each cube specializes in modeling a class of subjects, so that MultiCube-RAG flexibly selects the most suitable cubes to acquire the relevant knowledge precisely. To enhance the query-based reasoning and retrieval, our method decomposes a complex multi-hop query into a set of simple subqueries along cube dimensions and conquers each of them sequentially. Experiments on four multi-hop QA datasets show that MultiCube-RAG improves response accuracy by 8.9% over the average performance of various baselines. Notably, we also demonstrate that our method performs with greater efficiency and inherent explainability.</li>
<li><strong>摘要：</strong>多跳问答 (QA) 需要跨互连主题、属性和关系进行多步骤推理和检索。现有的检索增强生成（RAG）方法很难准确地捕获这些结构语义，从而导致性能不佳。基于图的 RAG 在图中构造此类信息，但生成的图通常有噪声且计算成本昂贵。此外，大多数方法依赖于单步检索，忽略了多跳推理过程的需要。最近基于训练的方法试图激励大型语言模型（LLM）进行迭代推理和检索，但它们的训练过程容易出现不稳定的收敛和高计算开销。为了解决这些限制，我们设计了一种基于本体的立方体结构，具有多个正交维度来建模结构主题、属性和关系。基于立方体结构，我们提出了 MultiCube-RAG，这是一种由多个立方体组成的免训练方法，用于多步骤推理和检索。每个立方体专门对一类学科进行建模，因此MultiCube-RAG可以灵活地选择最合适的立方体来精确地获取相关知识。为了增强基于查询的推理和检索，我们的方法将复杂的多跳查询分解为沿立方体维度的一组简单子查询，并按顺序征服每个子查询。对四个多跳 QA 数据集的实验表明，MultiCube-RAG 的响应精度比各种基线的平均性能提高了 8.9%。值得注意的是，我们还证明了我们的方法具有更高的效率和固有的可解释性。</li>
</ul>

<h3>Title: Doc-to-LoRA: Learning to Instantly Internalize Contexts</h3>
<ul>
<li><strong>Authors: </strong>Rujikorn Charakorn, Edoardo Cetin, Shinnosuke Uesaka, Robert Tjarko Lange</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15902">https://arxiv.org/abs/2602.15902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15902">https://arxiv.org/pdf/2602.15902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15902]] Doc-to-LoRA: Learning to Instantly Internalize Contexts(https://arxiv.org/abs/2602.15902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.</li>
<li><strong>摘要：</strong>长输入序列对于大型语言模型 (LLM) 的上下文学习、文档理解和多步推理至关重要。然而，Transformers 的二次注意力成本使得推理内存密集且缓慢。虽然上下文蒸馏 (CD) 可以将信息传输到模型参数中，但由于训练成本和延迟，按提示蒸馏是不切实际的。为了解决这些限制，我们提出了 Doc-to-LoRA (D2L)，这是一种轻量级超网络，可以通过元学习在单次前向传递中执行近似 CD。给定一个看不见的提示，D2L 会为目标 LLM 生成 LoRA 适配器，从而能够在不重新消耗原始上下文的情况下回答后续查询，从而减少目标 LLM 推理期间的延迟和 KV 缓存内存消耗。在长上下文大海捞针任务中，D2L 成功学会将上下文映射到存储针信息的适配器中，在超过目标 LLM 本地上下文窗口 4 倍以上的序列长度上实现近乎完美的零样本精度。在计算有限的现实 QA 数据集上，D2L 的性能优于标准 CD，同时显着降低峰值内存消耗和更新延迟。我们设想 D2L 可以促进法学硕士的快速适应，开启频繁知识更新和个性化聊天行为的可能性。</li>
</ul>

<h3>Title: DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting</h3>
<ul>
<li><strong>Authors: </strong>Md Mofijul Islam, Md Sirajus Salekin, Nivedha Balakrishnan, Vincil C. Bishop III, Niharika Jain, Spencer Romo, Bob Strahan, Boyi Xie, Diego A. Socolinsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15958">https://arxiv.org/abs/2602.15958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15958">https://arxiv.org/pdf/2602.15958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15958]] DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting(https://arxiv.org/abs/2602.15958)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.</li>
<li><strong>摘要：</strong>现实应用中的文档理解通常需要处理包含多个拼接在一起的文档的异构多页文档包。尽管视觉文档理解方面最近取得了进展，但文档包分割的基本任务（涉及将文档包分成单独的单元）在很大程度上仍未得到解决。我们提出了第一个综合基准数据集 DocSplit，以及用于评估大型语言模型的文档包分割能力的新颖评估指标。 DocSplit 包含五个复杂程度不同的数据集，涵盖不同的文档类型、布局和多模式设置。我们将 DocSplit 任务形式化，该任务需要模型来识别文档边界、对文档类型进行分类并在文档包内维护正确的页面顺序。该基准测试解决了现实世界的挑战，包括无序页面、交错文档以及缺乏明确界限的文档。我们在我们的数据集上进行了广泛的实验来评估多模式法学硕士，揭示了当前模型在处理复杂文档分割任务的能力方面存在显着的性能差距。 DocSplit 基准数据集和提出的新颖评估指标提供了一个系统框架，可提高法律、金融、医疗保健和其他文档密集型领域所必需的文档理解能力。我们发布数据集以促进文档包处理的未来研究。</li>
</ul>

<h3>Title: CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill</h3>
<ul>
<li><strong>Authors: </strong>Bradley McDanel, Steven Li, Harshit Khaitan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16054">https://arxiv.org/abs/2602.16054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16054">https://arxiv.org/pdf/2602.16054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16054]] CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill(https://arxiv.org/abs/2602.16054)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\% compared to the Full KV Cache baseline.</li>
<li><strong>摘要：</strong>长上下文 LLM 推理中的预填充阶段仍然是一个计算瓶颈。最近的令牌排名启发式方法通过有选择地处理语义相关令牌的子集来加速推理。然而，现有的方法存在令牌重要性估计不稳定的问题，并且通常在层之间有所不同。独立于启发式特定架构来评估代币排名质量具有挑战性。为了解决这个问题，我们引入了 Answer-Informed Oracle，它通过测量生成答案对提示的注意力来定义真实标记的重要性。该预言揭示了现有的启发式方法在各层之间表现出很大的差异：特定层的排名可能会急剧下降，这是端到端基准测试不可见的故障模式。诊断结果提出了一个简单的解决方案：跨层汇总分数，而不是依赖于任何单一分数。我们将其实现为跨层注意力聚合（CLAA），与完整 KV 缓存基线相比，它缩小了与预言机上限的差距，并将首次令牌时间（TTFT）减少了 39%。</li>
</ul>

<h3>Title: Surgical Activation Steering via Generative Causal Mediation</h3>
<ul>
<li><strong>Authors: </strong>Aruna Sankaranarayanan, Amir Zur, Atticus Geiger, Dylan Hadfield-Menell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16080">https://arxiv.org/abs/2602.16080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16080">https://arxiv.org/pdf/2602.16080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16080]] Surgical Activation Steering via Generative Causal Mediation(https://arxiv.org/abs/2602.16080)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.</li>
<li><strong>摘要：</strong>我们应该在语言模型 (LM) 的哪里进行干预，以控制分散在长格式响应的许多标记中的行为？我们引入了生成因果中介（GCM），这是一种选择模型组件（例如注意力头）的过程，以从对比的长格式响应中引导二元概念（例如，诗歌中的谈话与散文中的谈话）。在 GCM 中，我们首先构建一个对比输入和响应的数据集。然后，我们量化各个模型组件如何调解对比概念，并选择最强的调解者进行指导。我们在三种语言模型中评估 GCM 的三个任务：拒绝、阿谀奉承和风格迁移。 GCM 成功地定位了以长格式响应表达的概念，并且在使用一组稀疏的注意力头进行转向时始终优于基于相关探针的基线。总之，这些结果表明 GCM 提供了一种有效的方法来定位和控制 LM 的长格式响应。</li>
</ul>

<h3>Title: Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs</h3>
<ul>
<li><strong>Authors: </strong>Sean Trott, Samuel Taylor, Cameron Jones, James A. Michaelov, Pamela D. Rivière</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16085">https://arxiv.org/abs/2602.16085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16085">https://arxiv.org/pdf/2602.16085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16085]] Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs(https://arxiv.org/abs/2602.16085)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.</li>
<li><strong>摘要：</strong>对语言模型 (LM) 中心理状态推理的研究有可能为人类社会认知理论（例如心理状态推理部分源于语言接触的理论）以及我们对 LM 本身的理解提供信息。然而，许多已发表的关于 LM 的工作依赖于相对较小的闭源 LM 样本，限制了我们严格测试心理理论和评估 LM 能力的能力。在这里，我们通过评估 41 个开放权重模型（来自不同的模型系列）的 LM 心理状态推理行为，复制并扩展了已发表的关于错误信念任务的工作。我们发现 34% 的测试 LM 对隐含知识状态敏感；然而，与之前的研究结果一致，没有一个研究能够完全“解释”这种对人类的影响。较大的 LM 表现出更高的敏感性，并且还表现出更高的心理测量预测能力。最后，我们使用 LM 行为来生成并测试关于人类认知的新假设：当使用非事实动词（“约翰认为……”）提示知识状态时，与间接提示（“约翰看着……”）相比，人类和 LM 都表现出倾向于归因错误信念的偏见。与知识状态的主要效应（人类敏感性超过语言模型）不同，人类知识提示效应的大小正好落在语言模型效应大小的分布范围内——这表明语言的分布统计原则上可以解释后者，但不能解释人类的前者。这些结果证明了使用更大的开放权重 LM 样本来测试人类认知理论和评估 LM 能力的价值。</li>
</ul>

<h3>Title: Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Shankar Padmanabhan, Mustafa Omer Gul, Tanya Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16093">https://arxiv.org/abs/2602.16093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16093">https://arxiv.org/pdf/2602.16093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16093]] Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities(https://arxiv.org/abs/2602.16093)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.</li>
<li><strong>摘要：</strong>Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation.不幸的是，现有的解决方案无法同时从适应文档语料库中学习新知识并减少对早期学习能力的遗忘。为了解决这个问题，我们引入了通过分割上下文蒸馏（DiSC），这是一种简单的基于上下文蒸馏的方法，用于持续的知识适应。 \methodname~通过训练示例的不同部分进行条件导出学生和教师分布，并最小化共享标记之间的 KL 散度。 This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains.与之前用于持续适应的微调和蒸馏方法相比，DiSC 始终如一地报告了学习新知识和减少忘记先前学习的技能（例如遵循指令、推理和事实知识）之间的最佳权衡。</li>
</ul>

<h3>Title: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution</h3>
<ul>
<li><strong>Authors: </strong>Nithin Sivakumaran, Shoubin Yu, Hyunji Lee, Yue Zhang, Ali Payani, Mohit Bansal, Elias Stengel-Eskin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16154">https://arxiv.org/abs/2602.16154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16154">https://arxiv.org/pdf/2602.16154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16154]] Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution(https://arxiv.org/abs/2602.16154)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who "execute" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.</li>
<li><strong>摘要：</strong>思想链 (CoT) 推理有时无法忠实地反映大型语言模型 (LLM) 的真实计算，从而妨碍了其在解释 LLM 如何得出答案时的实用性。此外，针对推理的忠实性和可解释性进行优化通常会降低任务性能。为了解决这种权衡问题并提高 CoT 的可信度，我们提出了多监听器推理执行 (REMUL)，这是一种多方强化学习方法。 REMUL 建立在这样的假设之上：其他方可以遵循的推理痕迹将更加忠实。说话者模型生成推理跟踪，该跟踪被截断并传递给“执行”跟踪的侦听器模型池，继续跟踪直到答案。演讲者因提出听众清晰的推理而获得奖励，并通过屏蔽监督微调进行额外的正确性正则化，以抵消忠实度和表现之间的权衡。在多个推理基准（BIG-Bench Extra Hard、MuSR、ZebraLogicBench 和 FOLIO）上，REMUL 一致且显着地改进了三个忠实度指标——提示归因、早期回答曲线上的面积 (AOC) 和错误注入 AOC——同时还提高了准确性。我们的分析发现，这些收益在整个训练领域都很强大，可以转化为易读性收益，并且与更短、更直接的 CoT 相关。</li>
</ul>

<h3>Title: LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers</h3>
<ul>
<li><strong>Authors: </strong>Peiqi Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16162">https://arxiv.org/abs/2602.16162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16162">https://arxiv.org/pdf/2602.16162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16162]] LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers(https://arxiv.org/abs/2602.16162)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the "uncertainty gap" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.</li>
<li><strong>摘要：</strong>我们认为，不确定性是法学硕士在创意写作方面表现的一个关键且尚未得到充分研究的限制，而创意写作通常被认为是陈词滥调和陈词滥调。文学理论将不确定性视为创造性表达的必要条件，而当前的调整策略则引导模型远离不确定的输出，以确保事实性并减少幻觉。我们通过量化人类创作的故事和模型生成的延续之间的“不确定性差距”来形式化这种紧张关系。通过对 28 名法学硕士在高质量讲故事数据集上进行的受控信息理论分析，我们证明人类写作始终表现出比模型输出更高的不确定性。我们发现，与基础模型相比，指令调整和推理模型加剧了这一趋势；此外，创意写作中的差距比功能领域中的差距更为明显，并且与写作质量密切相关。实现人类水平的创造力需要新的不确定性感知对齐范式，该范式可以区分破坏性幻觉和文学丰富性所需的建设性模糊性。</li>
</ul>

<h3>Title: Beyond Learning: A Training-Free Alternative to Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Namkyung Yoon, Kyeonghyun Yoo, Wooyong Jung, Sanghong Kim, Hwangnam Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16189">https://arxiv.org/abs/2602.16189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16189">https://arxiv.org/pdf/2602.16189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16189]] Beyond Learning: A Training-Free Alternative to Model Adaptation(https://arxiv.org/abs/2602.16189)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.</li>
<li><strong>摘要：</strong>尽管语言模型不断研究和发展，但它们有时表现不如以前的版本。克服这些挑战的现有方法需要大量资源，因此需要能够立即采取行动的替代方案。我们假设每个语言模型内部都有一个适合特定功能的本地模块。 First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis.随后，我们将针对特定任务正确激活的内部模块移植到目标模型中，从而导致立即且可测量的功能变化，而无需额外的培训或微调。 To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models.总的来说，这项工作为语言模型中的任务本地化模块化提供了经验证据，并提出了一个新的研究领域：模型移植。</li>
</ul>

<h3>Title: The Validity of Coreference-based Evaluations of Natural Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ian Porada</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16200">https://arxiv.org/abs/2602.16200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16200">https://arxiv.org/pdf/2602.16200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16200]] The Validity of Coreference-based Evaluations of Natural Language Understanding(https://arxiv.org/abs/2602.16200)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.</li>
<li><strong>摘要：</strong>在本文中，我通过扩展现有的评估实践并考虑评估结果趋同或冲突的程度，完善了我们对基于共指的评估可以得出哪些结论的理解。首先，我分析了标准共指评估，并表明，由于测量有效性问题，它们的设计通常会导致不可概括的结论，包括竞争性（共指的多个、相互竞争的定义）和收敛有效性（跨基准对模型进行不同排名的评估结果）。其次，我提出并实施了一项新颖的评估，重点关注测试系统推断事件相对合理性的能力，这是解决共指问题的一个关键方面。通过这种扩展的评估，我发现当代语言模型在标准基准上表现出强大的性能——在某些领域和共指类型中比早期的基线系统有所改进——但对评估条件仍然敏感：当评估上下文稍作修改时，它们通常无法以人们期望人类能够做到的方式进行概括。总而言之，这些发现阐明了当前 NLP 范式的优势（例如相对于广泛使用的评估的基线的准确性有所提高）和当前 NLP 范式的局限性（包括测量有效性的弱点），并为未来开发更好的评估方法和更真正的可推广系统的工作提出了方向。</li>
</ul>

<h3>Title: Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications</h3>
<ul>
<li><strong>Authors: </strong>Sanket Badhe, Deep Shah, Nehal Kathrotia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16201">https://arxiv.org/abs/2602.16201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16201">https://arxiv.org/pdf/2602.16201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16201]] Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications(https://arxiv.org/abs/2602.16201)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives. We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.</li>
<li><strong>摘要：</strong>Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently.虽然扩展提高了平均情况下的性能，但低频、特定领域、文化和时间知识的持续失败仍然没有得到很好的描述。本文开发了大型语言模型中长尾知识的结构化分类法和分析，综合了技术和社会技术角度的先前工作。我们引入了一个结构化的分析框架，该框架综合了四个互补轴上的先前工作：长尾知识的定义、在训练和推理过程中丢失或扭曲的机制、为减轻这些失败而提出的技术干预措施，以及这些失败对公平性、问责制、透明度和用户信任的影响。我们进一步研究现有的评估实践如何掩盖尾部行为，并使罕见但后果严重的失败的责任变得复杂。本文最后确定了与隐私、可持续性和治理相关的限制长尾知识表示的开放挑战。总而言之，本文提供了一个统一的概念框架，用于理解长尾知识是如何在已部署的语言模型系统中定义、丢失、评估和体现的。</li>
</ul>

<h3>Title: Are LLMs Ready to Replace Bangla Annotators?</h3>
<ul>
<li><strong>Authors: </strong>Md. Najib Hasan, Touseef Hasan, Souvika Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16241">https://arxiv.org/abs/2602.16241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16241">https://arxiv.org/pdf/2602.16241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16241]] Are LLMs Ready to Replace Bangla Annotators?(https://arxiv.org/abs/2602.16241)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.</li>
<li><strong>摘要：</strong>Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences.我们使用统一的评估框架对 17 个法学硕士进行了系统基准测试。我们的分析揭示了注释者偏差和模型判断的实质性不稳定性。 Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.</li>
</ul>

<h3>Title: Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Mutal, Perla Al Almaoui, Simon Hengchen, Pierrette Bouillon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16290">https://arxiv.org/abs/2602.16290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16290">https://arxiv.org/pdf/2602.16290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16290]] Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation(https://arxiv.org/abs/2602.16290)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.</li>
<li><strong>摘要：</strong>由于阿拉伯方言的非标准化和高变异性，长期以来在自然语言处理（NLP）研究中代表性不足，这给计算建模带来了挑战。该领域的最新进展，例如大型语言模型（LLM），通过使阿拉伯语能够被建模为多中心语言而不是单一系统，为解决这一差距提供了有希望的途径。本文介绍了我们向 AMIYA 共享任务提交的 Aladdin-FTI。所提出的系统旨在生成和翻译阿拉伯语方言（DA）。具体来说，该模型支持摩洛哥、埃及、巴勒斯坦、叙利亚和沙特方言的文本生成，以及这些方言、现代标准阿拉伯语 (MSA) 和英语之间的双向翻译。代码和训练模型是公开的。</li>
</ul>

<h3>Title: MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models</h3>
<ul>
<li><strong>Authors: </strong>Martin Hyben, Sebastian Kula, Jan Cegin, Jakub Simko, Ivan Srba, Robert Moro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16298">https://arxiv.org/abs/2602.16298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16298">https://arxiv.org/pdf/2602.16298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16298]] MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models(https://arxiv.org/abs/2602.16298)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 开始重塑媒体专业人士验证信息的方式，但对检测值得检查的声明（事实检查过程中的关键步骤）的自动支持仍然有限。我们引入了 Multi-Check-Worthy (MultiCW) 数据集，这是一个平衡的多语言基准，用于跨 16 种语言、7 个主题领域和 2 种写作风格的值得检查的声明检测。它由 123,722 个样本组成，均匀分布在噪声（非正式）和结构化（正式）文本之间，并在所有语言中均衡地表示值得检查和不值得检查的类。为了探讨稳健性，我们还引入了一个同样平衡的分布外评估集，包含 4 种其他语言的 27,761 个样本。为了提供基准，我们在零样本设置下针对 15 个不同的商业和开放法学硕士对 3 个常见的微调多语言转换器进行了基准测试。我们的研究结果表明，经过微调的模型在权利要求分类方面始终优于零样本法学硕士，并且在跨语言、领域和风格方面表现出强大的分布外泛化能力。 MultiCW 为推进自动化事实检查提供了严格的多语言资源，并能够在值得检查的索赔检测任务上对微调模型和前沿法学硕士进行系统比较。</li>
</ul>

<h3>Title: MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zexue He, Yu Wang, Churan Zhi, Yuanzhe Hu, Tzu-Ping Chen, Lang Yin, Ze Chen, Tong Arthur Wu, Siru Ouyang, Zihan Wang, Jiaxin Pei, Julian McAuley, Yejin Choi, Alex Pentland</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16313">https://arxiv.org/abs/2602.16313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16313">https://arxiv.org/pdf/2602.16313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16313]] MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks(https://arxiv.org/abs/2602.16313)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.</li>
<li><strong>摘要：</strong>现有的对记忆主体的评估通常是孤立地评估记忆和行动。一类基准通过测试对过去对话或文本的回忆来评估记忆力，但无法捕捉记忆如何用于指导未来的决策。另一类侧重于在不需要长期记忆的情况下执行单会话任务的代理。然而，在现实环境中，记忆和行动是紧密耦合的：智能体在与环境交互时获取记忆，然后依靠该记忆来解决未来的任务。为了捕获此设置，我们引入了 MemoryArena，这是一个统一的评估工具，用于在多会话内存-代理-环境循环中对代理内存进行基准测试。该基准由人工设计的代理任务和明确相互依赖的子任务组成，其中代理必须通过将经验提取到内存中来从早期的操作和反馈中学习，然后使用该内存来指导后续的操作来解决整个任务。 MemoryArena 支持跨网络导航、偏好约束规划、渐进式信息搜索和顺序形式推理的评估，并揭示了在现有长上下文记忆基准（如 LoCoMo）上性能接近饱和的智能体在我们的智能体环境中表现不佳，暴露了当前对具有记忆的智能体评估中的差距。</li>
</ul>

<h3>Title: Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Nivya Talokar, Ayush K Tarun, Murari Mandal, Maksym Andriushchenko, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16346">https://arxiv.org/abs/2602.16346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16346">https://arxiv.org/pdf/2602.16346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16346]] Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents(https://arxiv.org/abs/2602.16346)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.</li>
<li><strong>摘要：</strong>基于法学硕士的代理通过工具和内存执行现实世界的工作流程。这些可供性使居心不良的对手也能够使用这些代理来执行复杂的滥用场景。现有的代理滥用基准主要测试单提示指令，在衡量代理最终如何在多个回合中帮助完成有害或非法任务方面存在差距。 We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion.我们进一步介绍了一个分析框架，将多回合红队建模为首次越狱时间随机变量，支持发现曲线、攻击语言的危险比归因和新指标：受限平均越狱发现等分析工具。在 AgentHarm 场景中，与适用于使用工具的代理的单轮提示和面向聊天的多轮基线相比，STING 产生的非法任务完成率要高得多。 In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings.总体而言，STING 提供了一种实用的方法来评估和压力测试代理在实际部署环境中的滥用情况，其中交互本质上是多轮且通常是多语言的。</li>
</ul>

<h3>Title: Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Mohammad H.A. Monfared, Lucie Flek, Akbar Karimi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16379">https://arxiv.org/abs/2602.16379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16379">https://arxiv.org/pdf/2602.16379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16379]] Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents(https://arxiv.org/abs/2602.16379)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.</li>
<li><strong>摘要：</strong>我们提出了一种用于基于方面的情感分析（ABSA）的代理数据增强方法，该方法使用迭代生成和验证来生成高质量的合成训练示例。为了隔离主体结构的影响，我们还使用相同的模型和指令开发了一个紧密匹配的基于提示的基线。这两种方法都通过三个 ABSA 子任务（方面术语提取 (ATE)、方面情感分类 (ATSC) 和方面情感对提取 (ASPE)）、四个 SemEval 数据集和两个编码器-解码器模型：T5-Base 和 Tk-Instruct 进行评估。我们的结果表明，代理增强在增强数据的标签保存方面优于原始提示，特别是当任务需要方面术语生成时。此外，当与真实数据结合时，代理增强可以提供更高的收益，始终优于基于提示的生成。这些优势对于 T5-Base 最为明显，而经过更严格的预训练的 Tk-Instruct 则表现出较小的改进。因此，增强数据有助于 T5-Base 实现与其对应产品相当的性能。</li>
</ul>

<h3>Title: TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Ido Levy, Eilam Shapira, Yinon Goldshtein, Avi Yaeli, Nir Mashkif, Segev Shlomov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16429">https://arxiv.org/abs/2602.16429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16429">https://arxiv.org/pdf/2602.16429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16429]] TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers(https://arxiv.org/abs/2602.16429)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.</li>
<li><strong>摘要：</strong>代理系统是自动执行多步骤工作流程以实现复杂目标的人工智能架构，通常使用重复的大语言模型 (LLM) 调用来构建封闭集决策任务，例如路由、入围、门控和验证。虽然方便，但由于累积延迟和令牌使用，这种设计使部署变得缓慢且昂贵。我们提出了 TabAgent，这是一个框架，用于用在执行跟踪上训练的紧凑文本表格分类器替换封闭集选择任务中的生成决策组件。 TabAgent (i) 从轨迹中提取结构化模式、状态和依赖性特征 (TabSchema)，(ii) 通过模式对齐的综合监督 (TabSynth) 扩大覆盖范围，(iii) 使用轻量级分类器 (TabHead) 对候选者进行评分。在长期 AppWorld 基准测试中，TabAgent 保持了任务级成功，同时消除了入围时间的 LLM 调用，将延迟减少了约 95%，推理成本减少了 85-91%。除了工具入围之外，TabAgent 还可以推广到其他代理决策头，为生产代理架构中的生成瓶颈的学习判别性替换建立一个范例。</li>
</ul>

<h3>Title: IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Bharti, Gaurav Azad, Abhinaw Jagtap, Nachiket Tapas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16467">https://arxiv.org/abs/2602.16467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16467">https://arxiv.org/pdf/2602.16467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16467]] IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models(https://arxiv.org/abs/2602.16467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.</li>
<li><strong>摘要：</strong>The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages.在 Gemini 2.0 Flash、GPT-4、Claude 和 LLaMA 3-70B 上进行的实验揭示了三个主要发现。首先，CoT 提示持续提高推理准确性，在各个学科和语言方面都取得了显着的进步。其次，显着的跨模型性能差异仍然存在，特别是在高复杂性检查中。 Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions.这些结果凸显了双语推理和领域转移方面持续存在的差距。 Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.</li>
</ul>

<h3>Title: Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning</h3>
<ul>
<li><strong>Authors: </strong>Jenny Kunz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16469">https://arxiv.org/abs/2602.16469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16469">https://arxiv.org/pdf/2602.16469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16469]] Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning(https://arxiv.org/abs/2602.16469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.</li>
<li><strong>摘要：</strong>机器翻译数据广泛用于多语言 NLP，特别是在原生文本稀缺的情况下。然而，翻译文本与母语文本有系统的不同。 This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.</li>
</ul>

<h3>Title: Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling</h3>
<ul>
<li><strong>Authors: </strong>Jeffrey T. H. Wong, Zixi Zhang, Junyi Liu, Yiren Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16485">https://arxiv.org/abs/2602.16485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16485">https://arxiv.org/pdf/2602.16485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16485]] Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling(https://arxiv.org/abs/2602.16485)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.</li>
<li><strong>摘要：</strong>现有的多智能体系统（MAS）通常依赖于静态、同质的模型配置，限制了它们利用不同后训练模型的独特优势的能力。为了解决这个问题，我们引入了 Team-of-Thoughts，这是一种新颖的 MAS 架构，它通过协调器工具范例利用异构代理的互补功能。我们的框架引入了两个关键机制来优化性能：(1) 协调器校准方案，用于识别具有卓越协调能力的模型；(2) 自我评估协议，工具代理可以在其中描述自己的领域专业知识，以考虑培训后技能的变化。在推理过程中，协调器根据这些熟练程度动态激活最合适的工具代理。对五个推理和代码生成基准的实验表明，Team-of-Thoughts 始终提供卓越的任务性能。值得注意的是，在 AIME24 和 LiveCodeBench 上，我们的方法分别实现了 96.67% 和 72.53% 的准确率，大大优于同质角色扮演基线（得分为 80% 和 65.93%）。</li>
</ul>

<h3>Title: Learning to Learn from Language Feedback with Social Meta-Learning</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Cook, Diego Antognini, Martin Klissarov, Claudiu Musat, Edward Grefenstette</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16488">https://arxiv.org/abs/2602.16488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16488">https://arxiv.org/pdf/2602.16488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16488]] Learning to Learn from Language Feedback with Social Meta-Learning(https://arxiv.org/abs/2602.16488)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常很难从对话环境中的纠正反馈中学习。即使面临歧义，他们也很少主动征求这种反馈，这会让他们的对话感觉静态、片面，并且缺乏人类对话的适应性。为了解决这些局限性，我们从人类的社交元学习（SML）中汲取灵感，即学习如何向他人学习的过程。 We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML 有效地教导模型使用对话来解决单轮无法解决的问题。这种能力可以跨领域推广；数学问题上的 SML 生成的模型可以更好地利用反馈来解决编码问题，反之亦然。此外，尽管仅针对完全指定的问题进行训练，但这些模型能够更好地解决未指定的任务，其中关键信息会在多个回合中揭示。当面对这种模糊性时，SML 训练的模型会减少过早的回答尝试，并且更有可能询问他们需要的信息。这项工作提出了一种可扩展的方法来开发能够有效地从语言反馈中学习的人工智能系统。</li>
</ul>

<h3>Title: From Growing to Looping: A Unified View of Iterative Computation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ferdinand Kapl, Emmanouil Angelis, Kaitlin Maile, Johannes von Oswald, Stefan Bauer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16490">https://arxiv.org/abs/2602.16490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16490">https://arxiv.org/pdf/2602.16490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16490]] From Growing to Looping: A Unified View of Iterative Computation in LLMs(https://arxiv.org/abs/2602.16490)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.</li>
<li><strong>摘要：</strong>循环、跨深度重用层块，以及深度增长、通过复制中间层来训练从浅到深的模型，都与更强的推理有关，但它们的关系仍不清楚。我们提供了一种机械统一：循环和深度生长模型表现出收敛的深度特征，包括对后期层的依赖增加以及与循环或生长块对齐的重复模式。这些共享签名支持这样的观点：它们的收益源于迭代计算的通用形式。 Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\times$, despite the model never being trained to loop.当给出更多的上下文示例或额外的监督微调数据时，这两种方法也比基线更好地适应。 Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.</li>
</ul>

<h3>Title: Optimizing Soft Prompt Tuning via Structural Evolution</h3>
<ul>
<li><strong>Authors: </strong>Zhenzhen Huang, Chaoning Zhang, Haoyu Bian, Songbo Zhang, Chi-lok Andy Tai, Jiaquan Zhang, Caiyan Qin, Jingjing Qu, Yalan Ye, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16500">https://arxiv.org/abs/2602.16500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16500">https://arxiv.org/pdf/2602.16500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16500]] Optimizing Soft Prompt Tuning via Structural Evolution(https://arxiv.org/abs/2602.16500)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.</li>
<li><strong>摘要：</strong>软提示调优利用连续嵌入来捕获大型预训练语言模型 (LLM) 中的特定于任务的信息，从而在少量设置中实现具有竞争力的性能。然而，软提示依赖于高维、隐式表示，缺乏显式语义和可追溯的训练行为，这限制了它们的可解释性。为了解决这个限制，我们提出了一种基于拓扑形态演化的软提示调整优化方法。 Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution.定量分析表明，拓扑稳定且紧凑的软提示可以获得更好的下游性能。 Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss 通过量化参数间的连接性和冗余来指导模型学习结构稳定的适应。 Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.</li>
</ul>

<h3>Title: Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification</h3>
<ul>
<li><strong>Authors: </strong>Taja Kuzman Pungeršek, Peter Rupnik, Daniela Širinić, Nikola Ljubešić</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16516">https://arxiv.org/abs/2602.16516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16516">https://arxiv.org/pdf/2602.16516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16516]] Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification(https://arxiv.org/abs/2602.16516)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.</li>
<li><strong>摘要：</strong>本文介绍了 ParlaCAP，一个用于分析整个欧洲议会议程设置的大型数据集，并提出了一种用于构建特定领域政策主题分类器的经济有效的方法。我们将比较议程项目 (CAP) 模式应用于包含欧洲 28 个国家和自治区议会超过 800 万篇演讲的多语言 ParlaMint 语料库，遵循师生框架，其中高性能大语言模型 (LLM) 注释域内训练数据，并根据这些注释对多语言编码器模型进行微调，以实现可扩展的数据注释。我们证明这种方法可以生成适合目标域的分类器。 LLM 和人类注释者之间的一致性与人类之间注释者之间的一致性相当，并且生成的模型优于基于手动注释但域外数据训练的现有 CAP 分类器。除了 CAP 注释之外，ParlaCAP 数据集还提供丰富的演讲者和政党元数据，以及来自 ParlaSent 多语言转换器模型的情绪预测，从而能够对各国的政治关注度和代表性进行比较研究。我们通过三个用例说明了该数据集的分析潜力，检查了议会对政策主题的关注分布、议会演讲中的情绪模式以及政策关注中的性别差异。</li>
</ul>

<h3>Title: Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset</h3>
<ul>
<li><strong>Authors: </strong>Zhuqian Zhou, Kirk Vanacore, Bakhtawar Ahtisham, Jinsook Lee, Doug Pietrzak, Daryl Hedley, Jorge Dias, Chris Shaw, Ruth Schäfer, René F. Kizilcec</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16571">https://arxiv.org/abs/2602.16571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16571">https://arxiv.org/pdf/2602.16571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16571]] Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset(https://arxiv.org/abs/2602.16571)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the "numeric ambiguity" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.</li>
<li><strong>摘要：</strong>基于对话的数据的大规模共享有助于推进教学科学，但严格的去识别化仍然是一个主要障碍。在数学辅导成绩单中，数字表达式经常类似于结构化标识符（例如日期或 ID），导致通用个人身份信息 (PII) 检测系统过度编辑核心教学内容并降低数据集效用。这项工作探讨如何在数学辅导成绩单中检测 PII，同时保留其教育效用。为了应对这一挑战，我们研究了“数字歧义”问题，并引入了 MathEd-PII，这是数学辅导对话中 PII 检测的第一个基准数据集，它是通过人机交互 LLM 工作流程创建的，该工作流程审核上游修订并生成保护隐私的代理。该数据集包含 1,000 个辅导课程（115,620 条消息；769,628 个令牌）以及经过验证的 PII 注释。使用基于密度的分割方法，我们发现错误的 PII 编辑不成比例地集中在数学密集区域，证实数字模糊性是关键的故障模式。然后，我们比较四种检测策略：Presidio 基线和基于 LLM 的方法，以及基本的、数学感知的和分段感知的提示。数学感知提示显着提高了基线性能（F1：0.821 vs. 0.379），同时减少了数字误报，表明去识别必须结合域上下文以保留分析实用性。这项工作提供了一个新的基准和证据，证明辅导数据的效用保留去识别需要领域感知建模。</li>
</ul>

<h3>Title: CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes</h3>
<ul>
<li><strong>Authors: </strong>Miguel Marques, Ana Luísa Fernandes, Ana Filipa Pacheco, Rute Rebouças, Inês Cantante, José Isidro, Luís Filipe Cunha, Alípio Jorge, Nuno Guimarães, Sérgio Nunes, António Leal, Purificação Silvano, Ricardo Campos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16607">https://arxiv.org/abs/2602.16607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16607">https://arxiv.org/pdf/2602.16607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16607]] CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes(https://arxiv.org/abs/2602.16607)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.</li>
<li><strong>摘要：</strong>市政会议纪要是记录地方政府讨论和决策的正式记录，但其内容往往冗长、密集，公民难以理解。自动摘要可以通过为每个讨论主题生成简洁的摘要来帮助解决这一挑战。尽管有潜力，但在市政会议纪要中总结讨论主题的研究在很大程度上仍未得到探索，特别是在资源匮乏的语言中，这些文件固有的复杂性增加了进一步的挑战。主要瓶颈是缺乏包含高质量、手工制作摘要的数据集，这限制了该领域有效摘要模型的开发和评估。在本文中，我们提出了 CitiLink-Summ，这是一个新的欧洲葡萄牙市政会议纪要语料库，其中包含 100 份文档和 2,322 份手写摘要，每份摘要对应一个不同的讨论主题。利用该数据集，我们建立了该领域自动摘要的基线结果，采用最先进的生成模型（例如 BART、PRIMERA）以及大型语言模型（LLM），并使用词汇和语义指标（例如 ROUGE、BLEU、METEOR 和 BERTScore）进行评估。 CitiLink-Summ 提供了欧洲葡萄牙语市政领域摘要的第一个基准，为推进复杂行政文本的 NLP 研究提供了宝贵的资源。</li>
</ul>

<h3>Title: Who can we trust? LLM-as-a-jury for Comparative Assessment</h3>
<ul>
<li><strong>Authors: </strong>Mengjie Qian, Guangzhi Sun, Mark J.F. Gales, Kate M. Knill</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16610">https://arxiv.org/abs/2602.16610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16610">https://arxiv.org/pdf/2602.16610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16610]] Who can we trust? LLM-as-a-jury for Comparative Assessment(https://arxiv.org/abs/2602.16610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用作自然语言生成评估的自动评估器，通常使用成对比较判断。现有的方法通常依赖于单个法官或假设具有相同可靠性的多个法官的聚合。在实践中，LLM法官在不同任务和方面的表现差异很大，他们的判断概率可能存在偏见和不一致。此外，用于法官校准的人工标记监督可能不可用。我们首先凭经验证明 LLM 比较概率存在不一致，并表明它限制了基于概率的直接排名的有效性。为了解决这个问题，我们研究了法学硕士作为陪审团的设置，并提出了 BT-sigma，这是 Bradley-Terry 模型的法官感知扩展，它为每个法官引入了一个判别器参数，以共同推断项目排名并仅通过成对比较来判断可靠性。在基准 NLG 评估数据集上的实验表明，BT-sigma 始终优于基于平均的聚合方法，并且学习的判别器与 LLM 判断的循环一致性的独立度量密切相关。进一步分析表明，BT-sigma 可以解释为一种无监督校准机制，通过建模判断可靠性来提高聚合。</li>
</ul>

<h3>Title: AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Adib Sakhawat, Fardeen Sadab</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16639">https://arxiv.org/abs/2602.16639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16639">https://arxiv.org/pdf/2602.16639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16639]] AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models(https://arxiv.org/abs/2602.16639)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($\rho = 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.</li>
<li><strong>摘要：</strong>评估大型语言模型 (LLM) 的社交智能越来越需要超越静态文本生成，转向动态的对抗性交互。我们引入了对抗性资源抽取博弈（AREG），这是一个基准，将说服和阻力作为针对财务资源的多轮、零和谈判进行操作。 Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($\rho = 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa.在所有评估的模型中，阻力分数超过说服分数，表明对抗性对话环境中的系统防御优势。 Further linguistic analysis suggests that interaction structure plays a central role in these outcomes.增量承诺寻求策略与更高的提取成功率相关，而寻求验证响应在成功防御中比明确拒绝更普遍。总之，这些发现表明法学硕士的社会影响力并不是一种单一的能力，仅注重说服力的评估框架可能会忽视不对称的行为脆弱性。</li>
</ul>

<h3>Title: Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Subrit Dikshit</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16640">https://arxiv.org/abs/2602.16640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16640">https://arxiv.org/pdf/2602.16640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16640]] Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval(https://arxiv.org/abs/2602.16640)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a "resource divide." State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes "lexical density" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速普及彻底改变了自然语言处理 (NLP)，但同时也造成了“资源鸿沟”。最先进的法律情报系统通常依赖于大量参数计数（7B+）和基于云的推理，这使得资源有限的环境中的从业者无法访问它们，并带来重大的数据主权风险。本文介绍了 Quecto-V1，这是一种特定领域的小语言模型 (SLM)，旨在实现印度法律情报获取的民主化。 Quecto-V1 基于 GPT-2 架构的自定义配置（1.24 亿个参数）构建，专门针对印度刑法典 (IPC)、刑事诉讼法典 (CrPC) 和印度宪法等一系列印度法规从头开始进行训练。与优先考虑广泛的世界知识的通才模型不同，我们的方法最大限度地提高了法律领域内的“词汇密度”。此外，我们通过应用训练后 8 位量化（GGUF 格式）来解决部署瓶颈，将模型压缩到内存占用量低于 150 MB。我们的实证分析表明，Quecto-V1 在检索法定定义和刑罚条款方面实现了高保真度，在特定领域的精确匹配任务中优于通用 SLM，同时在消费级 CPU 上完全离线运行。我们进一步提出了一项消融研究，表明与全精度基线相比，8 位量化可使模型大小减少 74%，而检索精度下降不到 3.5%。这些发现表明，对于法律等专门的高风险领域，特定领域的训练与积极的量化相结合，为整体云模型提供了可行的、保护隐私的替代方案。</li>
</ul>

<h3>Title: Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuyan Bu, Xiaohao Liu, ZhaoXing Ren, Yaodong Yang, Juntao Dai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16660">https://arxiv.org/abs/2602.16660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16660">https://arxiv.org/pdf/2602.16660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16660]] Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment(https://arxiv.org/abs/2602.16660)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 跨语言社区的广泛部署需要可靠的多语言安全协调。然而，最近将对齐扩展到其他语言的努力通常需要大量资源，要么通过目标语言的大规模、高质量监督，要么通过与高资源语言的成对对齐，这限制了可扩展性。在这项工作中，我们提出了一种提高多语言安全一致性的资源有效方法。我们引入了一种即插即用的多语言一致性（MLC）损失，可以集成到现有的单语言对齐管道中。 By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.</li>
</ul>

<h3>Title: Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Ding, Nicholas Tomlin, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16699">https://arxiv.org/abs/2602.16699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16699">https://arxiv.org/pdf/2602.16699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16699]] Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents(https://arxiv.org/abs/2602.16699)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.</li>
<li><strong>摘要：</strong>法学硕士越来越多地用于解决复杂的问题，这些问题不一定能通过单一响应得到解决，而是需要与环境交互来获取信息。在这些情况下，法学硕士必须对固有的成本不确定性权衡进行推理，决定何时停止探索并致力于解决问题。例如，在编程任务中，如果不确定代码的正确性，法学硕士应该测试生成的代码片段； the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.</li>
</ul>

<h3>Title: Reinforced Fast Weights with Next-Sequence Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga Russakovsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16704">https://arxiv.org/abs/2602.16704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16704">https://arxiv.org/pdf/2602.16704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16704]] Reinforced Fast Weights with Next-Sequence Prediction(https://arxiv.org/abs/2602.16704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.</li>
<li><strong>摘要：</strong>快速权重架构通过保持恒定的内存开销（无论上下文长度如何），为基于注意力的变压器提供了一种有前景的替代方案，用于长上下文建模。然而，它们的潜力受到下一个令牌预测（NTP）训练范例的限制。 NTP 优化单标记预测并忽略前缀后的多个标记之间的语义一致性。 Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE 根据预测熵选择信息丰富的代币位置，生成多代币部署，分配自我监督的序列级奖励，并通过组相对策略优化 (GRPO) 来优化模型。 REFINE 适用于预训练语言模型的整个训练生命周期：训练中、训练后和测试时训练。我们在 LaCT-760M 和 DeltaNet-1.3B 上的实验表明，在大海捞针检索、长上下文问答和 LongBench 中的各种任务中，REFINE 始终优于 NTP 监督微调。 REFINE 提供了一个有效且通用的框架，用于改进快速权重架构中的长上下文建模。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
