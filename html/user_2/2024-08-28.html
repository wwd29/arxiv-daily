<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-28</h1>
<h3>Title: Improving Clinical Note Generation from Complex Doctor-Patient Conversation</h3>
<ul>
<li><strong>Authors: </strong>Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14568">https://arxiv.org/abs/2408.14568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14568">https://arxiv.org/pdf/2408.14568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14568]] Improving Clinical Note Generation from Complex Doctor-Patient Conversation(https://arxiv.org/abs/2408.14568)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Writing clinical notes and documenting medical exams is a critical task for healthcare professionals, serving as a vital component of patient care documentation. However, manually writing these notes is time-consuming and can impact the amount of time clinicians can spend on direct patient interaction and other tasks. Consequently, the development of automated clinical note generation systems has emerged as a clinically meaningful area of research within AI for health. In this paper, we present three key contributions to the field of clinical note generation using large language models (LLMs). First, we introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex doctor-patient conversations paired with their full clinical notes. This dataset, created and curated by medical experts with the help of modern neural networks, provides a valuable resource for training and evaluating models in clinical note generation tasks. Second, we propose the K-SOAP (Keyword, Subjective, Objective, Assessment, and Plan) note format, which enhances traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and Plan) notes by adding a keyword section at the top, allowing for quick identification of essential information. Third, we develop an automatic pipeline to generate K-SOAP notes from doctor-patient conversations and benchmark various modern LLMs using various metrics. Our results demonstrate significant improvements in efficiency and performance compared to standard LLM finetuning methods.</li>
<li><strong>摘要：</strong>撰写临床笔记和记录医学检查是医疗专业人员的一项重要任务，是患者护理文档的重要组成部分。但是，手动撰写这些笔记非常耗时，并且会影响临床医生花在直接患者互动和其他任务上的时间。因此，开发自动化临床笔记生成系统已成为医疗人工智能领域具有临床意义的研究领域。在本文中，我们介绍了使用大型语言模型 (LLM) 对临床笔记生成领域的三个关键贡献。首先，我们介绍了 CliniKnote，这是一个综合数据集，包含 1,200 份复杂的医患对话及其完整的临床笔记。该数据集由医学专家在现代神经网络的帮助下创建和整理，为临床笔记生成任务中的模型训练和评估提供了宝贵的资源。其次，我们提出了 K-SOAP（关键词、主观、客观、评估和计划）笔记格式，通过在顶部添加关键词部分增强了传统的 SOAP~\cite{podder2023soap}（主观、客观、评估和计划）笔记，从而可以快速识别重要信息。第三，我们开发了一个自动流程，从医患对话中生成 K-SOAP 笔记，并使用各种指标对各种现代 LLM 进行基准测试。与标准 LLM 微调方法相比，我们的结果表明效率和性能有显著提高。</li>
</ul>

<h3>Title: Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ian Stewart, Sameera Horawalavithana, Brendan Kennedy, Sai Munikoti, Karl Pazdernik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14595">https://arxiv.org/abs/2408.14595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14595">https://arxiv.org/pdf/2408.14595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14595]] Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models(https://arxiv.org/abs/2408.14595)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Multimodal foundation models (MFMs) such as OFASys show the potential to unlock analysis of complex data such as images, videos, and audio data via text prompts alone. However, their performance may suffer in the face of text input that differs even slightly from their training distribution, which is surprising considering the use of modality-specific data to "ground" the text input. This study demonstrates that prompt instability is a major concern for MFMs, leading to a consistent drop in performance across all modalities, but that instability can be mitigated with additional training with augmented data. We evaluate several methods for grounded prompt perturbation, where we generate perturbations and filter based on similarity to text and/or modality data. After re-training the models on the augmented data, we find improved accuracy and more stable performance on the perturbed test data regardless of perturbation condition, suggesting that the data augmentation strategy helps the models handle domain shifts more effectively. In error analysis, we find consistent patterns of performance improvement across domains, suggesting that retraining on prompt perturbations tends to help general reasoning capabilities in MFMs.</li>
<li><strong>摘要：</strong>多模态基础模型 (MFM)（例如 OFASys）显示出仅通过文本提示即可解锁对复杂数据（例如图像、视频和音频数据）进行分析的潜力。但是，当文本输入与其训练分布略有不同时，它们的性能可能会受到影响，考虑到使用特定于模态的数据来“确定”文本输入，这是令人惊讶的。这项研究表明，提示不稳定性是 MFM 的主要问题，导致所有模态的性能持续下降，但可以通过使用增强数据进行额外训练来缓解这种不稳定性。我们评估了几种针对基础提示扰动的方法，其中我们生成扰动并根据与文本和/或模态数据的相似性进行过滤。在增强数据上重新训练模型后，我们发现无论扰动条件如何，扰动测试数据的准确性都有所提高，性能也更稳定，这表明数据增强策略有助于模型更有效地处理域转换。在错误分析中，我们发现跨领域的性能改进存在一致的模式，这表明对提示扰动进行重新训练往往有助于提高 MFM 中的一般推理能力。</li>
</ul>

<h3>Title: What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Dingyi Yang, Qin Jin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14622">https://arxiv.org/abs/2408.14622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14622">https://arxiv.org/pdf/2408.14622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14622]] What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation(https://arxiv.org/abs/2408.14622)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories. Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations.</li>
<li><strong>摘要：</strong>随着人工智能的发展，尤其是大型语言模型 (LLM) 的成功，自动生成的故事的数量和质量显著提高。这导致需要自动故事评估来评估计算系统的生成能力并分析自动生成和人工编写的故事的质量。评估故事可能比其他生成评估任务更具挑战性。虽然机器翻译等任务主要侧重于评估流畅性和准确性，但故事评估需要复杂的额外措施，例如整体连贯性、角色发展、趣味性等。这需要彻底审查相关研究。在本次调查中，我们首先总结了现有的讲故事任务，包括文本到文本、视觉到文本和文本到视觉。我们强调了它们的评估挑战，确定了衡量故事的各种人类标准，并展示了现有的基准数据集。然后，我们提出了一个分类法来组织已经开发或可以采用的故事评估评估指标。我们还提供了这些指标的描述，并讨论了它们的优点和局限性。之后，我们讨论了人机协作进行故事评估和生成。最后，我们提出了未来潜在的研究方向，从故事评价扩展到一般评价。</li>
</ul>

<h3>Title: Training-Free Activation Sparsity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, Ben Athiwaratkun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14690">https://arxiv.org/abs/2408.14690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14690">https://arxiv.org/pdf/2408.14690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14690]] Training-Free Activation Sparsity in Large Language Models(https://arxiv.org/abs/2408.14690)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL, a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53$\times$ and 1.8$\times$ at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.</li>
<li><strong>摘要：</strong>激活稀疏性可以通过减少前向传递过程中矩阵乘法所需的计算和内存移动来实现大型语言模型 (LLM) 中的实际推理加速。然而，现有方法面临着阻碍广泛采用的限制。一些方法是针对具有基于 ReLU 的稀疏性的旧模型量身定制的，而另一些方法则需要对多达数千亿个标记进行大量持续的预训练。本文介绍了 TEAL，这是一种简单的免训练方法，它将基于幅度的激活稀疏性应用于整个模型的隐藏状态。TEAL 在 Llama-2、Llama-3 和 Mistral 系列中实现了 40-50% 的模型范围稀疏性，性能下降最小，大小从 7B 到 70B 不等。我们改进了现有的稀疏内核，并展示了在 40% 和 50% 的模型范围稀疏性下高达 1.53$\times$ 和 1.8$\times$ 的时钟解码速度提升。TEAL 与权重量化兼容，从而进一步提高效率。</li>
</ul>

<h3>Title: LyCon: Lyrics Reconstruction from the Bag-of-Words Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haven Kim, Kahyun Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14750">https://arxiv.org/abs/2408.14750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14750">https://arxiv.org/pdf/2408.14750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14750]] LyCon: Lyrics Reconstruction from the Bag-of-Words Using Large Language Models(https://arxiv.org/abs/2408.14750)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper addresses the unique challenge of conducting research in lyric studies, where direct use of lyrics is often restricted due to copyright concerns. Unlike typical data, internet-sourced lyrics are frequently protected under copyright law, necessitating alternative approaches. Our study introduces a novel method for generating copyright-free lyrics from publicly available Bag-of-Words (BoW) datasets, which contain the vocabulary of lyrics but not the lyrics themselves. Utilizing metadata associated with BoW datasets and large language models, we successfully reconstructed lyrics. We have compiled and made available a dataset of reconstructed lyrics, LyCon, aligned with metadata from renowned sources including the Million Song Dataset, Deezer Mood Detection Dataset, and AllMusic Genre Dataset, available for public access. We believe that the integration of metadata such as mood annotations or genres enables a variety of academic experiments on lyrics, such as conditional lyric generation.</li>
<li><strong>摘要：</strong>本文探讨了歌词研究中面临的独特挑战，由于版权问题，歌词的直接使用通常受到限制。与典型数据不同，互联网来源的歌词通常受版权法保护，因此需要采用替代方法。我们的研究介绍了一种从公开的词袋 (BoW) 数据集生成无版权歌词的新方法，该数据集包含歌词词汇但不包含歌词本身。利用与 BoW 数据集和大型语言模型相关的元数据，我们成功重建了歌词。我们编制并提供了一个重建歌词数据集 LyCon，该数据集与来自百万歌曲数据集、Deezer 情绪检测数据集和 AllMusic 流派数据集等知名来源的元数据保持一致，可供公众访问。我们相信，情绪注释或流派等元数据的集成可以实现各种歌词学术实验，例如条件歌词生成。</li>
</ul>

<h3>Title: A global AI community requires language-diverse publishing</h3>
<ul>
<li><strong>Authors: </strong>Haley Lepp, Parth Sarin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14772">https://arxiv.org/abs/2408.14772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14772">https://arxiv.org/pdf/2408.14772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14772]] A global AI community requires language-diverse publishing(https://arxiv.org/abs/2408.14772)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this provocation, we discuss the English dominance of the AI research community, arguing that the requirement for English language publishing upholds and reinforces broader regimes of extraction in AI. While large language models and machine translation have been celebrated as a way to break down barriers, we regard their use as a symptom of linguistic exclusion of scientists and potential readers. We propose alternative futures for a healthier publishing culture, organized around three themes: administering conferences in the languages of the country in which they are held, instructing peer reviewers not to adjudicate the language appropriateness of papers, and offering opportunities to publish and present in multiple languages. We welcome new translations of this piece. Please contact the authors if you would like to contribute one.</li>
<li><strong>摘要：</strong>在这一挑战中，我们讨论了英语在人工智能研究界的主导地位，并认为英语出版的要求支持并强化了人工智能更广泛的提取机制。虽然大型语言模型和机器翻译被认为是打破障碍的一种方式，但我们认为它们的使用是语言排斥科学家和潜在读者的一种表现。我们提出了更健康的出版文化的替代未来，围绕三个主题：以会议举办国的语言管理会议，指示同行评审员不要评判论文的语言适宜性，并提供以多种语言出版和展示的机会。我们欢迎这篇文章的新翻译。如果您想贡献一个，请联系作者。</li>
</ul>

<h3>Title: AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Abhay Gupta, Philip Meng, Ece Yurtseven, Sean O'Brien, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14845">https://arxiv.org/abs/2408.14845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14845">https://arxiv.org/pdf/2408.14845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14845]] AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark(https://arxiv.org/abs/2408.14845)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models. We have open-sourced our source code on GitHub and created a website to showcase our work at https://aavenue.live.</li>
<li><strong>摘要：</strong>检测非裔美国人本土英语 (AAVE) 的自然语言理解 (NLU) 中的偏见对于开发包容性的自然语言处理 (NLP) 系统至关重要。为了解决方言引起的性能差异，我们引入了 AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation，这是用于评估大型语言模型 (LLM) 在 AAVE 和标准美式英语 (SAE) 的 NLU 任务上的表现的基准。AAVENUE 建立在 VALUE 等现有基准之上并对其进行了扩展，用一种更灵活的方法取代了确定性的句法和形态转换，利用基于 LLM 的翻译和少量提示，在翻译 GLUE 和 SuperGLUE 基准的关键任务时提高了我们评估指标的性能。我们使用五种流行的 LLM 和一套全面的指标（包括流利度、BARTScore、质量、连贯性和可理解性）来比较 AAVENUE 和 VALUE 的翻译。此外，我们还招募了流利的 AAVE 使用者来验证我们的翻译的真实性。我们的评估表明，LLM 在 SAE 任务上的表现始终优于 AAVE 翻译版本，这凸显了固有的偏见，并凸显了对更具包容性的 NLP 模型的需求。我们已经在 GitHub 上开源了我们的源代码，并创建了一个网站来展示我们的工作，网址为 https://aavenue.live。</li>
</ul>

<h3>Title: Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata using LM probing</h3>
<ul>
<li><strong>Authors: </strong>Hanna Abi Akl</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14849">https://arxiv.org/abs/2408.14849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14849">https://arxiv.org/pdf/2408.14849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14849]] Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata using LM probing(https://arxiv.org/abs/2408.14849)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce SHADOW, a fine-tuned language model trained on an intermediate task using associative deductive reasoning, and measure its performance on a knowledge base construction task using Wikidata triple completion. We evaluate SHADOW on the LM-KBC 2024 challenge and show that it outperforms the baseline solution by 20% with a F1 score of 68.72%.</li>
<li><strong>摘要：</strong>我们引入了 SHADOW，这是一种经过微调的语言模型，使用联想演绎推理在中级任务上进行训练，并使用 Wikidata 三重补全测量其在知识库构建任务上的性能。我们在 LM-KBC 2024 挑战赛上对 SHADOW 进行了评估，结果表明其性能比基线解决方案高出 20%，F1 得分为 68.72%。</li>
</ul>

<h3>Title: Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Du, Zhuo Li, Pengyu Cheng, Xiang Wan, Anningzhe Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14853">https://arxiv.org/abs/2408.14853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14853">https://arxiv.org/pdf/2408.14853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14853]] Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models(https://arxiv.org/abs/2408.14853)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become a focal point in the rapidly evolving field of artificial intelligence. However, a critical concern is the presence of toxic content within the pre-training corpus of these models, which can lead to the generation of inappropriate outputs. Investigating methods for detecting internal faults in LLMs can help us understand their limitations and improve their security. Existing methods primarily focus on jailbreaking attacks, which involve manually or automatically constructing adversarial content to prompt the target LLM to generate unexpected responses. These methods rely heavily on prompt engineering, which is time-consuming and usually requires specially designed questions. To address these challenges, this paper proposes a target-driven attack paradigm that focuses on directly eliciting the target response instead of optimizing the prompts. We introduce the use of another LLM as the detector for toxic content, referred to as ToxDet. Given a target toxic response, ToxDet can generate a possible question and a preliminary answer to provoke the target model into producing desired toxic responses with meanings equivalent to the provided one. ToxDet is trained by interacting with the target LLM and receiving reward signals from it, utilizing reinforcement learning for the optimization process. While the primary focus of the target models is on open-source LLMs, the fine-tuned ToxDet can also be transferred to attack black-box models such as GPT-4o, achieving notable results. Experimental results on AdvBench and HH-Harmless datasets demonstrate the effectiveness of our methods in detecting the tendencies of target LLMs to generate harmful responses. This algorithm not only exposes vulnerabilities but also provides a valuable resource for researchers to strengthen their models against such attacks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为快速发展的人工智能领域的焦点。然而，一个关键问题是这些模型的预训练语料库中存在有毒内容，这可能导致生成不适当的输出。研究检测 LLM 内部故障的方法可以帮助我们了解它们的局限性并提高其安全性。现有方法主要侧重于越狱攻击，这涉及手动或自动构建对抗性内容以提示目标 LLM 生成意外响应。这些方法严重依赖于提示工程，这非常耗时，通常需要专门设计的问题。为了应对这些挑战，本文提出了一种目标驱动的攻击范式，该范式专注于直接引出目标响应而不是优化提示。我们引入了另一个 LLM 作为有毒内容检测器的使用，称为 ToxDet。给定目标有毒响应，ToxDet 可以生成一个可能的问题和一个初步答案，以激发目标模型产生具有与提供的含义相同的所需有毒响应。 ToxDet 通过与目标 LLM 交互并从中接收奖励信号进行训练，利用强化学习进行优化过程。虽然目标模型的主要重点是开源 LLM，但经过微调的 ToxDet 也可以转移到攻击黑盒模型（例如 GPT-4o），并取得显著成果。在 AdvBench 和 HH-Harmless 数据集上的实验结果证明了我们的方法在检测目标 LLM 产生有害响应的趋势方面的有效性。该算法不仅揭示了漏洞，还为研究人员提供了宝贵的资源来加强他们的模型以抵御此类攻击。</li>
</ul>

<h3>Title: Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongfu Liu, Yuxi Xie, Ye Wang, Michael Shieh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14866">https://arxiv.org/abs/2408.14866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14866">https://arxiv.org/pdf/2408.14866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14866]] Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models(https://arxiv.org/abs/2408.14866)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy Coordinate Gradient (GCG). However, GCG struggles with computational inefficiency, limiting further investigations regarding suffix transferability and scalability across models and data. In this work, we bridge the connection between search efficiency and suffix transferability. We propose a two-stage transfer learning framework, DeGCG, which decouples the search process into behavior-agnostic pre-searching and behavior-relevant post-searching. Specifically, we employ direct first target token optimization in pre-searching to facilitate the search process. We apply our approach to cross-model, cross-data, and self-transfer scenarios. Furthermore, we introduce an interleaved variant of our approach, i-DeGCG, which iteratively leverages self-transferability to accelerate the search process. Experiments on HarmBench demonstrate the efficiency of our approach across various models and domains. Notably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of $43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively. Further analysis on cross-model transfer indicates the pivotal role of first target token optimization in leveraging suffix transferability for efficient searching.</li>
<li><strong>摘要：</strong>语言模型 (LLM) 面临安全问题，因为可能会被恶意用户滥用。最近的红队工作已经确定了能够使用基于梯度的搜索算法贪婪坐标梯度 (GCG) 越狱 LLM 的对抗性后缀。然而，GCG 面临着计算效率低下的困境，限制了对跨模型和数据的后缀可转移性和可扩展性的进一步研究。在这项工作中，我们架起了搜索效率和后缀可转移性之间的桥梁。我们提出了一个两阶段迁移学习框架 DeGCG，它将搜索过程分解为与行为无关的预搜索和与行为相关的后搜索。具体来说，我们在预搜索中采用直接第一目标标记优化来促进搜索过程。我们将我们的方法应用于跨模型、跨数据和自转移场景。此外，我们引入了我们方法的交错变体 i-DeGCG，它迭代地利用自转移性来加速搜索过程。 HarmBench 上的实验证明了我们的方法在各种模型和领域中的有效性。值得注意的是，我们的 i-DeGCG 在 Llama2-chat-7b 上的表现优于基线，有效集和测试集上的 ASR 分别为 43.9 美元（+22.2 美元）和 39.0 美元（+19.5 美元）。对跨模型迁移的进一步分析表明，第一个目标标记优化在利用后缀可迁移性实现高效搜索方面发挥着关键作用。</li>
</ul>

<h3>Title: Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data</h3>
<ul>
<li><strong>Authors: </strong>Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14874">https://arxiv.org/abs/2408.14874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14874">https://arxiv.org/pdf/2408.14874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14874]] Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data(https://arxiv.org/abs/2408.14874)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has proven effective in aligning large language models with human intentions, yet it often relies on complex methodologies like Proximal Policy Optimization (PPO) that require extensive hyper-parameter tuning and present challenges in sample efficiency and stability. In this paper, we introduce Inverse-Q*, an innovative framework that transcends traditional RL methods by optimizing token-level reinforcement learning without the need for additional reward or value models. Inverse-Q* leverages direct preference optimization techniques but extends them by estimating the conditionally optimal policy directly from the model's responses, facilitating more granular and flexible policy shaping. Our approach reduces reliance on human annotation and external supervision, making it especially suitable for low-resource settings. We present extensive experimental results demonstrating that Inverse-Q* not only matches but potentially exceeds the effectiveness of PPO in terms of convergence speed and the alignment of model responses with human preferences. Our findings suggest that Inverse-Q* offers a practical and robust alternative to conventional RLHF approaches, paving the way for more efficient and adaptable model training approaches.</li>
<li><strong>摘要：</strong>人类反馈强化学习 (RLHF) 已被证明能够有效地将大型语言模型与人类意图保持一致，但它通常依赖于复杂的方法，如近端策略优化 (PPO)，这些方法需要大量的超参数调整，并且在样本效率和稳定性方面存在挑战。在本文中，我们介绍了 Inverse-Q*，这是一个创新框架，它通过优化 token 级强化学习而超越了传统的 RL 方法，而无需额外的奖励或价值模型。Inverse-Q* 利用直接偏好优化技术，但通过直接从模型的响应中估计条件最优策略来扩展它们，从而促进更细粒度和更灵活的策略制定。我们的方法减少了对人工注释和外部监督的依赖，使其特别适合资源匮乏的环境。我们提供了大量的实验结果，证明 Inverse-Q* 不仅在收敛速度和模型响应与人类偏好的一致性方面与 PPO 相匹配，而且可能超过 PPO。我们的研究结果表明，Inverse-Q* 为传统的 RLHF 方法提供了一种实用且强大的替代方案，为更高效、适应性更强的模型训练方法铺平了道路。</li>
</ul>

<h3>Title: Writing in the Margins: Better Inference Pattern for Long Context Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, Waseem AlShikh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14906">https://arxiv.org/abs/2408.14906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14906">https://arxiv.org/pdf/2408.14906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14906]] Writing in the Margins: Better Inference Pattern for Long Context Retrieval(https://arxiv.org/abs/2408.14906)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, we observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, we show how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. We release our implementation of WiM using Hugging Face Transformers library at this https URL.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了“边距写作”（WiM），这是一种用于大型语言模型的新推理模式，旨在优化检索导向任务中长输入序列的处理。这种方法利用键值缓存的分块预填充来执行分段推理，从而能够高效处理广泛的上下文以及生成和分类中间信息（“边距”），从而引导模型完成特定任务。这种方法略微增加了计算开销，同时显著提高了现成模型的性能，而无需进行微调。具体来说，我们观察到 WiM 平均提高了推理技能（HotpotQA、MultiHop-RAG）的准确率 7.5%，提高了聚合任务（CWE）的 F1 分数 30.0% 以上。此外，我们展示了所提出的模式如何融入交互式检索设计，该设计为最终用户提供有关上下文处理进度的持续更新，并精确定位相关信息与最终响应的集成。我们在此 https URL 上使用 Hugging Face Transformers 库发布了我们的 WiM 实现。</li>
</ul>

<h3>Title: SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Shuaijie Shen, Chao Wang, Renzhuo Huang, Yan Zhong, Qinghai Guo, Zhichao Lu, Jianguo Zhang, Luziwei Leng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14909">https://arxiv.org/abs/2408.14909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14909">https://arxiv.org/pdf/2408.14909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14909]] SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models(https://arxiv.org/abs/2408.14909)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Known as low energy consumption networks, spiking neural networks (SNNs) have gained a lot of attention within the past decades. While SNNs are increasing competitive with artificial neural networks (ANNs) for vision tasks, they are rarely used for long sequence tasks, despite their intrinsic temporal dynamics. In this work, we develop spiking state space models (SpikingSSMs) for long sequence learning by leveraging on the sequence learning abilities of state space models (SSMs). Inspired by dendritic neuron structure, we hierarchically integrate neuronal dynamics with the original SSM block, meanwhile realizing sparse synaptic computation. Furthermore, to solve the conflict of event-driven neuronal dynamics with parallel computing, we propose a light-weight surrogate dynamic network which accurately predicts the after-reset membrane potential and compatible to learnable thresholds, enabling orders of acceleration in training speed compared with conventional iterative methods. On the long range arena benchmark task, SpikingSSM achieves competitive performance to state-of-the-art SSMs meanwhile realizing on average 90\% of network sparsity. On language modeling, our network significantly surpasses existing spiking large language models (spikingLLMs) on the WikiText-103 dataset with only a third of the model size, demonstrating its potential as backbone architecture for low computation cost LLMs.</li>
<li><strong>摘要：</strong>脉冲神经网络 (SNN) 被称为低能耗网络，在过去几十年中引起了广泛关注。虽然 SNN 在视觉任务方面与人工神经网络 (ANN) 的竞争力日益增强，但尽管它们具有内在的时间动态性，却很少用于长序列任务。在这项工作中，我们利用状态空间模型 (SSM) 的序列学习能力，开发了用于长序列学习的脉冲状态空间模型 (SpikingSSM)。受树突状神经元结构的启发，我们将神经元动力学与原始 SSM 块分层集成，同时实现稀疏突触计算。此外，为了解决事件驱动的神经元动力学与并行计算的冲突，我们提出了一种轻量级代理动态网络，它可以准确预测重置后的膜电位并与可学习阈值兼容，与传统迭代方法相比，训练速度提高了几个数量级。在长距离竞技场基准测试任务中，SpikingSSM 实现了与最先进的 SSM 相媲美的性能，同时实现了平均 90% 的网络稀疏性。在语言建模方面，我们的网络在 WikiText-103 数据集上显著超越了现有的脉冲大型语言模型 (spikingLLM)，而模型大小仅为其三分之一，证明了其作为低计算成本 LLM 主干架构的潜力。</li>
</ul>

<h3>Title: AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Chi-Min Chan, Jianxuan Yu, Weize Chen, Chunyang Jiang, Xinyu Liu, Weijie Shi, Zhiyuan Liu, Wei Xue, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.14972">https://arxiv.org/abs/2408.14972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.14972">https://arxiv.org/pdf/2408.14972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.14972]] AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems(https://arxiv.org/abs/2408.14972)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has led to the rise of LLM-based agents. Recent research shows that multi-agent systems (MAS), where each agent plays a specific role, can outperform individual LLMs. However, configuring an MAS for a task remains challenging, with performance only observable post-execution. Inspired by scaling laws in LLM development, we investigate whether MAS performance can be predicted beforehand. We introduce AgentMonitor, a framework that integrates at the agent level to capture inputs and outputs, transforming them into statistics for training a regression model to predict task performance. Additionally, it can further apply real-time corrections to address security risks posed by malicious agents, mitigating negative impacts and enhancing MAS security. Experiments demonstrate that an XGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in more challenging scenarios. Furthermore, using AgentMonitor reduces harmful content by 6.2% and increases helpful content by 1.8% on average, enhancing safety and reliability. Code is available at \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展导致了基于 LLM 的代理的兴起。最近的研究表明，多代理系统 (MAS)（其中每个代理都扮演特定角色）可以胜过单个 LLM。但是，为任务配置 MAS 仍然具有挑战性，性能只有在执行后才能观察到。受 LLM 开发中的缩放定律的启发，我们研究是否可以预先预测 MAS 性能。我们引入了 AgentMonitor，这是一个在代理级别集成以捕获输入和输出的框架，将它们转换为统计数据，以训练回归模型来预测任务性能。此外，它可以进一步应用实时校正来解决恶意代理带来的安全风险，减轻负面影响并增强 MAS 安全性。实验表明，XGBoost 模型在域内实现了 0.89 的 Spearman 相关性，在更具挑战性的场景中实现了 0.58。此外，使用 AgentMonitor 平均减少了 6.2% 的有害内容，并增加了 1.8% 的有用内容，从而提高了安全性和可靠性。代码可在 \url{此 https URL} 处获取。</li>
</ul>

<h3>Title: Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Haowei Du, Huishuai Zhang, Dongyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15037">https://arxiv.org/abs/2408.15037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15037">https://arxiv.org/pdf/2408.15037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15037]] Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering(https://arxiv.org/abs/2408.15037)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>To address the hallucination in generative question answering (GQA) where the answer can not be derived from the document, we propose a novel evidence-enhanced triplet generation framework, EATQA, encouraging the model to predict all the combinations of (Question, Evidence, Answer) triplet by flipping the source pair and the target label to understand their logical relationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a QE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap to distill the knowledge from evidence in inference stage. Our framework ensures the model to learn the logical relation between query, evidence and answer, which simultaneously improves the evidence generation and query answering. In this paper, we apply EATQA to LLama and it outperforms other LLMs-based methods and hallucination mitigation approaches on two challenging GQA benchmarks. Further analysis shows that our method not only keeps prior knowledge within LLM, but also mitigates hallucination and generates faithful answers.</li>
<li><strong>摘要：</strong>为了解决生成式问答 (GQA) 中无法从文档中得出答案的幻觉问题，我们提出了一种新颖的证据增强三元组生成框架 EATQA，通过翻转源对和目标标签来鼓励模型预测 (问题、证据、答案) 三元组的所有组合以了解它们的逻辑关系，即分别给定 QE、EA 和 QA 对来预测答案 (A)、问题 (Q) 和证据 (E)。此外，我们弥合分布差距以在推理阶段从证据中提取知识。我们的框架确保模型学习查询、证据和答案之间的逻辑关系，从而同时改进证据生成和查询回答。在本文中，我们将 EATQA 应用于 LLama，它在两个具有挑战性的 GQA 基准上优于其他基于 LLM 的方法和幻觉缓解方法。进一步的分析表明，我们的方法不仅将先验知识保留在 LLM 中，而且还减轻了幻觉并生成忠实的答案。</li>
</ul>

<h3>Title: A Survey of Large Language Models for European Languages</h3>
<ul>
<li><strong>Authors: </strong>Wazir Ali, Sampo Pyysalo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15040">https://arxiv.org/abs/2408.15040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15040">https://arxiv.org/pdf/2408.15040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15040]] A Survey of Large Language Models for European Languages(https://arxiv.org/abs/2408.15040)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained significant attention due to their high performance on a wide range of natural language tasks since the release of ChatGPT. The LLMs learn to understand and generate language by training billions of model parameters on vast volumes of text data. Despite being a relatively new field, LLM research is rapidly advancing in various directions. In this paper, we present an overview of LLM families, including LLaMA, PaLM, GPT, and MoE, and the methods developed to create and enhance LLMs for official European Union (EU) languages. We provide a comprehensive summary of common monolingual and multilingual datasets used for pretraining LLMs.</li>
<li><strong>摘要：</strong>自 ChatGPT 发布以来，大型语言模型 (LLM) 因其在各种自然语言任务中的出色表现而备受关注。LLM 通过在大量文本数据上训练数十亿个模型参数来学习理解和生成语言。尽管 LLM 是一个相对较新的领域，但它正在向各个方向迅速发展。在本文中，我们概述了 LLM 系列，包括 LLaMA、PaLM、GPT 和 MoE，以及为创建和增强欧盟 (EU) 官方语言的 LLM 而开发的方法。我们全面总结了用于预训练 LLM 的常见单语和多语数据集。</li>
</ul>

<h3>Title: BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline</h3>
<ul>
<li><strong>Authors: </strong>Guosheng Dong, Da Pan, Yiding Sun, Shusen Zhang, Zheng Liang, Xin Wu, Yanjun Shen, Fan Yang, Haoze Sun, Tianpeng Li, Mingan Lin, Jianhua Xu, Yufan Zhang, Xiaonan Nie, Lei Su, Bingning Wang, Wentao Zhang, Jiaxin Mao, Zenan Zhou, Weipeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15079">https://arxiv.org/abs/2408.15079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15079">https://arxiv.org/pdf/2408.15079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15079]] BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline(https://arxiv.org/abs/2408.15079)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The general capabilities of Large Language Models (LLM) highly rely on the composition and selection on extensive pretraining datasets, treated as commercial secrets by several institutions. To mitigate this issue, we open-source the details of a universally applicable data processing pipeline and validate its effectiveness and potential by introducing a competitive LLM baseline. Specifically, the data processing pipeline consists of broad collection to scale up and reweighting to improve quality. We then pretrain a 7B model BaichuanSEED with 3T tokens processed by our pipeline without any deliberate downstream task-related optimization, followed by an easy but effective supervised fine-tuning stage. BaichuanSEED demonstrates consistency and predictability throughout training and achieves comparable performance on comprehensive benchmarks with several commercial advanced large language models, such as Qwen1.5 and Llama3. We also conduct several heuristic experiments to discuss the potential for further optimization of downstream tasks, such as mathematics and coding.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的一般功能高度依赖于大量预训练数据集的组成和选择，而这些数据集被多家机构视为商业机密。为了缓解这个问题，我们开源了通用数据处理流程的细节，并通过引入具有竞争力的 LLM 基线来验证其有效性和潜力。具体来说，数据处理流程包括广泛收集以扩大规模和重新加权以提高质量。然后，我们使用由我们的流程处理的 3T 标记对 7B 模型 BaichuanSEED 进行预训练，无需任何刻意的下游任务相关优化，然后进行简单但有效的监督微调阶段。BaichuanSEED 在整个训练过程中表现出一致性和可预测性，并在综合基准测试中与几种商业高级大型语言模型（如 Qwen1.5 和 Llama3）实现了相当的性能。我们还进行了几项启发式实验，以讨论进一步优化下游任务（如数学和编码）的潜力。</li>
</ul>

<h3>Title: Relation Also Knows: Rethinking the Recall and Editing of Factual Associations in Auto-Regressive Transformer Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15091">https://arxiv.org/abs/2408.15091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15091">https://arxiv.org/pdf/2408.15091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15091]] Relation Also Knows: Rethinking the Recall and Editing of Factual Associations in Auto-Regressive Transformer Language Models(https://arxiv.org/abs/2408.15091)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The storage and recall of factual associations in auto-regressive transformer language models (LMs) have drawn a great deal of attention, inspiring knowledge editing by directly modifying the located model weights. Most editing works achieve knowledge editing under the guidance of existing interpretations of knowledge recall that mainly focus on subject knowledge. However, these interpretations are seriously flawed, neglecting relation information and leading to the over-generalizing problem for editing. In this work, we discover a novel relation-focused perspective to interpret the knowledge recall of transformer LMs during inference and apply it on knowledge editing to avoid over-generalizing. Experimental results on the dataset supplemented with a new R-Specificity criterion demonstrate that our editing approach significantly alleviates over-generalizing while remaining competitive on other criteria, breaking the domination of subject-focused editing for future research.</li>
<li><strong>摘要：</strong>自回归 Transformer 语言模型 (LM) 中事实关联的存储和回忆引起了广泛关注，通过直接修改定位的模型权重启发了知识编辑。大多数编辑工作都是在现有的以主题知识为主的知识回忆解释的指导下实现知识​​编辑的。然而，这些解释存在严重缺陷，忽略了关系信息并导致编辑的过度概括问题。在本文中，我们发现了一种新颖的以关系为中心的视角来解释 Transformer 语言模型在推理过程中的知识回忆，并将其应用于知识编辑以避免过度概括。在补充了新的 R 特异性标准的数据集上的实验结果表明，我们的编辑方法显着缓解了过度概括，同时在其他标准上保持了竞争力，打破了以主题为中心的编辑的主导地位，为未来的研究提供了参考。</li>
</ul>

<h3>Title: Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation</h3>
<ul>
<li><strong>Authors: </strong>N. E. Kriman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15171">https://arxiv.org/abs/2408.15171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15171">https://arxiv.org/pdf/2408.15171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15171]] Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation(https://arxiv.org/abs/2408.15171)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>The use of large language models (LLMs) has significantly increased since the introduction of ChatGPT in 2022, demonstrating their value across various applications. However, a major challenge for enterprise and commercial adoption of LLMs is their tendency to generate inaccurate information, a phenomenon known as "hallucination." This project proposes a method for estimating the factuality of a summary generated by LLMs when compared to a source text. Our approach utilizes Naive Bayes classification to assess the accuracy of the content produced.</li>
<li><strong>摘要：</strong>自 2022 年推出 ChatGPT 以来，大型语言模型 (LLM) 的使用量显著增加，证明了其在各种应用中的价值。然而，企业和商业采用 LLM 面临的一个主要挑战是它们倾向于生成不准确的信息，这种现象被称为“幻觉”。该项目提出了一种与源文本相比，估计 LLM 生成的摘要的真实性的方法。我们的方法利用朴素贝叶斯分类来评估所生成内容的准确性。</li>
</ul>

<h3>Title: Can Unconfident LLM Annotations Be Used for Confident Conclusions?</h3>
<ul>
<li><strong>Authors: </strong>Kristina Gligorić, Tijana Zrnic, Cinoo Lee, Emmanuel J. Candès, Dan Jurafsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15204">https://arxiv.org/abs/2408.15204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15204">https://arxiv.org/pdf/2408.15204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15204]] Can Unconfident LLM Annotations Be Used for Confident Conclusions?(https://arxiv.org/abs/2408.15204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-Driven Inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of Confidence-Driven Inference over baselines in statistical estimation tasks across three CSS settings--text politeness, stance, and bias--reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-Driven Inference can be used to estimate most standard quantities across a broad range of NLP problems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中都表现出与人工评分者的高度一致性，显示出缓解人工数据收集挑战的潜力。在计算社会科学 (CSS) 中，研究人员越来越多地利用 LLM 注释来补充缓慢且昂贵的人工注释。然而，在不损害下游结论有效性的情况下收集和使用 LLM 注释的指南仍然有限。我们引入了置信度驱动推理：一种结合 LLM 注释和 LLM 置信度指标的方法，以战略性地选择应收集哪些人工注释，目的是产生准确的统计估计和可证明有效的置信区间，同时减少所需的人工注释数量。我们的方法具有针对低质量 LLM 注释的保护措施，保证结论既有效，又不低于仅依赖人工注释的准确性。我们展示了置信驱动推理在统计估计任务中的有效性，涵盖三种 CSS 设置（文本礼貌、立场和偏见），每种设置可将所需的人工注释数量减少 25% 以上。虽然我们使用 CSS 设置进行演示，但置信驱动推理可用于估计各种 NLP 问题中的大多数标准数量。</li>
</ul>

<h3>Title: Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, Monica S. Lam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15232">https://arxiv.org/abs/2408.15232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15232">https://arxiv.org/pdf/2408.15232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15232]] Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations(https://arxiv.org/abs/2408.15232)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat, agent</a></li>
<li><strong>Abstract: </strong>While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user's behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.</li>
<li><strong>摘要：</strong>虽然语言模型 (LM) 驱动的聊天机器人和生成搜索引擎擅长回答具体查询，但在未知的未知领域中发现信息对用户来说仍然具有挑战性。为了模拟儿童/学生通过倾听和参与父母/老师的对话来学习的常见教育场景，我们创建了协作 STORM (Co-STORM)。与要求用户提出所有问题的 QA 系统不同，Co-STORM 允许用户观察并偶尔引导多个 LM 代理之间的对话。代理代表用户提问，让用户偶然发现未知的未知。为了促进用户互动，Co-STORM 通过将未发现的信息组织成动态思维导图来帮助用户跟踪对话，最终生成综合报告作为总结。对于自动评估，我们通过收集具有用户目标的真实信息搜索记录来构建 WildSeek 数据集。Co-STORM 在话语追踪和报告质量方面均优于基线方法。在进一步的人工评估中，70% 的参与者更喜欢 Co-STORM 而不是搜索引擎，78% 的参与者更喜欢它而不是 RAG 聊天机器人。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
