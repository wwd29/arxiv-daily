<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-23</h1>
<h3>Title: On Leveraging Encoder-only Pre-trained Language Models for Effective  Keyphrase Generation</h3>
<ul>
<li><strong>Authors: </strong>Di Wu, Wasi Uddin Ahmad, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14052">https://arxiv.org/abs/2402.14052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14052">https://arxiv.org/pdf/2402.14052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14052]] On Leveraging Encoder-only Pre-trained Language Models for Effective  Keyphrase Generation(https://arxiv.org/abs/2402.14052)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify a favorable parameter allocation towards model depth rather than width when employing encoder-decoder architectures initialized with encoder-only PLMs. The study sheds light on the potential of utilizing encoder-only PLMs for advancing KPG systems and provides a groundwork for future KPG methods. Our code and pre-trained checkpoints are released at https://github.com/uclanlp/DeepKPG.</li>
<li><strong>摘要：</strong>本研究讨论了仅编码器预训练语言模型 (PLM) 在关键短语生成 (KPG) 中的应用，与编码器-解码器模型相比，仅域定制编码器模型的可用性更广泛。我们研究了三个核心问题：(1) KPG 中仅编码器 PLM 的功效，(2) 在 KPG 中采用仅编码器 PLM 的最佳架构决策，以及 (3) 域内仅编码器与编码器之间的性能比较- 跨不同资源设置的 PLM 解码器。我们的研究结果来自两个领域的广泛实验，结果表明，对于仅编码器的 PLM，尽管具有条件随机字段的 KPE 在识别当前关键短语方面略胜一筹，但 KPG 公式可呈现更广泛的关键短语预测。此外，仅编码器 PLM 的前缀 LM 微调成为 KPG 的强大且数据高效的策略，其性能优于通用域 seq2seq PLM。当采用仅编码器 PLM 初始化的编码器-解码器架构时，我们还确定了针对模型深度而不是宽度的有利参数分配。该研究揭示了利用仅编码器的 PLM 来推进 KPG 系统的潜力，并为未来的 KPG 方法奠定了基础。我们的代码和预训练的检查点发布在 https://github.com/uclanlp/DeepKPG。</li>
</ul>

<h3>Title: Improving Language Understanding from Screenshots</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Gao, Zirui Wang, Adithya Bhaskar, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14073">https://arxiv.org/abs/2402.14073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14073">https://arxiv.org/pdf/2402.14073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14073]] Improving Language Understanding from Screenshots(https://arxiv.org/abs/2402.14073)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT on 6 out of 8 GLUE tasks (within 2%) and improves up to 8% over prior work. Additionally, we extend PTP to train autoregressive screenshot LMs and demonstrate its effectiveness--our models can significantly reduce perplexity by utilizing the screenshot context. Together, we hope our findings can inspire future research on developing powerful screenshot LMs and extending their reach to broader applications.</li>
<li><strong>摘要：</strong>新兴的语言模型 (LM) 系列能够在单个视觉视图中处理文本和图像，有望解锁图表理解和 UI 导航等复杂任务。我们将这些模型称为屏幕截图语言模型。尽管它们很有吸引力，但现有的屏幕截图 LM 在语言理解任务上远远落后于纯文本模型。为了弥补这一差距，我们采用了一种简化的设置，其中模型输入是纯文本渲染的屏幕截图，并且我们专注于提高屏幕截图 LM 的文本能力。我们提出了一种新颖的补丁和文本预测（PTP）目标，它可以掩盖并恢复屏幕截图的图像补丁和屏幕截图中的文本。我们还对掩蔽率和补丁大小进行了广泛的消融研究，以及提高训练稳定性的设计。我们的预训练模型在仅采用视觉输入的情况下，在 8 个 GLUE 任务中的 6 个上实现了与 BERT 相当的性能（2% 以内），并且比之前的工作提高了 8%。此外，我们扩展 PTP 来训练自回归屏幕截图 LM 并证明其有效性——我们的模型可以通过利用屏幕截图上下文显着减少困惑。我们希望我们的发现能够激励未来的研究，开发强大的屏幕截图语言模型并将其扩展到更广泛的应用。</li>
</ul>

<h3>Title: LexC-Gen: Generating Data for Extremely Low-Resource Languages with  Large Language Models and Bilingual Lexicons</h3>
<ul>
<li><strong>Authors: </strong>Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14086">https://arxiv.org/abs/2402.14086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14086">https://arxiv.org/pdf/2402.14086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14086]] LexC-Gen: Generating Data for Extremely Low-Resource Languages with  Large Language Models and Bilingual Lexicons(https://arxiv.org/abs/2402.14086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classification tasks respectively. We show that conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen is also practical -- it only needs a single GPU to generate data at scale. It works well with open-access LLMs, and its cost is one-fifth of the cost of GPT4-based multilingual data generation.</li>
<li><strong>摘要：</strong>低资源语言的数据稀缺问题可以通过使用双语词典对高资源语言的标记任务数据进行逐字翻译来解决。然而，双语词典通常与任务数据的词汇重叠有限，这导致翻译覆盖率和词典利用率较差。我们提出了词典条件数据生成（LexC-Gen），这是一种大规模生成低资源语言分类任务数据的方法。具体来说，LexC-Gen首先使用双语词典中的高资源语言单词生成词典兼容的任务数据，然后通过单词翻译将其翻译为双语词典中的低资源语言。在 17 种资源极少的语言中，LexC-Gen 生成的数据与专家翻译的黄金数据具有竞争力，并且在情感分析和主题分类任务上比现有的基于词典的单词翻译方法平均分别提高了 5.6 和 8.9 个百分点。我们证明双语词典的调节是 LexC-Gen 的关键组成部分。 LexC-Gen 也很实用——它只需要一个 GPU 即可大规模生成数据。它与开放获取的法学硕士配合良好，其成本是基于 GPT4 的多语言数据生成成本的五分之一。</li>
</ul>

<h3>Title: FanOutQA: Multi-Hop, Multi-Document Question Answering for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14116">https://arxiv.org/abs/2402.14116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14116">https://arxiv.org/pdf/2402.14116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14116]] FanOutQA: Multi-Hop, Multi-Document Question Answering for Large  Language Models(https://arxiv.org/abs/2402.14116)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context</a></li>
<li><strong>Abstract: </strong>One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com</li>
<li><strong>摘要：</strong>日常场景中常见的一类问题是“扇出”问题，即复杂的多跳、多文档推理问题，需要查找有关大量实体的信息。然而，很少有资源来评估大型语言模型中的此类问答能力。为了更全面地评估法学硕士中的复杂推理，我们提出了 FanOutQA，这是一个以英语维基百科为知识库的扇出问答对和人工注释分解的高质量数据集。我们在数据集和基准 7 个 LLM 中制定了三个基准设置，包括 GPT-4、LLaMA 2、Claude-2.1 和 Mixtral-8x7B，发现当代模型仍然有空间改进长上下文中文档间依赖关系的推理。我们提供数据集和开源工具来运行模型，以鼓励在 https://fanoutqa.com 上进行评估</li>
</ul>

<h3>Title: DeiSAM: Segment Anything with Deictic Prompting</h3>
<ul>
<li><strong>Authors: </strong>Hikaru Shindo, Manuel Brack, Gopika Sudhakaran, Devendra Singh Dhami, Patrick Schramowski, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14123">https://arxiv.org/abs/2402.14123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14123">https://arxiv.org/pdf/2402.14123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14123]] DeiSAM: Segment Anything with Deictic Prompting(https://arxiv.org/abs/2402.14123)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as "The object that is on the desk and behind the cup.". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions. As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual prompts. Our empirical results demonstrate that DeiSAM is a substantial improvement over purely data-driven baselines for deictic promptable segmentation.</li>
<li><strong>摘要：</strong>大规模、预训练的神经网络在各种任务中表现出了强大的能力，包括零样本图像分割。为了识别复杂场景中的具体物体，人类本能地依赖自然语言中的指示性描述，即根据上下文来指代某些事物，例如“桌子上和杯子后面的物体”。然而，深度学习方法由于缺乏复杂场景下的推理能力，无法可靠地解释这种指示表示。为了解决这个问题，我们提出了 DeiSAM——大型预训练神经网络与可微逻辑推理器的组合——用于指示性提示分割。给定复杂的文本分段描述，DeiSAM 利用大型语言模型 (LLM) 生成一阶逻辑规则，并对生成的场景图执行可微分的前向推理。随后，DeiSAM 通过将对象与逻辑推断的图像区域进行匹配来分割对象。作为我们评估的一部分，我们提出了指示视觉基因组（DeiVG）数据集，其中包含配对的视觉输入和复杂的指示文本提示。我们的实证结果表明，DeiSAM 相对于纯粹数据驱动的指示性提示分割基线有了实质性的改进。</li>
</ul>

<h3>Title: Reinforcement Learning with Dynamic Multi-Reward Weighting for  Multi-Style Controllable Generation</h3>
<ul>
<li><strong>Authors: </strong>Karin de Langis, Ryan Koo, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14146">https://arxiv.org/abs/2402.14146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14146">https://arxiv.org/pdf/2402.14146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14146]] Reinforcement Learning with Dynamic Multi-Reward Weighting for  Multi-Style Controllable Generation(https://arxiv.org/abs/2402.14146)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author's emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how large language models can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a reinforcement learning (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that dynamic weighting generally outperforms static weighting approaches, and we explore its effectiveness in 2- and 3-style control, even compared to strong baselines like plug-and-play model. All code and data for RL pipelines with multiple style attributes will be publicly available.</li>
<li><strong>摘要：</strong>风格是文本的一个组成部分，它表达了多种信息，包括人际动态（例如形式）和作者的情感或态度（例如厌恶）。人类经常同时采用多种风格。一个悬而未决的问题是，如何显式控制大型语言模型，以便它们在生成文本时将目标样式编织在一起：例如，生成负面且无毒的文本。以前的工作研究了单个样式的受控生成，或者样式和其他属性的受控生成。在本文中，我们将其扩展为同时控制多种样式。具体来说，我们研究了用于控制多风格生成的强化学习（RL）方法的多种风格奖励的各种公式。这些奖励公式包括鉴别器的校准输出和鉴别器梯度大小的动态加权。我们发现动态加权通常优于静态加权方法，并且我们探索了其在 2 型和 3 型控制中的有效性，甚至与即插即用模型等强基线相比也是如此。具有多种样式属性的 RL 管道的所有代码和数据都将公开。</li>
</ul>

<h3>Title: MM-Soc: Benchmarking Multimodal Large Language Models in Social Media  Platforms</h3>
<ul>
<li><strong>Authors: </strong>Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14154">https://arxiv.org/abs/2402.14154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14154">https://arxiv.org/pdf/2402.14154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14154]] MM-Soc: Benchmarking Multimodal Large Language Models in Social Media  Platforms(https://arxiv.org/abs/2402.14154)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models' social understanding capabilities. Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement.</li>
<li><strong>摘要：</strong>社交媒体平台是多模式信息交换的中心，包括文本、图像和视频，这使得机器难以理解与在线空间交互相关的信息或情感。多模态大型语言模型 (MLLM) 已成为解决这些挑战的有前途的解决方案，但难以准确解释人类情感和错误信息等复杂内容。本文介绍了 MM-Soc，这是一个综合基准，旨在评估 MLLM 对多模式社交媒体内容的理解。 MM-Soc 编译了著名的多模态数据集，并合并了一个新颖的大规模 YouTube 标记数据集，针对错误信息检测、仇恨言论检测和社交上下文生成等一系列任务。通过对四个开源 MLLM 的十个尺寸变体的详尽评估，我们发现了显着的性能差异，强调了模型社会理解能力进步的必要性。我们的分析表明，在零样本环境中，各种类型的 MLLM 在处理社交媒体任务时通常表现出困难。然而，MLLM 展示了微调后的性能改进，表明了潜在的改进途径。</li>
</ul>

<h3>Title: TOOLVERIFIER: Generalization to New Tools via Self-Verification</h3>
<ul>
<li><strong>Authors: </strong>Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, Jane Dwivedi-Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14158">https://arxiv.org/abs/2402.14158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14158">https://arxiv.org/pdf/2402.14158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14158]] TOOLVERIFIER: Generalization to New Tools via Self-Verification(https://arxiv.org/abs/2402.14158)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.</li>
<li><strong>摘要：</strong>教授语言模型使用工具是构建通用助理的一个重要里程碑，但仍然是一个悬而未决的问题。尽管在通过微调学习使用特定工具方面取得了重大进展，但语言模型仍然难以通过少数演示来学习如何稳健地使用新工具。在这项工作中，我们引入了一种自我验证方法，该方法通过在（1）工具选择过程中自问对比问题来区分相近的候选者； (2)参数生成。我们使用 Llama-2 70B 构建合成的、高质量的、自行生成的数据，并打算公开发布。对 ToolBench 基准测试中的 4 个任务（包括 17 个未见过的工具）进行的广泛实验表明，即使在候选工具之间的差异非常细微的情况下，也比少数样本基线平均提高了 22%。</li>
</ul>

<h3>Title: Recursive Speculative Decoding: Accelerating LLM Inference via Sampling  Without Replacement</h3>
<ul>
<li><strong>Authors: </strong>Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher Lott</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14160">https://arxiv.org/abs/2402.14160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14160">https://arxiv.org/pdf/2402.14160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14160]] Recursive Speculative Decoding: Accelerating LLM Inference via Sampling  Without Replacement(https://arxiv.org/abs/2402.14160)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree. During RSD's drafting, the tree is built by either Gumbel-Top-$k$ trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of LLM. We empirically evaluate RSD with Llama 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at LLM.</li>
<li><strong>摘要：</strong>推测性解码是一种用于大型语言模型 (LLM) 的推理加速方法，其中小型语言模型生成草稿令牌序列，并由目标 LLM 并行进一步验证。最近的工作通过建立草案令牌树改进了这种方法，实现了优于单序列推测解码的性能。然而，这些作品在树的每个级别独立生成令牌，而不是利用树的整个多样性。此外，对于固定长度的序列，它们的经验优势已经显现出来，隐含地为基于树的方法的法学硕士授予了更多的计算资源。尽管对资源有限的设备很重要，但现有的工作都没有进行具有固定目标计算预算的实证研究。我们提出了递归推测解码（RSD），这是一种基于树的新颖方法，可以在不进行替换的情况下对草案令牌进行采样，并最大化树的多样性。在 RSD 的起草过程中，树是通过 Gumbel-Top-$k$ 技巧构建的，该技巧可以并行地绘制标记而不需要替换，或者通过随机波束搜索来对序列进行采样而不需要替换，同时提前截断不太可能的草稿序列并减少 LLM 的计算成本。我们使用 Llama 2 和 OPT 模型对 RSD 进行了实证评估，结果表明 RSD 优于基线方法，对于固定草案序列长度以及大多数情况下对于法学硕士的固定计算预算而言，RSD 始终优于基线方法。</li>
</ul>

<h3>Title: Bangla AI: A Framework for Machine Translation Utilizing Large Language  Models for Ethnic Media</h3>
<ul>
<li><strong>Authors: </strong>MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14179">https://arxiv.org/abs/2402.14179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14179">https://arxiv.org/pdf/2402.14179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14179]] Bangla AI: A Framework for Machine Translation Utilizing Large Language  Models for Ethnic Media(https://arxiv.org/abs/2402.14179)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Ethnic media, which caters to diaspora communities in host nations, serves as a vital platform for these communities to both produce content and access information. Rather than utilizing the language of the host nation, ethnic media delivers news in the language of the immigrant community. For instance, in the USA, Bangla ethnic media presents news in Bangla rather than English. This research delves into the prospective integration of large language models (LLM) and multi-lingual machine translations (MMT) within the ethnic media industry. It centers on the transformative potential of using LLM in MMT in various facets of news translation, searching, and categorization. The paper outlines a theoretical framework elucidating the integration of LLM and MMT into the news searching and translation processes for ethnic media. Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT in news translation procedures.</li>
<li><strong>摘要：</strong>少数民族媒体迎合东道国的侨民社区，是这些社区制作内容和获取信息的重要平台。少数民族媒体不是使用东道国的语言，而是使用移民社区的语言来传播新闻。例如，在美国，孟加拉族媒体用孟加拉语而不是英语播放新闻。这项研究深入探讨了大语言模型 (LLM) 和多语言机器翻译 (MMT) 在民族媒体行业内的前瞻性整合。它的重点是在新闻翻译、搜索和分类的各个方面使用 MMT 中的法学硕士的变革潜力。本文概述了一个理论框架，阐明了法学硕士和现代机器翻译在少数民族媒体新闻搜索和翻译过程中的整合。此外，它还简要讨论了将 LLM 和 MMT 纳入新闻翻译程序所带来的潜在道德挑战。</li>
</ul>

<h3>Title: Diversity-Aware Ensembling of Language Models Based on Topological Data  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Polina Proskura, Alexey Zaytsev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14184">https://arxiv.org/abs/2402.14184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14184">https://arxiv.org/pdf/2402.14184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14184]] Diversity-Aware Ensembling of Language Models Based on Topological Data  Analysis(https://arxiv.org/abs/2402.14184)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.</li>
<li><strong>摘要：</strong>集成是提高机器学习模型性能的重要工具。在与自然语言处理相关的情况下，由于开源中提供了多个大型模型，集成可以提高方法的性能。然而，现有的方法主要依赖于每个模型具有相同权重的集合对预测进行简单平均，忽略了模型质量和一致性的差异。我们建议不仅使用其个体性能的知识，而且还使用它们彼此之间的相似性来估计 NLP 模型集合的权重。通过采用基于拓扑数据分析（TDA）的距离测量，我们改进了我们的集成。文本分类准确性和相关不确定性估计的质量都得到提高。</li>
</ul>

<h3>Title: Learning to Reduce: Optimal Representations of Structured Data in  Prompting Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, Xiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14195">https://arxiv.org/abs/2402.14195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14195">https://arxiv.org/pdf/2402.14195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14195]] Learning to Reduce: Optimal Representations of Structured Data in  Prompting Large Language Models(https://arxiv.org/abs/2402.14195)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been widely used as general-purpose AI agents showing comparable performance on many downstream tasks. However, existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial. In this paper, we propose a framework, Learning to Reduce, that fine-tunes a language model to generate a reduced version of an input context, given a task description and context input. The model learns to reduce the input context using On-Policy Reinforcement Learning and aims to improve the reasoning performance of a fixed LLM. Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets. We further show that our model helps improve the LLM's performance on downstream tasks especially when the context is long.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已被广泛用作通用人工智能代理，在许多下游任务上表现出可比的性能。然而，现有的工作表明，法学硕士将结构化数据（例如知识图谱、表格、数据库）集成到他们的提示中具有挑战性；法学硕士需要理解长文本数据或在推理之前选择最相关的证据，这两种方法都不是微不足道的。在本文中，我们提出了一个框架“学习减少”，该框架可以在给定任务描述和上下文输入的情况下微调语言模型以生成输入上下文的简化版本。该模型学习使用策略强化学习来减少输入上下文，旨在提高固定 LLM 的推理性能。实验结果表明，我们的模型不仅在从输入上下文中选择相关证据方面达到了相当的准确性，而且还显示了在不同数据集上的普遍性。我们进一步表明，我们的模型有助于提高法学硕士在下游任务上的表现，尤其是在上下文很长的情况下。</li>
</ul>

<h3>Title: Towards Understanding Counseling Conversations: Domain Knowledge and  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Younghun Lee, Dan Goldwasser, Laura Schwab Reese</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14200">https://arxiv.org/abs/2402.14200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14200">https://arxiv.org/pdf/2402.14200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14200]] Towards Understanding Counseling Conversations: Domain Knowledge and  Large Language Models(https://arxiv.org/abs/2402.14200)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Understanding the dynamics of counseling conversations is an important task, yet it is a challenging NLP problem regardless of the recent advance of Transformer-based pre-trained language models. This paper proposes a systematic approach to examine the efficacy of domain knowledge and large language models (LLMs) in better representing conversations between a crisis counselor and a help seeker. We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome. To provide richer context to conversations, we incorporate human-annotated domain knowledge and LLM-generated features; simple integration of domain knowledge and LLM features improves the model performance by approximately 15%. We argue that both domain knowledge and LLM-generated features can be exploited to better characterize counseling conversations when they are used as an additional context to conversations.</li>
<li><strong>摘要：</strong>理解咨询对话的动态是一项重要的任务，但尽管基于 Transformer 的预训练语言模型最近取得了进展，但这仍然是一个具有挑战性的 NLP 问题。本文提出了一种系统方法来检查领域知识和大型语言模型（LLM）在更好地表示危机咨询师和寻求帮助者之间的对话方面的有效性。我们的经验表明，最先进的语言模型（例如基于 Transformer 的模型和 GPT 模型）无法预测对话结果。为了为对话提供更丰富的上下文，我们结合了人工注释的领域知识和法学硕士生成的特征；领域知识和 LLM 功能的简单集成可将模型性能提高约 15%。我们认为，当将领域知识和法学硕士生成的特征用作对话的附加上下文时，可以利用它们来更好地描述咨询对话的特征。</li>
</ul>

<h3>Title: Assisting in Writing Wikipedia-like Articles From Scratch with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14207">https://arxiv.org/abs/2402.14207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14207">https://arxiv.org/pdf/2402.14207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14207]] Assisting in Writing Wikipedia-like Articles From Scratch with Large  Language Models(https://arxiv.org/abs/2402.14207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.</li>
<li><strong>摘要：</strong>我们研究如何应用大型语言模型从头开始编写扎根且有组织的长篇文章，其广度和深度与维基百科页面相当。这个尚未充分探讨的问题在写作前阶段提出了新的挑战，包括如何在写作前研究主题并准备大纲。我们提出了 STORM，一种通过检索和多视角提问来综合主题大纲的写作系统。 STORM 通过以下方式对预写作阶段进行建模：(1) 在研究给定主题时发现不同的观点，(2) 模拟对话，持不同观点的作者向基于可信互联网来源的主题专家提出问题，(3) 整理收集到的信息创建一个轮廓。为了进行评估，我们策划了 FreshWiki（最新高质量维基百科文章的数据集），并制定了大纲评估来评估预写作阶段。我们进一步收集经验丰富的维基百科编辑的反馈。与由大纲驱动的检索增强基线生成的文章相比，STORM 的更多文章被认为是有组织的（绝对增加了 25%）并且覆盖范围广泛（增加了 10%）。专家反馈还有助于识别生成接地气的长文章的新挑战，例如来源偏见转移和不相关事实的过度关联。</li>
</ul>

<h3>Title: Content Conditional Debiasing for Fair Text Embedding</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Deng, Blair Chen, Xiaoxiao Li, Christos Thrampoulidis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14208">https://arxiv.org/abs/2402.14208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14208">https://arxiv.org/pdf/2402.14208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14208]] Content Conditional Debiasing for Fair Text Embedding(https://arxiv.org/abs/2402.14208)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embeddings, representing a pioneering effort in achieving conditional independence for fair text embeddings.</li>
<li><strong>摘要：</strong>减轻机器学习模型中的偏差在自然语言处理 (NLP) 领域受到越来越多的关注。然而，只有少数研究关注公平文本嵌入，这对于现实世界的应用至关重要但具有挑战性。在本文中，我们提出了一种学习公平文本嵌入的新方法。我们通过确保敏感属性和以内容为条件的文本嵌入之间的条件独立性来实现公平，同时保持效用权衡。具体来说，我们强制具有不同敏感属性但相同内容的文本的嵌入与其相应的中性文本的嵌入保持相同的距离。此外，我们通过使用大型语言模型（LLM）将文本扩展到不同的敏感群体来解决缺乏适当训练数据的问题。我们的广泛评估表明，我们的方法有效地提高了公平性，同时保留了嵌入的实用性，代表了在实现公平文本嵌入的条件独立性方面的开创性努力。</li>
</ul>

<h3>Title: COPR: Continual Human Preference Learning via Optimal Policy  Regularization</h3>
<ul>
<li><strong>Authors: </strong>Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14228">https://arxiv.org/abs/2402.14228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14228">https://arxiv.org/pdf/2402.14228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14228]] COPR: Continual Human Preference Learning via Optimal Policy  Regularization(https://arxiv.org/abs/2402.14228)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal policy, which prevents CF and avoids over-emphasizing unbalanced objectives. We also provide formal proof for the learnability of COPR. The experimental results show that COPR outperforms strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4 evaluations and human assessment. Furthermore, we validate the robustness of COPR under various CL settings, including different backbones, replay memory sizes, and learning orders.</li>
<li><strong>摘要：</strong>基于人类反馈的强化学习 (RLHF) 通常用于改善大型语言模型 (LLM) 与人类偏好的一致性。考虑到人类偏好不断变化的本质，与传统的静态对齐相比，持续对齐变得更加重要和实用。然而，由于其复杂的过程，使 RLHF 与持续学习 (CL) 兼容具有挑战性。同时，直接学习新的人类偏好可能会导致历史偏好的灾难性遗忘（CF），从而导致无助或有害的输出。为了克服这些挑战，我们从最优政策理论中汲取灵感，提出了持续最优政策正则化（COPR）方法。 COPR 利用抽样分布作为 CL 的演示和正则化约束。它采用拉格朗日对偶（LD）方法，根据历史最优策略动态正则化当前策略，防止CF并避免过分强调不平衡目标。我们还为 COPR 的可学习性提供了正式的证明。实验结果表明，在基于奖励的 GPT-4 评估和人类评估方面，COPR 在我们提出的基准上优于强大的 CL 基线。此外，我们验证了 COPR 在各种 CL 设置下的鲁棒性，包括不同的主干网、重放内存大小和学习顺序。</li>
</ul>

<h3>Title: MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback  and Dynamic Distance Constraint</h3>
<ul>
<li><strong>Authors: </strong>Xinglin Zhou, Yifu Yuan, Shaofu Yang, Jianye Hao</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14244">https://arxiv.org/abs/2402.14244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14244">https://arxiv.org/pdf/2402.14244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14244]] MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback  and Dynamic Distance Constraint(https://arxiv.org/abs/2402.14244)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into subgoals to guide the right learning direction, subgoals that are too difficult or too easy can still hinder downstream learning efficiency. We propose the Dynamic Distance Constraint (DDC) mechanism dynamically adjusting the space of optional subgoals. Thus MENTOR can generate subgoals matching the low-level policy learning process from easy to hard. Extensive experiments demonstrate that MENTOR uses a small amount of human feedback to achieve significant improvement in complex tasks with sparse rewards.</li>
<li><strong>摘要：</strong>分层强化学习（HRL）为具有稀疏智能代理奖励的复杂任务提供了一种有前景的解决方案，它使用分层框架将任务划分为子目标并按顺序完成它们。然而，当前的方法很难找到合适的子目标来确保稳定的学习过程。如果没有额外的指导，仅依靠探索或启发式方法来确定大目标空间中的子目标是不切实际的。为了解决这个问题，我们提出了一个结合人类反馈和动态距离约束（MENTOR）的通用分层强化学习框架。 MENTOR 充当“导师”，将人类反馈融入高层政策学习中，以找到更好的子目标。对于低层策略，MENTOR 分别设计了探索-利用解耦的双重策略来稳定训练。此外，虽然人类可以简单地将任务分解为子目标来指导正确的学习方向，但太难或太简单的子目标仍然会阻碍下游的学习效率。我们提出动态距离约束（DDC）机制，动态调整可选子目标的空间。因此，MENTOR 可以生成与低级策略学习过程从易到难相匹配的子目标。大量实验表明，MENTOR 使用少量的人类反馈即可在奖励稀疏的复杂任务中实现显着改进。</li>
</ul>

<h3>Title: Eagle: Ethical Dataset Given from Real Interactions</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Kaneko, Danushka Bollegala, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14258">https://arxiv.org/abs/2402.14258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14258">https://arxiv.org/pdf/2402.14258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14258]] Eagle: Ethical Dataset Given from Real Interactions(https://arxiv.org/abs/2402.14258)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content. The existing evaluation metrics and methods to address these ethical challenges use datasets intentionally created by instructing humans to create instances including ethical problems. Therefore, the data does not reflect prompts that users actually provide when utilizing LLM services in everyday contexts. This may not lead to the development of safe LLMs that can address ethical challenges arising in real-world applications. In this paper, we create Eagle datasets extracted from real interactions between ChatGPT and users that exhibit social biases, toxicity, and immoral problems. Our experiments show that Eagle captures complementary aspects, not covered by existing datasets proposed for evaluation and mitigation of such ethical challenges. Our code is publicly available at https://huggingface.co/datasets/MasahiroKaneko/eagle.</li>
<li><strong>摘要：</strong>最近的研究表明，大型语言模型（LLM）存在与伦理相关的问题，例如社会偏见、缺乏道德推理以及生成攻击性内容。解决这些道德挑战的现有评估指标和方法使用通过指导人类创建包括道德问题的实例而有意创建的数据集。因此，该数据并不反映用户在日常环境中使用LLM服务时实际提供的提示。这可能不会导致安全法学硕士的发展，无法解决现实应用中出现的道德挑战。在本文中，我们创建了从 ChatGPT 和用户之间的真实交互中提取的 Eagle 数据集，这些数据集表现出社会偏见、毒性和不道德问题。我们的实验表明，Eagle 捕捉到了互补的方面，而现有的用于评估和缓解此类道德挑战的数据集并未涵盖这些方面。我们的代码可在 https://huggingface.co/datasets/MasahiroKaneko/eagle 上公开获取。</li>
</ul>

<h3>Title: Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form  Medical Question Answering Applications and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Huaxiu Yao, Yue Zhang, Ren Wang, Kaidi Xu, Xiaoshuang Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14259">https://arxiv.org/abs/2402.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14259">https://arxiv.org/pdf/2402.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14259]] Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form  Medical Question Answering Applications and Beyond(https://arxiv.org/abs/2402.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on accurate uncertainty measurement under two standard criteria for correctness evaluation (e.g., WSE outperforms existing state-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, in terms of the potential for real-world medical QA applications, we achieve a significant enhancement in the performance of LLMs when employing sequences with lower uncertainty, identified by WSE, as final answers (e.g., +6.36% accuracy improvement on the COVID-QA dataset), without requiring any additional task-specific fine-tuning or architectural modifications.</li>
<li><strong>摘要：</strong>不确定性估计在确保安全关键的人机交互系统的可靠性方面发挥着关键作用，特别是在医疗领域。然而，在开放式医学问答（QA）任务中，尚未建立量化自由形式答案不确定性的通用方法，其中语义信息有限的不相关单词和序列可能是不确定性的主要来源。生成性不平等的存在。在本文中，我们提出了词序列熵（WSE），它根据语义相关性来校准词和序列级别的不确定性比例，在进行不确定性量化时更加注重关键词和更相关的序列。我们利用 7 个“现成的”大语言模型 (LLM)，在 5 个自由形式的医学 QA 数据集上将 WSE 与 6 种基线方法进行比较，结果表明，WSE 在正确性评估的两个标准下，在准确的不确定性测量方面表现出卓越的性能（例如，WSE 在 MedQA 数据集上的 AUROC 比现有最先进的方法高出 3.23%）。此外，就现实世界的医学 QA 应用的潜力而言，当采用 WSE 确定的不确定性较低的序列作为最终答案时，我们实现了法学硕士的性能显着提高（例如，对 COVID 的准确率提高了 6.36%） -QA 数据集），无需任何额外的特定于任务的微调或架构修改。</li>
</ul>

<h3>Title: Can Large Language Models Detect Misinformation in Scientific News  Reporting?</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Cao, Aishwarya Muralidharan Nair, Elyon Eyimife, Nastaran Jamalipour Soofi, K.P. Subbalakshmi, John R. Wullert II, Chumki Basu, David Shallcross</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14268">https://arxiv.org/abs/2402.14268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14268">https://arxiv.org/pdf/2402.14268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14268]] Can Large Language Models Detect Misinformation in Scientific News  Reporting?(https://arxiv.org/abs/2402.14268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustworthy sources, paired with related abstracts from the CORD-19 database. Our dataset includes both human-written and LLM-generated news articles, making it more comprehensive in terms of capturing the growing trend of using LLMs to generate popular press articles. Then, we identify dimensions of scientific validity in science news articles and explore how this can be integrated into the automated detection of scientific misinformation. We propose several baseline architectures using LLMs to automatically detect false representations of scientific findings in the popular press. For each of these architectures, we use several prompt engineering strategies including zero-shot, few-shot, and chain-of-thought prompting. We also test these architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B, Llama2-13B.</li>
<li><strong>摘要：</strong>大众媒体经常炮制科学事实，意图影响公众舆论和行动，正如 COVID-19 大流行期间所证明的那样。由于这两种媒体类型的写作风格不同，自动检测科学领域的错误信息具有挑战性，并且仍处于起步阶段。大多数关于科学报告有效性的研究都将此问题视为声明验证挑战。在此过程中，需要大量的专家人力来生成适当的声明。我们的解决方案绕过了此步骤，并解决了更现实的场景，在这种场景中，此类明确的、带标签的声明可能不可用。本文的中心研究问题是是否可以使用大型语言模型（LLM）来检测科学报告中的错误信息。为此，我们首先提出一个新的标记数据集 SciNews，其中包含来自可信和不可信来源的 2.4k 条科学新闻报道，并配有来自 CORD-19 数据库的相关摘要。我们的数据集包括人工撰写的新闻文章和法学硕士生成的新闻文章，使其在捕捉使用法学硕士生成流行新闻文章的日益增长的趋势方面更加全面。然后，我们确定科学新闻文章中科学有效性的维度，并探索如何将其整合到科学错误信息的自动检测中。我们提出了几种使用法学硕士的基线架构来自动检测大众媒体中科学发现的虚假陈述。对于每一种架构，我们都使用了几种提示工程策略，包括零样本提示、少样本提示和思维链提示。我们还在 GPT-3.5、GPT-4 和 Llama2-7B、Llama2-13B 上测试了这些架构和提示策略。</li>
</ul>

<h3>Title: Take the Bull by the Horns: Hard Sample-Reweighted Continual Training  Improves LLM Generalization</h3>
<ul>
<li><strong>Authors: </strong>Xuxi Chen, Zhendong Wang, Daouda Sow, Junjie Yang, Tianlong Chen, Yingbin Liang, Mingyuan Zhou, Zhangyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14270">https://arxiv.org/abs/2402.14270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14270">https://arxiv.org/pdf/2402.14270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14270]] Take the Bull by the Horns: Hard Sample-Reweighted Continual Training  Improves LLM Generalization(https://arxiv.org/abs/2402.14270)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protocols. Through rigorous experimentation with various models and datasets, our findings indicate that our sample-targeted methods significantly improve LLM performance across multiple benchmarks, in both continual pre-training and instruction tuning scenarios. Our codes are available at https://github.com/VITA-Group/HardFocusTraining.</li>
<li><strong>摘要：</strong>在快速发展的大型语言模型 (LLM) 领域，一个关键挑战是在高质量训练数据迫在眉睫的短缺情况下增强其能力。我们的研究从使用原始预训练数据集对法学硕士进行轻度持续训练的经验策略开始，特别关注选择性保留会产生中等高损失的样本。与损失最高的样本相比，这些样本被认为信息丰富且有利于模型细化，而损失最高的样本由于与数据噪声和复杂性相关而将被丢弃。然后，我们将该策略形式化为实例重新加权分布鲁棒优化（IR-DRO）的原则框架。 IR-DRO 旨在通过实例重新加权机制动态优先考虑信息样本的训练重点，并通过封闭式解决方案进行简化，以便直接集成到已建立的训练协议中。通过对各种模型和数据集进行严格的实验，我们的研究结果表明，我们的以样本为目标的方法在持续预训练和指令调整场景中显着提高了跨多个基准的 LLM 性能。我们的代码可在 https://github.com/VITA-Group/HardFocusTraining 获取。</li>
</ul>

<h3>Title: Qsnail: A Questionnaire Dataset for Sequential Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Yan Lei, Liang Pang, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14272">https://arxiv.org/abs/2402.14272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14272">https://arxiv.org/pdf/2402.14272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14272]] Qsnail: A Questionnaire Dataset for Sequential Question Generation(https://arxiv.org/abs/2402.14272)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors. However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure. Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure. Specifically, the questions should be relevant and specific to the given research topic and intent. The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly. Moreover, the sequence of questions should follow a logical order, grouping similar topics together. As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets. To address these issues, we present Qsnail, the first dataset specifically constructed for the questionnaire generation task, which comprises 13,168 human-written questionnaires gathered from online platforms. We further conduct experiments on Qsnail, and the results reveal that retrieval models and traditional generative models do not fully align with the given research topic and intents. Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires. Therefore, questionnaire generation is challenging and needs to be further explored. The dataset is available at: https://github.com/LeiyanGithub/qsnail.</li>
<li><strong>摘要：</strong>问卷是一种专业的研究方法，用于对人类的意见、偏好、态度和行为进行定性和定量分析。然而，由于问卷结构错综复杂，设计和评估问卷需要付出巨大的努力。调查问卷包含一系列问题，这些问题必须符合涉及问题、选项和整体结构的复杂约束。具体来说，问题应该与给定的研究主题和意图相关且具体。选项应根据问题量身定制，确保它们相互排斥、完整且合理排序。此外，问题的顺序应遵循逻辑顺序，将相似的主题分组在一起。因此，自动生成调查问卷提出了重大挑战，并且主要由于高质量数据集的稀缺，该领域受到的关注有限。为了解决这些问题，我们提出了 Qsnail，这是专门为问卷生成任务构建的第一个数据集，其中包含从在线平台收集的 13,168 份人工撰写的问卷。我们进一步在 Qsnail 上进行了实验，结果表明检索​​模型和传统生成模型并不完全符合给定的研究主题和意图。大型语言模型虽然与研究主题和意图更密切相关，但在多样性和特异性方面表现出显着的局限性。尽管通过思维链提示和微调进行了增强，但语言模型生成的调查问卷仍然低于人工编写的调查问卷。因此，问卷的生成具有挑战性，需要进一步探索。该数据集位于：https://github.com/LeiyanGithub/qsnail。</li>
</ul>

<h3>Title: Can Language Models Act as Knowledge Bases at Scale?</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan He, Yizhong Wang, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14273">https://arxiv.org/abs/2402.14273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14273">https://arxiv.org/pdf/2402.14273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14273]] Can Language Models Act as Knowledge Bases at Scale?(https://arxiv.org/abs/2402.14273)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating responses to complex queries through large-scale pre-training. However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable. Addressing this gap, our research investigates whether LLMs can effectively store, recall, and reason with knowledge on a large scale comparable to latest knowledge bases (KBs) such as Wikidata. Specifically, we focus on three crucial aspects to study the viability: (1) the efficiency of LLMs with different sizes in memorizing the exact knowledge in the large-scale KB; (2) the flexibility of recalling the memorized knowledge in response to natural language queries; (3) the capability to infer new knowledge through reasoning. Our findings indicate that while LLMs hold promise as large-scale KBs capable of retrieving and responding with flexibility, enhancements in their reasoning capabilities are necessary to fully realize their potential.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在通过大规模预训练理解复杂查询并生成响应方面表现出了卓越的能力。然而，这些模型在大规模结构化知识（尤其是明确涵盖大量事实信息的世界知识）中记忆和推理的功效仍然值得怀疑。为了解决这一差距，我们的研究调查了法学硕士是否能够有效地存储、回忆和推理大规模的知识，与维基数据等最新知识库 (KB) 相当。具体来说，我们关注三个关键方面来研究可行性：（1）不同规模的LLM在大规模知识库中记忆准确知识的效率； （2）针对自然语言查询的回忆记忆知识的灵活性； (3)通过推理推断新知识的能力。我们的研究结果表明，虽然法学硕士有望成为能够灵活检索和响应的大规模知识库，但要充分发挥其潜力，必须增强其推理能力。</li>
</ul>

<h3>Title: GATE X-E : A Challenge Set for Gender-Fair Translations from  Weakly-Gendered Languages</h3>
<ul>
<li><strong>Authors: </strong>Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14277">https://arxiv.org/abs/2402.14277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14277">https://arxiv.org/pdf/2402.14277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14277]] GATE X-E : A Challenge Set for Gender-Fair Translations from  Weakly-Gendered Languages(https://arxiv.org/abs/2402.14277)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with GPT-4 and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.</li>
<li><strong>摘要：</strong>神经机器翻译 (NMT) 的质量和采用率不断提高，但无意中延续的性别偏见仍然是一个重大问题。尽管有大量关于从弱性别语言翻译成英语时的性别偏见的研究，但没有评估这种现象或评估缓解策略的基准。为了解决这一差距，我们引入了 GATE X-E，它是 GATE（Rarrick 等人，2023）语料库的扩展，由土耳其语、匈牙利语、芬兰语和波斯语到英语的人工翻译组成。每个翻译都伴随着阴性、阳性和中性变体。该数据集包含四种语言对中每种语言的 1250 到 1850 个实例，其特点是具有各种句子长度和领域的自然句子，对各种语言现象的翻译重写者提出了挑战。此外，我们提出了一个使用 GPT-4 构建的翻译性别重写解决方案，并使用 GATE X-E 对其进行评估。我们开源我们的贡献，以鼓励进一步研究性别偏见。</li>
</ul>

<h3>Title: Mitigating the Linguistic Gap with Phonemic Representations for Robust  Multilingual Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Haeji Jung, Changdae Oh, Jooeon Kang, Jimin Sohn, Kyungwoo Song, Jinkyu Kim, David R. Mortensen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14279">https://arxiv.org/abs/2402.14279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14279">https://arxiv.org/pdf/2402.14279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14279]] Mitigating the Linguistic Gap with Phonemic Representations for Robust  Multilingual Language Understanding(https://arxiv.org/abs/2402.14279)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.</li>
<li><strong>摘要：</strong>提高多语言理解的方法通常在训练阶段需要多种语言，依赖于复杂的训练技术，而且重要的是，要努力解决高资源语言和低资源语言之间的巨大性能差距。我们假设语言之间的性能差距受到这些语言之间的语言差距的影响，并通过采用音素表示（具体来说，使用音素作为语言模型的输入标记而不是子词）为稳健的多语言语言建模提供了一种新颖的解决方案。我们提供了来自三个跨语言任务的定量证据，证明了音素表示的有效性，并通过跨语言性能差距的理论分析进一步证明了这一点。</li>
</ul>

<h3>Title: TinyLLaVA: A Framework of Small-scale Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, Lei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14289">https://arxiv.org/abs/2402.14289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14289">https://arxiv.org/pdf/2402.14289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14289]] TinyLLaVA: A Framework of Small-scale Large Multimodal Models(https://arxiv.org/abs/2402.14289)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.</li>
<li><strong>摘要：</strong>我们提出了 TinyLLaVA 框架，它为设计和分析小规模大型多模态模型 (LMM) 提供了统一的视角。我们实证研究了不同视觉编码器、连接模块、语言模型、训练数据和训练配方的效果。我们广泛的实验表明，与较大的 LMM 相比，更好的数据质量与更好的训练方案相结合，较小的 LMM 可以始终实现同等的性能。在我们的框架下，我们训练了一系列小型 LMM。我们最好的模型 TinyLLaVA-3.1B 比现有的 7B 模型（例如 LLaVA-1.5 和 Qwen-VL）实现了更好的整体性能。我们希望我们的研究结果可以作为未来在数据扩展、训练设置和模型选择方面的研究的基准。我们的模型重量和代码将公开。</li>
</ul>

<h3>Title: CEV-LM: Controlled Edit Vector Language Model for Shaping Natural  Language Generations</h3>
<ul>
<li><strong>Authors: </strong>Samraj Moorjani, Adit Krishnan, Hari Sundaram</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14290">https://arxiv.org/abs/2402.14290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14290">https://arxiv.org/pdf/2402.14290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14290]] CEV-LM: Controlled Edit Vector Language Model for Shaping Natural  Language Generations(https://arxiv.org/abs/2402.14290)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three metrics while preserving semantic content, using less training data, and containing fewer parameters.</li>
<li><strong>摘要：</strong>随着大规模语言模型成为文本生成的标准，我们更需要根据受众/应用程序来调整生成的文本，使其或多或少简洁、有针对性和信息丰富。现有的控制方法主要调整文本的语义（例如情感、主题）、结构（例如语法树、词性）和词汇（例如关键字/短语包含）属性，但不足以实现复杂的目标例如控制文本复杂性和可读性的节奏。在本文中，我们介绍了 CEV-LM——一种轻量级、半自回归语言模型，它利用约束编辑向量来控制量化文本形状（例如内容节奏）的三个互补指标（速度、体积和迂回性）。我们研究了一组广泛的最先进的 CTG 模型，发现 CEV-LM 对这三个指标提供了明显更有针对性和更精确的控制，同时保留语义内容、使用更少的训练数据并包含更少的参数。</li>
</ul>

<h3>Title: Leveraging Large Language Models for Concept Graph Recovery and Question  Answering in NLP Education</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14293">https://arxiv.org/abs/2402.14293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14293">https://arxiv.org/pdf/2402.14293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14293]] Leveraging Large Language Models for Concept Graph Recovery and Question  Answering in NLP Education(https://arxiv.org/abs/2402.14293)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the domain of Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated promise in text-generation tasks. However, their educational applications, particularly for domain-specific queries, remain underexplored. This study investigates LLMs' capabilities in educational scenarios, focusing on concept graph recovery and question-answering (QA). We assess LLMs' zero-shot performance in creating domain-specific concept graphs and introduce TutorQA, a new expert-verified NLP-focused benchmark for scientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA pairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating concept graphs with LLMs for answering diverse questions. Our results indicate that LLMs' zero-shot concept graph recovery is competitive with supervised methods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs achieve up to 26% F1 score enhancement. Moreover, human evaluation and analysis show that CGLLM generates answers with more fine-grained concepts.</li>
<li><strong>摘要：</strong>在自然语言处理 (NLP) 领域，大型语言模型 (LLM) 在文本生成任务中展现出了良好的前景。然而，它们的教育应用程序，特别是针对特定领域的查询，仍未得到充分开发。本研究调查了法学硕士在教育场景中的能力，重点关注概念图恢复和问答 (QA)。我们评估了法学硕士在创建特定领域概念图方面的零样本表现，并引入了 TutorQA，这是一种经过专家验证、以 NLP 为重点的科学图推理和 QA 基准。 TutorQA 由 5 个任务和 500 个 QA 对组成。为了解决 TutorQA 查询，我们提出了 CGLLM，这是一个将概念图与 LLM 集成的管道，用于回答各种问题。我们的结果表明，法学硕士的零样本概念图恢复与监督方法相比具有竞争力，平均 F1 分数提高了 3%。在 TutorQA 任务中，法学硕士的 F1 分数提高了 26%。此外，人类评估和分析表明，CGLLM 生成的答案具有更细粒度的概念。</li>
</ul>

<h3>Title: Mitigating Biases of Large Language Models in Stance Detection with  Calibration</h3>
<ul>
<li><strong>Authors: </strong>Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Kam-Fai Wong, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14296">https://arxiv.org/abs/2402.14296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14296">https://arxiv.org/pdf/2402.14296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14296]] Mitigating Biases of Large Language Models in Stance Detection with  Calibration(https://arxiv.org/abs/2402.14296)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在许多自然语言处理任务中取得了显着的进展。然而，我们的实验表明，在立场检测任务中，法学硕士可能会由于虚假的情绪-立场相关性以及对某些个人和主题的偏好而产生有偏见的立场，从而损害他们的表现。因此，在本文中，我们建议通过校准（MB-Cal）来减轻法学硕士在姿态检测中的偏差。其中，设计了一种新颖的门控校准网络来减轻法学硕士立场推理结果的偏差。此外，为了使校准更加准确和普遍化，我们构建了反事实增强数据来纠正立场偏差。目标内和零样本姿态检测任务的实验结果表明，所提出的 MB-Cal 可以有效减轻 LLM 的偏差，实现最先进的结果。</li>
</ul>

<h3>Title: Multi-modal Stance Detection: New Datasets and Model</h3>
<ul>
<li><strong>Authors: </strong>Bin Liang, Ang Li, Jingqian Zhao, Lin Gui, Min Yang, Yue Yu, Kam-Fai Wong, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14298">https://arxiv.org/abs/2402.14298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14298">https://arxiv.org/pdf/2402.14298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14298]] Multi-modal Stance Detection: New Datasets and Model(https://arxiv.org/abs/2402.14298)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today's fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our three benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.</li>
<li><strong>摘要：</strong>立场检测是一项具有挑战性的任务，旨在识别社交媒体平台上针对特定目标的舆论。之前关于姿态检测的工作主要集中在纯文本上。在本文中，我们研究了由文本和图像组成的推文的多模式立场检测，这些推文在当今快速增长的社交媒体平台中很普遍，人们经常在这些平台上发布多模式消息。为此，我们基于 Twitter 创建了五个新的不同领域的多模态姿态检测数据集，其中每个示例由文本和图像组成。此外，我们提出了一个简单而有效的目标多模态提示调整框架（TMPT），其中利用目标信息从文本和视觉模态中学习多模态姿态特征。我们三个基准数据集的实验结果表明，所提出的 TMPT 在多模态姿态检测中实现了最先进的性能。</li>
</ul>

<h3>Title: Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize  Encoded Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Jinlan Fu, Shenzhen Huangfu, Hang Yan, See-Kiong Ng, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14310">https://arxiv.org/abs/2402.14310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14310">https://arxiv.org/pdf/2402.14310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14310]] Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize  Encoded Knowledge(https://arxiv.org/abs/2402.14310)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned Llemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We make our code and dataset publicly available at \url{https://github.com/jinlanfu/HSP}.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）最近在各个领域表现出了卓越的通用性。尽管法学硕士拥有丰富的知识，但他们在有效利用编码知识来开发准确和逻辑推理过程方面仍然面临挑战。为了缓解这个问题，我们引入了解决前提示（HSP），它指导模型生成解决问题的提示（例如，特定知识或关键思想），然后生成包含中间推理步骤的解决方案。由于 HSP 与提示方法（例如思想链 (CoT)）正交，因此我们将 HSP 应用于 CoT、从最少到最多、计划和解决以及标准提示。在6个推理基准和4个开源LLM上进行的大量实验结果表明，HSP可以有效提高推理任务的准确性：（1）通过将高质量提示增强的HSP应用于CoT提示，Llama2-70B-Chat显示了提高了9.7。 （2）除了探索免训练的LLM能力之外，我们基于HSP和微调的Llemma-7B构建了HSPMATH数据集，达到了64.3的准确率，超越了GPT-3.5和WizardMath-13B。我们在 \url{https://github.com/jinlanfu/HSP} 上公开提供我们的代码和数据集。</li>
</ul>

<h3>Title: Assessing generalization capability of text ranking models in Polish</h3>
<ul>
<li><strong>Authors: </strong>Sławomir Dadas, Małgorzata Grębowiec</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14318">https://arxiv.org/abs/2402.14318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14318">https://arxiv.org/pdf/2402.14318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14318]] Assessing generalization capability of text ranking models in Polish(https://arxiv.org/abs/2402.14318)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is becoming an increasingly popular technique for integrating internal knowledge bases with large language models. In a typical RAG pipeline, three models are used, responsible for the retrieval, reranking, and generation stages. In this article, we focus on the reranking problem for the Polish language, examining the performance of rerankers and comparing their results with available retrieval models. We conduct a comprehensive evaluation of existing models and those trained by us, utilizing a benchmark of 41 diverse information retrieval tasks for the Polish language. The results of our experiments show that most models struggle with out-of-domain generalization. However, a combination of effective optimization method and a large training dataset allows for building rerankers that are both compact in size and capable of generalization. The best of our models establishes a new state-of-the-art for reranking in the Polish language, outperforming existing models with up to 30 times more parameters.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）正在成为一种日益流行的将内部知识库与大型语言模型集成的技术。在典型的 RAG 管道中，使用三个模型，负责检索、重新排序和生成阶段。在本文中，我们重点关注波兰语的重新排序问题，检查重新排序器的性能并将其结果与可用的检索模型进行比较。我们利用 41 个不同的波兰语信息检索任务的基准，对现有模型和我们训练的模型进行了全面评估。我们的实验结果表明，大多数模型都难以应对域外泛化。然而，有效的优化方法和大型训练数据集的结合允许构建尺寸紧凑且能够泛化的重排序器。我们最好的模型为波兰语重新排名建立了新的最先进技术，其参数比现有模型高出 30 倍。</li>
</ul>

<h3>Title: Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve  Knowledge Base Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14320">https://arxiv.org/abs/2402.14320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14320">https://arxiv.org/pdf/2402.14320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14320]] Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve  Knowledge Base Question Answering(https://arxiv.org/abs/2402.14320)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.</li>
<li><strong>摘要：</strong>基于 LLM 的代理的最新进展在各种任务中都显示出了有希望的结果。然而，它们在回答知识库问题中的用途在很大程度上仍未得到探索。由于缺乏特定于任务的训练数据以及创建以任务为中心的模型结构的复杂性，使用传统方法实现 KBQA 系统具有挑战性。在本文中，我们提出了 Triad，这是一个统一的框架，它利用基于 LLM 的代理来执行 KBQA 任务，该代理具有三个角色。代理被分配三个角色来处理不同的 KBQA 子任务：代理作为掌握各种子任务的多面手，作为选择候选人的决策者，以及作为用知识回答问题的顾问。我们的 KBQA 框架分四个阶段执行，涉及代理多个角色的协作。我们使用三个基准数据集评估了我们的框架的性能，结果表明我们的框架在 LC-QuAD 和 YAGO-QA 基准上优于最先进的系统，F1 分数分别为 11.8% 和 20.7% 。</li>
</ul>

<h3>Title: Understanding and Patching Compositional Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, Ying Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14328">https://arxiv.org/abs/2402.14328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14328">https://arxiv.org/pdf/2402.14328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14328]] Understanding and Patching Compositional Reasoning in LLMs(https://arxiv.org/abs/2402.14328)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modules. Our empirical evidence stands testament to CREME's effectiveness, paving the way for autonomously and continuously enhancing compositional reasoning capabilities in language models.</li>
<li><strong>摘要：</strong>法学硕士标志着一场革命性的转变，但在面对组合推理任务时，他们却步履蹒跚。我们的研究致力于揭示法学硕士组合推理失败的根本原因，发现其中大多数源于不正确生成或利用的隐式推理结果。受我们实证研究结果的启发，我们借助 Logit Lens 和干预实验来剖析法学硕士的内在隐藏状态。这种深入研究揭示了隐式推理结果确实在中间层中浮现出来，并在形成最终的显式推理结果中发挥着因果作用。我们的探索进一步在这些层中定位多头自注意力（MHSA）模块，这些模块是准确生成和利用隐式推理结果的关键。基于上述发现，我们开发了 CREME，一种轻量级方法，通过编辑定位的 MHSA 模块来修补组合推理中的错误。我们的经验证据证明了 CREME 的有效性，为自主、持续增强语言模型的组合推理能力铺平了道路。</li>
</ul>

<h3>Title: INSTRUCTIR: A Benchmark for Instruction Following of Information  Retrieval Models</h3>
<ul>
<li><strong>Authors: </strong>Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin, Hansol Jang, Changwook Jun, Minjoon Seo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14334">https://arxiv.org/abs/2402.14334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14334">https://arxiv.org/pdf/2402.14334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14334]] INSTRUCTIR: A Benchmark for Instruction Following of Information  Retrieval Models(https://arxiv.org/abs/2402.14334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite the critical need to align search targets with users' intention, retrievers often only prioritize query information without delving into the users' intended search context. Enhancing the capability of retrievers to understand intentions and preferences of users, akin to language model instructions, has the potential to yield more aligned search targets. Prior studies restrict the application of instructions in information retrieval to a task description format, neglecting the broader context of diverse and evolving search scenarios. Furthermore, the prevailing benchmarks utilized for evaluation lack explicit tailoring to assess instruction-following ability, thereby hindering progress in this field. In response to these limitations, we propose a novel benchmark,INSTRUCTIR, specifically designed to evaluate instruction-following ability in information retrieval tasks. Our approach focuses on user-aligned instructions tailored to each query instance, reflecting the diverse characteristics inherent in real-world search scenarios. Through experimental analysis, we observe that retrievers fine-tuned to follow task-style instructions, such as INSTRUCTOR, can underperform compared to their non-instruction-tuned counterparts. This underscores potential overfitting issues inherent in constructing retrievers trained on existing instruction-aware retrieval datasets.</li>
<li><strong>摘要：</strong>尽管迫切需要将搜索目标与用户意图保持一致，但检索器通常只对查询信息进行优先级排序，而不深入研究用户预期的搜索上下文。增强检索器理解用户意图和偏好的能力（类似于语言模型指令）有可能产生更一致的搜索目标。先前的研究将信息检索中的指令应用限制为任务描述格式，忽略了多样化和不断发展的搜索场景的更广泛背景。此外，用于评估的现行基准缺乏明确的定制来评估遵循指令的能力，从而阻碍了该领域的进展。针对这些限制，我们提出了一个新的基准，INSTRUCTIR，专门用于评估信息检索任务中的指令跟踪能力。我们的方法侧重于针对每个查询实例量身定制的用户对齐指令，反映了现实世界搜索场景中固有的不同特征。通过实验分析，我们观察到，经过微调以遵循任务型指令的检索器（例如“INSTRUCTOR”）与未经过指令调整的检索器相比，其表现可能较差。这强调了构建在现有指令感知检索数据集上训练的检索器所固有的潜在过度拟合问题。</li>
</ul>

<h3>Title: AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales</h3>
<ul>
<li><strong>Authors: </strong>Hazel Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14337">https://arxiv.org/abs/2402.14337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14337">https://arxiv.org/pdf/2402.14337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14337]] AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales(https://arxiv.org/abs/2402.14337)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationales and low-resource settings.</li>
<li><strong>摘要：</strong>答案背后的基本原理不仅可以解释模型决策，还可以提高语言模型在复杂推理任务上的推理能力。然而，获得无懈可击的理由通常是不可能的。此外，估计基本原理足以鼓励模型性能的忠实程度也并非易事。因此，此类推理任务通常会迫使模型在不理想的理由下输出正确的答案，并且与模型完全能够达到的能力相比，其结果是次优的。在这项工作中，我们提出了如何处理导致任意不确定性的不完美理由。我们首先使用模型先验信念作为信息性，用给定原理的熵分数定义模糊的原理。然后，我们根据基本原理的模糊性引导模型选择两种不同推理模型之一。我们凭经验认为，我们提出的方法针对基本原理和低资源环境的对抗性质量产生了稳健的性能优势。</li>
</ul>

<h3>Title: Rule or Story, Which is a Better Commonsense Expression for Talking with  Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Ning Bian, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14355">https://arxiv.org/abs/2402.14355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14355">https://arxiv.org/pdf/2402.14355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14355]] Rule or Story, Which is a Better Commonsense Expression for Talking with  Large Language Models?(https://arxiv.org/abs/2402.14355)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.</li>
<li><strong>摘要：</strong>由于常识规则的报告偏差和基于规则的常识推理的暴露偏差，用常识构建机器一直是 NLP 领域的长期挑战。相比之下，人类通过故事含蓄地传达和传承常识。本文研究了通过讲故事表达的大语言模型（LLM）固有的常识能力。我们系统地调查和比较故事和规则，以检索和利用法学硕士的常识。在 28 个常识 QA 数据集上的实验结果表明，故事作为从 LLM 检索常识的表达方式优于规则，表现出更高的生成置信度和常识准确性。此外，故事是回答有关日常事件的问题的更有效的常识表达，而规则对于科学问题更有效。这与文本语料库中常识的报告偏差是一致的。我们进一步表明，常识故事的正确性和相关性可以通过迭代的自我监督微调进一步提高。这些发现强调了使用适当的语言来表达、检索和利用法学硕士的常识的重要性，强调了更好地利用他们的常识能力的有希望的方向。</li>
</ul>

<h3>Title: Rethinking Scientific Summarization Evaluation: Grounding Explainable  Metrics on Facet-aware Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao, Zhiyong Lu, Xin Gao, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14359">https://arxiv.org/abs/2402.14359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14359">https://arxiv.org/pdf/2402.14359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14359]] Rethinking Scientific Summarization Evaluation: Grounding Explainable  Metrics on Facet-aware Benchmark(https://arxiv.org/abs/2402.14359)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. This suggests an area for future enhancement of LLMs.</li>
<li><strong>摘要：</strong>预训练和大型语言模型（LLM）的摘要能力已在一般领域得到广泛验证，但它们在涉及复杂句子和专业知识的科学语料库中的使用评估较少。本文提出了科学概括的概念和实验分析，强调了传统评估方法（例如 $n$-gram、嵌入比较和 QA）的不足，特别是在提供解释、掌握科学概念或识别关键内容方面。随后，我们引入了 Facet-aware Metric (FM)，利用 LLM 进行高级语义匹配来评估基于不同方面的摘要。这种分面感知方法通过将评估任务分解为更简单的子任务来提供对摘要的全面评估。认识到该领域缺乏评估基准，我们策划了一个带有分面级注释的基于分面的科学摘要数据集（FD）。我们的研究结果证实，FM 提供了一种更合乎逻辑的方法来评估科学摘要。此外，经过微调的较小模型可以在科学背景下与法学硕士竞争，而法学硕士在从科学领域的上下文信息中学习方面存在局限性。这表明了法学硕士未来的加强领域。</li>
</ul>

<h3>Title: OpenTab: Advancing Large Language Models as Open-domain Table Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Kezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Chuan Lei, Christos Faloutsos, Huzefa Rangwala, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14361">https://arxiv.org/abs/2402.14361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14361">https://arxiv.org/pdf/2402.14361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14361]] OpenTab: Advancing Large Language Models as Open-domain Table Reasoners(https://arxiv.org/abs/2402.14361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.</li>
<li><strong>摘要：</strong>经过大量数据训练的大型语言模型 (LLM) 擅长执行各种自然语言任务，但它们无法处理需要以前未训练过的知识的任务。一种解决方案是使用检索器来获取相关信息以扩展 LLM 的知识范围。然而，由于数据模式多样化和表规模庞大，现有的基于文本检索的法学硕士对于结构化表数据并不理想。在这项工作中，我们提出了 OpenTab，一个由法学硕士支持的开放域表推理框架。总体而言，OpenTab 利用表检索器来获取相关表，然后生成 SQL 程序来有效地解析检索到的表。利用从 SQL 执行中获得的中间数据，进行有根据的推理以产生准确的响应。广泛的实验评估表明，OpenTab 在开放域和封闭域设置中均显着优于基线，准确率提高了 21.5%。我们进一步进行消融研究，以验证我们提出的系统设计的有效性。</li>
</ul>

<h3>Title: Small Language Model Is a Good Guide for Large Language Model in Chinese  Entity Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Xuemei Tang, Jun Wang, Qi Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14373">https://arxiv.org/abs/2402.14373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14373">https://arxiv.org/pdf/2402.14373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14373]] Small Language Model Is a Good Guide for Large Language Model in Chinese  Entity Relation Extraction(https://arxiv.org/abs/2402.14373)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have been successful in relational extraction (RE) tasks, especially in the few-shot learning. An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches. Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem. In our framework, We use the ``\textit{Training-Guide-Predict}'' strategy to combine the strengths of pre-trained language models (PLMs) and LLMs, where a task-specific PLM framework acts as a tutor, transfers task knowledge to the LLM, and guides the LLM in performing RE tasks. Our experiments on a RE dataset rich in relation types show that the approach in this paper facilitates RE of long-tail relation types.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）在关系提取（RE）任务中取得了成功，特别是在小样本学习中。 RE领域的一个重要问题是长尾数据，而目前使用LLM方法对此问题并没有太多关注。因此，在本文中，我们提出了模型协作框架SLCoLM来缓解数据长尾问题。在我们的框架中，我们使用“\textit{Training-Guide-Predict}”策略来结合预训练语言模型 (PLM) 和 LLM 的优势，其中特定于任务的 PLM 框架充当导师，传输任务知识传授给法学硕士，并指导法学硕士执行RE任务。我们在富含关系类型的 RE 数据集上进行的实验表明，本文的方法有利于长尾关系类型的 RE。</li>
</ul>

<h3>Title: Novi jezički modeli za srpski jezik</h3>
<ul>
<li><strong>Authors: </strong>Mihailo Škorić</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14379">https://arxiv.org/abs/2402.14379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14379">https://arxiv.org/pdf/2402.14379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14379]] Novi jezički modeli za srpski jezik(https://arxiv.org/abs/2402.14379)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The paper will briefly present the development history of transformer-based language models for the Serbian language. Several new models for text generation and vectorization, trained on the resources of the Society for Language Resources and Technologies, will also be presented. Ten selected vectorization models for Serbian, including two new ones, will be compared on four natural language processing tasks. Paper will analyze which models are the best for each selected task, how does their size and the size of their training sets affect the performance on those tasks, and what is the optimal setting to train the best language models for the Serbian language.</li>
<li><strong>摘要：</strong>本文将简要介绍塞尔维亚语基于 Transformer 的语言模型的发展历史。还将介绍几种新的文本生成和矢量化模型，这些模型是在语言资源和技术协会的资源上进行培训的。十个选定的塞尔维亚语矢量化模型（包括两个新模型）将在四个自然语言处理任务上进行比较。论文将分析哪些模型最适合每个选定的任务，它们的大小和训练集的大小如何影响这些任务的性能，以及训练塞尔维亚语最佳语言模型的最佳设置是什么。</li>
</ul>

<h3>Title: Enhancing Temporal Knowledge Graph Forecasting with Large Language  Models via Chain-of-History Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiaoyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14382">https://arxiv.org/abs/2402.14382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14382">https://arxiv.org/pdf/2402.14382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14382]] Enhancing Temporal Knowledge Graph Forecasting with Large Language  Models via Chain-of-History Reasoning(https://arxiv.org/abs/2402.14382)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction. To address the third issue, we design CoH as a paly-and-plug module to enhance the performance of graph-based models for TKG prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH.</li>
<li><strong>摘要：</strong>时态知识图（TKG）预测旨在根据给定的历史预测未来的事实。最近的基于图的模型擅长捕获 TKG 中的结构信息，但缺乏语义理解能力。如今，随着LLM的激增，基于LLM的TKG预测模型应运而生。然而，现有的基于LLM的模型存在三个缺点：（1）它只关注一阶历史进行预测，而忽略了高阶历史信息，导致LLM提供的信息极其有限。 (2) 法学硕士在繁重的历史信息负载下难以获得最佳推理性能。 (3)对于TKG预测，单独LLM的时间推理能力是有限的。为了解决前两个挑战，我们提出了历史链（CoH）推理，逐步探索高阶历史，实现法学硕士在 TKG 预测中有效利用高阶历史信息。为了解决第三个问题，我们将 CoH 设计为 paly-and-plug 模块，以增强基于图的 TKG 预测模型的性能。对三个数据集和主干网的广泛实验证明了 CoH 的有效性。</li>
</ul>

<h3>Title: Securing Transactions: A Hybrid Dependable Ensemble Machine Learning  Model using IHT-LR and Grid Search</h3>
<ul>
<li><strong>Authors: </strong>Md. Alamin Talukder, Rakib Hossen, Md Ashraf Uddin, Mohammed Nasir Uddin, Uzzal Kumar Acharjee</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14389">https://arxiv.org/abs/2402.14389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14389">https://arxiv.org/pdf/2402.14389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14389]] Securing Transactions: A Hybrid Dependable Ensemble Machine Learning  Model using IHT-LR and Grid Search(https://arxiv.org/abs/2402.14389)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Financial institutions and businesses face an ongoing challenge from fraudulent transactions, prompting the need for effective detection methods. Detecting credit card fraud is crucial for identifying and preventing unauthorized transactions.Timely detection of fraud enables investigators to take swift actions to mitigate further losses. However, the investigation process is often time-consuming, limiting the number of alerts that can be thoroughly examined each day. Therefore, the primary objective of a fraud detection model is to provide accurate alerts while minimizing false alarms and missed fraud cases. In this paper, we introduce a state-of-the-art hybrid ensemble (ENS) dependable Machine learning (ML) model that intelligently combines multiple algorithms with proper weighted optimization using Grid search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor (KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To address the data imbalance issue, we employ the Instant Hardness Threshold (IHT) technique in conjunction with Logistic Regression (LR), surpassing conventional approaches. Our experiments are conducted on a publicly available credit card dataset comprising 284,807 transactions. The proposed model achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid ensemble model outperforms existing works, establishing a new benchmark for detecting fraudulent transactions in high-frequency scenarios. The results highlight the effectiveness and reliability of our approach, demonstrating superior performance metrics and showcasing its exceptional potential for real-world fraud detection applications.</li>
<li><strong>摘要：</strong>金融机构和企业面临着欺诈交易的持续挑战，因此需要有效的检测方法。检测信用卡欺诈对于识别和防止未经授权的交易至关重要。及时检测欺诈使调查人员能够迅速采取行动，以减少进一步的损失。然而，调查过程通常非常耗时，限制了每天可以彻底检查的警报数量。因此，欺诈检测模型的主要目标是提供准确的警报，同时最大限度地减少误报和遗漏的欺诈案例。在本文中，我们介绍了一种最先进的混合集成（ENS）可靠的机器学习（ML）模型，该模型使用网格搜索将多种算法与适当的加权优化智能结合，包括决策树（DT）、随机森林（RF） ）、K 最近邻（KNN）和多层感知器（MLP），以增强欺诈识别。为了解决数据不平衡问题，我们将即时硬度阈值（IHT）技术与逻辑回归（LR）结合使用，超越了传统方法。我们的实验是在包含 284,807 笔交易的公开信用卡数据集上进行的。所提出的模型在 DT、RF、KNN、MLP 和 ENS 模型上分别实现了 99.66%、99.73%、98.56% 和 99.79% 的令人印象深刻的准确率，以及完美的 100%。混合集成模型优于现有工作，为高频场景中的欺诈交易检测建立了新的基准。结果凸显了我们方法的有效性和可靠性，展示了卓越的性能指标，并展示了其在现实世界欺诈检测应用中的非凡潜力。</li>
</ul>

<h3>Title: On the Tip of the Tongue: Analyzing Conceptual Representation in Large  Language Models with Reverse-Dictionary Probe</h3>
<ul>
<li><strong>Authors: </strong>Ningyu Xu, Qi Zhang, Menghan Zhang, Peng Qian, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14404">https://arxiv.org/abs/2402.14404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14404">https://arxiv.org/pdf/2402.14404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14404]] On the Tip of the Tongue: Analyzing Conceptual Representation in Large  Language Models with Reverse-Dictionary Probe(https://arxiv.org/abs/2402.14404)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commonsense reasoning problems.</li>
<li><strong>摘要：</strong>探索和增强大型语言模型的推理能力仍然是一个至关重要的悬而未决的问题。在这里，我们将反向字典任务重新用作案例研究，以探讨法学硕士的概念推理能力。我们使用上下文学习来指导模型生成语言描述中隐含的对象概念的术语。模型在此任务中稳健地实现了高精度，并且它们的表示空间对有关对象类别和细粒度特征的信息进行编码。进一步的实验表明，尽管模型之间具有类似的句法泛化行为，但反向词典任务所探测的概念推理能力可以预测模型在多个基准上的一般推理性能。探索性分析表明，通过描述 $\Rightarrow$word 示例提示法学硕士可能会引发超越任务解释表面差异的泛化，并促进更广泛的常识推理问题的模型。</li>
</ul>

<h3>Title: Large-Scale Actionless Video Pre-Training via Discrete Diffusion for  Efficient Policy Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14407">https://arxiv.org/abs/2402.14407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14407">https://arxiv.org/pdf/2402.14407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14407]] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for  Efficient Policy Learning(https://arxiv.org/abs/2402.14407)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior generalization ability. Our project website is available at https://video-diff.github.io/.</li>
<li><strong>摘要：</strong>学习能够完成多项任务的通才实体代理提出了挑战，这主要源于动作标记的机器人数据集的稀缺。相比之下，存在大量人类视频，捕捉复杂的任务以及与物理世界的交互。利用无动作的人类视频进行预训练和转移知识以通过有限的机器人演示促进机器人策略学习的前景广阔。在本文中，我们介绍了一种新颖的框架，该框架利用统一的离散扩散将人类视频的生成预训练和少量动作标记的机器人视频的策略微调结合起来。我们首先将人类和机器人视频压缩成统一的视频令牌。在预训练阶段，我们采用具有掩模和替换扩散策略的离散扩散模型来预测潜在空间中的未来视频标记。在微调阶段，我们利用想象中的未来视频来指导在有限的机器人数据集上训练的低级动作学习。实验表明，与以前最先进的方法相比，我们的方法可以生成用于规划的高保真未来视频，并增强了微调策略，具有卓越的泛化能力。我们的项目网站位于 https://video-diff.github.io/。</li>
</ul>

<h3>Title: Transferring BERT Capabilities from High-Resource to Low-Resource  Languages Using Vocabulary Matching</h3>
<ul>
<li><strong>Authors: </strong>Piotr Rybak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14408">https://arxiv.org/abs/2402.14408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14408">https://arxiv.org/pdf/2402.14408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14408]] Transferring BERT Capabilities from High-Resource to Low-Resource  Languages Using Vocabulary Matching(https://arxiv.org/abs/2402.14408)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-trained language models have revolutionized the natural language understanding landscape, most notably BERT (Bidirectional Encoder Representations from Transformers). However, a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models. This work presents a novel approach to bridge this gap by transferring BERT capabilities from high-resource to low-resource languages using vocabulary matching. We conduct experiments on the Silesian and Kashubian languages and demonstrate the effectiveness of our approach to improve the performance of BERT models even when the target language has minimal training data. Our results highlight the potential of the proposed technique to effectively train BERT models for low-resource languages, thus democratizing access to advanced language understanding models.</li>
<li><strong>摘要：</strong>预训练的语言模型彻底改变了自然语言理解领域，最著名的是 BERT（来自 Transformers 的双向编码器表示）。然而，资源匮乏的语言仍然面临着重大挑战，有限的数据阻碍了此类模型的有效训练。这项工作提出了一种新颖的方法来弥合这一差距，即使用词汇匹配将 BERT 功能从高资源语言转移到低资源语言。我们对西里西亚语和 Kashubian 语言进行了实验，证明了即使目标语言的训练数据很少，我们的方法也能有效提高 BERT 模型的性能。我们的结果凸显了所提出的技术在有效训练低资源语言的 BERT 模型方面的潜力，从而使高级语言理解模型的访问民主化。</li>
</ul>

<h3>Title: Tug-of-War Between Knowledge: Exploring and Resolving Knowledge  Conflicts in Retrieval-Augmented Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia Li, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14409">https://arxiv.org/abs/2402.14409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14409">https://arxiv.org/pdf/2402.14409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14409]] Tug-of-War Between Knowledge: Exploring and Resolving Knowledge  Conflicts in Retrieval-Augmented Language Models(https://arxiv.org/abs/2402.14409)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability bias towards common knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence: We reveal that RALMs follow the principle of majority rule, leaning towards placing trust in evidence that appears more frequently. Moreover, we find that RALMs exhibit confirmation bias, and are more willing to choose evidence that is consistent with their internal memory. To solve the challenge of knowledge conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding (CD2) to better calibrate the model's confidence. Experimental results demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.</li>
<li><strong>摘要：</strong>检索增强语言模型（RALM）已显示出通过从外部来源检索证据来完善和扩展内部记忆的巨大潜力。然而，RALM 在将内部记忆与外部资源整合时不可避免地会遇到知识冲突。知识冲突可能会使 RALM 陷入知识之间的拉锯战，从而限制其实际适用性。在本文中，我们重点探讨和解决 RALM 中的知识冲突。首先，我们提出了一个评估框架，用于评估各个维度的知识冲突。然后，我们从以下两个角度研究 RALM 的行为和偏好：（1）内部记忆与外部来源之间的冲突：我们发现更强的 RALM 会随着邓宁-克鲁格效应而出现，即使有正确的证据，也会持续偏向其错误的内部记忆提供。此外，RALM 表现出对常识的可用性偏见； (2)真实、不相关和误导性证据之间的冲突：我们发现RALM遵循多数决定原则，倾向于信任出现频率更高的证据。此外，我们发现 RALM 表现出确认偏差，并且更愿意选择与其内部记忆一致的证据。为了解决知识冲突的挑战，我们提出了一种称为冲突解缠对比解码（CD2）的方法，以更好地校准模型的置信度。实验结果表明，我们的 CD2 可以有效解决 RALM 中的知识冲突。</li>
</ul>

<h3>Title: Automating Psychological Hypothesis Generation with AI: Large Language  Models Meet Causal Graph</h3>
<ul>
<li><strong>Authors: </strong>Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14424">https://arxiv.org/abs/2402.14424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14424">https://arxiv.org/pdf/2402.14424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14424]] Automating Psychological Hypothesis Generation with AI: Large Language  Models Meet Causal Graph(https://arxiv.org/abs/2402.14424)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p<0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal knowledge graphs can revolutionize automated discovery in psychology, extracting novel insights from the extensive literature. This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research.</li>
<li><strong>摘要：</strong>利用因果知识图和大型语言模型（LLM）之间的协同作用，我们的研究引入了一种用于心理学计算假设生成的突破性方法。我们使用法学硕士分析了 43,312 篇心理学文章，以提取因果关系对。该分析产生了专门的心理学因果图。应用链接预测算法，我们生成了 130 个关注“幸福感”的潜在心理假设，然后将它们与博士学者构思的研究思想以及法学硕士单独提出的研究思想进行比较。有趣的是，我们的法学硕士和因果图的组合方法在新颖性方面反映了专家级的见解，明显超越了仅法学硕士的假设（t（59）= 3.34，p=0.007和t（59）= 4.32，p<分别为 0.001）。使用深度语义分析进一步证实了这种一致性。我们的结果表明，将法学硕士与因果知识图等机器学习技术相结合，可以彻底改变心理学的自动发现，从大量文献中提取新颖的见解。这项工作处于心理学和人工智能的十字路口，为心理学研究中数据驱动的假设生成提供了一种新的丰富范式。</li>
</ul>

<h3>Title: KoCoSa: Korean Context-aware Sarcasm Detection Dataset</h3>
<ul>
<li><strong>Authors: </strong>Yumin Kim, Heejae Suh, Mingi Kim, Dongyeon Won, Hwanhee Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14428">https://arxiv.org/abs/2402.14428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14428">https://arxiv.org/pdf/2402.14428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14428]] KoCoSa: Korean Context-aware Sarcasm Detection Dataset(https://arxiv.org/abs/2402.14428)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on the dataset show that our baseline system outperforms strong baselines like large language models, such as GPT-3.5, in the Korean sarcasm detection task. We show that the sarcasm detection task relies deeply on the existence of sufficient context. We will release the dataset at https://anonymous.4open.science/r/KoCoSa-2372.</li>
<li><strong>摘要：</strong>讽刺是一种口头讽刺的方式，其中有人说与他们的意思相反的内容，通常是为了嘲笑一个人、情况或想法。通常很难检测对话中的讽刺，因为检测讽刺应该反映上下文（即对话历史）。在本文中，我们介绍了一个用于韩语对话讽刺检测任务的新数据集，KoCoSa（韩国语境感知讽刺检测数据集），它由 12.8K 每日韩语对话和最后一个响应上的该任务的标签组成。为了构建数据集，我们提出了一种有效的讽刺检测数据集生成管道：1）使用大型语言模型从源对话生成新的讽刺对话，2）自动和手动过滤异常和有毒对话，以及3）用于讽刺检测的人工注释任务。我们还为在我们的数据集上训练的韩国讽刺检测任务提供了一个简单但有效的基线。数据集上的实验结果表明，我们的基线系统在韩语讽刺检测任务中优于大型语言模型（例如 GPT-3.5）等强基线。我们表明，讽刺检测任务很大程度上依赖于足够上下文的存在。我们将在 https://anonymous.4open.science/r/KoCoSa-2372 发布数据集。</li>
</ul>

<h3>Title: A Language Model's Guide Through Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Dimitri von Rütte, Sotiris Anagnostidis, Gregor Bachmann, Thomas Hofmann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14433">https://arxiv.org/abs/2402.14433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14433">https://arxiv.org/pdf/2402.14433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14433]] A Language Model's Guide Through Latent Space(https://arxiv.org/abs/2402.14433)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive tuning to work, or even experience confusion. Moreover, we find that probes with optimal detection accuracies do not necessarily make for the optimal guides, contradicting previous observations for truthfulness. Our work warrants a deeper investigation into the interplay between detectability, guidability, and the nature of the concept, and we hope that our rich experimental test-bed for guidance research inspires stronger follow-up approaches.</li>
<li><strong>摘要：</strong>概念引导已成为一种廉价而简单的方法，通过探测概念向量的隐藏表示并使用它们在推理时扰乱激活来控制语言模型的行为。虽然以前的工作重点主要集中在真实性上，但在本文中，我们将这个框架扩展到更丰富的概念，例如适当性、幽默、创造力和质量，并探讨当前的检测和指导策略在这些具有挑战性的环境中发挥作用的程度。为了便于评估，我们开发了一种新的概念指导指标，该指标考虑了概念启发的成功以及指导模型流畅性的潜在下降。我们广泛的实验表明，虽然诸如诚实之类的一些概念更容易使用当前技术进行指导，但诸如适当性或幽默之类的新颖概念仍然难以引出，需要进行大量调整才能发挥作用，甚至会遇到混乱。此外，我们发现具有最佳检测精度的探针不一定能提供最佳指导，这与之前的真实性观察相矛盾。我们的工作需要对可检测性、可引导性和概念本质之间的相互作用进行更深入的研究，我们希望我们丰富的指导研究实验平台能够激发更强有力的后续方法。</li>
</ul>

<h3>Title: Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?</h3>
<ul>
<li><strong>Authors: </strong>Seiji Gobara, Hidetaka Kamigaito, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14453">https://arxiv.org/abs/2402.14453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14453">https://arxiv.org/pdf/2402.14453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14453]] Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?(https://arxiv.org/abs/2402.14453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Education that suits the individual learning level is necessary to improve students' understanding. The first step in achieving this purpose by using large language models (LLMs) is to adjust the textual difficulty of the response to students. This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text. To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of question-answering-based conversation. Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response. We also observed that some LLMs can surpass humans in handling text difficulty and the importance of instruction-tuning.</li>
<li><strong>摘要：</strong>适合个人学习水平的教育对于提高学生的理解力是必要的。使用大型语言模型 (LLM) 实现此目的的第一步是调整对学生的回答的文本难度。这项工作分析了法学硕士如何隐式调整用户输入及其生成的文本之间的文本难度。为了进行实验，我们从 Stack-Overflow 创建了一个新的数据集来探索基于问答的对话的性能。 Stack-Overflow 数据集和 TSCC 数据集（包括多轮对话）上的实验结果表明，LLM 可以隐式处理用户输入与其生成的响应之间的文本困难。我们还观察到，一些法学硕士在处理文本难度和指令调整的重要性方面可以超越人类。</li>
</ul>

<h3>Title: Annotation and Classification of Relevant Clauses in  Terms-and-Conditions Contracts</h3>
<ul>
<li><strong>Authors: </strong>Pietro Giovanni Bizzaro, Elena Della Valentina, Maurizio Napolitano, Nadia Mana, Massimo Zancanaro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14457">https://arxiv.org/abs/2402.14457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14457">https://arxiv.org/pdf/2402.14457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14457]] Annotation and Classification of Relevant Clauses in  Terms-and-Conditions Contracts(https://arxiv.org/abs/2402.14457)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种新的注释方案，对条款和条件合同中不同类型的条款进行分类，最终目标是支持法律专家快速识别和评估此类法律文件中的问题。为此，我们构建了一个小型条款和条件合约语料库，并最终确定了 14 个类别的注释方案，最终达到 0.92 的注释者间一致性。然后，对于其中的 11 个，我们使用多语言 T5 的少样本提示和两个基于 BERT 的意大利语 LLM 的两个微调版本来试验二元分类任务。我们的实验表明，通过在验证任务中达到 0.79 到 0.95 的准确度，自动分类我们的类别的可行性。</li>
</ul>

<h3>Title: Reframing the Expected Free Energy: Four Formulations and a Unification</h3>
<ul>
<li><strong>Authors: </strong>Théophile Champion, Howard Bowman, Dimitrije Marković, Marek Grześ</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14460">https://arxiv.org/abs/2402.14460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14460">https://arxiv.org/pdf/2402.14460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14460]] Reframing the Expected Free Energy: Four Formulations and a Unification(https://arxiv.org/abs/2402.14460)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Active inference is a leading theory of perception, learning and decision making, which can be applied to neuroscience, robotics, psychology, and machine learning. Active inference is based on the expected free energy, which is mostly justified by the intuitive plausibility of its formulations, e.g., the risk plus ambiguity and information gain / pragmatic value formulations. This paper seek to formalize the problem of deriving these formulations from a single root expected free energy definition, i.e., the unification problem. Then, we study two settings, each one having its own root expected free energy definition. In the first setting, no justification for the expected free energy has been proposed to date, but all the formulations can be recovered from it. However, in this setting, the agent cannot have arbitrary prior preferences over observations. Indeed, only a limited class of prior preferences over observations is compatible with the likelihood mapping of the generative model. In the second setting, a justification of the root expected free energy definition is known, but this setting only accounts for two formulations, i.e., the risk over states plus ambiguity and entropy plus expected energy formulations.</li>
<li><strong>摘要：</strong>主动推理是感知、学习和决策的领先理论，可应用于神经科学、机器人学、心理学和机器学习。主动推理基于预期的自由能，这主要通过其公式的直观合理性来证明，例如风险加模糊性和信息增益/实用价值公式。本文试图将从单根期望自由能定义导出这些公式的问题形式化，即统一问题。然后，我们研究两种设置，每种设置都有自己的根期望自由能定义。在第一种设置中，迄今为止尚未提出预期自由能的合理性，但所有公式都可以从中恢复。然而，在这种情况下，智能体不能对观察有任意的先验偏好。事实上，只有有限类别的对观察的先验偏好与生成模型的似然映射兼容。在第二种设置中，根预期自由能定义的合理性是已知的，但该设置仅考虑两种公式，即状态风险加上模糊性和熵加上预期能量公式。</li>
</ul>

<h3>Title: Data Science with LLMs and Interpretable Models</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Bordt, Ben Lengerich, Harsha Nori, Rich Caruana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14474">https://arxiv.org/abs/2402.14474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14474">https://arxiv.org/pdf/2402.14474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14474]] Data Science with LLMs and Interpretable Models(https://arxiv.org/abs/2402.14474)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans. In this work, we show that large language models (LLMs) are remarkably good at working with interpretable models, too. In particular, we show that LLMs can describe, interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique. LLMs can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an open-source LLM-GAM interface.</li>
<li><strong>摘要：</strong>近年来，在构建可解释模型和易于人类理解的机器学习模型方面取得了重要进展。在这项工作中，我们表明大型语言模型 (LLM) 也非常擅长与可解释模型一起工作。特别是，我们证明法学硕士可以描述、解释和调试广义加性模型（GAM）。将 LLM 的灵活性与 GAM 准确描述的统计模式的广度相结合，可以实现数据集汇总、问题回答和模型批判。法学硕士还可以改善领域专家和可解释模型之间的互动，并生成有关潜在现象的假设。我们发布 \url{https://github.com/interpretml/TalkToEBM} 作为开源 LLM-GAM 接口。</li>
</ul>

<h3>Title: Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation  and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Takehiro Takayanagi, Masahiro Suzuki, Ryotaro Kobayashi, Hiroki Sakaji, Kiyoshi Izumi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14484">https://arxiv.org/abs/2402.14484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14484">https://arxiv.org/pdf/2402.14484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14484]] Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation  and Analysis(https://arxiv.org/abs/2402.14484)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency to falsely recognize non-causal sequences as causal sequences. These issues become even more pronounced with advanced versions of the model, such as GPT-4. In addition, we highlight the constraints of ChatGPT in handling complex causality types, including both intra/inter-sentential and implicit causality. The model also faces challenges with effectively leveraging in-context learning and domain adaptation. Our code is available on \url{https://github.com/retarfi/gemcausal}</li>
<li><strong>摘要：</strong>因果关系是人类认知的基础，并引起了各个研究领域的关注。随着文本数据量的不断增长，辨别文本数据中的因果关系至关重要，而因果文本挖掘在提取有意义的模式方面发挥着关键作用。本研究对ChatGPT的因果文本挖掘能力进行了综合评估。首先，我们引入一个超越一般英语数据集的基准，包括特定领域和非英语数据集。我们还提供了一个评估框架，以确保 ChatGPT 和以前的方法之间进行公平比较。最后，我们的分析概述了使用 ChatGPT 进行因果文本挖掘的局限性和未来的挑战。具体来说，我们的分析表明 ChatGPT 是各种数据集的良好起点。然而，当配备足够量的训练数据时，之前的模型仍然超越 ChatGPT 的性能。此外，ChatGPT 还存在将非因果序列错误地识别为因果序列的倾向。对于 GPT-4 等模型的高级版本，这些问题变得更加明显。此外，我们强调了 ChatGPT 在处理复杂因果关系类型（包括句内/句间因果关系和隐式因果关系）方面的限制。该模型还面临着有效利用上下文学习和领域适应的挑战。我们的代码可以在 \url{https://github.com/retarfi/gemcausal} 上找到</li>
</ul>

<h3>Title: Does the Generator Mind its Contexts? An Analysis of Generative Model  Faithfulness under Context Transfer</h3>
<ul>
<li><strong>Authors: </strong>Xinshuo Hu, Baotian Hu, Dongfang Li, Xiaoguang Li, Lifeng Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14488">https://arxiv.org/abs/2402.14488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14488">https://arxiv.org/pdf/2402.14488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14488]] Does the Generator Mind its Contexts? An Analysis of Generative Model  Faithfulness under Context Transfer(https://arxiv.org/abs/2402.14488)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>The present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation. However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations. To gain deeper insights into the underlying causes of this phenomenon, we conduct a series of experiments that verify the critical role played by context in hallucination, both during training and testing, from various perspectives.</li>
<li><strong>摘要：</strong>本研究引入了知识增强生成器，该生成器专门设计用于生成仍然基于上下文知识的信息，而不管上下文如何变化。之前的研究主要集中在检查静态输入产生的幻觉，例如摘要或机器翻译领域。然而，我们的研究深入研究了动态知识存在下生成式问答的忠实度。我们的目标是探索当情境知识发生变化时参数记忆引起的幻觉是否存在，同时分析其发生的根本原因。为了有效地解决这个问题，我们提出了一种简单而有效的措施来检测此类幻觉。有趣的是，我们的调查发现，所有模型都表现出将先前答案生成为幻觉的倾向。为了更深入地了解这种现象的根本原因，我们进行了一系列实验，从不同的角度验证了训练和测试过程中情境在幻觉中所起的关键作用。</li>
</ul>

<h3>Title: INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction  Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Wei Han, Hui Chen, Soujanya Poria</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14492">https://arxiv.org/abs/2402.14492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14492">https://arxiv.org/pdf/2402.14492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14492]] INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction  Fine-tuning(https://arxiv.org/abs/2402.14492)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks. It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times. Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training data multiple times.</li>
<li><strong>摘要：</strong>在多任务指令跟踪数据上微调大型语言模型（LLM）已被证明是一种强大的学习范例，可以提高其在新任务上的零样本能力。最近关于高质量指令跟踪数据生成和选择的工作需要大量的人力来为给定的任务构思模型可理解的指令，并仔细过滤法学硕士生成的数据。在这项工作中，我们在多模态任务中引入了一种名为 INSTRAUG 的自动指令增强方法。它从一些基本且简单的元指令开始，但可以将指令跟踪数据集扩展 30 倍。两个流行的多模态指令跟踪基准 MULTIINSTRUCT 和 InstructBLIP 的结果表明，INSTRAUG 可以显着改善 12 个多模态任务中多模态大语言模型 (MLLM) 的对齐，这甚至相当于多次扩展训练数据的好处。</li>
</ul>

<h3>Title: Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment  Pre-training for Noisy Slot Filling Task</h3>
<ul>
<li><strong>Authors: </strong>Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song, Daichi Guo, Weiran Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14494">https://arxiv.org/abs/2402.14494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14494">https://arxiv.org/pdf/2402.14494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14494]] Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment  Pre-training for Noisy Slot Filling Task(https://arxiv.org/abs/2402.14494)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to improve the model's robustness. Experimental results demonstrate the superiority of our proposed approach over state-of-the-art models, and further analysis confirms its effectiveness and generalization ability.</li>
<li><strong>摘要：</strong>在现实的对话系统中，用户的输入信息经常受到各种类型的输入扰动，这会影响槽填充任务。尽管基于规则的数据增强方法取得了令人满意的结果，但在面对未知的噪声干扰时，它们无法表现出所需的泛化能力。在本研究中，我们通过提出 Noise-BERT（一种具有噪声对齐预训练的统一扰动鲁棒框架）来解决槽填充中输入扰动带来的挑战。我们的框架包含两个噪声对齐预训练任务：槽位掩蔽预测和句子噪声判别，旨在指导预训练语言模型捕获准确的槽位信息和噪声分布。在微调过程中，我们采用对比学习损失来增强实体和标签的语义表示。此外，我们引入了对抗性攻击训练策略来提高模型的鲁棒性。实验结果证明了我们提出的方法相对于最先进模型的优越性，进一步的分析证实了其有效性和泛化能力。</li>
</ul>

<h3>Title: "My Answer is C": First-Token Probabilities Do Not Match Text Answers in  Instruction-Tuned Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul Röttger, Frauke Kreuter, Dirk Hovy, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14499">https://arxiv.org/abs/2402.14499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14499">https://arxiv.org/pdf/2402.14499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14499]] "My Answer is C": First-Token Probabilities Do Not Match Text Answers in  Instruction-Tuned Language Models(https://arxiv.org/abs/2402.14499)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output, too and ii) caution against relying solely on first-token evaluation.</li>
<li><strong>摘要：</strong>语言生成的开放性使得自回归大型语言模型（LLM）的评估具有挑战性。一种常见的评估方法使用多项选择题 (MCQ) 来限制回答空间。然后通过根据第一个标记预测的对数概率对候选答案进行排名来评估模型。然而，由于模型的响应风格多种多样，例如以“当然”开头或拒绝回答，第一个标记可能无法始终如一地反映最终的响应输出。因此，MCQ 评估并不表示与用户交互时的模型行为。但增加多少呢？我们评估第一个标记评估与文本输出在几个维度上的一致性，即最终选项选择、拒绝率、选择分布和提示扰动下的鲁棒性。我们的结果表明，这两种方法在所有维度上都严重错位，错配率超过 60%。根据对话或安全数据进行大量微调的模型尤其受到影响。至关重要的是，即使我们越来越多地限制提示，即强制它们以选项字母或示例模板开头，模型仍然不一致。我们的发现 i) 也强调了检查文本输出的重要性，ii) 警告不要仅依赖第一个令牌评估。</li>
</ul>

<h3>Title: Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap  for Prompt-Based Large Language Models and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Hainiu Xu, Lin Gui, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14522">https://arxiv.org/abs/2402.14522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14522">https://arxiv.org/pdf/2402.14522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14522]] Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap  for Prompt-Based Large Language Models and Beyond(https://arxiv.org/abs/2402.14522)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-model scenarios, whilst maintaining their performance to be comparable to architecture-specific methods.</li>
<li><strong>摘要：</strong>任务嵌入是一种捕获特定任务信息的元学习技术，它已经变得很流行，特别是在多任务学习、模型编辑和可解释性等领域。然而，随着以无梯度方式运行的提示引导大型语言模型（LLM）的出现，它面临着挑战。现有的任务嵌入方法依赖于微调的、特定于任务的语言模型，这阻碍了任务嵌入在不同模型中的适应性，尤其是基于提示的法学硕士。为了在法学硕士时代释放任务嵌入的力量，我们提出了一个统一任务嵌入（FUTE）框架，在单个向量空间内协调来自各种模型的任务嵌入，包括较小的语言模型和具有不同提示的法学硕士。这种一致性使得能够比较和分析不同模型之间的相似性，扩展了现有任务嵌入方法在解决多模型场景中的范围和实用性，同时保持其性能与特定于架构的方法相当。</li>
</ul>

<h3>Title: Balanced Data Sampling for Language Model Training with Clustering</h3>
<ul>
<li><strong>Authors: </strong>Yunfan Shao, Linyang Li, Zhaoye Fei, Hang Yan, Dahua Lin, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14526">https://arxiv.org/abs/2402.14526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14526">https://arxiv.org/pdf/2402.14526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14526]] Balanced Data Sampling for Language Model Training with Clustering(https://arxiv.org/abs/2402.14526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models.</li>
<li><strong>摘要：</strong>数据在大型语言模型（LLM）的训练中发挥着基础作用。虽然数据集的收集和组成受到了关注，但确定训练中的数据采样策略仍然是一个悬而未决的问题。大多数法学硕士都采用简单的策略进行培训，即随机抽样。然而，这种采样策略忽略了训练数据分布的不平衡性质，这可能不是最优的。在本文中，我们提出 ClusterClip Sampling 来平衡训练数据的文本分布，以实现更好的模型训练。具体来说，ClusterClip Sampling利用数据聚类来反映训练集的数据分布，并根据聚类结果平衡训练过程中的常见样本和稀有样本。引入重复剪辑操作来减轻由来自某些集群的样本导致的过度拟合问题。大量的实验验证了 ClusterClip 采样的有效性，它在各种训练数据集和大型语言模型下优于随机采样和其他基于集群的采样变体。</li>
</ul>

<h3>Title: Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt  Politeness on LLM Performance</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14531">https://arxiv.org/abs/2402.14531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14531">https://arxiv.org/pdf/2402.14531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14531]] Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt  Politeness on LLM Performance(https://arxiv.org/abs/2402.14531)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs). Polite language in human communications often garners more compliance and effectiveness, while rudeness can cause aversion, impacting response quality. We consider that LLMs mirror human communication traits, suggesting they align with human cultural norms. We assess the impact of politeness in prompts on LLMs across English, Chinese, and Japanese tasks. We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language. This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts. Our findings highlight the need to factor in politeness for cross-cultural natural language processing and LLM usage.</li>
<li><strong>摘要：</strong>我们研究了提示中的礼貌水平对大型语言模型 (LLM) 性能的影响。人类交流中的礼貌语言通常会带来更多的依从性和有效性，而粗鲁的语言可能会引起厌恶，影响响应质量。我们认为法学硕士反映了人类的沟通特征，表明它们符合人类文化规​​范。我们评估了提示中的礼貌对法学硕士在英语、中文和日语任务中的影响。我们观察到，不礼貌的提示往往会导致表现不佳，但过于礼貌的语言并不能保证更好的结果。根据语言的不同，最佳的礼貌程度也不同。这种现象表明法学硕士不仅反映人类行为，而且还受到语言的影响，特别是在不同的文化背景下。我们的研究结果强调了在跨文化自然语言处理和法学硕士使用中考虑礼貌的必要性。</li>
</ul>

<h3>Title: Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for  GPT-3.5, GPT-4 and Bard</h3>
<ul>
<li><strong>Authors: </strong>Ariel Rosenfeld, Teddy Lazebnik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14533">https://arxiv.org/abs/2402.14533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14533">https://arxiv.org/pdf/2402.14533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14533]] Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for  GPT-3.5, GPT-4 and Bard(https://arxiv.org/abs/2402.14533)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality. However, it is unclear whether LLMs tend to exhibit distinctive linguistic styles akin to how human authors do. Through a comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse inputs. The results point to significant linguistic variations which, in turn, enable us to attribute a given text to its LLM origin with a favorable 88\% accuracy using a simple off-the-shelf classification model. Theoretical and practical implications of this intriguing finding are discussed.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）能够生成类似于或超越人类质量的文本。然而，目前尚不清楚法学硕士是否倾向于表现出类似于人类作者的独特语言风格。通过全面的语言分析，我们将当今三种最流行的 LLMS（GPT-3.5、GPT-4 和 Bard）生成的文本的词汇、词性 (POS) 分布、依存分布和情感进行了比较多样化的投入。结果表明存在显着的语言差异，这反过来又使我们能够使用简单的现成分类模型将给定文本归因于其 LLM 起源，准确率高达 88%。讨论了这一有趣发现的理论和实践意义。</li>
</ul>

<h3>Title: Less is More: Mitigating Multimodal Hallucination from an EOS Decision  Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zihao Yue, Liang Zhang, Qin Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14545">https://arxiv.org/abs/2402.14545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14545">https://arxiv.org/pdf/2402.14545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14545]] Less is More: Mitigating Multimodal Hallucination from an EOS Decision  Perspective(https://arxiv.org/abs/2402.14545)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>Large Multimodal Models (LMMs) often suffer from multimodal hallucinations, wherein they may create content that is not present in the visual inputs. In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits. By investigating how the model decides to terminate generation with EOS, the special end-of-sentence token, we find that the model assesses the completeness of the entire sequence by comparing the generated text with the image. This observation suggests that the model possesses an inherent potential of making proper EOS decisions based on its visual perception to avoid overly lengthy outputs. To take advantage of such potential, we explore two methods to mitigate multimodal hallucinations: a training objective that enables the model to reduce hallucinations by learning from regular instruction data, and a data filtering strategy to prevent harmful training data from exacerbating model hallucinations. Both methods significantly improve the hallucination performance of LMMs, without requiring any additional data or knowledge.</li>
<li><strong>摘要：</strong>大型多模态模型 (LMM) 经常遭受多模态幻觉的困扰，其中它们可能会创建视觉输入中不存在的内容。在本文中，我们探讨了这个问题的新角度：过于详细的训练数据阻碍了模型及时终止生成的能力，导致持续输出超出视觉感知限制。通过研究模型如何决定用 EOS（特殊的句尾标记）终止生成，我们发现模型通过将生成的文本与图像进行比较来评估整个序列的完整性。这一观察结果表明，该模型具有根据其视觉感知做出正确 EOS 决策的内在潜力，以避免输出过长。为了利用这种潜力，我们探索了两种减轻多模态幻觉的方法：训练目标使模型能够通过从常规指令数据中学习来减少幻觉，以及数据过滤策略以防止有害的训练数据加剧模型幻觉。这两种方法都显着提高了 LMM 的幻觉性能，而不需要任何额外的数据或知识。</li>
</ul>

<h3>Title: OmniPred: Language Models as Universal Regressors</h3>
<ul>
<li><strong>Authors: </strong>Xingyou Song, Oscar Li, Chansoo Lee, Bangding (Jeffrey)Yang, Daiyi Peng, Sagi Perel, Yutian Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14547">https://arxiv.org/abs/2402.14547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14547">https://arxiv.org/pdf/2402.14547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14547]] OmniPred: Language Models as Universal Regressors(https://arxiv.org/abs/2402.14547)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.</li>
<li><strong>摘要：</strong>在广泛的实验设计领域中，回归一直是一种强大的工具，可以在给定一组参数的情况下准确预测系统或模型的结果指标，但传统上仅限于仅适用于特定任务的方法。在本文中，我们提出了 OmniPred，这是一个框架，用于将语言模型训练为通用的端到端回归器，基于来自不同现实世界实验的 $(x,y)$ 评估数据。使用来自 Google Vizier（世界上最大的黑盒优化数据库之一）的数据，我们进行了大量的实验表明，仅通过数学参数和值的文本表示，语言模型就能够进行非常精确的数值回归，并且如果有机会训练在多个任务上，可以显着优于传统的回归模型。</li>
</ul>

<h3>Title: LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A  Survey</h3>
<ul>
<li><strong>Authors: </strong>Ashok Urlana, Charaka Vinayak Kumar, Ajeet Kumar Singh, Bala Mallikarjunarao Garlapati, Srinivasa Rao Chalamala, Rahul Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14558">https://arxiv.org/abs/2402.14558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14558">https://arxiv.org/pdf/2402.14558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14558]] LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A  Survey(https://arxiv.org/abs/2402.14558)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks. From natural language processing and sentiment analysis to content generation and personalized recommendations, their unparalleled adaptability has facilitated widespread adoption across industries. This transformative shift driven by LLMs underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization. In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context. To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为推动众多工业应用的秘密因素，展示了其在各种任务中卓越的多功能性。从自然语言处理和情感分析到内容生成和个性化推荐，它们无与伦比的适应性促进了各行业的广泛采用。由法学硕士推动的这一变革性转变强调需要探索潜在的相关挑战和提高其利用率的途径。在本文中，我们的目标是阐明和评估在工业背景下利用法学硕士所固有的障碍和机遇。为此，我们对一组行业从业者进行了一项调查，根据收集到的见解提出了四个研究问题，并审查了 68 篇行业论文来解决这些问题并得出有意义的结论。</li>
</ul>

<h3>Title: LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named  Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Junjie Ye, Nuo Xu, Yikun Wang, Jie Zhou, Qi Zhang, Tao Gui, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14568">https://arxiv.org/abs/2402.14568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14568">https://arxiv.org/pdf/2402.14568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14568]] LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named  Entity Recognition(https://arxiv.org/abs/2402.14568)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the impressive capabilities of large language models (LLMs), their performance on information extraction tasks is still not entirely satisfactory. However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. In this paper, we propose $LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot NER task. To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels. Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. Extensive experiments demonstrate the effectiveness of our approach in enhancing NER model performance with limited data. Furthermore, additional analyses provide further evidence supporting the assertion that the quality of the data we generate surpasses that of other existing methods.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）的能力令人印象深刻，但它们在信息提取任务上的表现仍然不完全令人满意。然而，他们卓越的重写能力和广泛的世界知识为改进这些任务提供了宝贵的见解。在本文中，我们提出了 $LLM-DA$，这是一种基于 LLM 的新型数据增强技术，用于少样本 NER 任务。为了克服现有数据增强方法的局限性（损害语义完整性）并解决 LLM 生成的文本固有的不确定性，我们通过在上下文和实体级别增强原始数据来利用 NER 任务的独特特征。我们的方法涉及采用 14 种上下文重写策略，设计相同类型的实体替换，并结合噪声注入来增强鲁棒性。大量的实验证明了我们的方法在利用有限数据增强 NER 模型性能方面的有效性。此外，额外的分析提供了进一步的证据，支持我们生成的数据质量超过其他现有方法的断言。</li>
</ul>

<h3>Title: The Role of LLMs in Sustainable Smart Cities: Applications, Challenges,  and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Amin Ullah, Guilin Qi, Saddam Hussain, Irfan Ullah, Zafar Ali</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14596">https://arxiv.org/abs/2402.14596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14596">https://arxiv.org/pdf/2402.14596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14596]] The Role of LLMs in Sustainable Smart Cities: Applications, Challenges,  and Future Directions(https://arxiv.org/abs/2402.14596)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Smart cities stand as pivotal components in the ongoing pursuit of elevating urban living standards, facilitating the rapid expansion of urban areas while efficiently managing resources through sustainable and scalable innovations. In this regard, as emerging technologies like Artificial Intelligence (AI), the Internet of Things (IoT), big data analytics, and fog and edge computing have become increasingly prevalent, smart city applications grapple with various challenges, including the potential for unauthorized disclosure of confidential and sensitive data. The seamless integration of emerging technologies has played a vital role in sustaining the dynamic pace of their development. This paper explores the substantial potential and applications of Deep Learning (DL), Federated Learning (FL), IoT, Blockchain, Natural Language Processing (NLP), and large language models (LLMs) in optimizing ICT processes within smart cities. We aim to spotlight the vast potential of these technologies as foundational elements that technically strengthen the realization and advancement of smart cities, underscoring their significance in driving innovation within this transformative urban milieu. Our discourse culminates with an exploration of the formidable challenges that DL, FL, IoT, Blockchain, NLP, and LLMs face within these contexts, and we offer insights into potential future directions.</li>
<li><strong>摘要：</strong>智慧城市是不断追求提高城市生活水平的关键组成部分，促进城市地区的快速扩张，同时通过可持续和可扩展的创新有效管理资源。在这方面，随着人工智能（AI）、物联网（IoT）、大数据分析以及雾和边缘计算等新兴技术变得越来越普遍，智慧城市应用程序面临着各种挑战，包括未经授权披露的可能性机密和敏感数据。新兴技术的无缝集成在维持其动态发展步伐方面发挥了至关重要的作用。本文探讨了深度学习 (DL)、联邦学习 (FL)、物联网、区块链、自然语言处理 (NLP) 和大型语言模型 (LLM) 在优化智慧城市 ICT 流程方面的巨大潜力和应用。我们的目标是强调这些技术作为基础要素的巨大潜力，从技术上加强智慧城市的实现和进步，强调它们在推动变革性城市环境中创新的重要性。我们的演讲最终探讨了深度学习、深度学习、物联网、区块链、自然语言处理和法学硕士在这些背景下面临的巨大挑战，并提供了对未来潜在方向的见解。</li>
</ul>

<h3>Title: The Impact of Word Splitting on the Semantic Content of Contextualized  Word Representations</h3>
<ul>
<li><strong>Authors: </strong>Aina Garí Soler, Matthieu Labeau, Chloé Clavel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14616">https://arxiv.org/abs/2402.14616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14616">https://arxiv.org/pdf/2402.14616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14616]] The Impact of Word Splitting on the Semantic Content of Contextualized  Word Representations(https://arxiv.org/abs/2402.14616)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.</li>
<li><strong>摘要：</strong>当从语言模型中导出上下文化的单词表示时，需要决定如何获取被分割为子词的词汇外 (OOV) 单词的表示。用单个向量表示这些单词的最佳方式是什么？这些表示的质量是否比词汇表中单词的表示质量更差？我们对涉及 OOV 单词的语义相似性任务的不同模型的嵌入进行了内在评估。我们的分析表明，除了其他有趣的发现之外，分割单词的表示质量通常（但并非总是）比已知单词的嵌入质量更差。然而，必须谨慎解释它们的相似值。</li>
</ul>

<h3>Title: Cleaner Pretraining Corpus Curation with Neural Web Scraping</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Xu, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Chenyan Xiong, Ge Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14652">https://arxiv.org/abs/2402.14652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14652">https://arxiv.org/pdf/2402.14652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14652]] Cleaner Pretraining Corpus Curation with Neural Web Scraping(https://arxiv.org/abs/2402.14652)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.</li>
<li><strong>摘要：</strong>网络蕴藏着大规模、多样、丰富的信息，满足了人类的信息检索需求。通过细致的数据收集、预处理和整理，网页可以作为语言模型预训练的基础数据资源。然而，当面对网页的逐渐革命性和复杂性时，基于规则/基于特征的网络抓取工具变得越来越不够用。本文提出了一种简单、快速且有效的神经网络爬虫（NeuScraper）来帮助从网页中提取主要且干净的文本内容。实验结果表明，NeuScraper 超越了基线抓取器，实现了 20% 以上的改进，展示了其在提取更高质量数据以促进语言模型预训练方面的潜力。所有代码均可在 https://github.com/OpenMatch/NeuScraper 获取。</li>
</ul>

<h3>Title: ConceptMath: A Bilingual Concept-wise Benchmark for Measuring  Mathematical Reasoning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14660">https://arxiv.org/abs/2402.14660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14660">https://arxiv.org/pdf/2402.14660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14660]] ConceptMath: A Bilingual Concept-wise Benchmark for Measuring  Mathematical Reasoning of Large Language Models(https://arxiv.org/abs/2402.14660)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models.</li>
<li><strong>摘要：</strong>本文介绍了 ConceptMath，这是一种双语（英语和中文）细粒度基准，用于评估大型语言模型 (LLM) 的概念数学推理。与以平均准确度评估一般数学推理的传统基准不同，ConceptMath 在数学概念的层次结构下系统地组织数学问题，以便可以在不同的粒度上以概念方面的准确度评估数学推理。基于我们的ConcepthMath，我们评估了广泛的法学硕士，并且我们观察到现有的法学硕士，尽管在传统基准上实现了较高的平均准确度，但在不同的数学概念上表现出显着的性能差异，甚至可能在最基本的数学概念上出现灾难性的失败。此外，我们还引入了有效的微调策略来弥补现有法学硕士的弱点。最后，我们希望ConceptMath能够引导开发者了解其模型的细粒度数学能力，并促进基础模型的发展。</li>
</ul>

<h3>Title: Middleware for LLMs: Tools Are Instrumental for Language Agents in  Complex Environments</h3>
<ul>
<li><strong>Authors: </strong>Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, Yu Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14672">https://arxiv.org/abs/2402.14672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14672">https://arxiv.org/pdf/2402.14672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14672]] Middleware for LLMs: Tools Are Instrumental for Language Agents in  Complex Environments(https://arxiv.org/abs/2402.14672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with these tools, GPT-4 achieves 2.8X the performance of the best baseline in tasks requiring access to database content and 2.2X in KB tasks. Our findings illuminate the path for advancing language agents in complex real-world applications.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的应用已经远远超出了文本处理的范围，标志着一个新时代的到来，LLM 被设想为能够在复杂的现实环境中运行的通用语言代理。这些环境通常非常广泛，使得法学硕士无法在其短期记忆中处理它们。受最近关于使用工具扩展法学硕士能力的研究的启发，本文研究了工具在处理此类复杂性方面增强法学硕士的有趣潜力。为此，我们设计了定制工具来帮助在这些巨大的环境中进行主动探索。此类工具可以充当中间件层，保护法学硕士免受环境复杂性的影响。在两个具有代表性的复杂环境——知识库（KB）和数据库——中，我们展示了在复杂环境中使用工具增强语言代理的巨大潜力。值得注意的是，配备这些工具后，GPT-4 在需要访问数据库内容的任务中实现了最佳基准性能的 2.8 倍，在 KB 任务中实现了 2.2 倍的性能。我们的研究结果阐明了在复杂的现实应用中推进语言代理的道路。</li>
</ul>

<h3>Title: Is Cognition and Action Consistent or Not: Investigating Large Language  Model's Personality</h3>
<ul>
<li><strong>Authors: </strong>Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14679">https://arxiv.org/abs/2402.14679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14679">https://arxiv.org/pdf/2402.14679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14679]] Is Cognition and Action Consistent or Not: Investigating Large Language  Model's Personality(https://arxiv.org/abs/2402.14679)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this study, we investigate the reliability of Large Language Models (LLMs) in professing human-like personality traits through responses to personality questionnaires. Our goal is to evaluate the consistency between LLMs' professed personality inclinations and their actual "behavior", examining the extent to which these models can emulate human-like personality patterns. Through a comprehensive analysis of LLM outputs against established human benchmarks, we seek to understand the cognition-action divergence in LLMs and propose hypotheses for the observed results based on psychological theories and metrics.</li>
<li><strong>摘要：</strong>在这项研究中，我们通过对人格问卷的回答，调查了大型语言模型（LLM）在表达类人人格特征方面的可靠性。我们的目标是评估法学硕士自称的人格倾向与他们实际“行为”之间的一致性，检查这些模型可以在多大程度上模拟类人的人格模式。通过根据既定的人类基准对法学硕士的输出进行全面分析，我们试图了解法学硕士的认知与行动差异，并根据心理学理论和指标对观察到的结果提出假设。</li>
</ul>

<h3>Title: Q-Probe: A Lightweight Approach to Reward Maximization for Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Kenneth Li, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg, David Brandfonbrener</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14688">https://arxiv.org/abs/2402.14688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14688">https://arxiv.org/pdf/2402.14688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14688]] Q-Probe: A Lightweight Approach to Reward Maximization for Language  Models(https://arxiv.org/abs/2402.14688)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be trained on top of an API since it only assumes access to sampling and embeddings. Code: https://github.com/likenneth/q_probe .</li>
<li><strong>摘要：</strong>我们提出了一种称为 Q-probing 的方法来调整预先训练的语言模型，以最大化特定于任务的奖励函数。在较高层面上，Q 探测介于微调等较重的方法和少量镜头提示等较轻的方法之间，但也可以与任何一种方法结合使用。这个想法是在模型的嵌入空间上学习一个简单的线性函数，可用于重新加权候选完成。我们从理论上证明，随着样本数量的增加，该采样过程相当于 Q 探针的 KL 约束最大化。为了训练 Q 探针，我们考虑奖励建模或一类基于重要性加权策略梯度的新型直接策略学习目标。通过这种技术，我们在具有真实奖励（代码生成）以及由偏好数据定义的隐式奖励的领域中看到了收益，甚至优于数据有限制度中的微调。此外，Q-probe 可以在 API 之上进行训练，因为它只假设可以访问采样和嵌入。代码：https://github.com/likenneth/q_probe。</li>
</ul>

<h3>Title: UFO: a Unified and Flexible Framework for Evaluating Factuality of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Ji-rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14690">https://arxiv.org/abs/2402.14690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14690">https://arxiv.org/pdf/2402.14690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14690]] UFO: a Unified and Flexible Framework for Evaluating Factuality of Large  Language Models(https://arxiv.org/abs/2402.14690)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or \textit{hallucination}. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose \texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most QA tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented QA tasks. In news fact generation tasks, search engine results and LLM knowledge are essential. Our dataset and code are available at \url{https://github.com/WaldenRUC/UFO}.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）可能会生成与人类知识缺乏一致性的文本，从而导致事实不准确或\textit{幻觉}。评估法学硕士真实性的现有研究涉及使用法学硕士提取事实主张，并根据预定义的事实来源对其进行验证。然而，这些评估指标是特定于任务的，不可扩展，并且不同任务中事实来源的可替代性尚未得到充分探索。为了应对这些挑战，我们对四种可用的事实来源进行了分类：人工编写的证据、参考文档、搜索引擎结果和法学硕士知识，以及包含六个代表性数据集的五个文本生成任务。然后，我们提出 \texttt{UFO}，一个基于 LLM 的统一且灵活的评估框架，用于根据即插即用的事实源验证事实。我们基于这个框架实施了五个评估场景。实验结果表明，对于大多数 QA 任务，人工编写的证据和参考文档至关重要，并且它们可以在检索增强的 QA 任务中相互替代。在新闻事实生成任务中，搜索引擎结果和法学硕士知识至关重要。我们的数据集和代码可在 \url{https://github.com/WaldenRUC/UFO} 获取。</li>
</ul>

<h3>Title: Unveiling Linguistic Regions in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14700">https://arxiv.org/abs/2402.14700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14700">https://arxiv.org/pdf/2402.14700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14700]] Unveiling Linguistic Regions in Large Language Models(https://arxiv.org/abs/2402.14700)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions exist for different monolingual families, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common occurrence observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已表现出相当大的跨语言对齐和泛化能力。目前的研究主要集中在提高法学硕士的跨语言泛化能力。然而，目前对于法学硕士实现跨语言对齐的内在机制仍缺乏研究。本文从区域划分的角度，对法学硕士的语言能力进行了多项研究。我们发现法学硕士的一个核心区域与语言能力相对应，约占模型参数总数的 1%。通过将参数设置为零来删除此核心区域会导致 30 种不同语言的性能显着下降。此外，该核心区域表现出显着的维度依赖性，即使特定维度上的单个参数的扰动也会导致语言能力的丧失。此外，我们发现不同的单语族存在不同的区域，对这些特定区域的破坏会大大降低法学硕士对相应语言的熟练程度。我们的研究还表明，在进一步预训练期间冻结核心语言区域可以缓解灾难性遗忘（CF）问题，这是法学硕士进一步预训练期间常见的情况。总体而言，探索法学硕士的职能区域可以深入了解他们的智力基础。</li>
</ul>

<h3>Title: COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies  with Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl Corcoran, Guillermo Cecchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14701">https://arxiv.org/abs/2402.14701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14701">https://arxiv.org/pdf/2402.14701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14701]] COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies  with Language Modeling(https://arxiv.org/abs/2402.14701)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic modeling techniques in combination with generative language prompting, we analyze the topical characteristics of different psychiatric conditions and incorporate temporal modeling to capture the evolution of topics at a turn-level resolution. This combined framework enhances the understanding of therapeutic interactions, enabling timely feedback for therapists regarding conversation quality and providing interpretable insights to improve the effectiveness of psychotherapy.</li>
<li><strong>摘要：</strong>治疗工作联盟是预测心理治疗成功的关键因素。传统上，工作联盟评估依赖于治疗师和患者共同完成的问卷。在本文中，我们提出了 COMPASS，这是一种新颖的框架，可以从心理治疗课程中使用的自然语言直接推断出治疗工作联盟。我们的方法利用先进的大语言模型来分析心理治疗会话的笔录，并将其与工作联盟库存中陈述的分布式表示进行比较。通过分析涵盖不同精神疾病的 950 多个会话的数据集，我们证明了我们的方法在显微绘制患者-治疗师对齐轨迹、为临床精神病学提供可解释性以及识别与所治疗疾病相关的新兴模式方面的有效性。通过采用各种神经主题建模技术与生成语言提示相结合，我们分析了不同精神疾病的主题特征，并结合时间建模来捕捉轮级分辨率的主题演变。这个组合框架增强了对治疗相互作用的理解，使治疗师能够及时反馈有关对话质量的信息，并提供可解释的见解以提高心理治疗的有效性。</li>
</ul>

<h3>Title: InfFeed: Influence Functions as a Feedback to Improve the Performance of  Subjective Tasks</h3>
<ul>
<li><strong>Authors: </strong>Somnath Banerjee, Maulindu Sarkar, Punyajoy Saha, Binny Mathew, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14702">https://arxiv.org/abs/2402.14702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14702">https://arxiv.org/pdf/2402.14702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14702]] InfFeed: Influence Functions as a Feedback to Improve the Performance of  Subjective Tasks(https://arxiv.org/abs/2402.14702)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction. Our objectives in this paper are twofold. First we incorporate influence functions as a feedback into the model to improve its performance. Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially `silver' annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance. To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance. Toward the first objective, we adjust the label of the target instance based on its influencer(s) label. In doing this, InfFeed outperforms the state-of-the-art baselines (including LLMs) by a maximum macro F1-score margin of almost 4% for hate speech classification, 3.5% for stance classification, and 3% for irony and 2% for sarcasm detection. Toward the second objective we show that manually re-annotating only those silver annotated data points in the extension set that have a negative influence can immensely improve the model performance bringing it very close to the scenario where all the data points in the extension set have gold labels. This allows for huge reduction of the number of data points that need to be manually annotated since out of the silver annotated extension dataset, the influence function scheme picks up ~1/1000 points that need manual correction.</li>
<li><strong>摘要：</strong>最近，影响函数提出了一种通过量化可能影响测试预测的各个训练实例的扰动来实现深度神经模型的可解释性的装置。我们在本文中的目标是双重的。首先，我们将影响函数作为反馈纳入模型中，以提高其性能。其次，在数据集扩展练习中，使用影响函数自动识别最初由某些现有方法“银色”注释的数据点，并且需要由注释器交叉检查（和纠正）以提高模型性能。为了实现这些目标，在本文中，我们引入了 InfFeed，它使用影响函数来计算目标实例的影响实例。为了实现第一个目标，我们根据目标实例的影响者标签调整其标签。在此过程中，InfFeed 的表现优于最先进的基线（包括法学硕士），仇恨言论分类的最大宏观 F1 分数差距接近 4%、立场分类为 3.5%、反讽为 3% 以及 2%用于讽刺检测。针对第二个目标，我们表明，仅手动重新注释扩展集中那些具有负面影响的银色注释数据点可以极大地提高模型性能，使其非常接近扩展集中所有数据点都具有金色注释的场景标签。这可以大大减少需要手动注释的数据点的数量，因为在银色注释的扩展数据集中，影响函数方案选取了约 1/1000 需要手动校正的点。</li>
</ul>

<h3>Title: An LLM-Enhanced Adversarial Editing System for Lexical Simplification</h3>
<ul>
<li><strong>Authors: </strong>Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14704">https://arxiv.org/abs/2402.14704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14704">https://arxiv.org/pdf/2402.14704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14704]] An LLM-Enhanced Adversarial Editing System for Lexical Simplification(https://arxiv.org/abs/2402.14704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.</li>
<li><strong>摘要：</strong>词汇简化（LS）旨在在词汇层面简化文本。现有方法严重依赖注释数据，使其在资源匮乏的场景中应用具有挑战性。在本文中，我们提出了一种无需并行语料库的新型 LS 方法。该方法采用对抗性编辑系统，在混淆损失和不变性损失的指导下预测原始句子中的词汇编辑。同时，我们引入了一种创新的 LLM 增强损失，能够将大型语言模型（LLM）中的知识提炼到小型 LS 系统中。由此，句子中的复杂单词被屏蔽，并设计了一个难度感知填充模块，用更简单的单词替换被屏蔽的位置。最后，对三个基准 LS 数据集的广泛实验结果和分析证明了我们提出的方法的有效性。</li>
</ul>

<h3>Title: IEPile: Unearthing Large-Scale Schema-Based Information Extraction  Corpus</h3>
<ul>
<li><strong>Authors: </strong>Honghao Gui, Hongbin Ye, Lin Yuan, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14710">https://arxiv.org/abs/2402.14710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14710">https://arxiv.org/pdf/2402.14710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14710]] IEPile: Unearthing Large-Scale Schema-Based Information Extraction  Corpus(https://arxiv.org/abs/2402.14710)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在各个领域都展现出巨大的潜力；然而，它们在信息提取（IE）方面表现出显着的性能差距。需要注意的是，高质量的教学数据是提升法学硕士专业能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化模式。为此，我们引入了IEPile，一个全面的双语（英文和中文）IE指令语料库，其中包含约0.32B个token。我们通过收集和清理 33 个现有 IE 数据集来构建 IEPile，并引入基于模式的指令生成来挖掘大规模语料库。 LLaMA 和 Baichuan 上的实验结果表明，使用 IEPile 可以增强 IE 的 LLM 的性能，尤其是零样本泛化能力。我们开源了资源和预训练模型，希望为 NLP 社区提供宝贵的支持。</li>
</ul>

<h3>Title: Efficient and Effective Vocabulary Expansion Towards Multilingual Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seungduk Kim, Seungtaek Choi, Myeongho Jeong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14714">https://arxiv.org/abs/2402.14714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14714">https://arxiv.org/pdf/2402.14714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14714]] Efficient and Effective Vocabulary Expansion Towards Multilingual Large  Language Models(https://arxiv.org/abs/2402.14714)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0} ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We open-source our models on Huggingface to empower the open research community in various languages.</li>
<li><strong>摘要：</strong>本报告介绍了 \texttt{EEVE-Korean-v1.0}，这是对大型语言模型的韩语改编，在英语和韩语文本理解方面表现出卓越的能力。基于最近能力很强但以英语为中心的法学硕士，例如 SOLAR-10.7B 和 Phi-2，其中以英语为中心的分词器处理非英语文本的效率很低，我们提出了一种高效且有效的词汇扩展（EEVE）方法，该方法包括参数冻结和子字初始化。与之前认为新嵌入需要数万亿个训练令牌的努力相比，我们证明我们的方法可以在短短 20 亿个令牌内显着提高非英语熟练程度。截至 2024 年 1 月，我们的模型 \texttt{EEVE-Korean-10.8B-v1.0} 超越了 Open Ko-LLM 排行榜上大多数指令调整的 LLM，成为开源社区中领先的韩国预训练模型，根据 Hugging Face 的排行榜。我们在 Huggingface 上开源我们的模型，以支持各种语言的开放研究社区。</li>
</ul>

<h3>Title: Back to Basics: Revisiting REINFORCE Style Optimization for Learning  from Human Feedback in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Ahmet Üstün, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14740">https://arxiv.org/abs/2402.14740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14740">https://arxiv.org/pdf/2402.14740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14740]] Back to Basics: Revisiting REINFORCE Style Optimization for Learning  from Human Feedback in LLMs(https://arxiv.org/abs/2402.14740)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed "RL-free" methods such as DPO and RAFT. Our work suggests that careful adaptation to LLMs alignment characteristics enables benefiting from online RL optimization at low cost.</li>
<li><strong>摘要：</strong>以人类反馈强化学习 (RLHF) 形式进行的人工智能对齐越来越被视为高性能大型语言模型的关键要素。 \textsc{近端策略优化}（PPO）已被最近的文献定位为 RLHF 的 RL 部分的规范方法。然而，它涉及高计算成本和敏感的超参数调整。我们认为导致 PPO 发展的大多数激励原则在 RLHF 中不太受实际关注，并主张采用一种计算成本较低的方法来保持甚至提高性能。我们在强化学习的背景下重新审视人类偏好对齐的\textit{公式}。以简单性为指导原则，我们证明 PPO 的许多组件在 RLHF 环境中是不必要的，并且简单得多的 REINFORCE 式优化变体优于 PPO 和新提出的“无 RL”方法（例如 DPO 和 RAFT）。我们的工作表明，仔细适应法学硕士的对齐特征可以以低成本从在线强化学习优化中受益。</li>
</ul>

<h3>Title: Dependency Annotation of Ottoman Turkish with Multilingual BERT</h3>
<ul>
<li><strong>Authors: </strong>Şaziye Betül Özateş, Tarık Emre Tıraş, Efe Eren Genç, Esma Fatıma Bilgin Taşdemir</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14743">https://arxiv.org/abs/2402.14743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14743">https://arxiv.org/pdf/2402.14743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14743]] Dependency Annotation of Ottoman Turkish with Multilingual BERT(https://arxiv.org/abs/2402.14743)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study introduces a pretrained large language model-based annotation methodology for the first dependency treebank in Ottoman Turkish. Our experimental results show that, iteratively, i) pseudo-annotating data using a multilingual BERT-based parsing model, ii) manually correcting the pseudo-annotations, and iii) fine-tuning the parsing model with the corrected annotations, we speed up and simplify the challenging dependency annotation process. The resulting treebank, that will be a part of the Universal Dependencies (UD) project, will facilitate automated analysis of Ottoman Turkish documents, unlocking the linguistic richness embedded in this historical heritage.</li>
<li><strong>摘要：</strong>本研究为奥斯曼土耳其语的第一个依存关系树库引入了一种基于预训练大型语言模型的注释方法。我们的实验结果表明，i）使用基于 BERT 的多语言解析模型对数据进行伪注释，ii）手动纠正伪注释，以及 iii）使用纠正后的注释微调解析模型，我们加快了简化具有挑战性的依赖注释过程。由此产生的树库将成为通用依赖项 (UD) 项目的一部分，将促进对奥斯曼土耳其文献的自动分析，解锁这一历史遗产中蕴藏的丰富语言。</li>
</ul>

<h3>Title: Large Language Models as Urban Residents: An LLM Agent Framework for  Personal Mobility Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14744">https://arxiv.org/abs/2402.14744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14744">https://arxiv.org/pdf/2402.14744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14744]] Large Language Models as Urban Residents: An LLM Agent Framework for  Personal Mobility Generation(https://arxiv.org/abs/2402.14744)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This research marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.</li>
<li><strong>摘要：</strong>本文介绍了一种使用大型语言模型（LLM）集成到代理框架中的新颖方法，以实现灵活高效的个人移动性生成。法学硕士通过有效处理语义数据并提供对各种任务建模的多功能性，克服了以前模型的局限性。我们的方法解决了将法学硕士与现实世界城市交通数据结合起来的迫切需求，重点关注三个研究问题：将法学硕士与丰富的活动数据结合起来，制定可靠的活动生成策略，以及探索法学硕士在城市交通中的应用。关键的技术贡献是一个新颖的法学硕士代理框架，该框架考虑了个人活动模式和动机，包括使法学硕士与现实世界活动数据保持一致的自洽方法以及用于生成可解释活动的检索增强策略。在实验研究中，使用真实世界数据进行综合验证。这项研究标志着基于现实世界人类活动数据设计用于活动生成的 LLM 代理框架的开创性工作，为城市交通分析提供了一个有前景的工具。</li>
</ul>

<h3>Title: Scaling Efficient LLMs</h3>
<ul>
<li><strong>Authors: </strong>B.N. Kausik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14746">https://arxiv.org/abs/2402.14746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14746">https://arxiv.org/pdf/2402.14746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14746]] Scaling Efficient LLMs(https://arxiv.org/abs/2402.14746)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.</li>
<li><strong>摘要：</strong>训练有素的法学硕士通常很稀疏，因为大多数参数都为零，这引发了效率问题。作为回应，我们研究了高效的法学硕士，即那些在训练语料库上达到所需准确性的参数最少的法学硕士。具体来说，我们比较了当前规模下训练损失的理论和经验估计，以获得自然训练语料库中唯一序列数量作为其大小的函数的上限和下限。我们的结果意味着 (1) 要使训练语料库中代表的技能数量加倍，语料库必须大致扩展三到五倍 (2) 对于高效的法学硕士，参数数量 $N$ 和参数的大小 $D$自然训练语料库规模为 $N \sim D^{0.58}$ (3) 如果 LLM 的参数数量小于训练语料库中唯一序列的数量，则扩大规模可以发现新兴技能。</li>
</ul>

<h3>Title: Prompting a Pretrained Transformer Can Be a Universal Approximator</h3>
<ul>
<li><strong>Authors: </strong>Aleksandar Petrov, Philip H.S. Torr, Adel Bibi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.FA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14753">https://arxiv.org/abs/2402.14753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14753">https://arxiv.org/pdf/2402.14753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14753]] Prompting a Pretrained Transformer Can Be a Universal Approximator(https://arxiv.org/abs/2402.14753)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it. Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length. Beyond these density-type results, we also offer Jackson-type bounds on the length of the prefix needed to approximate a function to a desired precision.</li>
<li><strong>摘要：</strong>尽管变压器模型的提示、即时调整和前缀调整被广泛采用，但我们对这些微调方法的理论理解仍然有限。一个关键问题是是否可以通过提示或前缀调整来任意修改预训练模型的行为。形式上，提示和前缀调整预训练模型是否可以普遍逼近序列到序列函数。本文的答案是肯定的，并证明比之前想象的小得多的预训练模型在添加前缀时可以成为通用逼近器。事实上，注意力机制特别适合万能逼近，前缀调整单个注意力头足以逼近任何连续函数。此外，任何序列到序列函数都可以通过在序列长度中添加深度为线性的变换器来近似。除了这些密度类型的结果之外，我们还提供了将函数近似到所需精度所需的前缀长度的 Jackson 类型界限。</li>
</ul>

<h3>Title: Generalizing Reward Modeling for Out-of-Distribution Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Chen Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14760">https://arxiv.org/abs/2402.14760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14760">https://arxiv.org/pdf/2402.14760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14760]] Generalizing Reward Modeling for Out-of-Distribution Preference Learning(https://arxiv.org/abs/2402.14760)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model for PL. We theoretically demonstrate the convergence rate of the bilevel optimization algorithm under reasonable assumptions. Additionally, we conduct experiments on two text generation tasks across 20 held-out domains and outperform a variety of strong baselines across various evaluation metrics.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 的偏好学习 (PL) 旨在使 LLM 的世代与人类偏好保持一致。先前关于基于人类反馈的强化学习 (RLHF) 的研究已经在分布 PL 中展示了有希望的结果。然而，由于难以获得人类反馈，为每个遇到的分布离散训练奖励模型具有挑战性。因此，分布外（OOD）PL 对于增强具有有限偏好反馈的 LLM 的泛化能力实际上很有用。这项工作通过元学习方法优化一般奖励模型来解决 OOD PL。在元训练期间，利用双层优化算法来学习能够指导策略学习以符合各种分布的人类偏好的奖励模型。当遇到测试分布时，元测试过程使用学习到的 PL 奖励模型进行正则化策略优化。我们从理论上证明了在合理假设下双层优化算法的收敛速度。此外，我们在 20 个保留域中的两个文本生成任务上进行了实验，并在各种评估指标上优于各种强大的基线。</li>
</ul>

<h3>Title: MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language  Models in Multi-Turn Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14762">https://arxiv.org/abs/2402.14762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14762">https://arxiv.org/pdf/2402.14762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14762]] MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language  Models in Multi-Turn Dialogues(https://arxiv.org/abs/2402.14762)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现极大地增强了对话系统。然而，全面评估法学硕士的对话能力仍然是一个挑战。以前的基准主要关注单轮对话或提供多轮对话的粗粒度和不完整的评估，忽略了现实生活对话的复杂性和细粒度的细微差别。为了解决这个问题，我们引入了 MT-Bench-101，专门用于评估法学硕士在多轮对话中的细粒度能力。通过对真实多轮对话数据进行详细分析，我们构建了一个三层分层能力分类法，其中包括 13 个不同任务中的 1388 个多轮对话中的 4208 个轮次。然后，我们基于 MT-Bench-101 评估 21 个热门的法学硕士，从能力和任务的角度进行全面分析，并观察法学硕士在不同任务的对话轮次中表现的不同趋势。进一步的分析表明，无论是使用常见的对齐技术还是特定于聊天的设计都没有导致法学硕士的多轮能力的明显增强。大量的案例研究表明，我们设计的任务准确地评估了相应的多回合能力。</li>
</ul>

<h3>Title: 2D Matryoshka Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14776">https://arxiv.org/abs/2402.14776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14776">https://arxiv.org/pdf/2402.14776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14776]] 2D Matryoshka Sentence Embeddings(https://arxiv.org/abs/2402.14776)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feasible. In this paper, we introduce a novel sentence embedding model called Two-dimensional Matryoshka Sentence Embedding (2DMSE). It supports elastic settings for both embedding sizes and Transformer layers, offering greater flexibility and efficiency than MRL. We conduct extensive experiments on STS tasks and downstream applications. The experimental results demonstrate the effectiveness of our proposed model in dynamically supporting different embedding sizes and Transformer layers, allowing it to be highly adaptable to various scenarios.</li>
<li><strong>摘要：</strong>常见的方法依赖于语言模型中的固定长度嵌入向量作为下游任务（例如语义文本相似性（STS））的句子嵌入。由于各种应用中未知的计算限制和预算，此类方法的灵活性受到限制。 Matryoshka 表示学习 (MRL)（Kusupati 等人，2022）以更细的粒度（即较低的嵌入维度）对信息进行编码，以自适应地适应临时任务。使用较小的嵌入大小可以实现类似的精度，从而加快下游任务的速度。尽管效率有所提高，MRL 在获得嵌入之前仍然需要遍历所有 Transformer 层，这仍然是时间和内存消耗的主要因素。这促使人们考虑固定数量的 Transformer 层是否会影响表示质量以及使用中间层进行句子表示是否可行。在本文中，我们介绍了一种新颖的句子嵌入模型，称为二维俄罗斯套娃句子嵌入（2DMSE）。它支持嵌入大小和 Transformer 层的弹性设置，提供比 MRL 更大的灵活性和效率。我们对STS任务和下游应用进行了广泛的实验。实验结果证明了我们提出的模型在动态支持不同嵌入大小和 Transformer 层方面的有效性，使其能够高度适应各种场景。</li>
</ul>

<h3>Title: Zero-shot cross-lingual transfer in instruction tuning of large language  model</h3>
<ul>
<li><strong>Authors: </strong>Nadezhda Chirkova, Vassilina Nikoulina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14778">https://arxiv.org/abs/2402.14778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14778">https://arxiv.org/pdf/2402.14778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14778]] Zero-shot cross-lingual transfer in instruction tuning of large language  model(https://arxiv.org/abs/2402.14778)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.</li>
<li><strong>摘要：</strong>指令调优 (IT) 广泛用于教导预训练的大型语言模型 (LLM) 遵循任意指令，但在多语言环境中的研究还不够。在这项工作中，我们对 IT 领域的零样本跨语言迁移进行了系统研究，法学硕士在纯英语数据上进行指令调整，然后在其他语言的用户提示上进行测试。我们研究了模型配置选择的影响，并为多语言教学遵循设计了多方面的评估策略。我们发现，即使模型训练的所有阶段都以英语为中心，但前提是在超参数调整中考虑到多语言性并且具有足够大的 IT 数据，跨语言迁移在 IT 领域确实会成功发生。受过英语培训的法学硕士能够用其他语言生成正确的语言、全面且有用的回答，但其真实性较低，并且偶尔可能会出现流畅性错误。</li>
</ul>

<h3>Title: Enhancing Systematic Decompositional Natural Language Inference Using  Informal Logic</h3>
<ul>
<li><strong>Authors: </strong>Nathaniel Weir, Kate Sanders, Orion Weller, Shreya Sharma, Dongwei Jiang, Zhengping Zhang, Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Jansen, Peter Clark, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14798">https://arxiv.org/abs/2402.14798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14798">https://arxiv.org/pdf/2402.14798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14798]] Enhancing Systematic Decompositional Natural Language Inference Using  Informal Logic(https://arxiv.org/abs/2402.14798)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in a modern neuro-symbolic reasoning engine significantly improves results (both accuracy and proof quality) over other entailment classifier baselines, illustrating the practical benefit of this advance for textual inference.</li>
<li><strong>摘要：</strong>当代语言模型为文本结构化推理提供了新的机会，例如在不依赖脆弱的形式逻辑的情况下构建和评估直观的、类似证明的文本蕴涵树。然而，由于长期缺乏明确的协议来确定什么是有效的组合蕴涵，这方面的进展受到了阻碍。这种缺失会导致数据集嘈杂，并限制现代神经符号引擎的性能提升。为了解决这些问题，我们制定了一种一致且有理论依据的方法来注释分解蕴涵数据集，并评估其对基于 LLM 的文本推理的影响。我们发现我们得到的数据集 RDTE（识别分解文本蕴涵）比之前的分解蕴涵数据集具有更高的内部一致性（+9%），这表明 RDTE 是在形成清晰的长期存在的问题上迈出的重要一步。辨别蕴涵的协议。我们还发现，通过知识蒸馏训练面向 RDTE 的蕴涵分类器并将其应用于现代神经符号推理引擎，与其他蕴涵分类器基线相比，显着提高了结果（准确性和证明质量），说明了这一进步对文本的实际好处。推理。</li>
</ul>

<h3>Title: Not All Experts are Equal: Efficient Expert Pruning and Skipping for  Mixture-of-Experts Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14800">https://arxiv.org/abs/2402.14800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14800">https://arxiv.org/pdf/2402.14800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14800]] Not All Experts are Equal: Efficient Expert Pruning and Skipping for  Mixture-of-Experts Large Language Models(https://arxiv.org/abs/2402.14800)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 进步的一个关键进步是混合专家 (MoE) LLM 的出现。与传统的 LLM 相比，MoE LLM 可以用更少的参数实现更高的性能，但由于参数量巨大，部署起来仍然很困难。与以往依赖专门设计的硬件的权值剪枝方法不同，本文主要旨在通过引入即插即用的专家级稀疏技术来提高MoE LLM的部署效率。具体来说，据我们所知，我们首次提出了与任务无关和特定于任务的专家修剪和跳过 MoE LLM 的训练后方法，旨在提高部署效率，同时在各种任务中保持模型性能。大量的实验表明，我们提出的方法可以同时减小模型大小并提高推理速度，同时保持令人满意的性能。数据和代码将在 https://github.com/Lucky-Lance/Expert_Sparsity 上提供。</li>
</ul>

<h3>Title: Identifying Multiple Personalities in Large Language Models with  External Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14805">https://arxiv.org/abs/2402.14805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14805">https://arxiv.org/pdf/2402.14805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14805]] Identifying Multiple Personalities in Large Language Models with  External Evaluation(https://arxiv.org/abs/2402.14805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their personalities. Many recent studies quantify LLMs' personalities using self-assessment tests that are created for humans. Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs. In this paper, we investigate LLM personalities using an alternate personality measurement method, which we refer to as the external evaluation method, where instead of prompting LLMs with multiple-choice questions in the Likert scale, we evaluate LLMs' personalities by analyzing their responses toward open-ended situational questions using an external machine learning model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor that outperforms the state-of-the-art models as the tool to analyze LLMs' responses. Then, we prompt the LLMs with situational questions and ask them to generate Twitter posts and comments, respectively, in order to assess their personalities when playing two different roles. Using the external personality evaluation method, we identify that the obtained personality types for LLMs are significantly different when generating posts versus comments, whereas humans show a consistent personality profile in these two different situations. This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans. With our work, we call for a re-evaluation of personality definition and measurement in LLMs.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 与人类日常应用的快速集成，人们对 LLM 的行为提出了许多社会和伦理问题。理解法学硕士行为的方法之一是分析他们的性格。最近的许多研究都使用为人类创建的自我评估测试来量化法学硕士的性格。然而，许多批评质疑这些自我评估测试应用于法学硕士的适用性和可靠性。在本文中，我们使用另一种人格测量方法（我们将其称为外部评估方法）来调查法学硕士的性格，该方法不是用李克特量表中的多项选择题来提示法学硕士，而是通过分析法学硕士的回答来评估法学硕士的性格。使用外部机器学习模型的开放式情境问题。我们首先对 Llama2-7B 模型进行了微调，作为 MBTI 性格预测器，该模型作为分析 LLM 反应的工具，其性能优于最先进的模型。然后，我们向法学硕士提出情境问题，并要求他们分别生成 Twitter 帖子和评论，以便评估他们在扮演两个不同角色时的个性。使用外部人格评估方法，我们发现法学硕士获得的人格类型在生成帖子和评论时显着不同，而人类在这两种不同情况下表现出一致的人格特征。这表明法学硕士可以根据不同的场景表现出不同的性格，从而凸显出法学硕士的性格与人类的根本区别。通过我们的工作，我们呼吁重新评估法学硕士的人格定义和测量。</li>
</ul>

<h3>Title: RelayAttention for Efficient Large Language Model Serving with Long  System Prompts</h3>
<ul>
<li><strong>Authors: </strong>Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. Lau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14808">https://arxiv.org/abs/2402.14808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14808">https://arxiv.org/pdf/2402.14808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14808]] RelayAttention for Efficient Large Language Model Serving with Long  System Prompts(https://arxiv.org/abs/2402.14808)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention.</li>
<li><strong>摘要：</strong>实用的大语言模型（LLM）服务可能会涉及很长的系统提示，它指定任务的说明、示例和知识文档，并在众多请求中重复使用。然而，较长的系统提示会导致吞吐量/延迟瓶颈，因为生成下一个令牌的成本会随着时间的推移而增长。序列长度。本文旨在提高涉及较长系统提示的LLM服务的效率。我们的主要观察是，处理这些系统提示需要在现有的因果注意计算算法中进行大量冗余的内存访问。具体来说，对于批量请求，系统提示的缓存隐藏状态（即键值对）会多次从片外 DRAM 传输到片上 SRAM，每次对应一个单独的请求。为了消除这种冗余，我们提出了 RelayAttention，这是一种注意力算法，允许对一批输入令牌从 DRAM 中读取这些隐藏状态一次。 RelayAttention 是一顿免费午餐：它保持了生成质量，同时不需要模型重新训练，因为它基于因果注意力的数学重新表述。</li>
</ul>

<h3>Title: CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14809">https://arxiv.org/abs/2402.14809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14809">https://arxiv.org/pdf/2402.14809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14809]] CriticBench: Benchmarking LLMs for Critique-Correct Reasoning(https://arxiv.org/abs/2402.14809)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 批判和完善其推理的能力对于其在评估、反馈提供和自我完善中的应用至关重要。本文介绍了 CriticBench，这是一个综合基准测试，旨在评估法学硕士在各种任务中批判和纠正其推理的能力。 CriticBench 包含五个推理领域：数学、常识、符号、编码和算法。它编译了 15 个数据集，并纳入了三个法学硕士家庭的回答。利用 CriticBench，我们评估并剖析了 17 位法学硕士在生成、批判和纠正推理（即 GQC 推理）方面的表现。我们的研究结果表明：(1) GQC 能力呈线性关系，以批评为中心的培训可显着提高绩效； (2) 纠正有效性随任务而变化，以逻辑为导向的任务更容易纠正； (3) GQC知识不一致随着模型规模的增加而减少； （4）一种有趣的模型间批评动态，较强的模型更擅长批评较弱的模型，而较弱的模型在自我批评方面却出人意料地超越了较强的模型。我们希望这些对法学硕士细致入微的批评与正确推理的见解将促进法学硕士批评和自我完善的进一步研究。</li>
</ul>

<h3>Title: Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity  Tracking</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, David Bau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14811">https://arxiv.org/abs/2402.14811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14811">https://arxiv.org/pdf/2402.14811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14811]] Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity  Tracking(https://arxiv.org/abs/2402.14811)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality: Entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned versions. (iii) Performance boost in the fine-tuned models is primarily attributed to its improved ability to handle the augmented positional information. To uncover these findings, we employ: Patch Patching, DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model.</li>
<li><strong>摘要：</strong>对指令跟踪、代码生成和数学等通用任务的微调已被证明可以增强语言模型在一系列任务上的性能。然而，对这种微调如何影响这些模型的内部计算的解释仍然难以捉摸。我们研究微调如何影响语言模型中实现的内部机制。作为一个案例研究，我们探讨了实体跟踪的属性，这是语言理解的一个关键方面，其中对数学进行微调的模型具有显着的性能提升。我们确定了实现实体跟踪的机制，并表明（i）在原始模型及其微调版本中，主要是相同的电路实现了实体跟踪。事实上，原始模型的实体跟踪电路在微调版本上的性能比完整的原始模型更好。 (ii) 所有模型的电路实现大致相同的功能：实体跟踪是通过跟踪原始模型及其微调版本中正确实体的位置来执行的。 (iii) 微调模型的性能提升主要归因于其处理增强位置信息的能力提高。为了揭示这些发现，我们采用了：补丁修补、DCM（自动检测负责特定语义的模型组件）和 CMAP（一种跨模型修补激活以揭示改进机制的新方法）。我们的研究结果表明，微调增强了而不是从根本上改变了模型的机械操作。</li>
</ul>

<h3>Title: PALO: A Polyglot Large Multimodal Model for 5B People</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14818">https://arxiv.org/abs/2402.14818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14818">https://arxiv.org/pdf/2402.14818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14818]] PALO: A Polyglot Large Multimodal Model for 5B People(https://arxiv.org/abs/2402.14818)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \textsc{Palo}. \textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Code: https://github.com/mbzuai-oryx/PALO.</li>
<li><strong>摘要：</strong>为了追求更具包容性的视觉语言模型（VLM），本研究引入了一种名为 \textsc{Palo} 的大型多语言多模态模型。 \textsc{Palo} 提供 10 种主​​要语言的视觉推理功能，包括英语、中文、印地语、西班牙语、法语、阿拉伯语、孟加拉语、俄语、乌尔都语和日语，覆盖总共 $\sim$5B 人（65\占世界人口的百分比）。我们的方法涉及半自动翻译方法，使用微调的大语言模型将多模式指令数据集从英语调整为目标语言，从而确保高语言保真度，同时由于最少的手动工作而允许可扩展性。不同指令集的结合有助于我们提高多种语言的整体性能，特别是那些代表性不足的语言，如印地语、阿拉伯语、孟加拉语和乌尔都语。生成的模型在三个尺度（1.7B、7B 和 13B 参数）上进行训练，以显示泛化性和可扩展性，与强大的基线相比，我们观察到了显着的改进。我们还为即将推出的方法提出了第一个多语言多模式基准，以评估其跨语言的视觉语言推理能力。代码：https://github.com/mbzuai-oryx/PALO。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
