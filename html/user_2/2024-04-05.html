<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-05</h1>
<h3>Title: Using Large Language Models to Understand Telecom Standards</h3>
<ul>
<li><strong>Authors: </strong>Athanasios Karapantelakis, Mukesh Shakur, Alexandros Nikou, Farnaz Moradi, Christian Orlog, Fitsum Gaim, Henrik Holm, Doumitrou Daniil Nimara, Vincent Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02929">https://arxiv.org/abs/2404.02929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02929">https://arxiv.org/pdf/2404.02929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02929]] Using Large Language Models to Understand Telecom Standards(https://arxiv.org/abs/2404.02929)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The Third Generation Partnership Project (3GPP) has successfully introduced standards for global mobility. However, the volume and complexity of these standards has increased over time, thus complicating access to relevant information for vendors and service providers. Use of Generative Artificial Intelligence (AI) and in particular Large Language Models (LLMs), may provide faster access to relevant information. In this paper, we evaluate the capability of state-of-art LLMs to be used as Question Answering (QA) assistants for 3GPP document reference. Our contribution is threefold. First, we provide a benchmark and measuring methods for evaluating performance of LLMs. Second, we do data preprocessing and fine-tuning for one of these LLMs and provide guidelines to increase accuracy of the responses that apply to all LLMs. Third, we provide a model of our own, TeleRoBERTa, that performs on-par with foundation LLMs but with an order of magnitude less number of parameters. Results show that LLMs can be used as a credible reference tool on telecom technical documents, and thus have potential for a number of different applications from troubleshooting and maintenance, to network operations and software product development.</li>
<li><strong>摘要：</strong>第三代合作伙伴计划 (3GPP) 已成功引入全球移动标准。然而，随着时间的推移，这些标准的数量和复杂性不断增加，从而使供应商和服务提供商获取相关信息变得更加复杂。使用生成人工智能（AI），特别是大型语言模型（LLM），可以更快地访问相关信息。在本文中，我们评估了最先进的法学硕士用作 3GPP 文档参考的问答 (QA) 助手的能力。我们的贡献是三重的。首先，我们提供了评估法学硕士表现的基准和衡量方法。其次，我们对这些法学硕士之一进行数据预处理和微调，并提供指南以提高适用于所有法学硕士的回答的准确性。第三，我们提供了自己的模型 TeleRoBERTa，其性能与基础 LLM 相当，但参数数量少了一个数量级。结果表明，法学硕士可以用作电信技术文档的可靠参考工具，因此具有从故障排除和维护到网络运营和软件产品开发的许多不同应用的潜力。</li>
</ul>

<h3>Title: GreedLlama: Performance of Financial Value-Aligned Large Language Models  in Moral Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jeffy Yu, Maximilian Huber, Kevin Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02934">https://arxiv.org/abs/2404.02934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02934">https://arxiv.org/pdf/2404.02934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02934]] GreedLlama: Performance of Financial Value-Aligned Large Language Models  in Moral Reasoning(https://arxiv.org/abs/2404.02934)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper investigates the ethical implications of aligning Large Language Models (LLMs) with financial optimization, through the case study of GreedLlama, a model fine-tuned to prioritize economically beneficial outcomes. By comparing GreedLlama's performance in moral reasoning tasks to a base Llama2 model, our results highlight a concerning trend: GreedLlama demonstrates a marked preference for profit over ethical considerations, making morally appropriate decisions at significantly lower rates than the base model in scenarios of both low and high moral ambiguity. In low ambiguity situations, GreedLlama's ethical decisions decreased to 54.4%, compared to the base model's 86.9%, while in high ambiguity contexts, the rate was 47.4% against the base model's 65.1%. These findings emphasize the risks of single-dimensional value alignment in LLMs, underscoring the need for integrating broader ethical values into AI development to ensure decisions are not solely driven by financial incentives. The study calls for a balanced approach to LLM deployment, advocating for the incorporation of ethical considerations in models intended for business applications, particularly in light of the absence of regulatory oversight.</li>
<li><strong>摘要：</strong>本文通过 GreedLlama 的案例研究，研究了大型语言模型 (LLM) 与财务优化相结合的伦理影响，GreedLlama 是一个经过微调以优先考虑经济利益结果的模型。通过将 GreedLlama 在道德推理任务中的表现与基本 Llama2 模型进行比较，我们的结果突出了一个令人担忧的趋势：GreedLlama 表现出对利润的明显偏好，而不是道德考虑，在低和低的情况下，以比基本模型低得多的速度做出道德上适当的决策。道德高度模糊。在低模糊性情况下，GreedLlama 的道德决策下降至 54.4%，而基础模型为 86.9%；而在高模糊性情况下，该比率为 47.4%，而基础模型为 65.1%。这些发现强调了法学硕士单一维度价值调整的风险，强调需要将更广泛的道德价值观融入人工智能开发，以确保决策不仅仅由经济激励驱动。该研究呼吁对法学硕士部署采取平衡的方法，主张将道德考虑纳入商业应用模型中，特别是在缺乏监管的情况下。</li>
</ul>

<h3>Title: KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual  Checking</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zhang, Chejian Xu, Yu Gai, Freddy Lecue, Dawn Song, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02935">https://arxiv.org/abs/2404.02935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02935">https://arxiv.org/pdf/2404.02935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02935]] KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual  Checking(https://arxiv.org/abs/2404.02935)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>This paper introduces KnowHalu, a novel approach for detecting hallucinations in text generated by large language models (LLMs), utilizing step-wise reasoning, multi-formulation query, multi-form knowledge for factual checking, and fusion-based detection mechanism. As LLMs are increasingly applied across various domains, ensuring that their outputs are not hallucinated is critical. Recognizing the limitations of existing approaches that either rely on the self-consistency check of LLMs or perform post-hoc fact-checking without considering the complexity of queries or the form of knowledge, KnowHalu proposes a two-phase process for hallucination detection. In the first phase, it identifies non-fabrication hallucinations--responses that, while factually correct, are irrelevant or non-specific to the query. The second phase, multi-form based factual checking, contains five key steps: reasoning and query decomposition, knowledge retrieval, knowledge optimization, judgment generation, and judgment aggregation. Our extensive evaluations demonstrate that KnowHalu significantly outperforms SOTA baselines in detecting hallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and 5.50% in summarization tasks, highlighting its effectiveness and versatility in detecting hallucinations in LLM-generated content.</li>
<li><strong>摘要：</strong>本文介绍了 KnowHalu，这是一种检测大型语言模型 (LLM) 生成的文本中的幻觉的新方法，利用逐步推理、多形式查询、用于事实检查的多形式知识以及基于融合的检测机制。随着法学硕士越来越多地应用于各个领域，确保其输出不被幻觉至关重要。认识到现有方法的局限性，这些方法要么依赖于法学硕士的自我一致性检查，要么在不考虑查询复杂性或知识形式的情况下执行事后事实检查，KnowHalu 提出了一种用于幻觉检测的两阶段过程。在第一阶段，它识别非虚构的幻觉——虽然事实上正确，但与查询无关或非特定的响应。第二阶段，基于多形式的事实检查，包含五个关键步骤：推理和查询分解、知识检索、知识优化、判断生成和判断聚合。我们的广泛评估表明，KnowHalu 在检测各种任务中的幻觉方面显着优于 SOTA 基线，例如，在 QA 任务中提高了 15.65%，在摘要任务中提高了 5.50%，突出了其在检测 LLM 生成内容中的幻觉方面的有效性和多功能性。</li>
</ul>

<h3>Title: Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Yang, Hai Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02936">https://arxiv.org/abs/2404.02936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02936">https://arxiv.org/pdf/2404.02936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02936]] Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large  Language Models(https://arxiv.org/abs/2404.02936)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. The current state-of-the-art approach, Min-K%, measures the raw token probability which we argue may not be the most informative signal. Instead, we propose Min-K%++ to normalize the token probability with statistics of the categorical distribution over the whole vocabulary, which accurately reflects the relative likelihood of the target token compared with other candidate tokens in the vocabulary. Theoretically, we back up our method by showing that the statistic it estimates is explicitly optimized during LLM training, thus serving as a reliable indicator for detecting training data. Empirically, on the WikiMIA benchmark, Min-K%++ outperforms the SOTA Min-K% by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, Min-K%++ consistently improves upon Min-K% and performs on par with reference-based method, despite not requiring an extra reference model.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的预训练数据检测问题因其对版权侵犯和测试数据污染等关键问题的影响而受到越来越多的关注。当前最先进的方法 Min-K% 测量原始代币概率，我们认为这可能不是信息最丰富的信号。相反，我们提出 Min-K%++ 通过整个词汇表的分类分布统计来标准化标记概率，这准确地反映了目标标记与词汇表中其他候选标记相比的相对可能性。理论上，我们通过证明其估计的统计量在 LLM 训练期间得到显式优化来支持我们的方法，从而作为检测训练数据的可靠指标。根据经验，在 WikiMIA 基准上，Min-K%++ 在五个模型的 AUROC 平均检测中比 SOTA Min-K% 好 6.2% 到 10.5%。在更具挑战性的 MIMIR 基准测试中，尽管不需要额外的参考模型，Min-K%++ 始终优于 Min-K%，并且与基于参考的方法的性能相当。</li>
</ul>

<h3>Title: Towards a Fully Interpretable and More Scalable RSA Model for Metaphor  Understanding</h3>
<ul>
<li><strong>Authors: </strong>Gaia Carenini, Luca Bischetti, Walter Schaeken, Valentina Bambini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02983">https://arxiv.org/abs/2404.02983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02983">https://arxiv.org/pdf/2404.02983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02983]] Towards a Fully Interpretable and More Scalable RSA Model for Metaphor  Understanding(https://arxiv.org/abs/2404.02983)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The Rational Speech Act (RSA) model provides a flexible framework to model pragmatic reasoning in computational terms. However, state-of-the-art RSA models are still fairly distant from modern machine learning techniques and present a number of limitations related to their interpretability and scalability. Here, we introduce a new RSA framework for metaphor understanding that addresses these limitations by providing an explicit formula - based on the mutually shared information between the speaker and the listener - for the estimation of the communicative goal and by learning the rationality parameter using gradient-based methods. The model was tested against 24 metaphors, not limited to the conventional $\textit{John-is-a-shark}$ type. Results suggest an overall strong positive correlation between the distributions generated by the model and the interpretations obtained from the human behavioral data, which increased when the intended meaning capitalized on properties that were inherent to the vehicle concept. Overall, findings suggest that metaphor processing is well captured by a typicality-based Bayesian model, even when more scalable and interpretable, opening up possible applications to other pragmatic phenomena and novel uses for increasing Large Language Models interpretability. Yet, results highlight that the more creative nuances of metaphorical meaning, not strictly encoded in the lexical concepts, are a challenging aspect for machines.</li>
<li><strong>摘要：</strong>理性言语行为 (RSA) 模型提供了一个灵活的框架来对计算术语中的实用推理进行建模。然而，最先进的 RSA 模型距离现代机器学习技术仍然相当遥远，并且在可解释性和可扩展性方面存在许多限制。在这里，我们引入了一种用于隐喻理解的新 RSA 框架，该框架通过提供一个明确的公式（基于说话者和听者之间相互共享的信息）来解决这些限制，用于估计交际目标，并使用梯度学习合理性参数 -为基础的方法。该模型针对 24 个隐喻进行了测试，不限于传统的 $\textit{John-is-a-shark}$ 类型。结果表明，模型生成的分布与从人类行为数据获得的解释之间总体上存在很强的正相关性，当预期含义利用了车辆概念固有的属性时，这种相关性就会增加。总体而言，研究结果表明，基于典型性的贝叶斯模型可以很好地捕获隐喻处理，即使更具可扩展性和可解释性，也为其他语用现象和增加大型语言模型可解释性的新用途提供了可能的应用。然而，结果强调，隐喻意义的更具创造性的细微差别（没有严格编码在词汇概念中）对机器来说是一个具有挑战性的方面。</li>
</ul>

<h3>Title: BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and  Multilingual Exploration of Persuasion in Memes</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Abaskohi, Amirhossein Dabiriaghdam, Lele Wang, Giuseppe Carenini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03022">https://arxiv.org/abs/2404.03022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03022">https://arxiv.org/pdf/2404.03022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03022]] BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and  Multilingual Exploration of Persuasion in Memes(https://arxiv.org/abs/2404.03022)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Memes, combining text and images, frequently use metaphors to convey persuasive messages, shaping public opinion. Motivated by this, our team engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task designed to identify rhetorical and psychological persuasion techniques embedded within memes. To tackle this problem, we introduced a caption generation step to assess the modality gap and the impact of additional semantic information from images, which improved our result. Our best model utilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as the text encoder and CLIP as the image encoder. It outperforms the baseline by a large margin in all 12 subtasks. In particular, it ranked in top-3 across all languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively strong performance. The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders. This highlights the potential for improving abstract visual semantics encoding.</li>
<li><strong>摘要：</strong>模因结合了文本和图像，经常使用隐喻来传达有说服力的信息，塑造公众舆论。受此启发，我们的团队参与了 SemEval-2024 任务 4，这是一项分层多标签分类任务，旨在识别模因中嵌入的修辞和心理说服技术。为了解决这个问题，我们引入了一个标题生成步骤来评估模态差距和图像中附加语义信息的影响，这改善了我们的结果。我们的最佳模型利用 GPT-4 生成的标题和 meme 文本来微调 RoBERTa 作为文本编码器和 CLIP 作为图像编码器。它在所有 12 个子任务中都大幅优于基线。特别是，它在子任务 2a 中的所有语言中排名前 3，在子任务 2b 中排名前 4，展示了在数量上强大的性能。引入的中间步骤所实现的改进可能归因于挑战视觉编码器的图像的隐喻本质。这凸显了改进抽象视觉语义编码的潜力。</li>
</ul>

<h3>Title: An Incomplete Loop: Deductive, Inductive, and Abductive Learning in  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Emmy Liu, Graham Neubig, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03028">https://arxiv.org/abs/2404.03028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03028">https://arxiv.org/pdf/2404.03028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03028]] An Incomplete Loop: Deductive, Inductive, and Abductive Learning in  Large Language Models(https://arxiv.org/abs/2404.03028)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures.</li>
<li><strong>摘要：</strong>现代语言模型（LM）可以学习以不同的方式执行新任务：在指令跟随中，目标任务用自然语言明确描述；在少样本提示中，任务是通过少量示例隐式指定的；在指令推理中，语言模型会被提供上下文中的示例，然后在做出预测之前提示生成自然语言任务描述。这些过程中的每一个都可以被认为调用不同形式的推理：指令遵循涉及演绎推理，小提示提示涉及归纳推理，指令推理涉及溯因推理。这些不同的能力有何关联？在四个 LM（来自 gpt 和 llama 系列）和两个学习问题（涉及算术函数和机器翻译）中，我们发现不同类型的推理之间存在很强的分离性：LM 有时可以从少量提示中有效学习，即使它们无法做到这一点解释自己的预测规则；相反，他们有时会推断出有用的任务描述，但完全无法从人类生成的同一任务描述中学习。我们的结果强调了推理的非系统性，即使在当今一些最大的语言模型中也是如此，并强调了这样一个事实：看似相似的提示程序可能会调用非常不同的学习机制。</li>
</ul>

<h3>Title: MuLan: A Study of Fact Mutability in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Constanza Fierro, Nicolas Garneau, Emanuele Bugliarello, Yova Kementchedjhieva, Anders Søgaard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03036">https://arxiv.org/abs/2404.03036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03036">https://arxiv.org/pdf/2404.03036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03036]] MuLan: A Study of Fact Mutability in Language Models(https://arxiv.org/abs/2404.03036)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular large language models, we consistently find differences in the LLMs' confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from LLMs.</li>
<li><strong>摘要：</strong>事实会受到偶然性的影响，在不同的情况下可能是真实的也可能是虚假的。其中一个偶然事件就是时间，其中一些事实在特定时期内发生变化，例如，国家总统或冠军得主。值得信赖的语言模型理想地识别可变事实并相应地处理它们。我们创建了 MuLan，这是一个评估英语语言模型预测时间偶然性的能力的基准，涵盖 1:1 和 1:N 关系。我们假设可变事实的编码方式与不可变事实不同，因此更容易更新。在对六种流行的大型语言模型的详细评估中，我们一致发现法学硕士的置信度、表示和更新行为存在差异，具体取决于事实的可变性。我们的研究结果应该为未来向法学硕士注入和诱导时间相关知识的工作提供信息。</li>
</ul>

<h3>Title: Language, Environment, and Robotic Navigation</h3>
<ul>
<li><strong>Authors: </strong>Johnathan E. Avery</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03049">https://arxiv.org/abs/2404.03049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03049">https://arxiv.org/pdf/2404.03049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03049]] Language, Environment, and Robotic Navigation(https://arxiv.org/abs/2404.03049)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition. It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field. By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences. Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems.</li>
<li><strong>摘要：</strong>本文探讨了机器人导航系统中语言输入的集成，利用符号相互依赖假说来弥合符号认知和具身认知之间的鸿沟。它研究了先前将语言和语义纳入神经网络 (NN) 和同步定位与地图绘制 (SLAM) 方法的工作，强调了这些集成如何推动该领域的发展。通过将抽象符号操作与感觉运动基础进行对比，我们提出了一个统一的框架，其中语言既充当抽象的交流系统，又充当感知体验的基础表征。我们对分布式语义认知模型及其在自主代理中的应用的回顾强调了语言集成系统的变革潜力。</li>
</ul>

<h3>Title: GPT-DETOX: An In-Context Learning-Based Paraphraser for Text  Detoxification</h3>
<ul>
<li><strong>Authors: </strong>Ali Pesaranghader, Nikhil Verma, Manasa Bharadwaj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03052">https://arxiv.org/abs/2404.03052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03052">https://arxiv.org/pdf/2404.03052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03052]] GPT-DETOX: An In-Context Learning-Based Paraphraser for Text  Detoxification(https://arxiv.org/abs/2404.03052)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms. Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content. Supervised and unsupervised learning are common approaches for designing text detoxification solutions. However, these methods necessitate fine-tuning, leading to computational overhead. In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences. To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES). We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA as benchmark detoxification datasets. Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA. Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets.</li>
<li><strong>摘要：</strong>有害和冒犯性的沟通或内容不利于社交媒体平台上用户的社交联系和精神状态。文本去毒是自然语言处理 (NLP) 中的一项关键任务，其目标是消除文本中的脏话和毒性，同时保留其内容。监督和无监督学习是设计文本解毒解决方案的常见方法。然而，这些方法需要微调，导致计算开销。在本文中，我们提出 GPT-DETOX 作为使用 GPT-3.5 Turbo 进行基于提示的上下文学习的框架。我们利用零样本和少样本提示技术来净化输入句子。为了生成少样本提示，我们提出了两种方法：单词匹配示例选择（WMES）和上下文匹配示例选择（CMES）。我们还考虑了集成上下文学习（EICL），其中集成是由零样本和所有少样本设置的基本提示塑造的。我们使用 ParaDetox 和 APPDIA 作为基准排毒数据集。我们的实验结果表明，零样本解决方案实现了有希望的性能，而我们最好的几次样本设置优于 ParaDetox 上最先进的模型，并在 APPDIA 上显示了可比较的结果。我们的 EICL 解决方案获得了最佳性能，针对这两个数据集至少提高了 10%。</li>
</ul>

<h3>Title: Mai Ho'omāuna i ka 'Ai: Language Models Improve Automatic Speech  Recognition in Hawaiian</h3>
<ul>
<li><strong>Authors: </strong>Kaavya Chaparala, Guido Zarrella, Bruce Torres Fischer, Larry Kimura, Oiwi Parker Jones</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03073">https://arxiv.org/abs/2404.03073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03073">https://arxiv.org/pdf/2404.03073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03073]] Mai Ho'omāuna i ka 'Ai: Language Models Improve Automatic Speech  Recognition in Hawaiian(https://arxiv.org/abs/2404.03073)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper we address the challenge of improving Automatic Speech Recognition (ASR) for a low-resource language, Hawaiian, by incorporating large amounts of independent text data into an ASR foundation model, Whisper. To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text. We then use the LM to rescore Whisper and compute word error rates (WERs) on a manually curated test set of labeled Hawaiian data. As a baseline, we use Whisper without an external LM. Experimental results reveal a small but significant improvement in WER when ASR outputs are rescored with a Hawaiian LM. The results support leveraging all available data in the development of ASR systems for underrepresented languages.</li>
<li><strong>摘要：</strong>在本文中，我们通过将大量独立文本数据合并到 ASR 基础模型 Whisper 中，解决了改进夏威夷语等低资源语言的自动语音识别 (ASR) 的挑战。为此，我们使用约 150 万个夏威夷语文本单词训练外部语言模型 (LM)。然后，我们使用 LM 对 Whisper 进行重新评分，并在手动策划的标记夏威夷语数据测试集上计算单词错误率 (WER)。作为基准，我们使用 Whisper，无需外部 LM。实验结果表明，当使用 Hawaiian LM 重新评分 ASR 输出时，WER 会得到微小但显着的改进。结果支持在开发针对代表性不足的语言的 ASR 系统时利用所有可用数据。</li>
</ul>

<h3>Title: Construction of Functional Materials Knowledge Graph in  Multidisciplinary Materials Science via Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Tong Xie, Wenjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03080">https://arxiv.org/abs/2404.03080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03080">https://arxiv.org/pdf/2404.03080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03080]] Construction of Functional Materials Knowledge Graph in  Multidisciplinary Materials Science via Large Language Model(https://arxiv.org/abs/2404.03080)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The convergence of materials science and artificial intelligence has unlocked new opportunities for gathering, analyzing, and generating novel materials sourced from extensive scientific literature. Despite the potential benefits, persistent challenges such as manual annotation, precise extraction, and traceability issues remain. Large language models have emerged as promising solutions to address these obstacles. This paper introduces Functional Materials Knowledge Graph (FMKG), a multidisciplinary materials science knowledge graph. Through the utilization of advanced natural language processing techniques, extracting millions of entities to form triples from a corpus comprising all high-quality research papers published in the last decade. It organizes unstructured information into nine distinct labels, covering Name, Formula, Acronym, Structure/Phase, Properties, Descriptor, Synthesis, Characterization Method, Application, and Domain, seamlessly integrating papers' Digital Object Identifiers. As the latest structured database for functional materials, FMKG acts as a powerful catalyst for expediting the development of functional materials and a fundation for building a more comprehensive material knowledge graph using full paper text. Furthermore, our research lays the groundwork for practical text-mining-based knowledge management systems, not only in intricate materials systems but also applicable to other specialized domains.</li>
<li><strong>摘要：</strong>材料科学和人工智能的融合为从大量科学文献中收集、分析和生成新材料提供了新的机会。尽管有潜在的好处，但手动注释、精确提取和可追溯性问题等持续存在的挑战仍然存在。大型语言模型已成为解决这些障碍的有前途的解决方案。本文介绍了功能材料知识图谱（FMKG），这是一种多学科材料科学知识图谱。通过利用先进的自然语言处理技术，从包含过去十年发表的所有高质量研究论文的语料库中提取数百万个实体形成三元组。它将非结构化信息组织成九个不同的标签，涵盖名称、公式、首字母缩略词、结构/阶段、属性、描述符、合成、表征方法、应用和领域，无缝集成论文的数字对象标识符。作为最新的功能材料结构化数据库，FMKG 是加快功能材料发展的强大催化剂，也是利用全文文本构建更全面的材料知识图谱的基础。此外，我们的研究为基于文本挖掘的实用知识管理系统奠定了基础，不仅适用于复杂的材料系统，而且适用于其他专业领域。</li>
</ul>

<h3>Title: Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning,  Repeating, or Just Biased?</h3>
<ul>
<li><strong>Authors: </strong>Vagrant Gautam, Eileen Bingert, Dawei Zhu, Anne Lauscher, Dietrich Klakow</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03134">https://arxiv.org/abs/2404.03134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03134">https://arxiv.org/pdf/2404.03134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03134]] Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning,  Repeating, or Just Biased?(https://arxiv.org/abs/2404.03134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time. To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors. We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters). We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns. Additionally, models are not robustly faithful to pronouns, as they are easily distracted. With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models. We show that widely-used large language models are still brittle, with large gaps in reasoning and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and reasoning to bridge them.</li>
<li><strong>摘要：</strong>随着代词使用的增加，对个人来说稳健、忠实和无害的代词使用是语言模型的一个重要目标，但之前的工作往往一次只研究其中的一两个组成部分。为了衡量实现综合目标的进展，我们引入了代词使用保真度的任务：给定引入共同指代实体和代词的上下文，任务是稍后重用正确的代词，而不受潜在干扰因素的影响。我们提供了一个包含超过 500 万个实例的精心设计的数据集，用于评估英语中的代词使用保真度，并使用它来评估跨架构（仅编码器、仅解码器和编码器-解码器）和规模（11M -70B参数）。我们发现，虽然模型在没有干扰因素的情况下大多可以忠实地重复使用先前指定的代词，但它们在处理“她/她/她”、单数“they”和新代词方面明显较差。此外，模型并不完全忠实于代词，因为它们很容易分散注意力。即使多一个句子包含干扰代词，准确率平均也会下降 34%。当有 5 个干扰句子时，仅解码器模型的准确度下降了 52%，仅编码器模型的准确度下降了 13%。我们表明，广泛使用的大型语言模型仍然很脆弱，在推理和在对人类来说非常简单的环境中处理不同代词方面存在很大差距，我们鼓励偏见和推理研究人员弥合这些差距。</li>
</ul>

<h3>Title: NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation  using Few-Shot Multi-Choice QA</h3>
<ul>
<li><strong>Authors: </strong>Anish Pahilajani, Samyak Rajesh Jain, Devasha Trivedi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03150">https://arxiv.org/abs/2404.03150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03150">https://arxiv.org/pdf/2404.03150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03150]] NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation  using Few-Shot Multi-Choice QA(https://arxiv.org/abs/2404.03150)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>This paper presents our submission to the SemEval 2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure. We present two approaches to solving the task of legal answer validation, given an introduction to the case, a question and an answer candidate. Firstly, we fine-tuned pre-trained BERT-based models and found that models trained on domain knowledge perform better. Secondly, we performed few-shot prompting on GPT models and found that reformulating the answer validation task to be a multiple-choice QA task remarkably improves the performance of the model. Our best submission is a BERT-based model that achieved the 7th place out of 20.</li>
<li><strong>摘要：</strong>本文介绍了我们向 SemEval 2024 任务 5：民事诉讼中的法律论证推理任务提交的材料。在介绍案例、问题和候选答案的情况下，我们提出了两种解决合法答案验证任务的方法。首先，我们对基于 BERT 的预训练模型进行了微调，发现基于领域知识训练的模型表现更好。其次，我们对 GPT 模型进行了小样本提示，发现将答案验证任务重新制定为多项选择 QA 任务可以显着提高模型的性能。我们最好的提交是基于 BERT 的模型，在 20 个模型中排名第 7。</li>
</ul>

<h3>Title: Uncertainty in Language Models: Assessment through Rank-Calibration</h3>
<ul>
<li><strong>Authors: </strong>Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, Osbert Bastani, Edgar Dobriban</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03163">https://arxiv.org/abs/2404.03163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03163">https://arxiv.org/pdf/2404.03163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03163]] Uncertainty in Language Models: Assessment through Rank-Calibration(https://arxiv.org/abs/2404.03163)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.</li>
<li><strong>摘要：</strong>语言模型（LM）在自然语言生成方面表现出了良好的性能。然而，由于 LM 经常产生不正确或幻觉的响应，因此正确量化其响应给定输入的不确定性至关重要。除了通过提示引发的言语信心之外，还提出了许多不确定性度量（例如，语义熵和基于亲和图的度量）。然而，这些度量可能差异很大，并且不清楚如何比较它们，部分原因是它们采用不同范围内的值（$例如$，$[0，\infty）$或$[0,1]$）。在这项工作中，我们通过开发一个新颖且实用的框架（称为 $Rank$-$Calibration$）来解决这个问题，以评估 LM 的不确定性和置信度度量。我们的主要原则是，平均而言，较高的不确定性（或较低的置信度）意味着较低的发电质量。排名校准以原则性的方式量化与这种理想关系的偏差，而不需要对正确性分数（$例如$、ROUGE或METEOR）进行专门的二进制阈值处理。我们的方法的广泛适用性和精细可解释性已通过经验得到证明。</li>
</ul>

<h3>Title: The Probabilities Also Matter: A More Faithful Metric for Faithfulness  of Free-Text Explanations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Noah Y. Siegel, Oana-Maria Camburu, Nicolas Heess, Maria Perez-Ortiz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03189">https://arxiv.org/abs/2404.03189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03189">https://arxiv.org/pdf/2404.03189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03189]] The Probabilities Also Matter: A More Faithful Metric for Faithfulness  of Free-Text Explanations in Large Language Models(https://arxiv.org/abs/2404.03189)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.</li>
<li><strong>摘要：</strong>为了监督先进的人工智能系统，了解其底层决策过程非常重要。当出现提示时，大型语言模型 (LLM) 可以提供听起来合理的自然语言解释或推理轨迹，并获得人类注释者的高度评价。然而，尚不清楚这些解释在多大程度上是忠实的，即真正捕捉到了模型预测的因素。在这项工作中，我们引入了相关解释忠诚度（CEF），这是一种可用于基于输入干预的忠诚度测试的指标。此类测试中使用的先前指标仅考虑预测中的二进制变化。我们的指标考虑了模型预测标签分布的总变化，更准确地反映了解释的可信度。然后，我们通过在 Atanasova 等人的反事实测试 (CT) 上实例化 CEF 来介绍相关反事实测试 (CCT)。 （2023）。我们评估了来自 Llama2 系列的少样本提示的 LLM 在三项 NLP 任务上生成的自由文本解释的忠实度。我们发现我们的指标衡量了 CT 所忽略的忠诚度方面。</li>
</ul>

<h3>Title: Okay, Let's Do This! Modeling Event Coreference with Generated  Rationales and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Abhijnan Nath, Shadi Manafi, Avyakta Chelle, Nikhil Krishnaswamy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03196">https://arxiv.org/abs/2404.03196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03196">https://arxiv.org/pdf/2404.03196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03196]] Okay, Let's Do This! Modeling Event Coreference with Generated  Rationales and Knowledge Distillation(https://arxiv.org/abs/2404.03196)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In NLP, Event Coreference Resolution (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems. In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive LLMs as distant supervision of smaller student models for cross-document coreference (CDCR) of events. We implement novel rationale-oriented event clustering and knowledge distillation methods for event coreference scoring that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document clustering. Our model using coreference specific knowledge distillation achieves SOTA B3 F1 on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found at https://github.com/csu-signal/llama_cdcr</li>
<li><strong>摘要：</strong>在 NLP 中，事件共指解析 (ECR) 是通常通过神经系统连接引用同一底层现实生活事件的事件集群的任务。在这项工作中，我们研究使用现代自回归法学硕士生成的溯因自由文本原理（FTR）作为事件跨文档共指（CDCR）的较小学生模型的远程监督。我们实现了新颖的面向原理的事件聚类和知识蒸馏方法来进行事件共指评分，该方法利用 FTR 中的丰富信息来改进 CDCR，而无需额外的注释或昂贵的文档聚类。我们的模型使用共指特定知识蒸馏在 ECB+ 和 GVC 语料库上实现了 SOTA B3 F1，并且我们在 AIDA 第一阶段语料库上建立了新的基线。我们的代码可以在 https://github.com/csu-signal/llama_cdcr 找到</li>
</ul>

<h3>Title: Probing Large Language Models for Scalar Adjective Lexical Semantics and  Scalar Diversity Pragmatics</h3>
<ul>
<li><strong>Authors: </strong>Fangru Lin, Daniel Altshuler, Janet B. Pierrehumbert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03301">https://arxiv.org/abs/2404.03301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03301">https://arxiv.org/pdf/2404.03301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03301]] Probing Large Language Models for Scalar Adjective Lexical Semantics and  Scalar Diversity Pragmatics(https://arxiv.org/abs/2404.03301)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale). Scalar implicatures arise from the consideration of alternative statements which could have been made. They can be triggered by scalar adjectives and require listeners to reason pragmatically about them. Some scalar adjectives are more likely to trigger scalar implicatures than others. This phenomenon is referred to as scalar diversity. In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity. We find that they encode rich lexical-semantic information about scalar adjectives. However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity. We also compare current models of different sizes and complexities and find that larger models are not always better. Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives.</li>
<li><strong>摘要：</strong>标量形容词属于不同的领域尺度，并且在每个尺度内的强度有所不同（例如，某些比可能性尺度上的可能性更强烈）。标量含义源于对可能做出的替代陈述的考虑。它们可以由标量形容词触发，并要求听众对它们进行务实的推理。一些标量形容词比其他形容词更有可能引发标量含义。这种现象称为标量多样性。在这项研究中，我们探讨了不同的大型语言模型家族（例如 GPT-4），以了解它们对标量形容词的词汇语义的了解以及其语用学的一个特定方面，即标量多样性。我们发现它们编码了有关标量形容词的丰富词汇语义信息。然而，丰富的词汇语义知识并不意味着对标量多样性的良好理解。我们还比较了不同尺寸和复杂性的当前模型，发现较大的模型并不总是更好。最后，我们利用语言直觉和模型训练目标来解释我们的探测结果。</li>
</ul>

<h3>Title: How Easily do Irrelevant Inputs Skew the Responses of Large Language  Models?</h3>
<ul>
<li><strong>Authors: </strong>Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03302">https://arxiv.org/abs/2404.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03302">https://arxiv.org/pdf/2404.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03302]] How Easily do Irrelevant Inputs Skew the Responses of Large Language  Models?(https://arxiv.org/abs/2404.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.</li>
<li><strong>摘要：</strong>通过利用从外部知识数据库检索信息，大型语言模型 (LLM) 展现出完成许多知识密集型任务的增强功能。然而，由于当前检索系统的固有缺陷，在检索排名靠前的段落中可能存在不相关的信息。在这项工作中，我们对法学硕士在各种条件下对不同类型的不相关信息的鲁棒性进行了全面的调查。我们最初引入了一个框架来构建高质量的不相关信息，这些信息包括语义上不相关、部分相关和与问题相关的信息。此外，我们的分析表明，构建的不相关信息不仅在相似性指标上得分很高，被现有系统高度检索，而且还与上下文具有语义联系。我们的调查表明，当前的法学硕士在区分语义高度相关的信息方面仍然面临挑战，并且很容易被这些不相关但具有误导性的内容分散注意力。此外，我们还发现，当前处理不相关信息的解决方案在提高法学硕士对此类干扰的鲁棒性方面存在局限性。资源可在 https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information 获取。</li>
</ul>

<h3>Title: Concept -- An Evaluation Protocol on Conversation Recommender Systems  with System- and User-centric Factors</h3>
<ul>
<li><strong>Authors: </strong>Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03304">https://arxiv.org/abs/2404.03304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03304">https://arxiv.org/pdf/2404.03304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03304]] Concept -- An Evaluation Protocol on Conversation Recommender Systems  with System- and User-centric Factors(https://arxiv.org/abs/2404.03304)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The conversational recommendation system (CRS) has been criticized regarding its user experience in real-world scenarios, despite recent significant progress achieved in academia. Existing evaluation protocols for CRS may prioritize system-centric factors such as effectiveness and fluency in conversation while neglecting user-centric aspects. Thus, we propose a new and inclusive evaluation protocol, Concept, which integrates both system- and user-centric factors. We conceptualise three key characteristics in representing such factors and further divide them into six primary abilities. To implement Concept, we adopt a LLM-based user simulator and evaluator with scoring rubrics that are tailored for each primary ability. Our protocol, Concept, serves a dual purpose. First, it provides an overview of the pros and cons in current CRS models. Second, it pinpoints the problem of low usability in the "omnipotent" ChatGPT and offers a comprehensive reference guide for evaluating CRS, thereby setting the foundation for CRS improvement.</li>
<li><strong>摘要：</strong>尽管最近在学术界取得了重大进展，但会话推荐系统（CRS）因其在现实场景中的用户体验而受到批评。现有的 CRS 评估协议可能会优先考虑以系统为中心的因素，例如对话的有效性和流畅性，而忽略以用户为中心的方面。因此，我们提出了一种新的、具有包容性的评估协议——Concept，它整合了以系统和用户为中心的因素。我们概念化了代表这些因素的三个关键特征，并进一步将它们分为六种主要能力。为了实施概念，我们采用了基于法学硕士的用户模拟器和评估器，其评分标准针对每种主要能力量身定制。我们的协议“概念”有双重目的。首先，它概述了当前 CRS 模型的优缺点。其次，指出了“万能”的ChatGPT可用性低的问题，为评估CRS提供了全面的参考指南，从而为CRS的改进奠定了基础。</li>
</ul>

<h3>Title: Towards Pareto Optimal Throughput in Small Language Model Serving</h3>
<ul>
<li><strong>Authors: </strong>Pol G.Recasens, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03353">https://arxiv.org/abs/2404.03353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03353">https://arxiv.org/pdf/2404.03353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03353]] Towards Pareto Optimal Throughput in Small Language Model Serving(https://arxiv.org/abs/2404.03353)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了许多不同自然语言处理任务的最新技术。尽管为 LLM 提供计算和内存要求很高，但小语言模型 (SLM) 的兴起为资源有限的用户提供了新的机会，他们现在能够为具有尖端性能的小型模型提供服务。在本文中，我们提出了一组旨在在性能和能量水平上对 SLM 推理进行基准测试的实验。我们的分析提供了服务的新视角，强调 SLM 的内存占用较小，可以在单个加速器的资源容量内达到帕累托最优吞吐量。在这方面，我们提出了一组初步研究结果，展示了模型复制如何有效提高服务 SLM 的资源利用率。</li>
</ul>

<h3>Title: nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion  Cause in Conversations with Chain-of-Thought on Emotion States</h3>
<ul>
<li><strong>Authors: </strong>Nicolay Rusnachenko, Huizhi Liang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03361">https://arxiv.org/abs/2404.03361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03361">https://arxiv.org/pdf/2404.03361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03361]] nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion  Cause in Conversations with Chain-of-Thought on Emotion States(https://arxiv.org/abs/2404.03361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Emotion expression is one of the essential traits of conversations. It may be self-related or caused by another speaker. The variety of reasons may serve as a source of the further emotion causes: conversation history, speaker's emotional state, etc. Inspired by the most recent advances in Chain-of-Thought, in this work, we exploit the existing three-hop reasoning approach (THOR) to perform large language model instruction-tuning for answering: emotion states (THOR-state), and emotion caused by one speaker to the other (THOR-cause). We equip THOR-cause with the reasoning revision (rr) for devising a reasoning path in fine-tuning. In particular, we rely on the annotated speaker emotion states to revise reasoning path. Our final submission, based on Flan-T5-base (250M) and the rule-based span correction technique, preliminary tuned with THOR-state and fine-tuned with THOR-cause-rr on competition training data, results in 3rd and 4th places (F1-proportional) and 5th place (F1-strict) among 15 participating teams. Our THOR implementation fork is publicly available: https://github.com/nicolay-r/THOR-ECAC</li>
<li><strong>摘要：</strong>情感表达是对话的基本特征之一。它可能是自身相关的，也可能是由另一个说话者引起的。各种原因可以作为进一步情绪原因的来源：对话历史、说话者的情绪状态等。受思想链最新进展的启发，在这项工作中，我们利用现有的三跳推理方法(THOR) 执行大语言模型指令调整以回答：情绪状态（THOR 状态）以及一个说话者对另一说话者引起的情绪（THOR 原因）。我们为 THOR-cause 配备了推理修订版 (rr)，用于设计微调中的推理路径。特别是，我们依靠注释的说话者情绪状态来修改推理路径。我们的最终提交，基于Flan-T5-base（250M）和基于规则的跨度校正技术，在比赛训练数据上使用THOR-state进行初步调整，并使用THOR-cause-rr进行微调，结果获得第三名和第四名（F1-比例）和 15 支参赛队伍中的第五名（F1-严格）。我们的 THOR 实现分支已公开：https://github.com/nicolay-r/THOR-ECAC</li>
</ul>

<h3>Title: Learning to Plan and Generate Text with Citations</h3>
<ul>
<li><strong>Authors: </strong>Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, Mirella Lapata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03381">https://arxiv.org/abs/2404.03381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03381">https://arxiv.org/pdf/2404.03381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03381]] Learning to Plan and Generate Text with Citations(https://arxiv.org/abs/2404.03381)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as blueprints of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an abstractive model where questions are generated from scratch, and an extractive model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.</li>
<li><strong>摘要：</strong>在信息寻求场景中部署法学硕士的需求不断增加，刺激了创建可验证系统的努力，该系统生成对查询的响应以及支持证据。在本文中，我们探索了基于计划的模型的归因能力，最近已证明该模型可以提高生成文本的忠实性、基础性和可控性。我们将计划概念化为一系列问题，作为生成内容及其组织的蓝图。我们提出了两种利用蓝图不同变体的归因模型，一种是从头开始生成问题的抽象模型，另一种是从输入复制问题的提取模型。长篇问答实验表明，规划可以持续提高归因质量。此外，与从缺乏规划组件的基于 LLM 的管道获得的引文相比，蓝图模型生成的引文更加准确。</li>
</ul>

<h3>Title: Can Small Language Models Help Large Language Models Reason Better?:  LM-Guided Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, Kai-Wei Chang, Chengwei Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03414">https://arxiv.org/abs/2404.03414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03414">https://arxiv.org/pdf/2404.03414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03414]] Can Small Language Models Help Large Language Models Reason Better?:  LM-Guided Chain-of-Thought(https://arxiv.org/abs/2404.03414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM in reasoning tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance.</li>
<li><strong>摘要：</strong>我们引入了一种新颖的框架，LM-Guided CoT，它利用轻量级（即 <1B）语言模型（LM）来指导推理任务中的黑盒大型（即 >10B）LM。具体来说，轻量级 LM 首先为每个输入实例生成一个基本原理。然后，Frozen 大型 LM 会根据轻量级 LM 生成的基本原理来预测任务输出。我们的方法是资源高效的，因为它只需要训练轻量级 LM。我们通过 1）知识蒸馏和 2）从面向基本原理和面向任务的奖励信号进行强化学习来优化模型。我们使用多跳提取问答 (QA) 基准、HotpotQA 和 2WikiMultiHopQA 来评估我们的方法。实验结果表明，我们的方法在答案预测准确性方面优于所有基线。我们还发现强化学习有助于模型产生更高质量的基本原理并提高 QA 性能。</li>
</ul>

<h3>Title: Edisum: Summarizing and Explaining Wikipedia Edits at Scale</h3>
<ul>
<li><strong>Authors: </strong>Marija Šakota, Isaac Johnson, Guosheng Feng, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03428">https://arxiv.org/abs/2404.03428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03428">https://arxiv.org/pdf/2404.03428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03428]] Edisum: Summarizing and Explaining Wikipedia Edits at Scale(https://arxiv.org/abs/2404.03428)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>An edit summary is a succinct comment written by a Wikipedia editor explaining the nature of, and reasons for, an edit to a Wikipedia page. Edit summaries are crucial for maintaining the encyclopedia: they are the first thing seen by content moderators and help them decide whether to accept or reject an edit. Additionally, edit summaries constitute a valuable data source for researchers. Unfortunately, as we show, for many edits, summaries are either missing or incomplete. To overcome this problem and help editors write useful edit summaries, we propose a model for recommending edit summaries generated by a language model trained to produce good edit summaries given the representation of an edit diff. This is a challenging task for multiple reasons, including mixed-quality training data, the need to understand not only what was changed in the article but also why it was changed, and efficiency requirements imposed by the scale of Wikipedia. We address these challenges by curating a mix of human and synthetically generated training data and fine-tuning a generative language model sufficiently small to be used on Wikipedia at scale. Our model performs on par with human editors. Commercial large language models are able to solve this task better than human editors, but would be too expensive to run on Wikipedia at scale. More broadly, this paper showcases how language modeling technology can be used to support humans in maintaining one of the largest and most visible projects on the Web.</li>
<li><strong>摘要：</strong>编辑摘要是由维基百科编辑撰写的简洁评论，解释维基百科页面编辑的性质和原因。编辑摘要对于维护百科全书至关重要：它们是内容审核者看到的第一件事，可以帮助他们决定是否接受或拒绝编辑。此外，编辑摘要对研究人员来说是一个有价值的数据源。不幸的是，正如我们所展示的，对于许多编辑，摘要要么丢失，要么不完整。为了克服这个问题并帮助编辑者编写有用的编辑摘要，我们提出了一种模型，用于推荐由经过训练的语言模型生成的编辑摘要，该模型经过训练可以在给定编辑差异表示的情况下生成良好的编辑摘要。由于多种原因，这是一项具有挑战性的任务，包括混合质量的训练数据、不仅需要了解文章中更改的内容，还需要了解更改的原因，以及维基百科规模所带来的效率要求。我们通过混合人类和合成生成的训练数据，并对生成语言模型进行微调，使其足够小以在维基百科上大规模使用，从而应对这些挑战。我们的模型的表现与人类编辑相当。商业大型语言模型能够比人类编辑更好地解决此任务，但在维基百科上大规模运行成本太高。更广泛地说，本文展示了如何使用语言建模技术来支持人类维护网络上最大、最引人注目的项目之一。</li>
</ul>

<h3>Title: Scaffolding Language Learning via Multi-modal Tutoring Systems with  Pedagogical Instructions</h3>
<ul>
<li><strong>Authors: </strong>Zhengyuan Liu, Stella Xin Yin, Carolyn Lee, Nancy F. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03429">https://arxiv.org/abs/2404.03429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03429">https://arxiv.org/pdf/2404.03429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03429]] Scaffolding Language Learning via Multi-modal Tutoring Systems with  Pedagogical Instructions(https://arxiv.org/abs/2404.03429)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Intelligent tutoring systems (ITSs) that imitate human tutors and aim to provide immediate and customized instructions or feedback to learners have shown their effectiveness in education. With the emergence of generative artificial intelligence, large language models (LLMs) further entitle the systems to complex and coherent conversational interactions. These systems would be of great help in language education as it involves developing skills in communication, which, however, drew relatively less attention. Additionally, due to the complicated cognitive development at younger ages, more endeavors are needed for practical uses. Scaffolding refers to a teaching technique where teachers provide support and guidance to students for learning and developing new concepts or skills. It is an effective way to support diverse learning needs, goals, processes, and outcomes. In this work, we investigate how pedagogical instructions facilitate the scaffolding in ITSs, by conducting a case study on guiding children to describe images for language learning. We construct different types of scaffolding tutoring systems grounded in four fundamental learning theories: knowledge construction, inquiry-based learning, dialogic teaching, and zone of proximal development. For qualitative and quantitative analyses, we build and refine a seven-dimension rubric to evaluate the scaffolding process. In our experiment on GPT-4V, we observe that LLMs demonstrate strong potential to follow pedagogical instructions and achieve self-paced learning in different student groups. Moreover, we extend our evaluation framework from a manual to an automated approach, paving the way to benchmark various conversational tutoring systems.</li>
<li><strong>摘要：</strong>模仿人类导师并旨在向学习者提供即时和定制的指令或反馈的智能辅导系统（ITS）已经显示出其在教育中的有效性。随着生成人工智能的出现，大型语言模型（LLM）进一步使系统能够进行复杂且连贯的对话交互。这些系统对语言教育有很大帮助，因为它涉及培养沟通技能，但相对较少受到关注。此外，由于年轻时的认知发展较为复杂，因此在实际应用中还需要付出更多的努力。支架是指教师为学生学习和发展新概念或技能提供支持和指导的教学技术。它是支持多样化学习需求、目标、过程和结果的有效方式。在这项工作中，我们通过指导儿童描述语言学习图像的案例研究，研究教学指导如何促进 ITS 中的支架。我们构建了基于四种基本学习理论的不同类型的脚手架辅导系统：知识建构、探究式学习、对话式教学和最近发展区。为了进行定性和定量分析，我们构建并完善了七维评估标准来评估脚手架过程。在我们的 GPT-4V 实验中，我们观察到法学硕士在遵循教学指导并在不同学生群体中实现自定进度学习方面表现出强大的潜力。此外，我们将评估框架从手动方法扩展到自动化方法，为对各种对话辅导系统进行基准测试铺平了道路。</li>
</ul>

<h3>Title: Reevaluating Bias Detection in Language Models: The Role of Implicit  Norm</h3>
<ul>
<li><strong>Authors: </strong>Farnaz Kohankhaki, Jacob-Junqi Tian, David Emerson, Laleh Seyyed-Kalantari, Faiza Khan Khattak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03471">https://arxiv.org/abs/2404.03471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03471">https://arxiv.org/pdf/2404.03471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03471]] Reevaluating Bias Detection in Language Models: The Role of Implicit  Norm(https://arxiv.org/abs/2404.03471)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), trained on vast datasets, can carry biases that manifest in various forms, from overt discrimination to implicit stereotypes. One facet of bias is performance disparities in LLMs, often harming underprivileged groups, such as racial minorities. A common approach to quantifying bias is to use template-based bias probes, which explicitly state group membership (e.g. White) and evaluate if the outcome of a task, sentiment analysis for instance, is invariant to the change of group membership (e.g. change White race to Black). This approach is widely used in bias quantification. However, in this work, we find evidence of an unexpectedly overlooked consequence of using template-based probes for LLM bias quantification. We find that in doing so, text examples associated with White ethnicities appear to be classified as exhibiting negative sentiment at elevated rates. We hypothesize that the scenario arises artificially through a mismatch between the pre-training text of LLMs and the templates used to measure bias through reporting bias, unstated norms that imply group membership without explicit statement. Our finding highlights the potential misleading impact of varying group membership through explicit mention in bias quantification</li>
<li><strong>摘要：</strong>在大量数据集上训练的大型语言模型 (LLM) 可能会带有各种形式的偏见，从明显的歧视到隐含的刻板印象。偏见的一方面是法学硕士的表现差异，通常会伤害弱势群体，例如少数族裔。量化偏差的一种常见方法是使用基于模板的偏差探测，它明确地说明群体成员身份（例如，白人），并评估任务的结果（例如情绪分析）是否对群体成员身份的变化（例如，改变白人）保持不变。比赛到黑色）。这种方法广泛应用于偏差量化。然而，在这项工作中，我们发现了使用基于模板的探针进行 LLM 偏差量化的意外被忽视后果的证据。我们发现，这样做时，与白人相关的文本示例似乎被归类为以较高的比例表现出负面情绪。我们假设这种情况是由于法学硕士的预训练文本与用于通过报告偏差来衡量偏差的模板之间的不匹配而人为产生的，未明确的规范暗示了没有明确声明的群体成员资格。我们的发现通过在偏差量化中明确提及，强调了不同群体成员资格的潜在误导性影响</li>
</ul>

<h3>Title: Generative AI and Teachers -- For Us or Against Us? A Case Study</h3>
<ul>
<li><strong>Authors: </strong>Jenny Pettersson, Elias Hult, Tim Eriksson, Tosin Adewumi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03486">https://arxiv.org/abs/2404.03486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03486">https://arxiv.org/pdf/2404.03486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03486]] Generative AI and Teachers -- For Us or Against Us? A Case Study(https://arxiv.org/abs/2404.03486)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>We present insightful results of a survey on the adoption of generative artificial intelligence (GenAI) by university teachers in their teaching activities. The transformation of education by GenAI, particularly large language models (LLMs), has been presenting both opportunities and challenges, including cheating by students. We prepared the online survey according to best practices and the questions were created by the authors, who have pedagogy experience. The survey contained 12 questions and a pilot study was first conducted. The survey was then sent to all teachers in multiple departments across different campuses of the university of interest in Sweden: Lule{\aa} University of Technology. The survey was available in both Swedish and English. The results show that 35 teachers (more than half) use GenAI out of 67 respondents. Preparation is the teaching activity with the most frequency that GenAI is used for and ChatGPT is the most commonly used GenAI. 59% say it has impacted their teaching, however, 55% say there should be legislation around the use of GenAI, especially as inaccuracies and cheating are the biggest concerns.</li>
<li><strong>摘要：</strong>我们提供了一项关于大学教师在教学活动中采用生成人工智能 (GenAI) 的调查结果，该调查富有洞察力。 GenAI 对教育的变革，特别是大型语言模型 (LLM)，既带来了机遇，也带来了挑战，其中包括学生作弊。我们根据最佳实践准备了在线调查，问题由具有教育学经验的作者提出。该调查包含 12 个问题，并首先进行了试点研究。然后，该调查被发送给瑞典感兴趣的大学：吕勒{\aa}科技大学不同校区的多个系的所有教师。该调查有瑞典语和英语两种版本。结果显示，67 名受访者中有 35 名教师（超过一半）使用 GenAI。准备是GenAI使用频率最高的教学活动，ChatGPT是最常用的GenAI。 59% 的人表示这影响了他们的教学，但 55% 的人表示应该就 GenAI 的使用进行立法，特别是因为不准确和作弊是最大的问题。</li>
</ul>

<h3>Title: A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded  Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03491">https://arxiv.org/abs/2404.03491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03491">https://arxiv.org/pdf/2404.03491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03491]] A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded  Dialogue Generation(https://arxiv.org/abs/2404.03491)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.</li>
<li><strong>摘要：</strong>在大规模预训练语言模型的支持下，现有的对话系统在进行流畅、自然的对话方面表现出了令人印象深刻的性能。然而，他们仍然受到幻觉问题的困扰，导致生成的响应出现不可预测的事实错误。最近，基于知识的对话生成模型，有意调用外部知识资源来提供更多信息的响应，也被证明可以有效减少幻觉。本着获取高质量知识的理念，一些努力在这个问题上取得了不错的成绩。由于一些不可避免的知识噪声也可能导致幻觉，因此迫切需要研究在 KGD 任务中构建耐噪声方法的原因和未来方向。在本文中，我们用反事实推理方法分析了这个问题背后的因果故事。基于因果效应分析，我们提出了一种通过利用对话-知识交互来减轻 KGD 中幻觉的可能解决方案。我们示例实现的实验结果表明，该方法可以在不破坏其他对话性能的情况下减少幻觉，同时保持对不同生成模型的适应性。我们希望我们的努力能够支持并呼吁人们更多地关注开发轻量级技术以实现健壮且值得信赖的对话系统。</li>
</ul>

<h3>Title: Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive  Model-Aware Approach</h3>
<ul>
<li><strong>Authors: </strong>Chengkai Huang, Rui Wang, Kaige Xie, Tong Yu, Lina Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03514">https://arxiv.org/abs/2404.03514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03514">https://arxiv.org/pdf/2404.03514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03514]] Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive  Model-Aware Approach(https://arxiv.org/abs/2404.03514)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval. Aiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs' pretraining data. However, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pretraining data. Moreover, these methods offer limited adaptability under fine-tuning or continual learning settings. We hypothesize that token embeddings are able to capture the model's intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data. Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings. Extensive experiments and in-depth analyses demonstrate the superiority of our model-aware approach.</li>
<li><strong>摘要：</strong>检索增强大型语言模型 (LLM) 在各种 NLP 任务中表现出色。尽管取得了巨大的成功，检索过程提供的知识并不总是对改进模型预测有用，因为在某些样本中法学硕士可能已经非常了解，因此无需检索就能正确回答问题。为了节省检索成本，之前的工作提出通过分析法学硕士的预训练数据，以数据感知的方式确定何时进行/跳过检索。然而，这些数据感知方法会带来隐私风险和内存限制，特别是在需要访问敏感或广泛的预训练数据时。此外，这些方法在微调或持续学习设置下提供的适应性有限。我们假设令牌嵌入能够捕获模型的内在知识，这提供了一种更安全、更直接的方法来判断检索的需要，而不存在与访问预训练数据相关的隐私风险。此外，它减少了保留模型预训练期间使用的所有数据的需要，只需要维护令牌嵌入。大量的实验和深入的分析证明了我们的模型感知方法的优越性。</li>
</ul>

<h3>Title: BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with  Semantic Neural Graph Filtering</h3>
<ul>
<li><strong>Authors: </strong>Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG, cs.NE, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03528">https://arxiv.org/abs/2404.03528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03528">https://arxiv.org/pdf/2404.03528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03528]] BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with  Semantic Neural Graph Filtering(https://arxiv.org/abs/2404.03528)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text.</li>
<li><strong>摘要：</strong>知识图（KG）已被证明在信息处理和推理应用中至关重要，因为它们链接相关实体并提供丰富的上下文信息，支持高效的信息检索和知识发现；以非常有效的方式呈现信息流。尽管孟加拉语在全球范围内得到广泛使用，但由于缺乏全面的数据集、编码器、NER（命名实体识别）模型、POS（词性）标注器和词形还原器，孟加拉语在知识图谱中的代表性相对不足，阻碍了高效的信息处理和推理应用在语言中。为了解决孟加拉语知识图谱的稀缺问题，我们提出了 BanglaAutoKG，这是一个开创性的框架，能够从任何孟加拉语文本自动构建孟加拉语知识图谱。我们利用多语言法学硕士来理解各种语言并普遍关联实体和关系。通过使用翻译词典来识别英语对等词并从预训练的 BERT 模型中提取单词特征，我们构建了基础知识图谱。为了减少噪音并使词嵌入与我们的目标保持一致，我们采用基于图的多项式滤波器。最后，我们实现了一个基于 GNN 的语义过滤器，它提高了上下文理解并修剪了不必要的边缘，最终形成了明确的知识图谱。实证研究结果和案例研究证明了我们的模型的普遍有效性，能够从任何文本自主构建语义丰富的知识图谱。</li>
</ul>

<h3>Title: Evaluating Generative Language Models in Information Extraction as  Subjective Question Correction</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Fan, Yantao Liu, Zijun Yao, Jifan Yu, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03532">https://arxiv.org/abs/2404.03532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03532">https://arxiv.org/pdf/2404.03532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03532]] Evaluating Generative Language Models in Information Extraction as  Subjective Question Correction(https://arxiv.org/abs/2404.03532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Modern Large Language Models (LLMs) have showcased remarkable prowess in various tasks necessitating sophisticated cognitive behaviors. Nevertheless, a paradoxical performance discrepancy is observed, where these models underperform in seemingly elementary tasks like relation extraction and event extraction due to two issues in conventional evaluation. (1) The imprecision of existing evaluation metrics that struggle to effectively gauge semantic consistency between model outputs and ground truth, and (2) The inherent incompleteness of evaluation benchmarks, primarily due to restrictive human annotation schemas, resulting in underestimated LLM performances. Inspired by the principles in subjective question correction, we propose a new evaluation method, SQC-Score. This method innovatively utilizes LLMs, fine-tuned through subjective question correction data, to refine matching between model outputs and golden labels. Additionally, by incorporating a Natural Language Inference (NLI) model, SQC-Score enriches golden labels, addressing benchmark incompleteness by acknowledging correct yet previously omitted answers. Results on three information extraction tasks show that SQC-Score is more preferred by human annotators than the baseline metrics. Utilizing SQC-Score, we conduct a comprehensive evaluation of the state-of-the-art LLMs and provide insights for future research for information extraction. Dataset and associated codes can be accessed at https://github.com/THU-KEG/SQC-Score.</li>
<li><strong>摘要：</strong>现代大型语言模型（LLM）在需要复杂认知行为的各种任务中表现出了非凡的能力。然而，我们观察到了一个矛盾的性能差异，由于传统评估中的两个问题，这些模型在关系提取和事件提取等看似基本的任务中表现不佳。 (1) 现有评估指标的不精确性，难以有效衡量模型输出和真实情况之间的语义一致性，(2) 评估基准固有的不完整性，主要是由于限制性的人工注释模式，导致 LLM 表现被低估。受主观题纠错原理的启发，我们提出了一种新的评估方法——SQC-Score。该方法创新性地利用LLM，通过主观问题校正数据进行微调，以细化模型输出和黄金标签之间的匹配。此外，通过结合自然语言推理 (NLI) 模型，SQC-Score 丰富了黄金标签，通过确认正确但之前遗漏的答案来解决基准的不完整性问题。三个信息提取任务的结果表明，与基线指标相比，人类注释者更喜欢 SQC-Score。利用 SQC-Score，我们对最先进的法学硕士进行全面评估，并为未来的信息提取研究提供见解。数据集和相关代码可以在 https://github.com/THU-KEG/SQC-Score 访问。</li>
</ul>

<h3>Title: How does Multi-Task Training Affect Transformer In-Context Capabilities?  Investigations with Function Classes</h3>
<ul>
<li><strong>Authors: </strong>Harmon Bhasin, Timothy Ossowski, Yiqiao Zhong, Junjie Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03558">https://arxiv.org/abs/2404.03558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03558">https://arxiv.org/pdf/2404.03558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03558]] How does Multi-Task Training Affect Transformer In-Context Capabilities?  Investigations with Function Classes(https://arxiv.org/abs/2404.03558)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) have recently shown the extraordinary ability to perform unseen tasks based on few-shot examples provided as text, also known as in-context learning (ICL). While recent works have attempted to understand the mechanisms driving ICL, few have explored training strategies that incentivize these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist models is a promising direction that offers transfer learning potential, enabling large parameterized models to be trained from simpler, related tasks. In this work, we investigate the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples. We propose several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence. Our experiments reveal that ICL models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed curriculum in this work. Our code and models are available at https://github.com/harmonbhasin/curriculum_learning_icl .</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近显示出基于以文本形式提供的少量示例执行未见过的任务的非凡能力，也称为上下文学习 (ICL)。虽然最近的工作试图理解驱动 ICL 的机制，但很少有人探索激励这些模型泛化到多个任务的训练策略。通才模型的多任务学习（MTL）是一个有前途的方向，它提供了迁移学习的潜力，使大型参数化模型能够从更简单的相关任务中进行训练。在这项工作中，我们研究了 MTL 与 ICL 的结合，以构建能够有效学习任务的模型，同时对分布外的示例具有鲁棒性。我们提出了几种有效的课程学习策略，使 ICL 模型能够实现更高的数据效率和更稳定的收敛。我们的实验表明，ICL 模型可以通过训练逐渐困难的任务，同时混合先前的任务，来有效地学习困难的任务，在这项工作中表示为混合课程。我们的代码和模型可在 https://github.com/harmonbhasin/curriculum_learning_icl 获取。</li>
</ul>

<h3>Title: Select and Summarize: Scene Saliency for Movie Script Summarization</h3>
<ul>
<li><strong>Authors: </strong>Rohit Saxena, Frank Keller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03561">https://arxiv.org/abs/2404.03561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03561">https://arxiv.org/pdf/2404.03561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03561]] Select and Summarize: Scene Saliency for Movie Script Summarization(https://arxiv.org/abs/2404.03561)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Abstractive summarization for long-form narrative texts such as movie scripts is challenging due to the computational and memory constraints of current language models. A movie script typically comprises a large number of scenes; however, only a fraction of these scenes are salient, i.e., important for understanding the overall narrative. The salience of a scene can be operationalized by considering it as salient if it is mentioned in the summary. Automatically identifying salient scenes is difficult due to the lack of suitable datasets. In this work, we introduce a scene saliency dataset that consists of human-annotated salient scenes for 100 movies. We propose a two-stage abstractive summarization approach which first identifies the salient scenes in script and then generates a summary using only those scenes. Using QA-based evaluation, we show that our model outperforms previous state-of-the-art summarization methods and reflects the information content of a movie more accurately than a model that takes the whole movie script as input.</li>
<li><strong>摘要：</strong>由于当前语言模型的计算和内存限制，对电影剧本等长篇叙事文本的抽象概括具有挑战性。电影剧本通常包含大量场景；然而，这些场景中只有一小部分是显着的，即对于理解整体叙述很重要。如果在摘要中提到场景，则可以通过将其视为显着来操作场景的显着性。由于缺乏合适的数据集，自动识别显着场景很困难。在这项工作中，我们引入了一个场景显着性数据集，其中包含 100 部电影的人工注释显着场景。我们提出了一种两阶段抽象摘要方法，首先识别脚本中的显着场景，然后仅使用这些场景生成摘要。使用基于 QA 的评估，我们表明我们的模型优于以前最先进的摘要方法，并且比以整个电影剧本作为输入的模型更准确地反映电影的信息内容。</li>
</ul>

<h3>Title: Personalized LLM Response Generation with Parameterized Memory Injection</h3>
<ul>
<li><strong>Authors: </strong>Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03565">https://arxiv.org/abs/2404.03565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03565">https://arxiv.org/pdf/2404.03565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03565]] Personalized LLM Response Generation with Parameterized Memory Injection(https://arxiv.org/abs/2404.03565)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \textbf{L}LM \textbf{P}ersonalization(\textbf{MiLP}).</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在理解和生成自然语言方面表现出了卓越的能力。另一方面，个性化的法学硕士回复有可能为医疗等关键领域的个人带来巨大的好处。现有的研究已经探索了记忆增强方法，以通过预先存储的用户特定知识来提示法学硕士，以便根据新查询生成个性化响应。我们认为这种范式无法感知细粒度信息。在这项研究中，我们提出了一种新颖的 \textbf{M}emory-\textbf{i}njected 方法，使用参数高效微调（PEFT）和贝叶斯优化搜索策略来实现 \textbf{L}LM \textbf {P}个性化(\textbf{MiLP})。</li>
</ul>

<h3>Title: Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning  Skills in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03577">https://arxiv.org/abs/2404.03577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03577">https://arxiv.org/pdf/2404.03577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03577]] Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning  Skills in Large Language Models(https://arxiv.org/abs/2404.03577)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters. However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory. While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge. Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning. To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering. KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions. (2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question. (3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions. We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances. Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT .</li>
<li><strong>摘要：</strong>为大型语言模型（LLM）提供知识文档已成为更新其参数中固有的静态知识的有前途的解决方案。然而，由于法学硕士参数中的知识过时或不正确，文档中的知识可能与法学硕士的记忆发生冲突。这导致有必要检查法学硕士吸收与其记忆相冲突的补充外部知识的能力。虽然之前的研究已经解释了法学硕士从提供的文本中提取冲突知识的程度，但他们忽略了对冲突知识进行推理的必要性。此外，缺乏对法学硕士通过提示、解码策略和监督微调来解决知识冲突的策略的详细分析。为了解决这些限制，我们构建了一个新的数据集，称为 KNOT，以问答形式进行知识冲突解决检查。 KNOT通过将冲突知识的推理分为三个层次来促进深入分析：（1）直接提取，直接提取冲突知识来回答问题。 (2)显式推理，当问题中明确提供推理路径时，用冲突的知识进行推理。 (3) 内隐推理，用冲突的知识进行推理，需要法学硕士独立推断推理路径来回答问题。我们还对 KNOT 进行了广泛的实验，为法学硕士在复杂情况下利用冲突知识建立经验指南。数据集和相关代码可以在 https://github.com/THU-KEG/KNOT 访问。</li>
</ul>

<h3>Title: ReFT: Representation Finetuning for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03592">https://arxiv.org/abs/2404.03592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03592">https://arxiv.org/pdf/2404.03592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03592]] ReFT: Representation Finetuning for Language Models(https://arxiv.org/abs/2404.03592)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. Here, we pursue this hypothesis by developing a family of $\textbf{Representation Finetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.</li>
<li><strong>摘要：</strong>参数高效微调（PEFT）方法寻求通过更新少量权重来适应大型模型。然而，许多先前的可解释性工作表明，表示编码了丰富的语义信息，这表明编辑表示可能是一种更强大的替代方案。在这里，我们通过开发一系列 $\textbf{Representation Finetuning (ReFT)}$ 方法来追求这一假设。 ReFT 方法在冻结的基础模型上运行，并学习对隐藏表示的特定任务干预。我们定义了 ReFT 系列的一个强大实例，即低秩线性子空间 ReFT (LoReFT)。 LoReFT 是现有 PEFT 的直接替代品，其学习干预措施的参数效率比之前最先进的 PEFT 高 10 至 50 倍。我们展示了 LoReFT 在八个常识推理任务、四个算术推理任务、Alpaca-Eval v1.0 和 GLUE 上的表现。在所有这些评估中，LoReFT 提供了效率和性能的最佳平衡，并且几乎总是优于最先进的 PEFT。我们在 https://github.com/stanfordnlp/pyreft 公开发布了通用 ReFT 训练库。</li>
</ul>

<h3>Title: Intent Detection and Entity Extraction from BioMedical Literature</h3>
<ul>
<li><strong>Authors: </strong>Ankan Mullick, Mukur Gupta, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03598">https://arxiv.org/abs/2404.03598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03598">https://arxiv.org/pdf/2404.03598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03598]] Intent Detection and Entity Extraction from BioMedical Literature(https://arxiv.org/abs/2404.03598)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Biomedical queries have become increasingly prevalent in web searches, reflecting the growing interest in accessing biomedical literature. Despite recent research on large-language models (LLMs) motivated by endeavours to attain generalized intelligence, their efficacy in replacing task and domain-specific natural language understanding approaches remains questionable. In this paper, we address this question by conducting a comprehensive empirical evaluation of intent detection and named entity recognition (NER) tasks from biomedical text. We show that Supervised Fine Tuned approaches are still relevant and more effective than general-purpose LLMs. Biomedical transformer models such as PubMedBERT can surpass ChatGPT on NER task with only 5 supervised examples.</li>
<li><strong>摘要：</strong>生物医学查询在网络搜索中变得越来越普遍，反映出人们对访问生物医学文献的兴趣日益浓厚。尽管最近对大语言模型（LLM）的研究是为了实现广义智能，但它们在替代任务和特定领域自然语言理解方法方面的功效仍然值得怀疑。在本文中，我们通过对生物医学文本中的意图检测和命名实体识别（NER）任务进行全面的实证评估来解决这个问题。我们表明，监督微调方法仍然比通用法学硕士更有意义且更有效。 PubMedBERT 等生物医学 Transformer 模型在 NER 任务上仅用 5 个监督示例就可以超越 ChatGPT。</li>
</ul>

<h3>Title: Evaluating LLMs at Detecting Errors in LLM Responses</h3>
<ul>
<li><strong>Authors: </strong>Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03602">https://arxiv.org/abs/2404.03602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03602">https://arxiv.org/pdf/2404.03602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03602]] Evaluating LLMs at Detecting Errors in LLM Responses(https://arxiv.org/abs/2404.03602)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在各种任务中广泛使用，检测其响应中的错误变得越来越重要。然而，很少有关于 LLM 答案错误检测的研究。由于许多 NLP 任务的主观性质，收集 LLM 回答的错误注释具有挑战性，因此之前的研究主要集中在实用价值不大的任务（例如，词排序）或有限的错误类型（例如，总结的忠实度）。这项工作介绍了ReaLMistake，这是第一个错误检测基准，由法学硕士所犯的客观、现实和多样化的错误组成。 RealLMistake 包含三个具有挑战性且有意义的任务，这些任务在四个类别（推理正确性、指令遵循、上下文忠实性和参数化知识）中引入客观可评估的错误，在 GPT-4 和 Llama 2 70B 的响应中引出自然观察到的各种错误，注释为专家。我们使用 RealLMistake 来评估基于 12 个 LLM 的错误检测器。我们的研究结果表明：1) 像 GPT-4 和 Claude 3 这样的顶级法学硕士能够以非常低的召回率检测到法学硕士所犯的错误，并且所有基于法学硕士的错误检测器的表现都比人类差得多。 2）基于LLM的错误检测器的解释缺乏可靠性。 3) 基于法学硕士的错误检测对提示中的微小变化很敏感，但仍难以改进。 4) 改进法学硕士的流行方法，包括自我一致性和多数投票，并不能提高错误检测性能。我们的基准测试和代码位于 https://github.com/psunlpgroup/ReaLMistake。</li>
</ul>

<h3>Title: Sailor: Open Language Models for South-East Asia</h3>
<ul>
<li><strong>Authors: </strong>Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03608">https://arxiv.org/abs/2404.03608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03608">https://arxiv.org/pdf/2404.03608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03608]] Sailor: Open Language Models for South-East Asia(https://arxiv.org/abs/2404.03608)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present Sailor, a family of open language models ranging from 0.5B to 7B parameters, tailored for South-East Asian (SEA) languages. These models are continually pre-trained from Qwen1.5, a great language model for multilingual use cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily covering the languages of English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao. The training leverages several techniques, including BPE dropout for improving the model robustness, aggressive data cleaning and deduplication, and small proxy models to optimize data mixture. Experimental results on four typical tasks indicate that Sailor models demonstrate strong performance across different benchmarks, including commonsense reasoning, question answering, reading comprehension and examination. Embracing the open-source spirit, we share our insights through this report to spark a wider interest in developing large language models for multilingual use cases.</li>
<li><strong>摘要：</strong>我们推出了 Sailor，这是一个开放语言模型系列，参数范围从 0.5B 到 7B，专为东南亚 (SEA) 语言量身定制。这些模型是从 Qwen1.5 开始不断进行预训练的，Qwen1.5 是一个适用于多语言用例的出色语言模型。从Qwen1.5开始，Sailor模型接受200B到400B的代币，主要涵盖英语、汉语、越南语、泰语、印尼语、马来语和老挝语。训练利用了多种技术，包括用于提高模型稳健性的 BPE dropout、积极的数据清理和重复数据删除以及用于优化数据混合的小型代理模型。四个典型任务的实验结果表明，Sailor 模型在常识推理、问答、阅读理解和考试等不同基准上都表现出了很强的性能。秉承开源精神，我们通过本报告分享我们的见解，以激发人们对为多语言用例开发大型语言模型的更广泛兴趣。</li>
</ul>

<h3>Title: Visualization-of-Thought Elicits Spatial Reasoning in Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03622">https://arxiv.org/abs/2404.03622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03622">https://arxiv.org/pdf/2404.03622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03622]] Visualization-of-Thought Elicits Spatial Reasoning in Large Language  Models(https://arxiv.org/abs/2404.03622)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as \textbf{the Mind's Eye}, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (\textbf{VoT}) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate \textit{mental images} to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在语言理解和各种推理任务中表现出了令人印象深刻的性能。然而，它们的空间推理能力（人类认知的一个重要方面）仍然相对未被开发。人类拥有一种非凡的能力，可以通过称为\textbf{心灵之眼}的过程来创造看不见的物体和行为的心理图像，从而实现对看不见的世界的想象。受这种认知能力的启发，我们提出了思维可视化（\textbf{VoT}）提示。 VoT旨在通过可视化法学硕士的推理轨迹来引发法学硕士的空间推理，从而指导后续的推理步骤。我们将 VoT 用于多跳空间推理任务，包括自然语言导航、视觉导航和 2D 网格世界中的视觉平铺。实验结果表明，VoT 显着增强了法学硕士的空间推理能力。值得注意的是，VoT 在这些任务中的表现优于现有的多模式大语言模型 (MLLM)。虽然 VoT 在 LLM 上的效果出奇的好，但生成 \textit{心理图像} 以促进空间推理的能力类似于心灵的眼睛过程，这表明它在 MLLM 中的潜在可行性。</li>
</ul>

<h3>Title: Unveiling LLMs: The Evolution of Latent Representations in a Temporal  Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03623">https://arxiv.org/abs/2404.03623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03623">https://arxiv.org/pdf/2404.03623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03623]] Unveiling LLMs: The Evolution of Latent Representations in a Temporal  Knowledge Graph(https://arxiv.org/abs/2404.03623)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of common factual knowledge information. However, unravelling the underlying reasoning of LLMs and explaining their internal mechanisms of exploiting this factual knowledge remain active areas of investigation. Our work analyzes the factual knowledge encoded in the latent representation of LLMs when prompted to assess the truthfulness of factual claims. We propose an end-to-end framework that jointly decodes the factual knowledge embedded in the latent space of LLMs from a vector space to a set of ground predicates and represents its evolution across the layers using a temporal knowledge graph. Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations. Consequently, we neither rely on external models nor training processes. We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The local interpretability analysis exposes different latent errors from representation to multi-hop reasoning errors. On the other hand, the global analysis uncovered patterns in the underlying evolution of the model's factual knowledge (e.g., store-and-seek factual information). By enabling graph-based analyses of the latent representations, this work represents a step towards the mechanistic interpretability of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出令人印象深刻的回忆大量常见事实知识信息的能力。然而，阐明法学硕士的根本推理并解释其利用事实知识的内部机制仍然是活跃的研究领域。当提示评估事实主张的真实性时，我们的工作分析了法学硕士潜在表示中编码的事实知识。我们提出了一个端到端框架，该框架将嵌入 LLM 潜在空间中的事实知识从向量空间联合解码为一组基本谓词，并使用时间知识图来表示其跨层的演变。我们的框架依赖于激活修补技术，该技术通过动态改变模型的潜在表示来干预模型的推理计算。因此，我们既不依赖外部模型，也不依赖训练过程。我们使用两个声明验证数据集：FEVER 和 CLIMATE-FEVER，通过本地和全球可解释性分析来展示我们的框架。局部可解释性分析揭示了从表示错误到多跳推理错误的不同潜在错误。另一方面，全局分析揭示了模型事实知识的潜在演变模式（例如，存储和寻求事实信息）。通过对潜在表示进行基于图形的分析，这项工作代表了法学硕士的机械解释性迈出了一步。</li>
</ul>

<h3>Title: Training LLMs over Neurally Compressed Text</h3>
<ul>
<li><strong>Authors: </strong>Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03626">https://arxiv.org/abs/2404.03626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03626">https://arxiv.org/pdf/2404.03626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03626]] Training LLMs over Neurally Compressed Text(https://arxiv.org/abs/2404.03626)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.</li>
<li><strong>摘要：</strong>在本文中，我们探讨了在高度压缩的文本上训练大型语言模型（LLM）的想法。虽然标准子词标记器以较小的因子压缩文本，但神经文本压缩器可以实现更高的压缩率。如果可以直接在神经压缩文本上训练法学硕士，这将在训练和服务效率方面带来优势，并且更容易处理长文本跨度。这一目标的主要障碍是强压缩往往会产生不透明的输出，不太适合学习。特别是，我们发现法学硕士不容易学习通过算术编码自然压缩的文本。为了克服这个问题，我们提出了 Equal-Info Windows，这是一种新颖的压缩技术，将文本分割成块，每个块压缩到相同的位使用这种方法，我们展示了对神经压缩文本的有效学习，该文本随着规模的扩大而提高，并且在困惑度和推理速度基准上大大优于字节级基线。虽然我们的方法为使用相同的参数数量，它的优点是序列长度更短。更短的序列长度需要更少的自回归生成步骤，并减少延迟。最后，我们对有助于可学习性的属性进行了广泛的分析，并为如何进一步提高可学习性提供了具体建议高压缩分词器的性能。</li>
</ul>

<h3>Title: Locating and Editing Factual Associations in Mamba</h3>
<ul>
<li><strong>Authors: </strong>Arnab Sen Sharma, David Atkinson, David Bau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03646">https://arxiv.org/abs/2404.03646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03646">https://arxiv.org/pdf/2404.03646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03646]] Locating and Editing Factual Associations in Mamba(https://arxiv.org/abs/2404.03646)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers. Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models. Third, we examine the linearity of Mamba's representations of factual relations. Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall. We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities.</li>
<li><strong>摘要：</strong>我们研究曼巴状态空间模型中的事实回忆机制。我们的工作受到了先前在自回归变压器语言模型中的发现的启发，表明它们的知识回忆被本地化到特定标记位置的特定模块；因此，我们想知道 Mamba 中的事实回忆是否可以类似地本地化。为了研究这一点，我们对曼巴进行了四行实验。首先，我们应用因果追踪或交换干预来定位 Mamba 内部负责回忆事实的关键组件，揭示中间层内的特定组件在主题的最后一个标记处表现出强烈的因果效应，而干预后面层的因果效应在提示的最后一个标记处最明显，与之前关于自回归变压器的发现相匹配。其次，我们表明一级模型编辑方法可以成功地在特定位置插入事实，这再次类似于变压器模型的发现。第三，我们检查曼巴对事实关系表示的线性。最后，我们将注意力剔除技术应用于 Mamba，以剖析事实回忆过程中的信息流。我们直接将 Mamba 与类似大小的 Transformer 进行比较，并得出结论，尽管架构方法存在显着差异，但在事实回忆方面，这两种架构有许多相似之处。</li>
</ul>

<h3>Title: AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web  Navigating Agent</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03648">https://arxiv.org/abs/2404.03648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03648">https://arxiv.org/pdf/2404.03648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03648]] AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web  Navigating Agent(https://arxiv.org/abs/2404.03648)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at \url{https://github.com/THUDM/AutoWebGLM}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 推动了许多智能代理任务，例如网络导航，但由于以下三个因素，大多数现有代理在现实网页中的表现远不能令人满意：(1) 网页上操作的多功能性，(2) HTML 文本超出模型处理能力，以及 (3) 由于 Web 的开放域性质而导致决策的复杂性。鉴于这一挑战，我们开发了 AutoWebGLM，这是一种基于 ChatGLM3-6B 构建的、性能优于 GPT-4 的自动 Web 导航代理。受人类浏览模式的启发，我们设计了一种 HTML 简化算法来表示网页，简洁地保留重要信息。我们采用混合人类人工智能方法来构建用于课程培训的网络浏览数据。然后，我们通过强化学习和拒绝采样来引导模型，以进一步促进网页理解、浏览器操作和高效的任务分解。为了进行测试，我们为现实世界的网页浏览任务建立了一个双语基准——AutoWebBench。我们通过不同的网络导航基准评估 AutoWebGLM，揭示其改进，但也揭示了应对真实环境的潜在挑战。相关代码、模型和数据将在\url{https://github.com/THUDM/AutoWebGLM}发布。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
