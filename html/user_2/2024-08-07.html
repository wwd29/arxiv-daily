<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-07</h1>
<h3>Title: LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory</h3>
<ul>
<li><strong>Authors: </strong>Jillian Ross, Yoon Kim, Andrew W. Lo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02784">https://arxiv.org/abs/2408.02784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02784">https://arxiv.org/pdf/2408.02784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02784]] LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory(https://arxiv.org/abs/2408.02784)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Humans are not homo economicus (i.e., rational economic beings). As humans, we exhibit systematic behavioral biases such as loss aversion, anchoring, framing, etc., which lead us to make suboptimal economic decisions. Insofar as such biases may be embedded in text data on which large language models (LLMs) are trained, to what extent are LLMs prone to the same behavioral biases? Understanding these biases in LLMs is crucial for deploying LLMs to support human decision-making. We propose utility theory-a paradigm at the core of modern economic theory-as an approach to evaluate the economic biases of LLMs. Utility theory enables the quantification and comparison of economic behavior against benchmarks such as perfect rationality or human behavior. To demonstrate our approach, we quantify and compare the economic behavior of a variety of open- and closed-source LLMs. We find that the economic behavior of current LLMs is neither entirely human-like nor entirely economicus-like. We also find that most current LLMs struggle to maintain consistent economic behavior across settings. Finally, we illustrate how our approach can measure the effect of interventions such as prompting on economic biases.</li>
<li><strong>摘要：</strong>人类不是经济人（即理性的经济人）。作为人类，我们表现出系统性的行为偏见，例如损失厌恶、锚定、框架等，这导致我们做出次优的经济决策。如果此类偏见可能嵌入在训练大型语言模型 (LLM) 的文本数据中，那么 LLM 在多大程度上容易出现相同的行为偏见？了解 LLM 中的这些偏见对于部署 LLM 来支持人类决策至关重要。我们提出了效用理论（现代经济理论的核心范式）作为评估 LLM 经济偏见的方法。效用理论可以量化和比较经济行为与基准（例如完全理性或人类行为）。为了展示我们的方法，我们量化和比较了各种开源和闭源 LLM 的经济行为。我们发现，当前 LLM 的经济行为既不完全像人类，也不完全像经济人。我们还发现，大多数当前的 LLM 都难以在不同环境中保持一致的经济行为。最后，我们说明我们的方法如何衡量诸如提示等干预措施对经济偏见的影响。</li>
</ul>

<h3>Title: A Framework for Fine-Tuning LLMs using Heterogeneous Feedback</h3>
<ul>
<li><strong>Authors: </strong>Ryan Aponte (1), Ryan A. Rossi (2), Shunan Guo (2), Franck Dernoncourt (2), Tong Yu (2), Xiang Chen (2), Subrata Mitra (2), Nedim Lipka (2) ((1) Carnegie Mellon University, (2) Adobe Research)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02861">https://arxiv.org/abs/2408.02861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02861">https://arxiv.org/pdf/2408.02861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02861]] A Framework for Fine-Tuning LLMs using Heterogeneous Feedback(https://arxiv.org/abs/2408.02861)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been applied to a wide range of tasks, including text summarization, web navigation, and chatbots. They have benefitted from supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) following an unsupervised pretraining. These datasets can be difficult to collect, limited in scope, and vary in sample quality. Additionally, datasets can vary extensively in supervision format, from numerical to binary as well as multi-dimensional with many different values. We present a framework for fine-tuning LLMs using heterogeneous feedback, which has two main components. First, we combine the heterogeneous feedback data into a single supervision format, compatible with methods like SFT and RLHF. Next, given this unified feedback dataset, we extract a high-quality and diverse subset to obtain performance increases potentially exceeding the full dataset. We conduct extensive experiments to understand the effectiveness of these techniques for incorporating heterogeneous feedback, and demonstrate improvements from using a high-quality and diverse subset of the data. We find that our framework is able to improve models in multiple areas simultaneously, such as in instruction following and bias reduction.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已应用于各种任务，包括文本摘要、网页导航和聊天机器人。它们受益于监督微调 (SFT) 和在无监督预训练之后的人工反馈强化学习 (RLHF)。这些数据集可能难以收集、范围有限且样本质量各不相同。此外，数据集的监督格式可能千差万别，从数值到二进制以及具有许多不同值的多维。我们提出了一个使用异构反馈微调 LLM 的框架，该框架有两个主要组成部分。首先，我们将异构反馈数据组合成单一监督格式，与 SFT 和 RLHF 等方法兼容。接下来，给定这个统一的反馈数据集，我们提取一个高质量且多样化的子集，以获得可能超过完整数据集的性能提升。我们进行了广泛的实验，以了解这些技术用于整合异构反馈的有效性，并展示了使用高质量且多样化的数据子集所带来的改进。我们发现我们的框架能够同时在多个领域改进模型，例如在指令遵循和减少偏差方面。</li>
</ul>

<h3>Title: Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language Model-Based Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Tiezheng Guo, Chen Wang, Yanyi Liu, Jiawei Tang, Pan Li, Sai Xu, Qingwen Yang, Xianlin Gao, Zhi Li, Yingyou Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02907">https://arxiv.org/abs/2408.02907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02907">https://arxiv.org/pdf/2408.02907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02907]] Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language Model-Based Question Answering(https://arxiv.org/abs/2408.02907)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Retrieving external knowledge and prompting large language models with relevant information is an effective paradigm to enhance the performance of question-answering tasks. Previous research typically handles paragraphs from external documents in isolation, resulting in a lack of context and ambiguous references, particularly in multi-document and complex tasks. To overcome these challenges, we propose a new retrieval framework IIER, that leverages Inter-chunk Interactions to Enhance Retrieval. This framework captures the internal connections between document chunks by considering three types of interactions: structural, keyword, and semantic. We then construct a unified Chunk-Interaction Graph to represent all external documents comprehensively. Additionally, we design a graph-based evidence chain retriever that utilizes previous paths and chunk interactions to guide the retrieval process. It identifies multiple seed nodes based on the target question and iteratively searches for relevant chunks to gather supporting evidence. This retrieval process refines the context and reasoning chain, aiding the large language model in reasoning and answer generation. Extensive experiments demonstrate that IIER outperforms strong baselines across four datasets, highlighting its effectiveness in improving retrieval and reasoning capabilities.</li>
<li><strong>摘要：</strong>检索外部知识并用相关信息提示大型语言模型是提高问答任务性能的有效范例。以前的研究通常孤立地处理外部文档中的段落，导致缺乏上下文和模糊引用，尤其是在多文档和复杂任务中。为了克服这些挑战，我们提出了一个新的检索框架 IIER，它利用块间交互来增强检索。该框架通过考虑三种类型的交互来捕获文档块之间的内部连接：结构、关键字和语义。然后，我们构建一个统一的块交互图来全面表示所有外部文档。此外，我们设计了一个基于图的证据链检索器，它利用先前的路径和块交互来指导检索过程。它根据目标问题识别多个种子节点，并迭代搜索相关块以收集支持证据。这个检索过程细化了上下文和推理链，帮助大型语言模型进行推理和答案生成。大量实验表明，IIER 在四个数据集上的表现优于强基线，凸显了其在提高检索和推理能力方面的有效性。</li>
</ul>

<h3>Title: Data Checklist: On Unit-Testing Datasets with Usable Information</h3>
<ul>
<li><strong>Authors: </strong>Heidi C. Zhang, Shabnam Behzad, Kawin Ethayarajh, Dan Jurafsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02919">https://arxiv.org/abs/2408.02919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02919">https://arxiv.org/pdf/2408.02919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02919]] Data Checklist: On Unit-Testing Datasets with Usable Information(https://arxiv.org/abs/2408.02919)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for understanding the behavior of LLMs, analogous to unit-testing in software engineering. However, despite datasets being a key determinant of model behavior, evaluating datasets, e.g., for the existence of annotation artifacts, is largely done ad hoc, once a problem in model behavior has already been found downstream. In this work, we take a more principled approach to unit-testing datasets by proposing a taxonomy based on the V-information literature. We call a collection of such unit tests a data checklist. Using a checklist, not only are we able to recover known artifacts in well-known datasets such as SNLI, but we also discover previously unknown artifacts in preference datasets for LLM alignment. Data checklists further enable a new kind of data filtering, which we use to improve the efficacy and data efficiency of preference alignment.</li>
<li><strong>摘要：</strong>模型检查表 (Ribeiro 等人，2020) 已成为理解 LLM 行为的有用工具，类似于软件工程中的单元测试。然而，尽管数据集是模型行为的关键决定因素，但评估数据集（例如，是否存在注释工件）主要是在下游发现模型行为问题后临时进行的。在这项工作中，我们通过提出基于 V 信息文献的分类法，采取了一种更有原则的方法来对数据集进行单元测试。我们将此类单元测试的集合称为数据检查表。使用检查表，我们不仅能够恢复众所周知的数据集（例如 SNLI）中的已知工件，而且还能发现偏好数据集中以前未知的工件，以进行 LLM 对齐。数据检查表进一步实现了一种新的数据过滤，我们利用这种过滤来提高偏好对齐的有效性和数据效率。</li>
</ul>

<h3>Title: Intermediate direct preference optimization</h3>
<ul>
<li><strong>Authors: </strong>Atsushi Kojima</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02923">https://arxiv.org/abs/2408.02923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02923">https://arxiv.org/pdf/2408.02923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02923]] Intermediate direct preference optimization(https://arxiv.org/abs/2408.02923)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We propose the intermediate direct preference optimization (DPO) method to calculate the DPO loss at selected intermediate layers as an auxiliary loss for finetuning large language models (LLMs). The conventional DPO method fine-tunes a supervised fine-tuning (SFT) model by calculating the DPO loss using logits from the final layer. In our intermediate DPO approach, DPO losses are calculated using the logits from K-selected intermediate layers and averaged to obtain the intermediate DPO loss. For training the intermediate DPO model, the final loss is obtained by calculating the weighted sum of the DPO and intermediate DPO losses. During inference, the intermediate DPO model decodes using the final layer logits similarly to the conventional DPO model. In experiments using the ultrafeedback dataset, the performance of the intermediate DPO model was evaluated using GPT-4. As a result, the intermediate DPO model trained using the intermediate DPO loss calculated at the 22nd layer of a 32-layer SFT model achieved win rates of 52.5% and 67.5% against the conventional DPO and SFT models, respectively, demonstrating the effectiveness of the proposed method. Furthermore, we report the relationships among the position of the selected intermediate layers, the number of layers, and performance.</li>
<li><strong>摘要：</strong>我们提出了中间直接偏好优化 (DPO) 方法来计算选定中间层的 DPO 损失，作为微调大型语言模型 (LLM) 的辅助损失。传统 DPO 方法通过使用来自最后一层的 logit 计算 DPO 损失来微调监督微调 (SFT) 模型。在我们的中间 DPO 方法中，使用来自 K 个选定中间层的 logit 计算 DPO 损失并取平均值以获得中间 DPO 损失。对于训练中间 DPO 模型，最终损失是通过计算 DPO 和中间 DPO 损失的加权和来获得的。在推理过程中，中间 DPO 模型使用最后一层 logit 进行解码，类似于传统 DPO 模型。在使用超反馈数据集的实验中，使用 GPT-4 评估了中间 DPO 模型的性能。结果显示，使用 32 层 SFT 模型第 22 层计算的中间 DPO 损失训练的中间 DPO 模型分别相对于传统 DPO 和 SFT 模型取得了 52.5% 和 67.5% 的胜率，证明了所提方法的有效性。此外，我们报告了所选中间层的位置、层数和性能之间的关系。</li>
</ul>

<h3>Title: Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Iman Azimi, Mohan Qi, Li Wang, Amir M. Rahmani, Youlin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02964">https://arxiv.org/abs/2408.02964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02964">https://arxiv.org/pdf/2408.02964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02964]] Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval(https://arxiv.org/abs/2408.02964)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education. Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient. In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries. Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels. In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses. Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains. GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency. For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency. RAP was particularly effective for GPT-4o to answer Expert level questions. Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 正在从根本上改变健康和福祉领域的面向人类的应用程序：提高患者参与度、加快临床决策和促进医学教育。尽管最先进的 LLM 在多个对话应用程序中表现出色，但在营养和饮食应用程序内的评估仍然不足。在本文中，我们建议采用注册营养师 (RD) 考试对最先进的 LLM、GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5 Pro 进行标准和全面的评估，评估营养查询的准确性和一致性。我们的评估包括 1050 道 RD 考试问题，涵盖多个营养主题和熟练程度。此外，我们首次研究了零样本 (ZS)、思路链 (CoT)、具有自我一致性的思路链 (CoT-SC) 和检索增强提示 (RAP) 对响应准确性和一致性的影响。我们的研究结果表明，虽然这些 LLM 获得了可接受的整体性能，但它们的结果因提示和问题领域的不同而存在很大差异。采用 CoT-SC 提示的 GPT-4o 表现优于其他方法，而采用 ZS 的 Gemini 1.5 Pro 记录了最高的一致性。对于 GPT-4o 和 Claude 3.5，CoT 提高了准确性，而 CoT-SC 提高了准确性和一致性。RAP 对 GPT-4o 回答专家级问题特别有效。因此，选择适合熟练程度和特定领域的适当 LLM 和提示技术可以减少饮食和营养聊天机器人中的错误和潜在风险。</li>
</ul>

<h3>Title: EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and Quantization</h3>
<ul>
<li><strong>Authors: </strong>Zhaopeng Feng, Zijie Meng, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02970">https://arxiv.org/abs/2408.02970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02970">https://arxiv.org/pdf/2408.02970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02970]] EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and Quantization(https://arxiv.org/abs/2408.02970)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have attracted considerable attention in various fields for their cost-effective solutions to diverse challenges, especially with advancements in instruction tuning and quantization. E-commerce, with its complex tasks and extensive product-user interactions, presents a promising application area for LLMs. However, the domain-specific concepts and knowledge inherent in e-commerce pose significant challenges for adapting general LLMs. To address this issue, we developed EC-Guide \href{this https URL}, a comprehensive e-commerce guide for instruction tuning and quantization of LLMs. We also heuristically integrated Chain-of-Thought (CoT) during inference to enhance arithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th place in Track 5 at the Amazon KDD Cup'24 \href{this https URL}. Additionally, our solution is model-agnostic, enabling effective scalability across larger systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其能够以经济高效的方式解决各种挑战而受到各个领域的广泛关注，尤其是在指令调整和量化方面取得的进步。电子商务具有复杂的任务和广泛的产品与用户交互，为 LLM 提供了一个有前途的应用领域。然而，电子商务固有的领域特定概念和知识对适应一般的 LLM 构成了重大挑战。为了解决这个问题，我们开发了 EC-Guide \href{this https URL}，这是一本全面的电子商务指南，用于 LLM 的指令调整和量化。我们还在推理过程中启发式地集成了思想链 (CoT) 以提高算术性能。我们的方法在 Amazon KDD Cup'24 \href{this https URL} 中获得了 Track 2 的第二名和 Track 5 的第 5 名。此外，我们的解决方案与模型无关，可在更大的系统中实现有效的可扩展性。</li>
</ul>

<h3>Title: Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation</h3>
<ul>
<li><strong>Authors: </strong>Hui Ma, Bo Zhang, Bo Xu, Jian Wang, Hongfei Lin, Xiao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.02976">https://arxiv.org/abs/2408.02976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.02976">https://arxiv.org/pdf/2408.02976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.02976]] Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation(https://arxiv.org/abs/2408.02976)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in the context, an empathy reward function containing three empathy communication mechanisms, i.e., emotional reaction, interpretation, and exploration, is constructed using pre-designed and pre-trained empathy identifiers. Finally, the proximal policy optimization algorithm is used to further train the policy to produce empathetic responses. Both automatic and manual evaluations demonstrate that the proposed EmpRL framework can improve the quality of generated responses, enhance the empathy level similarity between generated and target responses, and produce empathetic responses covering both affective and cognitive aspects.</li>
<li><strong>摘要：</strong>共情回应生成旨在理解用户的处境和感受并做出共情回应，对于构建类人对话系统至关重要。以前的方法主要集中于使用最大似然估计作为训练回应生成模型的优化目标，而没有考虑生成的回应和目标回应之间的共情水平对齐。为此，我们提出了一种使用强化学习 (EmpRL) 框架的共情回应生成。该框架设计了一个有效的共情奖励函数，并通过强化学习最大化预期奖励来生成共情回应。鉴于预训练语言模型强大的文本生成能力，EmpRL 利用预训练的 T5 模型作为生成器并进行进一步训练以初始化策略。为了使生成的回应和上下文中的目标回应之间的共情水平对齐，使用预先设计和预训练的共情标识符构建了一个共情奖励函数，包含三种共情沟通机制，即情绪反应、解释和探索。最后，使用近端策略优化算法进一步训练策略以产生共情响应。自动和手动评估均表明，所提出的 EmpRL 框架可以提高生成响应的质量，增强生成响应和目标响应之间的共情水平相似性，并产生涵盖情感和认知方面的共情响应。</li>
</ul>

<h3>Title: Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Daniel Steinigen, Roman Teucher, Timm Heine Ruland, Max Rudat, Nicolas Flores-Herr, Peter Fischer, Nikola Milosevic, Christopher Schymura, Angelo Ziletti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03010">https://arxiv.org/abs/2408.03010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03010">https://arxiv.org/pdf/2408.03010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03010]] Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs(https://arxiv.org/abs/2408.03010)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have showcased their proficiency in answering natural language queries. However, their effectiveness is hindered by limited domain-specific knowledge, raising concerns about the reliability of their responses. We introduce a hybrid system that augments LLMs with domain-specific knowledge graphs (KGs), thereby aiming to enhance factual correctness using a KG-based retrieval approach. We focus on a medical KG to demonstrate our methodology, which includes (1) pre-processing, (2) Cypher query generation, (3) Cypher query processing, (4) KG retrieval, and (5) LLM-enhanced response generation. We evaluate our system on a curated dataset of 69 samples, achieving a precision of 78\% in retrieving correct KG nodes. Our findings indicate that the hybrid system surpasses a standalone LLM in accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method. This positions the system as a promising tool for applications that demand factual correctness and completeness, such as target identification -- a critical process in pinpointing biological entities for disease treatment or crop enhancement. Moreover, its intuitive search interface and ability to provide accurate responses within seconds make it well-suited for time-sensitive, precision-focused research contexts. We publish the source code together with the dataset and the prompt templates used.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展展示了其在回答自然语言查询方面的能力。然而，它们的有效性受到有限领域特定知识的阻碍，引发了人们对其响应可靠性的担忧。我们引入了一个混合系统，该系统使用领域特定知识图谱 (KG) 增强 LLM，从而旨在使用基于 KG 的检索方法提高事实正确性。我们专注于医学 KG 来展示我们的方法，其中包括 (1) 预处理、(2) Cypher 查询生成、(3) Cypher 查询处理、(4) KG 检索和 (5) LLM 增强的响应生成。我们在 69 个样本的精选数据集上对我们的系统进行了评估，在检索正确的 KG 节点方面实现了 78\% 的精度。我们的研究结果表明，混合系统在准确性和完整性方面超越了独立的 LLM，这已通过 LLM-as-a-Judge 评估方法得到验证。这使得该系统成为一种有前途的工具，适用于需要事实正确性和完整性的应用，例如目标识别——这是确定生物实体以治疗疾病或改良作物的关键过程。此外，其直观的搜索界面和在几秒钟内提供准确响应的能力使其非常适合时间敏感、注重精确度的研究环境。我们将源代码与数据集和使用的提示模板一起发布。</li>
</ul>

<h3>Title: L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization</h3>
<ul>
<li><strong>Authors: </strong>Elvys Linhares Pontes, Carlos-Emiliano González-Gallardo, Mohamed Benjannet, Caryn Qu, Antoine Doucet</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03033">https://arxiv.org/abs/2408.03033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03033">https://arxiv.org/pdf/2408.03033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03033]] L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization(https://arxiv.org/abs/2408.03033)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This article details our participation (L3iTC) in the FinLLM Challenge Task 2024, focusing on two key areas: Task 1, financial text classification, and Task 2, financial text summarization. To address these challenges, we fine-tuned several large language models (LLMs) to optimize performance for each task. Specifically, we used 4-bit quantization and LoRA to determine which layers of the LLMs should be trained at a lower precision. This approach not only accelerated the fine-tuning process on the training data provided by the organizers but also enabled us to run the models on low GPU memory. Our fine-tuned models achieved third place for the financial classification task with an F1-score of 0.7543 and secured sixth place in the financial summarization task on the official test datasets.</li>
<li><strong>摘要：</strong>本文详细介绍了我们 (L3iTC) 参加 FinLLM 挑战赛 2024 的情况，重点关注两个关键领域：任务 1，金融文本分类，以及任务 2，金融文本摘要。为了应对这些挑战，我们对几个大型语言模型 (LLM) 进行了微调，以优化每个任务的性能。具体来说，我们使用 4 位量化和 LoRA 来确定哪些 LLM 层应该以较低的精度进行训练。这种方法不仅加速了组织者提供的训练数据的微调过程，还使我们能够在低 GPU 内存上运行模型。我们的微调模型在官方测试数据集上以 F1 得分 0.7543 在金融分类任务中取得第三名，并在金融摘要任务中取得第六名。</li>
</ul>

<h3>Title: Analysis of Argument Structure Constructions in a Deep Recurrent Language Model</h3>
<ul>
<li><strong>Authors: </strong>Pegah Ramezani, Achim Schilling, Patrick Krauss</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03062">https://arxiv.org/abs/2408.03062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03062">https://arxiv.org/pdf/2408.03062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03062]] Analysis of Argument Structure Constructions in a Deep Recurrent Language Model(https://arxiv.org/abs/2408.03062)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Understanding how language and linguistic constructions are processed in the brain is a fundamental question in cognitive computational neuroscience. In this study, we explore the representation and processing of Argument Structure Constructions (ASCs) in a recurrent neural language model. We trained a Long Short-Term Memory (LSTM) network on a custom-made dataset consisting of 2000 sentences, generated using GPT-4, representing four distinct ASCs: transitive, ditransitive, caused-motion, and resultative constructions. We analyzed the internal activations of the LSTM model's hidden layers using Multidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the sentence representations. The Generalized Discrimination Value (GDV) was calculated to quantify the degree of clustering within these representations. Our results show that sentence representations form distinct clusters corresponding to the four ASCs across all hidden layers, with the most pronounced clustering observed in the last hidden layer before the output layer. This indicates that even a relatively simple, brain-constrained recurrent neural network can effectively differentiate between various construction types. These findings are consistent with previous studies demonstrating the emergence of word class and syntax rule representations in recurrent language models trained on next word prediction tasks. In future work, we aim to validate these results using larger language models and compare them with neuroimaging data obtained during continuous speech perception. This study highlights the potential of recurrent neural language models to mirror linguistic processing in the human brain, providing valuable insights into the computational and neural mechanisms underlying language understanding.</li>
<li><strong>摘要：</strong>了解大脑如何处理语言和语言结构是认知计算神经科学的一个基本问题。在本研究中，我们探索了循环神经语言模型中论元结构构造 (ASC) 的表示和处理。我们在一个由 2000 个句子组成的定制数据集上训练了一个长短期记忆 (LSTM) 网络，这些句子使用 GPT-4 生成，代表四种不同的 ASC：及物动词、双及物动词、引起运动和结果结构。我们使用多维缩放 (MDS) 和 t 分布随机邻域嵌入 (t-SNE) 分析了 LSTM 模型隐藏层的内部激活，以可视化句子表示。计算了广义判别值 (GDV) 以量化这些表示内的聚类程度。我们的结果表明，句子表示在所有隐藏层中形成与四个 ASC 相对应的不同聚类，其中最明显的聚类出现在输出层之前的最后一个隐藏层中。这表明，即使是相对简单的、受大脑约束的循环神经网络也能有效区分各种构造类型。这些发现与之前的研究一致，这些研究证明了在下一个单词预测任务上训练的循环语言模型中出现了词类和语法规则表示。在未来的工作中，我们的目标是使用更大的语言模型来验证这些结果，并将它们与连续语音感知过程中获得的神经成像数据进行比较。这项研究强调了循环神经语言模型在反映人类大脑语言处理方面的潜力，为语言理解背后的计算和神经机制提供了宝贵的见解。</li>
</ul>

<h3>Title: Probing structural constraints of negation in Pretrained Language Models</h3>
<ul>
<li><strong>Authors: </strong>David Kletz (Lattice, LLF - UMR7110, UPCité), Marie Candito (UPCité, LLF - UMR7110), Pascal Amsili (Lattice)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03070">https://arxiv.org/abs/2408.03070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03070">https://arxiv.org/pdf/2408.03070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03070]] Probing structural constraints of negation in Pretrained Language Models(https://arxiv.org/abs/2408.03070)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Contradictory results about the encoding of the semantic impact of negation in pretrained language models (PLMs). have been drawn recently (e.g. Kassner and Sch{ü}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus rather on the way PLMs encode negation and its formal impact, through the phenomenon of the Negative Polarity Item (NPI) licensing in English. More precisely, we use probes to identify which contextual representations best encode 1) the presence of negation in a sentence, and 2) the polarity of a neighboring masked polarity item. We find that contextual representations of tokens inside the negation scope do allow for (i) a better prediction of the presence of not compared to those outside the scope and (ii) a better prediction of the right polarity of a masked polarity item licensed by not, although the magnitude of the difference varies from PLM to PLM. Importantly, in both cases the trend holds even when controlling for distance to not. This tends to indicate that the embeddings of these models do reflect the notion of negation scope, and do encode the impact of negation on NPI licensing. Yet, further control experiments reveal that the presence of other lexical items is also better captured when using the contextual representation of a token within the same syntactic clause than outside from it, suggesting that PLMs simply capture the more general notion of syntactic clause.</li>
<li><strong>摘要：</strong>最近，关于预训练语言模型 (PLM) 中否定语义影响的编码得出了相互矛盾的结果（例如 Kassner 和 Sch{ü}tze (2020)；Gubelmann 和 Handschuh (2022)）。在本文中，我们更关注 PLM 编码否定及其形式影响的方式，通过英语中的负极性项目 (NPI) 许可现象。更准确地说，我们使用探测器来确定哪些上下文表示最能编码 1) 句子中否定的存在，以及 2) 相邻掩码极性项目的极性。我们发现否定范围内的标记的上下文表示确实允许 (i) 与范围外的标记相比更好地预测 not 的存在，以及 (ii) 更好地预测 not 许可的掩码极性项目的正确极性，尽管不同 PLM 之间的差异幅度不同。重要的是，在这两种情况下，即使控制距离，这种趋势仍然成立。这往往表明这些模型的嵌入确实反映了否定范围的概念，并编码了否定对 NPI 许可的影响。然而，进一步的控制实验表明，当在同一句法子句中使用 token 的上下文表示时，其他词汇项的存在也比在其外部使用时更好，这表明 PLM 只是捕捉了更一般的句法子句概念。</li>
</ul>

<h3>Title: Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amelie Robrecht, Judith Sieker, Clara Lachenmaier, Sina Zarieß, Stefan Kopp</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03074">https://arxiv.org/abs/2408.03074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03074">https://arxiv.org/pdf/2408.03074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03074]] Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models(https://arxiv.org/abs/2408.03074)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this work, we want to give an overview on which pragmatic abilities have been tested in LLMs so far and how these tests have been carried out. To do this, we first discuss the scope of the field of pragmatics and suggest a subdivision into discourse pragmatics and interactional pragmatics. We give a non-exhaustive overview of the phenomena of those two subdomains and the methods traditionally used to analyze them. We subsequently consider the resulting heterogeneous set of phenomena and methods as a starting point for our survey of work on discourse pragmatics and interactional pragmatics in the context of LLMs.</li>
<li><strong>摘要：</strong>在本文中，我们想概述迄今为止法学硕士课程中测试了哪些语用能力以及这些测试是如何进行的。为此，我们首先讨论语用学领域的范围，并建议将其细分为话语语用学和互动语用学。我们对这两个子领域的现象以及传统上用于分析它们的方法进行了非详尽的概述。随后，我们将由此产生的异质现象和方法集视为我们在法学硕士背景下对话语语用学和互动语用学工作的调查的起点。</li>
</ul>

<h3>Title: Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jinglong Gao, Chen Lu, Xiao Ding, Zhongyang Li, Ting Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03079">https://arxiv.org/abs/2408.03079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03079">https://arxiv.org/pdf/2408.03079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03079]] Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion(https://arxiv.org/abs/2408.03079)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Event Causality Extraction (ECE) aims at extracting causal event pairs from texts. Despite ChatGPT's recent success, fine-tuning small models remains the best approach for the ECE task. However, existing fine-tuning based ECE methods cannot address all three key challenges in ECE simultaneously: 1) Complex Causality Extraction, where multiple causal-effect pairs occur within a single sentence; 2) Subtask~ Interaction, which involves modeling the mutual dependence between the two subtasks of ECE, i.e., extracting events and identifying the causal relationship between extracted events; and 3) Knowledge Fusion, which requires effectively fusing the knowledge in two modalities, i.e., the expressive pretrained language models and the structured knowledge graphs. In this paper, we propose a unified ECE framework (UniCE to address all three issues in ECE simultaneously. Specifically, we design a subtask interaction mechanism to enable mutual interaction between the two ECE subtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in the two modalities. Furthermore, we employ separate decoders for each subtask to facilitate complex causality extraction. Experiments on three benchmark datasets demonstrate that our method achieves state-of-the-art performance and outperforms ChatGPT with a margin of at least 30% F1-score. More importantly, our model can also be used to effectively improve the ECE performance of ChatGPT via in-context learning.</li>
<li><strong>摘要：</strong>事件因果关系提取 (ECE) 旨在从文本中提取因果事件对。尽管 ChatGPT 最近取得了成功，但微调小模型仍然是 ECE 任务的最佳方法。然而，现有的基于微调的 ECE 方法无法同时解决 ECE 中的所有三个关键挑战：1) 复杂因果关系提取，其中多个因果对出现在一个句子中；2) 子任务交互，涉及对 ECE 的两个子任务之间的相互依赖关系进行建模，即提取事件并识别提取事件之间的因果关系；3) 知识融合，需要有效地融合两种模态中的知识，即表达性预训练语言模型和结构化知识图谱。在本文中，我们提出了一个统一的 ECE 框架 (UniCE) 来同时解决 ECE 中的所有三个问题。具体而言，我们设计了一个子任务交互机制，以实现两个 ECE 子任务之间的相互作用。此外，我们设计了一种知识融合机制来融合两种模态中的知识。此外，我们为每个子任务使用单独的解码器以促进复杂的因果关系提取。在三个基准数据集上的实验表明，我们的方法达到了最先进的性能，并且比 ChatGPT 的表现好至少 30% 的 F1 分数。更重要的是，我们的模型还可以用于通过上下文学习有效提高 ChatGPT 的 ECE 性能。</li>
</ul>

<h3>Title: Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03092">https://arxiv.org/abs/2408.03092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03092">https://arxiv.org/pdf/2408.03092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03092]] Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement(https://arxiv.org/abs/2408.03092)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor parameter changes or Pre-Trained (PT) with substantial parameter shifts. However, existing methods often manually assign the model importance, rendering them feasible only for LLMs with similar parameter alterations, such as multiple FT LLMs. The diverse parameter changed ranges between FT and PT LLMs pose challenges for current solutions in empirically determining the optimal combination. In this paper, we make a pioneering effort to broaden the applicability of merging techniques from FT to PT LLMs. We initially examine the efficacy of current methods in merging FT and PT LLMs, discovering that they struggle to deal with PT LLMs. Subsequently, we introduce an approach based on WeIght DisENtanglement (WIDEN) to effectively extend the merging scope, which first disentangles model weights into magnitude and direction components, and then performs adaptive fusion by considering their respective contributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with instruction-following skills) with Sailor (a PT LLM with multilingual abilities) across 7B and 14B model scales. Results reveal that: (1) existing solutions usually fail when merging Sailor, either losing both abilities or only retaining instruction-following skills; (2) WIDEN successfully injects the multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in Southeast Asian languages, achieving enhancements in the fundamental capabilities. In light of previous research, we also merge multiple 13B FT LLMs and observe that WIDEN achieves a balanced amalgamation of instruction following, mathematical reasoning, and code generation skills.</li>
<li><strong>摘要：</strong>合并大型语言模型 (LLM) 旨在将多个同源 LLM 合并为一个具有所有功能的 LLM。理想情况下，任何共享相同主干的 LLM 都应该是可合并的，无论它们是具有较小参数变化的微调 (FT) 还是具有较大参数变化的预训练 (PT)。然而，现有方法通常手动分配模型重要性，使得它们仅适用于具有类似参数更改的 LLM，例如多个 FT LLM。FT 和 PT LLM 之间不同的参数变化范围对当前解决方案在经验上确定最佳组合方面提出了挑战。在本文中，我们做出了开创性的努力，以扩大合并技术从 FT 到 PT LLM 的适用性。我们首先检查了当前方法在合并 FT 和 PT LLM 方面的有效性，发现它们很难处理 PT LLM。随后，我们引入一种基于权重解缠（WIDEN）的方法来有效地扩展合并范围，该方法首先将模型权重解缠为幅度和方向分量，然后通过考虑它们各自的贡献进行自适应融合。在实验中，我们将 Qwen1.5-Chat（具有指令遵循技能的 FT LLM）与 Sailor（具有多语言能力的 PT LLM）在 7B 和 14B 模型规模上进行合并。结果表明：（1）现有解决方案在合并 Sailor 时通常会失败，要么失去两种能力，要么只保留指令遵循技能；（2）WIDEN 成功地将 Sailor 的多语言能力注入 Qwen1.5-Chat，使其能够熟练使用东南亚语言，从而实现了基础能力的增强。根据前人的研究，我们还合并了多个 13B FT LLM，并观察到 ​​WIDEN 实现了指令遵循、数学推理和代码生成技能的平衡融合。</li>
</ul>

<h3>Title: 500xCompressor: Generalized Prompt Compression for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zongqian Li, Yixuan Su, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03094">https://arxiv.org/abs/2408.03094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03094">https://arxiv.org/pdf/2408.03094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03094]] 500xCompressor: Generalized Prompt Compression for Large Language Models(https://arxiv.org/abs/2408.03094)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompt compression is crucial for enhancing inference speed, reducing costs, and improving user experience. However, current methods face challenges such as low compression ratios and potential data leakage during evaluation. To address these issues, we propose 500xCompressor, a method that compresses extensive natural language contexts into a minimum of one single special token. The 500xCompressor introduces approximately 0.3% additional parameters and achieves compression ratios ranging from 6x to 480x. It is designed to compress any text, answer various types of questions, and could be utilized by the original large language model (LLM) without requiring fine-tuning. Initially, 500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on the ArxivQA dataset, and subsequently evaluated on strictly unseen and classical question answering (QA) datasets. The results demonstrate that the LLM retained 62.26-72.89% of its capabilities compared to using non-compressed prompts. This study also shows that not all the compressed tokens are equally utilized and that K V values have significant advantages over embeddings in preserving information at high compression ratios. The highly compressive nature of natural language prompts, even for fine-grained complex information, suggests promising potential for future applications and further research into developing a new LLM language.</li>
<li><strong>摘要：</strong>提示压缩对于提高推理速度、降低成本和改善用户体验至关重要。然而，当前的方法面临着诸如压缩率低和评估过程中可能的数据泄漏等挑战。为了解决这些问题，我们提出了 500xCompressor，这种方法将大量自然语言上下文压缩为至少一个特殊标记。500xCompressor 引入了大约 0.3% 的额外参数，并实现了 6 倍到 480 倍的压缩率。它旨在压缩任何文本，回答各种类型的问题，并且可以被原始大型语言模型 (LLM) 使用而无需微调。最初，500xCompressor 在 Arxiv Corpus 上进行预训练，然后在 ArxivQA 数据集上进行微调，随后在严格看不见的和经典的问答 (QA) 数据集上进行评估。结果表明，与使用未压缩的提示相比，LLM 保留了 62.26-72.89% 的功能。这项研究还表明，并非所有压缩标记都得到了同等利用，并且 KV 值在高压缩率下保存信息方面比嵌入具有显著优势。自然语言提示的高压缩性，即使对于细粒度的复杂信息，也表明未来应用和进一步研究开发新的 LLM 语言的潜力巨大。</li>
</ul>

<h3>Title: Topic Modeling with Fine-tuning LLMs and Bag of Sentences</h3>
<ul>
<li><strong>Authors: </strong>Johannes Schneider</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03099">https://arxiv.org/abs/2408.03099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03099">https://arxiv.org/pdf/2408.03099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03099]] Topic Modeling with Fine-tuning LLMs and Bag of Sentences(https://arxiv.org/abs/2408.03099)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLM)'s are increasingly used for topic modeling outperforming classical topic models such as LDA. Commonly, pre-trained LLM encoders such as BERT are used out-of-the-box despite the fact that fine-tuning is known to improve LLMs considerably. The challenge lies in obtaining a suitable (labeled) dataset for fine-tuning. In this paper, we use the recent idea to use bag of sentences as the elementary unit in computing topics. In turn, we derive an approach FT-Topic to perform unsupervised fine-tuning relying primarily on two steps for constructing a training dataset in an automatic fashion. First, a heuristic method to identifies pairs of sentence groups that are either assumed to be of the same or different topics. Second, we remove sentence pairs that are likely labeled incorrectly. The dataset is then used to fine-tune an encoder LLM, which can be leveraged by any topic modeling approach using embeddings. However, in this work, we demonstrate its effectiveness by deriving a novel state-of-the-art topic modeling method called SenClu, which achieves fast inference through an expectation-maximization algorithm and hard assignments of sentence groups to a single topic, while giving users the possibility to encode prior knowledge on the topic-document distribution. Code is at \url{this https URL}</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于主题建模，其表现优于 LDA 等传统主题模型。通常，尽管众所周知微调可以显著改善 LLM，但预训练的 LLM 编码器（如 BERT）是开箱即用的。挑战在于获得适合微调的（标记）数据集。在本文中，我们使用最近的想法，使用句袋作为计算主题的基本单位。反过来，我们推导出一种 FT-Topic 方法，用于执行无监督微调，主要依靠两个步骤以自动方式构建训练数据集。首先，使用启发式方法来识别被认为是相同或不同主题的句子组对。其次，我们删除可能标记错误的句子对。然后使用该数据集来微调编码器 LLM，任何使用嵌入的主题建模方法都可以利用该编码器。然而，在这项工作中，我们通过推导一种名为 SenClu 的新颖的最先进的主题建模方法来证明其有效性，该方法通过期望最大化算法和将句子组硬分配到单个主题来实现快速推理，同时让用户能够对主题文档分布的先验知识进行编码。代码位于 \url{此 https URL}</li>
</ul>

<h3>Title: Evaluating the Translation Performance of Large Language Models Based on Euas-20</h3>
<ul>
<li><strong>Authors: </strong>Yan Huang, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03119">https://arxiv.org/abs/2408.03119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03119">https://arxiv.org/pdf/2408.03119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03119]] Evaluating the Translation Performance of Large Language Models Based on Euas-20(https://arxiv.org/abs/2408.03119)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.</li>
<li><strong>摘要：</strong>近年来，随着深度学习技术的快速发展，BERT、GPT 等大型语言模型（LLM）在自然语言处理任务上取得了突破性成果。机器翻译（MT）作为自然语言处理的核心任务之一，也受益于大型语言模型的发展，实现了质的飞跃。尽管大型语言模型在翻译性能上取得了显著进步，但机器翻译仍然面临诸多挑战。因此，本文构建了数据集 Euas-20，用于评估大型语言模型在翻译任务上的表现、在不同语言上的翻译能力以及预训练数据对 LLM 翻译能力的影响，供研究者和开发人员参考。</li>
</ul>

<h3>Title: Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Artur Guimarães, Bruno Martins, João Magalhães</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03127">https://arxiv.org/abs/2408.03127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03127">https://arxiv.org/pdf/2408.03127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03127]] Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation(https://arxiv.org/abs/2408.03127)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper describes our approach to the SemEval-2024 safe biomedical Natural Language Inference for Clinical Trials (NLI4CT) task, which concerns classifying statements about Clinical Trial Reports (CTRs). We explored the capabilities of Mistral-7B, a generalist open-source Large Language Model (LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized version of the model using an augmented version of the training dataset. The experimental results show that this approach can produce notable results in terms of the macro F1-score, while having limitations in terms of faithfulness and consistency. All the developed code is publicly available on a GitHub repository</li>
<li><strong>摘要：</strong>本文介绍了我们针对 SemEval-2024 安全生物医学临床试验自然语言推理 (NLI4CT) 任务的方法，该任务涉及对临床试验报告 (CTR) 的陈述进行分类。我们探索了 Mistral-7B 的功能，这是一种通用开源大型语言模型 (LLM)。我们为 NLI4CT 任务开发了一个提示，并使用训练数据集的增强版本对该模型的量化版本进行了微调。实验结果表明，这种方法可以在宏观 F1 分数方面产生显著的结果，但在忠实度和一致性方面存在局限性。所有开发的代码都可以在 GitHub 存储库中公开获取</li>
</ul>

<h3>Title: Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations</h3>
<ul>
<li><strong>Authors: </strong>Leo Donisch, Sigurd Schacht, Carsten Lanquillon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03130">https://arxiv.org/abs/2408.03130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03130">https://arxiv.org/pdf/2408.03130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03130]] Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations(https://arxiv.org/abs/2408.03130)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large language models are ubiquitous in natural language processing because they can adapt to new tasks without retraining. However, their sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.</li>
<li><strong>摘要：</strong>大型语言模型在自然语言处理中无处不在，因为它们无需重新训练即可适应新任务。然而，它们的规模和复杂性带来了独特的挑战和机遇，促使研究人员和从业者探索新的模型训练、优化和部署方法。本文献综述重点介绍了减少资源需求和压缩大型语言模型的各种技术，包括量化、修剪、知识提炼和架构优化。主要目标是深入探索每种方法，并强调其独特的挑战和实际应用。所讨论的方法被归类为一个分类法，该分类法概述了优化前景并帮助导航它以更好地了解研究轨迹。</li>
</ul>

<h3>Title: Conditioning LLMs with Emotion in Neural Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Charles Brazier, Jean-Luc Rouas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03150">https://arxiv.org/abs/2408.03150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03150">https://arxiv.org/pdf/2408.03150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03150]] Conditioning LLMs with Emotion in Neural Machine Translation(https://arxiv.org/abs/2408.03150)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable performance in Natural Language Processing tasks, including Machine Translation (MT). In this work, we propose a novel MT pipeline that integrates emotion information extracted from a Speech Emotion Recognition (SER) model into LLMs to enhance translation quality. We first fine-tune five existing LLMs on the Libri-trans dataset and select the most performant model. Subsequently, we augment LLM prompts with different dimensional emotions and train the selected LLM under these different configurations. Our experiments reveal that integrating emotion information, especially arousal, into LLM prompts leads to notable improvements in translation quality.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言处理任务（包括机器翻译 (MT)）中表现出色。在这项工作中，我们提出了一种新颖的 MT 流程，将从语音情感识别 (SER) 模型中提取的情感信息集成到 LLM 中，以提高翻译质量。我们首先在 Libri-trans 数据集上微调五个现有 LLM，并选择性能最佳的模型。随后，我们用不同维度的情感增强 LLM 提示，并在这些不同配置下训练选定的 LLM。我们的实验表明，将情感信息（尤其是唤醒）集成到 LLM 提示中可以显着提高翻译质量。</li>
</ul>

<h3>Title: Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi</h3>
<ul>
<li><strong>Authors: </strong>Pranita Deshmukh, Nikita Kulkarni, Sanhita Kulkarni, Kareena Manghani, Raviraj Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03172">https://arxiv.org/abs/2408.03172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03172">https://arxiv.org/pdf/2408.03172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03172]] Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi(https://arxiv.org/abs/2408.03172)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the surge in digital content in low-resource languages, there is an escalating demand for advanced Natural Language Processing (NLP) techniques tailored to these languages. BERT (Bidirectional Encoder Representations from Transformers), serving as the foundational framework for numerous NLP architectures and language models, is increasingly employed for the development of low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method for fine-tuning Large Language Models (LLMs) and reducing the training parameters to some extent to decrease the computational costs needed for training the model and achieve results comparable to a fully fine-tuned model. In this work, we present a study of PEFT methods for the Indic low-resource language Marathi. We conduct a comprehensive analysis of PEFT methods applied to various monolingual and multilingual Marathi BERT models. These approaches are evaluated on prominent text classification datasets like MahaSent, MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to significantly expedite the training speed of the models, addressing a critical aspect of model development and deployment. In this study, we explore Low-Rank Adaptation of Large Language Models (LoRA) and adapter methods for low-resource text classification. We show that these methods are competitive with full fine-tuning and can be used without loss in accuracy. This study contributes valuable insights into the effectiveness of Marathi BERT models, offering a foundation for the continued advancement of NLP capabilities in Marathi and similar Indic languages.</li>
<li><strong>摘要：</strong>随着低资源语言数字内容的激增，对针对这些语言定制的高级自然语言处理 (NLP) 技术的需求不断增长。BERT（来自 Transformer 的双向编码器表示）作为众多 NLP 架构和语言模型的基础框架，越来越多地用于开发低资源 NLP 模型。参数高效微调 (PEFT) 是一种微调大型语言模型 (LLM) 并在一定程度上减少训练参数的方法，以降低训练模型所需的计算成本并获得与完全微调模型相当的结果。在这项工作中，我们介绍了针对印度低资源语言马拉地语的 PEFT 方法的研究。我们对应用于各种单语和多语马拉地语 BERT 模型的 PEFT 方法进行了全面分析。这些方法在 MahaSent、MahaHate 和 MahaNews 等著名文本分类数据集上进行了评估。事实证明，PEFT 技术的结合可以显著加快模型的训练速度，解决模型开发和部署的一个关键方面。在本研究中，我们探索了大型语言模型的低秩自适应 (LoRA) 和用于低资源文本分类的适配器方法。我们表明，这些方法与完全微调相比具有竞争力，并且可以在不损失准确性的情况下使用。这项研究为 Marathi BERT 模型的有效性提供了宝贵的见解，为 Marathi 和类似印度语的 NLP 能力的持续发展奠定了基础。</li>
</ul>

<h3>Title: Making Long-Context Language Models Better Multi-Hop Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Yanyang Li, Shuo Liang, Michael R. Lyu, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03246">https://arxiv.org/abs/2408.03246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03246">https://arxiv.org/pdf/2408.03246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03246]] Making Long-Context Language Models Better Multi-Hop Reasoners(https://arxiv.org/abs/2408.03246)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts. In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning. We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience. Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy. Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant.</li>
<li><strong>摘要：</strong>长上下文建模的最新进展增强了跨多个 NLP 应用程序的复杂任务的语言模型 (LM)。尽管取得了这些进展，但我们发现这些模型在多跳推理方面仍存在困难，并且在存在嘈杂上下文的情况下性能会下降。在本文中，我们介绍了一种新颖的方法，即使用归因推理，该方法提示 LM 在推理过程中为每个断言提供归因。我们通过在三个多跳数据集上进行实验来验证我们的方法，采用专有和开源模型，并证明其有效性和弹性。此外，我们探索了通过微调增强推理能力的方法，并提供了归因注释的数据集和专门的训练策略。我们的微调模型在多跳推理基准上实现了具有竞争力的性能，与 ChatGPT 和 Claude-instant 等专有 LM 非常相似。</li>
</ul>

<h3>Title: Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03247">https://arxiv.org/abs/2408.03247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03247">https://arxiv.org/pdf/2408.03247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03247]] Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons(https://arxiv.org/abs/2408.03247)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.</li>
<li><strong>摘要：</strong>在本文中，我们研究大型语言模型 (LLM) 在面对推理任务时是否会主动回忆或检索其内部事实知识库。通过知识神经元分析 LLM 在每个推理步骤中的内部事实回忆，我们发现 LLM 在某些情况下无法利用关键事实关联。相反，他们倾向于选择替代的、类似捷径的途径来回答推理问题。通过手动操纵 LLM 中的参数知识回忆过程，我们证明增强此回忆过程可直接提高推理性能，而抑制它会导致显著下降。此外，我们评估了思路链 (CoT) 提示的效果，这是一种解决复杂推理任务的强大技术。我们的研究结果表明，CoT 可以通过鼓励 LLM 进行有序和可靠的推理来加强事实知识的回忆。此外，我们探讨了上下文冲突如何影响推理过程中的事实检索，以全面了解 LLM 的事实回忆行为。代码和数据将很快提供。</li>
</ul>

<h3>Title: Synthesizing Text-to-SQL Data from Weak and Strong LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03256">https://arxiv.org/abs/2408.03256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03256">https://arxiv.org/pdf/2408.03256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03256]] Synthesizing Text-to-SQL Data from Weak and Strong LLMs(https://arxiv.org/abs/2408.03256)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.</li>
<li><strong>摘要：</strong>开源和闭源大型语言模型 (LLM) 之间的能力差距仍然是文本到 SQL 任务中的挑战。在本文中，我们介绍了一种合成数据方法，该方法将更大、更强大的模型（强模型）生成的数据与较小、对齐不佳的模型（弱模型）生成的错误信息数据相结合。该方法不仅增强了文本到 SQL 模型的领域泛化能力，而且还探索了通过偏好学习进行错误数据监督的潜力。此外，我们采用合成数据方法对开源 LLM 进行指令调整，从而产生了专门的文本到 SQL 模型 SENSE。通过 SPIDER 和 BIRD 基准测试的最新结果证明了 SENSE 的有效性，从而弥合了开源模型与闭源模型所提示的方法之间的性能差距。</li>
</ul>

<h3>Title: StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng Zhan, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03281">https://arxiv.org/abs/2408.03281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03281">https://arxiv.org/pdf/2408.03281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03281]] StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation(https://arxiv.org/abs/2408.03281)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities. Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols.</li>
<li><strong>摘要：</strong>评估是大型语言模型开发的接力棒。目前的评估通常对每个原子测试目标采用单项评估范式，难以判断模型是否真正具备所需的能力，还是仅仅记住/猜测特定问题的答案。为此，我们提出了一个称为 StructEval 的新型评估框架。StructEval 从原子测试目标出发，通过跨多个认知水平和关键概念进行结构化评估，深化和拓宽了评估范围，从而为 LLM 提供了全面、稳健和一致的评估。在三个广泛使用的基准测试上的实验表明，StructEval 是一种可靠的工具，可以抵抗数据污染风险并减少潜在偏差的干扰，从而提供有关模型能力的更可靠和一致的结论。我们的框架还为未来设计原则性和可信赖的 LLM 评估协议提供了启示。</li>
</ul>

<h3>Title: KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03297">https://arxiv.org/abs/2408.03297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03297">https://arxiv.org/pdf/2408.03297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03297]] KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models(https://arxiv.org/abs/2408.03297)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios. To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in real retrieval scenarios. Concretely, we explore and simulate error types across diverse context combinations and learn how to avoid these negative signals through preference optimization methods. Simultaneously, by adjusting the balance between response length and the proportion of preference data representing different behavior patterns, we enhance the adherence capabilities and noise robustness of LLMs in a balanced manner. Experimental results show that KaPO outperforms previous methods for handling knowledge conflicts by over 37%, while also exhibiting robust generalization across various out-of-distribution datasets.</li>
<li><strong>摘要：</strong>通过整合外部知识，检索增强生成 (RAG) 已成为缓解大型语言模型 (LLM) 在处理知识密集型任务时遇到的幻觉问题的有效策略。然而，在将外部非参数支持证据与内部参数知识相结合的过程中，不可避免地会出现知识冲突，导致模型响应混乱。为了增强 LLM 在各种情况下的知识选择，一些研究专注于通过指令调整来细化其行为模式。尽管如此，由于缺乏明确的负面信号和比较目标，以这种方式微调的模型在复杂而现实的检索场景中仍可能表现出不良行为。为此，我们提出了一种知识感知偏好优化，称为 KaPO，旨在实现真实检索场景中的可控知识选择。具体而言，我们探索和模拟不同上下文组合中的错误类型，并学习如何通过偏好优化方法避免这些负面信号。同时，通过调整响应长度和代表不同行为模式的偏好数据比例之间的平衡，我们均衡地增强了LLM的遵循能力和噪声鲁棒性。实验结果表明，KaPO在处理知识冲突方面比以前的方法高出37%以上，同时在各种分布外数据集上也表现出强大的泛化能力。</li>
</ul>

<h3>Title: Training LLMs to Recognize Hedges in Spontaneous Narratives</h3>
<ul>
<li><strong>Authors: </strong>Amie J. Paige, Adil Soubki, John Murzaku, Owen Rambow, Susan E. Brennan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03319">https://arxiv.org/abs/2408.03319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03319">https://arxiv.org/pdf/2408.03319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03319]] Training LLMs to Recognize Hedges in Spontaneous Narratives(https://arxiv.org/abs/2408.03319)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or "fuzziness", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on the top performing approaches, we used an LLM-in-the-Loop approach to improve the gold standard coding, as well as to highlight cases in which hedges are ambiguous in linguistically interesting ways that will guide future research. This is the first step in our research program to train LLMs to interpret and generate collateral signals appropriately and meaningfully in conversation.</li>
<li><strong>摘要：</strong>模糊限制语允许说话者将话语标记为临时的，无论是表示非典型性或“模糊性”，表示对话语缺乏承诺，将陈述的责任归咎于他人，邀请合作伙伴提供意见，还是为了面子管理需要而软化批评性反馈。在这里，我们专注于实验参数化语料库中的模糊限制语，该语料库包含 63 个 Roadrunner 卡通叙述，由 21 位说话者根据记忆自发地为同时在场的收件人制作，并转录为文本（Galati 和 Brennan，2010 年）。我们创建了一个由人类编码员注释的模糊限制语的黄金标准（Roadrunner-Hedge 语料库），并比较了三种基于 LLM 的模糊限制语检测方法：微调 BERT，以及使用 GPT-4o 和 LLaMA-3 的零样本和少量样本提示。表现最佳的方法是微调 BERT 模型，其次是少量样本 GPT-4o。在对表现最佳的方法进行错误分析后，我们使用了 LLM-in-the-Loop 方法来改进黄金标准编码，并以语言上有趣的方式突出模糊限制语的案例，这将指导未来的研究。这是我们研究计划的第一步，旨在培训 LLM 在对话中恰当且有意义地解释和生成附带信号。</li>
</ul>

<h3>Title: CoverBench: A Challenging Benchmark for Complex Claim Verification</h3>
<ul>
<li><strong>Authors: </strong>Alon Jacovi, Moran Ambar, Eyal Ben-David, Uri Shaham, Amir Feder, Mor Geva, Dror Marcus, Avi Caciularu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03325">https://arxiv.org/abs/2408.03325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03325">https://arxiv.org/pdf/2408.03325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03325]] CoverBench: A Challenging Benchmark for Complex Claim Verification(https://arxiv.org/abs/2408.03325)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>There is a growing line of research on verifying the correctness of language models' outputs. At the same time, LMs are being used to tackle complex queries that require reasoning. We introduce CoverBench, a challenging benchmark focused on verifying LM outputs in complex reasoning settings. Datasets that can be used for this purpose are often designed for other complex reasoning tasks (e.g., QA) targeting specific use-cases (e.g., financial tables), requiring transformations, negative sampling and selection of hard examples to collect such a benchmark. CoverBench provides a diversified evaluation for complex claim verification in a variety of domains, types of reasoning, relatively long inputs, and a variety of standardizations, such as multiple representations for tables where available, and a consistent schema. We manually vet the data for quality to ensure low levels of label noise. Finally, we report a variety of competitive baseline results to show CoverBench is challenging and has very significant headroom. The data is available at this https URL .</li>
<li><strong>摘要：</strong>关于验证语言模型输出正确性的研究越来越多。与此同时，语言模型正被用于处理需要推理的复杂查询。我们推出了 CoverBench，这是一个具有挑战性的基准，专注于在复杂的推理环境中验证语言模型的输出。可用于此目的的数据集通常是为其他复杂的推理任务（例如 QA）设计的，这些任务针对特定用例（例如财务表），需要转换、负采样和选择困难示例来收集这样的基准。CoverBench 为各种领域、推理类型、相对较长的输入和各种标准化（例如表格的多种表示形式（如果可用）和一致的模式）中的复杂索赔验证提供了多样化的评估。我们手动审查数据的质量以确保标签噪声水平较低。最后，我们报告了各种有竞争力的基线结果，以表明 CoverBench 具有挑战性并且具有非常大的发展空间。数据可在此 https URL 上获取。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
