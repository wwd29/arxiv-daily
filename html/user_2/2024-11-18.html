<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-18</h1>
<h3>Title: Evaluating Gender Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Döll, Markus Döhring, Andreas Müller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09826">https://arxiv.org/abs/2411.09826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09826">https://arxiv.org/pdf/2411.09826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09826]] Evaluating Gender Bias in Large Language Models(https://arxiv.org/abs/2411.09826)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Gender bias in artificial intelligence has become an important issue, particularly in the context of language models used in communication-oriented applications. This study examines the extent to which Large Language Models (LLMs) exhibit gender bias in pronoun selection in occupational contexts. The analysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0 Pro using a self-generated dataset. The jobs considered include a range of occupations, from those with a significant male presence to those with a notable female concentration, as well as jobs with a relatively equal gender distribution. Three different sentence processing methods were used to assess potential gender bias: masked tokens, unmasked sentences, and sentence completion. In addition, the LLMs suggested names of individuals in specific occupations, which were then examined for gender distribution. The results show a positive correlation between the models' pronoun choices and the gender distribution present in U.S. labor force data. Female pronouns were more often associated with female-dominated occupations, while male pronouns were more often associated with male-dominated occupations. Sentence completion showed the strongest correlation with actual gender distribution, while name generation resulted in a more balanced 'politically correct' gender distribution, albeit with notable variations in predominantly male or female occupations. Overall, the prompting method had a greater impact on gender distribution than the model selection itself, highlighting the complexity of addressing gender bias in LLMs. The findings highlight the importance of prompting in gender mapping.</li>
<li><strong>摘要：</strong>人工智能中的性别偏见已成为一个重要问题，尤其是在面向通信的应用中使用的语言模型中。本研究考察了大型语言模型 (LLM) 在职业背景下的代词选择中表现出的性别偏见程度。该分析使用自生成的数据集评估了 GPT-4、GPT-4o、PaLM 2 Text Bison 和 Gemini 1.0 Pro 模型。考虑的工作包括一系列职业，从男性占主导地位的职业到女性集中度显著的职业，以及性别分布相对均衡的工作。使用了三种不同的句子处理方法来评估潜在的性别偏见：掩码标记、未掩码句子和句子完成。此外，LLM 建议了特定职业的个人姓名，然后检查其性别分布。结果显示，模型的代词选择与美国劳动力数据中的性别分布呈正相关。女性代词更常与女性占主导地位的职业相关，而男性代词更常与男性占主导地位的职业相关。句子完成与实际性别分布的相关性最强，而姓名生成则导致更平衡的“政治正确”性别分布，尽管以男性或女性为主的职业存在明显差异。总体而言，提示方法对性别分布的影响大于模型选择本身，这凸显了解决法学硕士性别偏见的复杂性。研究结果强调了提示在性别映射中的重要性。</li>
</ul>

<h3>Title: A Benchmark for Long-Form Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Pedram Hosseini, Jessica M. Sin, Bing Ren, Bryceton G. Thomas, Elnaz Nouri, Ali Farahanchi, Saeed Hassanpour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09834">https://arxiv.org/abs/2411.09834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09834">https://arxiv.org/pdf/2411.09834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09834]] A Benchmark for Long-Form Medical Question Answering(https://arxiv.org/abs/2411.09834)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>There is a lack of benchmarks for evaluating large language models (LLMs) in long-form medical question answering (QA). Most existing medical QA evaluation benchmarks focus on automatic metrics and multiple-choice questions. While valuable, these benchmarks fail to fully capture or assess the complexities of real-world clinical applications where LLMs are being deployed. Furthermore, existing studies on evaluating long-form answer generation in medical QA are primarily closed-source, lacking access to human medical expert annotations, which makes it difficult to reproduce results and enhance existing baselines. In this work, we introduce a new publicly available benchmark featuring real-world consumer medical questions with long-form answer evaluations annotated by medical doctors. We performed pairwise comparisons of responses from various open and closed-source medical and general-purpose LLMs based on criteria such as correctness, helpfulness, harmfulness, and bias. Additionally, we performed a comprehensive LLM-as-a-judge analysis to study the alignment between human judgments and LLMs. Our preliminary results highlight the strong potential of open LLMs in medical QA compared to leading closed models. Code & Data: this https URL</li>
<li><strong>摘要：</strong>目前缺乏用于评估长篇医学问答 (QA) 中的大型语言模型 (LLM) 的基准。大多数现有的医学 QA 评估基准都侧重于自动指标和多项选择题。虽然这些基准很有价值，但它们未能完全捕捉或评估部署 LLM 的实际临床应用的复杂性。此外，现有的评估医学 QA 中长篇答案生成的研究主要是闭源的，缺乏对人类医学专家注释的访问，这使得很难重现结果并增强现有基线。在这项工作中，我们引入了一个新的公开基准，其中包含现实世界的消费者医学问题和由医生注释的长篇答案评估。我们根据正确性、有用性、有害性和偏见等标准对来自各种开源和闭源医学和通用 LLM 的响应进行了成对比较。此外，我们还进行了全面的 LLM 作为判断分析，以研究人类判断与 LLM 之间的一致性。我们的初步结果突出了与领先的封闭模型相比，开放式 LLM 在医学 QA 中具有强大的潜力。代码和数据：此 https URL</li>
</ul>

<h3>Title: KULCQ: An Unsupervised Keyword-based Utterance Level Clustering Quality Metric</h3>
<ul>
<li><strong>Authors: </strong>Pranav Guruprasad, Negar Mokhberian, Nikhil Varghese, Chandra Khatri, Amol Kelkar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09853">https://arxiv.org/abs/2411.09853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09853">https://arxiv.org/pdf/2411.09853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09853]] KULCQ: An Unsupervised Keyword-based Utterance Level Clustering Quality Metric(https://arxiv.org/abs/2411.09853)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Intent discovery is crucial for both building new conversational agents and improving existing ones. While several approaches have been proposed for intent discovery, most rely on clustering to group similar utterances together. Traditional evaluation of these utterance clusters requires intent labels for each utterance, limiting scalability. Although some clustering quality metrics exist that do not require labeled data, they focus solely on cluster geometry while ignoring the linguistic nuances present in conversational transcripts. In this paper, we introduce Keyword-based Utterance Level Clustering Quality (KULCQ), an unsupervised metric that leverages keyword analysis to evaluate clustering quality. We demonstrate KULCQ's effectiveness by comparing it with existing unsupervised clustering metrics and validate its performance through comprehensive ablation studies. Our results show that KULCQ better captures semantic relationships in conversational data while maintaining consistency with geometric clustering principles.</li>
<li><strong>摘要：</strong>意图发现对于构建新的对话代理和改进现有的对话代理都至关重要。虽然已经提出了几种意图发现方法，但大多数方法都依赖于聚类将相似的话语归为一组。对这些话语聚类的传统评估需要为每个话语添加意图标签，这限制了可扩展性。尽管存在一些不需要标记数据的聚类质量指标，但它们仅关注聚类几何形状，而忽略了对话记录中存在的语言细微差别。在本文中，我们介绍了基于关键字的话语级别聚类质量 (KULCQ)，这是一种利用关键字分析来评估聚类质量的无监督指标。我们通过将 KULCQ 与现有的无监督聚类指标进行比较来证明其有效性，并通过全面的消融研究验证其性能。我们的结果表明，KULCQ 可以更好地捕捉对话数据中的语义关系，同时保持与几何聚类原理的一致性。</li>
</ul>

<h3>Title: Refined and Segmented Price Sentiment Indices from Survey Comments</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Suzuki, Hiroki Sakaji</a></li>
<li><strong>Subjects: </strong>cs.CL, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09937">https://arxiv.org/abs/2411.09937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09937">https://arxiv.org/pdf/2411.09937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09937]] Refined and Segmented Price Sentiment Indices from Survey Comments(https://arxiv.org/abs/2411.09937)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We aim to enhance a price sentiment index and to more precisely understand price trends from the perspective of not only consumers but also businesses. We extract comments related to prices from the Economy Watchers Survey conducted by the Cabinet Office of Japan and classify price trends using a large language model (LLM). We classify whether the survey sample reflects the perspective of consumers or businesses, and whether the comments pertain to goods or services by utilizing information on the fields of comments and the industries of respondents included in the Economy Watchers Survey. From these classified price-related comments, we construct price sentiment indices not only for a general purpose but also for more specific objectives by combining perspectives on consumers and prices, as well as goods and services. It becomes possible to achieve a more accurate classification of price directions by employing a LLM for classification. Furthermore, integrating the outputs of multiple LLMs suggests the potential for the better performance of the classification. The use of more accurately classified comments allows for the construction of an index with a higher correlation to existing indices than previous studies. We demonstrate that the correlation of the price index for consumers, which has a larger sample size, is further enhanced by selecting comments for aggregation based on the industry of the survey respondents.</li>
<li><strong>摘要：</strong>我们的目标是完善价格情绪指数，从消费者和企业的角度更准确地了解价格趋势。我们从日本内阁府进行的经济观察者调查中提取与价格相关的评论，并使用大型语言模型 (LLM) 对价格趋势进行分类。我们利用经济观察者调查中评论领域和受访者行业的信息，对调查样本是否反映了消费者或企业的观点，以及评论是否属于商品或服务进行分类。从这些分类的价格相关评论中，我们通过结合消费者和价格以及商品和服务的观点，构建价格情绪指数，不仅可以用于一般目的，还可以用于更具体的目标。通过使用 LLM 进行分类，可以更准确地对价格方向进行分类。此外，整合多个 LLM 的输出表明分类性能可能更好。使用更准确的分类评论可以构建与现有指数相关性高于先前研究的指数。我们证明，通过根据调查受访者的行业来选择聚合评论，样本量更大的消费者价格指数的相关性会得到进一步增强。</li>
</ul>

<h3>Title: SlimLM: An Efficient Small Language Model for On-Device Document Assistance</h3>
<ul>
<li><strong>Authors: </strong>Thang M. Pham, Phat T. Nguyen, Seunghyun Yoon, Viet Dac Lai, Franck Dernoncourt, Trung Bui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09944">https://arxiv.org/abs/2411.09944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09944">https://arxiv.org/pdf/2411.09944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09944]] SlimLM: An Efficient Small Language Model for On-Device Document Assistance(https://arxiv.org/abs/2411.09944)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While small language models (SLMs) show promises for mobile deployment, their real-world performance and applications on smartphones remains underexplored. We present SlimLM, a series of SLMs optimized for document assistance tasks on mobile devices. Through extensive experiments on a Samsung Galaxy S24, we identify the optimal trade-offs between model size (ranging from 125M to 7B parameters), context length, and inference time for efficient on-device processing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on DocAssist, our constructed dataset for summarization, question answering and suggestion tasks. Our smallest model demonstrates efficient performance on S24, while larger variants offer enhanced capabilities within mobile constraints. We evaluate SlimLM against existing SLMs, showing comparable or superior performance and offering a benchmark for future research in on-device language models. We also provide an Android application, offering practical insights into SLM deployment. Our findings provide valuable insights and illuminate the capabilities of running advanced language models on high-end smartphones, potentially reducing server costs and enhancing privacy through on-device processing.</li>
<li><strong>摘要：</strong>虽然小型语言模型 (SLM) 有望应用于移动设备，但它们在智能手机上的实际性能和应用仍未得到充分探索。我们介绍了 SlimLM，这是一系列针对移动设备上的文档辅助任务进行了优化的 SLM。通过在三星 Galaxy S24 上进行大量实验，我们确定了模型大小（从 125M 到 7B 参数）、上下文长度和推理时间之间的最佳权衡，以实现高效的设备处理。SlimLM 在 SlimPajama-627B 上进行了预训练，并在 DocAssist（我们为摘要、问答和建议任务构建的数据集）上进行了微调。我们最小的模型在 S24 上表现出高效的性能，而更大的变体在移动限制内提供了增强的功能。我们将 SlimLM 与现有的 SLM 进行比较，显示出相当或更优越的性能，并为未来在设备上的语言模型方面的研究提供了基准。我们还提供了一个 Android 应用程序，为 SLM 部署提供了实用的见解。我们的研究结果提供了宝贵的见解，并阐明了在高端智能手机上运行高级语言模型的能力，有可能降低服务器成本并通过设备处理增强隐私。</li>
</ul>

<h3>Title: LoRA-LiteE: A Computationally Efficient Framework for Chatbot Preference-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yahe Yang, Chunliang Tao, Xiaojing Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09947">https://arxiv.org/abs/2411.09947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09947">https://arxiv.org/pdf/2411.09947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09947]] LoRA-LiteE: A Computationally Efficient Framework for Chatbot Preference-Tuning(https://arxiv.org/abs/2411.09947)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>Effective preference tuning is pivotal in aligning chatbot responses with human expectations, enhancing user satisfaction and engagement. Traditional approaches, notably Reinforcement Learning from Human Feedback (RLHF) as employed in advanced models like GPT-4, have demonstrated considerable success in this domain. However, RLHF methods are often computationally intensive and resource-demanding, limiting their scalability and accessibility for broader applications. To address these challenges, this study introduces LoRA-Lite Ensemble (LoRA-LiteE), an innovative framework that combines Supervised Fine-tuning (SFT) with Low-Rank Adaptation (LoRA) and Ensemble Learning techniques to effectively aggregate predictions of lightweight models, which aim to achieve a balance between the performance and computational cost. Utilizing the Chatbot Arena benchmark dataset, we conduct a comprehensive comparative analysis among our LoRA-LiteE model, corresponding base models at different scales, and GPT-4 trained with RLHF. Our empirical results demonstrate that the proposed LoRA-LiteE model achieves comparable performance to un-finetuned GPT-4 and outperforms the single larger-scale models under limited resource constraints. These findings highlight that our LoRA-LiteE provides a feasible and efficient methodology for human preference prediction in chatbot systems, enhancing scalability and accessibility, and thereby broadening the applicability of preference-tuned chatbots in resource-constrained environments.</li>
<li><strong>摘要：</strong>有效的偏好调整对于使聊天机器人的响应与人类的期望保持一致、提高用户满意度和参与度至关重要。传统方法，尤其是 GPT-4 等高级模型中采用的人类反馈强化学习 (RLHF)，已在该领域取得了相当大的成功。然而，RLHF 方法通常计算量大且资源要求高，限制了它们在更广泛应用中的可扩展性和可访问性。为了应对这些挑战，本研究引入了 LoRA-Lite Ensemble (LoRA-LiteE)，这是一个创新框架，它将监督微调 (SFT) 与低秩自适应 (LoRA) 和集成学习技术相结合，以有效地聚合轻量级模型的预测，旨在实现性能和计算成本之间的平衡。利用 Chatbot Arena 基准数据集，我们对我们的 LoRA-LiteE 模型、不同规模的相应基础模型和使用 RLHF 训练的 GPT-4 进行了全面的比较分析。我们的实证结果表明，所提出的 LoRA-LiteE 模型实现了与未微调的 GPT-4 相当的性能，并且在有限的资源约束下优于单个较大规模模型。这些发现突出表明，我们的 LoRA-LiteE 为聊天机器人系统中的人类偏好预测提供了一种可行且有效的方法，增强了可扩展性和可访问性，从而扩大了偏好调整聊天机器人在资源受限环境中的适用性。</li>
</ul>

<h3>Title: Large Language Models as User-Agents for Evaluating Task-Oriented-Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Taaha Kazi, Ruiliang Lyu, Sizhe Zhou, Dilek Hakkani-Tur, Gokhan Tur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09972">https://arxiv.org/abs/2411.09972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09972">https://arxiv.org/pdf/2411.09972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09972]] Large Language Models as User-Agents for Evaluating Task-Oriented-Dialogue Systems(https://arxiv.org/abs/2411.09972)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Traditionally, offline datasets have been used to evaluate task-oriented dialogue (TOD) models. These datasets lack context awareness, making them suboptimal benchmarks for conversational systems. In contrast, user-agents, which are context-aware, can simulate the variability and unpredictability of human conversations, making them better alternatives as evaluators. Prior research has utilized large language models (LLMs) to develop user-agents. Our work builds upon this by using LLMs to create user-agents for the evaluation of TOD systems. This involves prompting an LLM, using in-context examples as guidance, and tracking the user-goal state. Our evaluation of diversity and task completion metrics for the user-agents shows improved performance with the use of better prompts. Additionally, we propose methodologies for the automatic evaluation of TOD models within this dynamic framework.</li>
<li><strong>摘要：</strong>传统上，离线数据集已用于评估面向任务的对话 (TOD) 模型。这些数据集缺乏上下文感知，因此它们不是对话系统的次优基准。相比之下，具有上下文感知的用户代理可以模拟人类对话的多变性和不可预测性，使其成为更好的评估者替代方案。先前的研究已利用大型语言模型 (LLM) 来开发用户代理。我们的工作在此基础上建立，使用 LLM 创建用户代理以评估 TOD 系统。这涉及提示 LLM、使用上下文示例作为指导以及跟踪用户目标状态。我们对用户代理的多样性和任务完成度指标的评估表明，使用更好的提示可以提高性能。此外，我们提出了在此动态框架内自动评估 TOD 模型的方法。</li>
</ul>

<h3>Title: HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of Historical Texts -- A Case Application of Yantie Lun</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.09978">https://arxiv.org/abs/2411.09978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.09978">https://arxiv.org/pdf/2411.09978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.09978]] HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of Historical Texts -- A Case Application of Yantie Lun(https://arxiv.org/abs/2411.09978)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper proposes HistoLens, a multi-layered analysis framework for historical texts based on Large Language Models (LLMs). Using the important Western Han dynasty text "Yantie Lun" as a case study, we demonstrate the framework's potential applications in historical research and education. HistoLens integrates NLP technology (especially LLMs), including named entity recognition, knowledge graph construction, and geographic information visualization. The paper showcases how HistoLens explores Western Han culture in "Yantie Lun" through multi-dimensional, visual, and quantitative methods, focusing particularly on the influence of Confucian and Legalist thoughts on political, economic, military, and ethnic. We also demonstrate how HistoLens constructs a machine teaching scenario using LLMs for explainable analysis, based on a dataset of Confucian and Legalist ideas extracted with LLM assistance. This approach offers novel and diverse perspectives for studying historical texts like "Yantie Lun" and provides new auxiliary tools for history education. The framework aims to equip historians and learners with LLM-assisted tools to facilitate in-depth, multi-layered analysis of historical texts and foster innovation in historical education.</li>
<li><strong>摘要：</strong>本文提出了基于大型语言模型（LLM）的历史文本多层次分析框架HistoLens，并以西汉重要文献《燕铁论》为例，展示该框架在历史研究和教育中的潜在应用。HistoLens集成了NLP技术（尤其是LLM），包括命名实体识别、知识图谱构建、地理信息可视化等。本文展示了HistoLens如何通过多维度、可视化、量化的方法探究《燕铁论》中的西汉文化，重点关注儒法思想对政治、经济、军事和民族的影响。我们还展示了HistoLens如何基于LLM辅助提取的儒法思想数据集，构建使用LLM进行可解释分析的机器教学场景。该方法为《燕铁论》等历史文献的研究提供了新颖而多样的视角，为历史教育提供了新的辅助工具。该框架旨在为历史学家和学习者提供法学硕士辅助工具，以促进对历史文本的深入、多层次分析，并促进历史教育的创新。</li>
</ul>

<h3>Title: Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10006">https://arxiv.org/abs/2411.10006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10006">https://arxiv.org/pdf/2411.10006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10006]] Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits(https://arxiv.org/abs/2411.10006)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models has catalyzed the development of personalized dialogue systems, numerous role-playing conversational agents have emerged. While previous research predominantly focused on enhancing the model's capability to follow instructions by designing character profiles, neglecting the psychological factors that drive human conversations. In this paper, we propose Orca, a framework for data processing and training LLMs of custom characters by integrating personality traits. Orca comprises four stages: (1) Personality traits inferring, leverage LLMs to infer user's BigFive personality trait reports and scores. (2) Data Augment, simulate user's profile, background story, and psychological activities. (3) Dataset construction, personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4) Modeling and Training, personality-conditioned instruction tuning (PTIT and PSIT), using the generated data to enhance existing open-source LLMs. We introduce OrcaBench, the first benchmark for evaluating the quality of content generated by LLMs on social platforms across multiple scales. Our experiments demonstrate that our proposed model achieves superior performance on this benchmark, demonstrating its excellence and effectiveness in perceiving personality traits that significantly improve role-playing abilities. Our Code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型催化了个性化对话系统的发展，大量角色扮演对话代理应运而生。而先前的研究主要集中在通过设计角色档案来增强模型遵循指令的能力，而忽略了驱动人类对话的心理因素。在本文中，我们提出了 Orca，一个通过整合性格特征来处理数据和训练自定义角色 LLM 的框架。Orca 包括四个阶段：（1）性格特征推断，利用 LLM 推断用户的 BigFive 性格特征报告和分数。（2）数据增强，模拟用户的档案、背景故事和心理活动。（3）数据集构建，性格条件指令提示 (PCIP) 来刺激 LLM。（4）建模和训练，性格条件指令调整 (PTIT 和 PSIT)，使用生成的数据增强现有的开源 LLM。我们推出了 OrcaBench，这是第一个用于评估跨多个规模的社交平台上 LLM 生成的内容质量的基准。我们的实验表明，我们提出的模型在此基准上取得了优异的表现，证明了其在感知个性特征方面的卓越性和有效性，从而显著提高了角色扮演能力。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Yan Hu, Xu Zuo, Yujia Zhou, Xueqing Peng, Jimin Huang, Vipina K. Keloth, Vincent J. Zhang, Ruey-Ling Weng, Qingyu Chen, Xiaoqian Jiang, Kirk E. Roberts, Hua Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10020">https://arxiv.org/abs/2411.10020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10020">https://arxiv.org/pdf/2411.10020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10020]] Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?(https://arxiv.org/abs/2411.10020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Backgrounds: Information extraction (IE) is critical in clinical natural language processing (NLP). While large language models (LLMs) excel on generative tasks, their performance on extractive tasks remains debated. Methods: We investigated Named Entity Recognition (NER) and Relation Extraction (RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples, MIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical entities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3 against BiomedBERT in terms of performance, generalizability, computational resources, and throughput to BiomedBERT. Results: LLaMA models outperformed BiomedBERT across datasets. With sufficient training data, LLaMA showed modest improvements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited training data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7% (F1) on NER and 4% on RE. However, LLaMA models required more computing resources and ran up to 28 times slower. We implemented "Kiwi," a clinical IE package featuring both models, available at this https URL. Conclusion: This study is among the first to develop and evaluate a comprehensive clinical IE system using open-source LLMs. Results indicate that LLaMA models outperform BiomedBERT for clinical NER and RE but with higher computational costs and lower throughputs. These findings highlight that choosing between LLMs and traditional deep learning methods for clinical IE applications should remain task-specific, taking into account both performance metrics and practical considerations such as available computing resources and the intended use case scenarios.</li>
<li><strong>摘要：</strong>背景：信息提取 (IE) 在临床自然语言处理 (NLP) 中至关重要。虽然大型语言模型 (LLM) 在生成任务上表现出色，但它们在提取任务上的表现仍存在争议。方法：我们使用来自四个来源（UT Physicians、MTSamples、MIMIC-III 和 i2b2）的 1,588 份临床记录研究了命名实体识别 (NER) 和关系提取 (RE)。我们开发了一个带注释的语料库，涵盖 4 个临床实体和 16 个修饰符，并在性能、通用性、计算资源和吞吐量方面将指令调整的 LLaMA-2 和 LLaMA-3 与 BiomedBERT 进行了比较。结果：LLaMA 模型在数据集上的表现优于 BiomedBERT。在有足够的训练数据的情况下，LLaMA 显示出适度的改进（NER 上为 1%，RE 上为 1.5-3.7%）；在训练数据有限的情况下，改进幅度更大。在未见过的 i2b2 数据上，LLaMA-3-70B 在 NER 上的表现比 BiomedBERT 高 7% (F1)，在 RE 上的表现比 BiomedBERT 高 4%。然而，LLaMA 模型需要更多的计算资源，运行速度最多慢 28 倍。我们实现了“Kiwi”，这是一个包含这两种模型的临床 IE 包，可从此 https URL 获得。结论：本研究是首批使用开源 LLM 开发和评估综合临床 IE 系统的研究之一。结果表明，LLaMA 模型在临床 NER 和 RE 方面的表现优于 BiomedBERT，但计算成本更高，吞吐量更低。这些发现强调，在临床 IE 应用中，选择 LLM 和传统深度学习方法时应保持任务特定性，同时考虑性能指标和实际考虑因素，例如可用的计算资源和预期的用例场景。</li>
</ul>

<h3>Title: Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Zichen Song, Sitan Huang, Yuxin Wu, Zhongfeng Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10069">https://arxiv.org/abs/2411.10069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10069">https://arxiv.org/pdf/2411.10069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10069]] Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity(https://arxiv.org/abs/2411.10069)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Evaluating the importance of different layers in large language models (LLMs) is crucial for optimizing model performance and interpretability. This paper first explores layer importance using the Activation Variance-Sparsity Score (AVSS), which combines normalized activation variance and sparsity to quantify each layer's contribution to overall model performance. By ranking layers based on AVSS and pruning the least impactful 25\%, our experiments on tasks such as question answering, language modeling, and sentiment classification show that over 90\% of the original performance is retained, highlighting potential redundancies in LLM architectures. Building on AVSS, we propose an enhanced version tailored to assess hallucination propensity across layers (EAVSS). This improved approach introduces Hallucination-Specific Activation Variance (HSAV) and Hallucination-Specific Sparsity (HSS) metrics, allowing precise identification of hallucination-prone layers. By incorporating contrastive learning on these layers, we effectively mitigate hallucination generation, contributing to more robust and efficient LLMs(The maximum performance improvement is 12\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and WikiQA datasets demonstrate the efficacy of this method, offering a comprehensive framework for both layer importance evaluation and hallucination mitigation in LLMs.</li>
<li><strong>摘要：</strong>评估大型语言模型 (LLM) 中不同层的重要性对于优化模型性能和可解释性至关重要。本文首先使用激活方差-稀疏度分数 (AVSS) 探索层的重要性，该分数结合了归一化的激活方差和稀疏度来量化每个层对整体模型性能的贡献。通过根据 AVSS 对层进行排名并修剪影响最小的 25%，我们在问答、语言建模和情感分类等任务上的实验表明，保留了 90% 以上的原始性能，突出了 LLM 架构中的潜在冗余。基于 AVSS，我们提出了一个增强版本，专门用于评估跨层幻觉倾向 (EAVSS)。这种改进的方法引入了幻觉特定激活方差 (HSAV) 和幻觉特定稀疏度 (HSS) 指标，可以精确识别容易产生幻觉的层。通过在这些层上加入对比学习，我们有效地缓解了幻觉的产生，有助于实现更稳健、更高效的 LLM（最大性能提升为 12%）。我们在 NQ、SciQ、TriviaQA、TruthfulQA 和 WikiQA 数据集上的结果证明了该方法的有效性，为 LLM 中的层重要性评估和幻觉缓解提供了一个全面的框架。</li>
</ul>

<h3>Title: Understanding The Effect Of Temperature On Alignment With Human Opinions</h3>
<ul>
<li><strong>Authors: </strong>Maja Pavlovic, Massimo Poesio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10080">https://arxiv.org/abs/2411.10080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10080">https://arxiv.org/pdf/2411.10080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10080]] Understanding The Effect Of Temperature On Alignment With Human Opinions(https://arxiv.org/abs/2411.10080)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>With the increasing capabilities of LLMs, recent studies focus on understanding whose opinions are represented by them and how to effectively extract aligned opinion distributions. We conducted an empirical analysis of three straightforward methods for obtaining distributions and evaluated the results across a variety of metrics. Our findings suggest that sampling and log-probability approaches with simple parameter adjustments can return better aligned outputs in subjective tasks compared to direct prompting. Yet, assuming models reflect human opinions may be limiting, highlighting the need for further research on how human subjectivity affects model uncertainty.</li>
<li><strong>摘要：</strong>随着 LLM 功能的不断增强，最近的研究重点是了解它们代表了谁的观点以及如何有效地提取一致的观点分布。我们对三种获取分布的简单方法进行了实证分析，并根据各种指标评估了结果。我们的研究结果表明，与直接提示相比，使用简单参数调整的抽样和对数概率方法可以在主观任务中返回更一致的输出。然而，假设模型反映人类的观点可能会受到限制，这凸显了进一步研究人类主观性如何影响模型不确定性的必要性。</li>
</ul>

<h3>Title: Xmodel-1.5: An 1B-scale Multilingual LLM</h3>
<ul>
<li><strong>Authors: </strong>Wang Qun, Liu Yang, Lin Qingquan, Jiang Ling</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10083">https://arxiv.org/abs/2411.10083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10083">https://arxiv.org/pdf/2411.10083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10083]] Xmodel-1.5: An 1B-scale Multilingual LLM(https://arxiv.org/abs/2411.10083)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing a Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn University's School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at this https URL.</li>
<li><strong>摘要：</strong>我们推出了 Xmodel-1.5，这是一个新颖的 10 亿参数多语言大型模型，已在约 2 万亿个标记上进行了预训练。该模型在多种语言中表现出色，在泰语、阿拉伯语和法语中效果尤为显著，同时在中文和英语中也表现出色。此外，我们还发布了泰语评估数据集，为研究界做出了贡献，该数据集包括朱拉隆功大学综合创新学院学生注释的数百个问题。虽然结果令人鼓舞，但我们承认仍有改进空间。我们希望这项工作能够推动多语言 AI ​​研究的持续努力，并促进各种自然语言处理任务中更好的跨语言理解。我们的模型和代码在 GitHub 上的 https URL 上公开提供。</li>
</ul>

<h3>Title: Legal Evalutions and Challenges of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wang, Huan Zhao, Zhenyuan Yang, Peng Shu, Junhao Chen, Haobo Sun, Ruixi Liang, Shixin Li, Pengcheng Shi, Longjun Ma, Zongjia Liu, Zhengliang Liu, Tianyang Zhong, Yutong Zhang, Chong Ma, Xin Zhang, Tuo Zhang, Tianli Ding, Yudan Ren, Tianming Liu, Xi Jiang, Shu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10137">https://arxiv.org/abs/2411.10137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10137">https://arxiv.org/pdf/2411.10137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10137]] Legal Evalutions and Challenges of Large Language Models(https://arxiv.org/abs/2411.10137)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we review legal testing methods based on Large Language Models (LLMs), using the OPENAI o1 model as a case study to evaluate the performance of large models in applying legal provisions. We compare current state-of-the-art LLMs, including open-source, closed-source, and legal-specific models trained specifically for the legal domain. Systematic tests are conducted on English and Chinese legal cases, and the results are analyzed in depth. Through systematic testing of legal cases from common law systems and China, this paper explores the strengths and weaknesses of LLMs in understanding and applying legal texts, reasoning through legal issues, and predicting judgments. The experimental results highlight both the potential and limitations of LLMs in legal applications, particularly in terms of challenges related to the interpretation of legal language and the accuracy of legal reasoning. Finally, the paper provides a comprehensive analysis of the advantages and disadvantages of various types of models, offering valuable insights and references for the future application of AI in the legal field.</li>
<li><strong>摘要：</strong>本文回顾了基于大型语言模型（LLM）的法律测试方法，以OPENAI o1模型为例，评估大型模型在应用法律条款方面的表现。我们比较了当前最先进的LLM，包括开源、闭源和专门为法律领域训练的法律专用模型。对英文和中文法律案例进行了系统测试，并对结果进行了深入分析。通过对普通法系和中国法律案例的系统测试，本文探讨了LLM在理解和应用法律文本、通过法律问题进行推理和预测判决方面的优势和劣势。实验结果突出了LLM在法律应用中的潜力和局限性，特别是在与法律语言解释和法律推理准确性相关的挑战方面。最后，本文对各类模型的优缺点进行了全面分析，为未来人工智能在法律领域的应用提供了宝贵的见解和参考。</li>
</ul>

<h3>Title: An Effective Framework to Help Large Language Models Handle Numeric-involved Long-context Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yijiong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10145">https://arxiv.org/abs/2411.10145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10145">https://arxiv.org/pdf/2411.10145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10145]] An Effective Framework to Help Large Language Models Handle Numeric-involved Long-context Tasks(https://arxiv.org/abs/2411.10145)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long texts and have almost perfect performance in traditional retrieval tasks. However, their performance significantly degrades when it comes to numerical calculations in the long-context. Numeric-involved long-context tasks typically cannot be addressed by current LLMs in normal settings due to their inherent limitations in simultaneously handling complex and massive information. Some CoT like prompting methods can improve accuracy but demands massive output tokens, which is costly and slow. To address this issue, we propose a workflow, which decompose a numeric-involved long-context task into 4 low-level subtasks: judging, extracting and processing with code and conclusion. The former 2 subtasks is relatively simple, which allows us to use smaller models for efficiently processing long context. When numerical calculations are required, we use code generated by LLMs to avoid the disadvantage of LLM not being good at calculations. The results in 2 numeric-involved long-context benchmarks demonstrate our workflow can not only improve accuracy, but also significantly reduce the cost of API calls.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在处理长文本方面表现出色，在传统检索任务中表现近乎完美。然而，当涉及到长上下文中的数值计算时，它们的性能会显著下降。涉及数值的长上下文任务通常无法在常规设置下由当前的LLM解决，因为它们在同时处理复杂和海量信息方面存在固有的局限性。一些类似CoT的提示方法可以提高准确率，但需要大量输出token，这既昂贵又缓慢。为了解决这个问题，我们提出了一个工作流，将涉及数值的长上下文任务分解为4个低级子任务：判断、提取、用代码处理和结论。前两个子任务相对简单，这使我们能够使用较小的模型来高效处理长上下文。当需要数值计算时，我们使用LLM生成的代码来避免LLM不擅长计算的缺点。 2 个涉及数字的长上下文基准的结果表明，我们的工作流程不仅可以提高准确性，还可以显著降低 API 调用的成本。</li>
</ul>

<h3>Title: Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions</h3>
<ul>
<li><strong>Authors: </strong>Yutao Hou, Yajing Luo, Zhiwen Ruan, Hongru Wang, Weifeng Ge, Yun Chen, Guanhua Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10163">https://arxiv.org/abs/2411.10163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10163">https://arxiv.org/pdf/2411.10163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10163]] Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions(https://arxiv.org/abs/2411.10163)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable performance across various tasks, prompting researchers to develop diverse evaluation benchmarks. However, existing benchmarks typically measure the ability of LLMs to respond to individual questions, neglecting the complex interactions in real-world applications. In this paper, we introduce Compound Question Synthesis (CQ-Syn) to create the Compound-QA benchmark, focusing on compound questions with multiple sub-questions. This benchmark is derived from existing QA datasets, annotated with proprietary LLMs and verified by humans for accuracy. It encompasses five categories: Factual-Statement, Cause-and-Effect, Hypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion. It evaluates the LLM capability in terms of three dimensions including understanding, reasoning, and knowledge. Our assessment of eight open-source LLMs using Compound-QA reveals distinct patterns in their responses to compound questions, which are significantly poorer than those to non-compound questions. Additionally, we investigate various methods to enhance LLMs performance on compound questions. The results indicate that these approaches significantly improve the models' comprehension and reasoning abilities on compound questions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中表现出色，促使研究人员开发出各种评估基准。然而，现有的基准通常衡量 LLM 回答单个问题的能力，而忽略了实际应用中的复杂交互。在本文中，我们引入了复合问题合成 (CQ-Syn) 来创建复合 QA 基准，重点关注具有多个子问题的复合问题。该基准源自现有的 QA 数据集，使用专有 LLM 注释并由人工验证准确性。它包含五个类别：事实陈述、因果关系、假设分析、比较和选择以及评估和建议。它从理解、推理和知识三个维度评估 LLM 能力。我们使用复合 QA 对八个开源 LLM 进行了评估，发现它们对复合问题的回答模式不同，明显比对非复合问题的回答差。此外，我们还研究了各种方法来提高 LLM 在复合问题上的表现。结果表明这些方法显著提高了模型对复合问题的理解和推理能力。</li>
</ul>

<h3>Title: Increasing the Accessibility of Causal Domain Knowledge via Causal Information Extraction Methods: A Case Study in the Semiconductor Manufacturing Industry</h3>
<ul>
<li><strong>Authors: </strong>Houssam Razouk, Leonie Benischke, Daniel Garber, Roman Kern</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10172">https://arxiv.org/abs/2411.10172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10172">https://arxiv.org/pdf/2411.10172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10172]] Increasing the Accessibility of Causal Domain Knowledge via Causal Information Extraction Methods: A Case Study in the Semiconductor Manufacturing Industry(https://arxiv.org/abs/2411.10172)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The extraction of causal information from textual data is crucial in the industry for identifying and mitigating potential failures, enhancing process efficiency, prompting quality improvements, and addressing various operational challenges. This paper presents a study on the development of automated methods for causal information extraction from actual industrial documents in the semiconductor manufacturing industry. The study proposes two types of causal information extraction methods, single-stage sequence tagging (SST) and multi-stage sequence tagging (MST), and evaluates their performance using existing documents from a semiconductor manufacturing company, including presentation slides and FMEA (Failure Mode and Effects Analysis) documents. The study also investigates the effect of representation learning on downstream tasks. The presented case study showcases that the proposed MST methods for extracting causal information from industrial documents are suitable for practical applications, especially for semi structured documents such as FMEAs, with a 93\% F1 score. Additionally, MST achieves a 73\% F1 score on texts extracted from presentation slides. Finally, the study highlights the importance of choosing a language model that is more aligned with the domain and in-domain fine-tuning.</li>
<li><strong>摘要：</strong>从文本数据中提取因果信息对于识别和减轻潜在故障、提高流程效率、促进质量改进和应对各种操作挑战至关重要。本文介绍了一项关于从半导体制造行业的实际工业文档中自动提取因果信息的方法的开发研究。该研究提出了两种因果信息提取方法，即单级序列标记 (SST) 和多级序列标记 (MST)，并使用半导体制造公司的现有文档（包括演示幻灯片和 FMEA（故障模式和影响分析）文档）评估它们的性能。该研究还调查了表示学习对下游任务的影响。所提出的案例研究表明，用于从工业文档中提取因果信息的 MST 方法适用于实际应用，尤其是对于半结构化文档（如 FMEA），F1 得分为 93\%。此外，MST 在从演示幻灯片中提取的文本上获得了 73\% 的 F1 分数。最后，该研究强调了选择更符合领域和领域内微调的语言模型的重要性。</li>
</ul>

<h3>Title: Measuring Non-Adversarial Reproduction of Training Data in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Aerni, Javier Rando, Edoardo Debenedetti, Nicholas Carlini, Daphne Ippolito, Florian Tramèr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10242">https://arxiv.org/abs/2411.10242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10242">https://arxiv.org/pdf/2411.10242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10242]] Measuring Non-Adversarial Reproduction of Training Data in Large Language Models(https://arxiv.org/abs/2411.10242)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim sequences of memorized text when prompted by a motivated adversary. In this work, we investigate an intermediate regime of memorization that we call non-adversarial reproduction, where we quantify the overlap between model responses and pretraining data when responding to natural and benign prompts. For a variety of innocuous prompt categories (e.g., writing a letter or a tutorial), we show that up to 15% of the text output by popular conversational language models overlaps with snippets from the Internet. In worst cases, we find generations where 100% of the content can be found exactly online. For the same tasks, we find that human-written text has far less overlap with Internet data. We further study whether prompting strategies can close this reproduction gap between models and humans. While appropriate prompting can reduce non-adversarial reproduction on average, we find that mitigating worst-case reproduction of training data requires stronger defenses -- even for benign interactions.</li>
<li><strong>摘要：</strong>大型语言模型会记住部分训练数据。记住简短的片段和事实是回答有关世界的问题和流利使用任何语言的必要条件。但也有研究表明，当受到有动机的对手提示时，模型也会重现长篇逐字记忆的文本序列。在这项工作中，我们研究了一种中间的记忆机制，我们称之为非对抗性重现，在响应自然和良性提示时，我们量化了模型响应和预训练数据之间的重叠。对于各种无害的提示类别（例如，写一封信或一个教程），我们发现流行的对话语言模型输出的文本中多达 15% 与来自互联网的片段重叠。在最坏的情况下，我们发现 100% 的内容可以在网上找到。对于同样的任务，我们发现人类书写的文本与互联网数据的重叠程度要小得多。我们进一步研究提示策略是否可以缩小模型和人类之间的这种重现差距。虽然适当的提示平均可以减少非对抗性再现，但我们发现减轻训练数据的最坏情况再现需要更强的防御——即使对于良性交互也是如此。</li>
</ul>

<h3>Title: A Survey of Event Causality Identification: Principles, Taxonomy, Challenges, and Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zefan Zeng, Qing Cheng, Xingchen Hu, Yuehang Si, Zhong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10371">https://arxiv.org/abs/2411.10371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10371">https://arxiv.org/pdf/2411.10371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10371]] A Survey of Event Causality Identification: Principles, Taxonomy, Challenges, and Assessment(https://arxiv.org/abs/2411.10371)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Event Causality Identification (ECI) has become a crucial task in Natural Language Processing (NLP), aimed at automatically extracting causalities from textual data. In this survey, we systematically address the foundational principles, technical frameworks, and challenges of ECI, offering a comprehensive taxonomy to categorize and clarify current research methodologies, as well as a quantitative assessment of existing models. We first establish a conceptual framework for ECI, outlining key definitions, problem formulations, and evaluation standards. Our taxonomy classifies ECI methods according to the two primary tasks of sentence-level (SECI) and document-level (DECI) event causality identification. For SECI, we examine feature pattern-based matching, deep semantic encoding, causal knowledge pre-training and prompt-based fine-tuning, and external knowledge enhancement methods. For DECI, we highlight approaches focused on event graph reasoning and prompt-based techniques to address the complexity of cross-sentence causal inference. Additionally, we analyze the strengths, limitations, and open challenges of each approach. We further conduct an extensive quantitative evaluation of various ECI methods on two benchmark datasets. Finally, we explore future research directions, highlighting promising pathways to overcome current limitations and broaden ECI applications.</li>
<li><strong>摘要：</strong>事件因果关系识别 (ECI) 已成为自然语言处理 (NLP) 中的一项关键任务，旨在自动从文本数据中提取因果关系。在本次调查中，我们系统地介绍了 ECI 的基本原理、技术框架和挑战，提供了一个全面的分类法来对当前的研究方法进行分类和阐明，并对现有模型进行了定量评估。我们首先为 ECI 建立了一个概念框架，概述了关键定义、问题表述和评估标准。我们的分类法根据句子级 (SECI) 和文档级 (DECI) 事件因果关系识别的两个主要任务对 ECI 方法进行分类。对于 SECI，我们研究基于特征模式的匹配、深度语义编码、因果知识预训练和基于提示的微调以及外部知识增强方法。对于 DECI，我们重点介绍了专注于事件图推理和基于提示的技术的方法，以解决跨句因果推理的复杂性。此外，我们分析了每种方法的优势、局限性和未解决的挑战。我们进一步在两个基准数据集上对各种 ECI 方法进行了广泛的定量评估。最后，我们探索了未来的研究方向，重点介绍了克服当前限制和拓宽 ECI 应用的有希望的途径。</li>
</ul>

<h3>Title: Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Xirong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10436">https://arxiv.org/abs/2411.10436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10436">https://arxiv.org/pdf/2411.10436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10436]] Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization(https://arxiv.org/abs/2411.10436)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, hallucination</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.</li>
<li><strong>摘要：</strong>众所周知，多模态大型语言模型 (MLLM) 会产生幻觉，这限制了它们的实际应用。最近的研究尝试应用直接偏好优化 (DPO) 来增强 MLLM 的性能，但在减轻幻觉方面表现出的改进并不一致。为了更有效地解决这个问题，我们引入了针对幻觉的直接偏好优化 (HDPO) 来减少 MLLM 中的幻觉。与以前的方法不同，我们的方法从幻觉的不同形式和原因入手。具体来说，我们开发了三种类型的偏好对数据，针对以下导致 MLLM 幻觉的原因：(1) 视觉能力不足、(2) 长上下文生成和 (3) 多模态冲突。实验结果表明，我们的方法在多个幻觉评估数据集中取得了优异的表现，超越了大多数最先进的 (SOTA) 方法，并凸显了我们方法的潜力。消融研究和深入分析进一步证实了我们方法的有效性，并表明通过扩大规模可以进一步改进。</li>
</ul>

<h3>Title: Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, Jifeng Dai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.10442">https://arxiv.org/abs/2411.10442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.10442">https://arxiv.org/pdf/2411.10442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.10442]] Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization(https://arxiv.org/abs/2411.10442)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10x larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released.</li>
<li><strong>摘要：</strong>现有的开源多模态大型语言模型 (MLLM) 通常遵循涉及预训练和监督微调的训练过程。然而，这些模型受到分布变化的影响，这限制了它们的多模态推理，特别是在思路链 (CoT) 性能方面。为了解决这个问题，我们引入了一个偏好优化 (PO) 流程来增强 MLLM 的多模态推理能力。具体来说，(1) 在数据方面，我们设计了一个自动化的偏好数据构建管道来创建 MMPR，这是一个高质量、大规模的多模态推理偏好数据集。(2) 在模型方面，我们探索将 PO 与 MLLM 集成，开发一种简单而有效的方法，称为混合偏好优化 (MPO)，它可以提高多模态 CoT 性能。我们的方法在多个基准测试中都表现出了更好的性能，特别是在多模态推理任务中。值得注意的是，我们的模型 InternVL2-8B-MPO 在 MathVista 上的准确率达到 67.0，比 InternVL2-8B 高出 8.7 个百分点，性能可与 10 倍大的 InternVL2-76B 相媲美。我们希望这项研究能够激发 MLLM 的进一步发展。代码、数据和模型将公开发布。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
