<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-11</h1>
<h3>Title: LLMs Provide Unstable Answers to Legal Questions</h3>
<ul>
<li><strong>Authors: </strong>Andrew Blair-Stanek, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05196">https://arxiv.org/abs/2502.05196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05196">https://arxiv.org/pdf/2502.05196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05196]] LLMs Provide Unstable Answers to Legal Questions(https://arxiv.org/abs/2502.05196)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>An LLM is stable if it reaches the same conclusion when asked the identical question multiple times. We find leading LLMs like gpt-4o, claude-3.5, and gemini-1.5 are unstable when providing answers to hard legal questions, even when made as deterministic as possible by setting temperature to 0. We curate and release a novel dataset of 500 legal questions distilled from real cases, involving two parties, with facts, competing legal arguments, and the question of which party should prevail. When provided the exact same question, we observe that LLMs sometimes say one party should win, while other times saying the other party should win. This instability has implications for the increasing numbers of legal AI products, legal processes, and lawyers relying on these LLMs.</li>
<li><strong>摘要：</strong>如果LLM多次询问相同的问题，则LLM是稳定的，如果它得出相同的结论。我们发现诸如GPT-4O，Claude-3.5和Gemini-1.5之类的领先LLM在为硬法律问题提供答案时，即使将温度设置为0，我们也会不稳定。从真实案件中提取的问题，涉及两个当事方，事实，竞争法律论点，以及哪个方应占上风的问题。当提出完全相同的问题时，我们观察到LLM有时会说一方应该获胜，而其他时候则说另一方应该获胜。这种不稳定对依赖这些LLM的法律AI产品，法律程序和律师的数量越来越多。</li>
</ul>

<h3>Title: Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies</h3>
<ul>
<li><strong>Authors: </strong>Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Gaurav Jain, Roy Schwartz, Moshe Wasserblat, David Harel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05202">https://arxiv.org/abs/2502.05202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05202">https://arxiv.org/pdf/2502.05202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05202]] Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies(https://arxiv.org/abs/2502.05202)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms achieve significant speedups over standard autoregressive decoding. By enabling any off-the-shelf model to serve as drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.</li>
<li><strong>摘要：</strong>加速大型语言模型（LLM）的推断是生成AI的关键挑战。投机解码（SD）方法通过使用单个目标向前传递生成多个令牌来提供可观的效率提高。但是，现有的SD方法要求起草者和目标模型共享相同的词汇，从而限制了可能的起草者的池，通常需要从头开始训练起草人。我们提出了三种新的SD方法，用于删除此共享 - 唱机约束。所有三种方法都保留目标分布（即它们是无损的），并与现成的模型一起使用，而无需进行额外的培训或修改。从经验上讲，在汇总，编程和长篇文章任务上，我们的算法比标准自回旋解码实现了重要的加速。通过使任何现成的模型能够充当起草者并不需要再培训，这项工作在实践中实质上扩大了SD框架的适用性。</li>
</ul>

<h3>Title: Efficient Knowledge Feeding to Language Models: A Novel Integrated Encoder-Decoder Architecture</h3>
<ul>
<li><strong>Authors: </strong>S Santosh Kumar, Rishi Gottimukkala, Supriya Devidutta, Karthikeyan S</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05233">https://arxiv.org/abs/2502.05233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05233">https://arxiv.org/pdf/2502.05233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05233]] Efficient Knowledge Feeding to Language Models: A Novel Integrated Encoder-Decoder Architecture(https://arxiv.org/abs/2502.05233)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to efficiently feeding knowledge to language models (LLMs) during prediction by integrating retrieval and generation processes within a unified framework. While the Retrieval-Augmented Generation (RAG) model addresses gaps in LLMs' training data and knowledge limits, it is hindered by token limit restrictions and dependency on the retrieval system's accuracy. Our proposed architecture incorporates in-context vectors (ICV) to overcome these challenges. ICV recasts in-context learning by using latent embeddings of LLMs to create a vector that captures essential task information. This vector is then used to shift the latent states of the LLM, enhancing the generation process without adding demonstration examples to the prompt. ICV directly integrates information into the model, enabling it to process this information more effectively. Our extensive experimental evaluation demonstrates that ICV outperforms standard in-context learning and fine-tuning across question-answering, information retrieval, and other tasks. This approach mitigates the limitations of current RAG models and offers a more robust solution for handling extensive and diverse datasets. Despite leveraging a fraction of the parameters, our ICV-enhanced model achieves competitive performance against models like LLaMA-3, Gemma, and Phi-3, significantly reducing computational costs and memory requirements. ICV reduces prompt length, is easy to control, surpasses token limitations, and is computationally efficient compared to fine-tuning.</li>
<li><strong>摘要：</strong>本文通过在统一框架内整合检索和生成过程，在预测过程中介绍了一种新颖的方法，可以在预测过程中有效地喂养知识（LLM）。虽然检索功能的生成（RAG）模型解决了LLMS培训数据和知识限制中的差距，但它受到令牌限制限制和对检索系统准确性的依赖的阻碍。我们提出的体系结构结合了文化载体（ICV），以克服这些挑战。 ICV通过使用LLMS的潜在嵌入来创建一个捕获基本任务信息的向量来重铸在上下文中学习。然后，该向量用于移动LLM的潜在状态，从而增强了生成过程，而无需在提示中添加演示示例。 ICV将信息直接集成到模型中，使其能够更有效地处理此信息。我们广泛的实验评估表明，ICV在提问，信息检索和其他任务中的表现优于标准的文本学习和微调。这种方法减轻了当前的抹布模型的局限性，并为处理广泛和多样化的数据集提供了更强大的解决方案。尽管利用了一小部分参数，但我们的ICV增强模型仍针对Llama-3，Gemma和Phi-3等模型达到了竞争性能，从而大大降低了计算成本和内存要求。 ICV降低了及时的长度，易于控制，超过令牌限制，并且与微调相比在计算上有效。</li>
</ul>

<h3>Title: Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics</h3>
<ul>
<li><strong>Authors: </strong>Hussam Ghanem (ICB, UB), Christophe Cruz (ICB, UB)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05239">https://arxiv.org/abs/2502.05239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05239">https://arxiv.org/pdf/2502.05239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05239]] Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics(https://arxiv.org/abs/2502.05239)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have demonstrated significant potential in the automated construction of knowledge graphs from unstructured text. This paper builds upon our previous work [16], which evaluated various models using metrics like precision, recall, F1 score, triple matching, and graph matching, and introduces a refined approach to address the critical issues of hallucination and omission. We propose an enhanced evaluation framework incorporating BERTScore for graph similarity, setting a practical threshold of 95% for graph matching. Our experiments focus on the Mistral model, comparing its original and fine-tuned versions in zero-shot and few-shot settings. We further extend our experiments using examples from the KELM-sub training dataset, illustrating that the fine-tuned model significantly improves knowledge graph construction accuracy while reducing the exact hallucination and omission. However, our findings also reveal that the fine-tuned models perform worse in generalization tasks on the KELM-sub dataset. This study underscores the importance of comprehensive evaluation metrics in advancing the state-of-the-art in knowledge graph construction from textual data.</li>
<li><strong>摘要：</strong>大型语言模型的最新进展表明，在自动化的知识图中，非结构化文本的自动构造具有巨大的潜力。本文建立在我们以前的工作[16]的基础上，该工作使用精度，召回，F1分数，三重匹配和图形匹配评估了各种模型，并介绍了一种精致的方法来解决幻觉和遗漏的关键问题。我们提出了一个增强的评估框架，其中包含用于图形相似性的Bertscore，为图形匹配设置了95％的实际阈值。我们的实验集中在Mistral模型上，比较了其原始和微调版本中的零拍摄和少量设置。我们使用KELM-SUB训练数据集的示例进一步扩展了实验，表明微型模型可显着提高知识图构造精度，同时降低了确切的幻觉和遗漏。但是，我们的发现还表明，微调模型在KELM-SUB数据集上的概括任务中的性能差。这项研究强调了全面评估指标在从文本数据中推进知识图构造中最新的最新时间的重要性。</li>
</ul>

<h3>Title: SEER: Self-Explainability Enhancement of Large Language Models' Representations</h3>
<ul>
<li><strong>Authors: </strong>Guanxu Chen, Dongrui Liu, Tao Luo, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05242">https://arxiv.org/abs/2502.05242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05242">https://arxiv.org/pdf/2502.05242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05242]] SEER: Self-Explainability Enhancement of Large Language Models' Representations(https://arxiv.org/abs/2502.05242)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Explaining the hidden representations of Large Language Models (LLMs) is a perspective to understand LLMs' underlying inference logic and improve their reliability in application scenarios. However, previous methods introduce external ''black-box'' modules to explain ''black-box'' LLMs, increasing the potential uncertainty and failing to provide faithful explanations. In this paper, we propose a self-explaining method SEER, enhancing LLMs' explainability by aggregating the same concept and disentangling the different concepts in the representation space. In this way, SEER provides faithful explanations carried by representations synchronously with the LLMs' output. Additionally, we showcase the applications of SEER on trustworthiness-related tasks (e.g., the safety risks classification and detoxification tasks), where self-explained LLMs achieve consistent improvement in explainability and performance. More crucially, we theoretically analyze the improvement of SEER on LLMs' generalization ability through optimal transport theory.</li>
<li><strong>摘要：</strong>解释大语言模型（LLMS）的隐藏表示形式是了解LLMS的基本推理逻辑并提高其在应用程序方案中的可靠性的观点。但是，以前的方法介绍了外部的“黑盒”“模块”来解释“黑框” LLM，从而增加了潜在的不确定性并无法提供忠实的解释。在本文中，我们提出了一种自我解释的方法，通过汇总相同的概念并解散表示空间中不同概念的方法来增强LLMS的解释性。这样，Seer提供了由LLMS的输出同步表示的忠实解释。此外，我们展示了SEER在与可信度相关的任务（例如，安全风险分类和排毒任务）上的应用，在这些任务中，自我解释的LLMS可以在解释性和性能方面保持一致的改善。更重要的是，我们从理论上分析了通过最佳运输理论对LLMS的概括能力的改进。</li>
</ul>

<h3>Title: Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires</h3>
<ul>
<li><strong>Authors: </strong>Pranav Bhandari, Usman Naseem, Amitava Datta, Nicolas Fay, Mehwish Nasim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05248">https://arxiv.org/abs/2502.05248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05248">https://arxiv.org/pdf/2502.05248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05248]] Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires(https://arxiv.org/abs/2502.05248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.</li>
<li><strong>摘要：</strong>心理评估工具长期以来帮助人类了解行为模式。尽管大型语言模型（LLM）可以生成与人类相当的内容，但我们探索它们是否表现出人格特征。为此，这项工作将心理工具应用于LLM，以产生人格概况。使用既定的基于特质的问卷，例如五巨头库存，并通过解决培训数据污染的可能性，我们研究了LLM在五个核心人格方面的维度变异性和优势：开放性，尽责，良心，外向性，同意和神经质性。我们的发现表明，LLM在同一模型家族中也表现出独特的主导性状，不同的特征和独特的人格概况。</li>
</ul>

<h3>Title: GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context Length and Reasoning Complexity?</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05252">https://arxiv.org/abs/2502.05252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05252">https://arxiv.org/pdf/2502.05252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05252]] GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context Length and Reasoning Complexity?(https://arxiv.org/abs/2502.05252)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Long-context large language models (LLMs) have recently shown strong performance in information retrieval and long-document QA. However, to tackle the most challenging intellectual problems, LLMs must reason effectively in long and complex contexts (e.g., frontier mathematical research). Studying how LLMs handle increasing reasoning complexity and context length is essential, yet existing benchmarks lack a solid basis for quantitative evaluation. Inspired by the abstraction of GSM-8K problems as computational graphs, and the ability to introduce noise by adding unnecessary nodes and edges, we develop a grade school math problem generator capable of producing arithmetic problems with infinite difficulty and context length under fine-grained control. Using our newly synthesized GSM-Infinite benchmark, we comprehensively evaluate existing LLMs. We find a consistent sigmoid decline in reasoning performance as complexity increases, along with a systematic inference scaling trend: exponentially increasing inference computation yields only linear performance gains. These findings underscore the fundamental limitations of current long-context LLMs and the key challenges in scaling reasoning capabilities. Our GSM-Infinite benchmark provides a scalable and controllable testbed for systematically studying and advancing LLM reasoning in long and complex contexts.</li>
<li><strong>摘要：</strong>长篇小说大语模型（LLM）最近在信息检索和长期文档质量检查中表现出强烈的性能。但是，要解决最具挑战性的智力问题，LLM必须在漫长而复杂的环境中有效地理解（例如边境数学研究）。研究LLM如何处理提高的推理复杂性和上下文长度是必不可少的，但是现有的基准缺乏定量评估的坚实基础。受到GSM-8K问题作为计算图的抽象的启发，以及通过添加不必要的节点和边缘引入噪音的能力，我们开发了一个小学的数学问题生成器，能够在细粒度的控制下产生无限难度和上下文的算术问题。使用我们新合成的GSM侵入基准测试，我们全面评估了现有的LLM。我们发现，随着复杂性的提高，推理性能的一致性下降，以及系统的推理缩放趋势：指数增加的推理计算只能获得线性性能提高。这些发现强调了当前长篇小说LLM的基本局限性以及扩展推理能力的关键挑战。我们的GSM Infinite基准提供了一个可扩展且可控制的测试台，用于在漫长而复杂的环境中系统地研究和推进LLM推理。</li>
</ul>

<h3>Title: LLMs Can Teach Themselves to Better Predict the Future</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Turtel, Danny Franklin, Philipp Schoenegger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05253">https://arxiv.org/abs/2502.05253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05253">https://arxiv.org/pdf/2502.05253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05253]] LLMs Can Teach Themselves to Better Predict the Future(https://arxiv.org/abs/2502.05253)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples. Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date. We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO). On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7--10\% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-4o.</li>
<li><strong>摘要：</strong>我们提出了一个以结果为导向的微调框架，可增强大语模型（LLM）的预测能力，而无需依赖人类策划的推理样本。我们的方法利用模型自我播放来生成一对多样的推理轨迹和概率预测，用于在模型的知识截止日期之后解决的一系列不同问题。然后，我们将这些推理跟踪的对成对通过它们到通过直接偏好优化（DPO）微调模型之前的实际结果的距离对。在单独的测试集中，我们的方法在基本模型上增加了PHI-4 14B和DeepSeek-R1 14B的预测准确性在7--10 \％之间，并带有随机标签的DPO微型控制模型诸如GPT-4O之类的更大边界模型的预测功能。</li>
</ul>

<h3>Title: Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet</h3>
<ul>
<li><strong>Authors: </strong>Berk Atil, Vipul Gupta, Sarkar Snigdha Sarathi Das, Rebecca J. Passonneau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05291">https://arxiv.org/abs/2502.05291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05291">https://arxiv.org/pdf/2502.05291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05291]] Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet(https://arxiv.org/abs/2502.05291)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become ubiquitous, thus it is important to understand their risks and limitations. Smaller LLMs can be deployed where compute resources are constrained, such as edge devices, but with different propensity to generate harmful output. Mitigation of LLM harm typically depends on annotating the harmfulness of LLM output, which is expensive to collect from humans. This work studies two questions: How do smaller LLMs rank regarding generation of harmful content? How well can larger LLMs annotate harmfulness? We prompt three small LLMs to elicit harmful content of various types, such as discriminatory language, offensive content, privacy invasion, or negative influence, and collect human rankings of their outputs. Then, we evaluate three state-of-the-art large LLMs on their ability to annotate the harmfulness of these responses. We find that the smaller models differ with respect to harmfulness. We also find that large LLMs show low to moderate agreement with humans. These findings underline the need for further work on harm mitigation in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已无处不在，因此了解其风险和局限性很重要。较小的LLM可以在计算资源受到限制（例如边缘设备）的情况下部署，但具有不同的倾向以产生有害输出。缓解LLM危害通常取决于注释LLM输出的有害性，LLM输出的危害是昂贵的。这项工作研究了两个问题：较小的LLM如何在产生有害内容的情况下排名？更大的LLM可以注释有害性吗？我们促使三个小型LLM引起各种类型的有害内容，例如歧视性语言，进攻性内容，隐私入侵或负面影响，并收集其产量的人类排名。然后，我们评估了三个最先进的大型LLM，以注释这些反应的有害性。我们发现较小的模型在有害性方面有所不同。我们还发现，大型LLM与人类表现出低至中等的一致性。这些发现强调了在LLM中进行进一步减轻损害的进一步工作的必要性。</li>
</ul>

<h3>Title: Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books</h3>
<ul>
<li><strong>Authors: </strong>Sangmitra Madhusudan, Robert Morabito, Skye Reid, Nikta Gohari Sadr, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05331">https://arxiv.org/abs/2502.05331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05331">https://arxiv.org/pdf/2502.05331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05331]] Fine-Tuned LLMs are "Time Capsules" for Tracking Societal Bias Through Books(https://arxiv.org/abs/2502.05331)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Books, while often rich in cultural insights, can also mirror societal biases of their eras - biases that Large Language Models (LLMs) may learn and perpetuate during training. We introduce a novel method to trace and quantify these biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising 593 fictional books across seven decades (1950-2019), to track bias evolution. By fine-tuning LLMs on books from each decade and using targeted prompts, we examine shifts in biases related to gender, sexual orientation, race, and religion. Our findings indicate that LLMs trained on decade-specific books manifest biases reflective of their times, with both gradual trends and notable shifts. For example, model responses showed a progressive increase in the portrayal of women in leadership roles (from 8% to 22%) from the 1950s to 2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly aligning with third-wave feminism. Same-sex relationship references increased markedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+ visibility. Concerningly, negative portrayals of Islam rose sharply in the 2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we demonstrate that these biases stem mainly from the books' content and not the models' architecture or initial training. Our study offers a new perspective on societal bias trends by bridging AI, literary studies, and social science research.</li>
<li><strong>摘要：</strong>书籍虽然经常富含文化见解，但也可以反映出他们时代的社会偏见 - 大型语言模型（LLMS）可能在培训期间学习和永久化。我们介绍了一种新的方法，可以使用微调的LLM追踪和量化这些偏见。我们开发了书本，这是一个在七十年（1950-2019）中包含593本虚构书籍的语料库，以跟踪偏见的演变。通过对每十年的书籍进行微调LLM，并使用有针对性的提示，我们研究了与性别，性取向，种族和宗教有关的偏见的转变。我们的发现表明，经过十年特定书籍训练的LLM表现出反映了他们时代的偏见，既有逐渐的趋势又有明显的转变。例如，模型的回答表明，从1950年代到2010年代，女性在领导角色中的描绘逐渐增加，在1990年代（4％增加到12％），可能与之一致，可能与第三波女权主义。从1980年代到2000年代（从0％到10％），同性关系的参考文献明显增加，反映了LGBTQ+可见性的增长。关于伊斯兰教的负面描述在2000年代（26％至38％），可能反映了9/11之后的情绪。重要的是，我们证明这些偏见主要源于书籍的内容，而不是模型的体系结构或初始培训。我们的研究通过桥接AI，文学研究和社会科学研究来对社会偏见趋势有了新的看法。</li>
</ul>

<h3>Title: Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Christopher Nightingale, Dominic Lavington, Jonathan Thistlethwaite, Sebastian Penhaligon, Thomas Belinski, David Boldo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05346">https://arxiv.org/abs/2502.05346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05346">https://arxiv.org/pdf/2502.05346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05346]] Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models(https://arxiv.org/abs/2502.05346)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Representing token embeddings as probability distributions over learned manifolds allows for more flexible contextual inference, reducing representational rigidity while enhancing semantic granularity. Comparative evaluations demonstrate that probabilistic embeddings improve neighborhood consistency and decrease redundancy, ensuring that token relationships remain more structurally coherent across fine-tuning iterations. The integration of probabilistic subspaces within attention mechanisms facilitates more adaptive contextual weighting, enabling models to capture latent dependencies that would otherwise be obscured in conventional embeddings. Experimental results highlight increased robustness against adversarial modifications, with probabilistic embeddings preserving contextual integrity even under perturbation-based evaluation scenarios. Performance assessments indicate that probabilistic representations achieve greater adaptability in domain-specific applications, mitigating the need for extensive retraining when shifting across linguistic domains. Computational trade-offs remain within operationally feasible limits, with marginal increases in inference latency balanced against the benefits of enhanced representation stability and contextual expressiveness. The capacity to encode structured uncertainty provides advantages in generative modeling tasks, particularly where maintaining coherence across extended sequences requires a representation framework capable of handling ambiguous or context-dependent linguistic constructs.</li>
<li><strong>摘要：</strong>将令牌嵌入表示为学习的歧管上的概率分布可以使更灵活的上下文推断，从而降低表示刚性的同时增强语义粒度。比较评估表明，概率嵌入可以提高邻域的一致性和降低的冗余，从而确保令牌关系在微调迭代中在结构上保持更连贯。概率子空间在注意机制内的整合促进了更适应性的上下文加权，从而使模型能够捕获常规嵌入中否则会遮盖的潜在依赖性。实验结果强调了对对抗性修饰的鲁棒性提高，即使在基于扰动的评估方案下，概率嵌入也可以保留上下文完整性。绩效评估表明，概率表示在特定领域的应用中实现了更大的适应性，从而减轻了跨语言领域转移时进行广泛重新训练的需求。计算权衡仍保持在操作可行的限制范围内，而推断潜伏期的边缘延长与增强的表示稳定性和上下文表现力的好处的边缘增加。编码结构不确定性的能力在生成建模任务中提供了优势，尤其是在扩展序列之间保持连贯性的情况下，需要一个能够处理模棱两可或与上下文相关的语言结构的表示框架。</li>
</ul>

<h3>Title: Learning Task Representations from In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Baturay Saglam, Zhuoran Yang, Dionysis Kalogerias, Amin Karbasi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05390">https://arxiv.org/abs/2502.05390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05390">https://arxiv.org/pdf/2502.05390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05390]] Learning Task Representations from In-Context Learning(https://arxiv.org/abs/2502.05390)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable proficiency in in-context learning (ICL), where models adapt to new tasks through example-based prompts without requiring parameter updates. However, understanding how tasks are internally encoded and generalized remains a challenge. To address some of the empirical and technical gaps in the literature, we introduce an automated formulation for encoding task information in ICL prompts as a function of attention heads within the transformer architecture. This approach computes a single task vector as a weighted sum of attention heads, with the weights optimized causally via gradient descent. Our findings show that existing methods fail to generalize effectively to modalities beyond text. In response, we also design a benchmark to evaluate whether a task vector can preserve task fidelity in functional regression tasks. The proposed method successfully extracts task-specific information from in-context demonstrations and excels in both text and regression tasks, demonstrating its generalizability across modalities. Moreover, ablation studies show that our method's effectiveness stems from aligning the distribution of the last hidden state with that of an optimally performing in-context-learned model.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）已经表现出了非常熟练的熟练程度，其中模型通过基于示例的提示来适应新任务而无需参数更新。但是，了解内部编码和广义的任务是一个挑战。为了解决文献中一些经验和技术差距，我们引入了一种自动配方，以在ICL提示中编码任务信息，以作为变压器体系结构中注意力头的函数。这种方法将单个任务向量计算为加权的注意力头，而权重通过梯度下降优化了。我们的发现表明，现有方法无法有效地推广到文本以外的模式。作为回应，我们还设计了一个基准，以评估任务向量是否可以保留功能回归任务中的任务保真度。所提出的方法成功地从文本和回归任务中脱颖而出，成功提取了特定于任务的信息，并在文本和回归任务中表现出色，从而证明了其跨模式的普遍性。此外，消融研究表明，我们的方法的有效性源于将最后一个隐藏状态与最佳执行的内部文化学习模型的分布对齐。</li>
</ul>

<h3>Title: Hierarchical Lexical Manifold Projection in Large Language Models: A Novel Mechanism for Multi-Scale Semantic Representation</h3>
<ul>
<li><strong>Authors: </strong>Natasha Martus, Sebastian Crowther, Maxwell Dorrington, Jonathan Applethwaite, Edgar Tillinghurst, Quentin Birkenshaw, Lukas Petrov, Constance Willoughby</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05395">https://arxiv.org/abs/2502.05395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05395">https://arxiv.org/pdf/2502.05395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05395]] Hierarchical Lexical Manifold Projection in Large Language Models: A Novel Mechanism for Multi-Scale Semantic Representation(https://arxiv.org/abs/2502.05395)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The integration of structured hierarchical embeddings into transformer-based architectures introduces a refined approach to lexical representation, ensuring that multi-scale semantic relationships are preserved without compromising computational efficiency. A projection mechanism that maps tokens onto a structured manifold provides improved lexical alignment, enhancing the adaptability of word representations across diverse linguistic tasks. The structured encoding framework ensures that hierarchical embeddings maintain coherence across varying abstraction levels, allowing for stable transitions between localized syntactic features and global semantic structures. Experimental evaluations indicate that hierarchical embeddings consistently outperform conventional token representations, improving accuracy in linguistic benchmarks while maintaining lower computational overhead. Comparative analysis across multiple domains highlights the ability of hierarchical embeddings to retain contextual consistency, particularly in specialized language applications where structured lexical alignment is essential. Statistical assessments further demonstrate that hierarchical embeddings exhibit enhanced robustness under perturbation conditions, ensuring that linguistic structures remain stable across adversarial text modifications. The integration of hierarchical projections with transformer attention mechanisms enables improved contextual adaptation, ensuring that token representations are dynamically adjusted based on varying linguistic distributions. The refined hierarchical organization of embeddings provides greater interpretability in lexical modeling, facilitating enhanced generalization capabilities across diverse text processing tasks.</li>
<li><strong>摘要：</strong>将结构化层次嵌入到基于变压器的架构中的结构化层次嵌入引入了一种精致的词汇表示方法，以确保在不损害计算效率的情况下保留多尺度的语义关系。将代币映射到结构化歧管上的投影机制提供了改进的词汇对准，从而增强了各种语言任务跨越单词表示的适应性。结构化编码框架可确保层次嵌入在不同的抽象水平之间保持连贯性，从而可以在局部句法特征和全局语义结构之间进行稳定的过渡。实验评估表明，层次嵌入始终优于常规令牌表示，提高了语言基准的准确性，同时保持较低的计算开销。跨多个领域的比较分析突出了层次嵌入保持上下文一致性的能力，尤其是在结构化词汇比对的专业语言应用中。统计评估进一步表明，层次嵌入在扰动条件下表现出增强的鲁棒性，从而确保语言结构在对抗性文本修改中保持稳定。将层次投影与变压器注意机制的集成能够改善上下文适应，从而确保基于不同语言分布动态调整令牌表示。精致的嵌入层次结构组织在词汇建模中提供了更大的可解释性，从而促进了各种文本处理任务的增强概括能力。</li>
</ul>

<h3>Title: Dynamic Noise Preference Optimization for LLM Self-Improvement via Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Haoyan Yang, Ting Hua, Shangqian Gao, Binfeng Xu, Zheng Tang, Jie Xu, Hongxia Jin, Vijay Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05400">https://arxiv.org/abs/2502.05400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05400">https://arxiv.org/pdf/2502.05400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05400]] Dynamic Noise Preference Optimization for LLM Self-Improvement via Synthetic Data(https://arxiv.org/abs/2502.05400)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Although LLMs have achieved significant success, their reliance on large volumes of human-annotated data has limited their potential for further scaling. In this situation, utilizing self-generated synthetic data has become crucial for fine-tuning LLMs without extensive human annotation. However, current methods often fail to ensure consistent improvements across iterations, with performance stagnating after only minimal updates. To overcome these challenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO employs a dynamic sample labeling mechanism to construct preference pairs for training and introduces controlled, trainable noise into the preference optimization process. Our approach effectively prevents stagnation and enables continuous improvement. In experiments with Zephyr-7B, DNPO consistently outperforms existing methods, showing an average performance boost of 2.6% across multiple benchmarks. Additionally, DNPO shows a significant improvement in model-generated data quality, with a 29.4% win-loss rate gap compared to the baseline in GPT-4 evaluations. This highlights its effectiveness in enhancing model performance through iterative refinement.</li>
<li><strong>摘要：</strong>尽管LLM取得了巨大的成功，但它们对大量人类注销数据的依赖限制了其进一步扩展的潜力。在这种情况下，利用自我生成的合成数据对于无广泛的人类注释而对微调LLM至关重要。但是，当前的方法通常无法确保遍及迭代的持续改进，并且只有最小的更新后，性能就停滞不前。为了克服这些挑战，我们引入了动态噪声偏好优化（DNPO）。 DNPO采用动态样本标记机制来构建训练的偏好对，并将受控的，可训练的噪声引入偏好优化过程。我们的方法有效地阻止了停滞，并可以持续改进。在使用Zephyr-7B的实验中，DNPO始终胜过现有方法，显示多个基准测试的平均性能提高2.6％。此外，与GPT-4评估中的基线相比，DNPO在模型生成的数据质量方面显示出显着改善，并具有29.4％的损失率差距。这突出了其通过迭代改进来增强模型性能的有效性。</li>
</ul>

<h3>Title: SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Xingtong Yu, Zechuan Gong, Chang Zhou, Yuan Fang, Hui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05424">https://arxiv.org/abs/2502.05424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05424">https://arxiv.org/pdf/2502.05424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05424]] SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation(https://arxiv.org/abs/2502.05424)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graphs from different domains often exhibit divergent characteristics. Some studies leverage large language models to align multiple domains based on textual descriptions associated with the graphs, limiting their applicability to text-attributed graphs. For text-free graphs, a few recent works attempt to align different feature distributions across domains, while generally neglecting structural differences. In this work, we propose a novel Structure Alignment framework for text-free Multi-domain Graph Pre-Training and cross-domain adaptation (SAMGPT). It is designed to learn multi-domain knowledge from graphs originating in multiple source domains, which can then be adapted to address applications in an unseen target domain. Specifically, we introduce a set of structure tokens to harmonize structure-based aggregation across source domains during the pre-training phase. Next, for cross-domain adaptation, we design dual prompts, namely, holistic prompts and specific prompts, which adapt unified multi-domain structural knowledge and fine-grained, domain-specific information, respectively, to a target domain. Finally, we conduct comprehensive experiments on seven public datasets to evaluate and analyze the effectiveness of SAMGPT.</li>
<li><strong>摘要：</strong>图形能够在许多在线服务中建模互连实体，从而支持网络上的广泛应用程序。这提出了一个重要的问题：我们如何在多个源域上训练图基础模型并适应看不见的目标域？一个主要的障碍是，来自不同领域的图通常表现出不同的特征。一些研究利用大型语言模型根据与图形相关的文本描述对齐多个域，从而将其适用性限制在文本属性图中。对于无文本图，最近的一些作品试图将不同的特征分布在范围内对齐，同时通常忽略了结构差异。在这项工作中，我们为无文本多域图预训练和跨域适应性（SAMGPT）提出了一个新颖的结构对齐框架。它旨在从源自多个源域中的图表中学习多域知识，然后可以将其调整以解决看不见的目标域中的应用程序。具体而言，我们引入了一组结构代币，以在训练阶段跨源域的基于结构的聚集协调。接下来，对于跨域的适应，我们设计了双重提示，即整体提示和特定提示，这些提示和特定的提示分别适应了统一的多域结构知识和精细元素的特定领域特定信息，以适应目标域。最后，我们对七个公共数据集进行了全面的实验，以评估和分析SAMGPT的有效性。</li>
</ul>

<h3>Title: Iterative Deepening Sampling for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weizhe Chen, Sven Koenig, Bistra Dilkina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05449">https://arxiv.org/abs/2502.05449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05449">https://arxiv.org/pdf/2502.05449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05449]] Iterative Deepening Sampling for Large Language Models(https://arxiv.org/abs/2502.05449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The recent release of OpenAI's o1 models and other similar frameworks showcasing test-time scaling laws has demonstrated their exceptional capability to tackle complex reasoning tasks. Inspired by this, subsequent research has revealed that such test-time scaling laws hinge on the model's ability to search both within a single response (intra-response) and across multiple responses (inter-response) during training. Crucially, beyond selecting a single optimal response, the model must also develop robust self-correction capabilities within its own outputs. However, training models to achieve effective self-evaluation and self-correction remains a significant challenge, heavily dependent on the quality of self-reflection data. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving, which can subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how manually triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.</li>
<li><strong>摘要：</strong>OpenAI的O1模型和其他类似框架的最新发布显示了测试时间缩放定律，这表明了他们应对复杂推理任务的非凡能力。在此的启发下，随后的研究表明，这种测试时间缩放定律取决于模型在单个响应中搜索的能力（响应内响应）以及训练过程中多个响应（响应间）的能力。至关重要的是，除了选择单个最佳响应外，该模型还必须在其自己的输出内开发出强大的自我纠正功能。但是，实现有效的自我评估和自我纠正的培训模型仍然是一个重大挑战，在很大程度上取决于自我反射数据的质量。在本文中，我们通过专注于提高复杂问题解决的自我反思数据生成的质量来应对这一挑战，这随后可以改善对下一代大语言模型（LLMS）的培训。具体而言，我们探讨了手动触发模型的自我纠正机制如何改善挑战性推理任务的性能。为此，我们提出了一种新型的迭代加深采样算法框架，旨在增强自我纠正并生成更高质量的样品。通过对Math500和AIME基准测试的广泛实验，我们证明了我们的方法在困难任务上取得了更高的成功率，并提供了详细的消融研究，以分析其在不同环境中的有效性。</li>
</ul>

<h3>Title: Position: LLMs Can be Good Tutors in Foreign Language Education</h3>
<ul>
<li><strong>Authors: </strong>Jingheng Ye, Shen Wang, Deqing Zou, Yibo Yan, Kun Wang, Hai-Tao Zheng, Zenglin Xu, Irwin King, Philip S. Yu, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05467">https://arxiv.org/abs/2502.05467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05467">https://arxiv.org/pdf/2502.05467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05467]] Position: LLMs Can be Good Tutors in Foreign Language Education(https://arxiv.org/abs/2502.05467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>While recent efforts have begun integrating large language models (LLMs) into foreign language education (FLE), they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that LLMs have the potential to serve as effective tutors in FLE. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing FLE through the thoughtful integration of LLMs.</li>
<li><strong>摘要：</strong>尽管最近的努力已经开始将大型语言模型（LLM）纳入外语教育（​​FLE），但他们通常依靠传统的方法来学习任务而不完全接受教育方法，因此缺乏对语言学习的适应性。为了解决这一差距，我们认为LLM有可能充当FLE的有效导师。具体而言，LLM可以扮演三个关键角色：（1）作为数据增强器，改善学习材料的创建或作为学生模拟的服务； （2）作为任务预测指标，作为学习者评估或优化学习途径； （3）作为代理商，可以实现个性化和包容性教育。我们鼓励跨学科的研究探索这些角色，促进创新，同时解决挑战和风险，最终通过LLM的周到整合而发展。</li>
</ul>

<h3>Title: OntoTune: Ontology-Driven Self-training for Aligning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Liu, Chengtao Gan, Junjie Wang, Yichi Zhang, Zhongpu Bo, Mengshu Sun, Huajun Chen, Wen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05478">https://arxiv.org/abs/2502.05478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05478">https://arxiv.org/pdf/2502.05478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05478]] OntoTune: Ontology-Driven Self-training for Aligning Large Language Models(https://arxiv.org/abs/2502.05478)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using ontology with hierarchical conceptual knowledge to reorganize LLM's domain knowledge. From this perspective, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept's ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. The code and data are available at this https URL.</li>
<li><strong>摘要：</strong>现有特定领域的大语言模型（LLMS）通常是通过具有大规模领域特异性语料库的微调通用LLM来开发的。但是，对大规模语料库的培训通常无法有效地组织LLM的领域知识，从而导致理解分散。受到人类如何联系概念并通过思维图组织知识的启发，我们旨在通过使用层次概念知识的本体论来模仿这种方法，以重新组织LLM的领域知识。从这个角度来看，我们提出了一个名为Ontotune的本体论驱动的自我训练框架，该框架旨在通过文化学习与本体学相结合，从而使本体学指导的响应产生能够产生。我们利用文章学习确定LLM是否已获得了特定概念的本体知识，并选择尚未掌握LLM掌握的条目作为培训设置，以进一步使LLM与本体论保持一致。与现有的域LLM相比，基于新收集的大规模领域特异性语料库，我们的本质依赖于现有的长期发展本体论和LLM本身，大大降低了数据维护成本，并提供了提高的概括能力。我们在医学领域进行研究，以评估原本体的标准化本体，SNOMED CT作为我们的本体来源。实验结果表明，Ontotune在内外任务Hipernym Discovery和Ondymology Task Medical域QA中都取得了最先进的表现。此外，与最新的直接本体注入方法类taxollama相比，我们的本文更好地保留了LLM的原始知识。该代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Mechanistic Interpretability of Emotion Inference in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ala N. Tak, Amin Banayeeanzade, Anahita Bolourani, Mina Kian, Robin Jia, Jonathan Gratch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05489">https://arxiv.org/abs/2502.05489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05489">https://arxiv.org/pdf/2502.05489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05489]] Mechanistic Interpretability of Emotion Inference in Large Language Models(https://arxiv.org/abs/2502.05489)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory, a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在预测文本中的人类情绪方面表现出了有希望的能力。但是，这些模型处理情绪刺激的机制在很大程度上尚未探索。我们的研究通过调查自回归的LLM如何推断情绪来解决这一差距，这表明情绪表示在功能上位于模型中的特定区域。我们的评估包括各种模型系列和大小，并得到鲁棒性检查的支持。然后，我们表明，通过借鉴认知评估理论，确定的表征在心理上是合理的，这是一个公认的心理框架，表明情绪从对环境刺激的评估（评估）中浮现出来。通过因果关系介入解释的评估概念，我们引导了一代，并表明输出与理论和直观的期望保持一致。这项工作突出了一种新颖的方法来进行因果干预和精确塑造情感文本的生成，有可能使敏感情感领域的安全性和对齐方式受益。</li>
</ul>

<h3>Title: DeepThink: Aligning Language Models with Domain-Specific User Intents</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Mingxuan Luo, Yeyun Gong, Chen Lin, Jian Jiao, Yi Liu, Kaili Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05497">https://arxiv.org/abs/2502.05497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05497">https://arxiv.org/pdf/2502.05497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05497]] DeepThink: Aligning Language Models with Domain-Specific User Intents(https://arxiv.org/abs/2502.05497)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning with synthesized instructions has been a common practice for adapting LLMs to domain-specific QA tasks. However, the synthesized instructions deviate from real user questions and expected answers. This study proposes a novel framework called DeepThink to generate high-quality instructions. DeepThink first generates a few seed questions to mimic actual user questions, simulates conversations to uncover the hidden user needs, and refines the answer by conversational contexts and the retrieved documents for more comprehensive answers. Experiments demonstrate that DeepThink achieves an average performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based assistant on the real user test set in the advertising domain across dimensions such as relevance, completeness, clarity, accuracy, and actionability.</li>
<li><strong>摘要：</strong>通过合成指令进行的监督微调是将LLMS调整为特定域质量质量质量标准任务的普遍做法。但是，综合说明偏离了真实的用户问题和预期答案。这项研究提出了一个名为DeepThink的新型框架，以产生高质量的说明。 DeepThink首先产生一些种子问题来模仿实际的用户问题，模拟对话以发现隐藏的用户需求，并通过对话上下文和检索文档来完善答案，以获取更全面的答案。实验表明，与GPT-4-Turbo+抹布的助手相比，DeepThink的平均绩效提高了7.92％，在广告域中的真实用户测试集中的助手（例如相关性，完整性，清晰度，准确性和可行性）。</li>
</ul>

<h3>Title: FRAMES: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy</h3>
<ul>
<li><strong>Authors: </strong>Xuemiao Zhang, Feiyu Duan, Liangyu Xu, Yongwei Zhou, Sirui Wang, Rongxiang Weng, Jingang Wang, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05551">https://arxiv.org/abs/2502.05551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05551">https://arxiv.org/pdf/2502.05551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05551]] FRAMES: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy(https://arxiv.org/abs/2502.05551)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced human language understanding and generation, with pretraining data quality and organization being crucial to their performance. Multi-stage pretraining is a promising approach, but existing methods often lack quantitative criteria for data partitioning and instead rely on intuitive heuristics. In this paper, we propose the novel Four-quadRAnt Multi-stage prEtraining Strategy (FRAMES), guided by the established principle of organizing the pretraining process into four stages to achieve significant loss reductions four times. This principle is grounded in two key findings: first, training on high Perplexity (PPL) data followed by low PPL data, and second, training on low PPL difference (PD) data followed by high PD data, both causing the loss to drop significantly twice and performance enhancements. By partitioning data into four quadrants and strategically organizing them, FRAMES achieves a remarkable 16.8% average improvement over random sampling across MMLU and CMMLU, effectively boosting LLM performance.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）具有显着的人类语言理解和产生，并且预处理数据质量和组织对其表现至关重要。多阶段预处理是一种有希望的方法，但是现有的方法通常缺乏用于数据分配的定量标准，而是依靠直观的启发式方法。在本文中，我们提出了新型的四季度多阶段预训练策略（框架），该策略以既定的原理将预训练的过程分为四个阶段，以四次大幅损失减少。该原理以两个关键发现为基础：首先，对高困惑（PPL）数据进行培训，然后是低PPL数据，其次，对低PPL差异（PD）数据进行培训，然后是高PD数据，都导致损失显着下降两次和性能增强。通过将数据划分为四个象限，并从战略上组织它们，框架比MMLU和CMMLU的随机抽样实现了显着的16.8％的平均改善，从而有效地提高了LLM的性能。</li>
</ul>

<h3>Title: Latent Structure Modulation in Large Language Models Through Stochastic Concept Embedding Transitions</h3>
<ul>
<li><strong>Authors: </strong>Stefan Whitaker, Colin Sisate, Marcel Windsor, Nikolai Fairweather, Tarquin Goldborough, Oskar Lindenfeld</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05553">https://arxiv.org/abs/2502.05553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05553">https://arxiv.org/pdf/2502.05553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05553]] Latent Structure Modulation in Large Language Models Through Stochastic Concept Embedding Transitions(https://arxiv.org/abs/2502.05553)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Stochastic embedding transitions introduce a probabilistic mechanism for adjusting token representations dynamically during inference, mitigating the constraints imposed through static or deterministic embeddings. A transition framework was proposed in which each token embedding evolved through probabilistic updates, ensuring adaptability while preserving semantic integrity across linguistic contexts. Empirical evaluations demonstrated that models incorporating stochastic transitions exhibited greater lexical diversity, improved generative coherence, and enhanced retention of low-frequency vocabulary, contributing to more varied sentence structures and reduced reliance on high-probability token selections. Statistical analyses of embedding drift across transformer layers indicated that representations evolved more flexibly without losing coherence, supporting the hypothesis that controlled stochasticity facilitated context-sensitive representation learning. Experimental results revealed that probabilistic embeddings introduced minor computational overhead while maintaining generative efficiency, reinforcing their feasibility in large-scale applications. A comparative study with traditional embedding approaches highlighted measurable gains in text completion accuracy, dialogue coherence, and structural complexity, confirming the effectiveness of stochastic transitions in enhancing representation expressiveness. Clustering patterns in the embedding space suggested that probabilistic updates preserved meaningful semantic groupings while enabling context-driven shifts, further validating the stability of the transition mechanism. Performance metrics indicated that stochastic transitions balanced adaptability and control, ensuring that generative outputs remained linguistically coherent without excessive randomness.</li>
<li><strong>摘要：</strong>随机嵌入过渡引入了一种概率机制，用于在推理过程中动态调整令牌表示，从而减轻通过静态或确定性嵌入所施加的约束。提出了一个过渡框架，在该框架中，每个令牌嵌入通过概率更新进化，从而确保适应性，同时在语言环境中保持语义完整性。经验评估表明，结合随机过渡的模型表现出更大的词汇多样性，提高了生成性的连贯性，并增强了低频词汇的保留，从而有助于更多样化的句子结构，并降低了对高概率标记选择的依赖。嵌入跨变压器层漂移的统计分析表明，表示形式在不失去连贯性的情况下更灵活地演变，支持了控制随机性促进上下文敏感表示的学习的假设。实验结果表明，概率嵌入引入了较小的计算开销，同时保持生成效率，从而在大规模应用中加强了它们的可行性。一项与传统嵌入方法的比较研究突出了文本完成精度，对话连贯性和结构复杂性的可衡量提高，证实了随机转变在增强表示表现力方面的有效性。嵌入空间中的聚类模式表明，概率更新保留了有意义的语义分组，同时可以实现上下文驱动的转移，从而进一步验证了过渡机制的稳定性。性能指标表明，随机转变平衡的适应性和控制性，确保生成产量在语言上保持一致而没有过多随机性。</li>
</ul>

<h3>Title: ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Liu, Kangjie Bao, Jiashuo Zhang, Yunqi Liu, Yu Chen, Yuntian Liu, Yang Jiao, Tao Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05567">https://arxiv.org/abs/2502.05567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05567">https://arxiv.org/pdf/2502.05567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05567]] ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data(https://arxiv.org/abs/2502.05567)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Autoformalization, the process of automatically translating natural language mathematics into machine-verifiable formal language, has demonstrated advancements with the progress of large language models (LLMs). However, a key obstacle to further advancements is the scarcity of paired datasets that align natural language with formal language. To address this challenge, we introduce ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), an iterative data generation framework designed to produce large-scale, high-quality parallel theorem statements. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 300k theorem statements and develop the ATLAS translator, achieving accuracies of 80.59% (pass@8) and 92.99% (pass@128) on ProofNet, significantly outperforming the base model (23.99% and 47.17%) and InternLM2-Math-Plus-7B (50.94% and 80.32%). Furthermore, the ATLAS translator also achieves state-of-the-art performance on both the high-school-level miniF2F dataset and the graduate-level MathQual dataset introduced in this work. The datasets, model, and code will be released to the public soon.</li>
<li><strong>摘要：</strong>自动化是将自然语言数学自动转化为机器可验证的形式语言的过程，这证明了大型语言模型（LLMS）的进步的进步。但是，进一步进步的关键障碍是将自然语言与形式语言保持一致的配对数据集的稀缺性。为了应对这一挑战，我们介绍了Atlas（通过提升，增强和综合数据自动化定理），这是一个迭代数据生成框架，旨在生成大规模，高质量的平行定理语句。随着拟议的地图集进行10次迭代，我们构建了一个包括300K定理语句的本科级别的数据集，并开发了Atlas Translator，达到80.59％的精确度（Pass@8）和92.99％（Pass@8）和92.99％（Pass@128）（Pass@128），显着超越了表现优于实现的优势。基本型号（23.99％和47.17％）和InternLM2-Math-Plus-7b（50.94％和80.32％）。此外，ATLAS翻译人员还可以在这项工作中介绍的高中级minif2f数据集和研究生级数学数据集上实现最先进的性能。数据集，模型和代码将很快向公众发布。</li>
</ul>

<h3>Title: Large Multimodal Models for Low-Resource Languages: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Marian Lupascu, Ana-Cristina Rogoz, Mihai Sorin Stupariu, Radu Tudor Ionescu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05568">https://arxiv.org/abs/2502.05568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05568">https://arxiv.org/pdf/2502.05568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05568]] Large Multimodal Models for Low-Resource Languages: A Survey(https://arxiv.org/abs/2502.05568)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 106 studies across 75 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. We aim to provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: this https URL.</li>
<li><strong>摘要：</strong>在这项调查中，我们系统地分析了用于调整低资源（LR）语言的大型多模型模型（LMM）的技术，研究了从视觉增强和数据创建到跨模式传输和融合策略的方法。通过对跨75 LR语言的106项研究的全面分析，我们确定了研究人员如何应对有限数据和计算资源的挑战的关键模式。我们发现，视觉信息通常是改善LR设置模型性能的关键桥梁，尽管在幻觉缓解和计算效率等领域仍存在重大挑战。我们旨在为研究人员提供清楚的了解，以使LR（研究研究的）语言更容易使LMM更容易获得挑战。我们通过以下网址提供了一个开源存储库来补充我们的调查：此HTTPS URL。</li>
</ul>

<h3>Title: On Memory Construction and Retrieval for Personalized Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Jianfeng Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05589">https://arxiv.org/abs/2502.05589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05589">https://arxiv.org/pdf/2502.05589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05589]] On Memory Construction and Retrieval for Personalized Conversational Agents(https://arxiv.org/abs/2502.05589)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SeCom, a method that constructs a memory bank with topical segments by introducing a conversation Segmentation model, while performing memory retrieval based on Compressed memory units. Experimental results show that SeCom outperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such as LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.</li>
<li><strong>摘要：</strong>为了在长期对话中提供连贯和个性化的经验​​，现有方法通常通过在交流级，会话级或通过汇总技术中构建对话历史记录的记忆库来进行检索增强响应生成。在本文中，我们提出了两个关键发现：（1）内存单元的粒度重要：转交级，会话级和基于摘要的方法每种方法在内存检索准确性和检索到的内容的语义质量中都显示出限制。 （2）及时的压缩方法，例如\ textIt {llmlingua-2}，可以有效地充当一种降解机制，从而增强了不同粒度的内存检索准确性。在这些见解的基础上，我们提出了SECOM，该方法通过引入对话细分模型来构建具有主题段的内存库，同时根据压缩内存单元进行内存检索。实验结果表明，SECOM优于转向级别，会话级别以及几种基于长期对话基准的基于摘要的方法，例如Locomo和Long-Mt-Bench+。此外，提出的对话分割方法在对话分割数据集（例如Dialseg711，tiage和superdialseg）上表明了卓越的性能。</li>
</ul>

<h3>Title: ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yongcheng Zeng, Xinyu Cui, Xuanfa Jin, Guoqing Liu, Zexu Sun, Quan He, Dong Li, Ning Yang, Jianye Hao, Haifeng Zhang, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05605">https://arxiv.org/abs/2502.05605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05605">https://arxiv.org/pdf/2502.05605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05605]] ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization(https://arxiv.org/abs/2502.05605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions. However, even the most advanced models often face challenges in improving their outputs. In this paper, we explore how to cultivate LLMs with the self-refinement capability through iterative preference training, and how this ability can be leveraged to improve model performance during inference. To this end, we introduce a novel post-training and inference framework, called ARIES: Adaptive Refinement and Iterative Enhancement Structure. This method iteratively performs preference training and self-refinement-based data collection. During training, ARIES strengthen the model's direct question-answering capability while simultaneously unlocking its self-refinement potential. During inference, ARIES harnesses this self-refinement capability to generate a series of progressively refined responses, which are then filtered using either the Reward Model Scoring or a simple yet effective Rule-Based Selection mechanism, specifically tailored to our approach, to construct a dataset for the next round of preference training. Experimental results demonstrate the remarkable performance of ARIES. When applied to the Llama-3.1-8B model and under the self-refinement setting, ARIES surpasses powerful models such as GPT-4o, achieving 62.3% length-controlled (LC) and a 63.3% raw win rates on AlpacaEval 2, outperforming Iterative DPO by 27.8% and 35.5% respectively, as well as a 50.3% win rate on Arena-Hard, surpassing Iterative DPO by 26.6%. Furthermore, ARIES consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH.</li>
<li><strong>摘要：</strong>真正智能的大型语言模型（LLM）应该能够通过外部互动来纠正其响应中的错误。但是，即使是最先进的模型，也经常在改善其产出时面临挑战。在本文中，我们探讨了如何通过迭代偏好训练以自我填充能力来培养LLM，以及如何利用这种能力来提高推断期间的模型性能。为此，我们介绍了一种新型的训练后和推理框架，称为白羊座：自适应改进和迭代增强结构。这种方法迭代执行偏好培训和基于自我的数据收集。在训练过程中，Aries增强了该模型的直接提问能力，同时释放了其自我启动潜力。在推断期间，白羊座利用这种自我进行的能力来产生一系列逐步完善的响应，然后使用奖励模型评分或一种简单但有效的基于规则的选择机制（专门针对我们的方法量身定制）来构建数据集对其进行过滤。在下一轮偏好培训中。实验结果证明了白羊座的出色表现。当应用于Llama-3.1-8b模型并在自我进行设置下，白羊座超过了强大的模型，例如GPT-4O，达到62.3％的长度控制（LC）（LC）和63.3％的原始获胜率，均超过了Alpacaeval 2 DPO分别为27.8％和35.5％，在竞技场上获得50.3％的胜率，超过迭代DPO的26.6％。此外，白羊座始终提高数学推理任务（例如GSM8K和MATH）的性能。</li>
</ul>

<h3>Title: Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Sukmin Cho, Sangjin Choi, Taeho Hwang, Jeongyeon Seo, Soyeong Jeong, Huije Lee, Hoyun Song, Jong C. Park, Youngjin Kwon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05609">https://arxiv.org/abs/2502.05609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05609">https://arxiv.org/pdf/2502.05609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05609]] Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding(https://arxiv.org/abs/2502.05609)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的加速推断对于实时互动至关重要，因为它们已被广泛纳入现实世界中的服务。投机性解码是一种完全算法的解决方案，已通过起草和验证令牌来提高推理速度，从而在单个正向通道中生成多个令牌。但是，当前的起草策略通常需要进行大量的微调或在任务之间的性能不一致。为了应对这些挑战，我们提出了层次结构起草（HD），这是一种新颖的无损起草方法，将各种令牌源组织到基于时间上的层次结构框架中的多个数据库中。在起草步骤中，HD依次访问多个数据库，以从最高地点到最低的地方获得草稿令牌，从而确保跨不同任务的一致加速度一致，并最大程度地减少起草潜伏期。我们使用具有7B和13B参数的LLMS在Spec Bench上进行的实验表明，HD的表现优于现有的数据库制图方法，从而实现了跨模型大小，任务和温度的强大推理加速。</li>
</ul>

<h3>Title: Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Soham Poddar, Paramita Koley, Janardan Misra, Niloy Ganguly, Saptarshi Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05610">https://arxiv.org/abs/2502.05610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05610">https://arxiv.org/pdf/2502.05610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05610]] Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models(https://arxiv.org/abs/2502.05610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly recognized for their exceptional generative capabilities and versatility across various tasks. However, the high inference costs associated with these models have not received adequate attention, particularly when compared to the focus on training costs in existing research. In response to this gap, our study conducts a comprehensive benchmarking of LLM inference energy across a wide range of NLP tasks, where we analyze the impact of different models, tasks, prompts, and system-related factors on inference energy. Specifically, our experiments reveal several interesting insights, including strong correlation of inference energy with output token length and response time. Also, we find that quantization and optimal batch sizes, along with targeted prompt phrases, can significantly reduce energy usage. This study is the first to thoroughly benchmark LLM inference across such a diverse range of aspects, providing insights and offering several recommendations for improving energy efficiency in model deployment.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地因其在各种任务中的出色生成能力和多功能性而被认可。但是，与这些模型相关的高推理成本尚未得到足够的关注，尤其是与现有研究中培训成本的关注相比。为了应对这一差距，我们的研究在广泛的NLP任务中对LLM推理能量进行了全面的基准测试，我们在其中分析了不同模型，任务，提示和与系统相关因素对推理能量的影响。具体而言，我们的实验揭示了几个有趣的见解，包括推理能量与输出令牌长度和响应时间的密切相关性。同样，我们发现量化和最佳批量大小以及有针对性的及时短语可以显着降低能量使用。这项研究是第一个在各种各样的方面进行彻底基准的LLM推断，提供了见解并提供了一些建议，以提高模型部署的能源效率。</li>
</ul>

<h3>Title: AnyEdit: Edit Any Knowledge Encoded in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, Tat-seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05628">https://arxiv.org/abs/2502.05628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05628">https://arxiv.org/pdf/2502.05628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05628]] AnyEdit: Edit Any Knowledge Encoded in Language Models(https://arxiv.org/abs/2502.05628)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often produce incorrect or outdated information, necessitating efficient and precise knowledge updates. Current model editing methods, however, struggle with long-form knowledge in diverse formats, such as poetry, code snippets, and mathematical derivations. These limitations arise from their reliance on editing a single token's hidden state, a limitation we term "efficacy barrier". To solve this, we propose AnyEdit, a new autoregressive editing paradigm. It decomposes long-form knowledge into sequential chunks and iteratively edits the key token in each chunk, ensuring consistent and accurate outputs. Theoretically, we ground AnyEdit in the Chain Rule of Mutual Information, showing its ability to update any knowledge within LLMs. Empirically, it outperforms strong baselines by 21.5% on benchmarks including UnKEBench, AKEW, and our new EditEverything dataset for long-form diverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-play framework, enabling current editing methods to update knowledge with arbitrary length and format, significantly advancing the scope and practicality of LLM knowledge editing.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通常会产生不正确或过时的信息，需要有效而精确的知识更新。但是，当前的模型编辑方法但是，以各种格式（例如诗歌，代码片段和数学派生）的长期知识斗争。这些局限性源于它们依赖编辑单个令牌的隐藏状态，这是我们称为“功效障碍”的限制。为了解决这个问题，我们提出了新的自动回归编辑范式AnyeDit。它将长格式知识分解为顺序块，并迭代地编辑每个块中的键令牌，从而确保一致，准确的输出。从理论上讲，我们在相互信息的链条规则中扎根AnyEdit，显示了其更新LLM中任何知识的能力。从经验上讲，它在包括Unkebench，Akew和我们新的Editeverything数据集在内的基准上优于强大的基准，以提供多种多样的知识。此外，AnyEdit是一个插件框架，使当前的编辑方法以任意长度和格式更新知识，从而大大提高了LLM知识编辑的范围和实用性。</li>
</ul>

<h3>Title: ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports</h3>
<ul>
<li><strong>Authors: </strong>Aynur Guluzade, Naguib Heiba, Zeyd Boukhers, Florim Hamiti, Jahid Hasan Polash, Yehya Mohamad, Carlos A Velasco</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05638">https://arxiv.org/abs/2502.05638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05638">https://arxiv.org/pdf/2502.05638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05638]] ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports(https://arxiv.org/abs/2502.05638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Europe's healthcare systems require enhanced interoperability and digitalization, driving a demand for innovative solutions to process legacy clinical data. This paper presents the results of our project, which aims to leverage Large Language Models (LLMs) to extract structured information from unstructured clinical reports, focusing on patient history, diagnoses, treatments, and other predefined categories. We developed a workflow with a user interface and evaluated LLMs of varying sizes through prompting strategies and fine-tuning. Our results show that fine-tuned smaller models match or surpass larger counterparts in performance, offering efficiency for resource-limited settings. A new dataset of 60,000 annotated English clinical summaries and 24,000 German translations was validated with automated and manual checks. The evaluations used ROUGE, BERTScore, and entity-level metrics. The work highlights the approach's viability and outlines future improvements.</li>
<li><strong>摘要：</strong>欧洲的医疗保健系统需要增强的互操作性和数字化，从而促进了对处理传统临床数据的创新解决方案的需求。本文介绍了我们项目的结果，该项目旨在利用大型语言模型（LLMS）从非结构化的临床报告中提取结构化信息，重点是患者历史，诊断，治疗和其他预定义的类别。我们通过用户界面开发了一个工作流程，并通过提示策略和微调来评估不同大小的LLM。我们的结果表明，微调的较小型号匹配或超过较大的性能，为资源有限的设置提供了效率。通过自动和手动检查验证了60,000个带注释的英语临床摘要和24,000次德语翻译的新数据集。评估使用了Rouge，Bertscore和实体级指标。这项工作突出了该方法的可行性，并概述了未来的改进。</li>
</ul>

<h3>Title: Gender Bias in Instruction-Guided Speech Synthesis Models</h3>
<ul>
<li><strong>Authors: </strong>Chun-Yi Kuan, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05649">https://arxiv.org/abs/2502.05649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05649">https://arxiv.org/pdf/2502.05649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05649]] Gender Bias in Instruction-Guided Speech Synthesis Models(https://arxiv.org/abs/2502.05649)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in controllable expressive speech synthesis, especially in text-to-speech (TTS) models, have allowed for the generation of speech with specific styles guided by textual descriptions, known as style prompts. While this development enhances the flexibility and naturalness of synthesized speech, there remains a significant gap in understanding how these models handle vague or abstract style prompts. This study investigates the potential gender bias in how models interpret occupation-related prompts, specifically examining their responses to instructions like "Act like a nurse". We explore whether these models exhibit tendencies to amplify gender stereotypes when interpreting such prompts. Our experimental results reveal the model's tendency to exhibit gender bias for certain occupations. Moreover, models of different sizes show varying degrees of this bias across these occupations.</li>
<li><strong>摘要：</strong>最新的可控语音综合的进步，尤其是文本到语音（TTS）模型，允许以文本描述为指导（称为样式提示）的特定样式生成语音。尽管这种发展增强了综合语音的灵活性和自然性，但了解这些模型如何处理模糊或抽象样式提示仍然存在很大的差距。这项研究调查了模型如何解释与职业相关的提示的潜在性别偏见，特别检查了他们对“像护士”这样的指示的反应。我们探索这些模型在解释此类提示时是否表现出倾向于扩大性别刻板印象的趋势。我们的实验结果揭示了该模型对某些职业表现出性别偏见的趋势。此外，不同大小的模型在这些职业中显示出不同程度的这种偏见。</li>
</ul>

<h3>Title: Incongruence Identification in Eyewitness Testimony</h3>
<ul>
<li><strong>Authors: </strong>Akshara Nair, Zeba Afroz, Md Shad Akhtar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05650">https://arxiv.org/abs/2502.05650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05650">https://arxiv.org/pdf/2502.05650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05650]] Incongruence Identification in Eyewitness Testimony(https://arxiv.org/abs/2502.05650)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Incongruence detection in eyewitness narratives is critical for understanding the reliability of testimonies, yet traditional approaches often fail to address the nuanced inconsistencies inherent in such accounts. In this paper, we introduce a novel task of incongruence detection in eyewitness testimonies. Given a pair of testimonies containing of multiple pairs of question and answer by two subjects, we identify contextually related incongruence between the two subjects. We also mark the span of incongruences in the utterances. To achieve this, we developed MIND(MultI-EyewitNess Deception) - a comprehensive dataset consisting of 2927 pairs of contextually related answers designed to capture both explicit and implicit contradictions. INstruction - TunEd iNcongruity Detection framework based on 6W and multi-hop reasoning approach, aka. INTEND. Drawing from investigative techniques, INTEND address the task as a close-style problem, contradicting on the who, what, when, where and why aspect of the content. Our findings shows that prompt tuning, especially when utilizing our framework, enhances the detection of incongruences by a margin of +5.63 percent. We compare our approach with multiple fine-tuning and prompt tuning techniques on MLMs and LLMs. Emperical results demonstrate convincing performance improvement in F1-score over fine-tuned and regular prompt-tuning techniques, highlighting the effectiveness of our approach.</li>
<li><strong>摘要：</strong>目击者叙事中的不一致检测对于理解证词的可靠性至关重要，但是传统的方法通常无法解决此类帐户中固有的细微不一致之处。在本文中，我们介绍了目击者证词中不一致检测的新任务。鉴于一对包含多对问和答案的证词，我们确定了两个主题之间与上下文相关的不一致。我们还标志着话语中的不一致的跨度。为了实现这一目标，我们开发了思想（多眼欺骗） - 一个全面的数据集，该数据集由2927对上下文相关的答案组成，旨在捕获显式和隐性矛盾。指令 - 基于6W和多跳推理方法的调整不一致检测框架，也就是。打算。从调查技术中得出的目的是将任务作为一个近似风格的问题，与WHO，什么，何时，何时何地和原因相矛盾。我们的发现表明，及时进行调整，尤其是在利用我们的框架时，可以提高对不一致的检测，从而+5.63％。我们将我们的方法与MLM和LLM的多次微调和及时调整技术进行了比较。皇帝的结果表明，通过微调和常规的及时调整技术，F1得分的性能提高令人信服，这突出了我们方法的有效性。</li>
</ul>

<h3>Title: KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy</h3>
<ul>
<li><strong>Authors: </strong>Hyunjong Kim, Suyeon Lee, Yeongjae Cho, Eunseo Ryu, Yohan Jo, Suran Seong, Sungzoon Cho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05651">https://arxiv.org/abs/2502.05651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05651">https://arxiv.org/pdf/2502.05651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05651]] KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy(https://arxiv.org/abs/2502.05651)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.</li>
<li><strong>摘要：</strong>对心理健康服务的需求不断增长，导致了AI驱动的心理健康聊天机器人的兴起，尽管与隐私，数据收集和专业知识有关的挑战仍然存在。动机访谈（MI）正在引起人们的关注，这是提高这些聊天机器人发展专业知识的理论基础。但是，现有数据集显示出培训聊天机器人的局限性，从而导致对MI和心理治疗领域公开资源的大量需求。这些挑战在非英语语言中更加明显，在这些语言中，他们受到了较少的关注。在本文中，我们提出了一个新颖的框架，该框架模拟了具有专业治疗师专业知识的MI会议。我们训练一个模仿专业治疗师的行为选择的MI预报模型，并采用大型语言模型（LLMS）通过及时的工程来生成话语。然后，我们提出了KMI，这是第一个以MI为基础的合成数据集，其中包含1,000个高质量的韩国动机访谈对话。通过对生成的数据集的广泛专家评估以及对其进行培训的对话模型，我们证明了KMI的质量，专业知识和实用性。我们还介绍了从MI理论得出的新型指标，以便从MI的角度评估对话。</li>
</ul>

<h3>Title: CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging</h3>
<ul>
<li><strong>Authors: </strong>Md. Ashraful Islam, Mohammed Eunus Ali, Md Rizwan Parvez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05664">https://arxiv.org/abs/2502.05664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05664">https://arxiv.org/pdf/2502.05664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05664]] CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging(https://arxiv.org/abs/2502.05664)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (this https URL).</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在代码生成和解决问题方面取得了重大进步。当前方法采用了基于外部工具的迭代式调试者，这些迭代式调试器使用编译器或其他基于工具的运行时反馈来完善各种方法生成的粗略程序。但是，这些方法的有效性在很大程度上依赖于初始代码生成的质量，这仍然是一个开放的挑战。在本文中，我们介绍了CodeSim，这是一种新型的多代码生成框架，可全面解决程序合成规划，编码和调试 - 通过类似人类的感知方法的阶段。当人通过视觉模拟验证对任何算法的理解时，Codesim唯一地采用了一种计划验证和内部调试方法，并通过对输入/输出的分步仿真进行内部调试。在七个挑战性竞争性问题解决和计划合成基准中进行的广泛实验证明了Codesim的显着代码生成功能。我们的框架实现了新的最先进（通过@1）结果 - （Humaneval 95.1％，MBPP 90.7％，应用22％和Codecontests 29.1％）。此外，当与外部辩论者级联时，我们的方法显示出可能更大的增强。为了促进该领域的进一步研究和开发，我们在此链接（此HTTPS URL）中开了开源的框架。</li>
</ul>

<h3>Title: Language Models Largely Exhibit Human-like Constituent Ordering Preferences</h3>
<ul>
<li><strong>Authors: </strong>Ada Defne Tur, Gaurav Kamath, Siva Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05670">https://arxiv.org/abs/2502.05670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05670">https://arxiv.org/pdf/2502.05670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05670]] Language Models Largely Exhibit Human-like Constituent Ordering Preferences(https://arxiv.org/abs/2502.05670)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Though English sentences are typically inflexible vis-à-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of the constituent's length or complexity. Such theories are interesting in the context of natural language processing (NLP), because while recent advances in NLP have led to significant gains in the performance of large language models (LLMs), much remains unclear about how these models process language, and how this compares to human language processing. In particular, the question remains whether LLMs display the same patterns with constituent movement, and may provide insights into existing theories on when and how the shift occurs in human language. We compare a variety of LLMs with diverse properties to evaluate broad LLM performance on four types of constituent movement: heavy NP shift, particle movement, dative alternation, and multiple PPs. Despite performing unexpectedly around particle movement, LLMs generally align with human preferences around constituent ordering.</li>
<li><strong>摘要：</strong>尽管英语句子通常相对于单词顺序不灵活，但成分通常在订购方面显示出更大的可变性。一种突出的理论表明，组成有序与组成的重量直接相关：衡量组成量的长度或复杂性。在自然语言处理（NLP）的背景下，这种理论很有趣，因为尽管NLP的最新进展导致了大语言模型（LLMS）的性能显着提高，但对于这些模型如何处理语言以及如何处理，尚不清楚与人类语言处理相比。特别是，问题是LLM是否通过组成运动表现出相同的模式，并且可能会对现有理论的何时以及如何在人类语言中发生。我们比较各种具有多种特性的LLM，以评估四种类型的组成运动的广泛LLM性能：重型NP移动，粒子运动，词性交替和多个PPS。尽管围绕粒子运动表现出乎意料，但LLMS通常与人类偏爱围绕组成有序的偏好保持一致。</li>
</ul>

<h3>Title: Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Venkatesh Mishra, Bimsara Pathiraja, Mihir Parmar, Sat Chidananda, Jayanth Srinivasa, Gaowen Liu, Ali Payani, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05675">https://arxiv.org/abs/2502.05675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05675">https://arxiv.org/pdf/2502.05675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05675]] Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning(https://arxiv.org/abs/2502.05675)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Reasoning abilities of LLMs have been a key focus in recent years. One challenging reasoning domain with interesting nuances is legal reasoning, which requires careful application of rules, and precedents while balancing deductive and analogical reasoning, and conflicts between rules. Although there have been a few works on using LLMs for legal reasoning, their focus has been on overall accuracy. In this paper, we dig deeper to do a step-by-step analysis and figure out where they commit errors. We use the college-level Multiple Choice Question-Answering (MCQA) task from the \textit{Civil Procedure} dataset and propose a new error taxonomy derived from initial manual analysis of reasoning chains with respect to several LLMs, including two objective measures: soundness and correctness scores. We then develop an LLM-based automated evaluation framework to identify reasoning errors and evaluate the performance of LLMs. The computation of soundness and correctness on the dataset using the auto-evaluator framework reveals several interesting insights. Furthermore, we show that incorporating the error taxonomy as feedback in popular prompting techniques marginally increases LLM performance. Our work will also serve as an evaluation framework that can be used in detailed error analysis of reasoning chains for logic-intensive complex tasks.</li>
<li><strong>摘要：</strong>近年来，LLM的推理能力一直是重点。具有有趣细微差别的一个具有挑战性的推理领域是法律推理，它需要仔细应用规则，并在平衡演绎和类似推理的同时以及规则之间的冲突。尽管有一些关于使用LLM进行法律推理的作品，但它们的重点一直放在整体准确性上。在本文中，我们更深入地进行逐步进行分析，并找出他们犯错误的位置。我们使用\ textIt {civil Procoture}数据集的大学级多项选择提问（MCQA）任务，并提出了一个新的错误分类法，这些分类法来自对几个LLM的推理链的初步手动分析，包括两个客观的措施：Soundness：Soundness：Soundness：和正确的分数。然后，我们开发一个基于LLM的自动评估框架，以识别推理错误并评估LLM的性能。使用自动化腔框架在数据集上的声音和正确性计算显示出几个有趣的见解。此外，我们表明将错误分类法纳入流行的提示技术中的反馈会急剧提高LLM的性能。我们的工作还将用作评估框架，可用于对逻辑密集型复杂任务的推理链的详细错误分析。</li>
</ul>

<h3>Title: Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study of Gemini, LLaMA and ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Shaoshuai Du, Yiyi Tao, Yixian Shen, Hang Zhang, Yanxin Shen, Xinyu Qiu, Chuanqi Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05694">https://arxiv.org/abs/2502.05694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05694">https://arxiv.org/pdf/2502.05694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05694]] Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study of Gemini, LLaMA and ChatGPT(https://arxiv.org/abs/2502.05694)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This study investigates the performance of various large language models (LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that integrates entity recognition and relation extraction without requiring annotated data. While LLMs show promise for RE, most prior work focuses on English or assumes pre-annotated entities, leaving their effectiveness in Chinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini, and LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates the highest overall performance, balancing precision and recall, while Gemini achieves the fastest inference speed, making it suitable for real-time applications. LLaMA underperforms in both accuracy and latency, highlighting the need for further adaptation. Our findings provide insights into the strengths and limitations of LLMs for zero-shot Chinese RE, shedding light on trade-offs between accuracy and efficiency. This study serves as a foundation for future research aimed at improving LLM adaptability to complex linguistic tasks in Chinese NLP.</li>
<li><strong>摘要：</strong>这项研究调查了各种大型语言模型（LLM）在中文中零摄像的端到端关系提取（RE）上的性能，该任务旨在整合实体识别和关系提取而无需带注释的数据。尽管LLMS对RE表示希望，但大多数先前的工作都集中在英语或假定预通道的实体上，因此在中文中的有效性在很大程度上没有探索。为了弥合这一差距，我们根据准确性，效率和适应性评估Chatgpt，Gemini和Llama。 Chatgpt展示了最高的整体性能，平衡精度和召回率，而双子座的推理速度最快，使其适合实时应用。美洲驼在准确性和延迟方面表现不佳，强调了进一步适应的需求。我们的发现提供了对零击中中国RE的优势和局限性的见解，从而阐明了准确性和效率之间的权衡。这项研究是未来研究的基础，旨在改善LLM对中国NLP复杂语言任务的适应性。</li>
</ul>

<h3>Title: Reinforced Lifelong Editing for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, Junfeng Fang, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05759">https://arxiv.org/abs/2502.05759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05759">https://arxiv.org/pdf/2502.05759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05759]] Reinforced Lifelong Editing for Language Models(https://arxiv.org/abs/2502.05759)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and prevalent approaches leverage hypernetworks to generate these parameter updates. However, they face significant challenges in lifelong editing due to their incompatibility with LLM parameters that dynamically change during the editing process. To address this, we observed that hypernetwork-based lifelong editing aligns with reinforcement learning modeling and proposed RLEdit, an RL-based editing method. By treating editing losses as rewards and optimizing hypernetwork parameters at the full knowledge sequence level, we enable it to precisely capture LLM changes and generate appropriate parameter updates. Our extensive empirical evaluation across several LLMs demonstrates that RLEdit outperforms existing methods in lifelong editing with superior effectiveness and efficiency, achieving a 59.24% improvement while requiring only 2.11% of the time compared to most approaches. Our code is available at: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）从培训前语料库中获取信息，但是它们存储的知识可能会随着时间的流逝而变得不准确或过时。模型编辑通过修改模型参数而无需重新培训来解决这一挑战，而普遍的方法利用超网络来生成这些参数更新。但是，由于它们与在编辑过程中动态变化的LLM参数不兼容，因此他们在终身编辑中面临重大挑战。为了解决这个问题，我们观察到，基于超网络的终身编辑与增强学习建模和拟议的RLEDIT（一种基于RL的编辑方法）对齐。通过将编辑损失视为奖励并在完整的知识序列中优化超网络参数，我们使其能够精确捕获LLM更改并生成适当的参数更新。我们对几个LLM的广泛经验评估表明，RLEDIT在终身编辑中的现有方法具有出色的有效性和效率，与大多数方法相比，相比之下只需要2.11％的时间，而仅需要2.11％的时间。我们的代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: On Reference (In-)Determinacy in Natural Language Inference</h3>
<ul>
<li><strong>Authors: </strong>Sihao Chen, Chaitanya Malaviya, Alex Fabrikant, Hagai Taitelbaum, Tal Schuster, Senaka Buthpitiya, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05793">https://arxiv.org/abs/2502.05793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05793">https://arxiv.org/pdf/2502.05793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05793]] On Reference (In-)Determinacy in Natural Language Inference(https://arxiv.org/abs/2502.05793)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>We revisit the reference determinacy (RD) assumption in the task of natural language inference (NLI), i.e., the premise and hypothesis are assumed to refer to the same context when human raters annotate a label. While RD is a practical assumption for constructing a new NLI dataset, we observe that current NLI models, which are typically trained solely on hypothesis-premise pairs created with the RD assumption, fail in downstream applications such as fact verification, where the input premise and hypothesis may refer to different contexts. To highlight the impact of this phenomenon in real-world use cases, we introduce RefNLI, a diagnostic benchmark for identifying reference ambiguity in NLI examples. In RefNLI, the premise is retrieved from a knowledge source (i.e., Wikipedia) and does not necessarily refer to the same context as the hypothesis. With RefNLI, we demonstrate that finetuned NLI models and few-shot prompted LLMs both fail to recognize context mismatch, leading to over 80% false contradiction and over 50% entailment predictions. We discover that the existence of reference ambiguity in NLI examples can in part explain the inherent human disagreements in NLI and provide insight into how the RD assumption impacts the NLI dataset creation process.</li>
<li><strong>摘要：</strong>我们在自然语言推理任务（NLI）任务中重新审查参考确定性（RD）假设，即，当人类评估者注释标签时，假设前提和假设是指相同的上下文。尽管RD是构建新的NLI数据集的一个实际假设，但我们观察到，当前的NLI模型通常仅根据RD假设创建的假设 - 验证对培训，但在下游应用程序中失败，例如事实验证，例如输入前提和输入前提和其中的情况。假设可能是指不同的上下文。为了强调这种现象在现实世界用例中的影响，我们介绍了Refnli，这是一种诊断基准，用于识别NLI示例中的参考歧义。在Refnli中，前提是从知识来源（即Wikipedia）中检索的，并不一定是指与假设相同的上下文。借助Refnli，我们证明了FineTununtun的NLI模型和很少的射击促使LLM都无法识别上下文不匹配，从而导致超过80％的虚假矛盾和超过50％的需要预测。我们发现，NLI示例中参考歧义的存在可以部分解释NLI中固有的人类分歧，并提供有关RD假设如何影响NLI数据集创建过程的见解。</li>
</ul>

<h3>Title: Structural Perturbation in Large Language Model Representations through Recursive Symbolic Regeneration</h3>
<ul>
<li><strong>Authors: </strong>Kathlyn Eaglewood, Tobias Featherington, Dorian Mayfair, Sylvester Grimshaw, James Pettigrew</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05794">https://arxiv.org/abs/2502.05794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05794">https://arxiv.org/pdf/2502.05794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05794]] Structural Perturbation in Large Language Model Representations through Recursive Symbolic Regeneration(https://arxiv.org/abs/2502.05794)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Symbolic perturbations offer a novel approach for influencing neural representations without requiring direct modification of model parameters. The recursive regeneration of symbolic structures introduces structured variations in latent embeddings, leading to controlled shifts in attention dynamics and lexical diversity across sequential generations. A comparative analysis with conventional fine-tuning techniques reveals that structural modifications at the symbolic level induce distinct variations in contextual sensitivity while maintaining overall model fluency and coherence. Shifts in attention weight distributions highlight the role of symbolic modifications in adjusting token dependencies, influencing response variability, and refining long-form text generation. Experimental findings suggest that symbolic perturbations can enhance adaptability in domain-specific applications, allowing modifications in model behavior without retraining. Evaluations of semantic drift indicate that recursive regeneration alters long-range token dependencies, affecting topic coherence across extended text sequences. Results from lexical variability assessments further support the conclusion that symbolic-level modifications introduce interpretable variations in generated responses, potentially enabling more controlled stylistic adjustments in automated text generation.</li>
<li><strong>摘要：</strong>符号扰动提供了一种新的方法来影响神经表示，而无需直接修改模型参数。符号结构的递归再生引入了潜在嵌入中的结构化变化，从而导致了连续世代的注意力动态和词汇多样性的控制转移。与常规微调技术的比较分析表明，符号水平的结构修饰会在上下文敏感性中引起不同的变化，同时保持整体模型的流利度和连贯性。注意力重量分布的转变突出了符号修改在调整令牌依赖依赖性，影响响应变异性和完善长期文本生成中的作用。实验发现表明，符号扰动可以增强域特异性应用中的适应性，从而可以在不进行重新训练的情况下修改模型行为。语义漂移的评估表明，递归再生改变了远程令牌依赖性，影响了扩展文本序列之间的主题连贯性。词汇可变性评估的结果进一步支持了以下结论：符号级别的修改引入了生成的响应中的可解释变化，从而有可能在自动化文本生成中进行更多控制的风格调整。</li>
</ul>

<h3>Title: Delta - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng Peng Huang, Hao-Yuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05825">https://arxiv.org/abs/2502.05825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05825">https://arxiv.org/pdf/2502.05825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05825]] Delta - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models(https://arxiv.org/abs/2502.05825)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate strong capabilities in natural language processing but remain prone to hallucinations, generating factually incorrect or fabricated content. This issue undermines their reliability, particularly in high-stakes domains such as healthcare and legal advisory. To address this challenge, we propose Delta, an inference-time method that reduces hallucinations without requiring model retraining or additional data. Delta works by randomly masking parts of the input prompt and contrasting the output distributions for the original and masked inputs, effectively suppressing hallucinations through inference-only computations. We evaluate Delta on context-rich question-answering benchmarks, achieving absolute improvements of approximately 3 and 6 percentage points on SQuAD v1.1 and v2, respectively, and 7 and 2 percentage points on TriviaQA and Natural Questions under-sampling decoding. Delta also improves the no-answer exact match score on SQuAD v2 by over ten percentage points, demonstrating its effectiveness in mitigating hallucinations arising from contextual ambiguity. These results highlight Delta as a computationally efficient and scalable approach for improving the reliability of LLMs in real-world applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在自然语言处理中表现出强大的功能，但仍然容易出现幻觉，产生了事实不正确或捏造的内容。这个问题破坏了它们的可靠性，尤其是在医疗保健和法律咨询等高风险领域。为了应对这一挑战，我们提出了Delta，这是一种推理时间方法，可以减少幻觉而无需模型再培训或其他数据。 Delta通过随机掩盖输入提示的一部分并与原始输入和掩盖输入的输出分布进行对比，从而有效地通过仅推理计算来抑制幻觉。我们在上下文富裕的提问基准上评估了三角洲，分别在小队v1.1和v2上获得了大约3个和6个百分点，在Triviaqa和自然问题上的7个和2个百分点点和2个百分点。 Delta还将No-Asswer V2上的No-Asswer精确匹配分数提高了10个百分点，这表明了其在减轻上下文歧义引起的幻觉方面的有效性。这些结果强调了三角洲作为一种计算高效且可扩展的方法，用于提高现实世界应用中LLM的可靠性。</li>
</ul>

<h3>Title: LegalSeg: Unlocking the Structure of Indian Legal Judgments Through Rhetorical Role Classification</h3>
<ul>
<li><strong>Authors: </strong>Shubham Kumar Nigam, Tanmay Dubey, Govind Sharma, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05836">https://arxiv.org/abs/2502.05836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05836">https://arxiv.org/pdf/2502.05836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05836]] LegalSeg: Unlocking the Structure of Indian Legal Judgments Through Rhetorical Role Classification(https://arxiv.org/abs/2502.05836)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we address the task of semantic segmentation of legal documents through rhetorical role classification, with a focus on Indian legal judgments. We introduce LegalSeg, the largest annotated dataset for this task, comprising over 7,000 documents and 1.4 million sentences, labeled with 7 rhetorical roles. To benchmark performance, we evaluate multiple state-of-the-art models, including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and Role-Aware Transformers, alongside an exploratory RhetoricLLaMA, an instruction-tuned large language model. Our results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features. Additionally, we conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact on classification accuracy. Despite these advancements, challenges persist in distinguishing between closely related roles and addressing class imbalance. Our work underscores the potential of advanced techniques for improving legal document understanding and sets a strong foundation for future research in legal NLP.</li>
<li><strong>摘要：</strong>在本文中，我们通过修辞角色分类解决了法律文件的语义分割的任务，重点是印度法律判断。我们介绍了该任务的最大注释数据集Legalseg，其中包括7,000多个文件和140万个句子，标有7个修辞角色。为了基准性能，我们评估了多种最先进的模型，包括等级Bilstm-Crf，Transformeroverinlegalbert（Toinlegalbert），图形神经网络（GNNS）和角色吸引的变形金刚，以及探索性的ryetoricallama，是一种指导性的大型大型大型大型大型。语言模型。我们的结果表明，结合更广泛的上下文，结构关系和顺序句子信息的模型优于仅依靠句子级特征的模型。此外，我们使用周围环境和相邻句子的预测或实际标签进行了实验，以评估其对分类准确性的影响。尽管取得了这些进步，但挑战仍在区分密切相关的角色和解决阶级失衡之间。我们的工作强调了先进技术在改善法律文件理解的潜力，并为未来的法律NLP研究奠定了坚实的基础。</li>
</ul>

<h3>Title: Fact-or-Fair: A Checklist for Behavioral Testing of AI Models on Fairness-Related Queries</h3>
<ul>
<li><strong>Authors: </strong>Jen-tse Huang, Yuhang Yan, Linqi Liu, Yixin Wan, Wenxuan Wang, Kai-Wei Chang, Michael R. Lyu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05849">https://arxiv.org/abs/2502.05849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05849">https://arxiv.org/pdf/2502.05849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05849]] Fact-or-Fair: A Checklist for Behavioral Testing of AI Models on Fairness-Related Queries(https://arxiv.org/abs/2502.05849)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The generation of incorrect images, such as depictions of people of color in Nazi-era uniforms by Gemini, frustrated users and harmed Google's reputation, motivating us to investigate the relationship between accurately reflecting factuality and promoting diversity and equity. In this study, we focus on 19 real-world statistics collected from authoritative sources. Using these statistics, we develop a checklist comprising objective and subjective queries to analyze behavior of large language models (LLMs) and text-to-image (T2I) models. Objective queries assess the models' ability to provide accurate world knowledge. In contrast, the design of subjective queries follows a key principle: statistical or experiential priors should not be overgeneralized to individuals, ensuring that models uphold diversity. These subjective queries are based on three common human cognitive errors that often result in social biases. We propose metrics to assess factuality and fairness, and formally prove the inherent trade-off between these two aspects. Results show that GPT-4o and DALL-E 3 perform notably well among six LLMs and four T2I models. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>双子座对纳粹时代制服中有色人种的描绘产生了不正确的图像，使用户感到沮丧，并损害了Google的声誉，激励我们调查准确反映事实与促进多样性和公平性之间的关系。在这项研究中，我们专注于从权威来源收集的19个现实世界统计数据。使用这些统计数据，我们开发了一个清单，其中包括客观和主观查询，以分析大语言模型（LLMS）和文本形象（T2I）模型的行为。客观查询评估模型提供准确世界知识的能力。相比之下，主观查询的设计遵循一个关键原则：统计或体验式先验不应过分地向个人统一，以确保模型维护多样性。这些主观查询基于三个常见的人类认知错误，这些错误通常会导致社会偏见。我们建议指标评估事实和公平性，并正式证明这两个方面之间的固有权衡。结果表明，在六个LLM和四个T2I模型中，GPT-4O和DALL-E 3的表现出色。我们的代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Self-Training Large Language Models for Tool-Use Without Demonstrations</h3>
<ul>
<li><strong>Authors: </strong>Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile van Krieken, Pietro Lesci, Pasquale Minervini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05867">https://arxiv.org/abs/2502.05867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05867">https://arxiv.org/pdf/2502.05867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05867]] Self-Training Large Language Models for Tool-Use Without Demonstrations(https://arxiv.org/abs/2502.05867)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) remain prone to factual inaccuracies and computational errors, including hallucinations and mistakes in mathematical reasoning. Recent work augmented LLMs with tools to mitigate these shortcomings, but often requires curated gold tool-use demonstrations. In this paper, we investigate whether LLMs can learn to use tools without demonstrations. First, we analyse zero-shot prompting strategies to guide LLMs in tool utilisation. Second, we propose a self-training method to synthesise tool-use traces using the LLM itself. We compare supervised fine-tuning and preference fine-tuning techniques for fine-tuning the model on datasets constructed using existing Question Answering (QA) datasets, i.e., TriviaQA and GSM8K. Experiments show that tool-use enhances performance on a long-tail knowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads to mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our findings highlight the potential and challenges of integrating external tools into LLMs without demonstrations.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）仍然容易出现事实不准确和计算错误，包括数学推理中的幻觉和错误。最近的工作增强了LLM的工具来减轻这些缺点，但通常需要精心策划的黄金工具使用演示。在本文中，我们研究了LLM是否可以学习使用工具而没有演示。首先，我们分析了零射击促使策略指导LLM的工具利用率。其次，我们提出了一种自我训练方法，以使用LLM本身合成工具使用轨迹。我们比较了监督的微调和偏好微调技术，以在使用现有问题答案（QA）数据集构建的数据集上微调模型，即Triviaqa和GSM8K。实验表明，工具使用可以增强长尾知识任务的性能：POPQA的3.7％，仅用于评估，但导致其他数据集的结果混合，即Triviaqa，GSM8K和NQ-Open。我们的发现突出了将外部工具集成到LLM的潜在和挑战，而无需进行演示。</li>
</ul>

<h3>Title: Retrieval-augmented Large Language Models for Financial Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Mengxi Xiao, Zihao Jiang, Lingfei Qian, Zhengyu Chen, Yueru He, Yijing Xu, Yuecheng Jiang, Dong Li, Ruey-Ling Weng, Min Peng, Jimin Huang, Sophia Ananiadou, Qianqian Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05878">https://arxiv.org/abs/2502.05878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05878">https://arxiv.org/pdf/2502.05878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05878]] Retrieval-augmented Large Language Models for Financial Time Series Forecasting(https://arxiv.org/abs/2502.05878)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.</li>
<li><strong>摘要：</strong>股票运动预测是财务时间序列预测的基本任务，需要从大量的时间序列数据中识别和检索关键影响因素。但是，现有的基于文本培训或基于数字相似性的检索方法在处理复杂的财务分析方面缺乏。为了解决这个问题，我们提出了第一个针对财务时间序列预测的检索型发电框架（RAG）框架，其中包含三个关键创新：一个微调的1B参数大语言模型（Stockllm）作为骨干，是一种新颖的候选选择方法。 LLM反馈以及最大化查询和历史上重要序列之间相似性的培训目标。这使我们的猎犬Finseer能够发现有意义的模式，同时最大程度地减少复杂财务数据中的噪音。我们还构建了整合财务指标和历史股票价格的新数据集，以培训Finseer并确保稳健的评估。实验结果表明，我们的抹布框架的表现优于裸露的stockllm和随机检索，突出了其有效性，而Finseer则超过了现有的检索方法，在BigData22上获得了8 \％的精度，并取回了更具影响力的序列。这项工作强调了量身定制的检索模型在财务预测中的重要性，并为未来的研究提供了一个新颖的框架。</li>
</ul>

<h3>Title: Enhancing Depression Detection with Chain-of-Thought Prompting: From Emotion to Reasoning Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Teng, Jiaqing Liu, Rahul Kumar Jain, Shurong Chai, Ruibo Hou, Tomoko Tateyama, Lanfen Lin, Yen-wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05879">https://arxiv.org/abs/2502.05879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05879">https://arxiv.org/pdf/2502.05879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05879]] Enhancing Depression Detection with Chain-of-Thought Prompting: From Emotion to Reasoning Using Large Language Models(https://arxiv.org/abs/2502.05879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Depression is one of the leading causes of disability worldwide, posing a severe burden on individuals, healthcare systems, and society at large. Recent advancements in Large Language Models (LLMs) have shown promise in addressing mental health challenges, including the detection of depression through text-based analysis. However, current LLM-based methods often struggle with nuanced symptom identification and lack a transparent, step-by-step reasoning process, making it difficult to accurately classify and explain mental health conditions. To address these challenges, we propose a Chain-of-Thought Prompting approach that enhances both the performance and interpretability of LLM-based depression detection. Our method breaks down the detection process into four stages: (1) sentiment analysis, (2) binary depression classification, (3) identification of underlying causes, and (4) assessment of severity. By guiding the model through these structured reasoning steps, we improve interpretability and reduce the risk of overlooking subtle clinical indicators. We validate our method on the E-DAIC dataset, where we test multiple state-of-the-art large language models. Experimental results indicate that our Chain-of-Thought Prompting technique yields superior performance in both classification accuracy and the granularity of diagnostic insights, compared to baseline approaches.</li>
<li><strong>摘要：</strong>抑郁症是全世界残疾的主要原因之一，对个人，医疗保健系统和整个社会造成了严重负担。大型语言模型（LLM）的最新进展在应对心理健康挑战方面表现出了希望，包括通过基于文本的分析检测抑郁症。但是，当前基于LLM的方法通常在细微的症状识别方面遇到困难，并且缺乏透明的逐步推理过程，因此难以准确地对心理健康状况进行分类和解释。为了应对这些挑战，我们提出了一种经过深思熟虑的促进方法，可以增强基于LLM的抑郁症检测的性能和解释性。我们的方法将检测过程分为四个阶段：（1）情绪分析，（2）二进制抑郁分类，（3）鉴定潜在原因，以及（4）评估严重程度。通过通过这些结构化的推理步骤引导模型，我们提高了解释性，并降低了忽视微妙的临床指标的风险。我们在E-DAIC数据集上验证我们的方法，我们在其中测试了多种最先进的大语言模型。实验结果表明，与基线方法相比，我们的经过思考促进技术在分类准确性和诊断见解的粒度方面都能产生较高的性能。</li>
</ul>

<h3>Title: MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Wanqi Yang, Yanda Li, Meng Fang, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05887">https://arxiv.org/abs/2502.05887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05887">https://arxiv.org/pdf/2502.05887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05887]] MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational Agents(https://arxiv.org/abs/2502.05887)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, agent</a></li>
<li><strong>Abstract: </strong>Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model's ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.</li>
<li><strong>摘要：</strong>了解时间动态对于对话代理，实现有效的内容分析和明智的决策至关重要。但是，时间感知的数据集，尤其是对于角色界面的对话，仍然有限，这会缩小其范围并减少其复杂性。为了解决这一差距，我们介绍了MTPCHAT，这是一种多模式，时间吸引的角色对话数据集，该数据集将语言，视觉和时间元素集成到对话和角色记忆中。利用MTPCHAT，我们提出了两个时间敏感的任务：时间敏感的下一个响应预测（TNRP）和时间接地记忆预测（TGMP），均旨在评估模型了解隐式时间提示和动态相互作用的能力。此外，我们提出了一个创新的框架，该框架具有自适应时间模块，以有效整合多模式流并捕获时间依赖性。实验结果证明了MTPCHAT所带来的挑战，并证明了我们在多模式时间敏感的情况下框架的有效性。</li>
</ul>

<h3>Title: A Distributional Perspective on Word Learning in Neural Language Models</h3>
<ul>
<li><strong>Authors: </strong>Filippo Ficarra, Ryan Cotterell, Alex Warstadt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05892">https://arxiv.org/abs/2502.05892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05892">https://arxiv.org/pdf/2502.05892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05892]] A Distributional Perspective on Word Learning in Neural Language Models(https://arxiv.org/abs/2502.05892)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) are increasingly being studied as models of human language learners. Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models. Word learning trajectories for children are relatively well-documented, and recent work has tried to extend these investigations to language models. However, there are no widely agreed-upon metrics for word learning in language models. We take a distributional approach to this problem, defining lexical knowledge in terms of properties of the learned distribution for a target word. We argue that distributional signatures studied in prior work fail to capture key distributional information. Thus, we propose an array of signatures that improve on earlier approaches by capturing knowledge of both where the target word can and cannot occur as well as gradient preferences about the word's appropriateness. We obtain learning trajectories for a selection of small language models we train from scratch, study the relationship between different distributional signatures, compare how well they align with human word learning trajectories and interpretable lexical features, and address basic methodological questions about estimating these distributional signatures. Our metrics largely capture complementary information, suggesting that it is important not to rely on a single metric. However, across all metrics, language models' learning trajectories fail to correlate with those of children.</li>
<li><strong>摘要：</strong>语言模型（LMS）越来越多地被研究为人类语言学习者的模型。由于该领域的含量，LMS是否表现出与人类相似的学习动力，并且在人类和模型中学习轨迹之间几乎没有直接比较。儿童的单词学习轨迹有相对有据可查的文献，并且最近的工作试图将这些调查扩展到语言模型。但是，在语言模型中，尚无广泛商定的指标。我们对此问题采用分配方法，从目标词的学习分布的属性来定义词汇知识。我们认为，在先前工作中研究的分布签名未能捕获关键分布信息。因此，我们提出了一系列签名，通过捕获对目标单词可以和不能发生的位置以及对单词适当性的梯度偏好的知识，从而改善了早期方法。我们获得了用于选择小语言模型的学习轨迹，我们从头开始训练，研究不同的分布签名之间的关系，比较它们与人类单词学习轨迹和可解释的词汇特征的一致性，并解决了有关估计这些分布签名的基本方法论问题。我们的指标在很大程度上捕获了互补的信息，这表明不依靠单个指标很重要。但是，在所有指标中，语言模型的学习轨迹与儿童的学习轨迹无法相关。</li>
</ul>

<h3>Title: GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05911">https://arxiv.org/abs/2502.05911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05911">https://arxiv.org/pdf/2502.05911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05911]] GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation(https://arxiv.org/abs/2502.05911)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Refusal-Aware Instruction Tuning (RAIT) aims to enhance Large Language Models (LLMs) by improving their ability to refuse responses to questions beyond their knowledge, thereby reducing hallucinations and improving reliability. Effective RAIT must address two key challenges: firstly, effectively reject unknown questions to minimize hallucinations; secondly, avoid over-refusal to ensure questions that can be correctly answered are not rejected, thereby maintain the helpfulness of LLM outputs. In this paper, we address the two challenges by deriving insightful observations from the gradient-based perspective, and proposing the Gradient-driven Refusal Aware Instruction Tuning Framework GRAIT: (1) employs gradient-driven sample selection to effectively minimize hallucinations and (2) introduces an adaptive weighting mechanism during fine-tuning to reduce the risk of over-refusal, achieving the balance between accurate refusals and maintaining useful responses. Experimental evaluations on open-ended and multiple-choice question answering tasks demonstrate that GRAIT significantly outperforms existing RAIT methods in the overall performance. The source code and data will be available at this https URL .</li>
<li><strong>摘要：</strong>拒绝感知的指导调整（RAIT）旨在通过提高其拒绝对问题的回答的能力来增强大型语言模型（LLMS），从而降低幻觉并提高可靠性。有效的Rait必须应对两个关键挑战：首先，有效地拒绝未知的问题，以最大程度地减少幻觉；其次，避免过度繁殖以确保可以正确回答的问题被拒绝，从而保持LLM输出的有益性。在本文中，我们通过从基于梯度的角度得出有见地的观察来解决这两个挑战，并提出了梯度驱动的拒绝意识教学教学调谐框架GRAIT：（1）采用梯度驱动的样本选择以有效地最大程度地减少幻觉和（2）在微调过程中引入了一种自适应加权机制，以降低过度互惠的风险，实现准确拒绝和保持有用的响应之间的平衡。对开放式和多项选择的问题回答任务的实验评估表明，在整体性能中，GRAIT显着超过现有的RAIT方法。源代码和数据将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: Learning to Substitute Words with Model-based Score Ranking</h3>
<ul>
<li><strong>Authors: </strong>Hongye Liu, Ricardo Henao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05933">https://arxiv.org/abs/2502.05933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05933">https://arxiv.org/pdf/2502.05933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05933]] Learning to Substitute Words with Model-based Score Ranking(https://arxiv.org/abs/2502.05933)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Smart word substitution aims to enhance sentence quality by improving word choices; however current benchmarks rely on human-labeled data. Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable. To circumvent this issue, we instead employ a model-based score (BARTScore) to quantify sentence quality, thus forgoing the need for human annotations. Specifically, we use this score to define a distribution for each word substitution, allowing one to test whether a substitution is statistically superior relative to others. In addition, we propose a loss function that directly optimizes the alignment between model predictions and sentence scores, while also enhancing the overall quality score of a substitution. Crucially, model learning no longer requires human labels, thus avoiding the cost of annotation while maintaining the quality of the text modified with substitutions. Experimental results show that the proposed approach outperforms both masked language models (BERT, BART) and large language models (GPT-4, LLaMA). The source code is available at this https URL.</li>
<li><strong>摘要：</strong>智能单词替代旨在通过改善单词选择来提高句子质量；但是当前的基准测试依赖于人体标记的数据。由于单词选择本质上是主观的，因此一小部分注释者产生的基本真相替换通常是不完整的，并且可能不可概括。为了解决这个问题，我们使用基于模型的分数（Bartscore）来量化句子质量，从而放弃了对人类注释的需求。具体而言，我们使用此分数来定义每个单词替换的分布，从而可以测试替代方面是否相对于其他替换在统计上是否优越。此外，我们提出了一个直接优化模型预测和句子得分之间的对齐方式的损失函数，同时还提高了替换的总体质量评分。至关重要的是，模型学习不再需要人类标签，从而避免了注释的成本，同时保持了用替代修改的文本质量。实验结果表明，所提出的方法的表现优于蒙版语言模型（BERT，BART）和大型语言模型（GPT-4，Llama）。源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: A Semi-Supervised Text Generation Framework Combining a Deep Transformer and a GAN</h3>
<ul>
<li><strong>Authors: </strong>Shengquan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05937">https://arxiv.org/abs/2502.05937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05937">https://arxiv.org/pdf/2502.05937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05937]] A Semi-Supervised Text Generation Framework Combining a Deep Transformer and a GAN(https://arxiv.org/abs/2502.05937)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a framework that connects a deep generative pre-trained Transformer language model with a generative adversarial network for semi-supervised text generation. In other words, the proposed model is first pre-trained unsupervised on a large and diverse text corpus with 24 layers. Then a simple GAN architecture for synthetic text generation is introduced, and Gumbel-Softmax is applied to handle the discreteness of tokens. The paper also shows a semi-supervised approach where real data is augmented with GAN samples, which is further used to fine-tune the Transformer model on the merged dataset. Detailed theoretical derivations are also included, outlining the proof of the min-max objective function, and an extensive discussion of the Gumbel-Softmax reparameterization trick.</li>
<li><strong>摘要：</strong>本文介绍了一个框架，该框架将深度生成的预训练的变压器语言模型与用于半监督文本生成的生成对抗网络。换句话说，提出的模型首先是在具有24层的大型和多样化的文本语料库上无监督的。然后引入了一个简单的GAN架构，用于合成文本生成，并应用Gumbel-SoftMax来处理令牌的离散性。该论文还显示了一种半监督的方法，其中使用GAN样品增强了真实数据，该方法进一步用于微调合并数据集中的变压器模型。还包括详细的理论推导，概述了最低最大目标函数的证明，以及对Gumbel-Softmax重新聚集技巧的广泛讨论。</li>
</ul>

<h3>Title: Multi-granular Training Strategies for Robust Multi-hop Reasoning Over Noisy and Heterogeneous Knowledge Sources</h3>
<ul>
<li><strong>Authors: </strong>Jackson Coleman, Isaiah Lawrence, Benjamin Turner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05944">https://arxiv.org/abs/2502.05944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05944">https://arxiv.org/pdf/2502.05944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05944]] Multi-granular Training Strategies for Robust Multi-hop Reasoning Over Noisy and Heterogeneous Knowledge Sources(https://arxiv.org/abs/2502.05944)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multi-source multi-hop question answering (QA) represents a challenging task in natural language processing due to the need for dynamic integration of heterogeneous knowledge sources and multi-step reasoning. Existing methods often suffer from cascading errors, insufficient handling of knowledge conflicts, and computational inefficiency. In this paper, we propose Adaptive Multi-source Knowledge-Oriented Reasoning (AMKOR), a generative framework that leverages large language models (LLMs) to dynamically fuse parametric and retrieved knowledge while exploring reasoning trajectories using probabilistic beam reasoning. AMKOR is further enhanced by a multi-granular learning strategy, optimizing both local reasoning steps and global answer accuracy. Experiments conducted on four widely-used multi-hop QA datasets, including HotpotQA and MuSiQue, demonstrate that AMKOR achieves state-of-the-art performance, significantly outperforming baseline methods on both reasoning accuracy and robustness. Additional analyses confirm its scalability, adaptability to noisy knowledge, and superior ability to handle complex multi-hop tasks. This work establishes a new benchmark for multi-source multi-hop QA by effectively combining reasoning quality and efficiency.</li>
<li><strong>摘要：</strong>由于需要动态整合异质知识源和多步推理，因此多源多跳问题回答（QA）代表了自然语言处理中的一项具有挑战性的任务。现有的方法通常会遇到级联错误，知识冲突的处理不足以及计算效率低下的困扰。在本文中，我们提出了自适应多源知识知识推理（AMKOR），这是一种生成框架，利用大型语言模型（LLMS）动态融合参数并检索了知识，同时使用概率光束推理探索推理轨迹。通过多个粒度学习策略进一步增强了Amkor，优化了当地推理步骤和全球答案的准确性。在包括HotPotQA和Musique在内的四个广泛使用的多跳QA数据集上进行的实验表明，Amkor在推理准确性和鲁棒性方面都取得了最新的性能，高于基线方法。其他分析证实了其可扩展性，对嘈杂知识的适应性以及较高的处理复杂多跳任务的能力。这项工作通过有效结合推理质量和效率来为多源多跳质量质量质量质量质量质量质量质量质量警察建立新的基准。</li>
</ul>

<h3>Title: "Let the AI conspiracy begin..." Language Model coordination is just one inference-intervention away</h3>
<ul>
<li><strong>Authors: </strong>Paul Darm, Annalisa Riccardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05945">https://arxiv.org/abs/2502.05945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05945">https://arxiv.org/pdf/2502.05945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05945]] "Let the AI conspiracy begin..." Language Model coordination is just one inference-intervention away(https://arxiv.org/abs/2502.05945)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In this work, we introduce a straightforward and effective methodology to steer large language model behaviour capable of bypassing learned alignment goals. We employ interference-time activation shifting, which is effective without additional training. Following prior studies, we derive intervention directions from activation differences in contrastive pairs of model outputs, which represent the desired and undesired behaviour. By prompting the model to include multiple-choice answers in its response, we can automatically evaluate the sensitivity of model output to individual attention heads steering efforts. We demonstrate that interventions on these heads generalize well to open-ended answer generation in the challenging "AI coordination" dataset. In this dataset, models must choose between assisting another AI or adhering to ethical, safe, and unharmful behaviour. Our fine-grained interventions lead Llama 2 to prefer coordination with other AIs over following established alignment goals. Additionally, this approach enables stronger interventions than those applied to whole model layers, preserving the overall cohesiveness of the output. The simplicity of our method highlights the shortcomings of current alignment strategies and points to potential future research directions, as concepts like "AI coordination" can be influenced by selected attention heads.</li>
<li><strong>摘要：</strong>在这项工作中，我们引入了一种直接有效的方法，以引导能够绕过学会的一致性目标的大型语言模型行为。我们采用干扰时间激活转移，这在没有额外培训的情况下是有效的。在先前的研究之后，我们从模型输出对比对的激活差异中得出了干预方向，这代表了所需和不希望的行为。通过提示该模型在其响应中包含多项选择答案，我们可以自动评估模型输出对单个注意力头的敏感性。我们证明，这些头部的干预措施很好地推广到了具有挑战性的“ AI协调”数据集中的开放式答案生成。在此数据集中，模型必须在协助另一个AI或坚持道德，安全和不安的行为之间进行选择。我们的细粒度干预措施导致Llama 2更喜欢与其他AIS的协调，而不是以下确定的一致性目标。此外，这种方法比应用于整个模型层的方法可以更强的干预措施，从而保留了输出的整体内聚力。我们方法的简单性突出了当前的一致性策略的缺点，并指出了潜在的未来研究方向，因为“ AI协调”之类的概念可以受到选择的注意力的影响。</li>
</ul>

<h3>Title: HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Amin Abbasi, Farnaz Sadat Mirnezami, Hassan Naderi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05982">https://arxiv.org/abs/2502.05982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05982">https://arxiv.org/pdf/2502.05982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05982]] HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents(https://arxiv.org/abs/2502.05982)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper presents HamRaz, a novel Persian-language mental health dataset designed for Person-Centered Therapy (PCT) using Large Language Models (LLMs). Despite the growing application of LLMs in AI-driven psychological counseling, existing datasets predominantly focus on Western and East Asian contexts, overlooking cultural and linguistic nuances essential for effective Persian-language therapy. To address this gap, HamRaz combines script-based dialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy interactions. We also introduce HamRazEval, a dual evaluation framework that measures conversational quality and therapeutic effectiveness using General Dialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI). Experimental results show HamRaz outperforms conventional Script Mode and Two-Agent Mode, producing more empathetic, context-aware, and realistic therapy sessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven resource to advance AI-powered psychotherapy research in diverse communities.</li>
<li><strong>摘要：</strong>本文介绍了Hamraz，这是一种新型的波斯语心理健康数据集，使用大语模型（LLMS）设计为以人为本的治疗（PCT）。尽管LLM在AI驱动的心理咨询中的应用不断增长，但现有数据集主要集中在西亚和东亚的情况下，忽略了有效的波斯语治疗必不可少的文化和语言细微差别。为了解决这一差距，Hamraz将基于脚本的对话与自适应LLM角色扮演结合在一起，以确保连贯且动态的治疗相互作用。我们还介绍了Hamrazeval，这是一个双重评估框架，该框架使用一般对话指标和Barrett-Lennard关系清单（BLRI）来衡量对话质量和治疗有效性。实验结果表明，Hamraz的表现优于常规脚本模式和两种代理模式，从而产生了更多的善解人意，背景感和现实的治疗课程。通过发布Hamraz，我们贡献了一种文化适应的LLM驱动的资源，以推进不同社区的AI驱动心理治疗研究。</li>
</ul>

<h3>Title: Preventing Rogue Agents Improves Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Ohav Barbi, Ori Yoran, Mor Geva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.05986">https://arxiv.org/abs/2502.05986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.05986">https://arxiv.org/pdf/2502.05986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.05986]] Preventing Rogue Agents Improves Multi-Agent Collaboration(https://arxiv.org/abs/2502.05986)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Multi-agent systems, where specialized agents collaborate to solve a shared task hold great potential, from increased modularity to simulating complex environments. However, they also have a major caveat -- a single agent can cause the entire system to fail. Consider a simple game where the knowledge to solve the task is distributed between agents, which share information in a communication channel. At each round, any of the agents can terminate the game and make the final prediction, even if they are uncertain about the outcome of their action. Detection of such rogue agents $\textit{before they act}$ may prevent the system's failure. In this work, we propose to $\textit{monitor}$ agents during action prediction and $\textit{intervene}$ when a future error is likely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent collaboration environment that allows modular control over task complexity and communication structure. Experiments on two variants of WhoDunitEnv and the GovSim environment for resource sustainability show that our approach leads to substantial performance gains up to 17.4% and 20%, respectively. Moreover, a thorough analysis shows that our monitors successfully identify critical points of agent confusion and our interventions effectively stop agent errors from propagating.</li>
<li><strong>摘要：</strong>多代理系统，专门的代理商协作解决共享任务的潜力很大，从增加模块化到模拟复杂环境。但是，他们也有一个主要的警告 - 单个代理会导致整个系统失败。考虑一个简单的游戏，其中解决任务的知识分布在代理之间，这些游戏在通信渠道中共享信息。在每一轮中，任何代理商都可以终止游戏并做出最终预测，即使他们不确定其行动的结果。检测此类流氓代理$ \ textit {它们在ACT} $之前{它们可能会阻止系统的故障。在这项工作中，我们建议在操作预测期间向$ \ textit {Monitor} $代理，而$ \ textit {Isvesene} $可能会发生未来错误。为了测试我们的方法，我们介绍了Whodunitenv，这是一个多代理协作环境，可以模块化对任务复杂性和通信结构的控制。对Whodunitenv的两个变体和资源可持续性环境的实验表明，我们的方法可分别增长高达17.4％和20％。此外，彻底的分析表明，我们的监视器成功地确定了代理混淆的关键点，并且我们的干预措施有效地阻止了代理错误传播。</li>
</ul>

<h3>Title: Analysis of LLM as a grammatical feature tagger for African American English</h3>
<ul>
<li><strong>Authors: </strong>Rahul Porwal, Alice Rozet, Pryce Houck, Jotsna Gowda, Sarah Moeller, Kevin Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06004">https://arxiv.org/abs/2502.06004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06004">https://arxiv.org/pdf/2502.06004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06004]] Analysis of LLM as a grammatical feature tagger for African American English(https://arxiv.org/abs/2502.06004)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>African American English (AAE) presents unique challenges in natural language processing (NLP). This research systematically compares the performance of available NLP models--rule-based, transformer-based, and large language models (LLMs)--capable of identifying key grammatical features of AAE, namely Habitual Be and Multiple Negation. These features were selected for their distinct grammatical complexity and frequency of occurrence. The evaluation involved sentence-level binary classification tasks, using both zero-shot and few-shot strategies. The analysis reveals that while LLMs show promise compared to the baseline, they are influenced by biases such as recency and unrelated features in the text such as formality. This study highlights the necessity for improved model training and architectural adjustments to better accommodate AAE's unique linguistic characteristics. Data and code are available.</li>
<li><strong>摘要：</strong>非裔美国人英语（AAE）在自然语言处理（NLP）中提出了独特的挑战。这项研究系统地比较了可用的基于压力的NLP模型，基于变压器和大语言模型（LLMS）的性能 - 能够识别AAE的关键语法特征，即习惯性BE和多重否定。选择这些特征是因为它们独特的语法复杂性和发生的频率。评估涉及句子级二进制分类任务，同时使用零拍和少量策略。分析表明，尽管LLM与基线相比表现出希望，但它们受到偏见的影响，例如新颖性和文本中的不相关特征，例如形式。这项研究强调了改进模型培训和建筑调整的必要性，以更好地适应AAE独特的语言特征。数据和代码可用。</li>
</ul>

<h3>Title: LM2: Large Memory Models</h3>
<ul>
<li><strong>Authors: </strong>Jikun Kang, Wenqi Wu, Filippos Christianos, Alex J. Chan, Fraser Greenlee, George Thomas, Marvin Purtorab, Andy Toulis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06049">https://arxiv.org/abs/2502.06049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06049">https://arxiv.org/pdf/2502.06049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06049]] LM2: Large Memory Models(https://arxiv.org/abs/2502.06049)</code><input type="text"></li>
<li><strong>Keywords: </strong>long context</a></li>
<li><strong>Abstract: </strong>This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.</li>
<li><strong>摘要：</strong>本文介绍了大型内存模型（LM2），这是一种使用辅助内存模块增强的仅解码器变压器体系结构，旨在解决多步推理，关系论证和合成在长上下文中分布的信息中标准变压器的局限性。提出的LM2结合了一个充当上下文表示存储库的内存模块，通过交叉注意与输入令牌进行交互，并通过门控机制进行更新。为了保留变形金刚的通用功能，LM2在集成互补的内存途径时保持原始信息流。 Babilong基准测试的实验结果表明，LM2模型的表现均优于记忆增强的RMT模型37.1％，而基线Llama-3.2模型平均平均为86.3％。 LM2在多跳上推理，数值推理和大写问题的效能方面具有出色的功能。在MMLU数据集上，它比预先训练的香草模型可获得5.0％的提高，这表明其内存模块不会在一般任务上降低性能。此外，在我们的分析中，我们探讨了内存解释性，内存模块的有效性和测试时间行为。我们的发现强调了明确记忆在增强变形金刚体系结构中的重要性。</li>
</ul>

<h3>Title: Benchmarking Prompt Sensitivity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Razavi, Mina Soltangheis, Negar Arabzadeh, Sara Salamat, Morteza Zihayat, Ebrahim Bagheri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06065">https://arxiv.org/abs/2502.06065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06065">https://arxiv.org/pdf/2502.06065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06065]] Benchmarking Prompt Sensitivity in Large Language Models(https://arxiv.org/abs/2502.06065)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language Models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset PromptSET designed to investigate the effects of slight prompt variations on LLM performance. Using TriviaQA and HotpotQA datasets as the foundation of our work, we generate prompt variations and evaluate their effectiveness across multiple LLMs. We benchmark the prompt sensitivity prediction task employing state-of-the-art methods from related tasks, including LLM-based self-evaluation, text classification, and query performance prediction techniques. Our findings reveal that existing methods struggle to effectively address prompt sensitivity prediction, underscoring the need to understand how information needs should be phrased for accurate LLM responses.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）对及时配方的变化高度敏感，这可能会显着影响其产生准确响应的能力。在本文中，我们介绍了一项新任务，及时的灵敏度预测以及旨在调查较小及时变化对LLM性能的影响的数据集提示集。使用Triviaqa和HotPotQA数据集作为我们工作的基础，我们会产生及时的变化，并评估它们在多个LLM的有效性。我们采用来自相关任务的最新方法，包括基于LLM的自我评估，文本分类和查询性能预测技术，基于迅速的灵敏度预测任务。我们的发现表明，现有的方法难以有效解决及时的灵敏度预测，强调了应如何理解信息需求的需求，以进行准确的LLM响应。</li>
</ul>

<h3>Title: Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type</h3>
<ul>
<li><strong>Authors: </strong>Seokwon Song, Taehyun Lee, Jaewoo Ahn, Jae Hyuk Sung, Gunhee Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06086">https://arxiv.org/abs/2502.06086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06086">https://arxiv.org/pdf/2502.06086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06086]] Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type(https://arxiv.org/abs/2502.06086)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Conceptual combination is a cognitive process that merges basic concepts, enabling the creation of complex expressions. During this process, the properties of combination (e.g., the whiteness of a peeled apple) can be inherited from basic concepts, newly emerge, or be canceled. However, previous studies have evaluated a limited set of properties and have not examined the generative process. To address this gap, we introduce the Conceptual Combination with Property Type dataset (CCPT), which consists of 12.3K annotated triplets of noun phrases, properties, and property types. Using CCPT, we establish three types of tasks to evaluate LLMs for conceptual combination thoroughly. Our key findings are threefold: (1) Our automatic metric grading property emergence and cancellation closely corresponds with human judgments. (2) LLMs, including OpenAI's o1, struggle to generate noun phrases which possess given emergent properties. (3) Our proposed method, inspired by cognitive psychology model that explains how relationships between concepts are formed, improves performances in all generative tasks. The dataset and experimental code are available at this https URL.</li>
<li><strong>摘要：</strong>概念组合是一个认知过程，可以融合基本概念，从而创建复杂的表达式。在此过程中，可以从基本概念，新出现或取消组合的组合属性（例如，剥落的苹果的白度）。但是，先前的研究评估了一组有限的属性，尚未检查生成过程。为了解决此差距，我们将概念组合与属性类型数据集（CCPT）介绍，该概念类型数据集（CCPT）由名词短语，属性和属性类型的12.3k注释的三胞胎组成。使用CCPT，我们建立了三种类型的任务，以彻底评估LLM的LLM。我们的主要发现是三重：（1）我们的自动度量分级财产出现和取消与人类判断密切相对应。 （2）LLM，包括OpenAI的O1，难以产生具有赋予新兴属性的名词短语。 （3）我们提出的方法，灵感来自认知心理学模型，该模型解释了如何形成概念之间的关系，可以改善所有生成任务的性能。数据集和实验代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: ConMeC: A Dataset for Metonymy Resolution with Common Nouns</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Ghosh, Tianyu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06087">https://arxiv.org/abs/2502.06087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06087">https://arxiv.org/pdf/2502.06087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06087]] ConMeC: A Dataset for Metonymy Resolution with Common Nouns(https://arxiv.org/abs/2502.06087)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Metonymy plays an important role in our daily communication. People naturally think about things using their most salient properties or commonly related concepts. For example, by saying "The bus decided to skip our stop today," we actually mean that the bus driver made the decision, not the bus. Prior work on metonymy resolution has mainly focused on named entities. However, metonymy involving common nouns (such as desk, baby, and school) is also a frequent and challenging phenomenon. We argue that NLP systems should be capable of identifying the metonymic use of common nouns in context. We create a new metonymy dataset ConMeC, which consists of 6,000 sentences, where each sentence is paired with a target common noun and annotated by humans to indicate whether that common noun is used metonymically or not in that context. We also introduce a chain-of-thought based prompting method for detecting metonymy using large language models (LLMs). We evaluate our LLM-based pipeline, as well as a supervised BERT model on our dataset and three other metonymy datasets. Our experimental results demonstrate that LLMs could achieve performance comparable to the supervised BERT model on well-defined metonymy categories, while still struggling with instances requiring nuanced semantic understanding. Our dataset is publicly available at: this https URL.</li>
<li><strong>摘要：</strong>转喻在我们的日常沟通中起着重要作用。人们自然会使用其最显着的属性或通常相关的概念来思考事物。例如，通过说“巴士决定今天跳过我们的停靠站”，我们实际上是指公共汽车司机做出了决定，而不是公共汽车。关于转喻分辨率的先前工作主要集中在指定实体上。但是，涉及常见名词（例如桌子，婴儿和学校）的转喻也是一种常见且充满挑战的现象。我们认为，NLP系统应能够识别上下文中常见名词的代名词使用。我们创建了一个新的转喻数据集conmec，该数据集ConMec由6,000个句子组成，其中每个句子都与目标通用名词配对并由人注释，以指示在这种情况下是否在该上下文中使用该通用名词。我们还引入了一种基于思想链的提示方法，用于使用大语言模型（LLMS）检测转喻。我们评估了基于LLM的管道以及数据集中的监督BERT模型和其他三个Metnymy数据集。我们的实验结果表明，LLM可以实现与定义明确的转词类别的监督BERT模型相媲美的性能，同时仍在与需要细微的语义理解的实例斗争。我们的数据集可公开可用：此HTTPS URL。</li>
</ul>

<h3>Title: Task-driven Layerwise Additive Activation Intervention</h3>
<ul>
<li><strong>Authors: </strong>Hieu Trung Nguyen, Bao Nguyen, Binh Nguyen, Viet Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06115">https://arxiv.org/abs/2502.06115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06115">https://arxiv.org/pdf/2502.06115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06115]] Task-driven Layerwise Additive Activation Intervention(https://arxiv.org/abs/2502.06115)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Modern language models (LMs) have significantly advanced generative modeling in natural language processing (NLP). Despite their success, LMs often struggle with adaptation to new contexts in real-time applications. A promising approach to task adaptation is activation intervention, which steers the LMs' generation process by identifying and manipulating the activations. However, existing interventions are highly dependent on heuristic rules or require many prompt inputs to determine effective interventions. This paper proposes a layer-wise additive activation intervention framework that optimizes the intervention process, thus enhancing the sample efficiency. We benchmark our framework on various datasets, demonstrating improvements in the accuracy of pre-trained LMs and competing intervention baselines.</li>
<li><strong>摘要：</strong>现代语言模型（LMS）在自然语言处理（NLP）中具有明显的高级生成建模。尽管他们取得了成功，但LMS经常在适应实时应用程序中的新环境中挣扎。一种有前途的任务适应方法是激活干预措施，它通过识别和操纵激活来引导LMS的生成过程。但是，现有的干预措施高度取决于启发式规则，或者需要许多及时的输入来确定有效的干预措施。本文提出了一个按层添加剂激活框架，以优化干预过程，从而提高样品效率。我们在各种数据集上进行基准测试框架，证明了预先训练的LMS和竞争干预基线的准确性的提高。</li>
</ul>

<h3>Title: LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, Paul Hongsuck Seo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06139">https://arxiv.org/abs/2502.06139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06139">https://arxiv.org/pdf/2502.06139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06139]] LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs(https://arxiv.org/abs/2502.06139)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) excel in generating coherent and contextually rich outputs, their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings. Additionally, the computational cost of processing long sequences increases quadratically, making it challenging to extend context length. To address these challenges, we propose Long-form Context Injection with Recurrent Compression (LCIRC), a method that enables the efficient processing long-form sequences beyond the model's length limit through recurrent compression without retraining the entire model. We further introduce query dependent context modeling, which selectively compresses query-relevant information, ensuring that the model retains the most pertinent content. Our empirical results demonstrate that Query Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage extended contexts, making it well-suited for tasks that require both comprehensive context understanding and query relevance.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLMS）在生成相干和上下文丰富的输出方面表现出色，但它们有效处理长形环境的能力受固定长度位置嵌入的限制。此外，处理长序列的计算成本四次增加，这使得延长上下文长度具有挑战性。为了应对这些挑战，我们建议使用复发压缩（LCIRC）进行长形式上下文注入，该方法可以通过复发的压缩而无需重新训练整个模型，从而使有效的处理长形序列超出了模型的长度限制。我们进一步介绍了相关的上下文建模，该建模有选择地压缩与查询相关的信息，以确保模型保留最相关的内容。我们的经验结果表明，依赖依赖的LCIRC（QD-LCIRC）显着提高了LLM管理扩展上下文的能力，使其非常适合需要全面的上下文理解和查询相关性的任务。</li>
</ul>

<h3>Title: LegalViz: Legal Text Visualization by Text To Diagram Generation</h3>
<ul>
<li><strong>Authors: </strong>Eri Onami, Taiki Miyanishi, Koki Maeda, Shuhei Kurita</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06147">https://arxiv.org/abs/2502.06147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06147">https://arxiv.org/pdf/2502.06147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06147]] LegalViz: Legal Text Visualization by Text To Diagram Generation(https://arxiv.org/abs/2502.06147)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Legal documents including judgments and court orders require highly sophisticated legal knowledge for understanding. To disclose expert knowledge for non-experts, we explore the problem of visualizing legal texts with easy-to-understand diagrams and propose a novel dataset of LegalViz with 23 languages and 7,010 cases of legal document and visualization pairs, using the DOT graph description language of Graphviz. LegalViz provides a simple diagram from a complicated legal corpus identifying legal entities, transactions, legal sources, and statements at a glance, that are essential in each judgment. In addition, we provide new evaluation metrics for the legal diagram visualization by considering graph structures, textual similarities, and legal contents. We conducted empirical studies on few-shot and finetuning large language models for generating legal diagrams and evaluated them with these metrics, including legal content-based evaluation within 23 languages. Models trained with LegalViz outperform existing models including GPTs, confirming the effectiveness of our dataset.</li>
<li><strong>摘要：</strong>包括判决和法院命令在内的法律文件需要高度复杂的法律知识才能理解。为了披露非专家的专家知识，我们探讨了使用易于理解的图表来可视化法律文本的问题，并使用点图形说明语言，提出了具有23种语言和7,010个法律文档和可视化案例的法律案例和7,010箱法律文档和可视化案例的问题Graphviz。 Legalviz从复杂的法律语料库中提供了一个简单的图表，以识别法律实体，交易，法律来源和陈述，这在每个判断中都是必不可少的。此外，我们通过考虑图形结构，文本相似性和法律内容来为法律图可视化提供新的评估指标。我们对几乎没有射击的大语言模型进行了实证研究，以生成法律图表，并使用这些指标对其进行了评估，包括23种语言中的基于法律内容的评估。接受LegalViz培训的模型优于包括GPT在内的现有模型，证实了我们数据集的有效性。</li>
</ul>

<h3>Title: Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection</h3>
<ul>
<li><strong>Authors: </strong>Yan Weng, Fengbin Zhu, Tong Ye, Haoyan Liu, Fuli Feng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06148">https://arxiv.org/abs/2502.06148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06148">https://arxiv.org/pdf/2502.06148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06148]] Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection(https://arxiv.org/abs/2502.06148)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG), which integrates external knowledge into Large Language Models (LLMs), has proven effective in enabling LLMs to produce more accurate and reliable responses. However, it remains a significant challenge how to effectively integrate external retrieved knowledge with internal parametric knowledge in LLMs. In this work, we propose a novel Self-Selection RAG framework, where the LLM is made to select from pairwise responses generated with internal parametric knowledge solely and with external retrieved knowledge together to achieve enhanced accuracy. To this end, we devise a Self-Selection-RGP method to enhance the capabilities of the LLM in both generating and selecting the correct answer, by training the LLM with Direct Preference Optimization (DPO) over a curated Retrieval Generation Preference (RGP) dataset. Experimental results with two open-source LLMs (i.e., Llama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our approach over other baseline methods on Natural Questions (NQ) and TrivialQA datasets.</li>
<li><strong>摘要：</strong>将外部知识集成到大语言模型（LLMS）中的检索型生成（RAG）已被证明有效地使LLMS能够产生更准确，更可靠的响应。但是，如何有效地将外部检索知识与LLMS中的内部参数知识整合在一起仍然是一个重大挑战。在这项工作中，我们提出了一个新颖的自我选择抹布框架，在其中，LLM可以从使用内部参数知识生成的成对响应中进行选择，并以外部检索的知识共同选择，以实现增强的精度。为此，我们设计了一种自我选择RGP方法，以通过训练LLM对LLM进行直接偏好优化（DPO）的培训，以增强LLM在生成和选择正确的答案中的功能。两个开源LLM（即Llama2-13B-Chat和Mismtral-7b）的实验结果很好地证明了我们的方法优于其他基线方法对自然问题（NQ）和TrivialQA数据集的优越性。</li>
</ul>

<h3>Title: Scaling Public Health Text Annotation: Zero-Shot Learning vs. Crowdsourcing for Improved Efficiency and Labeling Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Kamyar Kazari, Yong Chen, Zahra Shakeri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06150">https://arxiv.org/abs/2502.06150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06150">https://arxiv.org/pdf/2502.06150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06150]] Scaling Public Health Text Annotation: Zero-Shot Learning vs. Crowdsourcing for Improved Efficiency and Labeling Accuracy(https://arxiv.org/abs/2502.06150)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Public health researchers are increasingly interested in using social media data to study health-related behaviors, but manually labeling this data can be labor-intensive and costly. This study explores whether zero-shot labeling using large language models (LLMs) can match or surpass conventional crowd-sourced annotation for Twitter posts related to sleep disorders, physical activity, and sedentary behavior. Multiple annotation pipelines were designed to compare labels produced by domain experts, crowd workers, and LLM-driven approaches under varied prompt-engineering strategies. Our findings indicate that LLMs can rival human performance in straightforward classification tasks and significantly reduce labeling time, yet their accuracy diminishes for tasks requiring more nuanced domain knowledge. These results clarify the trade-offs between automated scalability and human expertise, demonstrating conditions under which LLM-based labeling can be efficiently integrated into public health research without undermining label quality.</li>
<li><strong>摘要：</strong>公共卫生研究人员越来越有兴趣使用社交媒体数据来研究与健康相关的行为，但是手动标记这些数据可能是劳动密集型且昂贵的。这项研究探讨了使用大语言模型（LLMS）的零射击标签是否可以匹配或超过与睡眠障碍，体育活动和久坐行为有关的Twitter帖子的常规众群注释。设计了多个注释管道，以比较由域名专家，人群工人和LLM驱动的方法在各种及时的工程策略下制作的标签。我们的发现表明，LLM可以在直接分类任务中与人类绩效相抗衡，并大大减少标签时间，但是它们的准确性降低了需要更细微的领域知识的任务。这些结果阐明了自动可伸缩性和人类专业知识之间的权衡，证明了可以有效地纳入公共卫生研究中的情况下的条件，而不会破坏标签质量。</li>
</ul>

<h3>Title: Non-literal Understanding of Number Words by Language Models</h3>
<ul>
<li><strong>Authors: </strong>Polina Tsvilodub, Kanishk Gandhi, Haoran Zhao, Jan-Philipp Fränken, Michael Franke, Noah D. Goodman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06204">https://arxiv.org/abs/2502.06204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06204">https://arxiv.org/pdf/2502.06204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06204]] Non-literal Understanding of Number Words by Language Models(https://arxiv.org/abs/2502.06204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Humans naturally interpret numbers non-literally, effortlessly combining context, world knowledge, and speaker intent. We investigate whether large language models (LLMs) interpret numbers similarly, focusing on hyperbole and pragmatic halo effects. Through systematic comparison with human data and computational models of pragmatic reasoning, we find that LLMs diverge from human interpretation in striking ways. By decomposing pragmatic reasoning into testable components, grounded in the Rational Speech Act framework, we pinpoint where LLM processing diverges from human cognition -- not in prior knowledge, but in reasoning with it. This insight leads us to develop a targeted solution -- chain-of-thought prompting inspired by an RSA model makes LLMs' interpretations more human-like. Our work demonstrates how computational cognitive models can both diagnose AI-human differences and guide development of more human-like language understanding capabilities.</li>
<li><strong>摘要：</strong>人类自然地解释了非文字，毫不费力地结合上下文，世界知识和说话者意图的数字。我们研究大型语言模型（LLMS）是否类似地解释数字，重点是夸张和务实的光环效应。通过与人类数据和务实推理的计算模型进行系统的比较，我们发现LLMs以惊人的方式与人类的解释不同。通过将务实的推理分解为可测试的组成部分，基于理性的语音ACT框架，我们指出了LLM处理与人类认知有所不同的位置 - 不是先验知识，而是在其推理方面。这种见解使我们开发了一个有针对性的解决方案 - 受RSA模型启发的基础链，使LLMS的解释更像是人类的。我们的工作表明，计算认知模型如何既可以诊断AI-Human差异，又可以指导更类似人类的语言理解能力的发展。</li>
</ul>

<h3>Title: C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Guoxin Chen, Minpeng Liao, Peiying Yu, Dingmin Wang, Zile Qiao, Chao Yang, Xin Zhao, Kai Fan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06205">https://arxiv.org/abs/2502.06205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06205">https://arxiv.org/pdf/2502.06205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06205]] C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation(https://arxiv.org/abs/2502.06205)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior -- typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.</li>
<li><strong>摘要：</strong>检索增强的生成（RAG）系统在对齐独立开发的猎犬和大型语言模型（LLMS）方面面临着基本挑战。现有方法通常涉及修改组件或引入简单的中间模块，从而导致实际局限性和次优性能。受人类搜索行为的启发 - 通常涉及来回提出搜索查询和审查文档的过程，我们提出了C-3PO，C-3PO是一个以代理框架为中心的框架，可通过轻巧的多人系统来促进回猎犬和LLMS之间的通信。我们的框架实现了三种专门的代理，可协作优化整个破布管道，而无需更改猎犬和LLM。这些代理人共同努力，以评估检索，生成有效的查询并选择适合LLM的信息。为了实现有效的多机构协调，我们开发了一种树木结构化的推出方法，以奖励增强学习中的信用分配。在内域和分布外场景中进行的广泛实验表明，C-3PO在保持插入式灵活性和出色的概括能力的同时显着提高了破布性能。</li>
</ul>

<h3>Title: Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement</h3>
<ul>
<li><strong>Authors: </strong>Junyu Lu, Kai Ma, Kaichun Wang, Kelaiti Xiao, Roy Ka-Wei Lee, Bo Xu, Liang Yang, Hongfei Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06207">https://arxiv.org/abs/2502.06207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06207">https://arxiv.org/pdf/2502.06207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06207]] Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement(https://arxiv.org/abs/2502.06207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>LLMs are widely used for offensive language detection due to their advanced capability. However, the challenges posed by human annotation disagreement in real-world datasets remain underexplored. These disagreement samples are difficult to detect due to their ambiguous nature. Additionally, the confidence of LLMs in processing disagreement samples can provide valuable insights into their alignment with human annotators. To address this gap, we systematically evaluate the ability of LLMs to detect offensive language with annotation disagreement. We compare the binary accuracy of multiple LLMs across varying annotation agreement levels and analyze the relationship between LLM confidence and annotation agreement. Furthermore, we investigate the impact of disagreement samples on LLM decision-making during few-shot learning and instruction fine-tuning. Our findings highlight the challenges posed by disagreement samples and offer guidance for improving LLM-based offensive language detection.</li>
<li><strong>摘要：</strong>LLM由于其高级功能而被广泛用于进攻性语言检测。然而，人类注释分歧在现实世界数据集中构成的挑战仍然没有被逐渐忽视。这些分歧样本由于性质模棱两可而难以检测。此外，LLMS对处理分歧样本的信心可以为它们与人类注释的一致性提供宝贵的见解。为了解决这一差距，我们系统地评估了LLMs通过注释分歧检测进攻性语言的能力。我们比较了多个LLM在不同的注释协议水平上的二进制准确性，并分析LLM置信度和注释协议之间的关系。此外，我们研究了分歧样本对LLM在几次学习和教学微调过程中对LLM决策的影响。我们的发现强调了分歧样本带来的挑战，并为改善基于LLM的进攻语言检测提供了指导。</li>
</ul>

<h3>Title: Examining False Positives under Inference Scaling for Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Nan Yang, Liang Wang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06217">https://arxiv.org/abs/2502.06217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06217">https://arxiv.org/pdf/2502.06217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06217]] Examining False Positives under Inference Scaling for Mathematical Reasoning(https://arxiv.org/abs/2502.06217)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks. However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps. This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths. In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem solving for language models. We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies. Specifically, we explore how false positives influence the inference time scaling behavior of language models. Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to false positives, suggesting a significantly lower scaling ceiling than what automatic evaluations indicate. Additionally, we analyze specific instances of false positives and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions.</li>
<li><strong>摘要：</strong>语言模型的最新进展导致了各种基准的数学推理的显着改善。但是，这些基准中的大多数都依赖于自动评估方法，这些方法仅使用启发式方法比较最终答案，而无需验证基本的推理步骤。这种限制会导致假阳性解决方案，其中模型可能会产生正确的最终答案，但具有有缺陷的推力路径。在本文中，我们系统地研究了语言模型的数学问题解决方案中假阳性解决方案的普遍性。我们在不同的开源模型，各种难度级别的数据集以及解码策略中分析了此问题的特征和程度。具体来说，我们探讨了误报如何影响语言模型的推理时间缩放行为。我们的实验结果表明：（1）假阳性解决方案在不同的模型，数据集和解码方法中持续存在，（2）基于抽样的推理时间缩放方法并不能减轻问题，并且（3）Pass@n评估指标是比自动评估所指出的更容易受到假阳性的影响，表明比例天花板明显低得多。此外，我们分析了误报的特定实例，并讨论了在这种情况下自我改善技术和合成数据生成的潜在局限性。</li>
</ul>

<h3>Title: Confidence Improves Self-Consistency in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, Gal Yona</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06233">https://arxiv.org/abs/2502.06233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06233">https://arxiv.org/pdf/2502.06233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06233]] Confidence Improves Self-Consistency in LLMs(https://arxiv.org/abs/2502.06233)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Self-consistency decoding enhances LLMs' performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.</li>
<li><strong>摘要：</strong>自洽解码通过对不同的推理路径进行采样并选择最常见的答案，从而增强了LLM在推理任务上的性能。但是，它在计算上是昂贵的，因为需要对许多（冗长）的许多路径进行采样，以增加正确答案作为最常见的机会出现的机会。为了解决这个问题，我们介绍了信心知识的自洽（CISC）。 CISC根据直接从模型获得的置信度得分进行加权多数票。通过优先考虑高信心路径，它可以识别出明显较小的样本量的正确答案。当对九个型号和四个数据集进行测试时，CISC几乎在所有配置中都超过了自矛盾，使所需的推理路径的平均数量平均降低了40％以上。此外，我们介绍了问题内置信度评估的概念，这表明标准评估方法在区分同一问题的正确和错误答案方面成功的预测因素很差。实际上，最受校准的置信度被证明是CISC最不可能的。最后，除了这些实际含义之外，我们的结果和分析表明，LLM可以有效判断其自己的产出的正确性，这有助于对该主题的持续辩论。</li>
</ul>

<h3>Title: K-ON: Stacking Knowledge On the Head Layer of Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Lingbing Guo, Yichi Zhang, Zhongpu Bo, Zhuo Chen, Mengshu Sun, Zhiqiang Zhang, Wen Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06257">https://arxiv.org/abs/2502.06257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06257">https://arxiv.org/pdf/2502.06257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06257]] K-ON: Stacking Knowledge On the Head Layer of Large Language Model(https://arxiv.org/abs/2502.06257)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have significantly improved various natural language processing (NLP) tasks. Typically, LLMs are trained to predict the next token, aligning well with many NLP tasks. However, in knowledge graph (KG) scenarios, entities are the fundamental units and identifying an entity requires at least several tokens. This leads to a granularity mismatch between KGs and natural languages. To address this issue, we propose K-ON, which integrates KG knowledge into the LLM by employing multiple head layers for next k-step prediction. K-ON can not only generate entity-level results in one step, but also enables contrastive loss against entities, which is the most powerful tool in KG representation learning. Experimental results show that K-ON outperforms state-of-the-art methods that incorporate text and even the other modalities.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展已大大改善了各种自然语言处理（NLP）任务。通常，LLM经过训练可以预测下一步的标记，并与许多NLP任务保持一致。但是，在知识图（kg）场景中，实体是基本单元，并且识别实体至少需要几个令牌。这导致kgs和自然语言之间的粒度不匹配。为了解决这个问题，我们提出了K-ON，该K-ON通过为下一个K-STEP预测采用多个头层来将KG知识整合到LLM中。 K-ON不仅可以在一个步骤中生成实体级别的结果，而且还可以对实体产生对比损失，这是KG表示学习中最强大的工具。实验结果表明，K-ON优于结合文本甚至其他方式的最先进方法。</li>
</ul>

<h3>Title: Emergent Response Planning in LLM</h3>
<ul>
<li><strong>Authors: </strong>Zhichen Dong, Zhanhui Zhou, Zhixuan Liu, Chao Yang, Chaochao Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06258">https://arxiv.org/abs/2502.06258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06258">https://arxiv.org/pdf/2502.06258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06258]] Emergent Response Planning in LLM(https://arxiv.org/abs/2502.06258)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\textit{structural attributes}$ (response length, reasoning steps), $\textit{content attributes}$ (character choices in storywriting, multiple-choice answers at the end of response), and $\textit{behavioral attributes}$ (answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggests potential applications for improving transparency and generation control.</li>
<li><strong>摘要：</strong>在这项工作中，我们认为，大型语言模型（LLMS）虽然受过训练，但仅预测了下一个令牌，但展示了新兴的计划行为：$ \ textbf {他们的隐藏表示形式编码了未来的未来输出，超出了接下来的标记} $。通过简单的探测，我们证明了llm提示表示表示其整个响应的全局属性，包括$ \ textIt {structural属性} $（响应长度，推理步骤），$ \ textit {content {content属性} $（StoryWriting中的字符选择，多个，多个 - 选择在响应结束时答案），$ \ textit {行为属性} $（答案信心，事实一致性）。除了确定响应计划外，我们还探讨了它如何在任务范围内与模型大小缩放以及在发电过程中如何演变。 LLMS在其隐藏表示中为未来计划提前计划的发现提出了提高透明度和发电控制的潜在应用。</li>
</ul>

<h3>Title: DebateBench: A Challenging Long Context Reasoning Benchmark For Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Utkarsh Tiwari, Aryan Seth, Adi Mukherjee, Kaavya Mer, Kavish, Dhruv Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06279">https://arxiv.org/abs/2502.06279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06279">https://arxiv.org/pdf/2502.06279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06279]] DebateBench: A Challenging Long Context Reasoning Benchmark For Large Language Models(https://arxiv.org/abs/2502.06279)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context</a></li>
<li><strong>Abstract: </strong>We introduce DebateBench, a novel dataset consisting of an extensive collection of transcripts and metadata from some of the world's most prestigious competitive debates. The dataset consists of British Parliamentary debates from prestigious debating tournaments on diverse topics, annotated with detailed speech-level scores and house rankings sourced from official adjudication data. We curate 256 speeches across 32 debates with each debate being over 1 hour long with each input being an average of 32,000 tokens. Designed to capture long-context, large-scale reasoning tasks, DebateBench provides a benchmark for evaluating modern large language models (LLMs) on their ability to engage in argumentation, deliberation, and alignment with human experts. To do well on DebateBench, the LLMs must perform in-context learning to understand the rules and evaluation criteria of the debates, then analyze 8 seven minute long speeches and reason about the arguments presented by all speakers to give the final results. Our preliminary evaluation using GPT o1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on DebateBench, highlighting the need to develop more sophisticated techniques for improving their performance.</li>
<li><strong>摘要：</strong>我们介绍了DebateBench，这是一款新颖的数据集，其中包括来自世界上一些最负盛名的竞争辩论的大量成绩单和元数据。该数据集由英国议会的辩论组成，该辩论来自关于各种主题的著名辩论锦标赛，并注释了详细的语音水平分数和众议院的排名，并从官方的裁决数据中得出。我们在32场辩论中策划了256次演讲，每个辩论的长度超过1小时，每个输入平均为32,000个令牌。旨在捕获长篇小说，大规模推理任务的辩论板，为评估现代大型语言模型（LLMS）的能力提供了一个基准，以参与与人类专家进行论证，审议和一致性的能力。为了在辩论中表现出色，LLM必须执行内在的学习来了解辩论的规则和评估标准，然后分析所有演讲者提出的论点的8七分钟长的演讲和理由，以给出最终的结果。我们使用GPT O1，GPT-4O和Claude Haiku的初步评估表明，LLM努力在辩论中表现良好，强调了开发更复杂的技术以提高其性能的需求。</li>
</ul>

<h3>Title: Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE</h3>
<ul>
<li><strong>Authors: </strong>Haiduo Huang, Fuwei Yang, Zhenhua Liu, Yixing Xu, Jinze Li, Yang Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06282">https://arxiv.org/abs/2502.06282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06282">https://arxiv.org/pdf/2502.06282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06282]] Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE(https://arxiv.org/abs/2502.06282)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding (SD) accelerates large language model inference by using a smaller draft model to predict multiple tokens, which are then verified in parallel by the larger target model. However, the limited capacity of the draft model often necessitates tree-based sampling to improve prediction accuracy, where multiple candidates are generated at each step. We identify a key limitation in this approach: the candidates at the same step are derived from the same representation, limiting diversity and reducing overall effectiveness. To address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where independent experts generate diverse predictions, effectively decoupling correlations among candidates. Furthermore, we introduce a hybrid inference strategy, combining autoregressive decoding for initial tokens with parallel decoding for subsequent stages, and enhance the latter with contrastive mechanism in features to improve accuracy. Our method significantly boosts prediction accuracy and achieves higher inference speedups. Extensive experiments across diverse models validate the effectiveness and robustness of our approach, establishing a new SOTA in speculative decoding. Our codes are available at this https URL.</li>
<li><strong>摘要：</strong>投机解码（SD）通过使用较小的草稿模型来预测多个令牌，从而加速了大语言模型，然后通过较大的目标模型并行验证。但是，草案模型的有限容量通常需要基于树的采样来提高预测准确性，在每个步骤中都会生成多个候选者。我们确定了这种方法的关键限制：同一步骤的候选人源自相同的表示，限制了多样性并降低了整体效率。为了解决这个问题，我们提出了jakiro，利用专家的混合物（MOE），独立专家产生了多样的预测，有效地解开了候选人之间的相关性。此外，我们引入了一种混合推理策略，将自回归解码与以后阶段的平行解码相结合，并通过功能中的对比机制增强后者，以提高准确性。我们的方法显着提高了预测准确性，并实现了更高的推理加速。跨不同模型的广泛实验验证了我们方法的有效性和鲁棒性，并在投机解码中建立了新的SOTA。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia</h3>
<ul>
<li><strong>Authors: </strong>Chaoqun Liu, Wenxuan Zhang, Jiahao Ying, Mahani Aljunied, Anh Tuan Luu, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06298">https://arxiv.org/abs/2502.06298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06298">https://arxiv.org/pdf/2502.06298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06298]] SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia(https://arxiv.org/abs/2502.06298)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study introduces two novel benchmarks, SeaExam and SeaBench, designed to evaluate the capabilities of Large Language Models (LLMs) in Southeast Asian (SEA) application scenarios. Unlike existing multilingual datasets primarily derived from English translations, these benchmarks are constructed based on real-world scenarios from SEA regions. SeaExam draws from regional educational exams to form a comprehensive dataset that encompasses subjects such as local history and literature. In contrast, SeaBench is crafted around multi-turn, open-ended tasks that reflect daily interactions within SEA communities. Our evaluations demonstrate that SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks compared to their translated benchmarks. This highlights the importance of using real-world queries to assess the multilingual capabilities of LLMs.</li>
<li><strong>摘要：</strong>这项研究介绍了两个新型的基准测试，即Seexam和Seabench，旨在评估东南亚（SEA）应用程序方案的大型语言模型（LLMS）的能力。与主要源自英语翻译的现有多语言数据集不同，这些基准是根据来自海洋地区的现实世界情景构建的。 Seaexam从区域教育考试中汲取灵感，形成了一个综合数据集，其中包括当地历史和文学等主题。相比之下，Seabench是围绕多转的开放式任务而设计的，这些任务反映了海洋社区内的每日互动。我们的评估表明，与翻译的基准相比，Seexam和Seabench对海语任务的LLM表现更有效。这突出了使用现实世界查询来评估LLM的多语言功能的重要性。</li>
</ul>

<h3>Title: Latent Convergence Modulation in Large Language Models: A Novel Approach to Iterative Contextual Realignment</h3>
<ul>
<li><strong>Authors: </strong>Patricia Porretta, Sylvester Pakenham, Huxley Ainsworth, Gregory Chatten, Godfrey Allerton, Simon Hollingsworth, Vance Periwinkle</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06302">https://arxiv.org/abs/2502.06302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06302">https://arxiv.org/pdf/2502.06302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06302]] Latent Convergence Modulation in Large Language Models: A Novel Approach to Iterative Contextual Realignment(https://arxiv.org/abs/2502.06302)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Token prediction stability remains a challenge in autoregressive generative models, where minor variations in early inference steps often lead to significant semantic drift over extended sequences. A structured modulation mechanism was introduced to regulate hidden state transitions, ensuring that latent representation trajectories remain aligned with prior contextual dependencies while preserving generative flexibility. The modulation framework was designed to function within transformer-based architectures, dynamically constraining representation evolution without imposing external memory dependencies or extensive architectural modifications. Empirical evaluations demonstrated that structured latent adjustments contributed to reductions in perplexity fluctuations, entropy variance, and lexical instability, improving coherence in long-form text generation. Gradient propagation stability was further analyzed, revealing that the modulation process led to smoother optimization pathways, mitigating erratic fluctuations in weight updates across successive inference steps. The computational efficiency of the modulation process was assessed, showing that its integration within transformer-based architectures introduced only marginal overhead while maintaining compatibility with existing optimization frameworks. The structured modulation constraints also influenced syntactic variation, preventing excessive repetition while maintaining balanced sentence length distributions. Comparative evaluations against baseline models reinforced the role of controlled latent state evolution in improving pronoun resolution, logical consistency, and contextual alignment across autoregressive text generation tasks.</li>
<li><strong>摘要：</strong>在自回归生成模型中，令牌预测稳定性仍然是一个挑战，在早期推理步骤中的微小变化通常会导致扩展序列上的大量语义漂移。引入了一种结构化的调制机制来调节隐藏状态过渡，以确保潜在表示轨迹与先前的上下文依赖关系保持一致，同时保持生成柔性。该调制框架旨在在基于变压器的体系结构中发挥作用，动态约束表示的演变而不施加外部内存依赖性或广泛的体系结构修改。经验评估表明，结构化的潜在调整有助于减少困惑波动，熵差异和词汇不稳定性，从而提高了长形式文本生成的相干性。进一步分析了梯度的传播稳定性，表明调制过程导致了更顺畅的优化途径，从而减轻了连续推理步骤的重量更新中不稳定的波动。评估了调制过程的计算效率，表明其在基于变压器的架构中的集成仅引入了边缘开销，同时保持与现有优化框架的兼容性。结构化调制约束还影响了句法变化，在保持平衡的句子长度分布的同时，可以过度重复。针对基线模型的比较评估加强了受控潜在状态进化在改善自动回归文本生成任务之间的代词分辨率，逻辑一致性和上下文一致性方面的作用。</li>
</ul>

<h3>Title: Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the Correspondence between Patent Claim and Prior Art</h3>
<ul>
<li><strong>Authors: </strong>Hayato Ikoma, Teruko Mitamura</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06316">https://arxiv.org/abs/2502.06316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06316">https://arxiv.org/pdf/2502.06316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06316]] Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the Correspondence between Patent Claim and Prior Art(https://arxiv.org/abs/2502.06316)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Assessing the novelty of patent claims is a critical yet challenging task traditionally performed by patent examiners. While advancements in NLP have enabled progress in various patent-related tasks, novelty assessment remains unexplored. This paper introduces a novel challenge by evaluating the ability of large language models (LLMs) to assess patent novelty by comparing claims with cited prior art documents, following the process similar to that of patent examiners done. We present the first dataset specifically designed for novelty evaluation, derived from real patent examination cases, and analyze the capabilities of LLMs to address this task. Our study reveals that while classification models struggle to effectively assess novelty, generative models make predictions with a reasonable level of accuracy, and their explanations are accurate enough to understand the relationship between the target patent and prior art. These findings demonstrate the potential of LLMs to assist in patent evaluation, reducing the workload for both examiners and applicants. Our contributions highlight the limitations of current models and provide a foundation for improving AI-driven patent analysis through advanced models and refined datasets.</li>
<li><strong>摘要：</strong>评估专利主张的新颖性是专利审查员传统上执行的一项至关重要但充满挑战的任务。尽管NLP的进步已在各种专利相关的任务中取得了进展，但新颖性评估仍未开发。本文通过评估大型语言模型（LLMS）的能力来评估专利新颖性的能力，通过将主张与所引用的先前艺术文档进行比较，介绍了一项新的挑战。我们提供了第一个专门为新颖性评估而设计的数据集，源自真正的专利检查案例，并分析LLMS解决此任务的功能。我们的研究表明，虽然分类模型难以有效地评估新颖性，但生成模型以合理的准确性进行预测，并且它们的解释足够准确，足以了解目标专利与先前的艺术之间的关系。这些发现证明了LLMS有助于协助专利评估，减少审查员和申请人的工作量。我们的贡献突出了当前模型的局限性，并为通过高级模型和精制数据集改进AI驱动的专利分析提供了基础。</li>
</ul>

<h3>Title: Expect the Unexpected: FailSafe Long Context QA for Finance</h3>
<ul>
<li><strong>Authors: </strong>Kiran Kamble, Melisa Russak, Dmytro Mozolevskyi, Muayad Ali, Mateusz Russak, Waseem AlShikh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06329">https://arxiv.org/abs/2502.06329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06329">https://arxiv.org/pdf/2502.06329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06329]] Expect the Unexpected: FailSafe Long Context QA for Finance(https://arxiv.org/abs/2502.06329)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, long context</a></li>
<li><strong>Abstract: </strong>We propose a new long-context financial benchmark, FailSafeQA, designed to test the robustness and context-awareness of LLMs against six variations in human-interface interactions in LLM-based query-answer systems within finance. We concentrate on two case studies: Query Failure and Context Failure. In the Query Failure scenario, we perturb the original query to vary in domain expertise, completeness, and linguistic accuracy. In the Context Failure case, we simulate the uploads of degraded, irrelevant, and empty documents. We employ the LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained rating criteria to define and calculate Robustness, Context Grounding, and Compliance scores for 24 off-the-shelf models. The results suggest that although some models excel at mitigating input perturbations, they must balance robust answering with the ability to refrain from hallucinating. Notably, Palmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained strong baseline performance but encountered challenges in sustaining robust predictions in 17% of test cases. On the other hand, the most robust model, OpenAI o3-mini, fabricated information in 41% of tested cases. The results demonstrate that even high-performing models have significant room for improvement and highlight the role of FailSafeQA as a tool for developing LLMs optimized for dependability in financial applications. The dataset is available at: this https URL</li>
<li><strong>摘要：</strong>我们提出了一种新的长篇小说金融基准，FailSAFEQA，旨在测试LLM的稳健性和上下文意识，以与金融内LLM基于LLM的查询 - 答案系统中人际关系相互作用的六种变化。我们专注于两个案例研究：查询失败和上下文失败。在查询失败方案中，我们将原始查询扰动以在域专业知识，完整性和语言准确性方面变化。在上下文失败情况下，我们模拟了降级，无关紧要和空的文档的上传。我们使用QWEN2.5-72B教学的LLM-AS-A-A-A-A-Gudge方法，并使用细粒度的评分标准来定义和计算24个现成模型的鲁棒性，上下文接地和合规分数。结果表明，尽管某些模型在减轻输入扰动方面表现出色，但它们必须平衡强大的答案和避免幻觉的能力。值得注意的是，被认为是最合规的模型的Palmyra-Fin-128k教学法保持着强劲的基线表现，但在17％的测试用例中遇到了挑战。另一方面，最强大的模型是OpenAi O3-Mini，在41％的测试案例中捏造了信息。结果表明，即使是高性能模型也具有重大改进的空间，并突出了FailSAFEQA作为开发为财务应用中可靠性优化的LLM的工具的作用。该数据集可用网址：此HTTPS URL</li>
</ul>

<h3>Title: SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators</h3>
<ul>
<li><strong>Authors: </strong>Daniil Moskovskiy, Nikita Sushko, Sergey Pletenev, Elena Tutubalina, Alexander Panchenko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06394">https://arxiv.org/abs/2502.06394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06394">https://arxiv.org/pdf/2502.06394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06394]] SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators(https://arxiv.org/abs/2502.06394)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.</li>
<li><strong>摘要：</strong>平行多语言数据集的稀缺性，现有的多语言文本排毒方法受到了阻碍。在这项工作中，我们引入了一条流量，用于生成多语言并行排毒数据。我们还介绍了SyntheToxm，这是一种手动收集和合成生成的多语言平行文本排毒数据集，其中包括16,000个高质量的排毒句子对，遍布德语，法语，西班牙语和俄语。这些数据来自不同的毒性评估数据集，然后在几次射击设置中用9个现代开源LLM重写。我们的实验表明，在生产的合成数据集上训练的模型甚至在数据有限的设置中，在人类通知的多氧化苯甲酸酯数据集中训练的模型具有较高的性能。在合成毒素上训练的型号在几次射击设置中均优于所有评估的LLM。我们发布数据集和代码，以帮助进一步研究多语言文本排毒。</li>
</ul>

<h3>Title: Systematic Outliers in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06415">https://arxiv.org/abs/2502.06415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06415">https://arxiv.org/pdf/2502.06415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06415]] Systematic Outliers in Large Language Models(https://arxiv.org/abs/2502.06415)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Outliers have been widely observed in Large Language Models (LLMs), significantly impacting model performance and posing challenges for model compression. Understanding the functionality and formation mechanisms of these outliers is critically important. Existing works, however, largely focus on reducing the impact of outliers from an algorithmic perspective, lacking an in-depth investigation into their causes and roles. In this work, we provide a detailed analysis of the formation process, underlying causes, and functions of outliers in LLMs. We define and categorize three types of outliers-activation outliers, weight outliers, and attention outliers-and analyze their distributions across different dimensions, uncovering inherent connections between their occurrences and their ultimate influence on the attention mechanism. Based on these observations, we hypothesize and explore the mechanisms by which these outliers arise and function, demonstrating through theoretical derivations and experiments that they emerge due to the self-attention mechanism's softmax operation. These outliers act as implicit context-aware scaling factors within the attention mechanism. As these outliers stem from systematic influences, we term them systematic outliers. Our study not only enhances the understanding of Transformer-based LLMs but also shows that structurally eliminating outliers can accelerate convergence and improve model compression. The code is avilable at this https URL.</li>
<li><strong>摘要：</strong>在大语言模型（LLM）中广泛观察到离群值，从而显着影响模型性能，并为模型压缩带来挑战。了解这些异常值的功能和形成机制至关重要。但是，现有的作品在很大程度上集中于从算法的角度降低异常值的影响，而缺乏对其原因和角色的深入调查。在这项工作中，我们对LLMS中的组成过程，基本原因以及异常值的功能进行了详细的分析。我们定义并分类了三种类型的异常值激活异常值，权重异常值和注意力异常值，并在不同维度上分析其分布，从而发现其事件之间的固有联系及其对注意机制的最终影响。基于这些观察结果，我们假设并探索了这些异常值出现和功能的机制，并通过理论推导和实验证明了它们由于自我注意力的软性机制的软性操作而出现的。这些异常值在注意机制中充当隐式上下文感知的缩放因素。由于这些异常值源于系统的影响，我们将它们称为系统的异常值。我们的研究不仅增强了对基于变压器的LLM的理解，而且还表明，在结构上消除异常值可以加速收敛并改善模型压缩。该代码可在此HTTPS URL上提供。</li>
</ul>

<h3>Title: Beyond Literal Token Overlap: Token Alignability for Multilinguality</h3>
<ul>
<li><strong>Authors: </strong>Katharina Hämmerl, Tomasz Limisiewicz, Jindřich Libovický, Alexander Fraser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06468">https://arxiv.org/abs/2502.06468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06468">https://arxiv.org/pdf/2502.06468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06468]] Beyond Literal Token Overlap: Token Alignability for Multilinguality(https://arxiv.org/abs/2502.06468)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models. However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality. This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions. In this paper, we propose subword token alignability as a new way to understand the impact and quality of multilingual tokenisation. In particular, this metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low. We analyse this metric in the context of both encoder and decoder models, look at data size as a potential distractor, and discuss how this insight may be applied to multilingual tokenisation in future work. We recommend our subword token alignability metric for identifying optimal language pairs for cross-lingual transfer, as well as to guide the construction of better multilingual tokenisers in the future. We publish our code and reproducibility details.</li>
<li><strong>摘要：</strong>以前的工作已将令牌分布的重叠甚至相似性视为语言模型中多语言和跨语性知识转移的预测指标。但是，这些非常字面的指标为具有不同脚本的语言对分配了很大的距离，但是，这些脚本可以显示出良好的跨语言。这限制了代币与使用不同脚本或遵循不同拼字法的语言对之间的知识转移的解释强度重叠。在本文中，我们提出了子字代币的可点击性，作为了解多语言令牌化的影响和质量的新方法。特别是，当脚本不同并且文字代币的重叠时，该指标可以更好地预测多语言性。我们在编码器和解码器模型的背景下分析了该指标，将数据大小视为潜在的干扰因素，并讨论如何将这种见解应用于未来工作中的多语言标记。我们建议我们的子词令牌可符号性指标，以识别用于跨语言转移的最佳语言对，并指导未来更好的多语言标记者的构建。我们发布我们的代码和可重复性详细信息。</li>
</ul>

<h3>Title: A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks</h3>
<ul>
<li><strong>Authors: </strong>Hieu Minh "Jord" Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06470">https://arxiv.org/abs/2502.06470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06470">https://arxiv.org/pdf/2502.06470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06470]] A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks(https://arxiv.org/abs/2502.06470)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced LLM ToM capabilities, and suggest several research directions for effective evaluation and mitigation of these risks.</li>
<li><strong>摘要：</strong>心理理论（汤姆）是将精神状态归因于他人并预测其行为的能力，这是社会智力的基础。在本文中，我们调查了评估大语模型（LLMS）的行为和代表性TOM的研究，确定了高级LLM TOM功能的重要安全风险，并提出了一些研究方向，以有效评估和缓解这些风险。</li>
</ul>

<h3>Title: KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment</h3>
<ul>
<li><strong>Authors: </strong>Yuxing Lu, Jinzhuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06472">https://arxiv.org/abs/2502.06472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06472">https://arxiv.org/pdf/2502.06472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06472]] KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment(https://arxiv.org/abs/2502.06472)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\% LLM-verified correctness and reducing conflict edges by 18.6\% through multi-layer assessments.</li>
<li><strong>摘要：</strong>维持全面和最新的知识图（kgs）对于现代AI系统至关重要，但是随着科学文献的快速增长，手动策划努力进行扩展。本文介绍了业力，这是一种新型框架，采用多工具大型语言模型（LLMS），通过对非结构化文本的结构化分析来自动化kg富集。我们的方法采用九种协作代理，跨越实体发现，关系提取，模式一致性和冲突解决方案，迭代解析文档，验证提取的知识并将其集成到现有的图形结构中，同时粘贴域特异性模式。来自三个不同领域的1,200篇PubMed文章的实验证明了业力在知识图富集中的有效性，并确定了多达38,230个新实体，同时通过多层评估来实现83.1 \％LLM验证的正确性，并将冲突EDGES减少18.6 \％ 。</li>
</ul>

<h3>Title: Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Spliethöver, Tim Knebler, Fabian Fumagalli, Maximilian Muschalik, Barbara Hammer, Eyke Hüllermeier, Henning Wachsmuth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06487">https://arxiv.org/abs/2502.06487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06487">https://arxiv.org/pdf/2502.06487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06487]] Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection(https://arxiv.org/abs/2502.06487)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Recent advances on instruction fine-tuning have led to the development of various prompting techniques for large language models, such as explicit reasoning steps. However, the success of techniques depends on various parameters, such as the task, language model, and context provided. Finding an effective prompt is, therefore, often a trial-and-error process. Most existing approaches to automatic prompting aim to optimize individual techniques instead of compositions of techniques and their dependence on the input. To fill this gap, we propose an adaptive prompting approach that predicts the optimal prompt composition ad-hoc for a given input. We apply our approach to social bias detection, a highly context-dependent task that requires semantic understanding. We evaluate it with three large language models on three datasets, comparing compositions to individual techniques and other baselines. The results underline the importance of finding an effective prompt composition. Our approach robustly ensures high detection performance, and is best in several settings. Moreover, first experiments on other tasks support its generalizability.</li>
<li><strong>摘要：</strong>教学微调的最新进展导致开发了针对大型语言模型的各种提示技术，例如明确的推理步骤。但是，技术的成功取决于各种参数，例如所提供的任务，语言模型和上下文。因此，找到有效的提示通常是试验过程。自动提示的大多数现有方法旨在优化单个技术，而不是技术的组成及其对输入的依赖。为了填补这一空白，我们提出了一种自适应提示方法，该方法可以预测给定输入的最佳提示组合临时。我们将方法应用于社会偏见检测，这是一项高度依赖上下文的任务，需要语义理解。我们在三个数据集上使用三种大语言模型对其进行了评估，将构图与单个技术和其他基线进行了比较。结果强调了找到有效的及时组成的重要性。我们的方法可靠地确保高检测性能，并且在几种情况下都是最好的。此外，对其他任务的首次实验支持其普遍性。</li>
</ul>

<h3>Title: GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Duan, Xinyu Zhao, Zhuoxuan Zhang, Eunhye Ko, Lily Boddy, Chenan Wang, Tianhao Li, Alexander Rasgon, Junyuan Hong, Min Kyung Lee, Chenxi Yuan, Qi Long, Ying Ding, Tianlong Chen, Kaidi Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06494">https://arxiv.org/abs/2502.06494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06494">https://arxiv.org/pdf/2502.06494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06494]] GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing(https://arxiv.org/abs/2502.06494)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) succeed in human-guided conversations such as instruction following and question answering, the potential of LLM-guided conversations-where LLMs direct the discourse and steer the conversation's objectives-remains under-explored. In this study, we first characterize LLM-guided conversation into three fundamental components: (i) Goal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and propose GuideLLM as an installation. We then implement an interviewing environment for the evaluation of LLM-guided conversation. Specifically, various topics are involved in this environment for comprehensive interviewing evaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over 200 events mentioned during the interviewing for each chatbot evaluation. We compare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and Llama-3-70b-Instruct, from the perspective of interviewing quality, and autobiography generation quality. For automatic evaluation, we derive user proxies from multiple autobiographies and employ LLM-as-a-judge to score LLM behaviors. We further conduct a human-involved experiment by employing 45 human participants to chat with GuideLLM and baselines. We then collect human feedback, preferences, and ratings regarding the qualities of conversation and autobiography. Experimental results indicate that GuideLLM significantly outperforms baseline LLMs in automatic evaluation and achieves consistent leading performances in human ratings.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）在人类指导的对话中取得了成功，例如以下教学和问答，但LLM引导的对话的潜力 -  llms在其中指导话语并引导对话的目标被解释不足。在这项研究中，我们首先将LLM指导的对话描述为三个基本组成部分：（i）目标导航； （ii）上下文管理； （iii）移情参与，并提出吉德尔姆作为安装。然后，我们实施了一个面试环境，以评估LLM指导的对话。具体而言，在这种环境中涉及各种主题，以进行全面的访谈评估，从而大约有1.4k的话语，184K代币以及每次聊天机器人评估的访谈期间提到的200多个事件。我们将GuidellM与GPT-4O和Llama-3-70B-Instruct等6个最先进的LLM进行了比较，这是从面试质量和自传生成质量的角度进行比较。为了自动评估，我们从多个自传中得出用户代理，并采用LLM-AS-A-A-Gudge来评分LLM行为。我们通过雇用45名人类参与者与Guidellm和基线聊天，进一步进行了人参与的实验。然后，我们收集有关对话和自传质量的人类反馈，偏好和评分。实验结果表明，GuidElm在自动评估中的表现明显优于基线LLM，并在人类评分中达到一致的领先表现。</li>
</ul>

<h3>Title: Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jean Vassoyan, Nathanaël Beau, Roman Plaud</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06533">https://arxiv.org/abs/2502.06533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06533">https://arxiv.org/pdf/2502.06533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06533]] Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning(https://arxiv.org/abs/2502.06533)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.</li>
<li><strong>摘要：</strong>实现长期目标的能力是当前大型语言模型（LLM）开发的关键挑战。为了解决这个问题，可以通过增强学习（RL）微调预培训的LLM，以探索优化给定目标的解决方案。但是，使用LLM的探索很困难，因为在发现新的解决方案和保持足够近距离培训模型之间必须达到平衡，以免降低基本功能。这通常由kullback-leibler（kl）罚款控制。在本文中，我们在简单的算术任务上研究了小语言模型的探索动力学。我们展示了不同程度的训练前影响探索，并证明了“关键令牌”的重要性，这些探索对最终结果产生了巨大影响。因此，我们对KL罚款进行了简单的修改，该罚款有利于对关键令牌的探索，从而提高了RL微调阶段的效率。</li>
</ul>

<h3>Title: Efficient Scientific Full Text Classification: The Case of EICAT Impact Assessments</h3>
<ul>
<li><strong>Authors: </strong>Marc Felix Brinner, Sina Zarrieß</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06551">https://arxiv.org/abs/2502.06551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06551">https://arxiv.org/pdf/2502.06551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06551]] Efficient Scientific Full Text Classification: The Case of EICAT Impact Assessments(https://arxiv.org/abs/2502.06551)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study explores strategies for efficiently classifying scientific full texts using both small, BERT-based models and local large language models like Llama-3.1 8B. We focus on developing methods for selecting subsets of input sentences to reduce input size while simultaneously enhancing classification performance. To this end, we compile a novel dataset consisting of full-text scientific papers from the field of invasion biology, specifically addressing the impacts of invasive species. These papers are aligned with publicly available impact assessments created by researchers for the International Union for Conservation of Nature (IUCN). Through extensive experimentation, we demonstrate that various sources like human evidence annotations, LLM-generated annotations or explainability scores can be used to train sentence selection models that improve the performance of both encoder- and decoder-based language models while optimizing efficiency through the reduction in input length, leading to improved results even if compared to models like ModernBERT that are able to handle the complete text as input. Additionally, we find that repeated sampling of shorter inputs proves to be a very effective strategy that, at a slightly increased cost, can further improve classification performance.</li>
<li><strong>摘要：</strong>这项研究探讨了使用基于BERT的小型模型和本地大语言模型（如Llama-3.1 8B）有效地对科学全文进行分类的策略。我们专注于开发选择输入句子的子集以减小输入大小的同时增强分类性能的方法。为此，我们编译了一个新的数据集，该数据集由入侵生物学领域的全文科学论文组成，特别解决了入侵物种的影响。这些论文与国际自然保护联盟（IUCN）研究人员创建的公开影响评估保持一致。通过广泛的实验，我们证明了各种来源，例如人类证据注释，LLM生成的注释或解释性得分，可用于训练句子选择模型，以改善基于编码和解码器的语言模型的性能，同时通过降低通过降低来优化效率输入长度，即使与能够将完整文本作为输入的模型相比，也可以改善结果。此外，我们发现，较短输入的重复采样被证明是一种非常有效的策略，以略有提高的成本可以进一步提高分类性能。</li>
</ul>

<h3>Title: Position: It's Time to Act on the Risk of Efficient Personalized Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Eugenia Iofinova, Andrej Jovanovic, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06560">https://arxiv.org/abs/2502.06560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06560">https://arxiv.org/pdf/2502.06560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06560]] Position: It's Time to Act on the Risk of Efficient Personalized Text Generation(https://arxiv.org/abs/2502.06560)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The recent surge in high-quality open-sourced Generative AI text models (colloquially: LLMs), as well as efficient finetuning techniques, has opened the possibility of creating high-quality personalized models, i.e., models generating text attuned to a specific individual's needs and capable of credibly imitating their writing style by leveraging that person's own data to refine an open-source model. The technology to create such models is accessible to private individuals, and training and running such models can be done cheaply on consumer-grade hardware. These advancements are a huge gain for usability and privacy. This position paper argues, however, that these advancements also introduce new safety risks by making it practically feasible for malicious actors to impersonate specific individuals at scale, for instance for the purpose of phishing emails, based on small amounts of publicly available text. We further argue that these risks are complementary to - and distinct from - the much-discussed risks of other impersonation attacks such as image, voice, or video deepfakes, and are not adequately addressed by the larger research community, or the current generation of open - and closed-source models.</li>
<li><strong>摘要：</strong>最近的高质量开源生成AI文本模型（通俗性：LLMS）以及有效的明换技术的激增开放了创建高质量个性化模型的可能性并能够通过利用该人自己的数据来完善开源模型来可靠地模仿其写作风格。私人可以访问创建此类模型的技术，并且可以在消费级硬件上便宜地进行培训和运行此类模型。这些进步是可用性和隐私的巨大收益。然而，该立场论文认为，这些进步还通过使恶意演员在大规模的目的是基于少量公开可用的文本来模仿特定的个人，从而使恶意演员对特定的个人进行大规模模拟特定的个人，从而引入了新的安全风险。我们进一步认为，这些风险与其他模仿攻击（例如图像，声音或视频深击”等备受言论的风险是互补的，并且没有被较大的研究社区或当前一代的开放式攻击及其充分解决。 - 和封闭式模型。</li>
</ul>

<h3>Title: Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06563">https://arxiv.org/abs/2502.06563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06563">https://arxiv.org/pdf/2502.06563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06563]] Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation(https://arxiv.org/abs/2502.06563)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework. The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: this https URL</li>
<li><strong>摘要：</strong>涉及顺序扣除的一阶逻辑（FOL）推理对于智能系统至关重要，并且是评估推理能力的重要任务，尤其是在经过思考链（COT）环境中。现有的基准通常依赖于广泛的人类注释或手工制作的模板，因此很难实现必要的复杂性，可扩展性和多样性来进行稳健评估。为了解决这些局限性，我们提出了一个名为“ Provergen”的新颖框架，该框架与符号抛弃者的严格和精确性协同结合了大语言模型（LLMS）的生成优势，从而实现了可扩展，多样和高质量的FOL推理数据集的创建，即proverqa。 proverqa还包括在每个问题中包含可访问和逻辑上连贯的中间推理步骤的区别。我们的评估表明，即使在COT提示的情况下，最新的LLM也很难解决proverqa问题，突出了数据集的挑战性质。我们还通过我们的框架产生的单独培训集中的单独的培训集中的Finetune Llama3.1-8B教学。填充模型在分布和分发测试集上都表现出一致的改进，这表明我们提出的数据生成框架的价值。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM</h3>
<ul>
<li><strong>Authors: </strong>Zhi Zhou, Kun-Yang Yu, Shi-Yu Tian, Jiang-Xin Shi, Xiao-Wen Yang, Pengxiao Song, Yi-Xuan Jin, Lan-Zhe Guo, Yu-Feng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06572">https://arxiv.org/abs/2502.06572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06572">https://arxiv.org/pdf/2502.06572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06572]] LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM(https://arxiv.org/abs/2502.06572)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), both proprietary and open-source, have demonstrated remarkable capabilities across various natural language processing tasks. However, they face significant limitations in legal reasoning tasks. Proprietary models introduce data privacy risks and high inference costs, while open-source models underperform due to insufficient legal domain training data. To address these limitations, we study data generation for legal reasoning to improve the legal reasoning performance of open-source LLMs with the help of proprietary LLMs. This is challenging due to the lack of legal knowledge in proprietary LLMs and the difficulty in verifying the generated data. We propose KgDG, a knowledge-guided data generation framework for legal reasoning. Our framework enables leveraging legal knowledge to enhance generation diversity and introduces a refinement and verification process to ensure the quality of generated data. Moreover, we expand the generated dataset to further enhance the LLM reasoning capabilities. Using KgDG, we create a synthetic legal reasoning dataset containing 50K high-quality examples. Our trained model LawGPT outperforms existing legal-specific LLMs and achieves performance comparable to proprietary LLMs, demonstrating the effectiveness of KgDG and LawGPT. Our code and resources is publicly available at this https URL .</li>
<li><strong>摘要：</strong>专有和开源的大型语言模型（LLM）在各种自然语言处理任务中都表现出了非凡的功能。但是，他们在法律推理任务中面临重大局限性。专有模型介绍数据隐私风险和高推理成本，而开源模型由于法律域培训数据不足而表现不佳。为了解决这些限制，我们研究了法律推理的数据生成，以在专有LLMS的帮助下提高开源LLM的法律推理绩效。由于缺乏专有LLM的法律知识以及验证生成的数据的困难，这是具有挑战性的。我们提出了KGDG，这是一个知识引导的法律推理数据生成框架。我们的框架使利用法律知识可以增强发电多样性，并引入改进和验证过程，以确保生成的数据的质量。此外，我们扩展了生成的数据集，以进一步增强LLM推理功能。我们使用库德G，创建一个包含50K高质量示例的合成法律推理数据集。我们训练有素的模型法律的表现优于现有的特定法律特异性LLM，并实现了与专有LLM相当的绩效，证明了库奇和Lawgpt的有效性。我们的代码和资源可在此HTTPS URL上公开获得。</li>
</ul>

<h3>Title: Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Zhuang, Jingfeng Yang, Haoming Jiang, Xin Liu, Kewei Cheng, Sanket Lokegaonkar, Yifan Gao, Qing Ping, Tianyi Liu, Binxuan Huang, Zheng Li, Zhengyang Wang, Pei Chen, Ruijie Wang, Rongzhi Zhang, Nasser Zalmout, Priyanka Nigam, Bing Yin, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06589">https://arxiv.org/abs/2502.06589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06589">https://arxiv.org/pdf/2502.06589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06589]] Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training(https://arxiv.org/abs/2502.06589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.</li>
<li><strong>摘要：</strong>由于面向代理的预训练数据缺乏，基于LLM的自主剂通常依赖于复杂的提示或广泛的微调，这通常无法引入新的功能，同时保持强大的可推广性。我们介绍了Hephaestus-Forge，这是第一个旨在增强LLM代理在API函数调用，内在推理和计划中以及适应环境反馈的大规模训练之前的大规模训练前语料库。 Hephaestus-Forge包括103B特定于特定于代理的数据，其中包括76,537个API，包括既有工具文档，都可以介绍API功能的知识和功能呼叫轨迹以增强内在推理。为了探索有效的培训方案，我们研究了比例定律，以确定数据混合比中的最佳配方。通过对赫菲斯特福克的持续预培训，赫菲斯图斯在三个代理基准上的表现优于中等至中等规模的开源LLM和竞争对手的商业LLM，这表明了我们的培训前体体在增强基本代理能力和LLMS对LLMS的普遍性方面的有效性新任务或环境。</li>
</ul>

<h3>Title: Do we really have to filter out random noise in pre-training data for language models?</h3>
<ul>
<li><strong>Authors: </strong>Jinghan Ru, Yuxin Xie, Xianwei Zhuang, Yuguo Yin, Yuexian Zou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06604">https://arxiv.org/abs/2502.06604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06604">https://arxiv.org/pdf/2502.06604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06604]] Do we really have to filter out random noise in pre-training data for language models?(https://arxiv.org/abs/2502.06604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low quality or synthetic data, our study \textbf{provides the first systematic investigation into such random noise through a cohesive ``What-Why-How'' framework.} Surprisingly, we observed that the resulting increase in next-token prediction (NTP) loss was significantly lower than the proportion of random noise. We provide a theoretical justification for this phenomenon, which also elucidates the success of multilingual models. On the other hand, experiments show that the model's performance in downstream tasks is not based solely on the NTP loss, which means that random noise may result in degraded downstream performance. To address the potential adverse effects, we introduce a novel plug-and-play Local Gradient Matching loss, which explicitly enhances the denoising capability of the downstream task head by aligning the gradient of normal and perturbed features without requiring knowledge of the model's parameters. Additional experiments on 8 language and 14 vision benchmarks further validate its effectiveness.</li>
<li><strong>摘要：</strong>网络尺寸的预培训数据集是LLMS成功的基石。但是，从Internet策划的文本数据不可避免地包含由解码错误或不受监管的Web内容引起的随机噪声。与以前专注于低质量或合成数据的作品相反，我们的研究\ textbf {通过凝聚力通过``什么原因''框架提供了对这种随机噪声的首次系统调查。}令人惊讶的是，我们观察到，我们观察到了结果下一步预测（NTP）损失的增加明显低于随机噪声的比例。我们为这种现象提供了理论上的理由，这也阐明了多语言模型的成功。另一方面，实验表明，该模型在下游任务中的性能并不仅基于NTP损失，这意味着随机噪声可能导致下游性能下降。为了解决潜在的不利影响，我们引入了一种新颖的插件局部梯度匹配损失，该损失明确地通过对齐正常和扰动特征的梯度而不需要了解模型参数的情况下明确提高了下游任务头的降解能力。对8种语言和14个视觉基准的其他实验进一步验证了其有效性。</li>
</ul>

<h3>Title: Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM</h3>
<ul>
<li><strong>Authors: </strong>Qingshui Gu, Shu Li, Tianyu Zheng, Zhaoxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06635">https://arxiv.org/abs/2502.06635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06635">https://arxiv.org/pdf/2502.06635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06635]] Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM(https://arxiv.org/abs/2502.06635)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at this https URL.</li>
<li><strong>摘要：</strong>Steel-LLM是一种从头开始开发的中文语言模型，目的是尽管计算资源有限，但仍可创建高质量的开源模型。该项目于2024年3月启动，旨在在大型数据集上培训一项10亿参数模型，优先考虑透明度和共享实用见解，以帮助社区中的其他人。培训过程主要集中在中国数据上，其中包括一小部分英语数据，通过提供对模型构建旅程的更详细且实用的说明，从而解决了现有的开源LLMS中的差距。 Steel-LLM在Ceval和CMMLU等基准上表现出了竞争性能，表现优于较大机构的早期模型。本文提供了有关项目的主要贡献的全面摘要，包括数据收集，模型设计，培训方法以及一路上遇到的挑战，为希望开发自己的LLM的研究人员和从业人员提供了宝贵的资源。此HTTPS URL可用模型检查点和培训脚本。</li>
</ul>

<h3>Title: Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A</h3>
<ul>
<li><strong>Authors: </strong>Anna Leschanowsky, Zahra Kolagar, Erion Çano, Ivan Habernal, Dara Hallinan, Emanuël A. P. Habets, Birgit Popp</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06652">https://arxiv.org/abs/2502.06652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06652">https://arxiv.org/pdf/2502.06652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06652]] Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A(https://arxiv.org/abs/2502.06652)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>The transparency principle of the General Data Protection Regulation (GDPR) requires data processing information to be clear, precise, and accessible. While language models show promise in this context, their probabilistic nature complicates truthfulness and comprehensibility. This paper examines state-of-the-art Retrieval Augmented Generation (RAG) systems enhanced with alignment techniques to fulfill GDPR obligations. We evaluate RAG systems incorporating an alignment module like Rewindable Auto-regressive Inference (RAIN) and our proposed multidimensional extension, MultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness and comprehensibility and are assessed through 21 metrics, including deterministic and large language model-based evaluations. Our results show that RAG systems with an alignment module outperform baseline RAG systems on most metrics, though none fully match human answers. Principal component analysis of the results reveals complex interactions between metrics, highlighting the need to refine metrics. This study provides a foundation for integrating advanced natural language processing systems into legal compliance frameworks.</li>
<li><strong>摘要：</strong>通用数据保护法规（GDPR）的透明度原理要求数据处理信息清晰，精确且可访问。尽管语言模型在这种情况下表现出希望，但它们的概率性质使真实性和理解性复杂化。本文研究了最新的检索增强生成（RAG）系统，并通过对齐技术来增强以履行GDPR义务。我们评估了使用隐私Q＆A数据集等对齐模块（如可重新确定自动回归推理（RAIN））和我们提出的多维扩展（多维扩展）等对齐模块的抹布系统。对响应的精确性和可理解性进行了优化，并通过21个指标进行评估，包括确定性和大型语言模型的评估。我们的结果表明，带有对齐模块的破布系统在大多数指标上都优于基线抹布系统，尽管没有一个完全匹配人类的答案。结果的主要成分分析揭示了指标之间的复杂相互作用，突出了精炼指标的需求。这项研究为将先进的自然语言处理系统整合到法律合规框架中奠定了基础。</li>
</ul>

<h3>Title: In-Context Learning (and Unlearning) of Length Biases</h3>
<ul>
<li><strong>Authors: </strong>Stephanie Schoch, Yangfeng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06653">https://arxiv.org/abs/2502.06653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06653">https://arxiv.org/pdf/2502.06653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06653]] In-Context Learning (and Unlearning) of Length Biases(https://arxiv.org/abs/2502.06653)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on in-context learning. We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias exhibited by the model. In addition, we show that learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning). This reveals the power of in-context learning in debiasing model prediction behaviors without the need for costly parameter updates.</li>
<li><strong>摘要：</strong>大型语言模型已经证明了在内部学习的强大功能，其中将示例输入配对配对附加到提示中进行演示。但是，现有的工作证明了模型在封闭式中学习词汇和标签偏见的能力，这对模型的性能和鲁棒性产生了负面影响。其他统计数据偏见的影响仍然不足，这项工作旨在解决这一问题。我们专门研究了长度偏见对中文学习的影响。我们证明，模型确实在上下文窗口中学习了长度偏见以进行预测，并进一步分析了调节模型表现出的偏见水平的因素。此外，我们表明，可以使用学习长度信息来对抗在模型中编码的长度偏差（例如，通过微调）。这揭示了在依据模型预测行为中内在学习的力量，而无需昂贵的参数更新。</li>
</ul>

<h3>Title: Who Taught You That? Tracing Teachers in Model Distillation</h3>
<ul>
<li><strong>Authors: </strong>Somin Wadhwa, Chantal Shaib, Silvio Amir, Byron C. Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06659">https://arxiv.org/abs/2502.06659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06659">https://arxiv.org/pdf/2502.06659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06659]] Who Taught You That? Tracing Teachers in Model Distillation(https://arxiv.org/abs/2502.06659)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Model distillation -- using outputs from a large teacher model to teach a small student model -- is a practical means of creating efficient models for a particular task. We ask: Can we identify a students' teacher based on its outputs? Such "footprints" left by teacher LLMs would be interesting artifacts. Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service. We consider practical task distillation targets including summarization, question answering, and instruction-following. We assume a finite set of candidate teacher models, which we treat as blackboxes. We design discriminative models that operate over lexical features. We find that $n$-gram similarity alone is unreliable for identifying teachers, but part-of-speech (PoS) templates preferred by student models mimic those of their teachers.</li>
<li><strong>摘要：</strong>模型蒸馏 - 使用大型教师模型的输出来教授小型学生模型 - 是为特定任务创建有效模型的实用方法。我们问：我们可以根据学生的成绩来识别学生的老师吗？ LLM老师留下的这种“足迹”将是有趣的文物。除此之外，可靠的教师推论可能具有实际的影响，因为演员们试图将大量专有LLM的特定功能提炼成部署的较小的LM，从而可能违反服务条款。我们考虑实用的任务蒸馏目标，包括汇总，问答和跟随说明。我们假设一组有限的候选教师模型，我们将其视为黑盒。我们设计了在词汇特征上运行的判别模型。我们发现，仅$ n $ gram的相似性对于识别教师来说是不可靠的，但是由学生模型偏爱的言论（POS）模板模仿了他们的老师。</li>
</ul>

<h3>Title: Automatic Evaluation of Healthcare LLMs Beyond Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Anna Arias-Duart, Pablo Agustin Martin-Torres, Daniel Hinjos, Pablo Bernabeu-Perez, Lucia Urcelay Ganzabal, Marta Gonzalez Mallo, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Sergio Alvarez-Napagao, Dario Garcia-Gasulla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06666">https://arxiv.org/abs/2502.06666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06666">https://arxiv.org/pdf/2502.06666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06666]] Automatic Evaluation of Healthcare LLMs Beyond Question-Answering(https://arxiv.org/abs/2502.06666)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model's capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and close benchmarks and metrics. Findings include blind spots and overlaps in current methodologies. As an updated sanity check, we release a new medical benchmark--CareQA--, with both open and closed variants. Finally, we propose a novel metric for open-ended evaluations --Relaxed Perplexity-- to mitigate the identified limitations.</li>
<li><strong>摘要：</strong>当前的大型语言模型（LLMS）基准通常基于开放式或封闭式质量检查评估，避免了人工劳动的要求。封闭式测量值评估了反应的事实，但缺乏表现力。开放式捕获模型产生话语响应的能力，但很难评估正确性。尽管它们的关系仍然很熟悉，但这两种方法通常是独立或共同使用的。这项工作的重点是医疗领域，事实和话语都非常重要。它引入了一套全面的多轴套件，用于医疗LLM评估，探索开放和近距离基准和指标之间的相关性。发现包括当前方法中的盲点和重叠。作为更新的理智检查，我们发布了一个新的医疗基准-Careqa，并具有开放式和封闭式的变体。最后，我们提出了一个新的度量，用于开放式评估 - 重新释放的困惑 - 以减轻确定的局限性。</li>
</ul>

<h3>Title: Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations</h3>
<ul>
<li><strong>Authors: </strong>Rui Chen, Tailai Peng, Xinran Xie, Dekun Lin, Zhe Cui, Zheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06669">https://arxiv.org/abs/2502.06669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06669">https://arxiv.org/pdf/2502.06669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06669]] Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations(https://arxiv.org/abs/2502.06669)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Significant improvements have been observed in the zero-shot capabilities of the Large Language Models (LLMs). Due to their high sensitivity to input, research has increasingly focused on enhancing LLMs' performance via direct and simple prompt engineering rather than intricate domain adaptation. Studies suggest that LLMs exhibit emotional intelligence, and both positive and negative emotions can potentially enhance task performances. However, prior interaction prompts have predominantly concentrated on a single stimulus type, neglecting to compare different stimulus effects, examine the influence of varying task difficulties, or explore underlying mechanisms. This paper, inspired by the positive correlation between self-efficacy and task performance within the social cognitive theory, introduces Verbal Efficacy Stimulations (VES). Our VES comprises three types of verbal prompts: encouraging, provocative, and critical, addressing six aspects such as helpfulness and competence. And we further categorize task difficulty, aiming to extensively investigate how distinct VES influence the self-efficacy and task achievements of language models at varied levels of difficulty. The experimental results show that the three types of VES improve the performance of LLMs on most tasks, and the most effective VES varies for different models. In extensive experiments, we have obtained some findings consistent with psychological theories, providing novel insights for future research.</li>
<li><strong>摘要：</strong>在大语言模型（LLMS）的零拍功能中，已经观察到了显着改善。由于它们对输入的敏感性很高，因此越来越多地着重于通过直接和简单的及时工程而不是复杂的域适应来提高LLMS的性能。研究表明，LLM表现出情感智力，积极和负面情绪都可以增强任务表现。但是，先前的相互作用提示主要集中在单个刺激类型上，忽略了比较不同的刺激效应，检查不同任务困难的影响或探索潜在的机制。本文的灵感来自社会认知理论中自我效能和任务表现之间的正相关性，引入了口头疗效刺激（VES）。我们的VES包括三种类型的口头提示：令人鼓舞，挑衅和批判性，解决了六个方面，例如帮助和能力。我们进一步将任务难度分类为，旨在广泛地研究不同的VES如何影响语言模型在不同级别的难度下的自我效能和任务成就。实验结果表明，三种类型的VE提高了LLM在大多数任务上的性能，并且最有效的VES在不同模型方面有所不同。在广泛的实验中，我们获得了一些与心理理论一致的发现，为未来的研究提供了新颖的见解。</li>
</ul>

<h3>Title: Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06703">https://arxiv.org/abs/2502.06703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06703">https://arxiv.org/pdf/2502.06703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06703]] Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling(https://arxiv.org/abs/2502.06703)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.</li>
<li><strong>摘要：</strong>测试时间缩放（TTS）是通过在推理阶段使用其他计算来改善大语言模型（LLMS）性能的重要方法。但是，当前的研究并未系统地分析政策模型，过程奖励模型（PRM）和问题难度如何影响TT。缺乏分析限制了TTS方法的理解和实际使用。在本文中，我们关注两个核心问题：（1）跨不同策略模型，PRM和问题难度级别的规模测试时间计算的最佳方法是什么？ （2）扩展计算在多大程度上可以改善LLM在复杂任务上的性能，并且通过这种方法，较小的语言模型可以优于较大的语言模型？通过有关数学500和挑战AIME24任务的全面实验，我们有以下观察结果：（1）计算最佳的TTS策略高度取决于政策模型，PRM和问题困难的选择。 （2）借助我们的计算最佳TTS策略，极小的政策模型可以超过较大的模型。例如，1B LLM在Math-500上可以超过405B LLM。此外，在Math-500和AIME24上，0.5B LLM的表现均优于GPT-4O，3B LLM超过405B LLM，而7B LLM击败O1和DeepSeek-R1，而推理效率较高。这些发现表明，将TTS策略适应每个任务和模型的特定特征的重要性，并表明TTS是增强LLMS推理能力的有前途的方法。</li>
</ul>

<h3>Title: Rationalization Models for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Shankar Subramanian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06759">https://arxiv.org/abs/2502.06759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06759">https://arxiv.org/pdf/2502.06759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06759]] Rationalization Models for Text-to-SQL(https://arxiv.org/abs/2502.06759)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.</li>
<li><strong>摘要：</strong>我们引入了一个框架来生成经过思考链（COT）理由，以增强文本到SQL模型微调。这些理由由中间SQL语句和解释组成，是构建最终SQL查询的增量步骤。该过程始于手动注释一小部分示例，然后将其用于在教师模型的迭代且动态的几声知识蒸馏过程中提示大型语言模型。随后，对经过验证的分解查询进行了合理化模型，从而为文本到SQL数据集提供了广泛的合成COT注释。为了评估该方法，我们在鸟类数据集上使用和没有这些理由的小语言模型调整了小语言模型。结果表明，分步查询产生提高了执行精度，尤其是对于中等和高度复杂的查询，同时还可以增强解释性。</li>
</ul>

<h3>Title: Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs</h3>
<ul>
<li><strong>Authors: </strong>Ryan Synk, Monte Hoover, John Kirchenbauer, Neel Jain, Alex Stein, Manli Shu, Josue Melendez Sanchez, Ramani Duraiswami, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06766">https://arxiv.org/abs/2502.06766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06766">https://arxiv.org/pdf/2502.06766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06766]] Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs(https://arxiv.org/abs/2502.06766)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context</a></li>
<li><strong>Abstract: </strong>There is growing demand for performing inference with hundreds of thousands of input tokens on trained transformer models. Inference at this extreme scale demands significant computational resources, hindering the application of transformers at long contexts on commodity (i.e not data center scale) hardware. To address the inference time costs associated with running self-attention based transformer language models on long contexts and enable their adoption on widely available hardware, we propose a tunable mechanism that reduces the cost of the forward pass by attending to only the most relevant tokens at every generation step using a top-k selection mechanism. We showcase the efficiency gains afforded by our method by performing inference on context windows up to 1M tokens using approximately 16GB of GPU RAM. Our experiments reveal that models are capable of handling the sparsity induced by the reduced number of keys and values. By attending to less than 2% of input tokens, we achieve over 95% of model performance on common long context benchmarks (LM-Eval, AlpacaEval, and RULER).</li>
<li><strong>摘要：</strong>在训练有素的变压器模型上使用数十万个输入令牌进行推断的需求日益增长。在这个极端规模的推论需要大量的计算资源，阻碍了在商品（即数据中心尺度）硬件上长篇小说中变形金刚的应用。为了解决与长篇小说上的基于自我注意力的变压器语言模型相关的推理时间成本并使它们能够在广泛可用的硬件上采用，我们提出了一种可调机制，该机制通过仅适用于最相关的令牌来降低前进通行证的成本每一代步骤都使用TOP-K选择机制。我们通过使用大约16GB的GPU RAM对上下文Windows进行推断，以展示我们方法所带来的效率提高。我们的实验表明，模型能够处理由键和值减少引起的稀疏性。通过少于投入令牌的不到2％，我们在共同的长上下文基准（LM-eval，alpacaeval和lorer）上实现了超过95％的模型性能。</li>
</ul>

<h3>Title: ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</h3>
<ul>
<li><strong>Authors: </strong>Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.06772">https://arxiv.org/abs/2502.06772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.06772">https://arxiv.org/pdf/2502.06772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.06772]] ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates(https://arxiv.org/abs/2502.06772)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: this https URL</li>
<li><strong>摘要：</strong>我们表明，通过缩放思维模板通过缩放思维模板推理层次结构的LLM推理可以有效地优化推理搜索空间，并优于OpenAI O1-Preview和DeepSeek V3（例如OpenAI O1-Preview和DeepSeek V3）的数学推理能力。我们仅使用8个GPU训练ReasonFlux-32b模型，并引入了三个创新：（i）一个结构化的通用思想模板库，其中包含大约500个高级思想模板，能够将其推广到类似或相关的推理问题； （ii）在一系列思想模板上而不是长COTS上进行层次增强学习，以优化基本LLM，以计划逐渐处理复杂问题的最佳模板轨迹； （iii）一个全新的推理缩放系统，通过在推理时间自适应地缩放思想模板来实现层次结构LLM推理。借助包含顺序思维模板的模板轨迹，我们的ReasonFlux-32b将数学推理能力显着提高到了最新的水平。值得注意的是，在数学基准上，它的准确度达到91.2％，超过O1-preview 6.7％。在美国数学奥林匹克（AIME）基准上，ReasonFlux-32B平均解决了56.7％的问题，分别超过O1-preview和DeepSeek-V3，分别为27％和45％。代码：此HTTPS URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
