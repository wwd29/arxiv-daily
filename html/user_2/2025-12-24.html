<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-24</h1>
<h3>Title: HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data</h3>
<ul>
<li><strong>Authors: </strong>Shashi Kant Gupta, Arijeet Pramanik, Jerrin John Thomas, Regina Schwind, Lauren Wiener, Avi Raju, Jeremy Kornbluth, Yanshan Wang, Zhaohui Su, Hrituraj Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19864">https://arxiv.org/abs/2512.19864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19864">https://arxiv.org/pdf/2512.19864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19864]] HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data(https://arxiv.org/abs/2512.19864)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale</li>
<li><strong>摘要：</strong>电子健康记录 (EHR) 中的非结构化注释包含对癌症治疗决策和研究至关重要的丰富临床信息，但由于广泛的可变性、专业术语和不一致的文档格式，可靠地提取结构化肿瘤数据仍然具有挑战性。手动抽象虽然准确，但成本高昂且无法扩展。现有的自动化方法通常解决狭窄的场景——要么使用合成数据集，将重点限制在文档级提取，要么隔离特定的临床变量（例如分期、生物标志物、组织学）——并且不能充分处理包含矛盾信息的大量临床文档中的患者级合成。在这项研究中，我们提出了一个代理框架，可以系统地将复杂的肿瘤学数据提取分解为模块化的自适应任务。具体来说，我们使用大型语言模型（LLM）作为推理代理，配备上下文敏感检索和迭代合成功能，从现实世界的肿瘤学笔记中详尽而全面地提取结构化临床变量。对涵盖 2,250 名癌症患者的超过 400,000 个非结构化临床记录和扫描 PDF 报告的大规模数据集进行评估，我们的方法实现了 0.93 的平均 F1 分数，103 个肿瘤特异性临床变量中有 100 个超过 0.85，关键变量（例如生物标志物和药物）超过 0.95。此外，将代理系统集成到数据管理工作流程中，直接手动批准率达到 0.94，显着降低了注释成本。据我们所知，这是基于 LLM 的代理首次详尽的端到端应用，用于大规模结构化肿瘤学数据提取</li>
</ul>

<h3>Title: How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse</h3>
<ul>
<li><strong>Authors: </strong>Kirk Vanacore, Rene F. Kizilcec</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19903">https://arxiv.org/abs/2512.19903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19903">https://arxiv.org/pdf/2512.19903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19903]] How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse(https://arxiv.org/abs/2512.19903)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 越来越多地在教育技术中用于各种任务，从生成教学材料、协助评估设计到辅导。虽然之前的工作已经研究了如何针对特定任务调整或优化模型，但对于法学硕士在无需大量定制的情况下解释真实教育场景的表现却知之甚少。随着基于法学硕士的系统在日常学术环境中被学习者和教育者广泛采用，了解其开箱即用的能力对于设定期望和基准变得越来越重要。我们比较了六位法学硕士，以评估他们在一项简单但重要的任务上的基线表现：对真实课堂记录中的教学动作进行分类。我们评估了典型的提示方法：零次提示、一次提示和几次提示。我们发现，虽然零样本性能中等，但提供全面的示例（少量样本提示）显着提高了最先进模型的性能，最强的配置针对专家编码的注释达到了 Cohen 的 Kappa = 0.58。与此同时，改进既不统一也不完整：性能因教学动作而有很大差异，更高的召回率常常以误报增加为代价。总体而言，这些发现表明基础模型表现出有意义但解释教学话语的能力有限，及时设计有助于表面能力但不能消除基本的可靠性约束。</li>
</ul>

<h3>Title: Counterfactual LLM-based Framework for Measuring Rhetorical Style</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Qiu, Hong Chen, Zongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19908">https://arxiv.org/abs/2512.19908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19908">https://arxiv.org/pdf/2512.19908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19908]] Counterfactual LLM-based Framework for Measuring Rhetorical Style(https://arxiv.org/abs/2512.19908)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.</li>
<li><strong>摘要：</strong>人工智能的兴起引发了人们对机器学习论文中“炒作”的日益担忧，但独立于实质性内容来量化修辞风格的可靠方法仍然难以捉摸。由于大胆的语言可能源于强有力的实证结果或纯粹的修辞风格，因此通常很难区分两者。为了将修辞风格与实质性内容分开，我们引入了一个基于法学硕士的反事实框架：多个法学硕士修辞角色从相同的实质性内容生成反事实写作，法学硕士法官通过成对评估对它们进行比较，并使用 Bradley-Terry 模型汇总结果。将此方法应用于 2017 年至 2025 年采样的 8,485 篇 ICLR 提交内容，我们生成了超过 250,000 篇反事实文章，并对 ML 论文中的修辞风格进行了大规模量化。我们发现，即使在控制了同行评审评估之后，愿景框架也能显着预测下游关注，包括引用和媒体关注。我们还观察到 2023 年之后修辞力度急剧上升，并提供经验证据表明，这种增长很大程度上是由基于法学硕士的写作辅助的采用推动的。我们框架的可靠性通过其对人物角色选择的稳健性以及法学硕士判断与人工注释之间的高度相关性得到了验证。我们的工作表明，法学硕士可以作为衡量和改进科学评估的工具。</li>
</ul>

<h3>Title: PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Lu, Xueyuan Deng, Yiran Liu, Yulong Li, Qiang Yan, Imran Razzak, Jionglong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19933">https://arxiv.org/abs/2512.19933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19933">https://arxiv.org/pdf/2512.19933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19933]] PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation(https://arxiv.org/abs/2512.19933)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.</li>
<li><strong>摘要：</strong>由于过于简单化的同质性假设，传统的基于主体的舆论动态模型（ABM）往往无法捕捉导致在线两极分化的心理异质性。这种限制掩盖了个人认知偏见和信息传播之间的关键相互作用，从而阻碍了对意识形态分歧如何放大的机械理解。为了应对这一挑战，我们引入了人格折射智能模拟模型（PRISM），这是一种混合框架，将用于连续情绪演化的随机微分方程（SDE）与用于离散决策的人格条件部分可观察马尔可夫决策过程（PC-POMDP）相结合。与连续特征方法相反，PRISM 将基于迈尔斯-布里格斯类型指标 (MBTI) 的独特认知策略分配给多模态大语言模型 (MLLM) 代理，并通过来自大规模社交媒体数据集的数据驱动先验进行初始化。 PRISM 实现了与人类基本事实一致的卓越个性一致性，显着优于标准同质基准和大五基准。该框架有效地复制了理性抑制和情感共鸣等新兴现象，为分析复杂的社交媒体生态系统提供了强大的工具。</li>
</ul>

<h3>Title: Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems</h3>
<ul>
<li><strong>Authors: </strong>Heet Bodara, Md Masum Mushfiq, Isma Farah Siddiqui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19950">https://arxiv.org/abs/2512.19950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19950">https://arxiv.org/pdf/2512.19950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19950]] Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems(https://arxiv.org/abs/2512.19950)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地应用于数字个人助理等会话系统中，塑造了人们通过语言与技术交互的方式。虽然他们的回答通常听起来流畅、自然，但他们也可能带有微妙的语气偏差，例如即使在人们期望保持中立的情况下，他们也可能听起来过于礼貌、开朗或谨慎。这些倾向会影响用户如何看待对话中的信任、同理心和公平性。在这项研究中，我们探索语气偏差作为大型语言模型的隐藏行为特征。这项研究的新颖之处在于基于可控大语言模型的对话合成与语气分类模型的集成，从而在个人助理交互中实现稳健且符合道德的情感识别。我们创建了两个合成对话数据集，一个是根据中性提示生成的，另一个是明确引导产生积极或消极语气的。令人惊讶的是，即使是中性组也表现出一致的语气偏差，这表明偏差可能源于模型的潜在对话风格。通过预训练的 DistilBERT 模型使用弱监督，我们标记了音调并训练了几个分类器来检测这些模式。集成模型的宏观 F1 分数高达 0.92，表明语气偏差是系统性的、可测量的，并且与设计公平且值得信赖的对话 AI 相关。</li>
</ul>

<h3>Title: Schoenfeld's Anatomy of Mathematical Reasoning by Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19995">https://arxiv.org/abs/2512.19995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19995">https://arxiv.org/pdf/2512.19995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19995]] Schoenfeld's Anatomy of Mathematical Reasoning by Language Models(https://arxiv.org/abs/2512.19995)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地暴露推理痕迹，但其底层认知结构和步骤仍然难以超越表面统计数据进行识别和分析。我们采用 Schoenfeld 的情节理论作为归纳的、中等规模的镜头，并引入 ThinkARM（模型推理剖析），这是一个可扩展的框架，它将推理轨迹明确地抽象为功能推理步骤，如分析、探索、实现、验证等。当应用于不同模型的数学问题解决时，这种抽象揭示了可再现的思维动态以及推理和非推理模型之间的结构差异，这些在令牌级视图中并不明显。我们进一步提出了两个诊断案例研究，表明探索功能是与正确性相关的关键分支步骤，并且以效率为导向的方法选择性地抑制评估反馈步骤而不是统一缩短响应。总之，我们的结果表明，情节级表示使推理步骤变得明确，从而能够系统地分析现代语言模型中推理的结构、稳定和改变方式。</li>
</ul>

<h3>Title: Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents</h3>
<ul>
<li><strong>Authors: </strong>Yiming Du, Baojun Wang, Yifan Xiang, Zhaowei Wang, Wenyu Huang, Boyang Xue, Bin Liang, Xingshan Zeng, Fei Mi, Haoli Bai, Lifeng Shang, Jeff Z. Pan, Yuxin Jiang, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20092">https://arxiv.org/abs/2512.20092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20092">https://arxiv.org/pdf/2512.20092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20092]] Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents(https://arxiv.org/abs/2512.20092)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at this https URL</li>
<li><strong>摘要：</strong>长时间、多会话对话的时间推理是会话代理的一项关键能力。然而，现有的工作和我们的试点研究表明，随着对话历史的长度增长和噪声的积累，当前的长上下文模型难以准确识别时间相关的信息，从而严重损害推理性能。为了解决这个问题，我们引入了 Memory-T1，这是一个使用强化学习 (RL) 学习时间感知内存选择策略的框架。它采用从粗到细的策略，首先使用时间和相关性过滤器将对话历史修剪为候选集，然后由 RL 代理选择精确的证据会话。强化学习训练以多级奖励函数为指导，优化 (i) 答案准确性、(ii) 证据基础和 (iii) 时间一致性。特别是，时间一致性奖励通过评估会话级别（时间顺序接近度）和话语级别（时间顺序保真度）与查询时间范围的一致性来提供密集信号，使代理能够解决微妙的时间顺序歧义。在 Time-Dialog 基准测试中，Memory-T1 将 7B 模型的总分提升至 67.0%，为开源模型建立了新的最先进性能，比 14B 基准高出 10.2%。消融研究表明，时间一致性和证据基础奖励共同促进了 15.0% 的绩效提升。此外，Memory-T1 保持了高达 128k 个标记的鲁棒性，其中基线模型崩溃，证明了在广泛的对话历史中对抗噪声的有效性。代码和数据集可在此 https URL 公开获取</li>
</ul>

<h3>Title: ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</h3>
<ul>
<li><strong>Authors: </strong>Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20111">https://arxiv.org/abs/2512.20111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20111">https://arxiv.org/pdf/2512.20111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20111]] ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language(https://arxiv.org/abs/2512.20111)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.</li>
<li><strong>摘要：</strong>随着连续决策任务的长度增加，在上下文中保留完整的交互历史在计算上变得不切实际。我们介绍了 LLM 智能体通过多步骤交互维持简洁上下文的通用框架：通过语言表达的信念瓶颈（ABBEL）进行操作，以及通过 RL 训练后进一步改进 ABBEL 智能体的方法。 ABBEL 用信念状态（即已发现的与任务相关的未知数的自然语言摘要）取代了漫长的多步骤交互历史。在 ABBEL 下，在每一步中，智能体首先使用来自环境的最新观察结果更新先验信念，以形成后验信念，然后仅使用后验信念来选择动作。我们在六个不同的多步骤环境中系统地评估了 ABBEL 下的前沿模型，发现 ABBEL 支持生成可解释的信念，同时在交互步骤中保持近乎恒定的内存使用。然而，瓶颈方法通常容易出现错误传播，我们观察到，由于置信更新中的错误，与完整的上下文设置相比，这会导致性能较差。因此，我们通过强化学习（RL）训练法学硕士在 ABBEL 框架内产生信念并采取行动。我们尝试进行信念分级，以奖励更高质量的信念，并尝试使用信念长度惩罚来奖励更压缩的信念。我们的实验证明了 RL 能够在完整的上下文设置之外提高 ABBEL 的性能，同时比同时期的方法使用更少的内存。</li>
</ul>

<h3>Title: M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Hyeongcheol Park, Jiyoung Seo, Jaewon Mun, Hogun Park, Wonmin Byeon, Sung June Kim, Hyeonsoo Im, JeungSub Lee, Sangpil Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20136">https://arxiv.org/abs/2512.20136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20136">https://arxiv.org/pdf/2512.20136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20136]] M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation(https://arxiv.org/abs/2512.20136)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）最近已扩展到多模态设置，将多模态大语言模型（MLLM）与大量外部知识（例如多模态知识图（MMKG））连接起来。尽管最近取得了成功，但视听领域的多模态 RAG 仍然具有挑战性，因为 1）现有 MMKG 的模态覆盖和多跳连接有限，2）仅基于共享多模态嵌入空间中的相似性进行检索，这无法过滤掉偏离主题或冗余的知识。为了解决这些限制，我们提出了 M$^3$KG-RAG，一种多跳多模态知识图增强型 RAG，它从 MMKG 中检索查询对齐的视听知识，提高 MLLM 中的推理深度和答案可信度。具体来说，我们设计了一个轻量级多代理管道来构造多跳 MMKG (M$^3$KG)，其中包含上下文丰富的多模态实体三元组，从而实现基于输入查询的模态检索。此外，我们引入了 GRASP（接地检索和选择性修剪），它确保查询的精确实体接地，评估答案支持的相关性，并修剪冗余上下文以仅保留响应生成所必需的知识。跨不同多模态基准的广泛实验表明，与现有方法相比，M$^3$KG-RAG 显着增强了 MLLM 的多模态推理和基础。</li>
</ul>

<h3>Title: Multi-hop Reasoning via Early Knowledge Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Wang, Shicheng Fang, Bo Wang, Qi Luo, Xuanjing Huang, Yining Zheng, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20144">https://arxiv.org/abs/2512.20144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20144">https://arxiv.org/pdf/2512.20144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20144]] Multi-hop Reasoning via Early Knowledge Alignment(https://arxiv.org/abs/2512.20144)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{this https URL}{Github}.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 已成为大型语言模型 (LLM) 的强大范例，用于解决需要特定领域或最新信息的知识密集型查询。为了处理对单步检索具有挑战性的复杂多跳问题，人们提出了结合强化学习的迭代 RAG 方法。然而，现有的迭代 RAG 系统通常计划在不利用可用检索语料库信息的情况下分解问题，从而导致检索和推理链效率低下，从而导致性能不佳。在本文中，我们介绍了早期知识对齐（EKA），这是一个简单但有效的模块，可在具有上下文相关检索知识的迭代 RAG 系统中进行规划之前，将法学硕士与检索集对齐。对六个标准 RAG 数据集的大量实验表明，通过建立更强大的推理基础，EKA 显着提高了检索精度，减少了级联错误，并提高了性能和效率。我们从熵角度的分析表明，结合早期知识可以减少推理过程中不必要的探索，使模型能够更有效地关注相关信息子集。此外，EKA 被证明是一种有效的多功能、免训练推理策略，可以无缝扩展到大型模型。跨不同数据集和检索语料库的泛化测试证实了我们方法的稳健性。总体而言，EKA 推进了迭代 RAG 系统的最先进水平，同时阐明了结构化推理与强化学习增强框架中的有效探索之间的关键相互作用。代码发布于\href{此 https URL}{Github}。</li>
</ul>

<h3>Title: Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20145">https://arxiv.org/abs/2512.20145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20145">https://arxiv.org/pdf/2512.20145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20145]] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models(https://arxiv.org/abs/2512.20145)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.</li>
<li><strong>摘要：</strong>预训练基础模型（PFM）对于促进大规模多模式学习至关重要。研究人员通过即时学习有效地运用了“预训练、提示和预测”范例，以提高小样本的性能。然而，PFM 的即时学习方法仍然遵循参数化学习范式。因此，记忆和死记硬背的泛化稳定性可能会受到损害。更具体地说，传统的即时学习在完全监督的训练过程中可能面临充分利用非典型实例并避免过度拟合有限数据的浅层模式的困难。为了克服这些限制，我们提出了名为 RetroPrompt 的方法，该方法旨在通过将知识与单纯的记忆解耦来实现记忆和泛化之间的平衡。与传统的提示方法不同，RetroPrompt 利用从训练数据生成的可公开访问的知识库，并在整个输入、训练和推理阶段结合了检索机制。这使得模型能够主动从语料库中检索相关的上下文信息，从而增强可用的线索。我们对自然语言处理和计算机视觉任务的各种数据集进行了全面的实验，以证明我们提出的方法 RetroPrompt 在零样本和少样本场景中的卓越性能。通过对记忆模式的详细分析，我们观察到 RetroPrompt 有效减少了对死记硬背的依赖，从而增强了泛化能力。</li>
</ul>

<h3>Title: Fun-Audio-Chat Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, Qiquan Zhang, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20156">https://arxiv.org/abs/2512.20156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20156">https://arxiv.org/pdf/2512.20156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20156]] Fun-Audio-Chat Technical Report(https://arxiv.org/abs/2512.20156)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.</li>
<li><strong>摘要：</strong>联合语音文本模型的最新进展显示出无缝语音交互的巨大潜力。然而，现有模型面临着严峻的挑战：语音标记（25Hz）和文本标记（~3Hz）之间的时间分辨率不匹配会稀释语义信息，产生高昂的计算成本，并导致文本 LLM 知识的灾难性遗忘。我们引入了 Fun-Audio-Chat，这是一种大型音频语言模型，通过我们之前的工作 DrVoice 的两项创新来解决这些限制。首先，双分辨率语音表示 (DRSR)：共享 LLM 以高效的 5Hz 处理音频（通过令牌分组），而语音优化头以 25Hz 生成高质量令牌，平衡效率（GPU 减少约 50%）和质量。其次，核心鸡尾酒训练，这是一种带有中间合并的两阶段微调，可以减轻灾难性遗忘。然后，我们应用多任务 DPO 培训来增强鲁棒性、音频理解、指令遵循和语音同理心。这种多阶段的后期培训使 Fun-Audio-Chat 能够保留文本 LLM 知识，同时获得强大的音频理解、推理和生成能力。与最近需要大规模音频文本预训练的 LALM 不同，Fun-Audio-Chat 利用预训练模型和广泛的后训练。 Fun-Audio-Chat 8B 和 MoE 30B-A3B 在 Speech-to-Text 和 Speech-to-Speech 任务上实现了具有竞争力的性能，在 Spoken QA 基准上的类似规模模型中名列前茅。他们还在音频理解、语音功能调用、指令遵循和语音同理心方面实现了竞争甚至卓越的表现。我们开发了 Fun-Audio-Chat-Duplex，这是一种全双工变体，在口语 QA 和全双工交互方面具有强大的性能。我们开源了 Fun-Audio-Chat-8B 以及训练和推理代码，并提供了交互式演示。</li>
</ul>

<h3>Title: AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications</h3>
<ul>
<li><strong>Authors: </strong>Honglin Mu, Jinghao Liu, Kaiyang Wan, Rui Xing, Xiuying Chen, Timothy Baldwin, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20164">https://arxiv.org/abs/2512.20164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20164">https://arxiv.org/pdf/2512.20164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20164]] AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications(https://arxiv.org/abs/2512.20164)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by "adversarial instructions" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 擅长文本理解和生成，使其成为代码审查和内容审核等自动化任务的理想选择。然而，我们的研究发现了一个漏洞：法学硕士可以被隐藏在输入数据（例如简历或代码）中的“对抗性指令”操纵，导致他们偏离预期任务。值得注意的是，虽然代码审查等成熟领域可能存在防御措施，但在简历筛选和同行评审等其他常见应用程序中通常不存在防御措施。本文介绍了一个在简历筛选中评估此漏洞的基准，显示某些攻击类型的攻击成功率超过 80%。我们评估了两种防御机制：基于提示的防御实现了 10.1% 的攻击减少和 12.5% 的错误拒绝增加，而我们提出的使用 LoRA 自适应的 FIDS（通过分离的外来指令检测）实现了 15.4% 的攻击减少和 10.4% 的错误拒绝增加。组合方法可将攻击减少 26.3%，这表明训练时防御在安全性和效用保留方面均优于推理时缓解。</li>
</ul>

<h3>Title: FaithLens: Detecting and Explaining Faithfulness Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20182">https://arxiv.org/abs/2512.20182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20182">https://arxiv.org/pdf/2512.20182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20182]] FaithLens: Detecting and Explaining Faithfulness Hallucination(https://arxiv.org/abs/2512.20182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.</li>
<li><strong>摘要：</strong>识别大型语言模型（LLM）的输出是否包含忠实幻觉对于现实世界的应用（例如检索增强生成和摘要）至关重要。在本文中，我们介绍了 FaithLens，这是一种经济高效且有效的忠诚幻觉检测模型，可以联合提供二元预测和相应的解释以提高可信度。为了实现这一目标，我们首先通过高级法学硕士合成带有解释的训练数据，并应用明确定义的数据过滤策略来确保标签正确性、解释质量和数据多样性。随后，我们在这些精心策划的训练数据上对模型进行微调作为冷启动，并通过基于规则的强化学习进一步优化模型，并使用预测正确性和解释质量的奖励。 12 项不同任务的结果表明，8B 参数 FaithLens 的性能优于 GPT-4.1 和 o3 等先进模型。此外，FaithLens 可以提供高质量的解释，实现可信性、效率和有效性的独特平衡。</li>
</ul>

<h3>Title: Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings</h3>
<ul>
<li><strong>Authors: </strong>Marko Čechovič, Natália Komorníková, Dominik Macháček, Ondřej Bojar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20204">https://arxiv.org/abs/2512.20204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20204">https://arxiv.org/pdf/2512.20204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20204]] Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings(https://arxiv.org/abs/2512.20204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings. Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.</li>
<li><strong>摘要：</strong>语音处理和翻译技术有潜力促进没有任何共同语言的个人的会议。为了评估此类任务的自动化系统，需要一个多功能且现实的评估语料库。因此，我们创建并呈现了没有共同语言的个体之间的跨语言对话语料库，这些对话通过自动同声语音翻译得以实现。该语料库包含 5 小时的语音录音，包括 12 种原始语言的 ASR 和黄金转录本，以及自动和更正的英语翻译。为了研究跨语言摘要，我们的语料库还包括会议的书面摘要（会议记录）。此外，我们建议自动检测误解。为了概述这项任务及其复杂性，我们尝试量化跨语言会议中的误解。我们手动注释误解，并测试当前大型语言模型自动检测它们的能力。结果表明，Gemini 模型能够识别存在误解的文本跨度，召回率为 77%，准确率为 47%。</li>
</ul>

<h3>Title: SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</h3>
<ul>
<li><strong>Authors: </strong>Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20292">https://arxiv.org/abs/2512.20292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20292">https://arxiv.org/pdf/2512.20292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20292]] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers(https://arxiv.org/abs/2512.20292)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.</li>
<li><strong>摘要：</strong>自动生成演示幻灯片可以极大地简化内容创建。然而，由于每个用户的偏好可能有所不同，现有的未指定的配方通常会导致次优结果，无法满足个人用户的需求。我们引入了一项新颖的任务，该任务根据用户指定的偏好来调节纸张到幻灯片的生成。我们提出了一个受人类行为启发的代理框架 SlideTailor，它以用户对齐的方式逐步生成可编辑的幻灯片。我们的系统不需要用户以详细的文本形式写下他们的偏好，而是只需要一个纸质幻灯片示例对和一个视觉模板——自然且易于提供的工件，可以跨内容和视觉风格隐式编码丰富的用户偏好。尽管这些输入具有隐式和未标记的性质，但我们的框架有效地提炼和概括了偏好以指导定制幻灯片生成。我们还引入了一种新颖的语音链机制，使幻灯片内容与计划的口头叙述保持一致。这种设计显着提高了生成幻灯片的质量，并支持视频演示等下游应用程序。为了支持这项新任务，我们构建了一个基准数据集，可以捕获不同的用户偏好，并使用精心设计的可解释指标来进行稳健的评估。大量的实验证明了我们框架的有效性。</li>
</ul>

<h3>Title: AprielGuard</h3>
<ul>
<li><strong>Authors: </strong>Jaykumar Kasundra, Anjaneya Praharaj, Sourabh Surana, Lakshmi Sirisha Chodisetty, Sourav Sharma, Abhigya Verma, Abhishek Bhardwaj, Debasish Kanhar, Aakash Bhagat, Khalil Slimi, Seganrasan Subramanian, Sathwik Tejaswi Madhusudhan, Ranga Prasad Chenna, Srinivas Sunkara</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20293">https://arxiv.org/abs/2512.20293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20293">https://arxiv.org/pdf/2512.20293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20293]] AprielGuard(https://arxiv.org/abs/2512.20293)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.</li>
<li><strong>摘要：</strong>保护大型语言模型 (LLM) 免受不安全或对抗行为的影响至关重要，因为它们越来越多地部署在对话和代理环境中。现有的审核工具通常将安全风险（例如毒性、偏见）和对抗性威胁（例如即时注入、越狱）视为单独的问题，限制了它们的稳健性和普遍性。我们引入了 AprielGuard，这是一个 8B 参数保护模型，它将这些维度统一在一个分类和学习框架内。 AprielGuard 经过各种开放和合成数据的训练，涵盖独立提示、多轮对话和代理工作流程，并通过结构化推理跟踪进行增强，以提高可解释性。在多个公共和专有基准测试中，AprielGuard 在检测有害内容和对抗性操作方面取得了出色的性能，优于 Llama-Guard 和 Granite Guardian 等现有开源护栏，特别是在多步骤和推理密集型场景中。通过发布该模型，我们的目标是推进对法学硕士可靠保障的透明且可重复的研究。</li>
</ul>

<h3>Title: Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives</h3>
<ul>
<li><strong>Authors: </strong>Karolina Drożdż, Kacper Dudzic, Anna Sterna, Marcin Moskalewicz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20298">https://arxiv.org/abs/2512.20298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20298">https://arxiv.org/pdf/2512.20298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20298]] Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives(https://arxiv.org/abs/2512.20298)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term "narcissism." Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.</li>
<li><strong>摘要：</strong>精神病学自我评估越来越依赖法学硕士，这引发了人们对他们解释定性患者叙述的能力的质疑。我们利用波兰语第一人称自传叙述，首次对最先进的法学硕士和心理健康专业人士在诊断边缘型（BPD）和自恋型（NPD）人格障碍方面进行了直接比较。我们发现，表现最好的 Gemini Pro 模型的整体诊断准确率超过了人类专业人士 21.91 个百分点（65.48% vs. 43.57%）。虽然模型和人类专家都擅长识别 BPD（分别为 F1 = 83.4 和 F1 = 80.0），但模型对 NPD 的诊断严重不足（F1 = 6.7 vs. 50.0），这表明模型不愿意使用充满价值的术语“自恋”。定性地讲，模型提供了专注于模式和形式类别的自信、详尽的论证，而人类专家则保持简洁和谨慎，强调患者的自我意识和时间体验。我们的研究结果表明，虽然法学硕士非常有能力解释复杂的第一人称临床数据，但他们仍然面临严重的可靠性和偏见问题。</li>
</ul>

<h3>Title: SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision</h3>
<ul>
<li><strong>Authors: </strong>Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20308">https://arxiv.org/abs/2512.20308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20308">https://arxiv.org/pdf/2512.20308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20308]] SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision(https://arxiv.org/abs/2512.20308)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at this https URL.</li>
<li><strong>摘要：</strong>语言建模和语音表示学习的并行进步提出了直接从语音学习语言而无需文本中间的前景。这需要直接从语音中提取语义表示。我们的贡献是三重的。首先，我们介绍 SpidR，这是一种自监督语音表示模型，可以有效地学习具有高度可访问的语音信息的表示，这使得它特别适合无文本口语建模。它使用屏蔽预测目标结合自蒸馏和在线聚类对原始波形进行训练。学生模型的中间层学习预测从教师的中间层派生的作业。与以前的方法相比，此学习目标稳定了在线聚类过程，从而产生更高质量的码本。 SpidR 在下游语言建模基准（sWUGGY、sBLIMP、tSC）上优于 wav2vec 2.0、HuBERT、WavLM 和 DinoSR。其次，我们跨模型和层系统地评估语音单元质量（ABX、PNMI）和语言建模性能之间的相关性，验证这些指标作为可靠的代理。最后，与 HuBERT 相比，SpidR 显着缩短了预训练时间，仅需要在 16 个 GPU 上进行一天的预训练，而不是一周。这种加速是通过预训练方法和高效的代码库实现的，它允许更快的迭代和更容易的实验。我们在此 https URL 开源训练代码和模型检查点。</li>
</ul>

<h3>Title: Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles</h3>
<ul>
<li><strong>Authors: </strong>Nurul Labib Sayeedi, Md. Faiyaz Abdullah Sayeedi, Khushnur Binte Jahangir, Swakkhar Shatabda, Sarah Masud Preum</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20324">https://arxiv.org/abs/2512.20324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20324">https://arxiv.org/pdf/2512.20324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20324]] Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles(https://arxiv.org/abs/2512.20324)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在许多 NLP 基准测试中显示出令人印象深刻的性能，但它们在比喻、文化基础和资源匮乏的环境中进行推理的能力仍未得到充分开发。我们通过引入 BanglaRiddleEval 来解决孟加拉语的这一差距，这是跨四个任务实例化的 1,244 个传统孟加拉语谜语的基准（总共 4,976 个谜语任务工件）。使用基于法学硕士的管道，我们生成思想链解释、语义连贯的干扰项和细粒度的歧义注释，并在不同的提示策略下评估各种开源和闭源模型。模型在生成 QA 上实现了适度的语义重叠，但正确性较低，MCQ 准确率峰值仅为 56% 左右，而人类基线为 83%，歧义分辨率范围约为 26% 到 68%，高质量的解释仅限于最强的模型。这些结果表明，当前的法学硕士捕获了孟加拉谜语推理所需的一些线索，但与人类水平的表现仍相距甚远，从而将 BanglaRiddleEval 确立为低资源比喻推理的具有挑战性的新基准。所有数据、代码和评估脚本均可在 GitHub 上获取：此 https URL。</li>
</ul>

<h3>Title: Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation</h3>
<ul>
<li><strong>Authors: </strong>Nilesh Jain, Seyi Adeyinka, Leor Roseman, Aza Allsop</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20352">https://arxiv.org/abs/2512.20352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20352">https://arxiv.org/pdf/2512.20352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20352]] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation(https://arxiv.org/abs/2512.20352)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, multi-run</a></li>
<li><strong>Abstract: </strong>Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($\kappa$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($\kappa = 0.907$, cosine=95.3%), followed by GPT-4o ($\kappa = 0.853$, cosine=92.6%) and Claude ($\kappa = 0.842$, cosine=92.1%). All three models achieve a high agreement ($\kappa > 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.</li>
<li><strong>摘要：</strong>定性研究面临着严峻的可靠性挑战：传统的评估者间一致方法需要多个人类编码员，耗时，并且通常会产生中等程度的一致性。我们提出了一个基于 LLM 的主题分析的多视角验证框架，该框架将集成验证与双重可靠性指标相结合：用于评估者间一致性的 Cohen's Kappa ($\kappa$) 和用于语义一致性的余弦相似度。我们的框架支持可配置的分析参数（1-6 个种子，温度 0.0-2.0），支持具有变量替换的自定义提示结构，并提供跨任何 JSON 格式的共识主题提取。作为概念验证，我们在迷幻艺术治疗访谈记录中评估了三位领先的法学硕士（Gemini 2.5 Pro、GPT-4o、Claude 3.5 Sonnet），每个模型进行六次独立运行。结果表明，Gemini 获得了最高的可靠性（$\kappa = 0.907$，余弦=95.3%），其次是 GPT-4o（$\kappa = 0.853$，余弦=92.6%）和 Claude（$\kappa = 0.842$，余弦=92.1%）。所有三个模型都达到了很高的一致性（$\kappa > 0.80$），验证了多运行集成方法。该框架成功地提取了运行中的共识主题，Gemini 确定了 6 个共识主题（一致性为 50-83%），GPT-4o 确定了 5 个主题，Claude 确定了 4 个主题。我们的开源实现为研究人员提供了透明的可靠性指标、灵活的配置和与结构无关的共识提取，为可靠的人工智能辅助定性研究奠定了方法论基础。</li>
</ul>

<h3>Title: Step-DeepResearch Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Chen Hu, Haikuo Du, Heng Wang, Lin Lin, Mingrui Chen, Peng Liu, Ruihang Miao, Tianchi Yue, Wang You, Wei Ji, Wei Yuan, Wenjin Deng, Xiaojian Yuan, Xiaoyun Zhang, Xiangyu Liu, Xikai Liu, Yanming Xu, Yicheng Cao, Yifei Zhang, Yongyao Wang, Yubo Shu, Yurong Zhang, Yuxiang Zhang, Zheng Gong, Zhichao Chang, Binyan Li, Dan Ma, Furong Jia, Hongyuan Wang, Jiayu Liu, Jing Bai, Junlan Liu, Manjiao Liu, Na Wang, Qiuping Wu, Qinxin Du, Shiwei Li, Wen Sun, Yifeng Gong, Yonglin Chen, Yuling Zhao, Yuxuan Lin, Ziqi Ren, Zixuan Wang, Aihu Zhang, Brian Li, Buyun Ma, Kang An, Li Xie, Mingliang Li, Pan Li, Shidong Yang, Xi Chen, Xiaojia Liu, Yuchu Luo, Yuan Song, YuanHao Ding, Yuanwei Liang, Zexi Li, Zhaoning Zhang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20491">https://arxiv.org/abs/2512.20491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20491">https://arxiv.org/pdf/2512.20491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20491]] Step-DeepResearch Technical Report(https://arxiv.org/abs/2512.20491)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.</li>
<li><strong>摘要：</strong>随着法学硕士转向自主代理，深度研究已成为一个关键指标。然而，像 BrowseComp 这样的现有学术基准往往无法满足现实世界对开放式研究的需求，这需要在意图识别、长期决策和跨源验证方面具备强大的技能。为了解决这个问题，我们引入了 Step-DeepResearch，一种经济高效的端到端代理。我们提出了一种基于原子能力的数据合成策略，以加强规划和报告编写，并结合从代理中期训练到 SFT 和 RL 的渐进式训练路径。通过检查表式判断器的增强，这种方法显着提高了稳健性。此外，为了弥合中国领域的评估差距，我们针对现实的深入研究场景建立了 ADR-Bench。实验结果表明，Step-DeepResearch (32B) 在 Scale AI Research Rubrics 上得分为 61.4%。在 ADR-Bench 上，它的性能显着优于同类模型，可与 OpenAI 和 Gemini DeepResearch 等 SOTA 闭源模型相媲美。这些发现证明，精细化训练能够使中型模型以行业领先的成本效率实现专家级能力。</li>
</ul>

<h3>Title: Distilling to Hybrid Attention Models via KL-Guided Layer Selection</h3>
<ul>
<li><strong>Authors: </strong>Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda, Jiawei Zhou, Yoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20569">https://arxiv.org/abs/2512.20569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20569">https://arxiv.org/pdf/2512.20569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20569]] Distilling to Hybrid Attention Models via KL-Guided Layer Selection(https://arxiv.org/abs/2512.20569)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.</li>
<li><strong>摘要：</strong>将预训练的 softmax 注意力 Transformer 提炼成更高效的混合架构，将 softmax 和线性注意力层交错在一起，是一种很有前途的方法，可以提高 LLM 的推理效率，而无需从头开始进行昂贵的预训练。转换过程中的一个关键因素是层选择，即决定将哪些层转换为线性注意变体。本文描述了一种简单而有效的图层选择方法，该方法使用从通用文本数据的少量训练中得出的图层重要性分数。一旦选择了层，我们就使用最近的管道进行蒸馏过程本身\citep[RADLADS;][]{goldstein2025radlads}，其中包括注意力权重转移，隐藏状态对齐，基于KL的分布匹配，然后是少量的微调。我们发现这种方法比现有的层选择方法更有效，包括基于固定比率均匀交错线性注意力的启发式方法，以及依赖于专门诊断数据集的更复杂的方法。</li>
</ul>

<h3>Title: Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits</h3>
<ul>
<li><strong>Authors: </strong>Amirhosein Ghasemabadi, Di Niu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20578">https://arxiv.org/abs/2512.20578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20578">https://arxiv.org/pdf/2512.20578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20578]] Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits(https://arxiv.org/abs/2512.20578)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）生成流畅且复杂的输出，但常常无法识别自己的错误和幻觉。现有的方法通常依赖于外部判断、多样本一致性或基于文本的自我批评，这会导致额外的计算或与真实正确性的相关性较弱。我们问：法学硕士可以通过在推理过程中检查内部状态来预测自己的失败吗？我们引入了 Gnosis，一种轻量级的自我意识机制，使冻结的 LLM 能够通过解码来自隐藏状态和注意力模式的信号来执行内在的自我验证。 Gnosis 被动地观察内部痕迹，将它们压缩为固定预算描述符，并以可忽略的推理成本预测正确性，仅添加约 5M 参数，并且独立于序列长度进行操作。在数学推理、开放域问答和学术知识基准测试以及从 1.7B 到 20B 参数范围的冻结主干中，Gnosis 在准确性和校准方面始终优于强大的内部基线和大型外部判断。此外，它将零样本推广到部分生成，从而能够及早检测故障轨迹和计算感知控制。这些结果表明，可靠的正确性线索是生成过程所固有的，并且可以在没有外部监督的情况下有效地提取。</li>
</ul>

<h3>Title: Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Anand, Ehsan Shareghi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20595">https://arxiv.org/abs/2512.20595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20595">https://arxiv.org/pdf/2512.20595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20595]] Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs(https://arxiv.org/abs/2512.20595)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.</li>
<li><strong>摘要：</strong>我们引入了 Cube Bench，这是一个魔方基准，用于评估多模态大语言模型 (MLLM) 中的空间和顺序推理。该基准将性能分解为五项技能：（i）从图像和文本重建立方体面，（ii）选择最佳的下一步行动，（iii）预测候选行动的结果而不应用它，（iv）在从错误中恢复的同时执行多步骤计划，以及（v）检测和修正自己的错误。使用一组共享的置乱立方体状态、相同的提示和解析器以及单个解决距离度量，我们将最近的 MLLM 作为置乱深度的函数并排进行比较。在 7 个 MLLM 中，精度随着深度的增加而急剧下降；一旦轨迹停滞或发散，模型很少恢复，并且高面部重建精度并不能保证有效的动作选择或多步骤执行。封闭与开源之间存在明显的差距：最强大的封闭模型在单步感知任务和多步控制任务上都领先，而开放权重模型在最困难的设置上几乎有机会聚集；然而，即使是最好的 MLLM，也会在更高的立方体复杂度下降低性能。通过反思性思维进行简单的自我纠正会产生一定的效果，但也可能导致过度思考。 Cube Bench 为 MLLM 中的顺序空间推理提供了一种紧凑、可重复的探针。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
