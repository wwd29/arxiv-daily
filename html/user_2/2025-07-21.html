<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-21</h1>
<h3>Title: Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Atharva Bhargude, Ishan Gonehal, Chandler Haney, Dave Yoon, Kevin Zhu, Aaron Sandoval, Sean O'Brien, Kaustubh Vinnakota</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13357">https://arxiv.org/abs/2507.13357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13357">https://arxiv.org/pdf/2507.13357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13357]] Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models(https://arxiv.org/abs/2507.13357)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Phishing attacks represent a significant cybersecurity threat, necessitating adaptive detection techniques. This study explores few-shot Adaptive Linguistic Prompting (ALP) in detecting phishing webpages through the multimodal capabilities of state-of-the-art large language models (LLMs) such as GPT-4o and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides LLMs to analyze textual deception by breaking down linguistic patterns, detecting urgency cues, and identifying manipulative diction commonly found in phishing content. By integrating textual, visual, and URL-based analysis, we propose a unified model capable of identifying sophisticated phishing attempts. Our experiments demonstrate that ALP significantly enhances phishing detection accuracy by guiding LLMs through structured reasoning and contextual analysis. The findings highlight the potential of ALP-integrated multimodal LLMs to advance phishing detection frameworks, achieving an F1-score of 0.93, surpassing traditional approaches. These results establish a foundation for more robust, interpretable, and adaptive linguistic-based phishing detection systems using LLMs.</li>
<li><strong>摘要：</strong>网络钓鱼攻击代表了一个重大的网络安全威胁，需要自适应检测技术。这项研究探讨了通过最先进的大语言模型（LLMS）（例如GPT-4O和Gemini 1.5 Pro）的多模式能力来检测网络钓鱼网页，从而探索了几乎没有射击的自适应语言提示（ALP）。 ALP是一种结构化的语义推理方法，它通过分解语言模式，检测紧迫性提示并识别网络钓鱼含量中常见的操纵词来指导LLM来分析文本欺骗。通过整合基于文本，视觉和URL分析的分析，我们提出了一个能够识别复杂网络钓鱼尝试的统一模型。我们的实验表明，ALP通过通过结构化推理和上下文分析引导LLMS可以显着提高网络钓鱼检测准确性。这些发现突出了ALP集成的多模式LLMS推进网络钓鱼检测框架的潜力，达到了0.93的F1得分，超过了传统方法。这些结果为使用LLMS的更健壮，可解释和自适应的基于语言的网络钓鱼检测系统奠定了基础。</li>
</ul>

<h3>Title: Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Keito Inoshita, Rushia Harada</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13380">https://arxiv.org/abs/2507.13380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13380">https://arxiv.org/pdf/2507.13380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13380]] Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition(https://arxiv.org/abs/2507.13380)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the field of emotion recognition, the development of high-performance models remains a challenge due to the scarcity of high-quality, diverse emotional datasets. Emotional expressions are inherently subjective, shaped by individual personality traits, socio-cultural backgrounds, and contextual factors, making large-scale, generalizable data collection both ethically and practically difficult. To address this issue, we introduce PersonaGen, a novel framework for generating emotionally rich text using a Large Language Model (LLM) through multi-stage persona-based conditioning. PersonaGen constructs layered virtual personas by combining demographic attributes, socio-cultural backgrounds, and detailed situational contexts, which are then used to guide emotion expression generation. We conduct comprehensive evaluations of the generated synthetic data, assessing semantic diversity through clustering and distributional metrics, human-likeness via LLM-based quality scoring, realism through comparison with real-world emotion corpora, and practical utility in downstream emotion classification tasks. Experimental results show that PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, demonstrating its potential as a robust alternative for augmenting or replacing real-world emotional datasets.</li>
<li><strong>摘要：</strong>在情感识别领域，由于缺乏高质量，多样化的情感数据集，高性能模型的发展仍然是一个挑战。情感表达本质上是主观的，它是由个人人格特征，社会文化背景和背景因素塑造的，这使大规模，可概括的数据收集在道德和实际上都很困难。为了解决这个问题，我们介绍了Personagen，这是一个新颖的框架，用于使用基于多阶段角色的条件使用大型语言模型（LLM）生成情感丰富的文本。 Personagen通过结合人口属性，社会文化背景和详细的情境环境来构建分层虚拟角色，然后将其用于指导情绪表达产生。我们对生成的合成数据进行了全面的评估，通过聚类和分布指标评估语义多样性，通过基于LLM的质量评分的人类风格，通过与现实世界情感语料库进行比较，以及在下游情感分类任务中的实用性。实验结果表明，Personagen在产生各种，连贯和歧视性的情绪表达方面显着胜过基线方法，这表明了其潜力是增加或替换现实世界情绪数据集的强大替代方案。</li>
</ul>

<h3>Title: SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Rafiq Kamel, Filippo Guerranti, Simon Geisler, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13381">https://arxiv.org/abs/2507.13381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13381">https://arxiv.org/pdf/2507.13381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13381]] SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation(https://arxiv.org/abs/2507.13381)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly applied to tasks involving structured inputs such as graphs. Abstract Meaning Representations (AMRs), which encode rich semantics as directed graphs, offer a rigorous testbed for evaluating LLMs on text generation from such structures. Yet, current methods often arbitrarily linearize AMRs, discarding key structural cues, or rely on architectures incompatible with standard LLMs. We introduce SAFT, a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes. We compute direction-sensitive positional encodings from the magnetic Laplacian of transformed AMRs and project them into the embedding space of the LLM. While possibly applicable to any graph-structured inputs, we focus on AMR-to-text generation as a representative and challenging benchmark. SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph complexity, highlighting the value of structure-aware representations in enhancing LLM performance. SAFT offers a general and effective pathway for bridging structured data and language models.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地应用于涉及结构化输入（例如图形）的任务。按照指示图编码丰富的语义的抽象含义表示（AMRS）为从此类结构中评估文本生成的LLM提供了严格的测试床。然而，当前方法通常任意将AMR线性化，丢弃关键结构提示或依赖与标准LLM不符的体系结构。我们介绍了Saft，这是一种结构感知的微调方法，将图形拓扑注入了预验证的LLM，而无需进行体系结构变化。我们从转化的AMR的磁性laplacian计算方向敏感的位置编码，并将其投射到LLM的嵌入空间中。虽然可能适用于任何图形结构化输入，但我们将重点放在AMR到文本生成作为代表性且具有挑战性的基准上。 Saft在AMR 3.0上设置了新的最新技术，比基线的BLEU改进了3.5。具有图形复杂性的增长量表，突出了结构感知表示在增强LLM性能中的价值。 SAFT为桥接结构化数据和语言模型提供了一般有效的途径。</li>
</ul>

<h3>Title: PARAM-1 BharatGen 2.9B Model</h3>
<ul>
<li><strong>Authors: </strong>Kundeshwar Pundalik, Piyush Sawarkar, Nihar Sahoo, Abhishek Shinde, Prateek Chanda, Vedant Goswami, Ajay Nagpal, Atul Singh, Viraj Thakur, Vijay Dewane, Aamod Thakur, Bhargav Patel, Smita Gautam, Bhagwan Panditi, Shyam Pawar, Madhav Kotcha, Suraj Racha, Saral Sureka, Pankaj Singh, Rishi Bal, Rohit Saluja, Ganesh Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13390">https://arxiv.org/abs/2507.13390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13390">https://arxiv.org/pdf/2507.13390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13390]] PARAM-1 BharatGen 2.9B Model(https://arxiv.org/abs/2507.13390)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful general-purpose reasoning systems, yet their development remains dominated by English-centric data, architectures, and optimization paradigms. This exclusionary design results in structural under-representation of linguistically diverse regions such as India, where over 20 official languages and 100+ dialects coexist alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a 2.9B parameter decoder-only, text-only language model trained from scratch with an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is trained on a bilingual dataset consisting of only Hindi and English, constructed with a strong focus on fact-rich, high-quality content. It is guided by three core principles: equitable representation of Indic languages through a 25% corpus allocation; tokenization fairness via a SentencePiece tokenizer adapted to Indian morphological structures; and culturally aligned evaluation benchmarks across IndicQA, code-mixed reasoning, and socio-linguistic robustness tasks. By embedding diversity at the pretraining level-rather than deferring it to post-hoc alignment-PARAM-1 offers a design-first blueprint for equitable foundation modeling. Our results demonstrate that it serves as both a competent general-purpose model and a robust baseline for India-centric applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已成为强大的通用推理系统，但其发展仍然由以英语为中心的数据，体系结构和优化范式主导。这种排他性的设计导致在语言上不同地区（例如印度）的结构不足，其中20多种官方语言和100多种方言与现象以及诸如代码转换和挖掘的现象并存。我们介绍了Param-1，这是一个仅2.9b参数解码器，仅文本语言模型，该模型从头开始训练，具有明确的建筑和语言对印度多样性的关注。 Param-1是在仅印度语和英语组成的双语数据集上训练的，该数据集的重点是富裕，高质量的内容。它以三个核心原则为指导：通过25％的语料库分配对指示语言的公平表示；通过句子令牌的代币化公平，适合印度的形态结构；以及跨文化的评估基准，跨indicqa，代码混合推理和社会语言鲁棒性任务。通过将多样性嵌入到预处理水平上，而不是将其推迟到事后对齐后-Param-1为公平基础建模提供设计优先的蓝图。我们的结果表明，它既是一个有能力的通用模型，又是以印度为中心的应用程序的强大基准。</li>
</ul>

<h3>Title: TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction</h3>
<ul>
<li><strong>Authors: </strong>Emil Häglund, Johanna Björklund</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13392">https://arxiv.org/abs/2507.13392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13392">https://arxiv.org/pdf/2507.13392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13392]] TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction(https://arxiv.org/abs/2507.13392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We improve the extraction of insights from customer reviews by restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores. Prior work has demonstrated that such units can be reliably extracted using large language models. The result is a heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. By correlating the topics and sentiments with business metrics, such as star ratings, we can gain insights on how specific customer concerns impact business outcomes. We present our system's implementation, use cases, and advantages over other topic modeling and classification solutions. We also evaluate its effectiveness in creating coherent topics and assess methods for integrating topic and sentiment modalities for accurate star-rating prediction.</li>
<li><strong>摘要：</strong>我们通过重组主题建模管道以在意见单位运行 - 包括相关文本摘录和相关情感分数的不同陈述，从而改善了从客户评论中提取见解。先前的工作表明，可以使用大语言模型可靠地提取此类单元。结果是随后的主题建模的性能提高，导致连贯且可解释的主题，同时还捕捉了与每个主题相关的情感。通过将主题和情感与业务指标（例如星级评级）相关联，我们可以了解特定客户关注如何影响业务成果。我们介绍了系统的实施，用例和优势，而不是其他主题建模和分类解决方案。我们还评估了其在创建连贯的主题和评估整合主题和情感方式的方法方面的有效性，以进行准确的星级预测。</li>
</ul>

<h3>Title: Causal Language Control in Multilingual Transformers via Sparse Feature Steering</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Ting Chou, George Liu, Jessica Sun, Cole Blondin, Kevin Zhu, Vasu Sharma, Sean O'Brien</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13410">https://arxiv.org/abs/2507.13410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13410">https://arxiv.org/pdf/2507.13410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13410]] Causal Language Control in Multilingual Transformers via Sparse Feature Steering(https://arxiv.org/abs/2507.13410)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Deterministically controlling the target generation language of large multilingual language models (LLMs) remains a fundamental challenge, particularly in zero-shot settings where neither explicit language prompts nor fine-tuning are available. In this work, we investigate whether sparse autoencoder (SAE) features, previously shown to correlate with interpretable model behaviors, can be leveraged to steer the generated language of LLMs during inference. Leveraging pretrained SAEs on the residual streams of Gemma-2B and Gemma-9B, we identify features whose activations differ most significantly between English and four target languages: Chinese, Japanese, Spanish, and French. By modifying just a single SAE feature at one transformer layer, we achieve controlled language shifts with up to 90\% success, as measured by FastText language classification, while preserving semantic fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding) similarity. Our analysis reveals that language steering is most effective in mid-to-late transformer layers and is amplified by specific attention heads disproportionately associated with language-sensitive SAE features. These results demonstrate the promise of sparse feature steering as a lightweight and interpretable mechanism for controllable multilingual generation.</li>
<li><strong>摘要：</strong>确定性地控制大型多语言模型（LLMS）的目标生成语言仍然是一个基本挑战，尤其是在零摄像的设置中，既没有明确的语言提示也不可用。在这项工作中，我们研究了稀疏的自动编码器（SAE）功能是否可以利用与可解释的模型行为相关的特征，可以利用推理期间引导LLMS的生成语言。利用预审预测的SAE在Gemma-2b和Gemma-9B的残留流中，我们确定了其激活在英语和四种目标语言之间的激活最大的特征：中文，日语，西班牙语和法语。通过仅修改一个变压器层的单个SAE功能，我们通过FastText语言分类来实现受控的语言转移，最多可以进行90 \％的成功，同时根据LABSE（语言 - 敏捷的bert句子嵌入）保留语义保真度。我们的分析表明，语言转向在中间到主体的变压器层中最有效，并且通过与对语言敏感的SAE特征不成比例的特定注意力头部放大。这些结果证明了稀疏特征转向的希望是可控的多语言生成的轻巧且可解释的机制。</li>
</ul>

<h3>Title: Aligning Knowledge Graphs and Language Models for Factual Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Nur A Zarin Nishat, Andrea Coletta, Luigi Bellomarini, Kossi Amouzouvi, Jens Lehmann, Sahar Vahdati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13411">https://arxiv.org/abs/2507.13411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13411">https://arxiv.org/pdf/2507.13411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13411]] Aligning Knowledge Graphs and Language Models for Factual Accuracy(https://arxiv.org/abs/2507.13411)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models like GPT-4, Gemini, and Claude have transformed natural language processing (NLP) tasks such as question answering, dialogue generation, summarization, and so forth; yet their susceptibility to hallucination stands as one of the major challenges. Among numerous approaches to overcome this challenge, integration of Knowledge Graphs (KGs) into language models has emerged as a promising solution as it provides structured, reliable, domain-specific, and up-to-date external information to the language models. In this paper, we introduce ALIGNed-LLM, a simple yet effective approach to improve language models' factuality via a lean strategy to infuse KGs into the latent space of language models inspired by LLaVA where visual and textual information is infused. We use embeddings from a pre-trained Knowledge Graph Embedding (KGE) model, such as TransE, and a trainable projection layer to align entity and text embeddings. This alignment enables the language model to distinguish between similar entities improving factual grounding and reducing hallucination. We tested our approach on three popular questions-answering benchmark datasets alongside language models of varying sizes, showing significant improvement. Furthermore, we applied our approach to a real-world financial use case from a large central bank in Europe, which demands high accuracy and precision, demonstrating a substantial improvement of the LLM answers.</li>
<li><strong>摘要：</strong>GPT-4，Gemini和Claude等大型语言模型已经改变了自然语言处理（NLP）任务，例如问答，对话生成，摘要等；然而，他们对幻觉的敏感性是主要挑战之一。在克服这一挑战的众多方法中，知识图（kg）纳入语言模型的方法已成为一种有前途的解决方案，因为它为语言模型提供了结构化，可靠，特定领域和最新的外部信息。在本文中，我们介绍了Aligned-llm，这是一种简单而有效的方法，可以通过精益策略来改善语言模型的事实，以将KG注入受LLAVA启发的语言模型的潜在空间，从而在其中注入了视觉和文本信息。我们使用预先训练的知识图嵌入（KGE）模型（例如Transe）以及可训练的投影层来对齐实体和文本嵌入的嵌入。这种对齐使语言模型能够区分改善事实基础和减少幻觉的相似实体。我们对三个流行问题的基准数据集进行了测试，以及不同尺寸的语言模型，显示出显着改善。此外，我们将我们的方法应用于欧洲一家大型央行的现实金融用例，这需要高准确性和准确性，这表明LLM答案有了很大的改善。</li>
</ul>

<h3>Title: Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers</h3>
<ul>
<li><strong>Authors: </strong>Liang Lin, Zhihao Xu, Xuehai Tang, Shi Liu, Biyu Zhou, Fuqing Zhu, Jizhong Han, Songlin Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13474">https://arxiv.org/abs/2507.13474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13474">https://arxiv.org/pdf/2507.13474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13474]] Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers(https://arxiv.org/abs/2507.13474)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The safety of large language models (LLMs) has garnered significant research attention. In this paper, we argue that previous empirical studies demonstrate LLMs exhibit a propensity to trust information from authoritative sources, such as academic papers, implying new possible vulnerabilities. To verify this possibility, a preliminary analysis is designed to illustrate our two findings. Based on this insight, a novel jailbreaking method, Paper Summary Attack (\llmname{PSA}), is proposed. It systematically synthesizes content from either attack-focused or defense-focused LLM safety paper to construct an adversarial prompt template, while strategically infilling harmful query as adversarial payloads within predefined subsections. Extensive experiments show significant vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on Deepseek-R1. More intriguingly, our work has further revealed diametrically opposed vulnerability bias across different base models, and even between different versions of the same model, when exposed to either attack-focused or defense-focused papers. This phenomenon potentially indicates future research clues for both adversarial methodologies and safety this http URL is available at this https URL</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的安全吸引了大量的研究关注。在本文中，我们认为以前的实证研究表明，LLMS表现出信任来自权威来源的信息的倾向，例如学术论文，这意味着新的可能的脆弱性。为了验证这种可能性，初步分析旨在说明我们的两个发现。基于这种见解，提出了一种新颖的越狱方法，即纸质摘要攻击（\ llmname {psa}）。它系统地综合了以攻击为中心或以防御为中心的LLM安全纸来构建对抗性及时及时模板的内容，同时从战略上填充有害查询为预定义的小节中的对抗性有效载荷。广泛的实验不仅显示了基本LLM中的重大漏洞，而且还显示了诸如DeepSeek-R1之类的最新推理模型。 PSA在诸如Claude3.5-Sonnet等良好模型上获得了97 \％的攻击成功率（ASR），在DeepSeek-R1上获得了更高的98 \％ASR。更有趣的是，我们的工作进一步揭示了不同基本模型之间的直径相对的脆弱性偏见，甚至在同一模型的不同版本之间，也暴露于以攻击为中心或以防御为中心的论文时。这种现象有可能表明对抗方法和安全的未来研究线索，此HTTP URL可在此HTTPS URL上获得</li>
</ul>

<h3>Title: Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?</h3>
<ul>
<li><strong>Authors: </strong>Siqi Shen, Mehar Singh, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13490">https://arxiv.org/abs/2507.13490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13490">https://arxiv.org/pdf/2507.13490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13490]] Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?(https://arxiv.org/abs/2507.13490)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>There has been extensive research on assessing the value orientation of Large Language Models (LLMs) as it can shape user experiences across demographic groups. However, several challenges remain. First, while the Multiple Choice Question (MCQ) setting has been shown to be vulnerable to perturbations, there is no systematic comparison of probing methods for value probing. Second, it is unclear to what extent the probed values capture in-context information and reflect models' preferences for real-world actions. In this paper, we evaluate the robustness and expressiveness of value representations across three widely used probing strategies. We use variations in prompts and options, showing that all methods exhibit large variances under input perturbations. We also introduce two tasks studying whether the values are responsive to demographic context, and how well they align with the models' behaviors in value-related scenarios. We show that the demographic context has little effect on the free-text generation, and the models' values only weakly correlate with their preference for value-based actions. Our work highlights the need for a more careful examination of LLM value probing and awareness of its limitations.</li>
<li><strong>摘要：</strong>在评估大语言模型（LLM）的价值方向方面，已经进行了广泛的研究，因为它可以塑造人群群体之间的用户体验。但是，仍然存在一些挑战。首先，虽然已证明多项选择问题（MCQ）设置容易受到扰动的影响，但没有系统的比较值探测值的探测方法。其次，目前尚不清楚探测值在多大程度上捕获了文本信息，并反映了模型对现实世界动作的偏好。在本文中，我们评估了三种广泛使用的探测策略中价值表示的鲁棒性和表现力。我们在提示和选项中使用变化，表明所有方法在输入扰动下显示出较大的差异。我们还介绍了两个任务，研究值是否对人口统计环境有响应，以及它们与模型在与价值相关的方案中的行为的一致性。我们表明，人口统计学上下文对自由文本的生成几乎没有影响，而模型的值仅与他们对基于价值的动作的偏爱微弱相关。我们的工作强调了需要更仔细地检查LLM价值探测和对其局限性的认识。</li>
</ul>

<h3>Title: A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Achref Ben Ammar, Mohamed Taha Bennani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13544">https://arxiv.org/abs/2507.13544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13544">https://arxiv.org/pdf/2507.13544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13544]] A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows(https://arxiv.org/abs/2507.13544)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>The analysis of conversational dynamics has gained increasing importance with the rise of large language model-based systems, which interact with users across diverse contexts. In this work, we propose a novel computational framework for constructing conversational graphs that capture the flow and structure of loosely organized dialogues, referred to as quasi-patterned conversations. We introduce the Filter & Reconnect method, a novel graph simplification technique that minimizes noise while preserving semantic coherence and structural integrity of conversational graphs. Through comparative analysis, we demonstrate that the use of large language models combined with our graph simplification technique has resulted in semantic metric S increasing by a factor of 2.06 compared to previous approaches while simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity, ensuring optimal clarity in conversation modeling. This work provides a computational method for analyzing large-scale dialogue datasets, with practical applications related to monitoring automated systems such as chatbots, dialogue management tools, and user behavior analytics.</li>
<li><strong>摘要：</strong>随着大型基于语言模型的系统的兴起，对话动态的分析已越来越重要，这些系统与各种环境之间的用户互动。在这项工作中，我们提出了一个新颖的计算框架，用于构建对话图，以捕获松散组织的对话的流量和结构，称为准图案对话。我们介绍了滤波器和重新连接方法，这是一种新型的图形简化技术，可在保持对话图的语义连贯性和结构完整性的同时最小化噪声。通过比较分析，我们证明，与以前的方法相比，使用大型语言模型与我们的图形简化技术相比，语义度量S增加了2.06倍，同时同时实施了具有0 {\ delta} -Hyhyperbololicity的树状结构，从而确保了对话模型中的最佳清晰度。这项工作提供了一种计算方法，用于分析大规模对话数据集，并具有与监视自动化系统（例如聊天机器人，对话管理工具和用户行为分析）相关的实用应用程序。</li>
</ul>

<h3>Title: Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder</h3>
<ul>
<li><strong>Authors: </strong>Feng Chen, Weizhe Xu, Changye Li, Serguei Pakhomov, Alex Cohen, Simran Bhola, Sandy Yin, Sunny X Tang, Michael Mackinley, Lena Palaniyappan, Dror Ben-Zeev, Trevor Cohen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13551">https://arxiv.org/abs/2507.13551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13551">https://arxiv.org/pdf/2507.13551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13551]] Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder(https://arxiv.org/abs/2507.13551)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Formal thought disorder (FTD), a hallmark of schizophrenia spectrum disorders, manifests as incoherent speech and poses challenges for clinical assessment. Traditional clinical rating scales, though validated, are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The use of utterance timestamps in ASR captures pause dynamics, which are thought to reflect the cognitive processes underlying speech production. However, the utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation. This study integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries (AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream narratives (PsyCL, n = 43). We evaluated pause related features alongside established coherence measures, using support vector regression (SVR) to predict clinical FTD scores. Key findings demonstrate that pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to \r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best \r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis.</li>
<li><strong>摘要：</strong>精神分裂症谱系障碍的标志形式思想障碍（FTD）表现为不连贯的语音，并为临床评估带来了挑战。传统的临床评级量表虽然得到了验证，但资源很密集，缺乏可扩展性。自动语音识别（ASR）的自动语音分析允许对语音和时间特征进行客观量化语音特征，从而提供可扩展的替代方案。在ASR中使用语音时间戳捕获了暂停动态，这些动态被认为反映了语音产生的认知过程。但是，整合这些ASR衍生的特征以评估FTD严重性的实用性需要进一步评估。这项研究将暂停特征与三个数据集的语义连贯性指标相结合：自然主义自我录制的日记（AVH，n = 140），结构化的图片描述（Topsy，n = 72）和梦想叙事（Psycl，n = 43）。我们使用支持矢量回归（SVR）评估了与已建立的连贯措施一起评估相关特征，以预测临床FTD分数。关键发现表明，单独的暂停特征可鲁棒地预测FTD的严重性。与单纯的模型相比，与语义一致性指标相比，将暂停功能与语义相干指标相比提高了预测性能，而独立模型的整合达到了相关性，可达到\ r {ho} = 0.649 = 0.649，而AUC = 83.71％的严重案例检测（Topsy检测，最佳\ r {HO} = 0.584和AUC = 79.23％的模型），以获取Semperantic = 79.23％的模型。尽管暂停模式的性质取决于数据集依赖于数据集，但从语义和暂停特征集成从语义和暂停特征进行了积分。这些发现表明，结合时间和语义分析的框架为精心评估言语评估并提高了精神病中的自动化语音分析提供了路线图。</li>
</ul>

<h3>Title: Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sergio E. Zanotto, Segun Aroyehun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13614">https://arxiv.org/abs/2507.13614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13614">https://arxiv.org/pdf/2507.13614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13614]] Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models(https://arxiv.org/abs/2507.13614)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. While recent research has primarily focused on using LLMs to classify text as either human-written and machine-generated texts, our study focus on characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We select a dataset of human-written and machine-generated texts spanning 8 domains and produced by 11 different LLMs. We calculate different linguistic features such as dependency length and emotionality and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls and model release date. Our statistical analysis reveals that human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Furthermore, we calculate the variability of our set of features across models and domains. Both human and machine texts show stylistic diversity across domains, with humans displaying greater variation in our features. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts. Notably, newer models output text that is similarly variable, pointing to an homogenization of machine-generated texts.</li>
<li><strong>摘要：</strong>大语言模型（LLM）的快速进步已大大提高了其产生自然语言的能力，使LLMS产生的文本与人写的文本越来越难以区分。尽管最近的研究主要集中在使用LLMS将文本分类为人类编写和机器生成的文本，但我们的研究重点是使用不同语言层面的一组语言特征来表征这些文本，例如形态学，语法和语义。我们选择一个跨越8个域并由11种不同LLM产生的人编写和机器生成的文本的数据集。我们计算不同的语言特征，例如依赖性长度和情感性，并使用它们来表征人写的和机器生成的文本以及不同的采样策略，重复控制和模型发布日期。我们的统计分析表明，人写的文本倾向于表现出更简单的句法结构和更多样化的语义内容。此外，我们计算跨模型和域的一组功能的变异性。人类和机器文本都表现出各个领域的风格多样性，人类在我们的功能方面表现出更大的差异。最后，我们将样式的嵌入方式应用于人写和机器生成的文本之间的进一步测试可变性。值得注意的是，较新的模型输出文本类似变量，指向机器生成的文本的均质化。</li>
</ul>

<h3>Title: Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</h3>
<ul>
<li><strong>Authors: </strong>Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Zhichao Huang, Tao Li, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, Runsheng Yu, Yiming Yu, Liehao Zou, Hang Li, Lu Lu, Yuxuan Wang, Yonghui Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13618">https://arxiv.org/abs/2507.13618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13618">https://arxiv.org/pdf/2507.13618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13618]] Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters(https://arxiv.org/abs/2507.13618)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.</li>
<li><strong>摘要：</strong>多语言翻译是大型语言模型（LLM）的一项挑战任务，以处理自动翻译中出现的复杂语言模式和刻板翻译。在本文中，我们介绍了一个由指示和推理模型组成的开源LLMS家族Seed-X，以7B参数大小推动了翻译能力的极限。基本模型已在多种多样的高质量数据集上进行了预训练，该数据集涵盖了28种语言的单语和双语内容，从而利用了多语言数据的全部潜力。然后，对指示模型进行了填补，以通过思想链（COT）推理进行翻译，并通过增强学习（RL）进一步增强，以在各种语言对之间进行更好的概括。 Seed-X的性能与28种语言的领先的封闭源模型相当，包括Gemini-2.5和GPT-4O在内，并且在自动指标和人类评估中都大大优于更大的开源模型。我们通过优化过程分享最佳实践，并使参数公开用于推进翻译研究和应用程序。</li>
</ul>

<h3>Title: CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer</h3>
<ul>
<li><strong>Authors: </strong>Teerapong Panboonyuen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13655">https://arxiv.org/abs/2507.13655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13655">https://arxiv.org/pdf/2507.13655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13655]] CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer(https://arxiv.org/abs/2507.13655)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data. We introduce CU-ICU, a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates, enabling efficient adaptation with minimal supervision. Our evaluation across critical ICU tasks--early sepsis detection, mortality prediction, and clinical note generation--demonstrates that CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters in its most efficient configuration. These results establish CU-ICU as a scalable, low-overhead solution for delivering accurate and interpretable clinical decision support in real-world ICU environments.</li>
<li><strong>摘要：</strong>将大型语言模型集成到医疗保健等专业领域中，提出了独特的挑战，包括适应性和有限的标记数据。我们介绍了Cu-ICU，这是一种通过利用文本到文本传输变压器（T5）体系结构来自定义ICU数据集的无监督指令的语言模型。 Cu-ICU采用了一种稀疏的微调方法，将几乎没有弹药的促使与选择性参数更新结合在一起，从而可以通过最小的监督来有效适应。我们跨关键的ICU任务的评估 - 败血症检测，死亡率预测和临床注释生成 - 表明Cu-ICU始终提高了对标准微调方法的预测准确性和可解释性。值得注意的是，Cu-ICU的败血症检测准确性提高了15％，在产生临床相关的解释方面增强了20％，同时以最有效的配置更新了少于1％的模型参数。这些结果将CU-ICU作为一种可扩展的低空解决方案建立，用于在现实世界中的ICU环境中提供准确且可解释的临床决策支持。</li>
</ul>

<h3>Title: KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Woo-Chan Kim, Ji-Hoon Park, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13666">https://arxiv.org/abs/2507.13666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13666">https://arxiv.org/pdf/2507.13666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13666]] KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs(https://arxiv.org/abs/2507.13666)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated state-of-the-art performance across a wide range of natural language processing tasks. However, high-performing models are typically accessible only via APIs, incurring substantial inference costs. Cascade methods address this by initially employing a cheaper model and escalating to a stronger one only when necessary. Nevertheless, existing cascade approaches struggle to select a reliable representative response and assess the overall reliability of free-form outputs, as they rely on exact text matching. To overcome these limitations, we propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient free-form text generation. KiC identifies the most representative answer among multiple outputs from a weaker model and evaluates the semantic alignment of other responses with it. Based on the degree of alignment, KiC determines whether to accept the weaker model's output or escalate to a stronger model. Experiments on three free-form text generation benchmarks show that KiC achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81 percent on average, and even outperforms GPT-4 in a specific benchmark.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已在各种自然语言处理任务中表现出最先进的表现。但是，高性能模型通常只能通过API访问，从而产生了大量推断成本。级联方法通过最初采用更便宜的模型来解决这一问题，并仅在必要时才升级为更强的模型。然而，现有的级联方法努力选择可靠的代表性响应并评估自由形式产出的总体可靠性，因为它们依赖于确切的文本匹配。为了克服这些局限性，我们提出了一个由关键字启发的级联（KIC），这是一个新颖的框架，用于具有成本效益的自由形式文本生成。 KIC从较弱的模型中确定了多个输出中最具代表性的答案，并评估了其他响应的语义一致性。基于对齐程度，KIC确定是接受较弱的模型的输出还是升级到更强的模型。三个自由文本生成基准的实验表明，KIC达到了GPT-4准确性的97.53％，同时将API的平均成本降低了28.81％，甚至在特定基准中均优于GPT-4。</li>
</ul>

<h3>Title: LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13681">https://arxiv.org/abs/2507.13681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13681">https://arxiv.org/pdf/2507.13681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13681]] LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues(https://arxiv.org/abs/2507.13681)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \href{this https URL}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.</li>
<li><strong>摘要：</strong>在大型语言模型（例如聊天机器人和虚拟助手）的许多真实应用程序中，多转话对话至关重要。随着对话历史的越来越长，现有的大型语言模型面临着越来越多的计算和记忆挑战，这阻碍了他们提供高效和响应互动的能力。大多数当前的加速方法可以压缩上下文或优化键值缓存，但是它们通常依赖于固定或基于位置的启发式方法，这些启发式方法不能很好地适应实际的多转交流中的动态和不可预测的模式。在本文中，我们介绍了Roopserve，这是多转向对话中大型语言模型的自适应双相推理框架。 Loopserve介绍了两个主要创新。首先，它通过动态选择每个新输入的注意力矩阵中最重要的部分，在预填充阶段进行在线稀疏。其次，它通过根据最近生成的输出令牌自适应维护相关和有效的高速缓存，在解码过程中使用渐进键值压缩。我们还提出了一个\ href {this HTTPS url} {新基准}，其中11个多转移数据集反映了现实的查询位置和对话依赖性。广泛的实验表明，与现有基线相比，Loopserve始终达到较高的有效性，并显着加速了LLM在广泛的长篇文章对话任务中的推断。</li>
</ul>

<h3>Title: Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Cedric Waterschoot, Nava Tintarev, Francesco Barile</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13705">https://arxiv.org/abs/2507.13705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13705">https://arxiv.org/pdf/2507.13705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13705]] Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations(https://arxiv.org/abs/2507.13705)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly being implemented as joint decision-makers and explanation generators for Group Recommender Systems (GRS). In this paper, we evaluate these recommendations and explanations by comparing them to social choice-based aggregation strategies. Our results indicate that LLM-generated recommendations often resembled those produced by Additive Utilitarian (ADD) aggregation. However, the explanations typically referred to averaging ratings (resembling but not identical to ADD aggregation). Group structure, uniform or divergent, did not impact the recommendations. Furthermore, LLMs regularly claimed additional criteria such as user or item similarity, diversity, or used undefined popularity metrics or thresholds. Our findings have important implications for LLMs in the GRS pipeline as well as standard aggregation strategies. Additional criteria in explanations were dependent on the number of ratings in the group scenario, indicating potential inefficiency of standard aggregation methods at larger item set sizes. Additionally, inconsistent and ambiguous explanations undermine transparency and explainability, which are key motivations behind the use of LLMs for GRS.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）越来越多地作为集团推荐系统（GRS）的联合决策者和解释发生器实施。在本文中，我们通过将这些建议与基于社会选择的聚合策略进行比较来评估这些建议和解释。我们的结果表明，LLM生成的建议通常类似于加性功利（ADD）聚合产生的建议。但是，解释通常是指平均评分（类似于但与添加聚合不同）。组结构（均匀或分歧）不会影响建议。此外，LLMS经常声称其他标准，例如用户或项目相似性，多样性或使用的未定义普及度指标或阈值。我们的发现对GRS管道中的LLM和标准聚合策略具有重要意义。解释中的其他标准取决于组方案中评分的数量，表明在较大项目集尺寸下标准聚合方法的潜在效率低下。此外，不一致且模棱两可的解释破坏了透明度和解释性，这是使用LLMS用于GRS的关键动机。</li>
</ul>

<h3>Title: The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Zambrano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13732">https://arxiv.org/abs/2507.13732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13732">https://arxiv.org/pdf/2507.13732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13732]] The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction(https://arxiv.org/abs/2507.13732)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study examines the role of human judges in legal decision-making by using machine learning to predict child physical custody outcomes in French appellate courts. Building on the legal realism-formalism debate, we test whether individual judges' decision-making patterns significantly influence case outcomes, challenging the assumption that judges are neutral variables that apply the law uniformly. To ensure compliance with French privacy laws, we implement a strict pseudonymization process. Our analysis uses 18,937 living arrangements rulings extracted from 10,306 cases. We compare models trained on individual judges' past rulings (specialist models) with a judge-agnostic model trained on aggregated data (generalist models). The prediction pipeline is a hybrid approach combining large language models (LLMs) for structured feature extraction and ML models for outcome prediction (RF, XGB and SVC). Our results show that specialist models consistently achieve higher predictive accuracy than the general model, with top-performing models reaching F1 scores as high as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x more samples. Specialist models capture stable individual patterns that are not transferable to other judges. In-Domain and Cross-Domain validity tests provide empirical support for legal realism, demonstrating that judicial identity plays a measurable role in legal outcomes. All data and code used will be made available.</li>
<li><strong>摘要：</strong>这项研究通过使用机器学习来预测法国上诉法院的儿童身体监护结果，研究了人类法官在法律决策中的作用。在法律现实主义形式的辩论的基础上，我们测试了个别法官的决策模式是否显着影响案件结果，挑战了法官是中立变量的假设，这些变量是统一适用的。为了确保遵守法国隐私法，我们实施了严格的化名过程。我们的分析使用了从10,306例案件提取的18,937个生活安排裁决。我们将对个人法官过去裁决（专业模型）培训的模型与对汇总数据（通才模型）培训的法官不可屈服的模型进行了比较。预测管道是一种混合方法，该方法结合了用于结构化特征提取的大语言模型（LLMS），用于结果预测（RF，XGB和SVC）。我们的结果表明，与一般模型相比，专业模型始终达到更高的预测精度，与通才模型的82.63％在20倍到100倍的样本中训练的82.63％相比，表现最高的模型达到了92.85％。专业模型捕获了无法转移到其他法官的稳定单个模式。内域和跨域有效性测试为法律现实主义提供了经验支持，表明司法身份在法律结果中起着可衡量的作用。所有使用的数据和代码将提供。</li>
</ul>

<h3>Title: PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Maluna Menke, Thilo Hagendorff</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13743">https://arxiv.org/abs/2507.13743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13743">https://arxiv.org/pdf/2507.13743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13743]] PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs(https://arxiv.org/abs/2507.13743)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently reproduce the gender- and sexual-identity prejudices embedded in their training corpora, leading to outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of great importance. To achieve this, we evaluate two parameter-efficient fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt tuning - as lightweight alternatives to full-model fine-tuning for mitigating such biases. Using the WinoQueer benchmark, we quantify bias in three open-source LLMs and observe baseline bias scores reaching up to 98 (out of 100) across a range of queer identities defined by gender and/or sexual orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1% additional parameters) on a curated QueerNews corpus reduces those scores by up to 50 points and raises neutrality from virtually 0% to as much as 36%. Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements. These findings show that LoRA can deliver meaningful fairness gains with minimal computation. We advocate broader adoption of community-informed PEFT, the creation of larger queer-authored corpora, and richer evaluation suites beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）经常重现其培训语料库中的性别和性认同偏见，从而导致LGBTQIA+用户边缘化的输出。因此，减少这种偏见至关重要。为了实现这一目标，我们评估了两种参数有效的微调（PEFT）技术 - 低级适应（LORA）和软启示调音 - 作为缓解此类偏见的全模型微调的轻巧替代方法。使用Winoqueer基准测试，我们在三个开源LLM中量化了偏差，并观察到基线偏差分数在性别和/或性取向定义的一系列酷儿身份中达到98（100分）（在100个中），其中50个表明中立性。在精选的女公墓上对Lora进行微调（<0.1％的其他参数）可将这些分数降低50分，并将中立性从几乎0％提高到36％。软提出调整（10个虚拟令牌）仅提供边缘改进。这些发现表明，洛拉（Lora）可以通过最少的计算提供有意义的公平收益。我们主张更广泛地采用社区信息的PEFT，创建更大的酷儿作者的Corpora以及Winoqueer以外的更丰富的评估套件，再加上正在进行的审核以保持LLMS（包括LLMS）。</li>
</ul>

<h3>Title: Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Palash Nandi, Maithili Joshi, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13761">https://arxiv.org/abs/2507.13761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13761">https://arxiv.org/pdf/2507.13761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13761]] Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models(https://arxiv.org/abs/2507.13761)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Language models are highly sensitive to prompt formulations - small changes in input can drastically alter their output. This raises a critical question: To what extent can prompt sensitivity be exploited to generate inapt content? In this paper, we investigate how discrete components of prompt design influence the generation of inappropriate content in Visual Language Models (VLMs). Specifically, we analyze the impact of three key factors on successful jailbreaks: (a) the inclusion of detailed visual information, (b) the presence of adversarial examples, and (c) the use of positively framed beginning phrases. Our findings reveal that while a VLM can reliably distinguish between benign and harmful inputs in unimodal settings (text-only or image-only), this ability significantly degrades in multimodal contexts. Each of the three factors is independently capable of triggering a jailbreak, and we show that even a small number of in-context examples (as few as three) can push the model toward generating inappropriate outputs. Furthermore, we propose a framework that utilizes a skip-connection between two internal layers of the VLM, which substantially increases jailbreak success rates, even when using benign images. Finally, we demonstrate that memes, often perceived as humorous or harmless, can be as effective as toxic visuals in eliciting harmful content, underscoring the subtle and complex vulnerabilities of VLMs.</li>
<li><strong>摘要：</strong>语言模型对及时配方非常敏感 - 输入的小变化会大大改变其输出。这提出了一个关键的问题：在多大程度上可以利用灵敏度来产生INAPT内容？在本文中，我们研究了及时设计的离散组成部分如何影响视觉语言模型（VLMS）中不适当内容的产生。具体而言，我们分析了三个关键因素对成功越狱的影响：（a）包含详细的视觉信息，（b）存在对抗性示例，以及（c）使用正面构图的开始短语。我们的发现表明，尽管VLM可以可靠地区分单峰设置（仅文本或仅图像）中的良性和有害输入，但这种能力在多模式上下文中大大降低。这三个因素中的每个因素都独立地能够触发越狱，我们表明，即使是少数文本示例（只有三个）也可以推动该模型产生不适当的产出。此外，我们提出了一个框架，该框架利用VLM的两个内部层之间的跳过连接，即使使用良性图像，该框架也大大提高了越狱的成功率。最后，我们证明了经常被认为是幽默或无害的模因在引起有害内容的情况下与有毒视觉效果一样有效，强调了VLM的微妙而复杂的脆弱性。</li>
</ul>

<h3>Title: Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hosein Azarbonyad, Zi Long Zhu, Georgios Cheirmpos, Zubair Afzal, Vikrant Yadav, Georgios Tsatsaronis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13827">https://arxiv.org/abs/2507.13827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13827">https://arxiv.org/pdf/2507.13827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13827]] Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models(https://arxiv.org/abs/2507.13827)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>When deciding to read an article or incorporate it into their research, scholars often seek to quickly identify and understand its main ideas. In this paper, we aim to extract these key concepts and contributions from scientific articles in the form of Question and Answer (QA) pairs. We propose two distinct approaches for generating QAs. The first approach involves selecting salient paragraphs, using a Large Language Model (LLM) to generate questions, ranking these questions by the likelihood of obtaining meaningful answers, and subsequently generating answers. This method relies exclusively on the content of the articles. However, assessing an article's novelty typically requires comparison with the existing literature. Therefore, our second approach leverages a Knowledge Graph (KG) for QA generation. We construct a KG by fine-tuning an Entity Relationship (ER) extraction model on scientific articles and using it to build the graph. We then employ a salient triplet extraction method to select the most pertinent ERs per article, utilizing metrics such as the centrality of entities based on a triplet TF-IDF-like measure. This measure assesses the saliency of a triplet based on its importance within the article compared to its prevalence in the literature. For evaluation, we generate QAs using both approaches and have them assessed by Subject Matter Experts (SMEs) through a set of predefined metrics to evaluate the quality of both questions and answers. Our evaluations demonstrate that the KG-based approach effectively captures the main ideas discussed in the articles. Furthermore, our findings indicate that fine-tuning the ER extraction model on our scientific corpus is crucial for extracting high-quality triplets from such documents.</li>
<li><strong>摘要：</strong>当决定阅读文章或将其纳入他们的研究中时，学者们经常寻求快速识别和理解其主要思想。在本文中，我们旨在以问题和答案的形式（QA）对从科学文章中提取这些关键概念和贡献。我们提出了两种产生QA的不同方法。第一种方法涉及使用大型语言模型（LLM）选择显着段落来产生问题，并通过获得有意义的答案并随后产生答案来对这些问题进行排名。此方法仅依赖文章的内容。但是，评估文章的新颖性通常需要与现有文献进行比较。因此，我们的第二种方法利用了质量保证生成的知识图（kg）。我们通过在科学文章上微调实体关系（ER）提取模型来构建kg，并使用它来构建图形。然后，我们采用一种显着的三重态提取方法来选择每篇文章最相关的ERS，从而利用基于三胞胎TF-IDF样量的实体的指标，例如实体的中心性。与文献中的流行相比，该措施根据文章中的重要性评估了三胞胎的显着性。为了进行评估，我们使用两种方法生成QA，并通过一组预定义的指标对主题专家（SME）进行评估，以评估问题和答案的质量。我们的评估表明，基于KG的方法有效地捕获了文章中讨论的主要思想。此外，我们的发现表明，在我们的科学语料库上微调ER提取模型对于从此类文档中提取高质量的三胞胎至关重要。</li>
</ul>

<h3>Title: Modeling Fair Play in Detective Stories with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eitan Wagner, Renana Keydar, Omri Abend</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13841">https://arxiv.org/abs/2507.13841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13841">https://arxiv.org/pdf/2507.13841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13841]] Modeling Fair Play in Detective Stories with Language Models(https://arxiv.org/abs/2507.13841)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Effective storytelling relies on a delicate balance between meeting the reader's prior expectations and introducing unexpected developments. In the domain of detective fiction, this tension is known as fair play, which includes the implicit agreement between the writer and the reader as to the range of possible resolutions the mystery story may have. In this work, we present a probabilistic framework for detective fiction that allows us to define desired qualities. Using this framework, we formally define fair play and design appropriate metrics for it. Stemming from these definitions is an inherent tension between the coherence of the story, which measures how much it ``makes sense'', and the surprise it induces. We validate the framework by applying it to LLM-generated detective stories. This domain is appealing since we have an abundance of data, we can sample from the distribution generating the story, and the story-writing capabilities of LLMs are interesting in their own right. Results show that while LLM-generated stories may be unpredictable, they generally fail to balance the trade-off between surprise and fair play, which greatly contributes to their poor quality.</li>
<li><strong>摘要：</strong>有效的讲故事依赖于满足读者先前的期望和引入意外发展之间的微妙平衡。在侦探小说的领域中，这种紧张局势被称为公平竞争，其中包括作家与读者之间关于神秘故事可能具有的一系列决议的隐性协议。在这项工作中，我们提出了侦探小说的概率框架，使我们能够定义所需的素质。使用此框架，我们正式为其定义了公平游戏，并为其设计适当的指标。来自这些定义是故事的连贯性之间的固有张力，它衡量了``有意义''的程度，以及它引起的惊喜。我们通过将其应用于LLM生成的侦探故事来验证该框架。由于我们有大量数据，因此我们可以从故事产生故事的分布中进行采样，并且LLM的故事编写功能很有趣，因此该领域很有吸引力。结果表明，尽管LLM生成的故事可能是不可预测的，但它们通常无法平衡惊喜和公平竞争之间的权衡，这极大地导致了他们的质量差。</li>
</ul>

<h3>Title: InTraVisTo: Inside Transformer Visualisation Tool</h3>
<ul>
<li><strong>Authors: </strong>Nicolò Brunello, Davide Rigamonti, Andrea Sassella, Vincenzo Scotti, Mark James Carman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13858">https://arxiv.org/abs/2507.13858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13858">https://arxiv.org/pdf/2507.13858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13858]] InTraVisTo: Inside Transformer Visualisation Tool(https://arxiv.org/abs/2507.13858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The reasoning capabilities of Large Language Models (LLMs) have increased greatly over the last few years, as have their size and complexity. Nonetheless, the use of LLMs in production remains challenging due to their unpredictable nature and discrepancies that can exist between their desired behavior and their actual model output. In this paper, we introduce a new tool, InTraVisTo (Inside Transformer Visualisation Tool), designed to enable researchers to investigate and trace the computational process that generates each token in a Transformer-based LLM. InTraVisTo provides a visualization of both the internal state of the Transformer model (by decoding token embeddings at each layer of the model) and the information flow between the various components across the different layers of the model (using a Sankey diagram). With InTraVisTo, we aim to help researchers and practitioners better understand the computations being performed within the Transformer model and thus to shed some light on internal patterns and reasoning processes employed by LLMs.</li>
<li><strong>摘要：</strong>在过去的几年中，大型语言模型（LLM）的推理能力（LLMS）的大小和复杂性也大大增加。尽管如此，由于其所需行为与实际模型输出之间可能存在的不可预测的性质和差异，因此在生产中使用LLM仍然具有挑战性。在本文中，我们引入了一种新工具Interavisto（内部变压器可视化工具），旨在使研究人员能够调查和跟踪在基于变压器的LLM中生成每个令牌的计算过程。 Interavisto提供了变压器模型的内部状态（通过在模型的每一层上解码令牌嵌入）以及在模型不同层（使用Sankey图）之间的信息流。借助Interavisto，我们旨在帮助研究人员和从业人员更好地了解变压器模型中执行的计算，从而阐明LLMS所采用的内部模式和推理过程。</li>
</ul>

<h3>Title: Using LLMs to identify features of personal and professional skills in an open-response situational judgment test</h3>
<ul>
<li><strong>Authors: </strong>Cole Walsh, Rodica Ivan, Muhammad Zafar Iqbal, Colleen Robb</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13881">https://arxiv.org/abs/2507.13881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13881">https://arxiv.org/pdf/2507.13881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13881]] Using LLMs to identify features of personal and professional skills in an open-response situational judgment test(https://arxiv.org/abs/2507.13881)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Academic programs are increasingly recognizing the importance of personal and professional skills and their critical role alongside technical expertise in preparing students for future success in diverse career paths. With this growing demand comes the need for scalable systems to measure, evaluate, and develop these skills. Situational Judgment Tests (SJTs) offer one potential avenue for measuring these skills in a standardized and reliable way, but open-response SJTs have traditionally relied on trained human raters for evaluation, presenting operational challenges to delivering SJTs at scale. Past attempts at developing NLP-based scoring systems for SJTs have fallen short due to issues with construct validity of these systems. In this article, we explore a novel approach to extracting construct-relevant features from SJT responses using large language models (LLMs). We use the Casper SJT to demonstrate the efficacy of this approach. This study sets the foundation for future developments in automated scoring for personal and professional skills.</li>
<li><strong>摘要：</strong>学术课程越来越多地认识到个人和专业技能的重要性及其在技术专长方面的重要性，在为学生准备多种职业道路上的成功方面做好了准备。随着需求不断增长，需要可扩展的系统来衡量，评估和发展这些技能。情境判断测试（SJTS）提供了一种潜在的途径，以标准化和可靠的方式衡量这些技能，但是传统上，开放式响应SJT依靠训练有素的人类评估者进行评估，从而提出了运营挑战，以使SJT大规模交付SJT。由于这些系统的构造有效性问题，过去试图为SJT开发基于NLP的评分系统的尝试不足。在本文中，我们探讨了一种新的方法，可以使用大语言模型（LLM）从SJT响应中提取相关特征。我们使用Casper SJT来证明这种方法的功效。这项研究为个人和专业技能的自动评分进展奠定了基础。</li>
</ul>

<h3>Title: The Levers of Political Persuasion with Conversational AI</h3>
<ul>
<li><strong>Authors: </strong>Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13919">https://arxiv.org/abs/2507.13919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13919">https://arxiv.org/pdf/2507.13919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13919]] The Levers of Political Persuasion with Conversational AI(https://arxiv.org/abs/2507.13919)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.</li>
<li><strong>摘要：</strong>人们普遍担心会话人工智会可能很快对人类信仰产生前所未有的影响。在这里，在三个大规模实验（n = 76,977）中，我们部署了19个LLMS，包括一些明确的后培训以说服，以评估他们对707个政治问题的说服力。然后，我们检查了由466,769个LLM索赔的事实准确性。与流行的关注相反，我们表明，当前和近未来的AI的说服力可能更多地源于培训后和促使方法，这使得有说服力分别提高了多达51％和27％，而不是个性化或增加模型量表。我们进一步表明，这些方法通过利用LLMS快速访问和战略部署信息的独特能力来提高说服力，并且令人惊讶的是，它们提高了AI的说服力，他们也系统地降低了事实准确性。</li>
</ul>

<h3>Title: Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support</h3>
<ul>
<li><strong>Authors: </strong>Jan Trienes, Anastasiia Derzhanskaia, Roland Schwarzkopf, Markus Mühling, Jörg Schlötterer, Christin Seifert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13937">https://arxiv.org/abs/2507.13937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13937">https://arxiv.org/pdf/2507.13937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13937]] Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support(https://arxiv.org/abs/2507.13937)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>We present Marcel, a lightweight and open-source conversational agent designed to support prospective students with admission-related inquiries. The system aims to provide fast and personalized responses, while reducing workload of university staff. We employ retrieval-augmented generation to ground answers in university resources and to provide users with verifiable, contextually relevant information. To improve retrieval quality, we introduce an FAQ retriever that maps user questions to knowledge-base entries, allowing administrators to steer retrieval, and improving over standard dense/hybrid retrieval strategies. The system is engineered for easy deployment in resource-constrained academic settings. We detail the system architecture, provide a technical evaluation of its components, and report insights from a real-world deployment.</li>
<li><strong>摘要：</strong>我们介绍了Marcel，这是一种轻巧和开源的对话代理，旨在为与入学有关的询问提供支持的潜在学生。该系统旨在提供快速和个性化的响应，同时减少大学工作人员的工作量。我们采用检索功能的生成来取得大学资源的答案，并为用户提供可验证的，上下文相关的信息。为了提高检索质量，我们介绍了一个常见问题检索器，将用户问题映射到知识库条目，使管理员可以指导检索并改善标准密集/混合检索策略。该系统经过设计，可轻松在资源受限的学术环境中部署。我们详细介绍系统体系结构，对其组件进行技术评估，并报告现实部署的见解。</li>
</ul>

<h3>Title: Exploiting Primacy Effect To Improve Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bianca Raimondi, Maurizio Gabbrielli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13949">https://arxiv.org/abs/2507.13949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13949">https://arxiv.org/pdf/2507.13949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13949]] Exploiting Primacy Effect To Improve Large Language Models(https://arxiv.org/abs/2507.13949)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become essential in many Natural Language Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to achieve high accuracy. However, like humans, LLMs exhibit biases, particularly positional biases such as primacy and recency effects, which can influence the accuracy of the answers. The primacy effect-where items presented first are more likely to be remembered or selected-plays a key role in Multiple Choice Question Answering (MCQA), where the order of answer options can affect prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We first show that fine-tuning amplifies this bias, probably due to exposure to human-like patterns. Hence, we strategically leverage this effect by reordering response options based on semantic similarity to the query, without requiring knowledge of the correct answer. Our experimental results show that this approach significantly improves performance in MCQA. More generally, our findings underscore the dual nature of biases as both challenges and opportunities, offering insights for bias-aware model design and NLP applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在许多自然语言处理（NLP）任务中已成为必不可少的，利用广泛的预培训和微调来实现高精度。但是，像人类一样，LLM也表现出偏见，尤其是位置偏见，例如首要效应和新近度效应，这会影响答案的准确性。首先提出的项目更有可能被记住或选择播放在多项选择问答（MCQA）中的关键作用，在此效果更可能会影响预测结果。这项研究的重点是微调LLMS中的首要偏置：我们首先表明，微调可以放大这种偏见，这可能是由于暴露于类似人类的模式所致。因此，我们通过基于与查询的语义相似性重新排序响应选项来战略性地利用这种效果，而无需了解正确的答案。我们的实验结果表明，这种方法显着提高了MCQA的性能。更普遍地，我们的发现强调了偏见的双重性质，既是挑战和机遇，也提供了偏见感知模型设计和NLP应用的见解。</li>
</ul>

<h3>Title: Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need</h3>
<ul>
<li><strong>Authors: </strong>Bhishma Dedhia, Yuval Kansal, Niraj K. Jha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.13966">https://arxiv.org/abs/2507.13966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.13966">https://arxiv.org/pdf/2507.13966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.13966]] Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need(https://arxiv.org/abs/2507.13966)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. We present a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. We fine-tune language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, we validate our approach in medicine, where reliable KGs exist. Using a medical KG, we curate 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. We fine-tune the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Our experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, we envision a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents.</li>
<li><strong>摘要：</strong>传统上用于跨域泛化的语言模型最近证明了特定于任务的推理。但是，他们关于一般语料库的自上而下的培训方法不足以获取深层领域专业知识所需的抽象。这可能需要一种自下而上的方法，通过学习将简单的领域概念构成更复杂的方法来获得专业知识。知识图（kg）提供了这种组成结构，其中域的原始词表示为头部融合尾部边缘及其路径编码更高级别的概念。我们提出了一个任务生成管道，该管道直接从kg原语中综合了任务，从而使模型能够获取并构成它们进行推理。我们在最终的KG接地课程上微调语言模型，以展示特定领域的独立性。虽然广泛适用，但我们验证了存在可靠KG的医学方法。我们使用医疗kg策划了24,000个推理任务，并与来自不同医学基础的思维痕迹配对。我们在此课程上微调了QWQ-32B模型，以获得QWQ-MED-3，该QWQ-MED-3迈向医学超级智能。我们还介绍了ICD板凳，这是一个评估套件，以量化15个医疗领域的推理能力。我们的实验表明，QWQ-MED-3在ICD基础类别上显着优于最先进的推理模型。进一步的分析表明，QWQ-MED-3利用获得的原始素来扩大ICD板凳最困难的任务的性能差距。最后，对医疗问题解答基准的评估表明，QWQ-MED-3转移获得了获得的专业知识，以提高基本模型的性能。尽管该行业的人工通用情报方法（AGI）强调了广泛的专业知识，但我们设想了一个未来，其中AGI从有效的特定领域特定于特定领域的独立药物的可组合相互作用中出现。</li>
</ul>

<h3>Title: Efficient Temporal Tokenization for Mobility Prediction with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14017">https://arxiv.org/abs/2507.14017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14017">https://arxiv.org/pdf/2507.14017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14017]] Efficient Temporal Tokenization for Mobility Prediction with Large Language Models(https://arxiv.org/abs/2507.14017)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a framework that leverages large language models (LLMs) as spatio-temporal predictors and trajectory reasoners. RHYTHM partitions trajectories into daily segments encoded as discrete tokens with hierarchical attention, capturing both daily and weekly dependencies while substantially reducing the sequence length. Token representations are enriched with pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability to capture interdependencies without extensive computational overhead. By freezing the LLM backbone, RHYTHM achieves significant computational efficiency. Evaluation on three real-world datasets demonstrates a 2.4% improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in training time compared to state-of-the-art methods.</li>
<li><strong>摘要：</strong>我们介绍节奏（以人类流动性为层次的时间令牌化推理），该框架将大型语言模型（LLMS）作为时空预测指标和轨迹推理器。节奏分区将轨迹轨迹分为每日段，并以分层的关注为离散令牌，捕获每日和每周依赖性，同时大大降低了序列长度。令牌表示通过冷冻LLM富含预计的及时嵌入，从而增强了模型捕获相互依存关系的能力而无需大量的计算开销。通过冻结LLM主链，节奏可实现明显的计算效率。与最新方法相比，对三个现实世界数据集的评估表明，准确性提高了2.4％，周末提高了5.0％，训练时间降低了24.6％。</li>
</ul>

<h3>Title: Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks</h3>
<ul>
<li><strong>Authors: </strong>Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14045">https://arxiv.org/abs/2507.14045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14045">https://arxiv.org/pdf/2507.14045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14045]] Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks(https://arxiv.org/abs/2507.14045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive evaluation of cost-efficient Large Language Models (LLMs) for diverse biomedical tasks spanning both text and image modalities. We evaluated a range of closed-source and open-source LLMs on tasks such as biomedical text classification and generation, question answering, and multimodal image processing. Our experimental findings indicate that there is no single LLM that can consistently outperform others across all tasks. Instead, different LLMs excel in different tasks. While some closed-source LLMs demonstrate strong performance on specific tasks, their open-source counterparts achieve comparable results (sometimes even better), with additional benefits like faster inference and enhanced privacy. Our experimental results offer valuable insights for selecting models that are optimally suited for specific biomedical applications.</li>
<li><strong>摘要：</strong>本文介绍了针对文本和图像方式的各种生物医学任务的经济高效大语模型（LLM）的全面评估。我们在诸如生物医学文本分类和生成，问答和多模式图像处理等任务上评估了一系列封闭源和开源LLM。我们的实验发现表明，没有单个LLM能够在所有任务中始终如一地胜过其他LLM。相反，不同的LLM在不同任务中表现出色。尽管某些封闭源LLM在特定任务上表现出强大的性能，但其开源对应物取得了可比的结果（有时甚至更好），并具有更快的推断和增强的隐私等其他好处。我们的实验结果为选择最适合特定生物医学应用的模型提供了宝贵的见解。</li>
</ul>

<h3>Title: Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog</h3>
<ul>
<li><strong>Authors: </strong>Lautaro Estienne, Gabriel Ben Zenou, Nona Naderi, Jackie Cheung, Pablo Piantanida</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14063">https://arxiv.org/abs/2507.14063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14063">https://arxiv.org/pdf/2507.14063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14063]] Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog(https://arxiv.org/abs/2507.14063)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>As AI systems take on collaborative roles, they must reason about shared goals and beliefs-not just generate fluent language. The Rational Speech Act (RSA) framework offers a principled approach to pragmatic reasoning, but existing extensions face challenges in scaling to multi-turn, collaborative scenarios. In this paper, we introduce Collaborative Rational Speech Act (CRSA), an information-theoretic (IT) extension of RSA that models multi-turn dialog by optimizing a gain function adapted from rate-distortion theory. This gain is an extension of the gain model that is maximized in the original RSA model but takes into account the scenario in which both agents in a conversation have private information and produce utterances conditioned on the dialog. We demonstrate the effectiveness of CRSA on referential games and template-based doctor-patient dialogs in the medical domain. Empirical results show that CRSA yields more consistent, interpretable, and collaborative behavior than existing baselines-paving the way for more pragmatic and socially aware language agents.</li>
<li><strong>摘要：</strong>当AI系统扮演协作角色时，他们必须建议共同的目标和信念，而不是会产生流利的语言。 《理性言语法》（RSA）框架为实用推理提供了一种原则性的方法，但是现有的扩展面临扩展到多转变，协作方案的挑战。在本文中，我们介绍了协作理性语音法（CRSA），这是一种信息理论（IT）扩展RSA，通过优化根据速率延伸理论的增益函数来对多转模式进行建模。这种增益是收益模型的扩展，该模型在原始RSA模型中最大化，但考虑到对话中的两个代理都有私人信息并在对话框上产生发言的情况。我们证明了CRSA对医疗领域中的参考游戏和基于模板的医生对话的有效性。经验结果表明，CRSA比现有的基本线更一致，可解释和协作行为更加一致，可解释和协作行为，为更务实和社会意识的语言推动了道路。</li>
</ul>

<h3>Title: DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits</h3>
<ul>
<li><strong>Authors: </strong>Garapati Keerthana, Manik Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14079">https://arxiv.org/abs/2507.14079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14079">https://arxiv.org/pdf/2507.14079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14079]] DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits(https://arxiv.org/abs/2507.14079)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care. We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes. We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.</li>
<li><strong>摘要：</strong>进度注释是电子健康记录（EHR）中临床上最有意义的文物之一，为患者的不断发展的状况，治疗和护理决定提供了暂时的见解。尽管它们的重要性，但它们在大规模EHR数据集中的代表性严重不足。例如，在广泛使用的重症监护III（MIMIC-III）数据集中，仅$ 8.56 \％的医院就诊包括进度注释，留下了纵向患者叙事的差距。相比之下，数据集包含各种各样的其他注释类型，每个注释类型都捕获了不同的护理方面。我们提出密集（记录了分散证据中不断发展的进度注释），该系统旨在通过模拟医生在起草进度笔记的同时如何参考过去的相遇，旨在与临床文档工作流程保持一致。该系统引入了细粒的音符分类和时间对齐机制，该机制在访问结构化的时间顺序输入中组织异质说明。从本质上讲，密集利用了临床知情的检索策略，从当前和先前的访问中识别时间和语义相关的内容。该检索的证据用于促使大型语言模型（LLM）产生临床连贯且具有暂时性的进度注释。我们评估具有多次访问和完整进度记录文档的精心策划的患者队列的密集。生成的笔记显示出强烈的纵向忠诚度，达到了$ 1.089 $的时间对齐比，超过了原始音符中观察到的连续性。通过恢复跨零散文档的叙事连贯性，我们的系统支持改进的下游任务，例如汇总，预测建模和临床决策支持，为LLM驱动的Note Note Synthesis提供现实世界中医疗设置中的可扩展解决方案。</li>
</ul>

<h3>Title: Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track</h3>
<ul>
<li><strong>Authors: </strong>Brian Ondov, William Xia, Kush Attal, Ishita Unde, Jerry He, Hoa Dang, Ian Soboroff, Dina Demner-Fushman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14096">https://arxiv.org/abs/2507.14096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14096">https://arxiv.org/pdf/2507.14096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14096]] Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track(https://arxiv.org/abs/2507.14096)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems. Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts. Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity. Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.</li>
<li><strong>摘要：</strong>目的：语言模型的最新进展表明，有可能使面向专业的生物医学文学适应普通语言，从而使患者和护理人员可以使用。但是，它们的不可预测性，加上该领域的高潜力，意味着必须进行严格的评估。我们使用这首歌的目标是刺激研究并提供对最有前途的系统的高质量评估。方法：我们在2023年和2024年文本检索会议上举办了生物医学摘要（PLABA）曲目的简单语言改编。任务包括完整的句子级，摘要的重写（任务1）以及识别和替换困难术语（任务2）。为了自动评估任务1，我们开发了一组四倍的专业编写参考文献。这两个任务1和2的提交均已从生物医学专家提供了大量的手动评估。结果：跨越十二个国家 /地区的十二支球队参加了赛道，从多层感知器到大型验证的变压器。在任务1的手动判断中，表现最好的模型与人类的事实准确性和完整性相媲美，但不是简单或简洁。自动，基于参考的指标通常与手动判断没有很好的相关性。在任务2中，系统努力确定困难的术语并分类如何替换它们。但是，在生成替换时，基于LLM的系统在手动判断的准确性，完整性和简单性方面表现出色，尽管并非简洁。结论：Plaba曲目显示了使用大型语言模型适应公众生物医学文献的希望，同时还强调了它们的缺陷以及需要改进自动基准测试工具的需求。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
