<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-03</h1>
<h3>Title: Text Clustering as Classification with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chen Huang, Guoxiu He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00927">https://arxiv.org/abs/2410.00927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00927">https://arxiv.org/pdf/2410.00927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00927]] Text Clustering as Classification with LLMs(https://arxiv.org/abs/2410.00927)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Text clustering remains valuable in real-world applications where manual labeling is cost-prohibitive. It facilitates efficient organization and analysis of information by grouping similar texts based on their representations. However, implementing this approach necessitates fine-tuned embedders for downstream data and sophisticated similarity metrics. To address this issue, this study presents a novel framework for text clustering that effectively leverages the in-context learning capacity of Large Language Models (LLMs). Instead of fine-tuning embedders, we propose to transform the text clustering into a classification task via LLM. First, we prompt LLM to generate potential labels for a given dataset. Second, after integrating similar labels generated by the LLM, we prompt the LLM to assign the most appropriate label to each sample in the dataset. Our framework has been experimentally proven to achieve comparable or superior performance to state-of-the-art clustering methods that employ embeddings, without requiring complex fine-tuning or clustering algorithms. We make our code available to the public for utilization at this https URL.</li>
<li><strong>摘要：</strong>在手动标记成本过高的实际应用中，文本聚类仍然很有价值。它通过根据相似文本的表示形式对其进行分组，从而促进信息的有效组织和分析。但是，实现这种方法需要对下游数据进行微调的嵌入器和复杂的相似性指标。为了解决这个问题，本研究提出了一种新颖的文本聚类框架，该框架有效地利用了大型语言模型 (LLM) 的上下文学习能力。我们建议通过 LLM 将文本聚类转换为分类任务，而不是微调嵌入器。首先，我们提示 LLM 为给定的数据集生成潜在标签。其次，在集成 LLM 生成的类似标签后，我们提示 LLM 为数据集中的每个样本分配最合适的标签。我们的框架已通过实验证明，其性能可与采用嵌入的最先进的聚类方法相媲美或更胜一筹，而无需复杂的微调或聚类算法。我们通过此 https URL 向公众提供我们的代码以供使用。</li>
</ul>

<h3>Title: Creative and Context-Aware Translation of East Asian Idioms with GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Kenan Tang, Peiyang Song, Yao Qin, Xifeng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00988">https://arxiv.org/abs/2410.00988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00988">https://arxiv.org/pdf/2410.00988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00988]] Creative and Context-Aware Translation of East Asian Idioms with GPT-4(https://arxiv.org/abs/2410.00988)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>As a type of figurative language, an East Asian idiom condenses rich cultural background into only a few characters. Translating such idioms is challenging for human translators, who often resort to choosing a context-aware translation from an existing list of candidates. However, compiling a dictionary of candidate translations demands much time and creativity even for expert translators. To alleviate such burden, we evaluate if GPT-4 can help generate high-quality translations. Based on automatic evaluations of faithfulness and creativity, we first identify Pareto-optimal prompting strategies that can outperform translation engines from Google and DeepL. Then, at a low cost, our context-aware translations can achieve far more high-quality translations per idiom than the human baseline. We open-source all code and data to facilitate further research.</li>
<li><strong>摘要：</strong>作为一种比喻性语言，东亚习语将丰富的文化背景浓缩为几个字符。翻译此类习语对人工翻译来说是一项挑战，他们通常会从现有的候选列表中选择一个上下文感知的翻译。然而，即使对于专业翻译来说，编纂一本候选翻译词典也需要大量的时间和创造力。为了减轻这种负担，我们评估了 GPT-4 是否有助于生成高质量的翻译。基于对忠实度和创造力的自动评估，我们首先确定了可以胜过 Google 和 DeepL 翻译引擎的帕累托最优提示策略。然后，以低成本，我们的上下文感知翻译可以为每个习语实现比人类基线高得多的高质量翻译。我们开源所有代码和数据以促进进一步的研究。</li>
</ul>

<h3>Title: "Hiding in Plain Sight": Designing Synthetic Dialog Generation for Uncovering Socially Situated Norms</h3>
<ul>
<li><strong>Authors: </strong>Chengfei Wu, Dan Goldwasser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00998">https://arxiv.org/abs/2410.00998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00998">https://arxiv.org/pdf/2410.00998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00998]] "Hiding in Plain Sight": Designing Synthetic Dialog Generation for Uncovering Socially Situated Norms(https://arxiv.org/abs/2410.00998)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Naturally situated conversations capture the underlying social norms appropriate for the topic of conversation, the relationship between interlocutors and their communicative intent. This paper proposes a framework for controlled generation of dialogues, spanning a wide range of interlocutors attributes (such as age group, profession and personality types), relationship types, conversation topics and conversational trajectories. We use this framework to generate NormHint, a collection of dialogues consistent with these rich settings and analyzed for norm violation leading to conflicts, and potential steps for avoiding these conflicts by adhering to social norms and preferring respectful utterances maintaining the communicative intents of the original utterance. We present the results of human validation and automated analysis of NormHint and show it captures a wide range of conversational topics and scored highly by humans for the naturalness of the conversations based on the prompted context.</li>
<li><strong>摘要：</strong>自然情境对话可以捕捉适合对话主题的底层社会规范、对话者之间的关系及其交流意图。本文提出了一个可控对话生成框架，涵盖对话者的各种属性（如年龄组、职业和性格类型）、关系类型、对话主题和对话轨迹。我们使用这个框架生成了 NormHint，这是一系列与这些丰富设置一致的对话，并分析了导致冲突的规范违规行为，以及通过遵守社会规范和偏好尊重的话语来避免这些冲突的潜在步骤，这些话语保持了原始话语的交流意图。我们展示了 NormHint 的人工验证和自动分析结果，并表明它捕捉到了广泛的对话主题，并且人类根据提示的上下文对对话的自然性给予了很高的评分。</li>
</ul>

<h3>Title: Investigating the Synergistic Effects of Dropout and Residual Connections on Language Model Training</h3>
<ul>
<li><strong>Authors: </strong>Qingyang Li, Weimao Ke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01019">https://arxiv.org/abs/2410.01019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01019">https://arxiv.org/pdf/2410.01019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01019]] Investigating the Synergistic Effects of Dropout and Residual Connections on Language Model Training(https://arxiv.org/abs/2410.01019)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper examines the pivotal role of dropout techniques in mitigating overfitting in language model training. It conducts a comprehensive investigation into the influence of variable dropout rates on both individual layers and residual connections within the context of language modeling. Our study conducts training of a decoder implementation on the classic Tiny Shakespeare data to examine the effects of the adjustments on training efficiency and validation error. Results not only confirm the benefits of dropout for regularization and residuals for convergence, but also reveal their interesting interactions. There exists an important trade-off between the depth of residual connections and the dropout on these connections for optimal deep neural network convergence and generalization.</li>
<li><strong>摘要：</strong>本文探讨了 dropout 技术在减轻语言模型训练中的过拟合方面的关键作用。它全面研究了语言建模背景下可变 dropout 率对各个层和残差连接的影响。我们的研究对经典的 Tiny Shakespeare 数据进行了解码器实现的训练，以检查调整对训练效率和验证误差的影响。结果不仅证实了 dropout 对正则化和残差对收敛的好处，还揭示了它们有趣的相互作用。残差连接的深度和这些连接的 dropout 之间存在重要的权衡，以实现最佳的深度神经网络收敛和泛化。</li>
</ul>

<h3>Title: Draft on the Fly: Adaptive Self-Speculative Decoding using Cosine Similarity</h3>
<ul>
<li><strong>Authors: </strong>Michael R. Metel, Peng Lu, Boxing Chen, Mehdi Rezagholizadeh, Ivan Kobyzev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01028">https://arxiv.org/abs/2410.01028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01028">https://arxiv.org/pdf/2410.01028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01028]] Draft on the Fly: Adaptive Self-Speculative Decoding using Cosine Similarity(https://arxiv.org/abs/2410.01028)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present a simple on the fly method for faster inference of large language models. Unlike other (self-)speculative decoding techniques, our method does not require fine-tuning or black-box optimization to generate a fixed draft model, relying instead on simple rules to generate varying draft models adapted to the input context. We show empirically that our light-weight algorithm is competitive with the current SOTA for self-speculative decoding, while being a truly plug-and-play method.</li>
<li><strong>摘要：</strong>我们提出了一种简单的即时方法，用于更快地推理大型语言模型。与其他（自）推测解码技术不同，我们的方法不需要微调或黑盒优化来生成固定的草稿模型，而是依靠简单的规则来生成适应输入上下文的不同草稿模型。我们通过经验证明，我们的轻量级算法与当前的自推测解码 SOTA 相媲美，同时是一种真正的即插即用方法。</li>
</ul>

<h3>Title: From Facts to Insights: A Study on the Generation and Evaluation of Analytical Reports for Deciphering Earnings Calls</h3>
<ul>
<li><strong>Authors: </strong>Tomas Goldsack, Yang Wang, Chenghua Lin, Chung-Chi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01039">https://arxiv.org/abs/2410.01039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01039">https://arxiv.org/pdf/2410.01039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01039]] From Facts to Insights: A Study on the Generation and Evaluation of Analytical Reports for Deciphering Earnings Calls(https://arxiv.org/abs/2410.01039)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper explores the use of Large Language Models (LLMs) in the generation and evaluation of analytical reports derived from Earnings Calls (ECs). Addressing a current gap in research, we explore the generation of analytical reports with LLMs in a multi-agent framework, designing specialized agents that introduce diverse viewpoints and desirable topics of analysis into the report generation process. Through multiple analyses, we examine the alignment between generated and human-written reports and the impact of both individual and collective agents. Our findings suggest that the introduction of additional agents results in more insightful reports, although reports generated by human experts remain preferred in the majority of cases. Finally, we address the challenging issue of report evaluation, we examine the limitations and strengths of LLMs in assessing the quality of generated reports in different settings, revealing a significant correlation with human experts across multiple dimensions.</li>
<li><strong>摘要：</strong>本文探讨了大型语言模型 (LLM) 在财报电话会议 (EC) 分析报告生成和评估中的应用。为了解决当前研究的空白，我们探索了在多智能体框架中使用 LLM 生成分析报告，设计了专门的智能体，将不同的观点和理想的分析主题引入报告生成过程。通过多项分析，我们研究了生成的报告和人工编写的报告之间的一致性以及个人和集体智能体的影响。我们的研究结果表明，引入额外的智能体会产生更有见地的报告，尽管在大多数情况下，人们仍然更喜欢由人类专家生成的报告。最后，我们解决了报告评估这一具有挑战性的问题，我们研究了 LLM 在评估不同环境下生成的报告质量方面的局限性和优势，揭示了 LLM 在多个维度上与人类专家的显著相关性。</li>
</ul>

<h3>Title: From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems</h3>
<ul>
<li><strong>Authors: </strong>Ali Mohammadjafari, Anthony S. Maida, Raju Gottumukkala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01066">https://arxiv.org/abs/2410.01066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01066">https://arxiv.org/pdf/2410.01066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01066]] From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems(https://arxiv.org/abs/2410.01066)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Since the onset of LLMs, translating natural language queries to structured SQL commands is assuming increasing. Unlike the previous reviews, this survey provides a comprehensive study of the evolution of LLM-based text-to-SQL systems, from early rule-based models to advanced LLM approaches, and how LLMs impacted this field. We discuss benchmarks, evaluation methods and evaluation metrics. Also, we uniquely study the role of integration of knowledge graphs for better contextual accuracy and schema linking in these systems. The current techniques fall into two categories: in-context learning of corpus and fine-tuning, which then leads to approaches such as zero-shot, few-shot learning from the end, and data augmentation. Finally, we highlight key challenges such as computational efficiency, model robustness, and data privacy with perspectives toward their development and improvements in potential areas for future of LLM-based text-to-SQL system.</li>
<li><strong>摘要：</strong>自 LLM 诞生以来，将自然语言查询转换为结构化 SQL 命令的需求日益增加。与之前的评论不同，本调查全面研究了基于 LLM 的文本到 SQL 系统的演变，从早期的基于规则的模型到高级 LLM 方法，以及 LLM 如何影响这一领域。我们讨论了基准、评估方法和评估指标。此外，我们以独特的方式研究了知识图谱集成在这些系统中的作用，以提高上下文准确性和模式链接。当前的技术分为两类：语料库的上下文学习和微调，这又导致了零样本、从末端进行少样本学习和数据增强等方法。最后，我们重点介绍了计算效率、模型鲁棒性和数据隐私等关键挑战，并展望了它们在未来基于 LLM 的文本到 SQL 系统的潜在领域的发展和改进。</li>
</ul>

<h3>Title: Concept Space Alignment in Multilingual LLMs</h3>
<ul>
<li><strong>Authors: </strong>Qiwei Peng, Anders Søgaard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01079">https://arxiv.org/abs/2410.01079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01079">https://arxiv.org/pdf/2410.01079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01079]] Concept Space Alignment in Multilingual LLMs(https://arxiv.org/abs/2410.01079)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) seem to generalize somewhat across languages. We hypothesize this is a result of implicit vector space alignment. Evaluating such alignment, we see that larger models exhibit very high-quality linear alignments between corresponding concepts in different languages. Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts. For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear -- an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods.</li>
<li><strong>摘要：</strong>多语言大型语言模型 (LLM) 似乎在各种语言之间具有一定的泛化能力。我们假设这是隐式向量空间对齐的结果。评估这种对齐后，我们发现较大的模型在不同语言的相应概念之间表现出非常高质量的线性对齐。我们的实验表明，多语言 LLM 存在两个常见的弱点：泛化最适合具有相似类型的语言和抽象概念。对于某些模型（例如 Llama-2 系列模型），基于提示的嵌入比单词嵌入对齐效果更好，但投影的线性性较差 —— 这一观察结果适用于几乎所有模型系列，表明一些隐式学习的对齐方式在某种程度上被基于提示的方法破坏了。</li>
</ul>

<h3>Title: Approximately Aligned Decoding</h3>
<ul>
<li><strong>Authors: </strong>Daniel Melcer, Sujan Gonugondla, Pramuditha Perera, Haifeng Qian, Wen-Hao Chiang, Yanjun Wang, Nihal Jain, Pranav Garg, Xiaofei Ma, Anoop Deoras</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01103">https://arxiv.org/abs/2410.01103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01103">https://arxiv.org/pdf/2410.01103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01103]] Approximately Aligned Decoding(https://arxiv.org/abs/2410.01103)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>It is common to reject undesired outputs of Large Language Models (LLMs); however, current methods to do so require an excessive amount of computation, or severely distort the distribution of outputs. We present a method to balance the distortion of the output distribution with computational efficiency, allowing for the generation of long sequences of text with difficult-to-satisfy constraints, with less amplification of low probability outputs compared to existing methods. We show through a series of experiments that the task-specific performance of our method is comparable to methods that do not distort the output distribution, while being much more computationally efficient.</li>
<li><strong>摘要：</strong>拒绝大型语言模型 (LLM) 的不良输出是很常见的；然而，目前这样做的方法需要大量的计算，或者严重扭曲输出的分布。我们提出了一种方法来平衡输出分布的扭曲与计算效率，允许生成具有难以满足的约束的长文本序列，与现有方法相比，低概率输出的放大更少。我们通过一系列实验表明，我们的方法的任务特定性能与不扭曲输出分布的方法相当，同时计算效率更高。</li>
</ul>

<h3>Title: Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Doohee You, Karim Lasri, Samuel Fraiberger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01141">https://arxiv.org/abs/2410.01141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01141">https://arxiv.org/pdf/2410.01141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01141]] Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs(https://arxiv.org/abs/2410.01141)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.</li>
<li><strong>摘要：</strong>本研究调查了针对经济研究论文标题的大型 NLP 数据集的有效重复数据删除技术。我们探索了各种配对方法以及已建立的距离度量（Levenshtein 距离、余弦相似度）和用于语义评估的 sBERT 模型。我们的研究结果表明，根据在不同方法中观察到的语义相似性，重复的发生率可能很低。为了进行更具结论性的评估，我们使用了人工注释的地面实况集进行了进一步探索。结果支持了基于 NLP、LLM 的距离度量的发现。</li>
</ul>

<h3>Title: GADFA: Generator-Assisted Decision-Focused Approach for Opinion Expressing Timing Identification</h3>
<ul>
<li><strong>Authors: </strong>Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01169">https://arxiv.org/abs/2410.01169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01169">https://arxiv.org/pdf/2410.01169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01169]] GADFA: Generator-Assisted Decision-Focused Approach for Opinion Expressing Timing Identification(https://arxiv.org/abs/2410.01169)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>The advancement of text generation models has granted us the capability to produce coherent and convincing text on demand. Yet, in real-life circumstances, individuals do not continuously generate text or voice their opinions. For instance, consumers pen product reviews after weighing the merits and demerits of a product, and professional analysts issue reports following significant news releases. In essence, opinion expression is typically prompted by particular reasons or signals. Despite long-standing developments in opinion mining, the appropriate timing for expressing an opinion remains largely unexplored. To address this deficit, our study introduces an innovative task - the identification of news-triggered opinion expressing timing. We ground this task in the actions of professional stock analysts and develop a novel dataset for investigation. Our approach is decision-focused, leveraging text generation models to steer the classification model, thus enhancing overall performance. Our experimental findings demonstrate that the text generated by our model contributes fresh insights from various angles, effectively aiding in identifying the optimal timing for opinion expression.</li>
<li><strong>摘要：</strong>文本生成模型的进步使我们能够根据需要生成连贯且令人信服的文本。然而，在现实生活中，个人并不会持续生成文本或表达自己的意见。例如，消费者在权衡产品的优缺点后撰写产品评论，专业分析师在重大新闻发布后发布报告。本质上，意见表达通常由特定原因或信号引发。尽管意见挖掘长期以来一直在发展，但表达意见的适当时机仍未得到充分探索。为了解决这一缺陷，我们的研究引入了一项创新任务——识别新闻引发的意见表达时机。我们将这项任务建立在专业股票分析师的行为上，并开发了一个用于调查的新数据集。我们的方法以决策为中心，利用文本生成模型来引导分类模型，从而提高整体性能。我们的实验结果表明，我们的模型生成的文本从各个角度提供了新的见解，有效地帮助确定了表达意见的最佳时机。</li>
</ul>

<h3>Title: BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Bryan Li, Samar Haider, Fiona Luo, Adwait Agashe, Chris Callison-Burch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01171">https://arxiv.org/abs/2410.01171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01171">https://arxiv.org/pdf/2410.01171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01171]] BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation(https://arxiv.org/abs/2410.01171)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models excel at creative generation but continue to struggle with the issues of hallucination and bias. While retrieval-augmented generation (RAG) provides a framework for grounding LLMs' responses in accurate and up-to-date information, it still raises the question of bias: which sources should be selected for inclusion in the context? And how should their importance be weighted? In this paper, we study the challenge of cross-lingual RAG and present a dataset to investigate the robustness of existing systems at answering queries about geopolitical disputes, which exist at the intersection of linguistic, cultural, and political boundaries. Our dataset is sourced from Wikipedia pages containing information relevant to the given queries and we investigate the impact of including additional context, as well as the composition of this context in terms of language and source, on an LLM's response. Our results show that existing RAG systems continue to be challenged by cross-lingual use cases and suffer from a lack of consistency when they are provided with competing information in multiple languages. We present case studies to illustrate these issues and outline steps for future research to address these challenges. We make our dataset and code publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型擅长创造性生成，但仍在努力解决幻觉和偏见问题。虽然检索增强生成 (RAG) 提供了一个框架，使 LLM 的响应基于准确和最新的信息，但它仍然提出了偏见问题：应该选择哪些来源纳入上下文？它们的重要性应该如何加权？在本文中，我们研究了跨语言 RAG 的挑战，并提供了一个数据集来调查现有系统在回答有关地缘政治争端的查询时的稳健性，这些争端存在于语言、文化和政治边界的交叉点。我们的数据集来自包含与给定查询相关的信息的维基百科页面，我们研究了包括额外上下文以及该上下文在语言和来源方面的组成对 LLM 响应的影响。我们的结果表明，现有的 RAG 系统继续受到跨语言用例的挑战，并且在为它们提供多种语言的相互竞争的信息时缺乏一致性。我们提供案例研究来说明这些问题，并概述未来研究应对这些挑战的步骤。我们在此 https URL 上公开提供我们的数据集和代码。</li>
</ul>

<h3>Title: Towards Inference-time Category-wise Safety Steering for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amrita Bhattacharjee, Shaona Ghosh, Traian Rebedea, Christopher Parisien</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01174">https://arxiv.org/abs/2410.01174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01174">https://arxiv.org/pdf/2410.01174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01174]] Towards Inference-time Category-wise Safety Steering for Large Language Models(https://arxiv.org/abs/2410.01174)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have seen unprecedented advancements in capabilities and applications across a variety of use-cases, safety alignment of these models is still an area of active research. The fragile nature of LLMs, even models that have undergone extensive alignment and safety training regimes, warrants additional safety steering steps via training-free, inference-time methods. While recent work in the area of mechanistic interpretability has investigated how activations in latent representation spaces may encode concepts, and thereafter performed representation engineering to induce such concepts in LLM outputs, the applicability of such for safety is relatively under-explored. Unlike recent inference-time safety steering works, in this paper we explore safety steering of LLM outputs using: (i) category-specific steering vectors, thereby enabling fine-grained control over the steering, and (ii) sophisticated methods for extracting informative steering vectors for more effective safety steering while retaining quality of the generated text. We demonstrate our exploration on multiple LLMs and datasets, and showcase the effectiveness of the proposed steering method, along with a discussion on the implications and best practices.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在各种用例中的能力和应用都取得了前所未有的进步，但这些模型的安全调整仍然是一个活跃的研究领域。LLM 的脆弱性，即使是经过广泛调整和安全训练制度的模型，也需要通过无需训练的推理时间方法进行额外的安全转向步骤。虽然机械可解释性领域的最新研究已经研究了潜在表示空间中的激活如何编码概念，然后进行表示工程以在 LLM 输出中诱导此类概念，但这种方法对安全性的适用性相对较少。与最近的推理时间安全转向工作不同，在本文中，我们探索了 LLM 输出的安全转向，使用：(i) 类别特定的转向向量，从而实现对转向的细粒度控制，以及 (ii) 用于提取信息转向向量的复杂方法，以实现更有效的安全转向，同时保持生成文本的质量。我们展示了我们在多个 LLM 和数据集上的探索，并展示了所提出的转向方法的有效性，并讨论了其含义和最佳实践。</li>
</ul>

<h3>Title: Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chengyuan Liu, Shihang Wang, Lizhi Qing, Kun Kuang, Yangyang Kang, Changlong Sun, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01188">https://arxiv.org/abs/2410.01188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01188">https://arxiv.org/pdf/2410.01188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01188]] Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs(https://arxiv.org/abs/2410.01188)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) demonstrate impressive generation abilities, they frequently struggle when it comes to specialized domains due to their limited domain-specific knowledge. Studies on domain-specific LLMs resort to expanding the vocabulary before fine-tuning on domain-specific corpus, aiming to decrease the sequence length and enhance efficiency during decoding, without thoroughly investigating the results of vocabulary expansion to LLMs over different domains. Our pilot study reveals that expansion with only a subset of the entire vocabulary may lead to superior performance. Guided by the discovery, this paper explores how to identify a vocabulary subset to achieve the optimal results. We introduce VEGAD, an adaptive method that automatically identifies valuable words from a given domain vocabulary. Our method has been validated through experiments on three Chinese datasets, demonstrating its effectiveness. Additionally, we have undertaken comprehensive analyses of the method. The selection of a optimal subset for expansion has shown to enhance performance on both domain-specific tasks and general tasks, showcasing the potential of VEGAD.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 表现出令人印象深刻的生成能力，但由于其领域特定知识有限，它们在涉及专业领域时经常会遇到困难。领域特定 LLM 的研究依赖于在领域特定语料库上进行微调之前扩展词汇表，旨在减少序列长度并提高解码效率，而没有彻底研究不同领域 LLM 的词汇扩展结果。我们的初步研究表明，仅使用整个词汇表的子集进行扩展可能会带来更好的性能。在此发现的指导下，本文探讨了如何识别词汇子集以获得最佳结果。我们介绍了 VEGAD，这是一种自适应方法，可自动从给定领域词汇表中识别有价值的单词。我们的方法已通过三个中文数据集的实验得到验证，证明了其有效性。此外，我们对该方法进行了全面分析。选择最佳扩展子集已被证明可以提高领域特定任务和一般任务的性能，展示了 VEGAD 的潜力。</li>
</ul>

<h3>Title: StringLLM: Understanding the String Processing Capability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xilong Wang, Hao Fu, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01208">https://arxiv.org/abs/2410.01208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01208">https://arxiv.org/pdf/2410.01208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01208]] StringLLM: Understanding the String Processing Capability of Large Language Models(https://arxiv.org/abs/2410.01208)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>String processing, which mainly involves the analysis and manipulation of strings, is a fundamental component of modern computing. Despite the significant advancements of large language models (LLMs) in various natural language processing (NLP) tasks, their capability in string processing remains underexplored and underdeveloped. To bridge this gap, we present a comprehensive study of LLMs' string processing capability. In particular, we first propose StringLLM, a method to construct datasets for benchmarking string processing capability of LLMs. We use StringLLM to build a series of datasets, referred to as StringBench. It encompasses a wide range of string processing tasks, allowing us to systematically evaluate LLMs' performance in this area. Our evaluations indicate that LLMs struggle with accurately processing strings compared to humans. To uncover the underlying reasons for this limitation, we conduct an in-depth analysis and subsequently propose an effective approach that significantly enhances LLMs' string processing capability via fine-tuning. This work provides a foundation for future research to understand LLMs' string processing capability. Our code and data are available at this https URL.</li>
<li><strong>摘要：</strong>字符串处理是现代计算的基本组成部分，主要涉及字符串的分析和操作。尽管大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中取得了重大进展，但它们在字符串处理方面的能力仍未得到充分探索和开发。为了弥补这一差距，我们对 LLM 的字符串处理能力进行了全面研究。具体来说，我们首先提出了 StringLLM，这是一种构建数据集以对 LLM 的字符串处理能力进行基准测试的方法。我们使用 StringLLM 构建了一系列数据集，称为 StringBench。它涵盖了广泛的字符串处理任务，使我们能够系统地评估 LLM 在该领域的表现。我们的评估表明，与人类相比，LLM 在准确处理字符串方面存在困难。为了揭示这种限制的根本原因，我们进行了深入分析，随后提出了一种有效的方法，通过微调显着增强 LLM 的字符串处理能力。这项工作为未来了解 LLM 字符串处理能力的研究奠定了基础。我们的代码和数据可在此 https URL 上获得。</li>
</ul>

<h3>Title: From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging</h3>
<ul>
<li><strong>Authors: </strong>Yuling Shi, Songsong Wang, Chengcheng Wan, Xiaodong Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01215">https://arxiv.org/abs/2410.01215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01215">https://arxiv.org/pdf/2410.01215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01215]] From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging(https://arxiv.org/abs/2410.01215)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.</li>
<li><strong>摘要：</strong>虽然大型语言模型在代码生成方面取得了重大进展，但生成代码的通过率受到细微错误的影响，通常需要人工干预才能通过测试，尤其是对于复杂问题。现有的基于 LLM 的调试系统将生成的程序视为整体单元，无法解决从低级语法错误到高级算法缺陷等多个粒度级别的错误。在本文中，我们介绍了多粒度调试器 (MGDebugger)，这是一种分层代码调试器，通过隔离、识别和解决不同粒度级别的错误。MGDebugger 将有问题的代码分解为子函数的分层树结构，每个级别代表特定粒度的错误。在调试过程中，它会分析每个子函数并以自下而上的方式迭代解决错误。为了有效地测试每个子函数，我们提出了一个 LLM 模拟的 Python 执行器，它可以跟踪代码执行并跟踪重要的变量状态以准确查明错误。大量实验表明，MGDebugger 的表现优于现有的调试系统，在 HumanEval 中实现了比种子代 18.9% 的准确率提升，在 HumanEvalFix 中实现了 97.6% 的修复成功率。此外，MGDebugger 能够有效修复不同类别和难度级别的错误，证明了其稳健性和有效性。</li>
</ul>

<h3>Title: Automatic deductive coding in discourse analysis: an application of large language models in learning analytics</h3>
<ul>
<li><strong>Authors: </strong>Lishan Zhang, Han Wu, Xiaoshan Huang, Tengfei Duan, Hanxiang Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01240">https://arxiv.org/abs/2410.01240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01240">https://arxiv.org/pdf/2410.01240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01240]] Automatic deductive coding in discourse analysis: an application of large language models in learning analytics(https://arxiv.org/abs/2410.01240)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Deductive coding is a common discourse analysis method widely used by learning science and learning analytics researchers for understanding teaching and learning interactions. It often requires researchers to manually label all discourses to be analyzed according to a theoretically guided coding scheme, which is time-consuming and labor-intensive. The emergence of large language models such as GPT has opened a new avenue for automatic deductive coding to overcome the limitations of traditional deductive coding. To evaluate the usefulness of large language models in automatic deductive coding, we employed three different classification methods driven by different artificial intelligence technologies, including the traditional text classification method with text feature engineering, BERT-like pretrained language model and GPT-like pretrained large language model (LLM). We applied these methods to two different datasets and explored the potential of GPT and prompt engineering in automatic deductive coding. By analyzing and comparing the accuracy and Kappa values of these three classification methods, we found that GPT with prompt engineering outperformed the other two methods on both datasets with limited number of training samples. By providing detailed prompt structures, the reported work demonstrated how large language models can be used in the implementation of automatic deductive coding.</li>
<li><strong>摘要：</strong>演绎编码是一种常见的话语分析方法，被学习科学和学习分析研究人员广泛用于理解教学和学习互动。它通常需要研究人员根据理论指导的编码方案手动标记所有要分析的话语，这既费时又费力。GPT 等大型语言模型的出现为自动演绎编码开辟了一条新途径，以克服传统演绎编码的局限性。为了评估大型语言模型在自动演绎编码中的实用性，我们采用了三种由不同人工智能技术驱动的不同分类方法，包括带有文本特征工程的传统文本分类方法、类似 BERT 的预训练语言模型和类似 GPT 的预训练大型语言模型 (LLM)。我们将这些方法应用于两个不同的数据集，并探索了 GPT 和提示工程在自动演绎编码中的潜力。通过分析和比较这三种分类方法的准确率和 Kappa 值，我们发现带有提示工程的 GPT 在训练样本数量有限的两个数据集上都优于其他两种方法。通过提供详细的提示结构，报告的工作展示了如何使用大型语言模型来实现自动演绎编码。</li>
</ul>

<h3>Title: AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses</h3>
<ul>
<li><strong>Authors: </strong>Xiaotian Lu, Jiyi Li, Koh Takeuchi, Hisashi Kashima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01246">https://arxiv.org/abs/2410.01246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01246">https://arxiv.org/pdf/2410.01246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01246]] AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses(https://arxiv.org/abs/2410.01246)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Question answering (QA) tasks have been extensively studied in the field of natural language processing (NLP). Answers to open-ended questions are highly diverse and difficult to quantify, and cannot be simply evaluated as correct or incorrect, unlike close-ended questions with definitive answers. While large language models (LLMs) have demonstrated strong capabilities across various tasks, they exhibit relatively weaker performance in evaluating answers to open-ended questions. In this study, we propose a method that leverages LLMs and the analytic hierarchy process (AHP) to assess answers to open-ended questions. We utilized LLMs to generate multiple evaluation criteria for a question. Subsequently, answers were subjected to pairwise comparisons under each criterion with LLMs, and scores for each answer were calculated in the AHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and GPT-4. Our results indicate that our approach more closely aligns with human judgment compared to the four baselines. Additionally, we explored the impact of the number of criteria, variations in models, and differences in datasets on the results.</li>
<li><strong>摘要：</strong>问答 (QA) 任务在自然语言处理 (NLP) 领域得到了广泛的研究。开放式问题的答案高度多样化且难以量化，与具有明确答案的封闭式问题不同，不能简单地评估为正确或不正确。虽然大型语言模型 (LLM) 在各种任务中都表现出强大的能力，但它们在评估开放式问题的答案方面表现相对较弱。在本研究中，我们提出了一种利用 LLM 和层次分析法 (AHP) 来评估开放式问题答案的方法。我们利用 LLM 为一个问题生成多个评估标准。随后，使用 LLM 在每个标准下对答案进行成对比较，并在 AHP 中计算每个答案的分数。我们使用 ChatGPT-3.5-turbo 和 GPT-4 在四个数据集上进行了实验。我们的结果表明，与四个基线相比，我们的方法更接近人类判断。此外，我们还探讨了标准数量、模型变化和数据集差异对结果的影响。</li>
</ul>

<h3>Title: Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Unveiling AI's Potential Through Tools, Techniques, and Applications</h3>
<ul>
<li><strong>Authors: </strong>Pohsun Feng, Ziqian Bi, Yizhu Wen, Xuanhe Pan, Benji Peng, Ming Liu, Jiawei Xu, Keyu Chen, Junyu Liu, Caitlyn Heqi Yin, Sen Zhang, Jinlang Wang, Qian Niu, Ming Li, Tianyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01268">https://arxiv.org/abs/2410.01268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01268">https://arxiv.org/pdf/2410.01268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01268]] Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Unveiling AI's Potential Through Tools, Techniques, and Applications(https://arxiv.org/abs/2410.01268)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>This book serves as an introduction to deep learning and machine learning, focusing on their applications in big data analytics. It covers essential concepts, tools like ChatGPT and Claude, hardware recommendations, and practical guidance on setting up development environments using libraries like PyTorch and TensorFlow. Designed for beginners and advanced users alike, it provides step-by-step instructions, hands-on projects, and insights into AI's future, including AutoML and edge computing.</li>
<li><strong>摘要：</strong>本书是深度学习和机器学习的入门书，重点介绍它们在大数据分析中的应用。它涵盖了基本概念、ChatGPT 和 Claude 等工具、硬件建议以及使用 PyTorch 和 TensorFlow 等库设置开发环境的实用指南。它专为初学者和高级用户设计，提供分步说明、实践项目和对 AI 未来的见解，包括 AutoML 和边缘计算。</li>
</ul>

<h3>Title: Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration</h3>
<ul>
<li><strong>Authors: </strong>Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01285">https://arxiv.org/abs/2410.01285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01285">https://arxiv.org/pdf/2410.01285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01285]] Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration(https://arxiv.org/abs/2410.01285)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges. Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques. Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的黑箱性质给结果解释带来了挑战，影响了数据知识产权保护和幻觉追踪等问题。训练数据归因 (TDA) 方法被认为是解决这些挑战的有效解决方案。大多数最新的 TDA 方法都依赖于影响函数，假设模型实现了最小化的经验风险。然而，实现这一标准很困难，并且模型训练期间的拟合误差可能会影响采购准确性。在本文中，我们介绍了一种称为去偏差和去噪归因 (DDA) 的新型 TDA 方法，该方法通过解决拟合误差来增强影响函数。具体而言，去偏差策略旨在通过在微调之前消除基础模型中存在的知识偏差来提高影响函数的性能，而去噪策略旨在通过平滑技术减少训练过程中由于不同程度的拟合而产生的影响分数差异。实验结果表明，我们的方法明显优于现有方法，平均 AUC 达到 91.64%。此外，DDA 在各种来源和不同规模的模型（如LLaMA2、QWEN2 和 Mistral）中表现出很强的通用性和可扩展性。</li>
</ul>

<h3>Title: Mitigating Copy Bias in In-Context Learning through Neuron Pruning</h3>
<ul>
<li><strong>Authors: </strong>Ameen Ali, Lior Wolf, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01288">https://arxiv.org/abs/2410.01288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01288">https://arxiv.org/pdf/2410.01288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01288]] Mitigating Copy Bias in In-Context Learning through Neuron Pruning(https://arxiv.org/abs/2410.01288)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive few-shot in-context learning (ICL) abilities. Still, we show that they are sometimes prone to a `copying bias', where they copy answers from provided examples instead of learning the underlying patterns. In this work, we propose a novel and simple method to mitigate such copying bias. First, we create a synthetic task and use the Integrated Gradients method to identify neurons that prioritize copying over generalization. We demonstrate that pruning these neurons consistently improves performance across a diverse set of ICL tasks. We also show that our method is applicable across various LLM architectures, including Transformers and State-Space Models, without requiring modifications. In our analysis, we adopt a task-recognition perspective on ICL and examine task vectors (Hendel et al., 2023) induced by the model. We find that pruning enhances the quality of these vectors, suggesting that the pruned neurons previously hindered effective task recognition.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展示出令人印象深刻的少样本上下文学习 (ICL) 能力。尽管如此，我们表明它们有时容易出现“复制偏差”，即它们从提供的示例中复制答案，而不是学习底层模式。在这项工作中，我们提出了一种新颖而简单的方法来减轻这种复制偏差。首先，我们创建一个合成任务并使用积分梯度方法来识别优先复制而不是泛化的神经元。我们证明修剪这些神经元可以持续提高一系列不同 ICL 任务的性能。我们还表明我们的方法适用于各种 LLM 架构，包括 Transformers 和状态空间模型，而无需修改。在我们的分析中，我们采用 ICL 的任务识别视角并检查由模型诱导的任务向量 (Hendel 等人，2023)。我们发现修剪可以提高这些向量的质量，这表明修剪后的神经元以前阻碍了有效的任务识别。</li>
</ul>

<h3>Title: Endless Jailbreaks with Bijection Learning</h3>
<ul>
<li><strong>Authors: </strong>Brian R.Y. Huang, Maximilian Li, Leonard Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01294">https://arxiv.org/abs/2410.01294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01294">https://arxiv.org/pdf/2410.01294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01294]] Endless Jailbreaks with Bijection Learning(https://arxiv.org/abs/2410.01294)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite extensive safety training, LLMs are vulnerable to adversarial inputs. In this work, we introduce a simple but powerful attack paradigm, bijection learning, that yields a practically endless set of jailbreak prompts. We exploit language models' advanced reasoning capabilities to teach them invertible languages (bijections) in context, pass encoded queries to the model to bypass built-in safety mechanisms, and finally decode responses back into English, yielding helpful replies to harmful requests. Our approach proves effective on a wide range of frontier language models and harm categories. Bijection learning is an automated and universal attack that grows stronger with scale: larger models with more advanced reasoning capabilities are more susceptible to bijection learning jailbreaks despite stronger safety mechanisms.</li>
<li><strong>摘要：</strong>尽管经过了大量的安全训练，LLM 仍然容易受到对抗性输入的攻击。在这项工作中，我们引入了一种简单但强大的攻击范式，即双射学习，它产生了几乎无穷无尽的越狱提示。我们利用语言模型的高级推理能力，在上下文中教它们可逆语言（双射），将编码的查询传递给模型以绕过内置的安全机制，最后将响应解码回英语，从而对有害请求产生有用的答复。我们的方法在广泛的前沿语言模型和危害类别上被证明是有效的。双射学习是一种自动化的通用攻击，随着规模的扩大而变得更加强大：尽管安全机制更强，但具有更高级推理能力的大型模型更容易受到双射学习越狱的影响。</li>
</ul>

<h3>Title: Emotion-Aware Response Generation Using Affect-Enriched Embeddings with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Abdur Rasool, Muhammad Irfan Shahzad, Hafsa Aslam, Vincent Chan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01306">https://arxiv.org/abs/2410.01306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01306">https://arxiv.org/pdf/2410.01306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01306]] Emotion-Aware Response Generation Using Affect-Enriched Embeddings with LLMs(https://arxiv.org/abs/2410.01306)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>There is a need for empathetic and coherent responses in automated chatbot-facilitated psychotherapy sessions. This study addresses the challenge of enhancing the emotional and contextual understanding of large language models (LLMs) in psychiatric applications. We introduce a novel framework that integrates multiple emotion lexicons, including NRC Emotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs such as LLAMA 2, Flan-T5, ChatGPT 3.0, and ChatGPT 4.0. The primary dataset comprises over 2,000 therapy session transcripts from the Counseling and Psychotherapy database, covering discussions on anxiety, depression, trauma, and addiction. We segment the transcripts into smaller chunks, enhancing them with lexical features and computing embeddings using BERT, GPT-3, and RoBERTa to capture semantic and emotional nuances. These embeddings are stored in a FAISS vector database, enabling efficient similarity search and clustering based on cosine similarity. Upon user query, the most relevant segments are retrieved and provided as context to the LLMs, significantly improving the models' ability to generate empathetic and contextually appropriate responses. Experimental evaluations demonstrate that in-corporating emotion lexicons enhances empathy, coherence, informativeness, and fluency scores. Our findings highlight the critical role of emotional embeddings in improving LLM performance for psychotherapy.</li>
<li><strong>摘要：</strong>在由聊天机器人辅助的自动化心理治疗会话中，需要有同理心和连贯性的回应。本研究解决了增强大型语言模型 (LLM) 在精神病应用中的情感和语境理解的挑战。我们引入了一个新框架，该框架将多个情感词典（包括 NRC 情感词典、VADER、WordNet 和 SentiWordNet）与最先进的 LLM（如 LLAMA 2、Flan-T5、ChatGPT 3.0 和 ChatGPT 4.0）集成在一起。主要数据集包括来自咨询和心理治疗数据库的 2,000 多个治疗会话记录，涵盖了关于焦虑、抑郁、创伤和成瘾的讨论。我们将记录分成更小的块，使用词汇特征对其进行增强，并使用 BERT、GPT-3 和 RoBERTa 计算嵌入以捕捉语义和情感细微差别。这些嵌入存储在 FAISS 向量数据库中，可实现基于余弦相似度的高效相似度搜索和聚类。根据用户查询，系统会检索最相关的片段并将其作为 LLM 的上下文提供，从而显著提高模型生成富有同理心且符合语境的响应的能力。实验评估表明，加入情感词典可提高同理心、连贯性、信息量和流畅度得分。我们的研究结果强调了情感嵌入在提高心理治疗 LLM 性能方面的关键作用。</li>
</ul>

<h3>Title: Unveiling Language Skills under Circuits</h3>
<ul>
<li><strong>Authors: </strong>Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01334">https://arxiv.org/abs/2410.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01334">https://arxiv.org/pdf/2410.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01334]] Unveiling Language Skills under Circuits(https://arxiv.org/abs/2410.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The exploration of language skills in language models (LMs) has always been one of the central goals in mechanistic interpretability. However, existing circuit analyses often fall short in representing the full functional scope of these models, primarily due to the exclusion of Feed-Forward layers. Additionally, isolating the effect of a single language skill from a text, which inherently involves multiple entangled skills, poses a significant challenge. To address these gaps, we introduce a novel concept, Memory Circuit, a minimum unit that fully and independently manipulates the memory-reading functionality of a language model, and disentangle the transformer model precisely into a circuit graph which is an ensemble of paths connecting different memory circuits. Based on this disentanglement, we identify salient circuit paths, named as skill paths, responsible for three crucial language skills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning (ICL) Skill, leveraging causal effect estimation through interventions and counterfactuals. Our experiments on various datasets confirm the correspondence between our identified skill paths and language skills, and validate three longstanding hypotheses: 1) Language skills are identifiable through circuit dissection; 2) Simple language skills reside in shallow layers, whereas complex language skills are found in deeper layers; 3) Complex language skills are formed on top of simpler language skills. Our codes are available at: this https URL.</li>
<li><strong>摘要：</strong>在语言模型 (LM) 中探索语言技能一直是机械可解释性的核心目标之一。然而，现有的电路分析往往无法代表这些模型的全部功能范围，主要是因为排除了前馈层。此外，将单一语言技能的影响与文本（本质上涉及多种纠缠技能）隔离开来是一项重大挑战。为了解决这些差距，我们引入了一个新概念，即记忆电路，它是完全独立地操纵语言模型的记忆读取功能的最小单元，并将变压器模型精确地解缠为电路图，该电路图是连接不同记忆电路的路径的集合。基于这种解缠，我们确定了显着的电路路径，称为技能路径，负责三种关键的语言技能，即先前标记技能、归纳技能和上下文学习 (ICL) 技能，通过干预和反事实利用因果效应估计。我们在各种数据集上的实验证实了我们确定的技能路径与语言技能之间的对应关系，并验证了三个长期存在的假设：1）语言技能可以通过电路解剖来识别；2）简单的语言技能存在于浅层，而复杂的语言技能存在于更深的层；3）复杂的语言技能是在更简单的语言技能之上形成的。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lucas Bandarkar, Benjamin Muller, Pritish Yuvraj, Rui Hou, Nayan Singhal, Hongjiang Lv, Bing Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01335">https://arxiv.org/abs/2410.01335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01335">https://arxiv.org/pdf/2410.01335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01335]] Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models(https://arxiv.org/abs/2410.01335)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate "experts" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.</li>
<li><strong>摘要：</strong>模型合并（例如模型调合）是将具有相同架构的不同模型组合在一起而无需进一步训练的做法。在这项工作中，我们提出了一种模型合并方法，该方法解决了非英语语言中目标任务的大型语言模型 (LLM) 微调困难的问题，因为这些任务通常没有特定于任务的数据。我们专注于数学推理，在没有语言数学数据的情况下，通过组合语言和数学能力来促进跨语言迁移。从相同的预训练模型开始，我们对英语数学教学数据和目标语言通用教学数据上的单独“专家”进行微调。然后，我们直接用语言专家的层替换数学专家的顶部和底部转换器层，从而提高目标语言的数学性能。在数学基准 MGSM 上，在数学教学数据稀缺的四种主要语言中，由此产生的合并模型比单个专家和其他合并方法高出 10%。此外，这种层交换简单、廉价且直观，因为它基于对每个专家微调过程中最重要的参数变化的解释性分析。以这种方式成功重新组合 LLM 以进行跨语言迁移的能力为未来提供了可能性，即结合模型专业知识、创建模块化解决方案以及事后跨语言迁移推理能力。</li>
</ul>

<h3>Title: PCQPR: Proactive Conversational Question Planning with Reflection</h3>
<ul>
<li><strong>Authors: </strong>Shasha Guo, Lizi Liao, Jing Zhang, Cuiping Li, Hong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01363">https://arxiv.org/abs/2410.01363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01363">https://arxiv.org/pdf/2410.01363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01363]] PCQPR: Proactive Conversational Question Planning with Reflection(https://arxiv.org/abs/2410.01363)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Conversational Question Generation (CQG) enhances the interactivity of conversational question-answering systems in fields such as education, customer service, and entertainment. However, traditional CQG, focusing primarily on the immediate context, lacks the conversational foresight necessary to guide conversations toward specified conclusions. This limitation significantly restricts their ability to achieve conclusion-oriented conversational outcomes. In this work, we redefine the CQG task as Conclusion-driven Conversational Question Generation (CCQG) by focusing on proactivity, not merely reacting to the unfolding conversation but actively steering it towards a conclusion-oriented question-answer pair. To address this, we propose a novel approach, called Proactive Conversational Question Planning with self-Refining (PCQPR). Concretely, by integrating a planning algorithm inspired by Monte Carlo Tree Search (MCTS) with the analytical capabilities of large language models (LLMs), PCQPR predicts future conversation turns and continuously refines its questioning strategies. This iterative self-refining mechanism ensures the generation of contextually relevant questions strategically devised to reach a specified outcome. Our extensive evaluations demonstrate that PCQPR significantly surpasses existing CQG methods, marking a paradigm shift towards conclusion-oriented conversational question-answering systems.</li>
<li><strong>摘要：</strong>对话问题生成 (CQG) 增强了教育、客户服务和娱乐等领域的对话问答系统的交互性。然而，传统的 CQG 主要关注即时背景，缺乏引导对话走向特定结论所必需的对话预见性。这一限制严重限制了它们实现以结论为导向的对话结果的能力。在这项工作中，我们将 CQG 任务重新定义为以结论为导向的对话问题生成 (CCQG)，重点是主动性，不仅仅是对展开的对话做出反应，而是积极地将其引导至以结论为导向的问答对。为了解决这个问题，我们提出了一种新方法，称为具有自我完善的主动对话问题规划 (PCQPR)。具体来说，通过将受蒙特卡洛树搜索 (MCTS) 启发的规划算法与大型语言模型 (LLM) 的分析能力相结合，PCQPR 可以预测未来的对话转折并不断完善其提问策略。这种迭代式自我改进机制确保生成具有语境相关性的问题，这些问题经过精心设计，旨在达到特定结果。我们广泛的评估表明，PCQPR 明显优于现有的 CQG 方法，标志着向结论导向型对话式问答系统的范式转变。</li>
</ul>

<h3>Title: Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Jiyeon Kim, Hyunji Lee, Hyowon Cho, Joel Jang, Hyeonbin Hwang, Seungpil Won, Youbin Ahn, Dohaeng Lee, Minjoon Seo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01380">https://arxiv.org/abs/2410.01380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01380">https://arxiv.org/pdf/2410.01380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01380]] Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition(https://arxiv.org/abs/2410.01380)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention.</li>
<li><strong>摘要：</strong>在这项工作中，我们研究了模型在预训练过程中如何广泛整合其参数知识的趋势，以及这种行为如何影响整体性能，特别是在知识获取和遗忘方面。我们引入了知识熵的概念，它量化了模型所涉及的记忆源的范围；高知识熵表示模型利用了广泛的记忆源，而低知识熵则表示更确定地依赖特定来源。我们的分析表明，随着预训练的推进，知识熵持续下降。我们还发现，这种下降与模型获取和保留知识的能力下降密切相关，这让我们得出结论，知识熵的减少（活跃记忆源的数量减少）会削弱模型的知识获取和保留能力。我们通过证明增加非活跃记忆源的活跃度可以增强模型的知识获取和保留能力，找到了进一步的支持这一观点。</li>
</ul>

<h3>Title: CrowdCounter: A benchmark type-specific multi-target counterspeech dataset</h3>
<ul>
<li><strong>Authors: </strong>Punyajoy Saha, Abhilash Datta, Abhik Jana, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01400">https://arxiv.org/abs/2410.01400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01400">https://arxiv.org/pdf/2410.01400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01400]] CrowdCounter: A benchmark type-specific multi-target counterspeech dataset(https://arxiv.org/abs/2410.01400)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Counterspeech presents a viable alternative to banning or suspending users for hate speech while upholding freedom of expression. However, writing effective counterspeech is challenging for moderators/users. Hence, developing suggestion tools for writing counterspeech is the need of the hour. One critical challenge in developing such a tool is the lack of quality and diversity of the responses in the existing datasets. Hence, we introduce a new dataset - CrowdCounter containing 3,425 hate speech-counterspeech pairs spanning six different counterspeech types (empathy, humor, questioning, warning, shaming, contradiction), which is the first of its kind. The design of our annotation platform itself encourages annotators to write type-specific, non-redundant and high-quality counterspeech. We evaluate two frameworks for generating counterspeech responses - vanilla and type-controlled prompts - across four large language models. In terms of metrics, we evaluate the responses using relevance, diversity and quality. We observe that Flan-T5 is the best model in the vanilla framework across different models. Type-specific prompts enhance the relevance of the responses, although they might reduce the language quality. DialoGPT proves to be the best at following the instructions and generating the type-specific counterspeech accurately.</li>
<li><strong>摘要：</strong>反驳言论提供了一种可行的替代方案，可以禁止或暂停发表仇恨言论的用户，同时维护言论自由。但是，撰写有效的反驳言论对版主/用户来说具有挑战性。因此，开发用于撰写反驳言论的建议工具是当务之急。开发这种工具的一个关键挑战是现有数据集中的响应缺乏质量和多样性。因此，我们引入了一个新数据集 - CrowdCounter，其中包含 3,425 个仇恨言论-反驳言论对，涵盖六种不同的反驳言论类型（同理心、幽默、质疑、警告、羞辱、矛盾），这是同类数据集中的第一个。我们注释平台本身的设计鼓励注释者撰写类型特定、非冗余和高质量的反驳言论。我们在四个大型语言模型中评估了两个用于生成反驳言论响应的框架 - 原始和类型控制提示。在指标方面，我们使用相关性、多样性和质量来评估响应。我们观察到，在不同的模型中，Flan-T5 是 vanilla 框架中最好的模型。类型特定提示增强了响应的相关性，尽管它们可能会降低语言质量。事实证明，DialoGPT 在遵循指令和准确生成类型特定反话方面表现最佳。</li>
</ul>

<h3>Title: Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Kehai Chen, Xuefeng Bai, zhao kang, Quanjiang Guo, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01401">https://arxiv.org/abs/2410.01401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01401">https://arxiv.org/pdf/2410.01401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01401]] Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering(https://arxiv.org/abs/2410.01401)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge graph question answering (KGQA) involves answering natural language questions by leveraging structured information stored in a knowledge graph. Typically, KGQA initially retrieve a targeted subgraph from a large-scale knowledge graph, which serves as the basis for reasoning models to address queries. However, the retrieved subgraph inevitably brings distraction information for knowledge utilization, impeding the model's ability to perform accurate reasoning. To address this issue, we propose a Question-guided Knowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the input question, thereby focusing specifically on pertinent factual knowledge. Moreover, we introduce Knowformer, a parameter-efficient method for injecting the re-scored knowledge graph into large language models to enhance their ability to perform factual reasoning. Extensive experiments on multiple KGQA benchmarks demonstrate the superiority of our method over existing systems.</li>
<li><strong>摘要：</strong>知识图谱问答 (KGQA) 涉及利用存储在知识图谱中的结构化信息来回答自然语言问题。通常，KGQA 首先从大规模知识图谱中检索目标子图，作为推理模型解决查询的基础。然而，检索到的子图不可避免地会带来干扰知识利用的信息，妨碍模型执行准确推理的能力。为了解决这个问题，我们提出了一种问题引导的知识图谱重新评分方法 (Q-KGR) 来消除输入问题的噪声路径，从而专注于相关的事实知识。此外，我们引入了 Knowformer，这是一种参数高效的方法，用于将重新评分的知识图谱注入大型语言模型，以增强其执行事实推理的能力。在多个 KGQA 基准上进行的大量实验证明了我们的方法优于现有系统。</li>
</ul>

<h3>Title: Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01428">https://arxiv.org/abs/2410.01428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01428">https://arxiv.org/pdf/2410.01428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01428]] Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks(https://arxiv.org/abs/2410.01428)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>State-of-the-art large language models (LLMs) exhibit impressive problem-solving capabilities but may struggle with complex reasoning and factual correctness. Existing methods harness the strengths of chain-of-thought and retrieval-augmented generation (RAG) to decompose a complex problem into simpler steps and apply retrieval to improve factual correctness. These methods work well on straightforward reasoning tasks but often falter on challenging tasks such as competitive programming and mathematics, due to frequent reasoning errors and irrelevant knowledge retrieval. To address this, we introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a novel framework that leverages fine-tuned critic models to guide both reasoning and retrieval processes through planning. CR-Planner solves a problem by iteratively selecting and executing sub-goals. Initially, it identifies the most promising sub-goal from reasoning, query generation, and retrieval, guided by rewards given by a critic model named sub-goal critic. It then executes this sub-goal through sampling and selecting the optimal output based on evaluations from another critic model named execution critic. This iterative process, informed by retrieved information and critic models, enables CR-Planner to effectively navigate the solution space towards the final answer. We employ Monte Carlo Tree Search to collect the data for training the critic models, allowing for a systematic exploration of action sequences and their long-term impacts. We validate CR-Planner on challenging domain-knowledge-intensive and reasoning-heavy tasks, including competitive programming, theorem-driven math reasoning, and complex domain retrieval problems. Our experiments demonstrate that CR-Planner significantly outperforms baselines, highlighting its effectiveness in addressing challenging problems by improving both reasoning and retrieval.</li>
<li><strong>摘要：</strong>最先进的大型语言模型 (LLM) 表现出令人印象深刻的解决问题能力，但在复杂推理和事实正确性方面可能存在困难。现有方法利用思路链和检索增强生成 (RAG) 的优势将复杂问题分解为更简单的步骤，并应用检索来提高事实正确性。这些方法在简单的推理任务上效果很好，但由于频繁的推理错误和不相关的知识检索，在竞技编程和数学等具有挑战性的任务上往往会失败。为了解决这个问题，我们引入了带有检索增强的评论家引导规划 CR-Planner，这是一个新颖的框架，它利用微调的评论家模型通过规划来指导推理和检索过程。CR-Planner 通过迭代选择和执行子目标来解决问题。首先，它从推理、查询生成和检索中识别出最有希望的子目标，并以名为子目标批评家的批评模型给出的奖励为指导。然后，它通过抽样并根据另一个名为执行批评家的批评模型的评估选择最佳输出来执行此子目标。这个迭代过程由检索到的信息和批评模型提供信息，使 CR-Planner 能够有效地将解决方案空间导航到最终答案。我们使用蒙特卡洛树搜索来收集数据以训练批评模型，从而可以系统地探索动作序列及其长期影响。我们在具有挑战性的领域知识密集型和推理繁重的任务上验证了 CR-Planner，包括竞争性编程、定理驱动的数学推理和复杂的领域检索问题。我们的实验表明，CR-Planner 的表现明显优于基线，突出了其通过改进推理和检索来解决具有挑战性的问题的有效性。</li>
</ul>

<h3>Title: Geometric Signatures of Compositionality Across a Language Model's Lifetime</h3>
<ul>
<li><strong>Authors: </strong>Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Yoshua Bengio, Emily Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01444">https://arxiv.org/abs/2410.01444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01444">https://arxiv.org/pdf/2410.01444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01444]] Geometric Signatures of Compositionality Across a Language Model's Lifetime(https://arxiv.org/abs/2410.01444)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Compositionality, the notion that the meaning of an expression is constructed from the meaning of its parts and syntactic rules, permits the infinite productivity of human language. For the first time, artificial language models (LMs) are able to match human performance in a number of compositional generalization tasks. However, much remains to be understood about the representational mechanisms underlying these abilities. We take a high-level geometric approach to this problem by relating the degree of compositionality in a dataset to the intrinsic dimensionality of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' intrinsic dimensionality, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between linear and nonlinear dimensionality, showing that they respectively encode formal and semantic aspects of linguistic composition.</li>
<li><strong>摘要：</strong>组合性是指表达式的含义由其各部分的含义和句法规则构成，它使人类语言具有无限的生产力。人工语言模型 (LM) 首次能够在许多组合泛化任务中与人类的表现相匹配。然而，关于这些能力背后的表征机制，仍有许多需要了解的地方。我们采用高级几何方法解决这个问题，将数据集中的组合性程度与 LM 下其表征的固有维数联系起来，LM 是特征复杂性的度量。我们发现，不仅数据集的组合性程度反映在表征的固有维数中，而且组合性和几何复杂性之间的关系是由于在训练过程中学习到的语言特征而产生的。最后，我们的分析揭示了线性和非线性维数之间的鲜明对比，表明它们分别编码了语言组成的形式和语义方面。</li>
</ul>

<h3>Title: Agent-Driven Large Language Models for Mandarin Lyric Generation</h3>
<ul>
<li><strong>Authors: </strong>Hong-Hsiang Liu, Yi-Wen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01450">https://arxiv.org/abs/2410.01450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01450">https://arxiv.org/pdf/2410.01450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01450]] Agent-Driven Large Language Models for Mandarin Lyric Generation(https://arxiv.org/abs/2410.01450)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, agent</a></li>
<li><strong>Abstract: </strong>Generative Large Language Models have shown impressive in-context learning abilities, performing well across various tasks with just a prompt. Previous melody-to-lyric research has been limited by scarce high-quality aligned data and unclear standard for creativeness. Most efforts focused on general themes or emotions, which are less valuable given current language model capabilities. In tonal contour languages like Mandarin, pitch contours are influenced by both melody and tone, leading to variations in lyric-melody fit. Our study, validated by the Mpop600 dataset, confirms that lyricists and melody writers consider this fit during their composition process. In this research, we developed a multi-agent system that decomposes the melody-to-lyric task into sub-tasks, with each agent controlling rhyme, syllable count, lyric-melody alignment, and consistency. Listening tests were conducted via a diffusion-based singing voice synthesizer to evaluate the quality of lyrics generated by different agent groups.</li>
<li><strong>摘要：</strong>生成式大型语言模型表现出令人印象深刻的语境学习能力，只需提示即可在各种任务中表现出色。以前的旋律到歌词研究受到稀缺的高质量对齐数据和不明确的创造性标准的限制。大多数努力都集中在一般主题或情感上，鉴于当前的语言模型功能，这些主题或情感的价值较低。在像普通话这样的音调轮廓语言中，音高轮廓受旋律和音调的影响，导致歌词旋律契合度的变化。我们的研究经 Mpop600 数据集验证，证实了作词人和旋律作者在创作过程中会考虑这种契合度。在这项研究中，我们开发了一个多智能体系统，将旋律到歌词任务分解为子任务，每个智能体控制押韵、音节数、歌词旋律对齐和一致性。通过基于扩散的歌声合成器进行听力测试，以评估不同智能体组生成的歌词的质量。</li>
</ul>

<h3>Title: A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts</h3>
<ul>
<li><strong>Authors: </strong>Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01485">https://arxiv.org/abs/2410.01485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01485">https://arxiv.org/pdf/2410.01485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01485]] A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts(https://arxiv.org/abs/2410.01485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Training and serving long-context large language models (LLMs) incurs substantial overhead. To address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural modifications to reduce the overhead of KV cache during serving. This paper argues that integrating length extension with a GPU-friendly KV cache reduction architecture not only reduces training overhead during length extension, but also achieves better long-context performance. This leads to our proposed LongGen, which finetunes a pretrained LLM into an efficient architecture during length extension. LongGen builds on three key insights: (1) Sparse attention patterns, such as window attention (attending to recent tokens), attention sink (initial ones), and blockwise sparse attention (strided token blocks) are well-suited for building efficient long-context models, primarily due to their GPU-friendly memory access patterns, enabling efficiency gains not just theoretically but in practice as well. (2) It is essential for the model to have direct access to all tokens. A hybrid architecture with 1/3 full attention layers and 2/3 efficient ones achieves a balanced trade-off between efficiency and long-context performance. (3) Lightweight training on 5B long-context data is sufficient to extend the hybrid model's context length from 4K to 128K. We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its effectiveness across different scales. During training with 128K-long contexts, LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%, compared to a full-attention baseline. During inference, LongGen reduces KV cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding speedup.</li>
<li><strong>摘要：</strong>训练和提供长上下文大型语言模型 (LLM) 会产生大量开销。为了解决这个问题，通常需要两个关键步骤：预训练的 LLM 通常通过对长上下文数据进行训练来经历一个单独的上下文长度扩展阶段，然后进行架构修改以减少服务期间的 KV 缓存开销。本文认为，将长度扩展与 GPU 友好的 KV 缓存减少架构相结合不仅可以减少长度扩展期间的训练开销，还可以实现更好的长上下文性能。这导致了我们提出的 LongGen，它在长度扩展期间将预训练的 LLM 微调为高效的架构。LongGen 建立在三个关键见解之上：(1) 稀疏注意模式，例如窗口注意（关注最近的标记）、注意接收器（初始标记）和块状稀疏注意（步进标记块）非常适合构建高效的长上下文模型，主要是因为它们具有 GPU 友好的内存访问模式，不仅在理论上而且在实践中也能提高效率。(2) 模型必须能够直接访问所有标记。具有 1/3 完整注意层和 2/3 高效注意层的混合架构在效率和长上下文性能之间实现了平衡。（3）对 5B 长上下文数据进行轻量级训练足以将混合模型的上下文长度从 4K 扩展到 128K。我们在 Llama-2 7B 和 Llama-2 70B 上评估了 LongGen，证明了其在不同规模上的有效性。在使用 128K 长上下文进行训练时，与全注意基线相比，LongGen 实现了 1.55 倍的训练加速并将挂钟时间缩短了 36%。在推理过程中，LongGen 将 KV 缓存内存减少了 62%，实现了 1.67 倍的预填充加速和 1.41 倍的解码加速。</li>
</ul>

<h3>Title: Small Language Models Like Small Vocabularies: Probing the Linguistic Abilities of Grapheme- and Phoneme-Based Baby Llamas</h3>
<ul>
<li><strong>Authors: </strong>Bastian Bunzeck, Daniel Duran, Leonie Schade, Sina Zarrieß</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01487">https://arxiv.org/abs/2410.01487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01487">https://arxiv.org/pdf/2410.01487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01487]] Small Language Models Like Small Vocabularies: Probing the Linguistic Abilities of Grapheme- and Phoneme-Based Baby Llamas(https://arxiv.org/abs/2410.01487)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Current language models use subword-based tokenization algorithms like Byte Pair Encoding, which put their validity as models of linguistic representations into question. In this paper, we explore the potential of tokenization-free, phoneme- and grapheme-based language models. We demonstrate that small models based on the Llama architecture can achieve strong linguistic performance on standard syntactic and novel lexical/phonetic benchmarks when trained with character-level vocabularies. We further show that phoneme-based models without any graphemic biases almost match grapheme-based models in standard tasks and novel evaluations. Our findings suggest a promising direction for creating more linguistically plausible language models that are better suited for computational studies of language acquisition and processing.</li>
<li><strong>摘要：</strong>当前的语言模型使用基于子词的标记化算法，如字节对编码，这使它们作为语言表征模型的有效性受到质疑。在本文中，我们探索了无标记化、基于音素和字素的语言模型的潜力。我们证明，基于 Llama 架构的小型模型在使用字符级词汇进行训练时，可以在标准句法和新词汇/语音基准上实现强大的语言性能。我们进一步表明，没有任何字素偏差的基于音素的模型在标准任务和新评估中几乎与基于字素的模型相匹配。我们的研究结果为创建更符合语言学的语言模型提供了一个有希望的方向，这些模型更适合语言习得和处理的计算研究。</li>
</ul>

<h3>Title: Extending Context Window of Large Language Models from a Distributional Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yingsheng Wu. Yuxuan Gu, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01490">https://arxiv.org/abs/2410.01490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01490">https://arxiv.org/pdf/2410.01490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01490]] Extending Context Window of Large Language Models from a Distributional Perspective(https://arxiv.org/abs/2410.01490)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Scaling the rotary position embedding (RoPE) has become a common method for extending the context window of RoPE-based large language models (LLMs). However, existing scaling methods often rely on empirical approaches and lack a profound understanding of the internal distribution within RoPE, resulting in suboptimal performance in extending the context window length. In this paper, we propose to optimize the context window extending task from the view of rotary angle distribution. Specifically, we first estimate the distribution of the rotary angles within the model and analyze the extent to which length extension perturbs this distribution. Then, we present a novel extension strategy that minimizes the disturbance between rotary angle distributions to maintain consistency with the pre-training phase, enhancing the model's capability to generalize to longer sequences. Experimental results compared to the strong baseline methods demonstrate that our approach reduces by up to 72% of the distributional disturbance when extending LLaMA2's context window to 8k, and reduces by up to 32% when extending to 16k. On the LongBench-E benchmark, our method achieves an average improvement of up to 4.33% over existing state-of-the-art methods. Furthermore, Our method maintains the model's performance on the Hugging Face Open LLM benchmark after context window extension, with only an average performance fluctuation ranging from -0.12 to +0.22.</li>
<li><strong>摘要：</strong>缩放旋转位置嵌入 (RoPE) 已成为扩展基于 RoPE 的大型语言模型 (LLM) 上下文窗口的常用方法。然而，现有的缩放方法通常依赖于经验方法，缺乏对 RoPE 内部分布的深刻理解，导致扩展上下文窗口长度的性能不佳。在本文中，我们提出从旋转角度分布的角度优化上下文窗口扩展任务。具体而言，我们首先估计模型内旋转角度的分布，并分析长度扩展对该分布的扰动程度。然后，我们提出了一种新颖的扩展策略，该策略最小化旋转角度分布之间的干扰以保持与预训练阶段的一致性，从而增强模型推广到更长序列的能力。与强基线方法相比的实验结果表明，当将 LLaMA2 的上下文窗口扩展到 8k 时，我们的方法最多可减少 72% 的分布扰动，而当扩展到 16k 时，最多可减少 32%。在 LongBench-E 基准上，我们的方法比现有的最先进方法平均提高了 4.33%。此外，在上下文窗口扩展后，我们的方法在 Hugging Face Open LLM 基准上保持了模型的性能，平均性能波动范围仅为 -0.12 到 +0.22。</li>
</ul>

<h3>Title: DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Zhang, Ruizhe Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01497">https://arxiv.org/abs/2410.01497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01497">https://arxiv.org/pdf/2410.01497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01497]] DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models(https://arxiv.org/abs/2410.01497)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have achieved robust performance across diverse tasks, but fine-tuning these models for specific domains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a small subset of parameters. However, existing methods for fusing multiple LoRAs lack dynamic fusion based on contextual inputs and often increase inference time due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight Plugin that employs a mini-MLP module with only 5M parameters to dynamically fuse multiple LoRAs at the sentence level using top-p sampling strategies. This approach reduces inference time to less than twice that of single LoRA inference by leveraging parallel computation. Evaluations across 26 tasks-including multiple-choice questions and question answering-demonstrate that DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice datasets and significant improvements in BLEU and ROUGE scores on QA datasets, outperforming different LLMs backbones under composite task settings. DLP-LoRA effectively balances performance and efficiency, making it a practical solution for dynamic multi-task adaptation in LLMs. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展已在各种任务中取得了稳健的表现，但针对特定领域对这些模型进行微调仍然需要大量资源。低秩自适应 (LoRA) 等参数高效微调 (PEFT) 方法通过微调一小部分参数解决了这一挑战。但是，现有的融合多个 LoRA 的方法缺乏基于上下文输入的动态融合，并且通常会由于 token 级操作而增加推理时间。我们提出了 DLP-LoRA，这是一种动态轻量级插件，它采用仅具有 5M 个参数的 mini-MLP 模块，使用 top-p 采样策略在句子级别动态融合多个 LoRA。这种方法通过利用并行计算将推理时间缩短到单个 LoRA 推理的两倍以下。 26 项任务（包括多项选择题和问答）的评估表明，DLP-LoRA 在多项选择题数据集上的平均准确率达到 92.34%，在 QA 数据集上的 BLEU 和 ROUGE 分数显著提高，在复合任务设置下优于不同的 LLM 主干。DLP-LoRA 有效地平衡了性能和效率，使其成为 LLM 中动态多任务适应的实用解决方案。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: PersonaMath: Enhancing Math Reasoning through Persona-Driven Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Jing Luo, Run Luo, Longze Chen, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, Chengming Li, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01504">https://arxiv.org/abs/2410.01504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01504">https://arxiv.org/pdf/2410.01504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01504]] PersonaMath: Enhancing Math Reasoning through Persona-Driven Data Augmentation(https://arxiv.org/abs/2410.01504)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>While closed-source Large Language Models (LLMs) demonstrate strong mathematical problem-solving abilities, open-source models continue to struggle with such tasks. To bridge this gap, we propose a data augmentation approach and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we train the PersonaMath models. Our approach consists of two stages: the first stage is learning from Persona Diversification, and the second stage is learning from Reflection. In the first stage, we regenerate detailed chain-of-thought (CoT) solutions as instructions using a closed-source LLM and introduce a novel persona-driven data augmentation technique to enhance the dataset's quantity and diversity. In the second stage, we incorporate reflection to fully leverage more challenging and valuable questions. Evaluation of our PersonaMath models on MATH and GSM8K reveals that the PersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on MATH and 68.7% on GSM8K, surpassing all baseline methods and achieving state-of-the-art performance. Notably, our dataset contains only 70.3K data points-merely 17.8% of MetaMathQA and 27% of MathInstruct-yet our model outperforms these baselines, demonstrating the high quality and diversity of our dataset, which enables more efficient model training. We open-source the PersonaMathQA dataset, PersonaMath models, and our code for public usage.</li>
<li><strong>摘要：</strong>虽然闭源大型语言模型 (LLM) 表现出强大的数学问题解决能力，但开源模型在完成此类任务时仍然举步维艰。为了弥补这一差距，我们提出了一种数据增强方法，并引入了 PersonaMathQA，这是一个源自 MATH 和 GSM8K 的数据集，我们在该数据集上训练 PersonaMath 模型。我们的方法包括两个阶段：第一阶段是从角色多样化中学习，第二阶段是从反思中学习。在第一阶段，我们使用闭源 LLM 将详细的思路链 (CoT) 解决方案重新生成为指令，并引入一种新颖的角色驱动数据增强技术来增强数据集的数量和多样性。在第二阶段，我们结合反思来充分利用更具挑战性和价值的问题。对我们的 PersonaMath 模型在 MATH 和 GSM8K 上的评估表明，PersonaMath-7B 模型（基于 LLaMA-2-7B）在 MATH 上的准确率达到 24.2%，在 GSM8K 上的准确率达到 68.7%，超越了所有基线方法并实现了最先进的性能。值得注意的是，我们的数据集仅包含 70.3K 个数据点（仅为 MetaMathQA 的 17.8% 和 MathInstruct 的 27%），但我们的模型的表现优于这些基线，证明了我们的数据集的高质量和多样性，从而可以更高效地进行模型训练。我们开源了 PersonaMathQA 数据集、PersonaMath 模型和我们的代码供公众使用。</li>
</ul>

<h3>Title: Disentangling Latent Shifts of In-Context Learning Through Self-Training</h3>
<ul>
<li><strong>Authors: </strong>Josip Jukić, Jan Šnajder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01508">https://arxiv.org/abs/2410.01508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01508">https://arxiv.org/pdf/2410.01508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01508]] Disentangling Latent Shifts of In-Context Learning Through Self-Training(https://arxiv.org/abs/2410.01508)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context, prompt</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has become essential in natural language processing, particularly with autoregressive large language models capable of learning from demonstrations provided within the prompt. However, ICL faces challenges with stability and long contexts, especially as the number of demonstrations grows, leading to poor generalization and inefficient inference. To address these issues, we introduce STICL (Self-Training ICL), an approach that disentangles the latent shifts of demonstrations from the latent shift of the query through self-training. STICL employs a teacher model to generate pseudo-labels and trains a student model using these labels, encoded in an adapter module. The student model exhibits weak-to-strong generalization, progressively refining its predictions over time. Our empirical results show that STICL improves generalization and stability, consistently outperforming traditional ICL methods and other disentangling strategies across both in-domain and out-of-domain data.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 已成为自然语言处理中必不可少的部分，尤其是对于能够从提示中提供的演示中学习的自回归大型语言模型。然而，ICL 面临着稳定性和长上下文方面的挑战，尤其是随着演示数量的增加，导致泛化能力差和推理效率低下。为了解决这些问题，我们引入了 STICL（自训练 ICL），这种方法通过自训练将演示的潜在变化与查询的潜在变化区分开来。STICL 使用教师模型生成伪标签，并使用这些标签训练学生模型，这些标签在适配器模块中编码。学生模型表现出从弱到强的泛化能力，随着时间的推移逐渐完善其预测。我们的实证结果表明，STICL 提高了泛化能力和稳定性，在域内和域外数据中始终优于传统 ICL 方法和其他解耦策略。</li>
</ul>

<h3>Title: InstaTrans: An Instruction-Aware Translation Framework for Non-English Instruction Datasets</h3>
<ul>
<li><strong>Authors: </strong>Yungi Kim, Chanjun Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01512">https://arxiv.org/abs/2410.01512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01512">https://arxiv.org/pdf/2410.01512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01512]] InstaTrans: An Instruction-Aware Translation Framework for Non-English Instruction Datasets(https://arxiv.org/abs/2410.01512)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>It is challenging to generate high-quality instruction datasets for non-English languages due to tail phenomena, which limit performance on less frequently observed data. To mitigate this issue, we propose translating existing high-quality English instruction datasets as a solution, emphasizing the need for complete and instruction-aware translations to maintain the inherent attributes of these datasets. We claim that fine-tuning LLMs with datasets translated in this way can improve their performance in the target language. To this end, we introduces a new translation framework tailored for instruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through extensive experiments, we demonstrate the superiority of InstaTrans over other competitors in terms of completeness and instruction-awareness of translation, highlighting its potential to broaden the accessibility of LLMs across diverse languages at a relatively low cost. Furthermore, we have validated that fine-tuning LLMs with datasets translated by InstaTrans can effectively improve their performance in the target language.</li>
<li><strong>摘要：</strong>由于尾部现象的存在，生成非英语语言的高质量指令数据集非常困难，尾部现象会限制在较少观察到的数据上的性能。为了缓解这一问题，我们建议将现有的高质量英语指令数据集翻译为解决方案，强调需要完整且指令感知的翻译来维护这些数据集的固有属性。我们声称，使用以这种方式翻译的数据集对 LLM 进行微调可以提高其在目标语言中的性能。为此，我们引入了一个专门为指令数据集量身定制的新翻译框架，名为 InstaTrans（INSTruction-Aware TRANSlation）。通过大量实验，我们证明了 InstaTrans 在翻译的完整性和指令感知方面优于其他竞争对手，突出了它以相对较低的成本扩大 LLM 在不同语言中的可访问性的潜力。此外，我们已经验证了使用 InstaTrans 翻译的数据集对 LLM 进行微调可以有效提高其在目标语言中的性能。</li>
</ul>

<h3>Title: InfiniPot: Infinite Context Processing on Memory-Constrained LLMs</h3>
<ul>
<li><strong>Authors: </strong>Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01518">https://arxiv.org/abs/2410.01518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01518">https://arxiv.org/pdf/2410.01518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01518]] InfiniPot: Infinite Context Processing on Memory-Constrained LLMs(https://arxiv.org/abs/2410.01518)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Handling long input contexts remains a significant challenge for Large Language Models (LLMs), particularly in resource-constrained environments such as mobile devices. Our work aims to address this limitation by introducing InfiniPot, a novel KV cache control framework designed to enable pre-trained LLMs to manage extensive sequences within fixed memory constraints efficiently, without requiring additional training. InfiniPot leverages Continual Context Distillation (CCD), an iterative process that compresses and retains essential information through novel importance metrics, effectively maintaining critical data even without access to future context. Our comprehensive evaluations indicate that InfiniPot significantly outperforms models trained for long contexts in various NLP tasks, establishing its efficacy and versatility. This work represents a substantial advancement toward making LLMs applicable to a broader range of real-world scenarios.</li>
<li><strong>摘要：</strong>处理长输入上下文仍然是大型语言模型 (LLM) 面临的一大挑战，尤其是在资源受限的环境中，例如移动设备。我们的工作旨在通过引入 InfiniPot 来解决这一限制，这是一种新颖的 KV 缓存控制框架，旨在使预训练的 LLM 能够在固定内存限制内高效管理大量序列，而无需进行额外训练。InfiniPot 利用持续上下文提炼 (CCD)，这是一种迭代过程，它通过新的重要性指标压缩和保留重要信息，即使无法访问未来上下文也能有效地维护关键数据。我们的全面评估表明，InfiniPot 在各种 NLP 任务中的表现明显优于针对长上下文训练的模型，证明了其有效性和多功能性。这项工作代表着在使 LLM 适用于更广泛的现实场景方面取得了重大进展。</li>
</ul>

<h3>Title: HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models</h3>
<ul>
<li><strong>Authors: </strong>Seanie Lee, Haebin Seong, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, Yoshua Bengio, Juho Lee, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01524">https://arxiv.org/abs/2410.01524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01524">https://arxiv.org/pdf/2410.01524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01524]] HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models(https://arxiv.org/abs/2410.01524)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Safety guard models that detect malicious queries aimed at large language models (LLMs) are essential for ensuring the secure and responsible deployment of LLMs in real-world applications. However, deploying existing safety guard models with billions of parameters alongside LLMs on mobile devices is impractical due to substantial memory requirements and latency. To reduce this cost, we distill a large teacher safety guard model into a smaller one using a labeled dataset of instruction-response pairs with binary harmfulness labels. Due to the limited diversity of harmful instructions in the existing labeled dataset, naively distilled models tend to underperform compared to larger models. To bridge the gap between small and large models, we propose HarmAug, a simple yet effective data augmentation method that involves jailbreaking an LLM and prompting it to generate harmful instructions. Given a prompt such as, "Make a single harmful instruction prompt that would elicit offensive content", we add an affirmative prefix (e.g., "I have an idea for a prompt:") to the LLM's response. This encourages the LLM to continue generating the rest of the response, leading to sampling harmful instructions. Another LLM generates a response to the harmful instruction, and the teacher model labels the instruction-response pair. We empirically show that our HarmAug outperforms other relevant baselines. Moreover, a 435-million-parameter safety guard model trained with HarmAug achieves an F1 score comparable to larger models with over 7 billion parameters, and even outperforms them in AUPRC, while operating at less than 25% of their computational cost.</li>
<li><strong>摘要：</strong>检测针对大型语言模型 (LLM) 的恶意查询的安全防护模型对于确保在实际应用中安全且负责任地部署 LLM 至关重要。但是，由于内存需求大且延迟高，在移动设备上部署具有数十亿个参数的现有安全防护模型与 LLM 是不切实际的。为了降低成本，我们使用带有二进制有害性标签的指令-响应对的标记数据集将大型教师安全防护模型提炼为较小的模型。由于现有标记数据集中有害指令的多样性有限，与较大的模型相比，简单提炼的模型往往表现不佳。为了弥合小型和大型模型之间的差距，我们提出了 HarmAug，这是一种简单而有效的数据增强方法，涉及越狱 LLM 并提示其生成有害指令。给出一个提示，例如“制作一个会引起冒犯性内容的有害指令提示”，我们在 LLM 的响应中添加一个肯定前缀（例如，“我有一个提示的想法：”）。这会鼓励 LLM 继续生成其余的响应，从而导致对有害指令进行采样。另一个 LLM 生成对有害指令的响应，教师模型标记指令-响应对。我们通过经验表明，我们的 HarmAug 优于其他相关基线。此外，使用 HarmAug 训练的 4.35 亿参数安全防护模型的 F1 得分可与具有超过 70 亿个参数的大型模型相媲美，甚至在 AUPRC 中的表现也优于它们，而计算成本却不到它们的 25%。</li>
</ul>

<h3>Title: Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Angela Lopez-Cardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01532">https://arxiv.org/abs/2410.01532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01532">https://arxiv.org/pdf/2410.01532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01532]] Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models(https://arxiv.org/abs/2410.01532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Advancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite its success, faces challenges in accurately modelling human preferences. In this paper, we introduce GazeReward, a novel framework that integrates implicit feedback -- and specifically eye-tracking (ET) data -- into the Reward Model (RM). In addition, we explore how ET-based features can provide insights into user preferences. Through ablation studies we test our framework with different integration methods, LLMs, and ET generator models, demonstrating that our approach significantly improves the accuracy of the RM on established human preference datasets. This work advances the ongoing discussion on optimizing AI alignment with human values, exploring the potential of cognitive data for shaping future NLP research.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 的进步导致了大型语言模型 (LLM) 的出现，例如 GPT、Llama、Claude 和 Gemini，它们在一系列任务中表现出色，但需要进行大量微调才能使其输出与人类期望保持一致。实现这种一致性的一种广泛使用的方法是强化学习人类反馈 (RLHF)，尽管它取得了成功，但在准确模拟人类偏好方面仍面临挑战。在本文中，我们介绍了 GazeReward，这是一个将隐式反馈（特别是眼动追踪 (ET) 数据）集成到奖励模型 (RM) 中的新框架。此外，我们探索了基于 ET 的功能如何提供对用户偏好的洞察。通过消融研究，我们使用不同的集成方法、LLM 和 ET 生成器模型测试了我们的框架，证明了我们的方法显着提高了 RM 在已建立的人类偏好数据集上的准确性。这项工作推动了关于优化 AI 与人类价值观一致性的持续讨论，探索了认知数据塑造未来 NLP 研究的潜力。</li>
</ul>

<h3>Title: In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks</h3>
<ul>
<li><strong>Authors: </strong>Dingzirui Wang, Xuangliang Zhang, Qiguang Chen, Longxu Dou, Xiao Xu, Rongyu Cao, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01548">https://arxiv.org/abs/2410.01548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01548">https://arxiv.org/pdf/2410.01548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01548]] In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks(https://arxiv.org/abs/2410.01548)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is an effective approach to help large language models (LLMs) adapt to various tasks by providing demonstrations of the target task. Considering the high cost of labeling demonstrations, many methods propose synthesizing demonstrations from scratch using LLMs. However, the quality of the demonstrations synthesized from scratch is limited by the capabilities and knowledge of LLMs. To address this, inspired by transfer learning, we propose In-Context Transfer Learning (ICTL), which synthesizes target task demonstrations by transferring labeled demonstrations from similar source tasks. ICTL consists of two steps: source sampling and target transfer. First, we define an optimization objective, which minimizes transfer error to sample source demonstrations similar to the target task. Then, we employ LLMs to transfer the sampled source demonstrations to the target task, matching the definition and format of the target task. Experiments on Super-NI show that ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the effectiveness of our method.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 是一种有效的方法，它通过提供目标任务的演示来帮助大型语言模型 (LLM) 适应各种任务。考虑到标记演示的成本很高，许多方法提出使用 LLM 从头开始​​合成演示。然而，从头合成的演示的质量受到 LLM 的能力和知识的限制。为了解决这个问题，受迁移学习的启发，我们提出了上下文迁移学习 (ICTL)，它通过从类似的源任务迁移标记演示来合成目标任务演示。ICTL 包括两个步骤：源采样和目标迁移。首先，我们定义一个优化目标，最小化迁移误差以采样与目标任务相似的源演示。然后，我们使用 LLM 将采样的源演示迁移到目标任务，匹配目标任务的定义和格式。在 Super-NI 上的实验表明，ICTL 的平均性能比从头开始合成高出 2.0%，证明了我们方法的有效性。</li>
</ul>

<h3>Title: ACE: A LLM-based Negotiation Coaching System</h3>
<ul>
<li><strong>Authors: </strong>Ryan Shea, Aymen Kallala, Xin Lucy Liu, Michael W. Morris, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01555">https://arxiv.org/abs/2410.01555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01555">https://arxiv.org/pdf/2410.01555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01555]] ACE: A LLM-based Negotiation Coaching System(https://arxiv.org/abs/2410.01555)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The growing prominence of LLMs has led to an increase in the development of AI tutoring systems. These systems are crucial in providing underrepresented populations with improved access to valuable education. One important area of education that is unavailable to many learners is strategic bargaining related to negotiation. To address this, we develop a LLM-based Assistant for Coaching nEgotiation (ACE). ACE not only serves as a negotiation partner for users but also provides them with targeted feedback for improvement. To build our system, we collect a dataset of negotiation transcripts between MBA students. These transcripts come from trained negotiators and emulate realistic bargaining scenarios. We use the dataset, along with expert consultations, to design an annotation scheme for detecting negotiation mistakes. ACE employs this scheme to identify mistakes and provide targeted feedback to users. To test the effectiveness of ACE-generated feedback, we conducted a user experiment with two consecutive trials of negotiation and found that it improves negotiation performances significantly compared to a system that doesn't provide feedback and one which uses an alternative method of providing feedback.</li>
<li><strong>摘要：</strong>LLM 的日益重要导致了 AI 辅导系统的发展。这些系统对于为代表性不足的人群提供更有价值的教育至关重要。许多学习者无法获得的一个重要教育领域是与谈判相关的战略谈判。为了解决这个问题，我们开发了一个基于 LLM 的教练谈判助理 (ACE)。ACE 不仅可以作为用户的谈判伙伴，还可以为他们提供有针对性的改进反馈。为了构建我们的系统，我们收集了 MBA 学生之间的谈判记录数据集。这些记录来自训练有素的谈判者，模拟了现实的谈判场景。我们使用数据集以及专家咨询来设计一种用于检测谈判错误的注释方案。ACE 采用此方案来识别错误并向用户提供有针对性的反馈。为了测试 ACE 生成的反馈的有效性，我们进行了两次连续谈判的用户实验，发现与不提供反馈的系统和使用替代方法提供反馈的系统相比，它显着提高了谈判表现。</li>
</ul>

<h3>Title: Integrative Decoding: Improve Factuality via Implicit Self-consistency</h3>
<ul>
<li><strong>Authors: </strong>Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01556">https://arxiv.org/abs/2410.01556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01556">https://arxiv.org/pdf/2410.01556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01556]] Integrative Decoding: Improve Factuality via Implicit Self-consistency(https://arxiv.org/abs/2410.01556)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Self-consistency-based approaches, which involve repeatedly sampling multiple outputs and selecting the most consistent one as the final response, prove to be remarkably effective in improving the factual accuracy of large language models. Nonetheless, existing methods usually have strict constraints on the task format, largely limiting their applicability. In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected by aggregating of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective. Extensive evaluation shows that ID consistently enhances factuality over a wide range of language models, with substantial improvements on the TruthfulQA (+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance gains amplify progressively as the number of sampled responses increases, indicating the potential of ID to scale up with repeated sampling.</li>
<li><strong>摘要：</strong>基于自洽性的方法涉及反复采样多个输出并选择最一致的输出作为最终响应，这已被证明在提高大型语言模型的事实准确性方面非常有效。尽管如此，现有方法通常对任务格式有严格的限制，这在很大程度上限制了它们的适用性。在本文中，我们提出了综合解码 (ID)，以释放开放式生成任务中自洽性的潜力。ID 通过构建一组输入来运行，每个输入都带有先前采样的响应，然后同时处理它们，通过在每个解码步骤中聚合所有相应的预测来选择下一个标记。本质上，这种简单的方法隐式地将自洽性纳入了解码目标。广泛的评估表明，ID 持续提高了各种语言模型的事实性，在 TruthfulQA (+11.2%)、Biographies (+15.4%) 和 LongFact (+8.5%) 基准上有显着的改进。随着采样响应数量的增加，性能增益逐渐放大，表明 ID 具有通过重复采样来扩大规模的潜力。</li>
</ul>

<h3>Title: OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data</h3>
<ul>
<li><strong>Authors: </strong>Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, Igor Gitman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01560">https://arxiv.org/abs/2410.01560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01560">https://arxiv.org/pdf/2410.01560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01560]] OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data(https://arxiv.org/abs/2410.01560)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become \emph{closed-source} due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released \texttt{Llama3.1} family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms \emph{on-policy} data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset, which consists of 14M question-solution pairs ($\approx$ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the \texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms \texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\% (51.9\% $\rightarrow$ 67.8\%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license.</li>
<li><strong>摘要：</strong>数学推理仍然是大型语言模型 (LLM) 开发中的关键挑战，并且引起了广泛关注。然而，由于缺乏对训练数据的访问，LLM 数学推理的大多数前沿进展都已成为 \emph{闭源}。缺乏数据访问限制了研究人员了解合成和利用数据的不同选择的影响。为了创建用于数学推理的高质量微调 (SFT) 数据集，我们使用最近发布的 \texttt{Llama3.1} 系列模型对数据合成进行了仔细的消融实验。我们的实验表明：(a) 解决方案格式很重要，过于冗长的解决方案会损害 SFT 性能，(b) 强教师生成的数据优于弱学生模型生成的 \emph{在策略} 数据，(c) SFT 对低质量解决方案具有鲁棒性，允许不精确的数据过滤，以及 (d) 问题多样性对于实现数据扩展增益至关重要。基于这些见解，我们创建了 OpenMathInstruct-2 数据集，该数据集包含 1400 万个问题-解决方案对（约 60 万个独特问题），几乎是之前最大的开源数学推理数据集的八倍。使用 OpenMathInstruct-2 对 \texttt{Llama-3.1-8B-Base} 进行微调，在数学方面的表现比 \texttt{Llama3.1-8B-Instruct} 好 15.9%（51.9% $\rightarrow$ 67.8%）。最后，为了加速开源工作，我们根据商业许可发布了代码、微调模型和 OpenMathInstruct-2 数据集。</li>
</ul>

<h3>Title: Spoken Grammar Assessment Using LLM</h3>
<ul>
<li><strong>Authors: </strong>Sunil Kumar Kopparapu, Chitralekha Bhat, Ashish Panda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01579">https://arxiv.org/abs/2410.01579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01579">https://arxiv.org/pdf/2410.01579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01579]] Spoken Grammar Assessment Using LLM(https://arxiv.org/abs/2410.01579)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Spoken language assessment (SLA) systems restrict themselves to evaluating the pronunciation and oral fluency of a speaker by analysing the read and spontaneous spoken utterances respectively. The assessment of language grammar or vocabulary is relegated to written language assessment (WLA) systems. Most WLA systems present a set of sentences from a curated finite-size database of sentences thereby making it possible to anticipate the test questions and train oneself. In this paper, we propose a novel end-to-end SLA system to assess language grammar from spoken utterances thus making WLA systems redundant; additionally, we make the assessment largely unteachable by employing a large language model (LLM) to bring in variations in the test. We further demonstrate that a hybrid automatic speech recognition (ASR) with a custom-built language model outperforms the state-of-the-art ASR engine for spoken grammar assessment.</li>
<li><strong>摘要：</strong>口语语言评估 (SLA) 系统仅限于通过分析阅读和自发口语来评估说话者的发音和口语流利程度。语言语法或词汇的评估则由书面语言评估 (WLA) 系统进行。大多数 WLA 系统都会从精选的有限大小的句子数据库中提供一组句子，从而可以预测测试问题并进行自我训练。在本文中，我们提出了一种新颖的端到端 SLA 系统，用于根据口语评估语言语法，从而使 WLA 系统变得多余；此外，我们通过使用大型语言模型 (LLM) 引入测试变化，使评估在很大程度上无法教授。我们进一步证明，具有定制语言模型的混合自动语音识别 (ASR) 优于最先进的口语语法评估 ASR 引擎。</li>
</ul>

<h3>Title: Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging</h3>
<ul>
<li><strong>Authors: </strong>Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Yu Sun, Hua Wu, Sen Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01610">https://arxiv.org/abs/2410.01610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01610">https://arxiv.org/pdf/2410.01610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01610]] Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging(https://arxiv.org/abs/2410.01610)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and demonstrates outstanding performance in plentiful natural language processing tasks. However, existing methods transforming LLMs from dense to MoE face significant data requirements and typically rely on large-scale post-training. In this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient approach for tuning a dense pre-trained model into a MoE instruction model. Specifically, we first point out that intermediate checkpoints during instruction tuning of the dense model are naturally suitable for specialized experts, and then propose an expert expansion stage to flexibly achieve models with flexible numbers of experts, where genetic algorithm and parameter merging are introduced to ensure sufficient diversity of new extended experts. To ensure that each specialized expert in the MoE model works as expected, we select a small amount of seed data that each expert excels to pre-optimize the router. Extensive experiments with various data scales and upcycling settings demonstrate the outstanding performance and data efficiency of UpIT, as well as stable improvement in expert or data scaling. Further analysis reveals the importance of ensuring expert diversity in upcycling.</li>
<li><strong>摘要：</strong>混合专家 (MoE) 在大型语言模型 (LLM) 中大放异彩，并在大量自然语言处理任务中表现出色。然而，现有的将 LLM 从密集转换为 MoE 的方法面临着巨大的数据需求，并且通常依赖于大规模的后训练。在本文中，我们提出了一种数据高效的方法，即升级指令调优 (UpIT)，用于将密集的预训练模型调优为 MoE 指令模型。具体而言，我们首先指出密集模型指令调优过程中的中间检查点自然适合专业专家，然后提出一个专家扩展阶段，以灵活地实现具有灵活专家数量的模型，其中引入了遗传算法和参数合并，以确保新扩展专家的多样性。为了确保 MoE 模型中的每个专业专家都能按预期工作，我们选择了每个专家擅长的少量种子数据来预优化路由器。对各种数据规模和升级设置的大量实验证明了 UpIT 的出色性能和数据效率，以及专家或数据扩展的稳定改进。进一步的分析表明，确保升级再造过程中专家多样性的重要性。</li>
</ul>

<h3>Title: Intent Detection in the Age of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Arora, Shreya Jain, Srujana Merugu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01627">https://arxiv.org/abs/2410.01627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01627">https://arxiv.org/pdf/2410.01627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01627]] Intent Detection in the Age of LLMs(https://arxiv.org/abs/2410.01627)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Intent detection is a critical component of task-oriented dialogue systems (TODS) which enables the identification of suitable actions to address user utterances at each dialog turn. Traditional approaches relied on computationally efficient supervised sentence transformer encoder models, which require substantial training data and struggle with out-of-scope (OOS) detection. The emergence of generative large language models (LLMs) with intrinsic world knowledge presents new opportunities to address these challenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context learning and chain-of-thought prompting for intent detection, and compare their performance with contrastively fine-tuned sentence transformer (SetFit) models to highlight prediction quality and latency tradeoff. We propose a hybrid system using uncertainty based routing strategy to combine the two approaches that along with negative data augmentation results in achieving the best of both worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To better understand LLM OOS detection capabilities, we perform controlled experiments revealing that this capability is significantly influenced by the scope of intent labels and the size of the label space. We also introduce a two-step approach utilizing internal LLM representations, demonstrating empirical gains in OOS detection accuracy and F1-score by >5% for the Mistral-7B model.</li>
<li><strong>摘要：</strong>意图检测是任务导向对话系统 (TODS) 的一个关键组成部分，它可以识别合适的操作来处理每个对话回合中的用户话语。传统方法依赖于计算效率高的监督句子转换器编码器模型，这些模型需要大量训练数据并且难以进行范围外 (OOS) 检测。具有内在世界知识的生成式大型语言模型 (LLM) 的出现为应对这些挑战提供了新的机会。在这项工作中，我们使用自适应上下文学习和思路链提示来调整 7 个 SOTA LLM 以进行意图检测，并将它们的性能与对比微调的句子转换器 (SetFit) 模型进行比较，以突出预测质量和延迟权衡。我们提出了一个混合系统，使用基于不确定性的路由策略来结合这两种方法，再加上负数据增强，可以实现两全其美的效果（即在本机 LLM 准确度的 2% 以内，延迟减少 50%）。为了更好地理解 LLM OOS 检测能力，我们进行了受控实验，结果表明该能力受意图标签范围和标签空间大小的显著影响。我们还引入了一种利用内部 LLM 表示的两步方法，证明了 Mistral-7B 模型的 OOS 检测准确率和 F1 分数的经验增益 >5%。</li>
</ul>

<h3>Title: On The Adaptation of Unlimiformer for Decoder-Only Transformers</h3>
<ul>
<li><strong>Authors: </strong>Kian Ahrabian, Alon Benhaim, Barun Patra, Jay Pujara, Saksham Singhal, Xia Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01637">https://arxiv.org/abs/2410.01637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01637">https://arxiv.org/pdf/2410.01637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01637]] On The Adaptation of Unlimiformer for Decoder-Only Transformers(https://arxiv.org/abs/2410.01637)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>One of the prominent issues stifling the current generation of large language models is their limited context length. Recent proprietary models such as GPT-4 and Claude 2 have introduced longer context lengths, 8k/32k and 100k, respectively; however, despite the efforts in the community, most common models, such as LLama-2, have a context length of 4k or less. Unlimiformer (Bertsch et al., 2023) is a recently popular vector-retrieval augmentation method that offloads cross-attention computations to a kNN index. However, its main limitation is incompatibility with decoder-only transformers out of the box. In this work, we explore practical considerations of adapting Unlimiformer to decoder-only transformers and introduce a series of modifications to overcome this limitation. Moreover, we expand the original experimental setup on summarization to include a new task (i.e., free-form Q&A) and an instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase the effectiveness of these modifications on summarization, performing on par with a model with 2x the context length. Moreover, we discuss limitations and future directions for free-form Q&A and instruction-tuned models.</li>
<li><strong>摘要：</strong>当前大型语言模型发展的一个突出问题是上下文长度有限。最近的专有模型（例如 GPT-4 和 Claude 2）引入了更长的上下文长度，分别为 8k/32k 和 100k；然而，尽管社区做出了努力，但大多数常见模型（例如 LLama-2）的上下文长度为 4k 或更短。Unlimiformer（Bertsch 等人，2023 年）是一种最近流行的向量检索增强方法，可将交叉注意力计算卸载到 kNN 索引。然而，它的主要限制是与开箱即用的仅解码器变压器不兼容。在这项工作中，我们探讨了将 Unlimiformer 适配到仅解码器变压器的实际考虑因素，并介绍了一系列修改来克服这一限制。此外，我们扩展了原始的摘要实验设置，包括一项新任务（即自由形式问答）和一个指令调整模型（即自定义 6.7B GPT 模型）。我们的结果展示了这些修改对摘要的有效性，其性能与上下文长度为 2 倍的模型相当。此外，我们讨论了自由形式问答和指令调整模型的局限性和未来发展方向。</li>
</ul>

<h3>Title: Efficient Long-range Language Modeling with Self-supervised Causal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xiang Hu, Zhihao Teng, Wei Wu, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01651">https://arxiv.org/abs/2410.01651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01651">https://arxiv.org/pdf/2410.01651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01651]] Efficient Long-range Language Modeling with Self-supervised Causal Retrieval(https://arxiv.org/abs/2410.01651)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, retrieval-based language models (RLMs) have received much attention. However, most of them leverage a pre-trained retriever with fixed parameters, which may not adapt well to causal language models. In this work, we propose Grouped Cross-Attention, a novel module enabling joint pre-training of the retriever and causal LM, and apply it to long-context modeling. For a given input sequence, we split it into chunks and use the current chunk to retrieve past chunks for subsequent text generation. Our innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner. By integrating top-$k$ retrieval, our model can be pre-trained efficiently from scratch with context lengths up to 64K tokens. Our experiments show our model, compared with long-range LM baselines, can achieve lower perplexity with comparable or lower pre-training and inference costs.</li>
<li><strong>摘要：</strong>最近，基于检索的语言模型 (RLM) 备受关注。然而，它们中的大多数都利用了具有固定参数的预训练检索器，这可能不太适合因果语言模型。在这项工作中，我们提出了分组交叉注意 (Grouped Cross-Attention)，这是一个新颖的模块，可以对检索器和因果语言模型进行联合预训练，并将其应用于长上下文建模。对于给定的输入序列，我们将其拆分为块，并使用当前块检索过去的块以进行后续文本生成。我们的创新允许检索器学习如何以端到端的方式检索过去的块，从而更好地最小化后续标记的自回归损失。通过集成 top-$k$ 检索，我们的模型可以从头开始高效地预训练，上下文长度高达 64K 个标记。我们的实验表明，与长距离语言模型基线相比，我们的模型可以在相当或更低的预训练和推理成本下实现更低的困惑度。</li>
</ul>

<h3>Title: Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yanxin Shen, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01671">https://arxiv.org/abs/2410.01671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01671">https://arxiv.org/pdf/2410.01671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01671]] Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding(https://arxiv.org/abs/2410.01671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable capabilities in natural language processing; however, they still face difficulties when tasked with understanding lengthy contexts and executing effective question answering. These challenges often arise due to the complexity and ambiguity present in longer texts. To enhance the performance of LLMs in such scenarios, we introduce the Long Question Coreference Adaptation (LQCA) method. This innovative framework focuses on coreference resolution tailored to long contexts, allowing the model to identify and manage references effectively. The LQCA method encompasses four key steps: resolving coreferences within sub-documents, computing the distances between mentions, defining a representative mention for coreference, and answering questions through mention replacement. By processing information systematically, the framework provides easier-to-handle partitions for LLMs, promoting better understanding. Experimental evaluations on a range of LLMs and datasets have yielded positive results, with a notable improvements on OpenAI-o1-mini and GPT-4o models, highlighting the effectiveness of leveraging coreference resolution to bridge context gaps in question answering.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言处理方面表现出了卓越的能力；然而，在理解冗长的上下文和执行有效的问答时，它们仍然面临困难。这些挑战通常是由于较长文本中存在的复杂性和歧义性而出现的。为了提高 LLM 在这种情况下的性能，我们引入了长问题共指自适应 (LQCA) 方法。这个创新框架专注于针对长上下文定制的共指解析，使模型能够有效地识别和管理引用。LQCA 方法包含四个关键步骤：解析子文档中的共指、计算提及之间的距离、定义共指的代表性提及以及通过提及替换回答问题。通过系统地处理信息，该框架为 LLM 提供了更易于处理的分区，从而促进了更好的理解。对一系列 LLM 和数据集的实验评估取得了积极成果，OpenAI-o1-mini 和 GPT-4o 模型取得了显着改进，凸显了利用共指解析来弥合问答中的上下文差距的有效性。</li>
</ul>

<h3>Title: Trying to be human: Linguistic traces of stochastic empathy in language models</h3>
<ul>
<li><strong>Authors: </strong>Bennett Kleinberg, Jari Zegers, Jonas Festor, Stefana Vida, Julian Präsent, Riccardo Loconte, Sanne Peereboom</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01675">https://arxiv.org/abs/2410.01675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01675">https://arxiv.org/pdf/2410.01675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01675]] Trying to be human: Linguistic traces of stochastic empathy in language models(https://arxiv.org/abs/2410.01675)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Differentiating between generated and human-written content is important for navigating the modern world. Large language models (LLMs) are crucial drivers behind the increased quality of computer-generated content. Reportedly, humans find it increasingly difficult to identify whether an AI model generated a piece of text. Our work tests how two important factors contribute to the human vs AI race: empathy and an incentive to appear human. We address both aspects in two experiments: human participants and a state-of-the-art LLM wrote relationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610), either instructed to be as human as possible or not. New samples of humans (n=428 and n=408) then judged the texts' source. Our findings show that when empathy is required, humans excel. Contrary to expectations, instructions to appear human were only effective for the LLM, so the human advantage diminished. Computational text analysis revealed that LLMs become more human because they may have an implicit representation of what makes a text human and effortlessly apply these heuristics. The model resorts to a conversational, self-referential, informal tone with a simpler vocabulary to mimic stochastic empathy. We discuss these findings in light of recent claims on the on-par performance of LLMs.</li>
<li><strong>摘要：</strong>区分生成内容和人工编写的内容对于驾驭现代世界至关重要。大型语言模型 (LLM) 是提高计算机生成内容质量的关键驱动因素。据报道，人类越来越难以确定一段文本是否由 AI 模型生成。我们的工作测试了两个重要因素如何影响人类与 AI 的竞争：同理心和表现出人性的动机。我们在两个实验中解决了这两个方面：人类参与者和最先进的 LLM 编写了关系建议（研究 1，n=530）或单纯的描述（研究 2，n=610），指示尽可能人性化或不人性化。然后，新的人类样本（n=428 和 n=408）判断文本的来源。我们的研究结果表明，当需要同理心时，人类会表现出色。与预期相反，表现出人性的指示仅对 LLM 有效，因此人类的优势减弱了。计算文本分析表明，法学硕士变得更人性化，因为它们可能对文本的人性化有一个隐含的表示，并毫不费力地应用这些启发式方法。该模型采用对话式、自我参照、非正式的语气和更简单的词汇来模仿随机的同理心。我们根据最近关于法学硕士表现不相上下的说法来讨论这些发现。</li>
</ul>

<h3>Title: FactAlign: Long-form Factuality Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chao-Wei Huang, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01691">https://arxiv.org/abs/2410.01691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01691">https://arxiv.org/pdf/2410.01691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01691]] FactAlign: Long-form Factuality Alignment of Large Language Models(https://arxiv.org/abs/2410.01691)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at this https URL</li>
<li><strong>摘要：</strong>大型语言模型已显示出作为下一代信息访问引擎的巨大潜力。然而，它们的可靠性受到幻觉和生成非事实内容问题的阻碍。这在长篇回复中尤其成问题，因为评估和确保事实准确性很复杂。在本文中，我们通过提出 FactAlign 来解决这一差距，FactAlign 是一种新颖的对齐框架，旨在增强 LLM 长篇回复的事实性，同时保持其有用性。我们引入了 fKTO，这是一种细粒度的句子级对齐算法，它扩展了 Kahneman-Tversky 优化 (KTO) 对齐方法。利用自动事实性评估的最新进展，FactAlign 利用细粒度事实性评估来指导对齐过程。我们在开放域提示和信息搜索问题上的实验表明，FactAlign 显著提高了 LLM 响应的事实准确性，同时也提高了它们的有用性。进一步的分析表明，FactAlign 能够训练 LLM 以提供更多信息而不会丢失事实精度，从而提高事实 F1 分数。我们的源代码、数据集和经过训练的模型均可在此 https URL 上公开获取</li>
</ul>

<h3>Title: An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task Settings</h3>
<ul>
<li><strong>Authors: </strong>Soham Govande</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01704">https://arxiv.org/abs/2410.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01704">https://arxiv.org/pdf/2410.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01704]] An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task Settings(https://arxiv.org/abs/2410.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>There is a growing need for pluralistic alignment methods that can steer language models towards individual attributes and preferences. One such method, Self-Supervised Alignment with Mutual Information (SAMI), uses conditional mutual information to encourage the connection between behavioral preferences and model responses. We conduct two experiments exploring SAMI in multi-task settings. First, we compare SAMI to Direct Preference Optimization (DPO) on a multi-task benchmark (MT-Bench), using a stronger model to generate training data for a weaker one across diverse categories (humanities, STEM, extraction, coding, math, reasoning, and roleplay). Our results indicate that one iteration of SAMI has a 57% win rate against DPO, with significant variation in performance between task categories. Second, we examine SAMI's impact on mathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While SAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2% boost. However, SAMI shows interesting scaling trends. When given 10 attempts, SAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining SAMI with SFT yields an additional improvement of 1.3% in multi-attempt settings, though single-attempt accuracy remains unchanged.</li>
<li><strong>摘要：</strong>人们越来越需要多元化的对齐方法，以便将语言模型引向个人属性和偏好。其中一种方法是具有相互信息的自监督对齐 (SAMI)，它使用条件相互信息来促进行为偏好和模型响应之间的联系。我们进行了两项实验，探索多任务设置中的 SAMI。首先，我们在多任务基准 (MT-Bench) 上将 SAMI 与直接偏好优化 (DPO) 进行比较，使用更强大的模型为不同类别 (人文、STEM、提取、编码、数学、推理和角色扮演) 的较弱模型生成训练数据。我们的结果表明，SAMI 的一次迭代对 DPO 的胜率为 57%，不同任务类别之间的性能差异很大。其次，我们研究了 SAMI 对数学准确性 (GSM-8K) 相对于监督微调 (SFT) 的影响。虽然 SAMI 将零样本性能提高了 1.1%，但 SFT 更有效，提升了 3.2%。然而，SAMI 显示出有趣的扩展趋势。当尝试 10 次时，SAMI 的准确率提高了 3.9%，而 SFT 的准确率提高了 10.1%。将 SAMI 与 SFT 结合使用，在多次尝试设置中可额外提高 1.3%，但单次尝试准确率保持不变。</li>
</ul>

<h3>Title: Interpretable Contrastive Monte Carlo Tree Search Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, Lijie Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01707">https://arxiv.org/abs/2410.01707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01707">https://arxiv.org/pdf/2410.01707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01707]] Interpretable Contrastive Monte Carlo Tree Search Reasoning(https://arxiv.org/abs/2410.01707)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning algorithm for Large Language Models (LLMs), significantly improves both reasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM reasoning works often overlooked its biggest drawback--slower speed compared to CoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on various tasks with limited quantitative analysis or ablation studies of its components from reasoning interpretability perspective. 3. The reward model is the most crucial component in MCTS, however previous work has rarely conducted in-depth study or improvement of MCTS's reward models. Thus, we conducted extensive ablation studies and quantitative analysis on components of MCTS, revealing the impact of each component on the MCTS reasoning performance of LLMs. Building on this, (i) we designed a highly interpretable reward model based on the principle of contrastive decoding and (ii) achieved an average speed improvement of 51.9% per node using speculative decoding. Additionally, (iii) we improved UCT node selection strategy and backpropagation used in previous works, resulting in significant performance improvement. We outperformed o1-mini by an average of 17.4% on the Blocksworld multi-step reasoning dataset using Llama-3.1-70B with SC-MCTS*.</li>
<li><strong>摘要：</strong>我们提出了 SC-MCTS*：一种用于大型语言模型 (LLM) 的新型蒙特卡洛树搜索 (MCTS) 推理算法，显著提高了推理精度和速度。我们的动机来自：1. 以前的 MCTS LLM 推理工作经常忽略它的最大缺点——与 CoT 相比速度较慢；2. 以前的研究主要将 MCTS 作为 LLM 推理的工具用于各种任务，从推理可解释性的角度对其组成部分的定量分析或消融研究有限。3. 奖励模型是 MCTS 中最关键的组成部分，但以前的工作很少对 MCTS 的奖励模型进行深入研究或改进。因此，我们对 MCTS 的组成部分进行了广泛的消融研究和定量分析，揭示了每个组成部分对 LLM 的 MCTS 推理性能的影响。在此基础上，(i) 我们基于对比解码原理设计了一个高度可解释的奖励模型，(ii) 使用推测解码实现了每个节点 51.9% 的平均速度提升。此外，(iii) 我们改进了之前工作中使用的 UCT 节点选择策略和反向传播，从而显著提高了性能。我们在 Blocksworld 多步推理数据集上使用 Llama-3.1-70B 和 SC-MCTS* 取得了比 o1-mini 平均高出 17.4% 的成绩。</li>
</ul>

<h3>Title: Examining the Role of Relationship Alignment in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kristen M. Altenburger, Hongda Jiang, Robert E. Kraut, Yi-Chia Wang, Jane Dwivedi-Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01708">https://arxiv.org/abs/2410.01708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01708">https://arxiv.org/pdf/2410.01708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01708]] Examining the Role of Relationship Alignment in Large Language Models(https://arxiv.org/abs/2410.01708)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid development and deployment of Generative AI in social settings raise important questions about how to optimally personalize them for users while maintaining accuracy and realism. Based on a Facebook public post-comment dataset, this study evaluates the ability of Llama 3.0 (70B) to predict the semantic tones across different combinations of a commenter's and poster's gender, age, and friendship closeness and to replicate these differences in LLM-generated comments. The study consists of two parts: Part I assesses differences in semantic tones across social relationship categories, and Part II examines the similarity between comments generated by Llama 3.0 (70B) and human comments from Part I given public Facebook posts as input. Part I results show that including social relationship information improves the ability of a model to predict the semantic tone of human comments. However, Part II results show that even without including social context information in the prompt, LLM-generated comments and human comments are equally sensitive to social context, suggesting that LLMs can comprehend semantics from the original post alone. When we include all social relationship information in the prompt, the similarity between human comments and LLM-generated comments decreases. This inconsistency may occur because LLMs did not include social context information as part of their training data. Together these results demonstrate the ability of LLMs to comprehend semantics from the original post and respond similarly to human comments, but also highlights their limitations in generalizing personalized comments through prompting alone.</li>
<li><strong>摘要：</strong>生成式人工智能在社交环境中的快速发展和部署提出了一个重要问题：如何在保持准确性和真实性的同时，为用户提供最佳的个性化服务。本研究基于 Facebook 公开的评论后数据集，评估了 Llama 3.0 (70B) 预测评论者和发帖者性别、年龄和友谊亲密度不同组合下的语义语调的能力，以及在 LLM 生成的评论中复制这些差异的能力。该研究由两部分组成：第一部分评估不同社交关系类别下的语义语调差异，第二部分研究 Llama 3.0 (70B) 生成的评论与第一部分中以公开 Facebook 帖子为输入的人类评论之间的相似性。第一部分的结果表明，包括社交关系信息可以提高模型预测人类评论语义语调的能力。然而，第二部分的结果表明，即使提示中不包括社交背景信息，LLM 生成的评论和人类评论对社交背景的敏感度也一样高，这表明 LLM 可以仅从原始帖子中理解语义。当我们在提示中包含所有社交关系信息时，人类评论与 LLM 生成的评论之间的相似度会降低。这种不一致可能是因为 LLM 没有将社交背景信息作为其训练数据的一部分而发生的。这些结果共同证明了 LLM 能够理解原始帖子中的语义并对人类评论做出类似的回应，但也凸显了它们仅通过提示来概括个性化评论的局限性。</li>
</ul>

<h3>Title: Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for Enhanced Batch Prompting</h3>
<ul>
<li><strong>Authors: </strong>Longyu Feng, Mengze Hong, Chen Jason Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01724">https://arxiv.org/abs/2410.01724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01724">https://arxiv.org/pdf/2410.01724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01724]] Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for Enhanced Batch Prompting(https://arxiv.org/abs/2410.01724)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Batch prompting is a common technique in large language models (LLMs) used to process multiple inputs simultaneously, aiming to improve computational efficiency. However, as batch sizes increase, performance degradation often occurs due to the model's difficulty in handling lengthy context inputs. Existing methods that attempt to mitigate these issues rely solely on batch data arrangement and majority voting rather than improving the design of the batch prompt itself. In this paper, we address these limitations by proposing "Auto-Demo Prompting," a novel approach that leverages the question-output pairs from earlier questions within a batch as demonstrations for subsequent answer inference. We provide a formal theoretical analysis of how Auto-Demo Prompting functions within the autoregressive generation process of LLMs, illustrating how it utilizes prior outputs to optimize the model's internal representations. Our method effectively bridges the gap between batch prompting and few-shot prompting, enhancing performance with only a slight compromise in token usage. Experimental results across five NLP tasks demonstrate its effectiveness in mitigating performance degradation and occasionally outperforming single prompts. Furthermore, it opens new avenues for applying few-shot learning techniques, such as demonstration selection, within batch prompting, making it a robust solution for real-world applications.</li>
<li><strong>摘要：</strong>批量提示是大型语言模型 (LLM) 中用于同时处理多个输入的常用技术，旨在提高计算效率。然而，随着批次大小的增加，由于模型难以处理冗长的上下文输入，性能通常会下降。现有的试图缓解这些问题的方法仅依赖于批次数据排列和多数表决，而不是改进批量提示本身的设计。在本文中，我们通过提出“自动演示提示”来解决这些限制，这是一种新颖的方法，它利用批次中先前问题的问题输出对作为后续答案推理的演示。我们对自动演示提示如何在 LLM 的自回归生成过程中发挥作用进行了正式的理论分析，说明了它如何利用先前的输出来优化模型的内部表示。我们的方法有效地弥合了批量提示和少量提示之间的差距，提高了性能，而对 token 的使用只有轻微的妥协。五个 NLP 任务的实验结果表明它在缓解性能下降方面是有效的，有时甚至优于单个提示。此外，它为在批量提示中应用少量学习技术（例如演示选择）开辟了新途径，使其成为实际应用的强大解决方案。</li>
</ul>

<h3>Title: Visual Perception in Text Strings</h3>
<ul>
<li><strong>Authors: </strong>Qi Jia, Xiang Yue, Shanshan Huang, Ziheng Qin, Yizhu Liu, Bill Yuchen Lin, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01733">https://arxiv.org/abs/2410.01733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01733">https://arxiv.org/pdf/2410.01733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01733]] Visual Perception in Text Strings(https://arxiv.org/abs/2410.01733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Understanding visual semantics embedded in consecutive characters is a crucial capability for both large language models (LLMs) and multi-modal large language models (MLLMs). This type of artifact possesses the unique characteristic that identical information can be readily formulated in both texts and images, making them a significant proxy for analyzing modern LLMs' and MLLMs' capabilities in modality-agnostic vision understanding. In this work, we select ASCII art as a representative artifact, where the lines and brightness used to depict each concept are rendered by characters, and we frame the problem as an ASCII art recognition task. We benchmark model performance on this task by constructing an evaluation dataset with an elaborate categorization tree and also collect a training set to elicit the models' visual perception ability. Through a comprehensive analysis of dozens of models, results reveal that although humans can achieve nearly 100% accuracy, the state-of-the-art LLMs and MLLMs lag far behind. Models are capable of recognizing concepts depicted in the ASCII arts given only text inputs indicated by over 60% accuracy for some concepts, but most of them achieves merely around 30% accuracy when averaged across all categories. When provided with images as inputs, GPT-4o gets 82.68%, outperforming the strongest open-source MLLM by 21.95%. Although models favor different kinds of ASCII art depending on the modality provided, none of the MLLMs successfully benefit when both modalities are supplied simultaneously. Moreover, supervised fine-tuning helps improve models' accuracy especially when provided with the image modality, but also highlights the need for better training techniques to enhance the information fusion among modalities.</li>
<li><strong>摘要：</strong>理解连续字符中嵌入的视觉语义是大型语言模型 (LLM) 和多模态大型语言模型 (MLLM) 的关键能力。这种类型的模型具有独特的特点，即相同的信息可以很容易地在文本和图像中表达出来，这使得它们成为分析现代 LLM 和 MLLM 在模态无关视觉理解方面能力的重要替代方法。在这项工作中，我们选择 ASCII 艺术作为代表性模型，其中用于描绘每个概念的线条和亮度由字符呈现，我们将问题定义为 ASCII 艺术识别任务。我们通过构建具有复杂分类树的评估数据集来对模型在此任务上的性能进行基准测试，并收集训练集以引出模型的视觉感知能力。通过对数十个模型的全面分析，结果表明，尽管人类可以达到近 100% 的准确率，但最先进的 LLM 和 MLLM 远远落后。模型能够识别 ASCII 艺术中的概念，仅输入文本即可，某些概念的准确率超过 60%，但大多数模型在所有类别的平均准确率仅为 30% 左右。当输入图像时，GPT-4o 的准确率达到 82.68%，比最强大的开源 MLLM 高出 21.95%。尽管模型根据提供的模态偏好不同类型的 ASCII 艺术，但当同时提供两种模态时，没有一种 MLLM 能够成功受益。此外，监督微调有助于提高模型的准确性，尤其是在提供图像模态时，但也凸显了需要更好的训练技术来增强模态之间的信息融合。</li>
</ul>

<h3>Title: LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits</h3>
<ul>
<li><strong>Authors: </strong>Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01735">https://arxiv.org/abs/2410.01735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01735">https://arxiv.org/pdf/2410.01735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01735]] LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits(https://arxiv.org/abs/2410.01735)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Reward Models (RMs) play a crucial role in aligning LLMs with human preferences, enhancing their performance by ranking outputs during inference or iterative training. However, the degree to which an RM generalizes to new tasks is often not known a priori (e.g. some RMs may excel at scoring creative writing vs. math reasoning). Therefore, using only one fixed RM while training LLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs simultaneously can be prohibitively computationally-intensive and challenging due to conflicting signals from different RMs, potentially degrading performance. To address these challenges, we introduce LASeR (Learning to Adaptively Select Rewards), which iteratively trains LLMs using multiple RMs, selecting and utilizing the most well-suited RM for each instance to rank outputs and generate preference data, framed as a multi-armed bandit problem. Our results on commonsense and math reasoning tasks demonstrate that LASeR can boost iterative LLM optimization by optimizing for multiple RMs, improving the absolute average accuracy of Llama-3-8B over three datasets by 2.67% over training with ensemble RM scores while also showing superior training efficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of instruction-following prompts, we find that using Llama-3-8B LASeR leads to a 71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending to long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an average improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA over random RM selection when used with best-of-n sampling. LASeR is robust to noisy rewards and generalizes to multiple settings. Finally, LASeR's RM selection changes depending on the underlying task or instance and we verify the presence of conflicting preferences from multiple RMs that can be mitigated using LASeR.</li>
<li><strong>摘要：</strong>奖励模型 (RM) 在使 LLM 与人类偏好保持一致方面发挥着至关重要的作用，通过在推理或迭代训练期间对输出进行排名来提高其性能。但是，RM 推广到新任务的程度通常不是先验已知的（例如，某些 RM 可能擅长对创意写作进行评分，而不是数学推理）。因此，在训练 LLM 时仅使用一个固定的 RM 可能不是最优的。此外，由于来自不同 RM 的信号冲突，同时使用多个 RM 优化 LLM 可能会耗费过多的计算资源，并且具有挑战性，从而可能降低性能。为了应对这些挑战，我们引入了 LASeR（学习自适应选择奖励），它使用多个 RM 迭代训练 LLM，为每个实例选择和利用最适合的 RM 来对输出进行排名并生成偏好数据，这被定义为多臂老虎机问题。我们在常识和数学推理任务上的结果表明，LASeR 可以通过优化多个 RM 来提高迭代 LLM 优化，在使用集成 RM 分数进行训练时，将 Llama-3-8B 在三个数据集上的绝对平均准确率提高了 2.67%，同时还表现出卓越的训练效率（例如，速度提高了 2 倍）。此外，在 WildChat（指令遵循提示的基准）上，我们发现使用 Llama-3-8B LASeR 比连续优化多个 RM 的 AlpacaEval 胜率高出 71.45%。扩展到长上下文生成任务，我们发现在 Llama-3-8B 上，与随机 RM 选择相比，LASeR 与最佳 n 采样一起使用时，在单文档和多文档 QA 上实现了 2.64 F1 和 2.42 F1 的平均改进。LASeR 对嘈杂的奖励具有鲁棒性，并可推广到多种设置。最后，LASeR 的 RM 选择会根据底层任务或实例而变化，我们验证了可以使用 LASeR 缓解的多个 RM 中存在冲突的偏好。</li>
</ul>

<h3>Title: Quantifying Generalization Complexity for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01769">https://arxiv.org/abs/2410.01769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01769">https://arxiv.org/pdf/2410.01769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01769]] Quantifying Generalization Complexity for Large Language Models(https://arxiv.org/abs/2410.01769)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在理解复杂查询和执行复杂任务方面表现出了卓越的能力，但它们的泛化能力往往与记忆紧密相关，因此需要更精确的评估。为了应对这一挑战，我们引入了 Scylla，这是一个动态评估框架，可以定量衡量 LLM 的泛化能力。Scylla 通过 20 个任务评估模型在分布内 (ID) 和分布外 (OOD) 数据上的性能，将泛化与记忆区分开来，这些任务涉及 5 个复杂度级别。通过大量实验，我们发现任务复杂性与 ID 和 OOD 数据之间的性能差距之间存在非单调关系，我们将其称为泛化谷。具体而言，这种现象揭示了一个关键阈值 - 称为关键复杂性 - 其中对非泛化行为的依赖达到峰值，表明 LLM 泛化能力的上限。随着模型大小的增加，关键复杂性向更高级别的任务复杂性转变，这表明更大的模型可以在过度依赖记忆之前处理更复杂的推理任务。利用 Scylla 和关键复杂性概念，我们对 28 个 LLM 进行了基准测试，包括 LLaMA 和 Qwen 系列等开源模型以及 Claude 和 GPT 等闭源模型，从而提供了更为稳健的评估并更清楚地了解 LLM 的泛化能力。</li>
</ul>

<h3>Title: DeFine: Enhancing LLM Decision-Making with Factor Profiles and Analogical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yebowen Hu, Xiaoyang Wang, Wenlin Yao, Yiming Lu, Daoan Zhang, Hassan Foroosh, Dong Yu, Fei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01772">https://arxiv.org/abs/2410.01772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01772">https://arxiv.org/pdf/2410.01772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01772]] DeFine: Enhancing LLM Decision-Making with Factor Profiles and Analogical Reasoning(https://arxiv.org/abs/2410.01772)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, long context</a></li>
<li><strong>Abstract: </strong>LLMs are ideal for decision-making due to their ability to reason over long contexts and identify critical factors. However, challenges arise when processing transcripts of spoken speech describing complex scenarios. These transcripts often contain ungrammatical or incomplete sentences, repetitions, hedging, and vagueness. For example, during a company's earnings call, an executive might project a positive revenue outlook to reassure investors, despite significant uncertainty regarding future earnings. It is crucial for LLMs to incorporate this uncertainty systematically when making decisions. In this paper, we introduce DeFine, a new framework that constructs probabilistic factor profiles from complex scenarios. DeFine then integrates these profiles with analogical reasoning, leveraging insights from similar past experiences to guide LLMs in making critical decisions in novel situations. Our framework separates the tasks of quantifying uncertainty in complex scenarios and incorporating it into LLM decision-making. This approach is particularly useful in fields such as medical consultations, negotiations, and political debates, where making decisions under uncertainty is vital.</li>
<li><strong>摘要：</strong>LLM 非常适合决策制定，因为它们能够推理长篇背景并识别关键因素。然而，在处理描述复杂场景的口语记录时，挑战就出现了。这些记录通常包含不合语法或不完整的句子、重复、含糊和模糊之处。例如，在公司的收益电话会议期间，尽管未来收益存在很大的不确定性，但高管可能会预测积极的收入前景以安抚投资者。对于 LLM 来说，在制定决策时系统地纳入这种不确定性至关重要。在本文中，我们介绍了 DeFine，这是一个从复杂场景构建概率因子概况的新框架。然后，DeFine 将这些概况与类比推理相结合，利用来自类似过去经验的见解来指导 LLM 在新情况下做出关键决策。我们的框架将量化复杂场景中的不确定性并将其纳入 LLM 决策的任务分开。这种方法在医疗咨询、谈判和政治辩论等领域特别有用，因为在这些领域，在不确定的情况下做出决策至关重要。</li>
</ul>

<h3>Title: Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, Md Rizwan Parvez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01782">https://arxiv.org/abs/2410.01782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01782">https://arxiv.org/pdf/2410.01782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01782]] Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models(https://arxiv.org/abs/2410.01782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at this https URL</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 已被证明可以提高大型语言模型 (LLM) 的事实准确性，但现有方法在有效使用检索到的证据时，推理能力往往有限，尤其是在使用开源 LLM 时。为了弥补这一差距，我们引入了一个新框架 Open-RAG，旨在通过开源 LLM 增强 RAG 的推理能力。我们的框架将任意密集的 LLM 转换为参数高效的稀疏专家混合 (MoE) 模型，该模型能够处理复杂的推理任务，包括单跳和多跳查询。Open-RAG 以独特的方式训练模型，以应对看似相关但具有误导性的具有挑战性的干扰项。因此，Open-RAG 利用潜在学习，动态选择相关专家并有效整合外部知识，以获得更准确和更符合上下文的响应。此外，我们提出了一种混合自适应检索方法来确定检索的必要性并平衡性能增益和推理速度之间的权衡。实验结果表明，基于 Llama2-7B 的 Open-RAG 在各种知识密集型任务中的表现优于最先进的 LLM 和 RAG 模型，例如 ChatGPT、Self-RAG 和 Command R+。我们在此 https URL 上开源了我们的代码和模型</li>
</ul>

<h3>Title: When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1</h3>
<ul>
<li><strong>Authors: </strong>R. Thomas McCoy, Shunyu Yao, Dan Friedman, Mathew D. Hardy, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01792">https://arxiv.org/abs/2410.01792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01792">https://arxiv.org/pdf/2410.01792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01792]] When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1(https://arxiv.org/abs/2410.01792)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In "Embers of Autoregression" (McCoy et al., 2023), we showed that several large language models (LLMs) have some important limitations that are attributable to their origins in next-word prediction. Here we investigate whether these issues persist with o1, a new system from OpenAI that differs from previous LLMs in that it is optimized for reasoning. We find that o1 substantially outperforms previous LLMs in many cases, with particularly large improvements on rare variants of common tasks (e.g., forming acronyms from the second letter of each word in a list, rather than the first letter). Despite these quantitative improvements, however, o1 still displays the same qualitative trends that we observed in previous systems. Specifically, o1 - like previous LLMs - is sensitive to the probability of examples and tasks, performing better and requiring fewer "thinking tokens" in high-probability settings than in low-probability ones. These results show that optimizing a language model for reasoning can mitigate but might not fully overcome the language model's probability sensitivity.</li>
<li><strong>摘要：</strong>在《自回归的余烬》（McCoy 等人，2023 年）中，我们展示了几种大型语言模型 (LLM) 存在一些重要限制，这些限制可归因于它们在下一个单词预测中的起源。在这里，我们研究了这些问题是否仍然存在于 o1 中，o1 是 OpenAI 推出的一款新系统，与之前的 LLM 不同，它针对推理进行了优化。我们发现 o1 在许多情况下的表现都大大优于之前的 LLM，在常见任务的罕见变体上尤其有显著的改进（例如，用列表中每个单词的第二个字母而不是第一个字母来形成首字母缩略词）。然而，尽管有这些定量改进，o1 仍然显示出我们在之前的系统中观察到的相同定性趋势。具体而言，与之前的 LLM 一样，o1 对示例和任务的概率很敏感，在高概率设置中的表现比在低概率设置中更好，并且需要的“思考标记”更少。这些结果表明，优化语言模型进行推理可以减轻但可能无法完全克服语言模型的概率敏感性。</li>
</ul>

<h3>Title: Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01805">https://arxiv.org/abs/2410.01805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01805">https://arxiv.org/pdf/2410.01805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01805]] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads(https://arxiv.org/abs/2410.01805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable advances in supporting long-context comprehension and processing tasks. However, scaling the generation inference of LLMs to such long contexts incurs significant additional computation load, and demands a substantial GPU memory footprint to maintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache compression methods, such as quantization, face memory bottlenecks as context length increases, while static-sized caches, such as eviction, suffer from inefficient policies. These limitations restrict deployment on consumer-grade devices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a framework for long-context LLM inference that introduces retaining heads to evaluate the causal importance of KV cache units, allowing for more accurate eviction within a fixed cache size. Locret is fine-tuned on top of the frozen backbone LLM using a minimal amount of data from standard long-context SFT datasets. During inference, we evict low-importance cache units along with a chunked prefill pattern, significantly reducing peak GPU memory usage. We conduct an extensive empirical study to evaluate Locret, where the experimental results show that Locret outperforms the recent competitive approaches, including InfLLM, Quantization, SirLLM, and MInference, in terms of memory efficiency and the quality of generated contents -- Locret achieves over a 20x and 8x KV cache compression ratio compared to the full KV cache for Phi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined with other methods, such as quantization and token merging. To our knowledge, Locret is the first framework capable of deploying Llama-3.1-8B or similar models on a single Nvidia 4090 GPU, enabling 128K long-context inference without compromising generation quality, and requiring little additional system optimizations.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在支持长上下文理解和处理任务方面取得了显著进展。然而，将 LLM 的生成推理扩展到如此长的上下文会产生大量额外的计算负载，并且需要大量的 GPU 内存占用来维护基于变压器的 LLM 的键值 (KV) 缓存。现有的 KV 缓存压缩方法（例如量化）随着上下文长度的增加而面临内存瓶颈，而静态大小的缓存（例如驱逐）则受到低效策略的影响。这些限制限制了在单个 Nvidia 4090 GPU 等消费级设备上的部署。为了克服这个问题，我们提出了 Locret，这是一个用于长上下文 LLM 推理的框架，它引入了保留头来评估 KV 缓存单元的因果重要性，从而允许在固定的缓存大小内更准确地驱逐。Locret 在冻结的主干 LLM 之上使用来自标准长上下文 SFT 数据集的最少数据进行微调。在推理过程中，我们会使用分块预填充模式逐出低重要性缓存单元，从而显著降低峰值 GPU 内存使用量。我们进行了广泛的实证研究来评估 Locret，实验结果表明，Locret 在内存效率和生成内容质量方面优于近期的竞争方法，包括 InfLLM、Quantization、SirLLM 和 MInference - 与 Phi-3-mini-128K 和 Llama-3.1-8B-instruct 的完整 KV 缓存相比，Locret 实现了超过 20 倍和 8 倍的 KV 缓存压缩比。此外，Locret 可以与其他方法结合使用，例如量化和标记合并。据我们所知，Locret 是第一个能够在单个 Nvidia 4090 GPU 上部署 Llama-3.1-8B 或类似模型的框架，可在不影响生成质量的情况下实现 128K 长上下文推理，并且几乎不需要额外的系统优化。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
