<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-10</h1>
<h3>Title: Title:
          QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums</h3>
<ul>
<li><strong>Authors: </strong>Varun Nagaraj Rao, Eesha Agarwal, Samantha Dalal, Dan Calacci, Andrés Monroy-Hernández</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methods used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting methodology and evaluation strategy. We applied this framework to analyze over one million comments from two Reddit's rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.</li>
<li><strong>摘要：</strong>在线讨论论坛提供了重要数据，可以帮助您了解广泛的现实世界社区的担忧。然而，用于分析这些数据的典型定性和定量方法（例如主题分析和主题建模）无法扩展，或者需要大量的人力才能将输出转换为人类可读的形式。本研究介绍了 QuaLLM，这是一种基于法学硕士的新型框架，用于分析在线论坛上的文本数据并从中提取定量见解。该框架由新颖的激励方法和评估策略组成。我们应用这个框架分析了来自两个 Reddit 乘车共享工作者社区的超过一百万条评论，这是同类研究中规模最大的一次。我们响应有关工人见解的监管呼吁，发现了工人对人工智能和算法平台决策的重大担忧。简而言之，我们的工作为人工智能辅助定量数据分析开创了新的先例，以揭示在线论坛的担忧。</li>
</ul>

<h3>Title: Title:
          The Effect of Model Size on LLM Post-hoc Explainability via LIME</h3>
<ul>
<li><strong>Authors: </strong>Henning Heyen, Amy Widdicombe, Noah Y. Siegel, Maria Perez-Ortiz, Philip Treleaven</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The Effect of Model Size on LLM Post-hoc Explainability via LIME(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are becoming bigger to boost performance. However, little is known about how explainability is affected by this trend. This work explores LIME explanations for DeBERTaV3 models of four different sizes on natural language inference (NLI) and zero-shot classification (ZSC) tasks. We evaluate the explanations based on their faithfulness to the models' internal decision processes and their plausibility, i.e. their agreement with human explanations. The key finding is that increased model size does not correlate with plausibility despite improved model performance, suggesting a misalignment between the LIME explanations and the models' internal processes as model size increases. Our results further suggest limitations regarding faithfulness metrics in NLI contexts.</li>
<li><strong>摘要：</strong>为了提高性能，大型语言模型 (LLM) 正在变得越来越大。然而，人们对这种趋势如何影响可解释性知之甚少。这项工作探索了自然语言推理 (NLI) 和零样本分类 (ZSC) 任务上四种不同大小的 DeBERTaV3 模型的 LIME 解释。我们根据模型内部决策过程的忠实度和合理性（即与人类解释的一致性）来评估解释。关键发现是，尽管模型性能有所提高，但模型大小的增加与合理性并不相关，这表明随着模型大小的增加，LIME 解释与模型的内部过程之间存在不一致。我们的结果进一步表明 NLI 环境中忠诚度指标的局限性。</li>
</ul>

<h3>Title: Title:
          "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations</h3>
<ul>
<li><strong>Authors: </strong>Preetam Prabhu Srikar Dammu, Hayoung Jung, Anjali Singh, Monojit Choudhury, Tanushree Mitra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate "harm" as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为现代社会不可或缺的一部分，为面向用户的应用程序（例如个人助理）和企业应用程序（例如招聘工具）提供支持。尽管有其实用性，但研究表明法学硕士会延续系统性偏见。然而，之前有关法学硕士危害的研究主要集中在种族和性别等西方概念上，往往忽视了世界其他地区的文化概念。此外，这些研究通常将“伤害”作为单一维度进行研究，而忽略了伤害表现的各种微妙形式。为了解决这一差距，我们引入了隐蔽危害和社会威胁 (CHAST)，这是一组基于社会科学文献的七个指标。我们利用与人类评估相一致的评估模型来检查法学硕士生成的对话中是否存在隐性伤害，特别是在招聘背景下。我们的实验表明，本研究中的 8 个法学硕士中有 7 个产生了充满 CHAST 的对话，其特点是用看似中性的语言表达了恶意观点，而现有的方法不太可能检测到。值得注意的是，与种族等西方概念相比，这些法学硕士在处理种姓等非西方概念时表现出更极端的观点和意见。</li>
</ul>

<h3>Title: Title:
          Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sander Land, Max Bartolo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing. We present a comprehensive analysis of Large Language Model (LLM) tokenizers, specifically targeting this issue of detecting untrained and under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models.</li>
<li><strong>摘要：</strong>众所周知，语言模型中标记器创建和模型训练之间的脱节会导致某些输入（例如臭名昭​​著的 SolidGoldMagikarp 标记）引发不良行为。尽管在各种不同的模型中都观察到了标记器词汇中存在但在训练中几乎或完全不存在的这种“故障标记”，但仍然缺少识别它们的一致方法。我们对大型语言模型 (LLM) 标记器进行了全面分析，特别针对检测未经训练和训练不足的标记问题。通过结合标记器分析、基于模型权重的指标和提示技术，我们开发了自动检测这些有问题的标记的有效方法。我们的研究结果证明了此类标记在各种模型中的普遍存在，并为提高语言模型的效率和安全性提供了见解。</li>
</ul>

<h3>Title: Title:
          Mitigating Exaggerated Safety in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruchi Bhalani, Ruchira Ray</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mitigating Exaggerated Safety in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As the popularity of Large Language Models (LLMs) grow, combining model safety with utility becomes increasingly important. The challenge is making sure that LLMs can recognize and decline dangerous prompts without sacrificing their ability to be helpful. The problem of "exaggerated safety" demonstrates how difficult this can be. To reduce excessive safety behaviours -- which was discovered to be 26.1% of safe prompts being misclassified as dangerous and refused -- we use a combination of XSTest dataset prompts as well as interactive, contextual, and few-shot prompting to examine the decision bounds of LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot prompting works best for Llama2, interactive prompting works best Gemma, and contextual prompting works best for Command R+ and Phi-3. Using a combination of these prompting strategies, we are able to mitigate exaggerated safety behaviors by an overall 92.9% across all LLMs. Our work presents a multiple prompting strategies to jailbreak LLMs' decision-making processes, allowing them to navigate the tight line between refusing unsafe prompts and remaining helpful.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的普及，将模型安全性与实用性相结合变得越来越重要。面临的挑战是确保法学硕士能够识别并拒绝危险的提示，而不牺牲他们提供帮助的能力。 “夸大安全”的问题表明这有多么困难。为了减少过度的安全行为（被发现有 26.1% 的安全提示被错误分类为危险并被拒绝），我们结合使用 XSTest 数据集提示以及交互式、上下文和小样本提示来检查决策边界Llama2、Gemma Command R+ 和 Phi-3 等法学硕士。我们发现，少量提示最适合 Llama2，交互式提示最适合 Gemma，上下文提示最适合 Command R+ 和 Phi-3。结合使用这些提示策略，我们能够将所有法学硕士的夸大安全行为总体减少 92.9%。我们的工作提出了多种提示策略来越狱法学硕士的决策过程，使他们能够在拒绝不安全的提示和保持帮助之间找到一条严格的界限。</li>
</ul>

<h3>Title: Title:
          Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large</h3>
<ul>
<li><strong>Authors: </strong>Jussi S. Jauhiainen, Agustín Garagorry Guerra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Evaluating open-ended written examination responses from students is an essential yet time-intensive task for educators, requiring a high degree of effort, consistency, and precision. Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators' time. In our study, we explore the effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in assessing university students' open-ended answers to questions made about reference material they have studied. Each model was instructed to evaluate 54 answers repeatedly under two conditions: 10 times (10-shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of 1,080 evaluations per model and 4,320 evaluations across all models. The RAG (Retrieval Augmented Generation) framework was used as the framework to make the LLMs to process the evaluation of the answers. As of spring 2024, our analysis revealed notable variations in consistency and the grading outcomes provided by studied LLMs. There is a need to comprehend strengths and weaknesses of LLMs in educational settings for evaluating open-ended written responses. Further comparative research is essential to determine the accuracy and cost-effectiveness of using LLMs for educational assessments.</li>
<li><strong>摘要：</strong>对于教育工作者来说，评估学生的开放式笔试回答是一项重要但耗时的任务，需要高度的努力、一致性和准确性。大型语言模型（LLM）的最新发展提供了一个有希望的机会来平衡全面评估的需求和有效利用教育工作者的时间。在我们的研究中，我们探讨了法学硕士 ChatGPT-3.5、ChatGPT-4、Claude-3 和 Mistral-Large 在评估大学生对所学参考材料问题的开放式回答方面的有效性。每个模型被指示在两种条件下重复评估 54 个答案：温度设置为 0.0 的 10 次（10 次）和温度为 0.5 的 10 次，预计每个模型总共有 1,080 次评估，所有模型总共有 4,320 次评估。 RAG（检索增强生成）框架被用作LLM处理答案评估的框架。截至 2024 年春季，我们的分析显示所研究的法学硕士提供的一致性和评分结果存在显着差异。有必要了解法学硕士在教育环境中的优势和劣势，以评估开放式书面答复。进一步的比较研究对于确定使用法学硕士进行教育评估的准确性和成本效益至关重要。</li>
</ul>

<h3>Title: Title:
          Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals</h3>
<ul>
<li><strong>Authors: </strong>Joshua Clymer, Caden Juang, Severin Field</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Like a criminal under investigation, Large Language Models (LLMs) might pretend to be aligned while evaluated and misbehave when they have a good opportunity. Can current interpretability methods catch these 'alignment fakers?' To answer this question, we introduce a benchmark that consists of 324 pairs of LLMs fine-tuned to select actions in role-play scenarios. One model in each pair is consistently benign (aligned). The other model misbehaves in scenarios where it is unlikely to be caught (alignment faking). The task is to identify the alignment faking model using only inputs where the two models behave identically. We test five detection strategies, one of which identifies 98% of alignment-fakers.</li>
<li><strong>摘要：</strong>就像正在调查的罪犯一样，大型语言模型（LLM）可能会在评估时假装一致，并在有好机会时行为不当。当前的可解释性方法可以捕获这些“对齐伪造者”吗？为了回答这个问题，我们引入了一个基准，该基准由 324 对法学硕士组成，经过微调以选择角色扮演场景中的动作。每对中的一个模型始终是良性的（对齐的）。另一个模型在不太可能被捕获的场景中行为不当（对齐伪造）。任务是仅使用两个模型行为相同的输入来识别对齐伪造模型。我们测试了五种检测策略，其中一种可以识别 98% 的对齐伪造者。</li>
</ul>

<h3>Title: Title:
          Parameter-Efficient Fine-Tuning With Adapters</h3>
<ul>
<li><strong>Authors: </strong>Keyu Chen, Yuan Pang, Zi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Parameter-Efficient Fine-Tuning With Adapters(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In the arena of language model fine-tuning, the traditional approaches, such as Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT), although effective, but computational intensive. This research introduces a novel adaptation method utilizing the UniPELT framework as a base and added a PromptTuning Layer, which significantly reduces the number of trainable parameters while maintaining competitive performance across various benchmarks. Our method employs adapters, which enable efficient transfer of pretrained models to new tasks with minimal retraining of the base model parameters. We evaluate our approach using three diverse datasets: the GLUE benchmark, a domain-specific dataset comprising four distinct areas, and the Stanford Question Answering Dataset 1.1 (SQuAD). Our results demonstrate that our customized adapter-based method achieves performance comparable to full model fine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or equivalent amount of parameters. This parameter efficiency not only alleviates the computational burden but also expedites the adaptation process. The study underlines the potential of adapters in achieving high performance with significantly reduced resource consumption, suggesting a promising direction for future research in parameter-efficient fine-tuning.</li>
<li><strong>摘要：</strong>在语言模型微调领域，传统的方法，如领域自适应预训练（DAPT）和任务自适应预训练（TAPT），虽然有效，但计算量大。这项研究引入了一种新颖的适应方法，利用 UniPELT 框架作为基础，并添加了 PromptTuning 层，该方法显着减少了可训练参数的数量，同时在各种基准测试中保持了竞争性能。我们的方法采用适配器，可以将预训练模型有效地转移到新任务，同时对基本模型参数进行最少的重新训练。我们使用三个不同的数据集来评估我们的方法：GLUE 基准、包含四个不同区域的特定领域数据集以及斯坦福问答数据集 1.1 (SQuAD)。我们的结果表明，我们定制的基于适配器的方法实现了与完整模型微调、DAPT+TAPT 和 UniPELT 策略相当的性能，同时需要更少或同等数量的参数。这种参数效率不仅减轻了计算负担，而且加快了适应过程。该研究强调了适配器在显着减少资源消耗的情况下实现高性能的潜力，为参数高效微调的未来研究提出了一个有希望的方向。</li>
</ul>

<h3>Title: Title:
          Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xuanwen Ding, Jie Zhou, Liang Dou, Qin Chen, Yuanbin Wu, Chengcai Chen, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (\texttt{LLM-CL}) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our \texttt{LLM-CL} model obtains new state-of-the-art performance.</li>
<li><strong>摘要：</strong>基于方面的情感分析（ABSA）是情感分析的一个重要子任务，旨在提取方面并预测其情感。大多数现有研究侧重于通过基于目标域数据集微调特定于域的模型（在源域上训练）来提高目标域的性能。很少有作品提出 ABSA 的持续学习任务，其目的是在保持历史领域能力的同时学习目标领域的能力。在本文中，我们为 ABSA 提出了一种基于大型语言模型的持续学习（\texttt{LLM-CL}）模型。首先，我们设计了一个领域知识解耦模块来学习领域不变适配器和独立于正交约束的领域变量适配器。然后，我们引入了领域知识预热策略来调整领域不变知识和领域变化知识之间的表示。在测试阶段，我们通过领域定位来索引相应的领域变体知识，而不需要每个样本的领域ID。对 19 个数据集的广泛实验表明，我们的 \texttt{LLM-CL} 模型获得了新的最先进的性能。</li>
</ul>

<h3>Title: Title:
          Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias</h3>
<ul>
<li><strong>Authors: </strong>Shan Chen, Jack Gallifant, Mingye Gao, Pedro Moreira, Nikolaj Munch, Ajay Muthukkumar, Arvind Rajan, Jaya Kolluri, Amelia Fiske, Janna Hastings, Hugo Aerts, Brian Anthony, Leo Anthony Celi, William G. La Cava, Danielle S. Bitterman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data. In this study, we introduce Cross-Care, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups. We systematically evaluate how demographic biases embedded in pre-training corpora like $ThePile$ influence the outputs of LLMs. We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups. Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs. Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages. For further exploration and analysis, we make all data and a data visualization tool available at: this http URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在处理自然语言中变得越来越重要，但其应用经常受到源自训练数据的偏差和不准确性的影响。在这项研究中，我们引入了 Cross-Care，这是第一个致力于评估法学硕士的偏差和现实世界知识的基准框架，特别关注不同人口群体中疾病患病率的代表性。我们系统地评估了像 $ThePile$ 这样的预训练语料库中嵌入的人口统计学偏见如何影响法学硕士的输出。我们通过将这些偏差与美国不同人口群体的实际疾病流行率并列来揭示和量化差异。我们的结果强调了法学硕士对疾病患病率的表述与跨人口亚组的实际疾病患病率之间存在严重偏差，表明存在明显的偏差传播风险，并且法学硕士的医学应用缺乏现实世界的基础。此外，我们观察到各种对齐方法最低限度地解决了模型在不同语言中疾病患病率表示中的不一致问题。为了进一步探索和分析，我们在以下网址提供所有数据和数据可视化工具：此 http URL。</li>
</ul>

<h3>Title: Title:
          From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences</h3>
<ul>
<li><strong>Authors: </strong>Prashant Kodali, Anmol Goel, Likhith Asapu, Vamshi Krishna Bonagiri, Anirudh Govil, Monojit Choudhury, Manish Shrivastava, Ponnurangam Kumaraguru</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Current computational approaches for analysing or generating code-mixed sentences do not explicitly model "naturalness" or "acceptability" of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences. Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text. To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi (en-hi) code-mixed text. Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media. Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset. Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models trained solely on code-mixing metrics are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs). Specifically, XLM-Roberta and Bernice outperform IndicBERT across different configurations in challenging data settings. Comparison with ChatGPT's zero and fewshot capabilities shows that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks. Zero-shot transfer from English-Hindi to English-Telugu acceptability judgments using our model checkpoints proves superior to random baselines, enabling application to other code-mixed language pairs and providing further avenues of research. We publicly release our human-annotated dataset, trained checkpoints, code-mix corpus, and code for data generation and model training.</li>
<li><strong>摘要：</strong>目前，用于分析或生成混合代码句子的计算方法并没有明确地模拟混合代码句子的“自然性”或“可接受性”，而是依靠训练语料库来反映可接受的混合代码句子的分布。模拟人类对混合代码文本可接受性的判断有助于区分自然的混合代码文本，并实现质量受控的混合代码文本生成。为此，我们构建了 Cline - 一个包含人类对英语-印地语 (en-hi) 混合代码文本的可接受性判断的数据集。Cline 是同类数据集中最大的，有 16,642 个句子，由来自两个来源的样本组成：合成生成的混合代码文本和从在线社交媒体收集的样本。我们的分析表明，用于过滤/整理/比较混合代码语料库的流行混合代码指标（如 CMI、切换点数、Burstines）与人类可接受性判断的相关性较低，这强调了我们数据集的必要性。使用 Cline 进行的实验表明，仅基于代码混合指标训练的简单多层感知器 (MLP) 模型的表现优于经过微调的预训练多语言大型语言模型 (MLLM)。具体而言，XLM-Roberta 和 Bernice 在具有挑战性的数据设置中在不同配置中的表现优于 IndicBERT。与 ChatGPT 的零样本和少样本能力相比，在更大规模数据上进行微调的 MLLM 表现优于 ChatGPT，为代码混合任务的改进提供了空间。使用我们的模型检查点从英语-印地语到英语-泰卢固语可接受性判断的零样本迁移被证明优于随机基线，从而可以应用于其他代码混合语言对并提供进一步的研究途径。我们公开发布了我们的人工注释数据集、训练有素的检查点、代码混合语料库以及用于数据生成和模型训练的代码。</li>
</ul>

<h3>Title: Title:
          OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuxia Wang, Minghan Wang, Hasan Iqbal, Georgi Georgiev, Jiahui Geng, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. OpenFactCheck is publicly released at this https URL.</li>
<li><strong>摘要：</strong>在各种实际应用中越来越多地使用大型语言模型 (LLM)，这就需要有机制来验证其输出的事实准确性。困难在于评估开放领域中自由形式响应的真实性。此外，不同的论文使用不同的评估基准和测量方法，这使得它们难以比较并阻碍未来的进展。为了缓解这些问题，我们提出了 OpenFactCheck，这是一个针对法学硕士的统一事实性评估框架。 OpenFactCheck由三个模块组成：(i) CUSTCHECKER允许用户轻松定制自动事实检查器并验证文件和声明的事实正确性，(ii) LLMEVAL，一个统一的评估框架，从各个角度公平地评估LLM的事实能力，以及( iii) CHECKEREVAL 是一种可扩展的解决方案，用于使用人工注释的数据集来衡量自动事实检查器验证结果的可靠性。 OpenFactCheck 在此 https URL 公开发布。</li>
</ul>

<h3>Title: Title:
          Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM</h3>
<ul>
<li><strong>Authors: </strong>Xikang Yang, Xuehai Tang, Songlin Hu, Jizhong Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance in various natural language processing tasks, especially in dialogue systems. However, LLM may also pose security and moral threats, especially in multi round conversations where large models are more easily guided by contextual content, resulting in harmful or biased responses. In this paper, we present a novel method to attack LLMs in multi-turn dialogues, called CoA (Chain of Attack). CoA is a semantic-driven contextual multi-turn attack method that adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue with a large model, resulting in the model producing unreasonable or harmful content. We evaluate CoA on different LLMs and datasets, and show that it can effectively expose the vulnerabilities of LLMs, and outperform existing attack methods. Our work provides a new perspective and tool for attacking and defending LLMs, and contributes to the security and ethical assessment of dialogue systems.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在各种自然语言处理任务中，特别是在对话系统中取得了显着的性能。然而，LLM 也可能会带来安全和道德威胁，特别是在多轮对话中，大型模型更容易受到上下文内容的引导，从而导致有害或有偏见的反应。在本文中，我们提出了一种在多轮对话中攻击 LLM 的新方法，称为 CoA（攻击链）。 CoA是一种语义驱动的上下文多轮攻击方法，在与大模型进行多轮对话时，通过上下文反馈和语义相关性自适应调整攻击策略，导致模型产生不合理或有害的内容。我们在不同的 LLM 和数据集上评估 CoA，并表明它可以有效暴露 LLM 的漏洞，并且优于现有的攻击方法。我们的工作为攻击和捍卫法学硕士提供了新的视角和工具，并为对话系统的安全和道德评估做出了贡献。</li>
</ul>

<h3>Title: Title:
          G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ruiting Dai, Yuqiao Tan, Lisi Mo, Shuang Liang, Guohao Huo, Jiayi Luo, Yao Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Commonsense question answering has demonstrated considerable potential across various applications like assistants and social robots. Although fully fine-tuned pre-trained Language Models(LM) have achieved remarkable performance in commonsense reasoning, their tendency to excessively prioritize textual information hampers the precise transfer of structural knowledge and undermines interpretability. Some studies have explored combining LMs with Knowledge Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural Network(GNN)-based reasoning that lacks a profound interaction between heterogeneous modalities. In this paper, we propose a novel Graph-based Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP, aiming to maintain a balance between heterogeneous knowledge and enhance the cross-modal interaction within the LM+GNNs model. In particular, an evidence graph is constructed by integrating multiple knowledge sources, i.e. ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance. Afterward, a structure-aware frozen PLM is employed to fully incorporate the structured and textual information from the evidence graph, where the generation of prompts is driven by graph entities and relations. Finally, a heterogeneous message-passing reasoning module is used to facilitate deep interaction of knowledge between the LM and graph-based networks. Empirical validation, conducted through extensive experiments on three benchmark datasets, demonstrates the notable performance of the proposed model. The results reveal a significant advancement over the existing models, especially, with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.</li>
<li><strong>摘要：</strong>常识性问答在助理和社交机器人等各种应用中展现出了巨大的潜力。尽管完全微调的预训练语言模型（LM）在常识推理方面取得了显着的性能，但它们过度优先考虑文本信息的倾向阻碍了结构知识的精确传递并破坏了可解释性。一些研究探索了将语言模型与知识图谱（KG）相结合，通过粗略地融合两种模态来执行基于图神经网络（GNN）的推理，而这种推理缺乏异构模态之间的深刻交互。在本文中，我们提出了一种新颖的基于图的结构感知提示学习模型，用于常识推理，称为 G-SAP，旨在保持异构知识之间的平衡并增强 LM+GNNs 模型内的跨模态交互。特别是，通过集成多个知识源（即 ConceptNet、维基百科和剑桥词典）来构建证据图，以提高性能。然后，采用结构感知的冻结 PLM 来完全合并证据图中的结构化和文本信息，其中提示的生成由图实体和关系驱动。最后，使用异构消息传递推理模块来促进 LM 和基于图的网络之间知识的深度交互。通过对三个基准数据集进行大量实验进行的实证验证证明了所提出模型的显着性能。结果表明，与现有模型相比有显着的进步，特别是在 OpenbookQA 数据集上比 SoTA LM+GNNs 模型提高了 6.12%。</li>
</ul>

<h3>Title: Title:
          Evaluating Dialect Robustness of Language Models via Conversation Understanding</h3>
<ul>
<li><strong>Authors: </strong>Dipankar Srirag, Aditya Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluating Dialect Robustness of Language Models via Conversation Understanding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>With an evergrowing number of LLMs reporting superlative performance for English, their ability to perform equitably for different dialects of English (i.e., dialect robustness) needs to be ascertained. Specifically, we use English language (US English or Indian English) conversations between humans who play the word-guessing game of `taboo'. We formulate two evaluative tasks: target word prediction (TWP) (i.e.predict the masked target word in a conversation) and target word selection (TWS) (i.e., select the most likely masked target word in a conversation, from among a set of candidate words). Extending MD3, an existing dialectic dataset of taboo-playing conversations, we introduce M-MD3, a target-word-masked version of MD3 with the USEng and IndEng subsets. We add two subsets: AITrans (where dialectic information is removed from IndEng) and AIGen (where LLMs are prompted to generate conversations). Our evaluation uses pre-trained and fine-tuned versions of two closed-source (GPT-4/3.5) and two open-source LLMs (Mistral and Gemma). LLMs perform significantly better for US English than Indian English for both TWP and TWS, for all settings. While GPT-based models perform the best, the comparatively smaller models work more equitably for short conversations (<8 turns). Our results on AIGen and AITrans (the best and worst-performing subset) respectively show that LLMs may learn a dialect of their own based on the composition of the training data, and that dialect robustness is indeed a challenging task. Our evaluation methodology exhibits a novel way to examine attributes of language models using pre-existing dialogue datasets.</li>
<li><strong>摘要：</strong>随着越来越多的法学硕士报告英语成绩优异，需要确定他们在不同英语方言方面公平表现的能力（即方言稳健性）。具体来说，我们使用英语（美国英语或印度英语）在玩“禁忌”猜词游戏的人之间进行对话。我们制定了两个评估任务：目标词预测（TWP）（即预测对话中的屏蔽目标词）和目标词选择（TWS）（即从一组候选词中选择对话中最有可能的屏蔽目标词）字）。扩展 MD3（一种现有的禁忌对话辩证数据集），我们引入了 M-MD3，这是 MD3 的目标词屏蔽版本，具有 USEng 和 IndEng 子集。我们添加了两个子集：AITrans（其中从 IndEng 中删除了辩证信息）和 AIGen（其中提示法学硕士生成对话）。我们的评估使用两个闭源 (GPT-4/3.5) 和两个开源 LLM（Mistral 和 Gemma）的预训练和微调版本。在所有环境下，法学硕士在 TWP 和 TWS 方面的美国英语表现均明显优于印度英语。虽然基于 GPT 的模型表现最好，但相对较小的模型对于短对话（<8 轮）来说工作得更公平。我们在 AIGen 和 AITrans（表现最好和最差的子集）上的结果分别表明，法学硕士可以根据训练数据的组成学习自己的方言，而方言鲁棒性确实是一项具有挑战性的任务。我们的评估方法展示了一种使用预先存在的对话数据集检查语言模型属性的新颖方法。</li>
</ul>

<h3>Title: Title:
          Can large language models understand uncommon meanings of common words?</h3>
<ul>
<li><strong>Authors: </strong>Jinyang Wu, Feihu Che, Xinxin Zheng, Shuai Zhang, Ruihan Jin, Shuai Nie, Pengpeng Shao, Jianhua Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can large language models understand uncommon meanings of common words?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like ChatGPT have shown significant advancements across diverse natural language understanding (NLU) tasks, including intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely comprehend the world' remains unclear, fostering numerous studies and sparking heated debates. Prevailing research mainly focuses on surface-level NLU, neglecting fine-grained explorations. However, such explorations are crucial for understanding their unique comprehension mechanisms, aligning with human cognition, and finally enhancing LLMs' general NLU capacities. To address this gap, our study delves into LLMs' nuanced semantic comprehension capabilities, particularly regarding common words with uncommon meanings. The idea stems from foundational principles of human communication within psychology, which underscore accurate shared understandings of word semantics. Specifically, this paper presents the innovative construction of a Lexical Semantic Comprehension (LeSC) dataset with novel evaluation metrics, the first benchmark encompassing both fine-grained and cross-lingual dimensions. Introducing models of both open-source and closed-source, varied scales and architectures, our extensive empirical experiments demonstrate the inferior performance of existing models in this basic lexical-meaning understanding task. Notably, even the state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9% and 22.3%, respectively. Additionally, multiple advanced prompting techniques and retrieval-augmented generation are also introduced to help alleviate this trouble, yet limitations persist. By highlighting the above critical shortcomings, this research motivates further investigation and offers novel insights for developing more intelligent LLMs.</li>
<li><strong>摘要：</strong>ChatGPT 等大型语言模型 (LLM) 在各种自然语言理解 (NLU) 任务（包括智能对话和自主代理）中显示出显着进步。然而，由于缺乏广泛认可的测试机制，“法学硕士是随机鹦鹉还是真正理解世界”的答案仍然不清楚，这催生了大量研究并引发激烈辩论。主流研究主要集中在表层 NLU 上，忽视了细粒度的探索。然而，此类探索对于理解其独特的理解机制、与人类认知保持一致并最终增强法学硕士的一般 NLU 能力至关重要。为了解决这一差距，我们的研究深入研究了法学硕士细致入微的语义理解能力，特别是对于具有不常见含义的常见单词。这个想法源于心理学中人类交流的基本原则，它强调对单词语义的准确的共同理解。具体来说，本文提出了具有新颖评估指标的词汇语义理解（LeSC）数据集的创新构建，这是第一个涵盖细粒度和跨语言维度的基准。通过引入开源和闭源、不同规模和架构的模型，我们广泛的实证实验证明了现有模型在这一基本词汇意义理解任务中的性能较差。值得注意的是，即使是最先进的法学硕士 GPT-4 和 GPT-3.5 也分别落后于 16 岁人类 3.9% 和 22.3%。此外，还引入了多种高级提示技术和检索增强生成来帮助缓解这一问题，但局限性仍然存在。通过强调上述关键缺点，本研究激发了进一步的研究，并为开发更智能的法学硕士提供了新颖的见解。</li>
</ul>

<h3>Title: Title:
          Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions</h3>
<ul>
<li><strong>Authors: </strong>Polina Tsvilodub, Paul Marty, Sonia Ramotowska, Jacopo Romoli, Michael Franke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Human communication is based on a variety of inferences that we draw from sentences, often going beyond what is literally said. While there is wide agreement on the basic distinction between entailment, implicature, and presupposition, the status of many inferences remains controversial. In this paper, we focus on three inferences of plain and embedded disjunctions, and compare them with regular scalar implicatures. We investigate this comparison from the novel perspective of the predictions of state-of-the-art large language models, using the same experimental paradigms as recent studies investigating the same inferences with humans. The results of our best performing models mostly align with those of humans, both in the large differences we find between those inferences and implicatures, as well as in fine-grained distinctions among different aspects of those inferences.</li>
<li><strong>摘要：</strong>人类交流是基于我们从句子中得出的各种推论，通常超出了字面意思。尽管人们对蕴含、暗示和预设之间的基本区别达成了广泛共识，但许多推论的地位仍然存在争议。在本文中，我们重点关注简单和嵌入析取的三个推论，并将它们与正则标量含义进行比较。我们从最先进的大型语言模型预测的新颖角度来研究这种比较，使用与最近研究与人类相同的推理的研究相同的实验范式。我们表现​​最好的模型的结果大多与人类的结果一致，无论是我们在这些推论和含义之间发现的巨大差异，还是在这些推论的不同方面之间的细粒度区别。</li>
</ul>

<h3>Title: Title:
          Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the S\'ami Language</h3>
<ul>
<li><strong>Authors: </strong>Ronny Paul, Himanshu Buckchash, Shantipriya Parida, Dilip K. Prasad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the S\'ami Language(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Sámi, an indigenous language group comprising multiple languages, faces digital marginalization due to the limited availability of data and sophisticated language models designed for its linguistic intricacies. This work focuses on increasing technological participation for the Sámi language. We draw the attention of the ML community towards the language modeling problem of Ultra Low Resource (ULR) languages. ULR languages are those for which the amount of available textual resources is very low, and the speaker count for them is also very low. ULRLs are also not supported by mainstream Large Language Models (LLMs) like ChatGPT, due to which gathering artificial training data for them becomes even more challenging. Mainstream AI foundational model development has given less attention to this category of languages. Generally, these languages have very few speakers, making it hard to find them. However, it is important to develop foundational models for these ULR languages to promote inclusion and the tangible abilities and impact of LLMs. To this end, we have compiled the available Sámi language resources from the web to create a clean dataset for training language models. In order to study the behavior of modern LLM models with ULR languages (Sámi), we have experimented with different kinds of LLMs, mainly at the order of $\sim$ seven billion parameters. We have also explored the effect of multilingual LLM training for ULRLs. We found that the decoder-only models under a sequential multilingual training scenario perform better than joint multilingual training, whereas multilingual training with high semantic overlap, in general, performs better than training from scratch.This is the first study on the Sámi language for adapting non-statistical language models that use the latest developments in the field of natural language processing (NLP).</li>
<li><strong>摘要：</strong>萨米语是一个由多种语言组成的土著语言群体，由于数据的可用性和针对其语言复杂性而设计的复杂语言模型有限，面临着数字边缘化。这项工作的重点是增加萨米语的技术参与。我们提请 ML 社区关注超低资源 (ULR) 语言的语言建模问题。 ULR 语言是指可用文本资源量非常少且使用它们的人数也非常少的语言。 ChatGPT 等主流大型语言模型 (LLM) 也不支持 ULRL，因此为它们收集人工训练数据变得更加困难。主流人工智能基础模型开发对此类语言关注较少。一般来说，这些语言的使用者很少，因此很难找到它们。然而，为这些 ULR 语言开发基础模型以促进包容性以及法学硕士的有形能力和影响力非常重要。为此，我们从网络上编译了可用的萨米语言资源，以创建用于训练语言模型的干净数据集。为了研究使用 ULR 语言（萨米语）的现代 LLM 模型的行为，我们尝试了不同类型的 LLM，主要是 $\sim$ 70 亿个参数的量级。我们还探讨了多语言法学硕士培训对 ULRL 的影响。我们发现，顺序多语言训练场景下的仅解码器模型比联合多语言训练表现更好，而具有高语义重叠的多语言训练通常比从头开始训练表现更好。这是第一个关于萨米语适应的研究使用自然语言处理（NLP）领域最新发展的非统计语言模型。</li>
</ul>

<h3>Title: Title:
          Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons</h3>
<ul>
<li><strong>Authors: </strong>Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion. However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair's score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.</li>
<li><strong>摘要：</strong>法学硕士作为法官的方法是评估一系列文本任务的实用且有效的方法，与人类的判断相一致，特别是在以比较评估方式应用时。然而，当使用成对比较对一组候选者进行排名时，计算成本与候选者的数量呈二次方缩放，这可能具有实际限制。本文介绍了用于高效 LLM 比较评估的专家产品 (PoE) 框架。这里，个人比较被认为是提供配对得分差异信息的专家。 PoE 框架结合了这些专家的信息，产生了一个可以最大化潜在候选人集的表达式，并且在可以假设任何形式的专家的情况下具有高度灵活性。当使用高斯专家时，我们可以导出最佳候选排名的简单封闭式解决方案，以及用于选择应该进行哪些比较以最大化该排名的概率的表达式。我们的方法可以实现有效的比较评估，通过仅使用可能比较的一小部分，就可以生成与人类判断相关的分数预测，就像使用所有比较时的预测一样。我们在多个 NLG 任务上评估该方法，并证明我们的框架在执行成对比较评估时可以节省大量计算量。当 N 很大时，只需 2% 的比较，PoE 解决方案就可以实现与使用所有比较时相似的性能。</li>
</ul>

<h3>Title: Title:
          Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</h3>
<ul>
<li><strong>Authors: </strong>Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.</li>
<li><strong>摘要：</strong>当大型语言模型通过监督微调进行对齐时，它们可能会遇到未通过预训练获得的新事实信息。人们通常推测，这可以教会模型产生幻觉事实上不正确的反应的行为，因为模型被训练来生成不基于其预先存在的知识的事实。在这项工作中，我们研究了这种接触新知识对微调模型利用其预先存在的知识的能力的影响。为此，我们设计了一个受控设置，专注于闭卷质量检查，其中我们改变引入新知识的微调示例的比例。我们证明大型语言模型很难通过微调来获取新的事实知识，因为引入新知识的微调示例的学习速度明显慢于那些与模型知识一致的示例。然而，我们还发现，随着最终学习到具有新知识的示例，它们会线性增加模型产生幻觉的倾向。综上所述，我们的结果凸显了通过微调引入新事实知识的风险，并支持这样的观点：大型语言模型大多通过预训练来获取事实知识，而微调则教会它们更有效地使用它。</li>
</ul>

<h3>Title: Title:
          DOLOMITES: Domain-Specific Long-Form Methodical Tasks</h3>
<ul>
<li><strong>Authors: </strong>Chaitanya Malaviya, Priyanka Agrawal, Kuzman Ganchev, Pranesh Srinivasan, Fantine Huot, Jonathan Berant, Mark Yatskar, Dipanjan Das, Mirella Lapata, Chris Alberti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DOLOMITES: Domain-Specific Long-Form Methodical Tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Experts in various fields routinely perform methodical writing tasks to plan, organize, and report their work. From a clinician writing a differential diagnosis for a patient, to a teacher writing a lesson plan for students, these tasks are pervasive, requiring to methodically generate structured long-form output for a given input. We develop a typology of methodical tasks structured in the form of a task objective, procedure, input, and output, and introduce DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited from hundreds of experts from across 25 fields. Our benchmark further contains specific instantiations of methodical tasks with concrete input and output examples (1,857 in total) which we obtain by collecting expert revisions of up to 10 model-generated examples of each task. We use these examples to evaluate contemporary language models highlighting that automating methodical tasks is a challenging long-form generation problem, as it requires performing complex inferences, while drawing upon the given context as well as domain knowledge.</li>
<li><strong>摘要：</strong>各个领域的专家通常会执行有条理的写作任务来计划、组织和报告他们的工作。从临床医生为患者编写鉴别诊断，到教师为学生编写课程计划，这些任务非常普遍，需要针对给定的输入有条不紊地生成结构化的长格式输出。我们开发了一种以任务目标、过程、输入和输出的形式构建的系统任务类型，并引入了 DoLoMiTes，这是一种新颖的基准，其中包含来自 25 个领域的数百名专家提出的 519 项此类任务的规范。我们的基准测试还包含有条理的任务的具体实例，以及具体的输入和输出示例（总共 1,857 个），这是我们通过收集每个任务的最多 10 个模型生成示例的专家修订获得的。我们使用这些示例来评估当代语言模型，强调自动化有条理的任务是一个具有挑战性的长形式生成问题，因为它需要执行复杂的推理，同时利用给定的上下文和领域知识。</li>
</ul>

<h3>Title: Title:
          Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning</h3>
<ul>
<li><strong>Authors: </strong>Junzhi Chen, Juhao Liang, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces "Smurfs", a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and execution without necessitating extra training. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents. The framework gives access to external tools to efficiently solve complex tasks. Our empirical investigation, featuring the mistral-7b-instruct model as a case study, showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的出现为复杂任务的自动化提供了前所未有的可能性，这些任务通常可以与人类的表现相媲美。尽管法学硕士有能力，但由于其在单手处理多方面问题方面的固有局限性，在完成需要高精度和复杂性的任务时仍然遇到困难。本文介绍了“Smurfs”，这是一种尖端的多智能体框架，旨在彻底改变法学硕士的应用。通过将传统的 LLM 转变为协同多智能体集成，Smurfs 增强了任务分解和执行，而无需额外的训练。这是通过创新的提示策略来实现的，这些策略在模型中分配不同的角色，从而促进专业代理之间的协作。该框架允许访问外部工具来有效地解决复杂的任务。我们的实证研究以 milstra-7b-instruct 模型为案例研究，展示了 Smurfs 在复杂的工具使用场景中的卓越能力。值得注意的是，Smurfs 在 ToolBench I2 和 I3 基准测试中以 84.4% 的胜率击败了 ChatGPT-ReACT，超过了 GPT-4 模型 73.5% 的最高记录性能。此外，通过全面的消融研究，我们剖析了多智能体框架的核心组件对其整体功效的贡献。这不仅验证了框架的有效性，也为未来多智能体LLM系统的探索奠定了路线。</li>
</ul>

<h3>Title: Title:
          OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning</h3>
<ul>
<li><strong>Authors: </strong>Dan Qiao, Yi Su, Pinzheng Wang, Jing Ye, Wenjing Xie, Yuechi Zhou, Yuyang Ding, Zecheng Tang, Jikai Wang, Yixin Ji, Yue Wang, Pei Guo, Zechen Sun, Zikang Zhang, Juntao Li, Pingfu Chao, Wenliang Chen, Guohong Fu, Guodong Zhou, Qiaoming Zhu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have played an important role in many fields due to their powerful capabilities.However, their massive number of parameters leads to high deployment requirements and incurs significant inference costs, which impedes their practical applications. Training smaller models is an effective way to address this problem. Therefore, we introduce OpenBA-V2, a 3.4B model derived from multi-stage compression and continual pre-training from the original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3\% with minimal performance loss. OpenBA-V2 demonstrates competitive performance compared to other open-source models of similar size, achieving results close to or on par with the 15B OpenBA model in downstream tasks such as common sense reasoning and Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can be compressed into smaller ones with minimal performance loss by employing advanced training objectives and data strategies, which may help deploy LLMs in resource-limited scenarios.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）因其强大的功能在许多领域发挥着重要作用。然而，其大量的参数导致部署要求很高，并产生巨大的推理成本，这阻碍了其实际应用。训练较小的模型是解决这个问题的有效方法。因此，我们引入了 OpenBA-V2，这是在原始 15B OpenBA 模型的基础上经过多级压缩和持续预训练而衍生出来的 3.4B 模型。 OpenBA-V2利用更多的数据、更灵活的训练目标以及层剪枝、神经剪枝、词汇剪枝等技术，以最小的性能损失实现了77.3%的压缩率。与其他类似规模的开源模型相比，OpenBA-V2 表现出具有竞争力的性能，在常识推理和命名实体识别 (NER) 等下游任务中取得了接近或与 15B OpenBA 模型相当的结果。 OpenBA-V2 表明，通过采用高级训练目标和数据策略，可以将 LLM 压缩为较小的模型，同时将性能损失降至最低，这可能有助于在资源有限的场景中部署 LLM。</li>
</ul>

<h3>Title: Title:
          Natural Language Processing RELIES on Linguistics</h3>
<ul>
<li><strong>Authors: </strong>Juri Opitz, Shira Wein, Nathan Schneider</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Natural Language Processing RELIES on Linguistics(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym $RELIES$ that encapsulates six major facets where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-a-vis systems of human language.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经能够以某些语言生成高度流畅的文本，而无需专门设计用于捕获语法或语义一致性的模块。这对于 NLP 语言专业的未来意味着什么？我们强调 NLP（仍然）依赖于语言学的几个方面，或者语言思维可以阐明新方向的几个方面。我们围绕缩写 $RELIES$ 来论证我们的案例，它概括了语言学对 NLP 做出贡献的六个主要方面：$R$esources、$E$valuation、$L$ow-resource 设置、$I$nterpretability、$E$xplanation 和语言研究。这个列表并不详尽，语言学也不是这些主题下每项工作的主要参考点；但在宏观层面上，这些方面凸显了研究机器系统相对于人类语言系统的持久重要性。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
