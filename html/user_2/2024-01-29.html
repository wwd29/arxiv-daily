<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-29</h1>
<h3>Title: Precision Mars Entry Navigation with Atmospheric Density Adaptation via  Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Felipe Giraldo-Grueso, Andrey A. Popov, Renato Zanetti</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14411">https://arxiv.org/abs/2401.14411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14411">https://arxiv.org/pdf/2401.14411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14411]] Precision Mars Entry Navigation with Atmospheric Density Adaptation via  Neural Networks(https://arxiv.org/abs/2401.14411)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Discrepancies between the true Martian atmospheric density and the onboard density model can significantly impair the performance of spacecraft entry navigation filters. This work introduces a new approach to online filtering for Martian entry by using a neural network to estimate atmospheric density and employing a consider analysis to account for the uncertainty in the estimate. The network is trained on an exponential atmospheric density model, and its parameters are dynamically adapted in real time to account for any mismatches between the true and estimated densities. The adaptation of the network is formulated as a maximum likelihood problem, leveraging the measurement innovations of the filter to identify optimal network parameters. The incorporation of a neural network enables the use of stochastic optimizers known for their efficiency in the machine learning domain within the context of the maximum likelihood approach. Performance comparisons against previous approaches are conducted in various realistic Mars entry navigation scenarios, resulting in superior estimation accuracy and precise alignment of the estimated density with a broad selection of realistic Martian atmospheres sampled from perturbed Mars-GRAM data.</li>
<li><strong>摘要：</strong>真实的火星大气密度与星载密度模型之间的差异会严重损害航天器进入导航过滤器的性能。这项工作引入了一种新的火星进入在线过滤方法，通过使用神经网络来估计大气密度并采用考虑分析来解释估计中的不确定性。该网络是在指数大气密度模型上进行训练的，其参数会实时动态调整，以解决真实密度和估计密度之间的任何不匹配问题。网络的适应被表述为最大似然问题，利用滤波器​​的测量创新来识别最佳网络参数。神经网络的结合使得能够在最大似然方法的背景下使用随机优化器，该优化器以其在机器学习领域的效率而闻名。在各种现实的火星进入导航场景中与以前的方法进行了性能比较，从而获得了卓越的估计精度，并将估计的密度与从扰动的 Mars-GRAM 数据中采样的广泛选择的现实火星大气精确对齐。</li>
</ul>

<h3>Title: Harnessing Neuron Stability to Improve DNN Verification</h3>
<ul>
<li><strong>Authors: </strong>Hai Duong, Dong Xu, ThanhVu Nguyen, Matthew B. Dwyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14412">https://arxiv.org/abs/2401.14412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14412">https://arxiv.org/pdf/2401.14412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14412]] Harnessing Neuron Stability to Improve DNN Verification(https://arxiv.org/abs/2401.14412)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNN) have emerged as an effective approach to tackling real-world problems. However, like human-written software, DNNs are susceptible to bugs and attacks. This has generated significant interests in developing effective and scalable DNN verification techniques and tools. In this paper, we present VeriStable, a novel extension of recently proposed DPLL-based constraint DNN verification approach. VeriStable leverages the insight that while neuron behavior may be non-linear across the entire DNN input space, at intermediate states computed during verification many neurons may be constrained to have linear behavior - these neurons are stable. Efficiently detecting stable neurons reduces combinatorial complexity without compromising the precision of abstractions. Moreover, the structure of clauses arising in DNN verification problems shares important characteristics with industrial SAT benchmarks. We adapt and incorporate multi-threading and restart optimizations targeting those characteristics to further optimize DPLL-based DNN verification. We evaluate the effectiveness of VeriStable across a range of challenging benchmarks including fully-connected feedforward networks (FNNs), convolutional neural networks (CNNs) and residual networks (ResNets) applied to the standard MNIST and CIFAR datasets. Preliminary results show that VeriStable is competitive and outperforms state-of-the-art DNN verification tools, including $\alpha$-$\beta$-CROWN and MN-BaB, the first and second performers of the VNN-COMP, respectively.</li>
<li><strong>摘要：</strong>深度神经网络（DNN）已成为解决现实世界问题的有效方法。然而，与人类编写的软件一样，DNN 也容易受到错误和攻击。这引起了人们对开发有效且可扩展的 DNN 验证技术和工具的浓厚兴趣。在本文中，我们提出了 VeriStable，这是最近提出的基于 DPLL 的约束 DNN 验证方法的新颖扩展。 VeriStable 利用了这样的见解：虽然神经元行为在整个 DNN 输入空间中可能是非线性的，但在验证过程中计算的中间状态下，许多神经元可能被限制为具有线性行为 - 这些神经元是稳定的。有效检测稳定的神经元可以降低组合复杂性，而不会影响抽象的精度。此外，DNN 验证问题中出现的条款结构与工业 SAT 基准具有重要特征。我们针对这些特征进行调整并合并多线程和重启优化，以进一步优化基于 DPLL 的 DNN 验证。我们评估了 VeriStable 在一系列具有挑战性的基准中的有效性，包括应用于标准 MNIST 和 CIFAR 数据集的全连接前馈网络 (FNN)、卷积神经网络 (CNN) 和残差网络 (ResNet)。初步结果表明，VeriStable 具有竞争力，并且优于最先进的 DNN 验证工具，包括分别在 VNN-COMP 中排名第一和第二的 $\alpha$-$\beta$-CROWN 和 MN-BaB。</li>
</ul>

<h3>Title: Multi-Agent Based Transfer Learning for Data-Driven Air Traffic  Applications</h3>
<ul>
<li><strong>Authors: </strong>Chuhao Deng, Hong-Cheol Choi, Hyunsang Park, Inseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA, eess.SY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14421">https://arxiv.org/abs/2401.14421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14421">https://arxiv.org/pdf/2401.14421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14421]] Multi-Agent Based Transfer Learning for Data-Driven Air Traffic  Applications(https://arxiv.org/abs/2401.14421)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>Research in developing data-driven models for Air Traffic Management (ATM) has gained a tremendous interest in recent years. However, data-driven models are known to have long training time and require large datasets to achieve good performance. To address the two issues, this paper proposes a Multi-Agent Bidirectional Encoder Representations from Transformers (MA-BERT) model that fully considers the multi-agent characteristic of the ATM system and learns air traffic controllers' decisions, and a pre-training and fine-tuning transfer learning framework. By pre-training the MA-BERT on a large dataset from a major airport and then fine-tuning it to other airports and specific air traffic applications, a large amount of the total training time can be saved. In addition, for newly adopted procedures and constructed airports where no historical data is available, this paper shows that the pre-trained MA-BERT can achieve high performance by updating regularly with little data. The proposed transfer learning framework and MA-BERT are tested with the automatic dependent surveillance-broadcast data recorded in 3 airports in South Korea in 2019.</li>
<li><strong>摘要：</strong>近年来，开发空中交通管理 (ATM) 数据驱动模型的研究引起了人们的极大兴趣。然而，众所周知，数据驱动模型的训练时间较长，并且需要大量数据集才能实现良好的性能。针对这两个问题，本文提出了一种多智能体双向编码器表示变换器（MA-BERT）模型，该模型充分考虑ATM系统的多智能体特征并学习空中交通管制员的决策，并提出了预训练和训练模型。微调迁移学习框架。通过在主要机场的大型数据集上对 MA-BERT 进行预训练，然后针对其他机场和特定的空中交通应用进行微调，可以节省大量的总训练时间。此外，对于新采用的程序和没有历史数据的新建机场，本文表明预训练的 MA-BERT 可以通过定期更新而用很少的数据实现高性能。所提出的迁移学习框架和 MA-BERT 使用 2019 年韩国 3 个机场记录的自动相关监视广播数据进行了测试。</li>
</ul>

<h3>Title: Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar  Power Generation</h3>
<ul>
<li><strong>Authors: </strong>Md Shazid Islam, A S M Jahid Hasan, Md Saydur Rahman, Jubair Yusuf, Md Saiful Islam Sajol, Farhana Akter Tumpa</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14422">https://arxiv.org/abs/2401.14422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14422">https://arxiv.org/pdf/2401.14422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14422]] Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar  Power Generation(https://arxiv.org/abs/2401.14422)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The prediction of solar power generation is a challenging task due to its dependence on climatic characteristics that exhibit spatial and temporal variability. The performance of a prediction model may vary across different places due to changes in data distribution, resulting in a model that works well in one region but not in others. Furthermore, as a consequence of global warming, there is a notable acceleration in the alteration of weather patterns on an annual basis. This phenomenon introduces the potential for diminished efficacy of existing models, even within the same geographical region, as time progresses. In this paper, a domain adaptive deep learning-based framework is proposed to estimate solar power generation using weather features that can solve the aforementioned challenges. A feed-forward deep convolutional network model is trained for a known location dataset in a supervised manner and utilized to predict the solar power of an unknown location later. This adaptive data-driven approach exhibits notable advantages in terms of computing speed, storage efficiency, and its ability to improve outcomes in scenarios where state-of-the-art non-adaptive methods fail. Our method has shown an improvement of $10.47 \%$, $7.44 \%$, $5.11\%$ in solar power prediction accuracy compared to best performing non-adaptive method for California (CA), Florida (FL) and New York (NY), respectively.</li>
<li><strong>摘要：</strong>太阳能发电的预测是一项具有挑战性的任务，因为它依赖于表现出空间和时间变化的气候特征。由于数据分布的变化，预测模型的性能在不同地方可能会有所不同，导致模型在一个地区效果很好，但在其他地区效果不佳。此外，由于全球变暖，每年天气模式的变化显着加速。随着时间的推移，这种现象导致现有模型的有效性可能会降低，即使在同一地理区域内也是如此。本文提出了一种基于领域自适应深度学习的框架，利用天气特征来估计太阳能发电量，可以解决上述挑战。以监督方式针对已知位置数据集训练前馈深度卷积网络模型，并用于稍后预测未知位置的太阳能。这种自适应数据驱动方法在计算速度、存储效率以及在最先进的非自适应方法失败的情况下改善结果的能力方面表现出显着的优势。与加利福尼亚州 (CA)、佛罗里达州 (FL) 和纽约州 (NY) 的最佳非自适应方法相比，我们的方法的太阳能预测精度提高了 $10.47 \%$、$7.44 \%$、$5.11\%$ ， 分别。</li>
</ul>

<h3>Title: Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo  Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14424">https://arxiv.org/abs/2401.14424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14424">https://arxiv.org/pdf/2401.14424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14424]] Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo  Tree Search(https://arxiv.org/abs/2401.14424)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Generative Pre-Trained Transformer (GPT). By using GPT to guide the MCTS process, the search efficiency of MCTS is significantly improved. Next, we utilize the MCTS results to further refine the GPT, enhancing its capabilities and providing more accurate guidance for the MCTS process. MCTS and GPT are coupled together and optimize each other until the target expression is successfully determined. We conducted extensive evaluations of SR-GPT using 222 expressions sourced from over 10 different symbolic regression datasets. The experimental results demonstrate that SR-GPT outperforms existing state-of-the-art algorithms in accurately recovering symbolic expressions both with and without added noise.</li>
<li><strong>摘要：</strong>寻找一个简洁且可解释的数学公式，准确地描述数据中每个变量与预测值之间的关系，是科学研究的关键任务，也是人工智能的重大挑战。这个问题被称为符号回归，是一个NP难问题。去年，提出了一种基于蒙特卡罗树搜索（MCTS）的符号回归方法，并在多个数据集上获得了sota。虽然与之前的方法相比，该算法在恢复目标表达方面显示出相当大的改进，但 MCTS 过程中缺乏指导严重影响了其搜索效率。最近，一些算法添加了预训练的策略网络来指导MCTS的搜索，但预训练的策略网络泛化性较差。为了平衡效率和通用性，我们结合 AlphaZero 的思想提出了 SR-GPT。 SR-GPT 是一种新的符号回归算法，它将 MCTS 与生成预训练变压器 (GPT) 相结合。通过使用GPT来指导MCTS过程，显着提高了MCTS的搜索效率。接下来，我们利用 MCTS 结果进一步完善 GPT，增强其能力，为 MCTS 流程提供更准确的指导。 MCTS和GPT耦合在一起并相互优化，直到成功确定目标表达。我们使用来自 10 多个不同符号回归数据集的 222 个表达式对 SR-GPT 进行了广泛的评估。实验结果表明，无论添加噪声还是不添加噪声，SR-GPT 在准确恢复符号表达式方面均优于现有的最先进算法。</li>
</ul>

<h3>Title: [Re] The Discriminative Kalman Filter for Bayesian Filtering with  Nonlinear and Non-Gaussian Observation Models</h3>
<ul>
<li><strong>Authors: </strong>Josue Casco-Rodriguez, Caleb Kemere, Richard G. Baraniuk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14429">https://arxiv.org/abs/2401.14429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14429">https://arxiv.org/pdf/2401.14429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14429]] [Re] The Discriminative Kalman Filter for Bayesian Filtering with  Nonlinear and Non-Gaussian Observation Models(https://arxiv.org/abs/2401.14429)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Kalman filters provide a straightforward and interpretable means to estimate hidden or latent variables, and have found numerous applications in control, robotics, signal processing, and machine learning. One such application is neural decoding for neuroprostheses. In 2020, Burkhart et al. thoroughly evaluated their new version of the Kalman filter that leverages Bayes' theorem to improve filter performance for highly non-linear or non-Gaussian observation models. This work provides an open-source Python alternative to the authors' MATLAB algorithm. Specifically, we reproduce their most salient results for neuroscientific contexts and further examine the efficacy of their filter using multiple random seeds and previously unused trials from the authors' dataset. All experiments were performed offline on a single computer.</li>
<li><strong>摘要：</strong>卡尔曼滤波器提供了一种直接且可解释的方法来估计隐藏变量或潜在变量，并在控制、机器人、信号处理和机器学习中得到了广泛的应用。其中一种应用是神经假体的神经解码。 2020 年，Burkhart 等人。彻底评估了他们的新版本卡尔曼滤波器，该滤波器利用贝叶斯定理来提高高度非线性或非高斯观测模型的滤波器性能。这项工作为作者的 MATLAB 算法提供了一个开源 Python 替代方案。具体来说，我们重现了他们在神经科学背景下最显着的结果，并使用多个随机种子和作者数据集中以前未使用的试验进一步检查了其过滤器的功效。所有实验均在一台计算机上离线进行。</li>
</ul>

<h3>Title: Semantic Sensitivities and Inconsistent Predictions: Measuring the  Fragility of NLI Models</h3>
<ul>
<li><strong>Authors: </strong>Erik Arakelyan, Zhaoqi Liu, Isabelle Augenstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14440">https://arxiv.org/abs/2401.14440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14440">https://arxiv.org/pdf/2401.14440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14440]] Semantic Sensitivities and Inconsistent Predictions: Measuring the  Fragility of NLI Models(https://arxiv.org/abs/2401.14440)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text generation, with the explicit condition that the NLI model predicts the relationship between the original and adversarial inputs as a symmetric equivalence entailment. We systematically study the effects of the phenomenon across NLI models for \emph{in-} and \emph{out-of} domain settings. Our experiments show that semantic sensitivity causes performance degradations of $12.92\%$ and $23.71\%$ average over \emph{in-} and \emph{out-of-} domain settings, respectively. We further perform ablation studies, analysing this phenomenon across models, datasets, and variations in inference and show that semantic sensitivity can lead to major inconsistency within model predictions.</li>
<li><strong>摘要：</strong>最近对基于 Transformer 的自然语言理解 (NLU) 模型的新兴能力的研究表明，它们能够理解词汇和组合语义。我们提供的证据表明这些说法应该持保留态度：我们发现最先进的自然语言推理（NLI）模型对保留表面形式变化的微小语义很敏感，这会导致模型出现相当大的不一致推理过程中的决定。值得注意的是，这种行为不同于对组合语义的有效和深入的理解，但是在标准基准上评估模型准确性时或在探究句法、单调和逻辑稳健推理时都不会出现。我们提出了一个新颖的框架来衡量语义敏感性的程度。为此，我们在对抗性生成的示例上评估 NLI 模型，这些示例包含保留少量语义的表面形式输入噪声。这是使用条件文本生成来实现的，明确的条件是 NLI 模型将原始输入和对抗性输入之间的关系预测为对称等价蕴涵。我们系统地研究了 NLI 模型中该现象对 \emph{in-} 和 \emph{out-of} 域设置的影响。我们的实验表明，语义敏感性导致 \emph{in-} 和 \emph{out-of-} 域设置的性能平均分别下降 $12.92\%$ 和 $23.71\%$。我们进一步进行消融研究，分析模型、数据集和推理变化中的这种现象，并表明语义敏感性可能导致模型预测中的重大不一致。</li>
</ul>

<h3>Title: Unveiling the Unseen: Identifiable Clusters in Trained Depthwise  Convolutional Kernels</h3>
<ul>
<li><strong>Authors: </strong>Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14469">https://arxiv.org/abs/2401.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14469">https://arxiv.org/pdf/2401.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14469]] Unveiling the Unseen: Identifiable Clusters in Trained Depthwise  Convolutional Kernels(https://arxiv.org/abs/2401.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of Gaussian (DoG) functions, and their first and second-order derivatives. Notably, we were able to classify over 95\% and 90\% of the filters from state-of-the-art ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long proposed for the vision systems of mammals. Our results thus deepen our understanding of the emergent properties of trained DS-CNNs and provide a bridge between artificial and biological visual processing systems. More broadly, they pave the way for more interpretable and biologically-inspired neural network designs in the future.</li>
<li><strong>摘要：</strong>深度可分离卷积神经网络 (DS-CNN) 的最新进展催生了新颖的架构，其性能在可观的可扩展性和准确性方面超越了经典 CNN。本文揭示了 DS-CNN 架构的另一个显着特性：在所有层中经过训练的深度卷积核中都出现了可辨别和可解释的模式。通过对数百万个不同大小和不同模型的经过训练的过滤器进行广泛分析，我们采用带有自动编码器的无监督聚类来对这些过滤器进行分类。令人惊讶的是，这些模式汇聚成几个主要簇，每个簇都类似于高斯 (DoG) 函数的差分及其一阶和二阶导数。值得注意的是，我们能够分别对来自最先进的 ConvNextV2 和 ConvNeXt 模型的超过 95% 和 90% 的滤波器进行分类。这一发现不仅仅是技术上的好奇心；它呼应了神经科学家长期以来为哺乳动物视觉系统提出的基本模型。因此，我们的结果加深​​了我们对经过训练的 DS-CNN 的新兴特性的理解，并在人工视觉处理系统和生物视觉处理系统之间架起了一座桥梁。更广泛地说，它们为未来更具可解释性和受生物学启发的神经网络设计铺平了道路。</li>
</ul>

<h3>Title: Scilab-RL: A software framework for efficient reinforcement learning and  cognitive modeling research</h3>
<ul>
<li><strong>Authors: </strong>Jan Dohmen, Frank Röder, Manfred Eppe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14488">https://arxiv.org/abs/2401.14488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14488">https://arxiv.org/pdf/2401.14488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14488]] Scilab-RL: A software framework for efficient reinforcement learning and  cognitive modeling research(https://arxiv.org/abs/2401.14488)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>One problem with researching cognitive modeling and reinforcement learning (RL) is that researchers spend too much time on setting up an appropriate computational framework for their experiments. Many open source implementations of current RL algorithms exist, but there is a lack of a modular suite of tools combining different robotic simulators and platforms, data visualization, hyperparameter optimization, and baseline experiments. To address this problem, we present Scilab-RL, a software framework for efficient research in cognitive modeling and reinforcement learning for robotic agents. The framework focuses on goal-conditioned reinforcement learning using Stable Baselines 3 and the OpenAI gym interface. It enables native possibilities for experiment visualizations and hyperparameter optimization. We describe how these features enable researchers to conduct experiments with minimal time effort, thus maximizing research output.</li>
<li><strong>摘要：</strong>研究认知模型和强化学习 (RL) 的一个问题是，研究人员花费太多时间为他们的实验建立合适的计算框架。目前存在许多 RL 算法的开源实现，但缺乏结合不同机器人模拟器和平台、数据可视化、超参数优化和基线实验的模块化工具套件。为了解决这个问题，我们提出了 Scilab-RL，这是一个软件框架，用于有效研究机器人代理的认知建模和强化学习。该框架侧重于使用 Stable Baselines 3 和 OpenAIgym 界面的目标条件强化学习。它为实验可视化和超参数优化提供了原生可能性。我们描述了这些功能如何使研究人员能够以最少的时间进行实验，从而最大限度地提高研究成果。</li>
</ul>

<h3>Title: LongHealth: A Question Answering Benchmark with Long Clinical Documents</h3>
<ul>
<li><strong>Authors: </strong>Lisa Adams, Felix Busch, Tianyu Han, Jean-Baptiste Excoffier, Matthieu Ortala, Alexander Löser, Hugo JWL. Aerts, Jakob Nikolas Kather, Daniel Truhn, Keno Bressem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14490">https://arxiv.org/abs/2401.14490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14490">https://arxiv.org/pdf/2401.14490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14490]] LongHealth: A Question Answering Benchmark with Long Clinical Documents(https://arxiv.org/abs/2401.14490)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>Background: Recent advancements in large language models (LLMs) offer potential benefits in healthcare, particularly in processing extensive patient records. However, existing benchmarks do not fully assess LLMs' capability in handling real-world, lengthy clinical data. Methods: We present the LongHealth benchmark, comprising 20 detailed fictional patient cases across various diseases, with each case containing 5,090 to 6,754 words. The benchmark challenges LLMs with 400 multiple-choice questions in three categories: information extraction, negation, and sorting, challenging LLMs to extract and interpret information from large clinical documents. Results: We evaluated nine open-source LLMs with a minimum of 16,000 tokens and also included OpenAI's proprietary and cost-efficient GPT-3.5 Turbo for comparison. The highest accuracy was observed for Mixtral-8x7B-Instruct-v0.1, particularly in tasks focused on information retrieval from single and multiple patient documents. However, all models struggled significantly in tasks requiring the identification of missing information, highlighting a critical area for improvement in clinical data interpretation. Conclusion: While LLMs show considerable potential for processing long clinical documents, their current accuracy levels are insufficient for reliable clinical use, especially in scenarios requiring the identification of missing information. The LongHealth benchmark provides a more realistic assessment of LLMs in a healthcare setting and highlights the need for further model refinement for safe and effective clinical application. We make the benchmark and evaluation code publicly available.</li>
<li><strong>摘要：</strong>背景：大语言模型 (LLM) 的最新进展为医疗保健提供了潜在的好处，特别是在处理大量患者记录方面。然而，现有的基准并没有充分评估法学硕士处理现实世界的冗长临床数据的能力。方法：我们提出了 LongHealth 基准，包括 20 个涉及各种疾病的详细虚构患者案例，每个案例包含 5,090 到 6,754 个单词。该基准向法学硕士提出了三类 400 个多项选择题的挑战：信息提取、否定和排序，挑战法学硕士从大型临床文档中提取和解释信息。结果：我们评估了 9 个开源 LLM，至少有 16,000 个代币，还包括 OpenAI 专有且经济高效的 GPT-3.5 Turbo 进行比较。 Mixtral-8x7B-Instruct-v0.1 的准确性最高，特别是在专注于从单个和多个患者文档中检索信息的任务中。然而，所有模型在需要识别缺失信息的任务中都表现不佳，这凸显了临床数据解释改进的关键领域。结论：虽然法学硕士在处理长临床文档方面显示出巨大的潜力，但其当前的准确性水平不足以可靠的临床使用，特别是在需要识别缺失信息的情况下。 LongHealth 基准为医疗保健环境中的法学硕士提供了更现实的评估，并强调需要进一步完善模型以实现安全有效的临床应用。我们公开提供基准测试和评估代码。</li>
</ul>

<h3>Title: K-QA: A Real-World Medical Q&A Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Itay Manes, Naama Ronn, David Cohen, Ran Ilan Ber, Zehavi Horowitz-Kugler, Gabriel Stanovsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14493">https://arxiv.org/abs/2401.14493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14493">https://arxiv.org/pdf/2401.14493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14493]] K-QA: A Real-World Medical Q&A Benchmark(https://arxiv.org/abs/2401.14493)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Ensuring the accuracy of responses provided by large language models (LLMs) is crucial, particularly in clinical settings where incorrect information may directly impact patient health. To address this challenge, we construct K-QA, a dataset containing 1,212 patient questions originating from real-world conversations held on K Health (an AI-driven clinical platform). We employ a panel of in-house physicians to answer and manually decompose a subset of K-QA into self-contained statements. Additionally, we formulate two NLI-based evaluation metrics approximating recall and precision: (1) comprehensiveness, measuring the percentage of essential clinical information in the generated answer and (2) hallucination rate, measuring the number of statements from the physician-curated response contradicted by the LLM answer. Finally, we use K-QA along with these metrics to evaluate several state-of-the-art models, as well as the effect of in-context learning and medically-oriented augmented retrieval schemes developed by the authors. Our findings indicate that in-context learning improves the comprehensiveness of the models, and augmented retrieval is effective in reducing hallucinations. We make K-QA available to to the community to spur research into medically accurate NLP applications.</li>
<li><strong>摘要：</strong>确保大语言模型 (LLM) 提供的响应的准确性至关重要，特别是在错误信息可能直接影响患者健康的临床环境中。为了应对这一挑战，我们构建了 K-QA，这是一个包含 1,212 个患者问题的数据集，这些问题源自 K Health（一个人工智能驱动的临床平台）上进行的现实世界对话。我们聘请了一组内部医生来回答并手动将 K-QA 的子集分解为独立的语句。此外，我们制定了两个基于 NLI 的评估指标，近似召回率和精确率：(1) 全面性，衡量生成答案中基本临床信息的百分比；(2) 幻觉率，衡量医生策划的反应中矛盾的陈述数量根据LLM的回答。最后，我们使用 K-QA 以及这些指标来评估几种最先进的模型，以及作者开发的上下文学习和面向医学的增强检索方案的效果。我们的研究结果表明，情境学习提高了模型的全面性，增强检索可以有效减少幻觉。我们向社区提供 K-QA，以促进医学上准确的 NLP 应用的研究。</li>
</ul>

<h3>Title: Evaluating GPT-3.5's Awareness and Summarization Abilities for European  Constitutional Texts with Shared Topics</h3>
<ul>
<li><strong>Authors: </strong>Candida M. Greco, A. Tagarelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.DL, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14524">https://arxiv.org/abs/2401.14524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14524">https://arxiv.org/pdf/2401.14524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14524]] Evaluating GPT-3.5's Awareness and Summarization Abilities for European  Constitutional Texts with Shared Topics(https://arxiv.org/abs/2401.14524)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, rag</a></li>
<li><strong>Abstract: </strong>Constitutions are foundational legal documents that underpin the governmental and societal structures. As such, they are a reflection of a nation's cultural and social uniqueness, but also contribute to establish topics of universal importance, like citizens' rights and duties (RD). In this work, using the renowned GPT-3.5, we leverage generative large language models to understand constitutional passages that transcend national boundaries. A key contribution of our study is the introduction of a novel application of abstractive summarization on a multi-source collection of constitutional texts, with a focus on European countries' constitution passages related to RD topics. Our results show the meaningfulness of GPT-3.5 to produce informative, coherent and faithful summaries capturing RD topics across European countries.</li>
<li><strong>摘要：</strong>宪法是支撑政府和社会结构的基本法律文件。因此，它们反映了一个国家的文化和社会独特性，但也有助于建立具有普遍重要性的主题，例如公民的权利和义务（RD）。在这项工作中，我们使用著名的 GPT-3.5，利用生成式大语言模型来理解超越国界的宪法段落。我们研究的一个关键贡献是在多源宪法文本集合上引入了抽象概括的新颖应用，重点关注欧洲国家与 RD 主题相关的宪法段落。我们的结果表明，GPT-3.5 在生成涵盖欧洲国家的 RD 主题的信息丰富、连贯且忠实的摘要方面具有重要意义。</li>
</ul>

<h3>Title: Relative Value Biases in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>William M. Hayes, Nicolas Yax, Stefano Palminteri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14530">https://arxiv.org/abs/2401.14530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14530">https://arxiv.org/pdf/2401.14530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14530]] Relative Value Biases in Large Language Models(https://arxiv.org/abs/2401.14530)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, agent</a></li>
<li><strong>Abstract: </strong>Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.</li>
<li><strong>摘要：</strong>对人类和动物强化学习的研究表明，人们偏爱过去能产生相对更好结果的选项，即使这些选项与较低的绝对奖励相关。本研究测试了大型语言模型是否会表现出类似的偏差。我们让 gpt-4-1106-preview (GPT-4 Turbo) 和 Llama-2-70B 在选项对之间进行重复选择，以实现收益最大化。每个提示中都包含先前结果的完整记录。两种模型都表现出与人类和动物中观察到的相似的相对价值决策偏差。对结果进行更明确的相对比较会放大偏差，而促使模型估计预期结果会导致偏差消失。这些结果对促进人类主体的情境依赖选择的潜在机制具有影响。</li>
</ul>

<h3>Title: Bayesian Optimization through Gaussian Cox Process Models for  Spatio-temporal Data</h3>
<ul>
<li><strong>Authors: </strong>Yongsheng Mei, Mahdi Imani, Tian Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, math.FA, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14544">https://arxiv.org/abs/2401.14544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14544">https://arxiv.org/pdf/2401.14544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14544]] Bayesian Optimization through Gaussian Cox Process Models for  Spatio-temporal Data(https://arxiv.org/abs/2401.14544)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Bayesian optimization (BO) has established itself as a leading strategy for efficiently optimizing expensive-to-evaluate functions. Existing BO methods mostly rely on Gaussian process (GP) surrogate models and are not applicable to (doubly-stochastic) Gaussian Cox processes, where the observation process is modulated by a latent intensity function modeled as a GP. In this paper, we propose a novel maximum a posteriori inference of Gaussian Cox processes. It leverages the Laplace approximation and change of kernel technique to transform the problem into a new reproducing kernel Hilbert space, where it becomes more tractable computationally. It enables us to obtain both a functional posterior of the latent intensity function and the covariance of the posterior, thus extending existing works that often focus on specific link functions or estimating the posterior mean. Using the result, we propose a BO framework based on the Gaussian Cox process model and further develop a Nystr\"om approximation for efficient computation. Extensive evaluations on various synthetic and real-world datasets demonstrate significant improvement over state-of-the-art inference solutions for Gaussian Cox processes, as well as effective BO with a wide range of acquisition functions designed through the underlying Gaussian Cox process model.</li>
<li><strong>摘要：</strong>贝叶斯优化 (BO) 已成为有效优化评估成本高昂的函数的领先策略。现有的 BO 方法主要依赖于高斯过程 (GP) 代理模型，不适用于（双随机）高斯 Cox 过程，其中观测过程由建模为 GP 的潜在强度函数进行调制。在本文中，我们提出了一种新颖的高斯 Cox 过程的最大后验推理。它利用拉普拉斯近似和核技术的变化将问题转换为新的再现核希尔伯特空间，在计算上变得更容易处理。它使我们能够获得潜在强度函数的函数后验和后验的协方差，从而扩展了通常专注于特定链接函数或估计后验均值的现有工作。利用结果，我们提出了一个基于高斯 Cox 过程模型的 BO 框架，并进一步开发了 Nystr\"om 近似以实现高效计算。对各种合成和真实数据集的广泛评估表明，与最先进的技术相比，有了显着的改进高斯 Cox 过程的推理解决方案，以及通过底层高斯 Cox 过程模型设计的具有广泛采集功能的有效 BO。</li>
</ul>

<h3>Title: Do Not (Always) Look Right: Investigating the Capabilities of  Decoder-Based Large Language Models for Sequence Labeling</h3>
<ul>
<li><strong>Authors: </strong>David Dukić, Jan Šnajder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14556">https://arxiv.org/abs/2401.14556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14556">https://arxiv.org/pdf/2401.14556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14556]] Do Not (Always) Look Right: Investigating the Capabilities of  Decoder-Based Large Language Models for Sequence Labeling(https://arxiv.org/abs/2401.14556)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Pre-trained language models based on masked language modeling (MLM) objective excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, a recent trend of scaling decoder models to multiple billion parameters resulted in large language models (LLMs), making them competitive with MLM-based encoders. Although scale amplifies their prowess in NLU tasks, LLMs fall short of SOTA results in information extraction (IE) tasks, many framed as sequence labeling (SL). However, whether this is an intrinsic limitation of LLMs or whether their SL performance can be improved remains unclear. To address this, we explore strategies to enhance the SL performance of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional information flow within groups of decoder blocks, applying layer-wise removal or enforcement of the causal mask (CM) during LLM fine-tuning. This approach yields performance gains competitive with SOTA SL models, matching or outperforming the results of CM removal from all blocks. Our findings hold for diverse SL tasks, proving that "open" LLMs with layer-dependent CM removal outperform strong MLM-based encoders and instruction-tuned LLMs. However, we observe no effect from CM removal on a small scale when maintaining an equivalent model size, pre-training steps, and pre-training and fine-tuning data.</li>
<li><strong>摘要：</strong>基于掩码语言建模 (MLM) 目标的预训练语言模型在自然语言理解 (NLU) 任务中表现出色。虽然经过微调的基于 MLM 的编码器始终优于同等大小的因果语言建模解码器，但最近将解码器模型扩展到数十亿个参数的趋势导致了大型语言模型 (LLM)，使其与基于 MLM 的编码器具有竞争力。尽管规模扩大了他们在 NLU 任务中的能力，但法学硕士在信息提取 (IE) 任务中仍达不到 SOTA 的结果，其中许多任务被称为序列标记 (SL)。然而，这是否是法学硕士的内在限制，或者他们的 SL 表现是否可以提高仍不清楚。为了解决这个问题，我们探索了增强“开放”LLM（Llama2 和 Mistral）在 IE 任务上的 SL 性能的策略。我们研究解码器块组内的双向信息流，在 LLM 微调期间应用逐层删除或强制执行因果掩码 (CM)。这种方法产生的性能增益与 SOTA SL 模型相比具有竞争力，匹配或优于从所有块中去除 CM 的结果。我们的研究结果适用于各种 SL 任务，证明具有依赖于层的 CM 去除的“开放”LLM 优于基于 MLM 的强大编码器和指令调整的 LLM。然而，当保持等效的模型大小、预训练步骤以及预训练和微调数据时，我们观察到小规模的 CM 去除没有影响。</li>
</ul>

<h3>Title: Language Modelling Approaches to Adaptive Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Yasmin Moslem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14559">https://arxiv.org/abs/2401.14559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14559">https://arxiv.org/pdf/2401.14559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14559]] Language Modelling Approaches to Adaptive Machine Translation(https://arxiv.org/abs/2401.14559)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. While real-time adaptation can make use of smaller amounts of in-domain data to improve the translation on the fly, it remains challenging due to supported context limitations and efficiency constraints. Large language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. Such capabilities have opened new horizons for domain-specific data augmentation and real-time adaptive MT. This work attempts to address two main relevant questions: 1) in scenarios involving human interaction and continuous feedback, can we employ language models to improve the quality of adaptive MT at inference time? and 2) in the absence of sufficient in-domain data, can we use pre-trained large-scale language models to improve the process of MT domain adaptation?</li>
<li><strong>摘要：</strong>一致性是高质量翻译的关键要求。遵守预先批准的术语并适应特定领域项目中更正的翻译尤为重要。机器翻译（MT）在领域适应领域取得了重大进展。然而，由于缺乏专门的数据集和术语，或者可用的域内翻译不一致和不准确，域内数据稀缺在翻译环境中很常见。在没有足够的领域内数据来微调机器翻译模型的情况下，生成与相关上下文一致的翻译是一项挑战。虽然实时适应可以利用少量的域内数据来改进动态翻译，但由于支持的上下文限制和效率限制，它仍然具有挑战性。大型语言模型（LLM）最近展示了上下文学习的有趣功能，它们学习复制某些输入输出文本生成模式，而无需进一步微调。这些功能为特定领域的数据增强和实时自适应机器翻译开辟了新的视野。这项工作试图解决两个主要相关问题：1）在涉及人类交互和持续反馈的场景中，我们是否可以利用语言模型来提高推理时自适应机器翻译的质量？ 2）在缺乏足够的领域内数据的情况下，我们是否可以使用预先训练的大规模语言模型来改进MT领域适应的过程？</li>
</ul>

<h3>Title: Diffusion Stochastic Optimization for Min-Max Problems</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Cai, Sulaiman A. Alghunaim, Ali H. Sayed</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14585">https://arxiv.org/abs/2401.14585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14585">https://arxiv.org/pdf/2401.14585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14585]] Diffusion Stochastic Optimization for Min-Max Problems(https://arxiv.org/abs/2401.14585)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The optimistic gradient method is useful in addressing minimax optimization problems. Motivated by the observation that the conventional stochastic version suffers from the need for a large batch size on the order of $\mathcal{O}(\varepsilon^{-2})$ to achieve an $\varepsilon$-stationary solution, we introduce and analyze a new formulation termed Diffusion Stochastic Same-Sample Optimistic Gradient (DSS-OG). We prove its convergence and resolve the large batch issue by establishing a tighter upper bound, under the more general setting of nonconvex Polyak-Lojasiewicz (PL) risk functions. We also extend the applicability of the proposed method to the distributed scenario, where agents communicate with their neighbors via a left-stochastic protocol. To implement DSS-OG, we can query the stochastic gradient oracles in parallel with some extra memory overhead, resulting in a complexity comparable to its conventional counterpart. To demonstrate the efficacy of the proposed algorithm, we conduct tests by training generative adversarial networks.</li>
<li><strong>摘要：</strong>乐观梯度法对于解决极小极大优化问题很有用。由于观察到传统的随机版本需要 $\mathcal{O}(\varepsilon^{-2})$ 数量级的大批量大小才能实现 $\varepsilon$ 平稳解决方案，我们介绍并分析一种称为扩散随机同样本乐观梯度 (DSS-OG) 的新公式。我们在非凸 Polyak-Lojasiewicz (PL) 风险函数的更一般设置下，通过建立更严格的上限来证明其收敛性并解决大批量问题。我们还将所提出的方法的适用性扩展到分布式场景，其中代理通过左随机协议与邻居进行通信。为了实现 DSS-OG，我们可以并行查询随机梯度预言机，但需要一些额外的内存开销，从而产生与传统对应物相当的复杂性。为了证明所提出算法的有效性，我们通过训练生成对抗网络进行测试。</li>
</ul>

<h3>Title: Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using  Large Language Models to Mitigate Cognitive Bias</h3>
<ul>
<li><strong>Authors: </strong>Yu He Ke, Rui Yang, Sui An Lie, Taylor Xin Yi Lim, Hairil Rizal Abdullah, Daniel Shu Wei Ting, Nan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14589">https://arxiv.org/abs/2401.14589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14589">https://arxiv.org/pdf/2401.14589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14589]] Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using  Large Language Models to Mitigate Cognitive Bias(https://arxiv.org/abs/2401.14589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>Background: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. This study explores the role of large language models (LLMs) in mitigating these biases through the utilization of a multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy. Methods: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent system, we leveraged GPT-4 Turbo to facilitate interactions among four simulated agents to replicate clinical team dynamics. Each agent has a distinct role: 1) To make the initial and final diagnosis after considering the discussions, 2) The devil's advocate and correct confirmation and anchoring bias, 3) The tutor and facilitator of the discussion to reduce premature closure bias, and 4) To record and summarize the findings. A total of 80 simulations were evaluated for the accuracy of initial diagnosis, top differential diagnosis and final two differential diagnoses. Findings: In a total of 80 responses evaluating both initial and final diagnoses, the initial diagnosis had an accuracy of 0% (0/80), but following multi-agent discussions, the accuracy for the top differential diagnosis increased to 71.3% (57/80), and for the final two differential diagnoses, to 80.0% (64/80). The system demonstrated an ability to reevaluate and correct misconceptions, even in scenarios with misleading initial investigations. Interpretation: The LLM-driven multi-agent conversation system shows promise in enhancing diagnostic accuracy in diagnostically challenging medical scenarios.</li>
<li><strong>摘要：</strong>背景：临床决策中的认知偏差会显着导致诊断错误和患者结果不佳。解决这些偏见给医学领域带来了巨大的挑战。本研究探讨了大型语言模型 (LLM) 通过利用多智能体框架来减轻这些偏差的作用。我们通过多智能体对话模拟临床决策过程，并评估其在提高诊断准确性方面的功效。方法：从文献中总共识别出 16 篇已发表和未发表的认知偏差导致误诊的病例报告。在多智能体系统中，我们利用 GPT-4 Turbo 促进四个模拟智能体之间的交互，以复制临床团队的动态。每个代理都有独特的角色：1）在考虑讨论后做出初步和最终诊断，2）魔鬼代言人和正确的确认和锚定偏差，3）讨论的导师和促进者，以减少过早结束偏差，以及4 ) 记录并总结调查结果。总共80次模拟评估了初始诊断、顶级鉴别诊断和最终两次鉴别诊断的准确性。结果：在评估初始和最终诊断的总共 80 份回复中，初始诊断的准确性为 0% (0/80)，但经过多方讨论后，最重要的鉴别诊断的准确性增加到 71.3% (57 /80)，对于最后两个鉴别诊断，达到 80.0% (64/80)。该系统表现出重新评估和纠正误解的能力，即使在初始调查具有误导性的情况下也是如此。解读：法学硕士驱动的多智能体对话系统有望在具有诊断挑战性的医疗场景中提高诊断准确性。</li>
</ul>

<h3>Title: Ricci flow-guided autoencoders in learning time-dependent dynamics</h3>
<ul>
<li><strong>Authors: </strong>Andrew Gracyk</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14591">https://arxiv.org/abs/2401.14591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14591">https://arxiv.org/pdf/2401.14591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14591]] Ricci flow-guided autoencoders in learning time-dependent dynamics(https://arxiv.org/abs/2401.14591)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.</li>
<li><strong>摘要：</strong>我们提出了一种基于流形的自动编码器方法，用于及时学习非线性动力学，特别是偏微分方程（PDE），其中流形潜在空间根据 Ricci 流演化。这可以通过在物理信息环境中模拟里奇流来实现，并且可以匹配流形量，以便凭经验实现里奇流。通过我们的方法，流形是作为训练过程的一部分来学习的，因此可以辨别理想的几何形状，而进化同时会产生比静态方法更适应的潜在表示。我们在一系列由偏微分方程组成的数值实验上展示了我们的方法，这些实验包含周期性和随机性等理想特征，并在分布和外推场景中标记误差。</li>
</ul>

<h3>Title: Query of CC: Unearthing Large Scale Domain-Specific Knowledge from  Public Corpora</h3>
<ul>
<li><strong>Authors: </strong>Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng Qiu, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14624">https://arxiv.org/abs/2401.14624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14624">https://arxiv.org/pdf/2401.14624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14624]] Query of CC: Unearthing Large Scale Domain-Specific Knowledge from  Public Corpora(https://arxiv.org/abs/2401.14624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated remarkable potential in various tasks, however, there remains a significant scarcity of open-source models and data for specific domains. Previous works have primarily focused on manually specifying resources and collecting high-quality data on specific domains, which significantly consume time and effort. To address this limitation, we propose an efficient data collection method~\textit{Query of CC} based on large language models. This method bootstraps seed information through a large language model and retrieves related data from public corpora. It not only collects knowledge-related data for specific domains but unearths the data with potential reasoning procedures. Through the application of this method, we have curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing four major domains, including stem and humanities sciences, among others. Experimental results demonstrate that~\textsc{Knowledge Pile} significantly improves the performance of large language models in mathematical and knowledge-related reasoning ability tests. To facilitate academic sharing, we open-source our dataset and code, providing valuable support to the academic community.</li>
<li><strong>摘要：</strong>大型语言模型在各种任务中表现出了巨大的潜力，但是，特定领域的开源模型和数据仍然严重缺乏。以前的工作主要集中在手动指定资源并收集特定领域的高质量数据，这极大地消耗了时间和精力。为了解决这个限制，我们提出了一种基于大语言模型的高效数据收集方法~\textit{CC的查询}。该方法通过大型语言模型引导种子信息，并从公共语料库中检索相关数据。它不仅收集特定领域的知识相关数据，而且通过潜在的推理过程挖掘数据。通过应用这种方法，我们策划了一个名为~\textsc{知识堆}的高质量数据集，涵盖四个主要领域，包括干科学和人文科学等。实验结果表明~\textsc{知识堆}显着提高了大型语言模型在数学和知识相关推理能力测试中的性能。为了促进学术共享，我们开源我们的数据集和代码，为学术界提供宝贵的支持。</li>
</ul>

<h3>Title: An Empirical Investigation of Domain Adaptation Ability for Chinese  Spelling Check Models</h3>
<ul>
<li><strong>Authors: </strong>Xi Wang, Ruoqing Zhao, Hongliang Dai, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14630">https://arxiv.org/abs/2401.14630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14630">https://arxiv.org/pdf/2401.14630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14630]] An Empirical Investigation of Domain Adaptation Ability for Chinese  Spelling Check Models(https://arxiv.org/abs/2401.14630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Chinese Spelling Check (CSC) is a meaningful task in the area of Natural Language Processing (NLP) which aims at detecting spelling errors in Chinese texts and then correcting these errors. However, CSC models are based on pretrained language models, which are trained on a general corpus. Consequently, their performance may drop when confronted with downstream tasks involving domain-specific terms. In this paper, we conduct a thorough evaluation about the domain adaption ability of various typical CSC models by building three new datasets encompassing rich domain-specific terms from the financial, medical, and legal domains. Then we conduct empirical investigations in the corresponding domain-specific test datasets to ascertain the cross-domain adaptation ability of several typical CSC models. We also test the performance of the popular large language model ChatGPT. As shown in our experiments, the performances of the CSC models drop significantly in the new domains.</li>
<li><strong>摘要：</strong>中文拼写检查（CSC）是自然语言处理（NLP）领域的一项有意义的任务，旨在检测中文文本中的拼写错误，然后纠正这些错误。然而，CSC 模型基于预训练的语言模型，这些模型是在通用语料库上训练的。因此，当遇到涉及特定领域术语的下游任务时，他们的性能可能会下降。在本文中，我们通过构建三个包含来自金融、医疗和法律领域的丰富领域特定术语的新数据集，对各种典型 CSC 模型的领域适应能力进行了全面评估。然后，我们在相应的特定领域测试数据集中进行实证研究，以确定几种典型 CSC 模型的跨领域适应能力。我们还测试了流行的大语言模型ChatGPT的性能。正如我们的实验所示，CSC 模型的性能在新领域中显着下降。</li>
</ul>

<h3>Title: Efficient Constraint Generation for Stochastic Shortest Path Problems</h3>
<ul>
<li><strong>Authors: </strong>Johannes Schmalz, Felipe Trevizan</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14636">https://arxiv.org/abs/2401.14636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14636">https://arxiv.org/pdf/2401.14636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14636]] Efficient Constraint Generation for Stochastic Shortest Path Problems(https://arxiv.org/abs/2401.14636)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Current methods for solving Stochastic Shortest Path Problems (SSPs) find states' costs-to-go by applying Bellman backups, where state-of-the-art methods employ heuristics to select states to back up and prune. A fundamental limitation of these algorithms is their need to compute the cost-to-go for every applicable action during each state backup, leading to unnecessary computation for actions identified as sub-optimal. We present new connections between planning and operations research and, using this framework, we address this issue of unnecessary computation by introducing an efficient version of constraint generation for SSPs. This technique allows algorithms to ignore sub-optimal actions and avoid computing their costs-to-go. We also apply our novel technique to iLAO* resulting in a new algorithm, CG-iLAO*. Our experiments show that CG-iLAO* ignores up to 57% of iLAO*'s actions and it solves problems up to 8x and 3x faster than LRTDP and iLAO*.</li>
<li><strong>摘要：</strong>当前解决随机最短路径问题（SSP）的方法通过应用贝尔曼备份来查找状态的运行成本，其中最先进的方法采用启发式方法来选择要备份和修剪的状态。这些算法的一个基本限制是它们需要计算每个状态备份期间每个适用操作的执行成本，从而导致对被识别为次优的操作进行不必要的计算。我们提出了规划和运筹学之间的新联系，并使用该框架，通过引入 SSP 约束生成的高效版本来解决不必要的计算问题。这种技术允许算法忽略次优动作并避免计算其执行成本。我们还将我们的新技术应用于 iLAO*，从而产生了新算法 CG-iLAO*。我们的实验表明，CG-iLAO* 忽略了高达 57% 的 iLAO* 操作，并且解决问题的速度比 LRTDP 和 iLAO* 快 8 倍和 3 倍。</li>
</ul>

<h3>Title: T-Rex: Text-assisted Retrosynthesis Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Liu, Hanwen Xu, Tangqi Fang, Haocheng Xi, Zixuan Liu, Sheng Zhang, Hoifung Poon, Sheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14637">https://arxiv.org/abs/2401.14637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14637">https://arxiv.org/pdf/2401.14637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14637]] T-Rex: Text-assisted Retrosynthesis Prediction(https://arxiv.org/abs/2401.14637)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code, chat</a></li>
<li><strong>Abstract: </strong>As a fundamental task in computational chemistry, retrosynthesis prediction aims to identify a set of reactants to synthesize a target molecule. Existing template-free approaches only consider the graph structures of the target molecule, which often cannot generalize well to rare reaction types and large molecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction approach that exploits pre-trained text language models, such as ChatGPT, to assist the generation of reactants. T-Rex first exploits ChatGPT to generate a description for the target molecule and rank candidate reaction centers based both the description and the molecular graph. It then re-ranks these candidates by querying the descriptions for each reactants and examines which group of reactants can best synthesize the target molecule. We observed that T-Rex substantially outperformed graph-based state-of-the-art approaches on two datasets, indicating the effectiveness of considering text information. We further found that T-Rex outperformed the variant that only use ChatGPT-based description without the re-ranking step, demonstrate how our framework outperformed a straightforward integration of ChatGPT and graph information. Collectively, we show that text generated by pre-trained language models can substantially improve retrosynthesis prediction, opening up new avenues for exploiting ChatGPT to advance computational chemistry. And the codes can be found at https://github.com/lauyikfung/T-Rex.</li>
<li><strong>摘要：</strong>作为计算化学的一项基本任务，逆合成预测旨在识别一组反应物来合成目标分子。现有的无模板方法仅考虑目标分子的图形结构，通常不能很好地推广到罕见的反应类型和大分子。在这里，我们提出了 T-Rex，一种文本辅助逆合成预测方法，利用预先训练的文本语言模型（例如 ChatGPT）来辅助反应物的生成。 T-Rex 首先利用 ChatGPT 生成目标分子的描述，并根据描述和分子图对候选反应中心进行排名。然后，它通过查询每种反应物的描述来重新排列这些候选物，并检查哪组反应物可以最好地合成目标分子。我们观察到，T-Rex 在两个数据集上的表现远远优于基于图的最先进方法，这表明考虑文本信息的有效性。我们进一步发现，T-Rex 优于仅使用基于 ChatGPT 的描述而无需重新排名步骤的变体，展示了我们的框架如何优于 ChatGPT 和图形信息的直接集成。总的来说，我们表明，由预先训练的语言模型生成的文本可以显着改善逆合成预测，为利用 ChatGPT 推进计算化学开辟新途径。代码可以在 https://github.com/lauyikfung/T-Rex 找到。</li>
</ul>

<h3>Title: Benchmarking Large Language Models in Complex Question Answering  Attribution using Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong Wu, Jeff Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14640">https://arxiv.org/abs/2401.14640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14640">https://arxiv.org/pdf/2401.14640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14640]] Benchmarking Large Language Models in Complex Question Answering  Attribution using Knowledge Graphs(https://arxiv.org/abs/2401.14640)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>The attribution of question answering is to provide citations for supporting generated statements, and has attracted wide research attention. The current methods for automatically evaluating the attribution, which are often based on Large Language Models (LLMs), are still inadequate, particularly in recognizing subtle differences between attributions, and complex relationships between citations and statements. To compare these attribution evaluation methods and develop new ones, we introduce a set of fine-grained categories (i.e., supportive, insufficient, contradictory and irrelevant) for measuring the attribution, and develop a Complex Attributed Question Answering (CAQA) benchmark by leveraging knowledge graphs (KGs) for automatically generating attributions of different categories to question-answer pairs. Our analysis reveals that existing evaluators perform poorly under fine-grained attribution settings and exhibit weaknesses in complex citation-statement reasoning. Our CAQA benchmark, validated with human annotations, emerges as a promising tool for selecting and developing LLM attribution evaluators.</li>
<li><strong>摘要：</strong>问答的归因是为支持生成的陈述提供引用，并引起了广泛的研究关注。当前自动评估归因的方法通常基于大型语言模型（LLM），但仍然不足，特别是在识别归因之间的细微差异以及引文和陈述之间的复杂关系方面。为了比较这些归因评估方法并开发新的归因评估方法，我们引入了一组细粒度类别（即支持、不足、矛盾和不相关）来衡量归因，并利用知识开发了复杂归因问答（CAQA）基准用于自动生成不同类别对问答对的归因的图（KG）。我们的分析表明，现有的评估者在细粒度归因设置下表现不佳，并且在复杂的引文陈述推理中表现出弱点。我们的 CAQA 基准经过人工注释验证，成为选择和开发 LLM 归因评估器的有前景的工具。</li>
</ul>

<h3>Title: Scientific Large Language Models: A Survey on Biological & Chemical  Domains</h3>
<ul>
<li><strong>Authors: </strong>Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Mengyao Zhang, Jinlu Zhang, Jiyu Cui, Renjun Xu, Hongyang Chen, Xiaohui Fan, Huabin Xing, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14656">https://arxiv.org/abs/2401.14656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14656">https://arxiv.org/pdf/2401.14656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14656]] Scientific Large Language Models: A Survey on Biological & Chemical  Domains(https://arxiv.org/abs/2401.14656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of "scientific language", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为增强自然语言理解的变革力量，代表着通用人工智能的重大进步。法学硕士的应用超越了传统的语言界限，涵盖了各种科学学科中开发的专业语言系统。这种日益增长的兴趣导致了科学法学硕士的出现，这是一个专门为促进科学发现而设计的新型子类别。作为科学人工智能领域的一个新兴领域，科学法学硕士值得全面探索。然而，目前缺乏系统的、最新的介绍它们的调查。在本文中，我们努力系统地描述“科学语言”的概念，同时对科学法学硕士的最新进展进行全面回顾。鉴于科学学科领域广阔，我们的分析采用聚焦镜头，重点关注生物和化学领域。这包括对法学硕士的文本知识、小分子、大分子蛋白质、基因组序列及其组合进行深入检查，并根据模型架构、功能、数据集和评估对其进行分析。最后，我们批判性地审视当前的挑战，并指出有希望的研究方向以及法学硕士的进步。通过全面概述该领域的技术发展，本次调查希望成为研究人员在科学法学硕士错综复杂的领域中探索的宝贵资源。</li>
</ul>

<h3>Title: MaLLaM -- Malaysia Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14680">https://arxiv.org/abs/2401.14680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14680">https://arxiv.org/pdf/2401.14680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14680]] MaLLaM -- Malaysia Large Language Model(https://arxiv.org/abs/2401.14680)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Addressing the gap in Large Language Model pretrained from scratch with Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion parameters on a substantial 349GB dataset, equivalent to 90 billion tokens based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch. MaLLaM contributes to enhanced natural language understanding and generation tasks in the Malay language. Although trained on a smaller dataset of 90 billion tokens, our instruction-tuned MaLLaM models perform competitively. When compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models demonstrate notable proficiency, underscoring the effectiveness of our approach in capturing and understanding the nuances of the Malaysian language. MaLLaM models mark a significant contribution to the field, providing comprehensive language representations grounded in Malaysian context. This endeavor aims to pave the way for enhanced natural language understanding and generation tasks specific to the linguistic nuances present in Malaysia. We discuss the training methodology, dataset composition, and the potential impact of MaLLaM in advancing the capabilities of large language models within the context of the Malay language. All models released at https://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f</li>
<li><strong>摘要：</strong>为了解决马来西亚语境中从头开始预训练的大型语言模型的差距，我们在 349GB 的大量数据集上训练了具有 11 亿、30 亿和 50 亿个参数的模型，相当于基于我们预训练的字节对编码 (BPE) 标记器的 900 亿个标记对于单个纪元。 MaLLaM 有助于增强马来语的自然语言理解和生成任务。尽管是在包含 900 亿个代币的较小数据集上进行训练，但我们的指令调整 MaLLaM 模型的表现仍具有竞争力。与 ChatGPT3.5 和 Malaysia Mistral 相比，MaLLaM 的指令调整模型表现出显着的熟练程度，强调了我们的方法在捕获和理解马来西亚语言的细微差别方面的有效性。 MaLLaM 模型标志着对该领域的重大贡献，提供了基于马来西亚语境的全面语言表示。这项努力旨在为增强自然语言理解和针对马来西亚存在的语言细微差别的生成任务铺平道路。我们讨论了训练方法、数据集组成以及 MaLLaM 在马来语背景下提升大型语言模型能力的潜在影响。所有模型发布于 https://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f</li>
</ul>

<h3>Title: Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with  Large Vision-Language Model Support</h3>
<ul>
<li><strong>Authors: </strong>Xiaojun Wu, Dixiang Zhang, Ruyi Gan, Junyu Lu, Ziwei Wu, Renliang Sun, Jiaxing Zhang, Pingjian Zhang, Yan Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14688">https://arxiv.org/abs/2401.14688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14688">https://arxiv.org/pdf/2401.14688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14688]] Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with  Large Vision-Language Model Support(https://arxiv.org/abs/2401.14688)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilingual image generation capabilities of Taiyi-Diffusion-XL surpass previous models. This research leads to the development and open-sourcing of the Taiyi-Diffusion-XL model, representing a notable advancement in the field of image generation, particularly for Chinese language applications. This contribution is a step forward in addressing the need for more diverse language support in multimodal research. The model and demonstration are made publicly available at \href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this https URL}, fostering further research and collaboration in this domain.</li>
<li><strong>摘要：</strong>文本到图像模型的最新进展显着增强了图像生成能力，但开源模型在双语或中文支持方面仍然存在显着差距。为了满足这一需求，我们提出了 Taiyi-Diffusion-XL，这是一种新的中英文双语文本到图像模型，它是通过双语连续预训练过程扩展 CLIP 和 Stable-Diffusion-XL 的功能而开发的。该方法包括通过将最常用的汉字集成到 CLIP 的标记器和嵌入层中来有效扩展词汇量，并结合绝对位置编码扩展。此外，我们通过大型视觉语言模型丰富了文本提示，从而获得更好的图像标题并拥有更高的视觉质量。这些增强功能随后应用于下游文本到图像模型。我们的实证结果表明，所开发的 CLIP 模型在双语图像文本检索方面表现出色。此外，Taiyi-Diffusion-XL 的双语图像生成能力超越了以前的模型。这项研究导致了 Taiyi-Diffusion-XL 模型的开发和开源，代表了图像生成领域的显着进步，特别是对于中文应用。这一贡献是解决多模态研究中对更多样化语言支持的需求向前迈出的一步。该模型和演示在 \href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this https URL} 上公开发布，促进了该领域的进一步研究和合作。</li>
</ul>

<h3>Title: TA-RNN: an Attention-based Time-aware Recurrent Neural Network  Architecture for Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Al Olaimat (1, 3), Serdar Bozdag (1, 2 and 3), the Alzheimer's Disease Neuroimaging Initiative ((1) Dept. of Computer Science and Engineering, University of North Texas, Denton, USA, (2) Dept. of Mathematics, University of North Texas, Denton, USA, (3) BioDiscovery Institute, University of North Texas, Denton, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14694">https://arxiv.org/abs/2401.14694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14694">https://arxiv.org/pdf/2401.14694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14694]] TA-RNN: an Attention-based Time-aware Recurrent Neural Network  Architecture for Electronic Health Records(https://arxiv.org/abs/2401.14694)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elapsed times between visits. For interpretability, we propose employing a dual-level attention mechanism that operates between visits and features within each visit. Results: The results of the experiments conducted on Alzheimer's Disease Neuroimaging Initiative (ADNI) and National Alzheimer's Coordinating Center (NACC) datasets indicated superior performance of proposed models for predicting Alzheimer's Disease (AD) compared to state-of-the-art and baseline approaches based on F2 and sensitivity. Additionally, TA-RNN showed superior performance on Medical Information Mart for Intensive Care (MIMIC-III) dataset for mortality prediction. In our ablation study, we observed enhanced predictive performance by incorporating time embedding and attention mechanisms. Finally, investigating attention weights helped identify influential visits and features in predictions. Availability: https://github.com/bozdaglab/TA-RNN</li>
<li><strong>摘要：</strong>动机：电子健康记录 (EHR) 代表患者病史的综合资源。 EHR 对于利用深度学习 (DL) 等先进技术至关重要，使医疗保健提供者能够分析大量数据、提取有价值的见解并做出精确的数据驱动的临床决策。递归神经网络 (RNN) 等深度学习方法已用于分析 EHR，以模拟疾病进展并预测诊断。然而，这些方法并没有解决 EHR 数据中一些固有的不规则性，例如临床就诊之间的不规则时间间隔。此外，大多数深度学习模型都是不可解释的。在本研究中，我们提出了两种基于 RNN 的可解释 DL 架构，即时间感知 RNN (TA-RNN) 和 TA-RNN-自动编码器 (TA-RNN-AE)，以预测患者下次就诊和多次就诊的 EHR 临床结果分别向前。为了减轻不规则时间间隔的影响，我们建议结合访问之间经过的时间的时间嵌入。为了可解释性，我们建议采用双层注意力机制，在访问和每次访问中的特征之间运行。结果：在阿尔茨海默病神经影像计划 (ADNI) 和国家阿尔茨海默病协调中心 (NACC) 数据集上进行的实验结果表明，与最先进的方法和基线方法相比，所提出的模型在预测阿尔茨海默病 (AD) 方面具有优越的性能基于 F2 和灵敏度。此外，TA-RNN 在重症监护医学信息集市 (MIMIC-III) 数据集上的死亡率预测方面表现出了卓越的性能。在我们的消融研究中，我们观察到通过结合时间嵌入和注意力机制来增强预测性能。最后，调查注意力权重有助于识别预测中有影响力的访问和特征。可用性：https://github.com/bozdaglab/TA-RNN</li>
</ul>

<h3>Title: Continuously Evolving Graph Neural Controlled Differential Equations for  Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jiajia Wu, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14695">https://arxiv.org/abs/2401.14695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14695">https://arxiv.org/pdf/2401.14695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14695]] Continuously Evolving Graph Neural Controlled Differential Equations for  Traffic Forecasting(https://arxiv.org/abs/2401.14695)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>As a crucial technique for developing a smart city, traffic forecasting has become a popular research focus in academic and industrial communities for decades. This task is highly challenging due to complex and dynamic spatial-temporal dependencies in traffic networks. Existing works ignore continuous temporal dependencies and spatial dependencies evolving over time. In this paper, we propose Continuously Evolving Graph Neural Controlled Differential Equations (CEGNCDE) to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Specifically, a continuously evolving graph generator (CEGG) based on NCDE is introduced to generate the spatial dependencies graph that continuously evolves over time from discrete historical observations. Then, a graph neural controlled differential equations (GNCDE) framework is introduced to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Extensive experiments demonstrate that CEGNCDE outperforms the SOTA methods by average 2.34% relative MAE reduction, 0.97% relative RMSE reduction, and 3.17% relative MAPE reduction.</li>
<li><strong>摘要：</strong>作为发展智慧城市的关键技术，交通预测几十年来已成为学术界和工业界的热门研究热点。由于交通网络中复杂且动态的时空依赖性，这项任务极具挑战性。现有的作品忽略了随时间演变的连续时间依赖性和空间依赖性。在本文中，我们提出了连续演化图神经控制微分方程（CEGNCDE）来同时捕获随时间变化的连续时间依赖性和空间依赖性。具体来说，引入了基于 NCDE 的连续演化图生成器（CEGG），以根据离散历史观测值生成随时间不断演化的空间依赖图。然后，引入图神经控制微分方程（GNCDE）框架来同时捕获随时间变化的连续时间依赖性和空间依赖性。大量实验表明，CEGNCDE 的性能优于 SOTA 方法，平均相对 MAE 降低 2.34%，相对 RMSE 降低 0.97%，相对 MAPE 降低 3.17%。</li>
</ul>

<h3>Title: Under the Surface: Tracking the Artifactuality of LLM-Generated Data</h3>
<ul>
<li><strong>Authors: </strong>Debarati Das, Karin De Langis, Anna Martin, Jaehyung Kim, Minhwa Lee, Zae Myung Kim, Shirley Hayati, Risako Owan, Bin Hu, Ritik Parkar, Ryan Koo, Jonginn Park, Aahan Tyagi, Libby Ferland, Sanjali Roy, Vincent Liu, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14698">https://arxiv.org/abs/2401.14698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14698">https://arxiv.org/pdf/2401.14698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14698]] Under the Surface: Tracking the Artifactuality of LLM-Generated Data(https://arxiv.org/abs/2401.14698)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>This work delves into the expanding role of large language models (LLMs) in generating artificial data. LLMs are increasingly employed to create a variety of outputs, including annotations, preferences, instruction prompts, simulated dialogues, and free text. As these forms of LLM-generated data often intersect in their application, they exert mutual influence on each other and raise significant concerns about the quality and diversity of the artificial data incorporated into training cycles, leading to an artificial data ecosystem. To the best of our knowledge, this is the first study to aggregate various types of LLM-generated text data, from more tightly constrained data like "task labels" to more lightly constrained "free-form text". We then stress test the quality and implications of LLM-generated artificial data, comparing it with human data across various existing benchmarks. Despite artificial data's capability to match human performance, this paper reveals significant hidden disparities, especially in complex tasks where LLMs often miss the nuanced understanding of intrinsic human-generated content. This study critically examines diverse LLM-generated data and emphasizes the need for ethical practices in data creation and when using LLMs. It highlights the LLMs' shortcomings in replicating human traits and behaviors, underscoring the importance of addressing biases and artifacts produced in LLM-generated content for future research and development. All data and code are available on our project page.</li>
<li><strong>摘要：</strong>这项工作深入探讨了大型语言模型 (LLM) 在生成人工数据方面的不断扩大的作用。法学硕士越来越多地被用来创建各种输出，包括注释、偏好、指令提示、模拟对话和自由文本。由于这些形式的法学硕士生成的数据经常在其应用中交叉，它们相互影响，并引起人们对纳入训练周期的人工数据的质量和多样性的严重担忧，从而形成人工数据生态系统。据我们所知，这是第一项聚合各种类型的法学硕士生成的文本数据的研究，从“任务标签”等更严格约束的数据到约束更宽松的“自由格式文本”。然后，我们对法学硕士生成的人工数据的质量和影响进行压力测试，将其与各种现有基准的人类数据进行比较。尽管人工数据具有与人类表现相匹配的能力，但本文揭示了显着的隐藏差异，特别是在复杂的任务中，法学硕士经常错过对人类生成的内在内容的细致入微的理解。这项研究批判性地审查了法学硕士生成的各种数据，并强调在数据创建和使用法学硕士时采取道德实践的必要性。它强调了法学硕士在复制人类特征和行为方面的缺点，强调了解决法学硕士生成内容中产生的偏见和伪影的重要性，以供未来的研究和开发。所有数据和代码都可以在我们的项目页面上找到。</li>
</ul>

<h3>Title: Turn-taking and Backchannel Prediction with Acoustic and Large Language  Model Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jinhan Wang, Long Chen, Aparna Khare, Anirudh Raju, Pranav Dheram, Di He, Minhua Wu, Andreas Stolcke, Venkatesh Ravichandran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14717">https://arxiv.org/abs/2401.14717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14717">https://arxiv.org/pdf/2401.14717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14717]] Turn-taking and Backchannel Prediction with Acoustic and Large Language  Model Fusion(https://arxiv.org/abs/2401.14717)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, agent</a></li>
<li><strong>Abstract: </strong>We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.</li>
<li><strong>摘要：</strong>我们提出了一种通过将神经声学模型与大语言模型（LLM）融合来连续预测口语对话中轮流和反向通道位置的方法。 Switchboard 人与人对话数据集上的实验表明，我们的方法始终优于单一模态的基线模型。我们还开发了一种新颖的多任务指令微调策略，以进一步受益于 LLM 编码的知识来理解任务和对话上下文，从而带来额外的改进。我们的方法展示了法学硕士和声学模型相结合的潜力，可以在人类和支持语音的人工智能代理之间实现更自然和对话式的交互。</li>
</ul>

<h3>Title: Residual Quantization with Implicit Neural Codebooks</h3>
<ul>
<li><strong>Authors: </strong>Iris Huijben, Matthijs Douze, Matthew Muckley, Ruud van Sloun, Jakob Verbeek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14732">https://arxiv.org/abs/2401.14732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14732">https://arxiv.org/pdf/2401.14732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14732]] Residual Quantization with Implicit Neural Codebooks(https://arxiv.org/abs/2401.14732)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods increase the rate by representing each vector using codewords across multiple codebooks. Residual quantization (RQ) is one such method, which increases accuracy by iteratively quantizing the error of the previous step. The error distribution is dependent on previously selected codewords. This dependency is, however, not accounted for in conventional RQ as it uses a generic codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant which predicts specialized codebooks per vector using a neural network that is conditioned on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12 bytes codes than other methods using 16 bytes on the BigANN and Deep1B dataset.</li>
<li><strong>摘要：</strong>矢量量化是数据压缩和矢量搜索的基本操作。为了获得高精度，多码本方法通过使用跨多个码本的码字来表示每个向量来提高速率。残差量化 (RQ) 就是这样一种方法，它通过迭代量化上一步的误差来提高精度。错误分布取决于先前选择的码字。然而，这种依赖性在传统 RQ 中并未得到考虑，因为它每个量化步骤都使用通用码本。在本文中，我们提出了 QINCo，一种神经 RQ 变体，它使用神经网络预测每个向量的专用码本，该神经网络以先前步骤中向量的近似为条件。实验表明，QINCo 在多个数据集和代码大小上大幅优于最先进的方法。例如，在 BigANN 和 Deep1B 数据集上，与其他使用 16 字节代码的方法相比，QINCo 使用 12 字节代码实现了更好的最近邻搜索精度。</li>
</ul>

<h3>Title: Off-Policy Primal-Dual Safe Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zifan Wu, Bo Tang, Qian Lin, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, Dong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14758">https://arxiv.org/abs/2401.14758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14758">https://arxiv.org/pdf/2401.14758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14758]] Off-Policy Primal-Dual Safe Reinforcement Learning(https://arxiv.org/abs/2401.14758)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose \textit{conservative policy optimization}, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce \textit{local policy convexification} to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and further verify them by extensive experiments. Results on benchmark tasks show that our method not only achieves an asymptotic performance comparable to state-of-the-art on-policy methods while using much fewer samples, but also significantly reduces constraint violation during training. Our code is available at https://github.com/ZifanWu/CAL.</li>
<li><strong>摘要：</strong>原对偶安全强化学习方法通​​常在策略的原始更新和拉格朗日乘子的对偶更新之间执行迭代。这种训练范式非常容易受到累积成本估计错误的影响，因为该估计是连接原始更新过程和双重更新过程的关键纽带。我们表明，当使用离策略方法时，这个问题会导致成本的显着低估，从而导致无法满足安全约束。为了解决这个问题，我们提出了 \textit{保守策略优化}，它通过考虑成本估计的不确定性来学习满足约束的区域中的策略。这提高了约束满意度，但也可能阻碍奖励最大化。然后，我们引入 \textit{局部策略凸化} 来通过逐渐减少估计不确定性来帮助消除这种次优性。我们对这两种成分的联合耦合效应提供了理论解释，并通过大量实验进一步验证。基准任务的结果表明，我们的方法不仅在使用更少的样本的情况下实现了与最先进的同策略方法相当的渐近性能，而且还显着减少了训练期间的约束违规。我们的代码可在 https://github.com/ZifanWu/CAL 获取。</li>
</ul>

<h3>Title: Large Language Model Adaptation for Financial Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Pau Rodriguez Inserte, Mariam Nakhlé, Raheel Qader, Gaetan Caillaut, Jingshu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14777">https://arxiv.org/abs/2401.14777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14777">https://arxiv.org/pdf/2401.14777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14777]] Large Language Model Adaptation for Financial Sentiment Analysis(https://arxiv.org/abs/2401.14777)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 最近通过提供对公司和市场财务文件非常有价值的见解而在金融机构中获得了关注。然而，由于文本的复杂性和特定术语的使用，金融领域的情况给 NLP 带来了额外的挑战。即使使用具有出色自然语言理解和生成能力的大型语言模型 (LLM)，通才语言模型在专门为金融量身定制的任务中往往也存在不足。本文提出了针对金融领域并高度重视金融情绪分析的法学硕士适应方法的研究。为此，我们使用多种策略对参数小于 1.5B 的两个基础模型进行了调整。我们表明，通过对财务文件和指令进行仔细微调，这些基础模型可以适应目标领域。此外，我们观察到小型法学硕士的性能与大型模型相当，同时在参数和数据方面更有效。除了模型之外，我们还展示了如何通过 LLM 生成人工指令以增加指令数据集的样本数量。</li>
</ul>

<h3>Title: ChemDFM: Dialogue Foundation Model for Chemistry</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Xin Chen, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14818">https://arxiv.org/abs/2401.14818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14818">https://arxiv.org/pdf/2401.14818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14818]] ChemDFM: Dialogue Foundation Model for Chemistry(https://arxiv.org/abs/2401.14818)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have established great success in the general domain of natural language processing. Their emerging task generalization and free-form dialogue capabilities can greatly help to design Chemical General Intelligence (CGI) to assist real-world research in chemistry. However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions as well as various data from the general domain. Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities. Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference. Further qualitative evaluations demonstrate the efficiency and effectiveness of ChemDFM in real-world research scenarios. We will open-source the ChemDFM model soon.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在自然语言处理的一般领域取得了巨大的成功。它们新兴的任务泛化和自由形式对话功能可以极大地帮助设计化学通用智能（CGI）来协助现实世界的化学研究。然而，化学领域专业语言和知识的存在，例如信息丰富的SMILES表示法，阻碍了化学领域通用领域法学硕士的表现。为此，我们开发了 ChemDFM，这是第一个面向 CGI 的法学硕士。 ChemDFM-13B 根据化学文献、教科书和说明书中的 34B 标记以及一般领域的各种数据进行训练。因此，它可以存储、理解和推理化学知识和语言，同时仍然拥有先进的自由形式语言理解能力。广泛的定量评估表明，ChemDFM 可以显着优于代表性的开源 LLM。此外，尽管大小差异很大，ChemDFM 在大部分化学任务上也可以超越 GPT-4。进一步的定性评估证明了 ChemDFM 在现实研究场景中的效率和有效性。我们将很快开源 ChemDFM 模型。</li>
</ul>

<h3>Title: GuardML: Efficient Privacy-Preserving Machine Learning Services Through  Hybrid Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Eugene Frimpong, Khoa Nguyen, Mindaugas Budzys, Tanveer Khan, Antonis Michalas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14840">https://arxiv.org/abs/2401.14840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14840">https://arxiv.org/pdf/2401.14840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14840]] GuardML: Efficient Privacy-Preserving Machine Learning Services Through  Hybrid Homomorphic Encryption(https://arxiv.org/abs/2401.14840)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Machine Learning (ML) has emerged as one of data science's most transformative and influential domains. However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models. To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models. One such approach is the use of Homomorphic Encryption (HE). However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios. Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges. Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices. We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all while preserving the privacy of the input data and ML model. We demonstrate the real-world applicability of our construction by developing and evaluating an HHE-based PPML application for classifying heart disease based on sensitive ECG data. Notably, our evaluations revealed a slight reduction in accuracy compared to inference on plaintext data. Additionally, both the analyst and end devices experience minimal communication and computation costs, underscoring the practical viability of our approach. The successful integration of HHE into PPML provides a glimpse into a more secure and privacy-conscious future for machine learning on relatively constrained end devices.</li>
<li><strong>摘要：</strong>机器学习 (ML) 已成为数据科学最具变革性和影响力的领域之一。然而，由于针对 ML 模型的恶意攻击数量不断增加，ML 的广泛采用引发了与隐私相关的问题。为了解决这些问题，引入了隐私保护机器学习 (PPML​​) 方法来保护 ML 模型的隐私和安全。其中一种方法是使用同态加密 (HE)。然而，传统 HE 的显着缺点和低效率使其对于高度可扩展的场景来说不切实际。幸运的是，最近出现了一种现代加密方案——混合同态加密（HHE），它结合了对称加密和 HE 的优点来克服这些挑战。我们的工作旨在通过设计专为终端设备定制的 PPML 方案，将 HHE 引入 ML。我们利用 HHE 作为基本构建块，通过加密数据实现分类结果的安全学习，同时保护输入数据和 ML 模型的隐私。我们通过开发和评估基于 HHE 的 PPML 应用程序（用于根据敏感心电图数据对心脏病进行分类）来证明我们的构建在现实世界中的适用性。值得注意的是，我们的评估显示与明文数据推断相比，准确性略有下降。此外，分析师和终端设备的通信和计算成本都极低，这凸显了我们方法的实际可行性。 HHE 与 PPML 的成功集成为相对受限的终端设备上的机器学习提供了一个更安全、更注重隐私的未来。</li>
</ul>

<h3>Title: F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods</h3>
<ul>
<li><strong>Authors: </strong>Yu Sun, Keyu Chen, Shujie Wang, Qipeng Guo, Hang Yan, Xipeng Qiu, Xuanjing Huang, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14869">https://arxiv.org/abs/2401.14869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14869">https://arxiv.org/pdf/2401.14869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14869]] F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods(https://arxiv.org/abs/2401.14869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs' fundamental abilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其前所未有的性能而受到广泛关注，导致越来越多的研究评估 LLM。然而，这些评估基准仅限于评估遵循指令的能力，忽略了预训练阶段出现的基本能力。以往的主观评价方法主要依靠API模型评分。然而，在缺乏参考的情况下，大型模型辨别细微差异的能力有限。为了弥补这一差距，我们提出了 F-Eval，这是一种双语评估基准，用于评估基本能力，包括表达、常识和逻辑。 F-Eval中的任务包括多选客观任务、开放式客观任务、基于参考的主观任务和无参考主观任务。对于无参考的主观任务，我们设计了新的评估方法，作为 API 模型评分的替代方法。我们对 13 名高级法学硕士进行评估。结果表明，我们的评估方法比其他评估者表现出更高的相关系数和更大的区别。此外，我们还讨论了不同模型大小、维度和标准化方法的影响。我们预计 F-Eval 将有助于法学硕士基本能力的研究。</li>
</ul>

<h3>Title: Reinforcement Learning Interventions on Boundedly Rational Human Agents  in Frictionful Tasks</h3>
<ul>
<li><strong>Authors: </strong>Eura Nofshin, Siddharth Swaroop, Weiwei Pan, Susan Murphy, Finale Doshi-Velez</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14923">https://arxiv.org/abs/2401.14923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14923">https://arxiv.org/pdf/2401.14923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14923]] Reinforcement Learning Interventions on Boundedly Rational Human Agents  in Frictionful Tasks(https://arxiv.org/abs/2401.14923)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans.</li>
<li><strong>摘要：</strong>许多重要的行为改变都是有摩擦的；它们要求个人长期付出努力，但很少能立即得到满足。在这里，人工智能 (AI) 代理可以提供个性化干预措施，帮助个人坚持自己的目标。在这些设置中，人工智能代理必须快速（在个体脱离之前）并可解释地进行个性化，以帮助我们理解行为干预。在本文中，我们介绍了行为模型强化学习（BMRL），这是一个人工智能代理对属于有限理性人类代理的马尔可夫决策过程（MDP）参数进行干预的框架。我们将人类决策者制定为规划代理，使我们能够将不良的人类政策（无法实现目标的政策）归因于其不适应的 MDP 参数，例如极低的折扣因子。此外，我们提出了一类易于处理的人类模型，可以捕捉摩擦性任务中的基本行为。引入特定于 BMRL 的 MDP 等价概念，我们从理论上和经验上表明，使用我们的人类模型进行人工智能规划可以为各种更复杂、更真实的人类制定有用的政策。</li>
</ul>

<h3>Title: Do LLMs Dream of Ontologies?</h3>
<ul>
<li><strong>Authors: </strong>Marco Bombieri, Paolo Fiorini, Simone Paolo Ponzetto, Marco Rospocher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14931">https://arxiv.org/abs/2401.14931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14931">https://arxiv.org/pdf/2401.14931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14931]] Do LLMs Dream of Ontologies?(https://arxiv.org/abs/2401.14931)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently revolutionized automated text understanding and generation. The performance of these models relies on the high number of parameters of the underlying neural architectures, which allows LLMs to memorize part of the vast quantity of data seen during the training. This paper investigates whether and to what extent general-purpose pre-trained LLMs have memorized information from known ontologies. Our results show that LLMs partially know ontologies: they can, and do indeed, memorize concepts from ontologies mentioned in the text, but the level of memorization of their concepts seems to vary proportionally to their popularity on the Web, the primary source of their training material. We additionally propose new metrics to estimate the degree of memorization of ontological information in LLMs by measuring the consistency of the output produced across different prompt repetitions, query languages, and degrees of determinism.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近彻底改变了自动文本理解和生成。这些模型的性能依赖于底层神经架构的大量参数，这使得法学硕士能够记住训练期间看到的大量数据的一部分。本文研究了通用预训练法学硕士是否以及在多大程度上记住了已知本体中的信息。我们的结果表明，法学硕士部分了解本体论：他们可以而且确实记住了文本中提到的本体论中的概念，但其概念的记忆水平似乎与他们在网络上的受欢迎程度成正比，而网络是他们培训的主要来源材料。我们还提出了新的指标，通过测量不同提示重复、查询语言和确定性程度产生的输出的一致性来估计法学硕士本体信息的记忆程度。</li>
</ul>

<h3>Title: Conserve-Update-Revise to Cure Generalization and Robustness Trade-off  in Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Shruthi Gowda, Bahram Zonooz, Elahe Arani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14948">https://arxiv.org/abs/2401.14948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14948">https://arxiv.org/pdf/2401.14948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14948]] Conserve-Update-Revise to Cure Generalization and Robustness Trade-off  in Adversarial Training(https://arxiv.org/abs/2401.14948)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating "robust overfitting". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.</li>
<li><strong>摘要：</strong>对抗性训练提高了神经网络对抗对抗性攻击的鲁棒性，尽管是以牺牲标准泛化和鲁棒泛化之间的权衡为代价的。为了揭示驱动这一现象的根本因素，我们研究了神经网络在从标准环境过渡到对抗环境期间的分层学习能力。我们的实证研究结果表明，有选择地更新特定层同时保留其他层可以显着增强网络的学习能力。因此，我们提出了 CURE，一种新颖的训练框架，它利用梯度突出标准来执行权重的选择性保存、更新和修订。重要的是，CURE 的设计与数据集和架构无关，确保其在各种场景中的适用性。它有效地解决了记忆和过度拟合问题，从而增强了鲁棒性和泛化性之间的权衡，此外，这种训练方法还有助于减轻“鲁棒过度拟合”。此外，我们的研究为选择性对抗训练的机制提供了宝贵的见解，并为未来的研究提供了一条有希望的途径。</li>
</ul>

<h3>Title: Learning Universal Predictors</h3>
<ul>
<li><strong>Authors: </strong>Jordi Grau-Moya, Tim Genewein, Marcus Hutter, Laurent Orseau, Grégoire Delétang, Elliot Catt, Anian Ruoss, Li Kevin Wenliang, Christopher Mattern, Matthew Aitchison, Joel Veness</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14953">https://arxiv.org/abs/2401.14953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14953">https://arxiv.org/pdf/2401.14953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14953]] Learning Universal Predictors(https://arxiv.org/abs/2401.14953)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal prediction strategies.</li>
<li><strong>摘要：</strong>元学习已成为训练神经网络从有限数据中快速学习新任务的强大方法。广泛接触不同的任务可以产生多种表征，从而能够解决一般问题。但是，元学习的局限性是什么？在这项工作中，我们探索了通过充分利用元学习将最强大的通用预测器（即所罗门诺夫归纳法（SI））分摊到神经网络中的潜力。我们使用通用图灵机 (UTM) 生成训练数据，用于将网络暴露给广泛的模式。我们提供 UTM 数据生成过程和元训练协议的理论分析。我们使用不同复杂性和通用性的神经架构（例如 LSTM、Transformers）和算法数据生成器进行全面的实验。我们的结果表明，UTM 数据是元学习的宝贵资源，可用于训练能够学习通用预测策略的神经网络。</li>
</ul>

<h3>Title: End-To-End Set-Based Training for Neural Network Verification</h3>
<ul>
<li><strong>Authors: </strong>Lukas Koller, Tobias Ladner, Matthias Althoff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14961">https://arxiv.org/abs/2401.14961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14961">https://arxiv.org/pdf/2401.14961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14961]] End-To-End Set-Based Training for Neural Network Verification(https://arxiv.org/abs/2401.14961)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trained neural networks outperform neural networks trained with state-of-the-art adversarial attacks.</li>
<li><strong>摘要：</strong>神经网络容易受到对抗性攻击，即小的输入扰动可能会导致神经网络的输出显着不同。安全关键环境需要神经网络能够抵抗输入扰动。然而，训练和形式验证稳健的神经网络具有挑战性。我们通过首次采用端到端的基于集合的训练程序来应对这一挑战，该程序可以训练强大的神经网络进行形式验证。我们的训练过程极大地简化了训练后的神经网络的后续形式稳健性验证。虽然之前的研究主要集中在通过对抗性攻击增强神经网络训练，但我们的方法利用基于集合的计算来训练具有整组扰动输入的神经网络。此外，我们证明了基于集合的训练过程可以有效地训练鲁棒的神经网络，并且更容易验证。在许多情况下，基于集合训练的神经网络优于经过最先进的对抗性攻击训练的神经网络。</li>
</ul>

<h3>Title: Airavata: Introducing Hindi Instruction-tuned LLM</h3>
<ul>
<li><strong>Authors: </strong>Jay Gala, Thanmay Jayakumar, Jaavid Aktar Husain, Aswanth Kumar M, Mohammed Safi Ur Rahman Khan, Diptesh Kanojia, Ratish Puduppully, Mitesh M. Khapra, Raj Dabre, Rudra Murthy, Anoop Kunchukuttan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15006">https://arxiv.org/abs/2401.15006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15006">https://arxiv.org/pdf/2401.15006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15006]] Airavata: Introducing Hindi Instruction-tuned LLM(https://arxiv.org/abs/2401.15006)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We announce the initial release of "Airavata," an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.</li>
<li><strong>摘要：</strong>我们宣布首次发布“Airavata”，这是一种经过指令调整的印地语法学硕士。 Airavata 是通过使用各种指令调整印地语数据集对 OpenHathi 进行微调而创建的，使其更适合辅助任务。除了模型之外，我们还共享 IndicInstruct 数据集，该数据集是各种指令调整数据集的集合，以便为印度法学硕士进行进一步研究。此外，我们还提供了评估基准和框架，用于评估印地语任务中法学硕士的表现。目前，Airavata 支持印地语，但我们计划将其扩展到所有 22 种预定的印度语。您可以在 https://ai4bharat.github.io/airavata 访问所有工件。</li>
</ul>

<h3>Title: SliceGPT: Compress Large Language Models by Deleting Rows and Columns</h3>
<ul>
<li><strong>Authors: </strong>Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James Hensman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15024">https://arxiv.org/abs/2401.15024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15024">https://arxiv.org/pdf/2401.15024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15024]] SliceGPT: Compress Large Language Models by Deleting Rows and Columns(https://arxiv.org/abs/2401.15024)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code</a></li>
<li><strong>Abstract: </strong>Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression</li>
<li><strong>摘要：</strong>大型语言模型已成为自然语言处理的基石，但它们的使用在计算和内存资源方面会带来巨大的成本。稀疏化提供了一种缓解这些资源限制的解决方案，最近的工作表明，经过训练的模型可以事后稀疏化。现有的稀疏化技术面临着挑战，因为它们需要额外的数据结构，并且当前硬件提供的加速效果有限。在本文中，我们提出了 SliceGPT，这是一种新的训练后稀疏化方案，它将每个权重矩阵替换为更小的（稠密）矩阵，从而减少了网络的嵌入维数。通过大量实验，我们表明 SliceGPT 可以删除 LLAMA2-70B、OPT 66B 和 Phi-2 模型高达 25% 的模型参数（包括嵌入），同时保持 99%、99% 和 90% 的零样本任务性能分别为稠密模型。我们的切片模型在更少的 GPU 上运行，并且运行速度更快，无需任何额外的代码优化：在 24GB 消费级 GPU 上，我们将 LLAMA2-70B 上的推理总计算量减少到密集模型的 64%；在 40GB A100 GPU 上，我们将其降低至 66%。我们提供了一种新的见解，即变压器网络中的计算不变性，这使得 SliceGPT 成为可能，我们希望它能够激发并实现未来减少预训练模型的内存和计算需求的途径。代码位于：https://github.com/microsoft/TransformerCompression</li>
</ul>

<h3>Title: On the generalization capacity of neural networks during generic  multimodal reasoning</h3>
<ul>
<li><strong>Authors: </strong>Takuya Ito, Soham Dan, Mattia Rigotti, James Kozloski, Murray Campbell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15030">https://arxiv.org/abs/2401.15030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15030">https://arxiv.org/pdf/2401.15030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15030]] On the generalization capacity of neural networks during generic  multimodal reasoning(https://arxiv.org/abs/2401.15030)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures). We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, either cross-modal attention or models with deeper attention layers are key architectural features required to integrate multimodal inputs. On the other hand, neither of these architectural features led to productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide Generic COG (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.</li>
<li><strong>摘要：</strong>Transformer 的出现促进了大型语言模型 (LLM) 的发展，它似乎展示了类人的能力。为了评估此类模型和各种其他基础神经网络架构对多模态领域的通用性，我们评估并比较了它们的多模态泛化能力。我们引入了多模态问答基准来评估三种特定类型的分布外（OOD）泛化性能：干扰项泛化（存在干扰项时的泛化）、系统组合泛化（对新任务排列的泛化）和高效组合泛化泛化（泛化到更复杂的任务结构）。我们发现，跨模型架构（例如 RNN、Transformers、Perceiver 等）、具有多个注意力层的模型或利用输入域之间的交叉注意力机制的模型表现更好。我们的积极结果表明，对于多模态干扰和系统泛化，跨模态注意力或具有更深注意力层的模型是集成多模态输入所需的关键架构特征。另一方面，这些架构特征都没有导致有效的泛化，这表明现有架构对于特定类型的多模态泛化存在根本限制。这些结果证明了现代多模态推理神经模型的特定架构组件的优点和局限性。最后，我们提供通用 COG (gCOG)，这是一个具有多个多模态泛化分割的可配置基准，供未来的研究探索。</li>
</ul>

<h3>Title: PROXYQA: An Alternative Framework for Evaluating Long-Form Text  Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15042">https://arxiv.org/abs/2401.15042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15042">https://arxiv.org/pdf/2401.15042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15042]] PROXYQA: An Alternative Framework for Evaluating Long-Form Text  Generation with Large Language Models(https://arxiv.org/abs/2401.15042)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \textsc{ProxyQA}'s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that evaluating through \textit{proxy-questions} is a highly self-consistent and human-criteria-correlated validation method. The dataset and leaderboard will be available at \url{https://github.com/Namco0816/ProxyQA}.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在长篇上下文理解任务中取得了显着的成功。然而，它们生成报告和文章等长内容的能力仍未得到充分探索。目前的基准没有充分评估法学硕士产生信息丰富且全面的内容的能力，因此需要更严格的评估方法。在这项研究中，我们引入了 \textsc{ProxyQA}，一个用于评估长文本生成的框架，其中包含跨越各个领域的深入的人工策划 \textit{meta-questions}。每个元问题都包含相应的 \textit{proxy-questions} 以及带注释的答案。法学硕士被提示生成大量内容来回答这些元问题。 \textsc{ProxyQA} 利用评估器并将生成的内容合并为背景上下文，根据评估器在回答 \textit{proxy-questions} 方面的表现来评估生成的内容的质量。我们研究了多个法学硕士，强调 \textsc{ProxyQA} 作为高质量评估工具的严格要求。人类评估表明，通过 \textit{proxy-questions} 进行评估是一种高度自洽且与人类标准相关的验证方法。数据集和排行榜将在 \url{https://github.com/Namco0816/ProxyQA} 上提供。</li>
</ul>

<h3>Title: Health Text Simplification: An Annotated Corpus for Digestive Cancer  Education and Novel Strategies for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S. Williams, Marcos Zampieri, Kevin Lybarger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15043">https://arxiv.org/abs/2401.15043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15043">https://arxiv.org/pdf/2401.15043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15043]] Health Text Simplification: An Annotated Corpus for Digestive Cancer  Education and Novel Strategies for Reinforcement Learning(https://arxiv.org/abs/2401.15043)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality. Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based approaches. Our experimentation encompasses Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a lightweight model adept at distinguishing between original and simplified texts, thereby enhancing the model's effectiveness with unlabeled data. Results: Fine-tuned Llama 2 models demonstrated high performance across various metrics. Our innovative RLHF reward function surpassed existing RL text simplification reward functions in effectiveness. The results underscore that RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text and improving performance. Additionally, these methods effectively adapt out-of-domain text simplification models to targeted domains.</li>
<li><strong>摘要：</strong>目的：健康教育材料的阅读水平显着影响信息的可理解性和可及性，特别是对于少数群体而言。许多患者教育资源超出了广泛接受的标准的阅读水平和复杂性。健康信息迫切需要高性能的文本简化模型，以加强传播和读写能力。这种需求在癌症教育中尤为迫切，有效的预防和筛查教育可以大大降低发病率和死亡率。方法：我们引入了简化消化癌（SimpleDC），这是一个专为健康文本简化研究量身定制的癌症教育材料平行语料库。利用 SimpleDC 以及现有的 Med-EASi 语料库，我们探索基于大语言模型 (LLM) 的简化方法，包括微调、强化学习 (RL)、人类反馈强化学习 (RLHF)、领域适应和基于提示的简化方法接近。我们的实验包括 Llama 2 和 GPT-4。引入了一种新颖的 RLHF 奖励函数，该函数具有擅长区分原始文本和简化文本的轻量级模型，从而提高了模型对未标记数据的有效性。结果：经过微调的 Llama 2 模型在各种指标上都表现出了高性能。我们创新的 RLHF 奖励函数在有效性上超越了现有的 RL 文本简化奖励函数。结果强调，RL/RLHF 可以增强微调，促进未标记文本的模型训练并提高性能。此外，这些方法有效地将域外文本简化模型适应目标域。</li>
</ul>

<h3>Title: Fully Independent Communication in Multi-Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15059">https://arxiv.org/abs/2401.15059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15059">https://arxiv.org/pdf/2401.15059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15059]] Fully Independent Communication in Multi-Agent Reinforcement Learning(https://arxiv.org/abs/2401.15059)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication may not always be needed and that the chosen agent network sizes need to be considered when used together with communication in order to achieve efficient learning.</li>
<li><strong>摘要：</strong>多智能体强化学习 (MARL) 涵盖多智能体系统领域的广泛研究领域。最近的几项工作特别关注 MARL 中的通信方法的研究。虽然已经提出了多种通信方法，但这些方法可能仍然太复杂并且不容易转移到更实际的环境中。原因之一是由于使用了著名的参数共享技巧。在本文中，我们研究了 MARL 中不共享参数的独立学习者如何进行交流。我们证明这种设置可能会产生一些问题，为此我们提出了一种新的学习方案作为解决方案。我们的结果表明，尽管面临挑战，独立代理仍然可以按照我们的方法学习沟通策略。此外，我们使用这种方法来研究 MARL 中的通信如何受到不同网络容量（共享和不共享参数）的影响。我们观察到，通信可能并不总是需要的，并且在与通信一起使用时需要考虑所选择的代理网络大小，以实现有效的学习。</li>
</ul>

<h3>Title: Expert with Clustering: Hierarchical Online Preference Learning  Framework</h3>
<ul>
<li><strong>Authors: </strong>Tianyue Zhou, Jung-Hoon Cho, Babak Rahimi Ardabili, Hamed Tabkhi, Cathy Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15062">https://arxiv.org/abs/2401.15062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15062">https://arxiv.org/pdf/2401.15062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15062]] Expert with Clustering: Hierarchical Online Preference Learning  Framework(https://arxiv.org/abs/2401.15062)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes. Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation. In this study, we consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users. We introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice. EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric. This metric is instrumental in generating more representative cluster centroids. In a recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options, our algorithm achieves a regret bound of $O(N\sqrt{T\log K} + NT)$. This bound consists of two parts: the first term is the regret from the Hedge algorithm, and the second term depends on the average loss from clustering. The algorithm performs with low regret, especially when a latent hierarchical structure exists among users. This regret bound underscores the theoretical and experimental efficacy of EWC, particularly in scenarios that demand rapid learning and adaptation. Experimental results highlight that EWC can substantially reduce regret by 27.57% compared to the LinUCB baseline. Our work offers a data-efficient approach to capturing both individual and collective behaviors, making it highly applicable to contexts with hierarchical structures. We expect the algorithm to be applicable to other settings with layered nuances of user preferences and information.</li>
<li><strong>摘要：</strong>新兴的移动系统越来越有能力向移动用户推荐选项，引导他们实现个性化且可持续的系统结果。比典型的推荐系统更重要的是，最大限度地减少遗憾至关重要，因为1）移动选项直接影响用户的生活，2）系统的可持续性依赖于足够的用户参与。在本研究中，我们考虑通过利用捕获用户移动偏好的低维潜在空间来加速用户偏好学习。我们引入了一个名为 Expert with Clustering (EWC) 的分层上下文强盗框架，它将聚类技术和预测与专家建议集成在一起。 EWC 有效地利用分层用户信息，并结合了一种新颖的损耗引导距离度量。该指标有助于生成更具代表性的聚类质心。在具有 $N$ 用户、每个用户 $T$ 轮次和 $K$ 个选项的推荐场景中，我们的算法实现了 $O(N\sqrt{T\log K} + NT)$ 的遗憾界限。该界限由两部分组成：第一项是对冲算法的遗憾，第二项取决于聚类的平均损失。该算法的执行后悔率较低，特别是当用户之间存在潜在的层次结构时。这种遗憾界限强调了 EWC 的理论和实验功效，特别是在需要快速学习和适应的场景中。实验结果表明，与 LinUCB 基线相比，EWC 可以大幅减少 27.57% 的后悔。我们的工作提供了一种高效的数据方法来捕获个人和集体行为，使其高度适用于具有分层结构的环境。我们希望该算法适用于具有用户偏好和信息分层细微差别的其他设置。</li>
</ul>

<h3>Title: EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.15077">https://arxiv.org/abs/2401.15077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.15077">https://arxiv.org/pdf/2401.15077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.15077]] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty(https://arxiv.org/abs/2401.15077)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of Huggingface's implementations.</li>
<li><strong>摘要：</strong>自回归解码使得大型语言模型 (LLM) 的推理非常耗时。我们提出了一个简单的框架，EAGLE（提高语言模型效率的外推算法），用于无损加速。与传统的推测采样方法不同，EAGLE 在更规则的（第二顶层）特征级别上自动回归地操作起草过程，并通过提前集成一个时间步的标记来解决下一特征预测问题中的采样不确定性问题。 EAGLE 提供的加速是无损的：它不涉及目标 LLM 的微调，并且生成的文本保持与普通自回归解码相同的分布。截至本文提交时，EAGLE 是推测性采样系列中最快的已知框架。在 MT-bench 上，EAGLE 比普通解码快 3 倍，比 Lookahead 快 2 倍，比 Medusa 快 1.6 倍。使用 gpt-fast，EAGLE 在单个 RTX 3090 GPU 上通过 LLaMA2-Chat 13B 实现平均 160 个令牌/秒，而 Huggingface 的实现为 24 个令牌/秒。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
