<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-24</h1>
<h3>Title: Multilingual Fine-Grained News Headline Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Shen, Tianqi Liu, Jialu Liu, Zhen Qin, Jay Pavagadhi, Simon Baumgartner, Michael Bendersky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15975">https://arxiv.org/abs/2407.15975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15975">https://arxiv.org/pdf/2407.15975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15975]] Multilingual Fine-Grained News Headline Hallucination Detection(https://arxiv.org/abs/2407.15975)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>The popularity of automated news headline generation has surged with advancements in pre-trained language models. However, these models often suffer from the ``hallucination'' problem, where the generated headline is not fully supported by its source article. Efforts to address this issue have predominantly focused on English, using over-simplistic classification schemes that overlook nuanced hallucination types. In this study, we introduce the first multilingual, fine-grained news headline hallucination detection dataset that contains over 11 thousand pairs in 5 languages, each annotated with detailed hallucination types by experts. We conduct extensive experiments on this dataset under two settings. First, we implement several supervised fine-tuning approaches as preparatory solutions and demonstrate this dataset's challenges and utilities. Second, we test various large language models' in-context learning abilities and propose two novel techniques, language-dependent demonstration selection and coarse-to-fine prompting, to boost the few-shot hallucination detection performance in terms of the example-F1 metric. We release this dataset to foster further research in multilingual, fine-grained headline hallucination detection.</li>
<li><strong>摘要：</strong>随着预训练语言模型的进步，自动新闻标题生成技术也越来越受欢迎。然而，这些模型经常受到“幻觉”问题的困扰，即生成的标题没有得到源文章的完全支持。解决这一问题的努力主要集中在英语上，使用过于简单的分类方案，忽略了细微的幻觉类型。在本研究中，我们介绍了第一个多语言、细粒度的新闻标题幻觉检测数据集，其中包含 5 种语言的 11,000 多对，每对都由专家标注了详细的幻觉类型。我们在两种环境下对该数据集进行了广泛的实验。首先，我们实施了几种监督微调方法作为准备解决方案，并展示了该数据集的挑战和实用性。其次，我们测试了各种大型语言模型的上下文学习能力，并提出了两种新技术，即语言相关的演示选择和由粗到细的提示，以提高示例 F1 指标方面的少样本幻觉检测性能。我们发布该数据集是为了促进多语言、细粒度标题幻觉检测的进一步研究。</li>
</ul>

<h3>Title: SocialQuotes: Learning Contextual Roles of Social Media Quotes on the Web</h3>
<ul>
<li><strong>Authors: </strong>John Palowitch, Hamidreza Alvari, Mehran Kazemi, Tanvir Amin, Filip Radlinski</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16007">https://arxiv.org/abs/2407.16007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16007">https://arxiv.org/pdf/2407.16007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16007]] SocialQuotes: Learning Contextual Roles of Social Media Quotes on the Web(https://arxiv.org/abs/2407.16007)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Web authors frequently embed social media to support and enrich their content, creating the potential to derive web-based, cross-platform social media representations that can enable more effective social media retrieval systems and richer scientific analyses. As step toward such capabilities, we introduce a novel language modeling framework that enables automatic annotation of roles that social media entities play in their embedded web context. Using related communication theory, we liken social media embeddings to quotes, formalize the page context as structured natural language signals, and identify a taxonomy of roles for quotes within the page context. We release SocialQuotes, a new data set built from the Common Crawl of over 32 million social quotes, 8.3k of them with crowdsourced quote annotations. Using SocialQuotes and the accompanying annotations, we provide a role classification case study, showing reasonable performance with modern-day LLMs, and exposing explainable aspects of our framework via page content ablations. We also classify a large batch of un-annotated quotes, revealing interesting cross-domain, cross-platform role distributions on the web.</li>
<li><strong>摘要：</strong>网络作者经常嵌入社交媒体来支持和丰富其内容，从而有可能获得基于网络的跨平台社交媒体表示，从而实现更有效的社交媒体检索系统和更丰富的科学分析。为了实现这些功能，我们引入了一个新颖的语言建模框架，该框架可以自动注释社交媒体实体在其嵌入的网络环境中所扮演的角色。使用相关的通信理论，我们将社交媒体嵌入比作引语，将页面上下文形式化为结构化的自然语言信号，并确定页面上下文中引语的角色分类。我们发布了 SocialQuotes，这是一个新的数据集，它基于 Common Crawl 构建，包含超过 3200 万条社交引语，其中 8.3k 条带有众包引语注释。使用 SocialQuotes 和随附的注释，我们提供了一个角色分类案例研究，展示了现代 LLM 的合理性能，并通过页面内容消融揭示了我们框架的可解释方面。我们还对大量未注释的引语进行了分类，揭示了网络上有趣的跨领域、跨平台角色分布。</li>
</ul>

<h3>Title: Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Shen, Ran Xu, Yennie Jun, Zhen Qin, Tianqi Liu, Carl Yang, Yi Liang, Simon Baumgartner, Michael Bendersky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16008">https://arxiv.org/abs/2407.16008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16008">https://arxiv.org/pdf/2407.16008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16008]] Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic Data Generation(https://arxiv.org/abs/2407.16008)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. They are trained using preference datasets where each example consists of one input prompt, two responses, and a preference label. As curating a high-quality human labeled preference dataset is both time-consuming and expensive, people often rely on existing powerful LLMs for preference label generation. This can potentially introduce noise and impede RM training. In this work, we present RMBoost, a novel synthetic preference data generation paradigm to boost reward model quality. Unlike traditional methods, which generate two responses before obtaining the preference label, RMBoost first generates one response and selects a preference label, followed by generating the second more (or less) preferred response conditioned on the pre-selected preference label and the first response. This approach offers two main advantages. First, RMBoost reduces labeling noise since preference pairs are constructed intentionally. Second, RMBoost facilitates the creation of more diverse responses by incorporating various quality aspects (e.g., helpfulness, relevance, completeness) into the prompts. We conduct extensive experiments across three diverse datasets and demonstrate that RMBoost outperforms other synthetic preference data generation techniques and significantly boosts the performance of four distinct reward models.</li>
<li><strong>摘要：</strong>奖励模型 (RM) 对于将大型语言模型 (LLM) 与人类偏好对齐至关重要。它们使用偏好数据集进行训练，其中每个示例由一个输入提示、两个响应和一个偏好标签组成。由于整理高质量的人工标记偏好数据集既耗时又昂贵，人们通常依赖现有的强大 LLM 来生成偏好标签。这可能会引入噪音并阻碍 RM 训练。在这项工作中，我们提出了 RMBoost，这是一种新颖的合成偏好数据生成范例，可提高奖励模型质量。与在获得偏好标签之前生成两个响应的传统方法不同，RMoost 首先生成一个响应并选择一个偏好标签，然后根据预先选择的偏好标签和第一个响应生成第二个更（或更不）偏好的响应。这种方法有两个主要优点。首先，RMoost 减少了标签噪音，因为偏好对是故意构建的。其次，RMoost 通过将各种质量方面（例如，有用性、相关性、完整性）纳入提示中来促进创建更多样化的响应。我们对三个不同的数据集进行了广泛的实验，并证明 RMBoost 优于其他合成偏好数据生成技术，并显著提升了四种不同奖励模型的性能。</li>
</ul>

<h3>Title: Enhancing Temporal Understanding in LLMs for Semi-structured Tables</h3>
<ul>
<li><strong>Authors: </strong>Irwin Deng, Kushagra Dixit, Vivek Gupta, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16030">https://arxiv.org/abs/2407.16030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16030">https://arxiv.org/pdf/2407.16030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16030]] Enhancing Temporal Understanding in LLMs for Semi-structured Tables(https://arxiv.org/abs/2407.16030)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Temporal reasoning over tabular data presents substantial challenges for large language models (LLMs), as evidenced by recent research. In this study, we conduct a comprehensive analysis of temporal datasets to pinpoint the specific limitations of LLMs. Our investigation leads to enhancements in TempTabQA, a dataset specifically designed for tabular temporal question answering. We provide critical insights for improving LLM performance in temporal reasoning tasks with tabular data. Furthermore, we introduce a novel approach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings demonstrate that our method significantly improves evidence-based reasoning across various models. Additionally, our experimental results reveal that indirect supervision with auxiliary data substantially boosts model performance in these tasks. This work contributes to a deeper understanding of LLMs' temporal reasoning abilities over tabular data and promotes advancements in their application across diverse fields.</li>
<li><strong>摘要：</strong>最近的研究表明，表格数据的时间推理对大型语言模型 (LLM) 提出了巨大挑战。在本研究中，我们对时间数据集进行了全面分析，以查明 LLM 的具体限制。我们的调查结果增强了 TempTabQA，这是一个专门为表格时间问答设计的数据集。我们为提高使用表格数据的时间推理任务中的 LLM 性能提供了关键见解。此外，我们引入了一种新方法 C.L.E.A.R 来增强该领域的 LLM 功能。我们的研究结果表明，我们的方法显著提高了各种模型的循证推理能力。此外，我们的实验结果表明，使用辅助数据的间接监督可显著提高这些任务中的模型性能。这项工作有助于更深入地了解 LLM 对表格数据的时间推理能力，并促进其在不同领域的应用进步。</li>
</ul>

<h3>Title: Leveraging Large Language Models to Geolocate Linguistic Variations in Social Media Posts</h3>
<ul>
<li><strong>Authors: </strong>Davide Savarro, Davide Zago, Stefano Zoia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16047">https://arxiv.org/abs/2407.16047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16047">https://arxiv.org/pdf/2407.16047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16047]] Leveraging Large Language Models to Geolocate Linguistic Variations in Social Media Posts(https://arxiv.org/abs/2407.16047)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Geolocalization of social media content is the task of determining the geographical location of a user based on textual data, that may show linguistic variations and informal language. In this project, we address the GeoLingIt challenge of geolocalizing tweets written in Italian by leveraging large language models (LLMs). GeoLingIt requires the prediction of both the region and the precise coordinates of the tweet. Our approach involves fine-tuning pre-trained LLMs to simultaneously predict these geolocalization aspects. By integrating innovative methodologies, we enhance the models' ability to understand the nuances of Italian social media text to improve the state-of-the-art in this domain. This work is conducted as part of the Large Language Models course at the Bertinoro International Spring School 2024. We make our code publicly available on GitHub this https URL.</li>
<li><strong>摘要：</strong>社交媒体内容的地理定位是根据文本数据确定用户地理位置的任务，文本数据可能显示语言变化和非正式语言。在这个项目中，我们利用大型语言模型 (LLM) 解决了 GeoLingIt 挑战，即对用意大利语撰写的推文进行地理定位。GeoLingIt 需要预测推文的区域和精确坐标。我们的方法涉及微调预先训练的 LLM 以同时预测这些地理定位方面。通过整合创新方法，我们增强了模型理解意大利社交媒体文本细微差别的能力，从而提高了该领域的最新水平。这项工作是 2024 年贝尔蒂诺罗国际春季学校大型语言模型课程的一部分。我们将我们的代码公开发布在 GitHub 上，此 https URL。</li>
</ul>

<h3>Title: KaPQA: Knowledge-Augmented Product Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Swetha Eppalapally, Daksh Dangi, Chaithra Bhat, Ankita Gupta, Ruiyi Zhang, Shubham Agarwal, Karishma Bagga, Seunghyun Yoon, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16073">https://arxiv.org/abs/2407.16073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16073">https://arxiv.org/pdf/2407.16073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16073]] KaPQA: Knowledge-Augmented Product Question-Answering(https://arxiv.org/abs/2407.16073)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Question-answering for domain-specific applications has recently attracted much interest due to the latest advancements in large language models (LLMs). However, accurately assessing the performance of these applications remains a challenge, mainly due to the lack of suitable benchmarks that effectively simulate real-world scenarios. To address this challenge, we introduce two product question-answering (QA) datasets focused on Adobe Acrobat and Photoshop products to help evaluate the performance of existing models on domain-specific product QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA framework to enhance the performance of the models in the product QA task. Our experiments demonstrated that inducing domain knowledge through query reformulation allowed for increased retrieval and generative performance when compared to standard RAG-QA methods. This improvement, however, is slight, and thus illustrates the challenge posed by the datasets introduced.</li>
<li><strong>摘要：</strong>由于大型语言模型 (LLM) 的最新进展，特定领域应用的问答系统最近引起了广泛关注。然而，准确评估这些应用程序的性能仍然是一个挑战，主要是因为缺乏能够有效模拟真实场景的合适基准。为了应对这一挑战，我们引入了两个专注于 Adob​​e Acrobat 和 Photoshop 产品的产品问答 (QA) 数据集，以帮助评估现有模型在特定领域产品 QA 任务上的性能。此外，我们提出了一种新颖的知识驱动的 RAG-QA 框架，以提高模型在产品 QA 任务中的性能。我们的实验表明，与标准 RAG-QA 方法相比，通过查询重构引入领域知识可以提高检索和生成性能。然而，这种改进很小，因此说明了引入的数据集带来的挑战。</li>
</ul>

<h3>Title: Analyzing the Polysemy Evolution using Semantic Cells</h3>
<ul>
<li><strong>Authors: </strong>Yukio Ohsawa, Dingming Xue, Kaira Sekiguchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16110">https://arxiv.org/abs/2407.16110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16110">https://arxiv.org/pdf/2407.16110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16110]] Analyzing the Polysemy Evolution using Semantic Cells(https://arxiv.org/abs/2407.16110)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>The senses of words evolve. The sense of the same word may change from today to tomorrow, and multiple senses of the same word may be the result of the evolution of each other, that is, they may be parents and children. If we view Juba as an evolving ecosystem, the paradigm of learning the correct answer, which does not move with the sense of a word, is no longer valid. This paper is a case study that shows that word polysemy is an evolutionary consequence of the modification of Semantic Cells, which has al-ready been presented by the author, by introducing a small amount of diversity in its initial state as an example of analyzing the current set of short sentences. In particular, the analysis of a sentence sequence of 1000 sentences in some order for each of the four senses of the word Spring, collected using Chat GPT, shows that the word acquires the most polysemy monotonically in the analysis when the senses are arranged in the order in which they have evolved. In other words, we present a method for analyzing the dynamism of a word's acquiring polysemy with evolution and, at the same time, a methodology for viewing polysemy from an evolutionary framework rather than a learning-based one.</li>
<li><strong>摘要：</strong>词义在不断演变。同一个词的词义可能从今天到明天发生变化，同一个词的多种词义可能是彼此演变的结果，也就是说，它们可能是父母和孩子。如果我们将朱巴视为一个不断发展的生态系统，那么学习正确答案的范式（不随词义的变化而变化）就不再有效。本文是一个案例研究，表明词义多义性是语义细胞修改的进化结果，作者已经提出了这一点，通过在其初始状态中引入少量多样性作为分析当前短句集的示例。特别是，使用 Chat GPT 收集了 1000 个句子的句子序列，其中每个句子按某种顺序排列，每个句子都有四个词义，结果表明，当词义按其进化顺序排列时，该词在分析中获得最多的单调多义性。换句话说，我们提出了一种分析词语随着进化而获得多义性的动态方法，同时提出了一种从进化框架而不是基于学习的框架来看待多义性的方法。</li>
</ul>

<h3>Title: Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16127">https://arxiv.org/abs/2407.16127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16127">https://arxiv.org/pdf/2407.16127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16127]] Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion(https://arxiv.org/abs/2407.16127)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Traditional knowledge graph (KG) completion models learn embeddings to predict missing facts. Recent works attempt to complete KGs in a text-generation manner with large language models (LLMs). However, they need to ground the output of LLMs to KG entities, which inevitably brings errors. In this paper, we present a finetuning framework, DIFT, aiming to unleash the KG completion ability of LLMs and avoid grounding errors. Given an incomplete fact, DIFT employs a lightweight model to obtain candidate entities and finetunes an LLM with discrimination instructions to select the correct one from the given candidates. To improve performance while reducing instruction data, DIFT uses a truncated sampling method to select useful facts for finetuning and injects KG embeddings into the LLM. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed framework.</li>
<li><strong>摘要：</strong>传统知识图谱 (KG) 补全模型通过学习嵌入来预测缺失的事实。近期的研究尝试使用大型语言模型 (LLM) 以文本生成的方式补全 KG。然而，他们需要将 LLM 的输出接地到 KG 实体，这不可避免地会带来错误。在本文中，我们提出了一个微调框架 DIFT，旨在释放 LLM 的 KG 补全能力并避免接地错误。给定一个不完整的事实，DIFT 采用轻量级模型来获取候选实体，并使用判别指令对 LLM 进行微调以从给定的候选实体中选择正确的实体。为了在减少指令数据的同时提高性能，DIFT 使用截断采样方法来选择有用的事实进行微调，并将 KG 嵌入注入 LLM。在基准数据集上的大量实验证明了我们提出的框架的有效性。</li>
</ul>

<h3>Title: CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support</h3>
<ul>
<li><strong>Authors: </strong>Chao-Chun Hsu, Erin Bransom, Jenna Sparks, Bailey Kuehl, Chenhao Tan, David Wadden, Lucy Lu Wang, Aakanksha Naik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16148">https://arxiv.org/abs/2407.16148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16148">https://arxiv.org/pdf/2407.16148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16148]] CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support(https://arxiv.org/abs/2407.16148)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review.</li>
<li><strong>摘要：</strong>文献综述需要研究人员综合大量信息，随着科学文献的扩大，文献综述也变得越来越具有挑战性。在这项工作中，我们研究了 LLM 生成科学研究的层次结构以协助研究人员进行文献综述的潜力。我们将层次结构组织定义为树结构，其中节点引用主题类别，每个节点都链接到分配给该类别的研究。我们基于 LLM 的简单流程从一组研究中生成层次结构，产生了有希望但不完善的层次结构，这促使我们收集 CHIME，这是一个由专家策划的数据集，用于专注于生物医学的这项任务。鉴于从头开始构建层次结构的挑战性和耗时性，我们使用了一个人机交互过程，其中专家纠正 LLM 生成的层次结构中的错误（类别和研究分配之间的链接）。CHIME 包含 2,174 个 LLM 生成的层次结构，涵盖 472 个主题，以及专家纠正的 100 个主题子集的层次结构。专家校正使我们能够量化 LLM 的表现，我们发现，虽然他们在生成和组织类别方面相当出色，但他们对研究的类别分配还有待改进。我们尝试使用人工反馈来训练校正模型，该模型将研究分配提高了 12.6 个 F1 点。我们发布我们的数据集和模型，以鼓励研究开发更好的文献综述辅助工具。</li>
</ul>

<h3>Title: DDK: Distilling Domain Knowledge for Efficient Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que, Ken Deng, Zhiqi Bai, Jie Liu, Ge Zhang, Jiakai Wang, Yanan Wu, Congnan Liu, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16154">https://arxiv.org/abs/2407.16154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16154">https://arxiv.org/pdf/2407.16154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16154]] DDK: Distilling Domain Knowledge for Efficient Large Language Models(https://arxiv.org/abs/2407.16154)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the performance of a smaller LLM (i.e., the student model) by transferring knowledge from a high-performing LLM (i.e., the teacher model). Prevailing techniques in LLM distillation typically use a black-box model API to generate high-quality pretrained and aligned datasets, or utilize white-box distillation by altering the loss function to better transfer knowledge from the teacher LLM. However, these methods ignore the knowledge differences between the student and teacher LLMs across domains. This results in excessive focus on domains with minimal performance gaps and insufficient attention to domains with large gaps, reducing overall performance. In this paper, we introduce a new LLM distillation framework called DDK, which dynamically adjusts the composition of the distillation dataset in a smooth manner according to the domain performance differences between the teacher and student models, making the distillation process more stable and effective. Extensive evaluations show that DDK significantly improves the performance of student models, outperforming both continuously pretrained baselines and existing knowledge distillation methods by a large margin.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 在各种应用中都具有先进的智能能力，但它们仍然面临着巨大的计算和存储需求。知识蒸馏 (KD) 已成为一种有效的策略，可通过从高性能 LLM（即教师模型）迁移知识来提高小型 LLM（即学生模型）的性能。LLM 蒸馏中的主流技术通常使用黑盒模型 API 来生成高质量的预训练和对齐数据集，或者通过改变损失函数来利用白盒蒸馏来更好地从教师 LLM 迁移知识。然而，这些方法忽略了学生和教师 LLM 之间跨领域的知识差异。这导致过度关注性能差距最小的领域，而对差距较大的领域关注不足，从而降低了整体性能。在本文中，我们介绍了一种名为 DDK 的新 LLM 蒸馏框架，它根据教师和学生模型之间的领域性能差异以平滑的方式动态调整蒸馏数据集的组成，使蒸馏过程更加稳定和有效。大量评估表明，DDK 显著提高了学生模型的性能，大大超越了连续预训练基线和现有的知识提炼方法。</li>
</ul>

<h3>Title: Robust Privacy Amidst Innovation with Large Language Models Through a Critical Assessment of the Risks</h3>
<ul>
<li><strong>Authors: </strong>Yao-Shun Chuang, Atiquer Rahman Sarkar, Noman Mohammed, Xiaoqian Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16166">https://arxiv.org/abs/2407.16166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16166">https://arxiv.org/pdf/2407.16166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16166]] Robust Privacy Amidst Innovation with Large Language Models Through a Critical Assessment of the Risks(https://arxiv.org/abs/2407.16166)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This study examines integrating EHRs and NLP with large language models (LLMs) to improve healthcare data management and patient care. It focuses on using advanced models to create secure, HIPAA-compliant synthetic patient notes for biomedical research. The study used de-identified and re-identified MIMIC III datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes. Text generation employed templates and keyword extraction for contextually relevant notes, with one-shot generation for comparison. Privacy assessment checked PHI occurrence, while text utility was tested using an ICD-9 coding task. Text quality was evaluated with ROUGE and cosine similarity metrics to measure semantic similarity with source notes. Analysis of PHI occurrence and text utility via the ICD-9 coding task showed that the keyword-based method had low risk and good performance. One-shot generation showed the highest PHI exposure and PHI co-occurrence, especially in geographic location and date categories. The Normalized One-shot method achieved the highest classification accuracy. Privacy analysis revealed a critical balance between data utility and privacy protection, influencing future data use and sharing. Re-identified data consistently outperformed de-identified data. This study demonstrates the effectiveness of keyword-based methods in generating privacy-protecting synthetic clinical notes that retain data usability, potentially transforming clinical data-sharing practices. The superior performance of re-identified over de-identified data suggests a shift towards methods that enhance utility and privacy by using dummy PHIs to perplex privacy attacks.</li>
<li><strong>摘要：</strong>本研究探讨了将 EHR 和 NLP 与大型语言模型 (LLM) 相结合以改善医疗数据管理和患者护理。它侧重于使用高级模型为生物医学研究创建安全、符合 HIPAA 要求的合成患者笔记。该研究使用去识别和重新识别的 MIMIC III 数据集以及 GPT-3.5、GPT-4 和 Mistral 7B 来生成合成笔记。文本生成使用模板和关键字提取来生成上下文相关的笔记，并使用一次性生成进行比较。隐私评估检查了 PHI 的发生情况，而文本实用性则使用 ICD-9 编码任务进行测试。使用 ROUGE 和余弦相似度指标评估文本质量，以衡量与源笔记的语义相似性。通过 ICD-9 编码任务对 PHI 的发生情况和文本实用性的分析表明，基于关键字的方法风险低且性能良好。一次性生成显示出最高的 PHI 暴露率和 PHI 共现率，尤其是在地理位置和日期类别中。标准化一次性方法实现了最高的分类准确率。隐私分析揭示了数据效用和隐私保护之间的关键平衡，影响着未来的数据使用和共享。重新识别的数据始终优于去识别的数据。这项研究证明了基于关键字的方法在生成保护隐私的合成临床笔记方面的有效性，这些笔记保留了数据的可用性，有可能改变临床数据共享实践。重新识别的数据比去识别的数据具有更优的性能，这表明人们正在转向使用虚拟 PHI 来迷惑隐私攻击，从而提高效用和隐私的方法。</li>
</ul>

<h3>Title: Graph-Structured Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16207">https://arxiv.org/abs/2407.16207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16207">https://arxiv.org/pdf/2407.16207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16207]] Graph-Structured Speculative Decoding(https://arxiv.org/abs/2407.16207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative decoding has emerged as a promising technique to accelerate the inference of Large Language Models (LLMs) by employing a small language model to draft a hypothesis sequence, which is then validated by the LLM. The effectiveness of this approach heavily relies on the balance between performance and efficiency of the draft model. In our research, we focus on enhancing the proportion of draft tokens that are accepted to the final output by generating multiple hypotheses instead of just one. This allows the LLM more options to choose from and select the longest sequence that meets its standards. Our analysis reveals that hypotheses produced by the draft model share many common token sequences, suggesting a potential for optimizing computation. Leveraging this observation, we introduce an innovative approach utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This structure enables us to efficiently predict and merge recurring token sequences, vastly reducing the computational demands of the draft model. We term this approach Graph-structured Speculative Decoding (GSD). We apply GSD across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly surpassing standard speculative decoding.</li>
<li><strong>摘要：</strong>推测解码已成为一种有前途的技术，它通过使用小型语言模型来起草假设序列，然后由 LLM 进行验证，从而加速大型语言模型 (LLM) 的推理。这种方法的有效性在很大程度上取决于草稿模型的性能和效率之间的平衡。在我们的研究中，我们专注于通过生成多个假设而不是一个假设来提高最终输出中接受的草稿标记的比例。这为 LLM 提供了更多选择，并选择符合其标准的最长序列。我们的分析表明，草稿模型产生的假设共享许多共同的标记序列，这表明优化计算的潜力。利用这一观察结果，我们引入了一种创新方法，利用有向无环图 (DAG) 来管理起草的假设。这种结构使我们能够有效地预测和合并重复的标记序列，大大减少了草稿模型的计算需求。我们将这种方法称为图结构推测解码 (GSD)。我们将 GSD 应用于一系列 LLM，包括 700 亿参数的 LLaMA-2 模型，并观察到显著的加速，从 1.73$\times$ 到 1.96$\times$，大大超过了标准推测解码。</li>
</ul>

<h3>Title: A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham Mehrotra, Zixu (James)Zhu, Xiang-Bo Mao, Sitaram Asur, Na (Claire)Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16216">https://arxiv.org/abs/2407.16216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16216">https://arxiv.org/pdf/2407.16216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16216]] A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More(https://arxiv.org/abs/2407.16216)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With advancements in self-supervised learning, the availability of trillions tokens in a pre-training corpus, instruction fine-tuning, and the development of large Transformers with billions of parameters, large language models (LLMs) are now capable of generating factual and coherent responses to human queries. However, the mixed quality of training data can lead to the generation of undesired responses, presenting a significant challenge. Over the past two years, various methods have been proposed from different perspectives to enhance LLMs, particularly in aligning them with human expectation. Despite these efforts, there has not been a comprehensive survey paper that categorizes and details these approaches. In this work, we aim to address this gap by categorizing these papers into distinct topics and providing detailed explanations of each alignment method, thereby helping readers gain a thorough understanding of the current state of the field.</li>
<li><strong>摘要：</strong>随着自监督学习的进步、预训练语料库中数万亿个 token 的出现、指令微调以及具有数十亿个参数的大型 Transformer 的开发，大型语言模型 (LLM) 现在能够对人类查询生成真实且连贯的响应。然而，训练数据的质量参差不齐可能会导致生成不理想的响应，这是一个重大挑战。在过去的两年中，从不同角度提出了各种方法来增强 LLM，特别是使其符合人类的期望。尽管做出了这些努力，但还没有一篇全面的综述论文对这些方法进行分类和详细介绍。在本文中，我们旨在通过将这些论文分类为不同的主题并详细解释每种对齐方法来解决这一空白，从而帮助读者彻底了解该领域的现状。</li>
</ul>

<h3>Title: Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nishanth Madhusudhan, Sathwik Tejaswi Madhusudhan, Vikas Yadav, Masoud Hashemi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16221">https://arxiv.org/abs/2407.16221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16221">https://arxiv.org/pdf/2407.16221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16221]] Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models(https://arxiv.org/abs/2407.16221)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) achieve remarkable performance across various NLP tasks, their reliability becomes essential for widespread adoption. This paper focuses on Abstention Ability (AA), a critical yet under explored aspect of reliability - the ability of LLMs to refrain from answering questions when they are uncertain or when definitive answer is not possible, while maintaining question-answering (QA) task performance. While previous works have focused on understanding the recollection abilities of LLMs or their ability to identify imponderable/unanswerable questions, we believe there is a need for an effective AA evaluation method. Therefore, we propose a black-box evaluation methodology to examine and understand the AA of LLMs across a variety of multiple-choice QA tasks. We measure AA by rewarding models for abstaining from answering when their predictions are incorrect or when the questions are inherently unanswerable. We investigate three strategies, Strict Prompting, Verbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their impact on abstention across different LLMs. Our findings reveal that while even state-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting such as CoT, can significantly enhance this ability. Furthermore, we demonstrate that improving AA also leads to better overall QA task performance, underscoring the importance of evaluating AA in LLMs.</li>
<li><strong>摘要：</strong>由于大型语言模型 (LLM) 在各种 NLP 任务中都取得了卓越的表现，因此它们的可靠性对于广泛采用至关重要。本文重点研究弃权能力 (AA)，这是可靠性的一个关键但尚未得到充分探索的方面 - LLM 在问题不确定或无法给出明确答案时避免回答问题，同时保持问答 (QA) 任务性能的能力。虽然之前的研究主要集中在了解 LLM 的回忆能力或其识别无法衡量/无法回答的问题的能力，但我们认为需要一种有效的 AA 评估方法。因此，我们提出了一种黑盒评估方法来检查和了解 LLM 在各种多项选择 QA 任务中的 AA。我们通过奖励模型在其预测不正确或问题本质上无法回答时弃权来衡量 AA。我们研究了三种策略，即严格提示、言语信心阈值和思维链 (CoT)，以了解它们对不同 LLM 中弃权的影响。我们的研究结果表明，尽管像 GPT-4 这样最先进的 LLM 也难以实现弃权，但像 CoT 这样的策略提示可以显著增强这种能力。此外，我们证明，改进 AA 还可以提高整体 QA 任务的表现，这强调了在 LLM 中评估 AA 的重要性。</li>
</ul>

<h3>Title: PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiahuan Li, Shujian Huang, Xinyu Dai, Jiajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16222">https://arxiv.org/abs/2407.16222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16222">https://arxiv.org/pdf/2407.16222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16222]] PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment(https://arxiv.org/abs/2407.16222)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate reasonable multilingual abilities, despite predominantly English-centric pretraining. However, the spontaneous multilingual alignment in these models is shown to be weak, leading to unsatisfactory cross-lingual transfer and knowledge sharing. Previous works attempt to address this issue by explicitly injecting multilingual alignment information during or after pretraining. Thus for the early stage in pretraining, the alignment is weak for sharing information or knowledge across languages. In this paper, we propose PreAlign, a framework that establishes multilingual alignment prior to language model pretraining. PreAlign injects multilingual alignment by initializing the model to generate similar representations of aligned words and preserves this alignment using a code-switching strategy during pretraining. Extensive experiments in a synthetic English to English-Clone setting demonstrate that PreAlign significantly outperforms standard multilingual joint training in language modeling, zero-shot cross-lingual transfer, and cross-lingual knowledge application. Further experiments in real-world scenarios further validate PreAlign's effectiveness across various model sizes.</li>
<li><strong>摘要：</strong>尽管预训练以英语为主，大型语言模型仍表现出合理的多语言能力。然而，这些模型中的自发多语言对齐被证明是薄弱的，导致跨语言迁移和知识共享不令人满意。先前的研究试图通过在预训练期间或之后明确注入多语言对齐信息来解决此问题。因此，在预训练的早期阶段，对齐对于跨语言共享信息或知识来说很弱。在本文中，我们提出了 PreAlign，这是一个在语言模型预训练之前建立多语言对齐的框架。PreAlign 通过初始化模型以生成对齐单词的相似表示来注入多语言对齐，并在预训练期间使用代码切换策略保留这种对齐。在合成英语到英语克隆设置中进行的大量实验表明，PreAlign 在语言建模、零样本跨语言迁移和跨语言知识应用方面明显优于标准多语言联合训练。在现实场景中的进一步实验进一步验证了 PreAlign 在不同模型大小中的有效性。</li>
</ul>

<h3>Title: LawLuo: A Chinese Law Firm Co-run by LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Jingyun Sun, Chengxiao Dai, Zhongze Luo, Yangbo Chang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16252">https://arxiv.org/abs/2407.16252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16252">https://arxiv.org/pdf/2407.16252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16252]] LawLuo: A Chinese Law Firm Co-run by LLM Agents(https://arxiv.org/abs/2407.16252)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate substantial potential in delivering legal consultation services to users without a legal background, attributed to their superior text comprehension and generation capabilities. Nonetheless, existing Chinese legal LLMs limit interaction to a single model-user dialogue, unlike the collaborative consultations typical of law firms, where multiple staff members contribute to a single consultation. This limitation prevents an authentic consultation experience. Additionally, extant Chinese legal LLMs suffer from critical limitations: (1) insufficient control over the quality of instruction fine-tuning data; (2) increased model hallucination resulting from users' ambiguous queries; and (3) a reduction in the model's ability to follow instructions over multiple dialogue turns. In response to these challenges, we propose a novel legal dialogue framework that leverages the collaborative capabilities of multiple LLM agents, termed LawLuo. This framework encompasses four agents: a receptionist, a lawyer, a secretary, and a boss, each responsible for different functionalities, collaboratively providing a comprehensive legal consultation to users. Additionally, we constructed two high-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned ChatGLM-3-6b using these datasets. We propose a legal query clarification algorithm called ToLC. Experimental results demonstrate that LawLuo outperforms baseline LLMs, including GPT-4, across three dimensions: lawyer-like language style, the usefulness of legal advice, and the accuracy of legal knowledge. Our code and datasets are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 凭借其出色的文本理解和生成能力，在为没有法律背景的用户提供法律咨询服务方面展现出巨大潜力。尽管如此，现有的中国法律 LLM 将交互限制为单个模型与用户对话，这与律师事务所典型的协作咨询不同，在协作咨询中，多名工作人员参与一次咨询。这种限制阻碍了真实的咨询体验。此外，现有的中国法律 LLM 还存在严重局限性：(1) 对指令微调数据质量的控制不足；(2) 用户模糊查询导致模型幻觉增加；(3) 模型在多个对话轮次中遵循指令的能力下降。为了应对这些挑战，我们提出了一种利用多个 LLM 代理的协作能力的新颖法律对话框架，称为 LawLuo。该框架包含四个代理：接待员、律师、秘书和老板，每个代理负责不同的功能，协作为用户提供全面的法律咨询。此外，我们构建了两个高质量的法律对话数据集 KINLED 和 MURLED，并利用这些数据集对 ChatGLM-3-6b 进行了微调。我们提出了一种名为 ToLC 的法律查询澄清算法。实验结果表明，LawLuo 在三个维度上的表现优于基线 LLM（包括 GPT-4）：律师般的语言风格、法律建议的实用性和法律知识的准确性。我们的代码和数据集可在此 https URL 上找到。</li>
</ul>

<h3>Title: Beyond Binary Gender: Evaluating Gender-Inclusive Machine Translation with Ambiguous Attitude Words</h3>
<ul>
<li><strong>Authors: </strong>Yijie Chen, Yijin Liu, Fandong Meng, Jinan Xu, Yufeng Chen, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16266">https://arxiv.org/abs/2407.16266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16266">https://arxiv.org/pdf/2407.16266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16266]] Beyond Binary Gender: Evaluating Gender-Inclusive Machine Translation with Ambiguous Attitude Words(https://arxiv.org/abs/2407.16266)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Gender bias has been a focal point in the study of bias in machine translation and language models. Existing machine translation gender bias evaluations are primarily focused on male and female genders, limiting the scope of the evaluation. To assess gender bias accurately, these studies often rely on calculating the accuracy of gender pronouns or the masculine and feminine attributes of grammatical gender via the stereotypes triggered by occupations or sentiment words ({\em i.e.}, clear positive or negative attitude), which cannot extend to non-binary groups. This study presents a benchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude words), which assesses gender bias beyond binary gender. Meanwhile, we propose a novel process to evaluate gender bias based on the Emotional Attitude Score (EAS), which is used to quantify ambiguous attitude words. In evaluating three recent and effective open-source LLMs and one powerful multilingual translation-specific model, our main observations are: (1) The translation performance within non-binary gender contexts is markedly inferior in terms of translation quality and exhibits more negative attitudes than binary-gender contexts. (2) The analysis experiments indicate that incorporating constraint context in prompts for gender identity terms can substantially reduce translation bias, while the bias remains evident despite the presence of the constraints. The code is publicly available at \url{this https URL}.</li>
<li><strong>摘要：</strong>性别偏见一直是机器翻译和语言模型偏见研究的焦点。现有的机器翻译性别偏见评估主要集中于男性和女性，限制了评估范围。为了准确评估性别偏见，这些研究通常依赖于通过职业或情感词（即明确的积极或消极态度）引发的刻板印象来计算性别代词或语法性别的男性和女性属性的准确性，而这无法扩展到非二元群体。本研究提出了一个基准 AmbGIMT（带有模糊态度词的性别包容性机器翻译），它可以评估二元性别之外的性别偏见。同时，我们提出了一种基于情感态度分数（EAS）评估性别偏见的新方法，该分数用于量化模糊态度词。在评估三个近期有效的开源法学硕士和一个强大的多语言翻译专用模型时，我们的主要观察结果是：（1）非二元性别语境中的翻译质量明显较差，且比二元性别语境表现出更多的负面态度。（2）分析实验表明，在性别认同术语提示中加入约束语境可以大大减少翻译偏见，而即使存在约束，偏见仍然很明显。代码可在 \url{此 https URL} 上公开获取。</li>
</ul>

<h3>Title: FACTTRACK: Time-Aware World State Tracking in Story Outlines</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Lyu, Kevin Yang, Lingpeng Kong, Daniel Klein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16347">https://arxiv.org/abs/2407.16347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16347">https://arxiv.org/pdf/2407.16347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16347]] FACTTRACK: Time-Aware World State Tracking in Story Outlines(https://arxiv.org/abs/2407.16347)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>While accurately detecting and correcting factual contradictions in language model outputs has become increasingly important as their capabilities improve, doing so is highly challenging. We propose a novel method, FACTTRACK, for tracking atomic facts and addressing factual contradictions. Crucially, FACTTRACK also maintains time-aware validity intervals for each fact, allowing for change over time. At a high level, FACTTRACK consists of a four-step pipeline to update a world state data structure for each new event: (1) decompose the event into directional atomic facts; (2) determine the validity interval of each atomic fact using the world state; (3) detect contradictions with existing facts in the world state; and finally (4) add new facts to the world state and update existing atomic facts. When we apply FACTTRACK to contradiction detection on structured story outlines, we find that FACTTRACK using LLaMA2-7B-Chat substantially outperforms a fair baseline using LLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline. Moreover, when using GPT4, FACTTRACK significantly outperforms the GPT4 baseline.</li>
<li><strong>摘要：</strong>随着语言模型能力的提升，准确检测和纠正语言模型输出中的事实矛盾变得越来越重要，但这样做却极具挑战性。我们提出了一种新方法 FACTTRACK，用于跟踪原子事实和解决事实矛盾。至关重要的是，FACTTRACK 还为每个事实维护时间感知的有效性间隔，允许随时间而变化。从高层次上讲，FACTTRACK 由一个四步流水线组成，用于更新每个新事件的世界状态数据结构：（1）将事件分解为定向原子事实；（2）使用世界状态确定每个原子事实的有效性间隔；（3）检测与世界状态中现有事实的矛盾；最后（4）向世界状态添加新事实并更新现有的原子事实。当我们将 FACTTRACK 应用于结构化故事大纲的矛盾检测时，我们发现使用 LLaMA2-7B-Chat 的 FACTTRACK 远远优于使用 LLaMA2-7B-Chat 的公平基线，并且实现了与 GPT4 基线相当的性能。此外，在使用 GPT4 时，FACTTRACK 的表现明显优于 GPT4 基线。</li>
</ul>

<h3>Title: Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Rithik Sachdev, Zhong-Qiu Wang, Chao-Han Huck Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16370">https://arxiv.org/abs/2407.16370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16370">https://arxiv.org/pdf/2407.16370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16370]] Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction(https://arxiv.org/abs/2407.16370)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Building upon the strength of modern large language models (LLMs), generative error correction (GEC) has emerged as a promising paradigm that can elevate the performance of modern automatic speech recognition (ASR) systems. One representative approach is to leverage in-context learning to prompt LLMs so that a better hypothesis can be generated by the LLMs based on a carefully-designed prompt and an $N$-best list of hypotheses produced by ASR systems. However, it is yet unknown whether the existing prompts are the most effective ones for the task of post-ASR error correction. In this context, this paper first explores alternative prompts to identify an initial set of effective prompts, and then proposes to employ an evolutionary prompt optimization algorithm to refine the initial prompts. Evaluations results on the CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show the effectiveness and potential of the proposed algorithms.</li>
<li><strong>摘要：</strong>基于现代大型语言模型 (LLM) 的优势，生成性纠错 (GEC) 已成为一种有前途的范例，可以提升现代自动语音识别 (ASR) 系统的性能。一种代表性方法是利用上下文学习来提示 LLM，以便 LLM 能够根据精心设计的提示和 ASR 系统生成的 $N$ 最佳假设列表生成更好的假设。然而，现有提示是否是 ASR 后纠错任务最有效的提示尚不清楚。在此背景下，本文首先探索替代提示以确定一组初始的有效提示，然后提出采用进化提示优化算法来细化初始提示。在 SLT $2024$ GenSEC 挑战赛任务 $1$ 的 CHiME-4 子集上的评估结果证明了所提出算法的有效性和潜力。</li>
</ul>

<h3>Title: TookaBERT: A Step Forward for Persian NLU</h3>
<ul>
<li><strong>Authors: </strong>MohammadAli SadraeiJavaheri, Ali Moghaddaszadeh, Milad Molazadeh, Fariba Naeiji, Farnaz Aghababaloo, Hamideh Rafiee, Zahra Amirmahani, Tohid Abedini, Fatemeh Zahra Sheikhi, Amirmohammad Salehoof</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16382">https://arxiv.org/abs/2407.16382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16382">https://arxiv.org/pdf/2407.16382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16382]] TookaBERT: A Step Forward for Persian NLU(https://arxiv.org/abs/2407.16382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The field of natural language processing (NLP) has seen remarkable advancements, thanks to the power of deep learning and foundation models. Language models, and specifically BERT, have been key players in this progress. In this study, we trained and introduced two new BERT models using Persian data. We put our models to the test, comparing them to seven existing models across 14 diverse Persian natural language understanding (NLU) tasks. The results speak for themselves: our larger model outperforms the competition, showing an average improvement of at least +2.8 points. This highlights the effectiveness and potential of our new BERT models for Persian NLU tasks.</li>
<li><strong>摘要：</strong>得益于深度学习和基础模型的强大功能，自然语言处理 (NLP) 领域取得了显著进步。语言模型，尤其是 BERT，是这一进步的关键因素。在这项研究中，我们使用波斯语数据训练并引入了两个新的 BERT 模型。我们对我们的模型进行了测试，在 14 个不同的波斯语自然语言理解 (NLU) 任务中将它们与 7 个现有模型进行了比较。结果不言而喻：我们的大型模型优于竞争对手，平均提高了至少 +2.8 分。这凸显了我们的新 BERT 模型在波斯语 NLU 任务中的有效性和潜力。</li>
</ul>

<h3>Title: FairFlow: An Automated Approach to Model-based Counterfactual Data Augmentation For NLP</h3>
<ul>
<li><strong>Authors: </strong>Ewoenam Kwaku Tokpo, Toon Calders</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16431">https://arxiv.org/abs/2407.16431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16431">https://arxiv.org/pdf/2407.16431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16431]] FairFlow: An Automated Approach to Model-based Counterfactual Data Augmentation For NLP(https://arxiv.org/abs/2407.16431)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite the evolution of language models, they continue to portray harmful societal biases and stereotypes inadvertently learned from training data. These inherent biases often result in detrimental effects in various applications. Counterfactual Data Augmentation (CDA), which seeks to balance demographic attributes in training data, has been a widely adopted approach to mitigate bias in natural language processing. However, many existing CDA approaches rely on word substitution techniques using manually compiled word-pair dictionaries. These techniques often lead to out-of-context substitutions, resulting in potential quality issues. The advancement of model-based techniques, on the other hand, has been challenged by the need for parallel training data. Works in this area resort to manually generated parallel data that are expensive to collect and are consequently limited in scale. This paper proposes FairFlow, an automated approach to generating parallel data for training counterfactual text generator models that limits the need for human intervention. Furthermore, we show that FairFlow significantly overcomes the limitations of dictionary-based word-substitution approaches whilst maintaining good performance.</li>
<li><strong>摘要：</strong>尽管语言模型不断发展，但它们仍然刻意刻画从训练数据中无意中学到的有害社会偏见和刻板印象。这些固有偏见通常会对各种应用产生不利影响。反事实数据增强 (CDA) 旨在平衡训练数据中的人口统计属性，已成为一种广泛采用的减轻自然语言处理偏见的方法。然而，许多现有的 CDA 方法依赖于使用手动编译的词对词典的单词替换技术。这些技术通常会导致脱离上下文的替换，从而导致潜在的质量问题。另一方面，基于模型的技术的进步受到对并行训练数据需求的挑战。该领域的研究依赖于手动生成的并行数据，这些数据收集成本高昂，因此规模有限。本文提出了 FairFlow，这是一种自动生成并行数据以训练反事实文本生成器模型的方法，可限制对人工干预的需求。此外，我们表明 FairFlow 显著克服了基于词典的单词替换方法的局限性，同时保持了良好的性能。</li>
</ul>

<h3>Title: Enhancing LLM's Cognition via Structurization</h3>
<ul>
<li><strong>Authors: </strong>Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16434">https://arxiv.org/abs/2407.16434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16434">https://arxiv.org/pdf/2407.16434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16434]] Enhancing LLM's Cognition via Structurization(https://arxiv.org/abs/2407.16434)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>When reading long-form text, human cognition is complex and structurized. While large language models (LLMs) process input contexts through a causal and sequential perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively. To enhance LLM's cognition capability, this paper presents a novel concept of context structurization. Specifically, we transform the plain, unordered contextual sentences into well-ordered and hierarchically structurized elements. By doing so, LLMs can better grasp intricate and extended contexts through precise attention and information-seeking along the organized structures. Extensive evaluations are conducted across various model architectures and sizes (including several 7B- to 72B-size auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval). Empirical results show consistent and significant performance gains afforded by a single-round structurization. In particular, we boost a 72B-parameter open-source model to achieve comparable performance against GPT-3.5-Turbo as the hallucination evaluator. Besides, we show the feasibility of distilling advanced LLMs' language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach. Code will be made public soon.</li>
<li><strong>摘要：</strong>阅读长文本时，人类的认知是复杂且结构化的。虽然大型语言模型 (LLM) 通过因果和顺序的视角处理输入上下文，但这种方法可能会限制它们有效处理错综复杂的输入的能力。为了增强 LLM 的认知能力，本文提出了一种新的上下文结构化概念。具体来说，我们将简单的、无序的上下文句子转换为有序且层次结构化的元素。通过这样做，LLM 可以通过精确的注意力和沿着有组织的结构进行信息搜索来更好地掌握复杂和扩展的上下文。在各种模型架构和大小（包括几个 7B 到 72B 大小的自回归 LLM 以及类似 BERT 的掩蔽模型）上对各种 NLP 任务（例如，基于上下文的问答、详尽的幻觉评估和段落级密集检索）进行了广泛的评估。实证结果表明，单轮结构化可以带来持续且显著的性能提升。具体来说，我们提升了一个 72B 参数开源模型，使其作为幻觉评估器的性能与 GPT-3.5-Turbo 相当。此外，我们展示了将高级 LLM 的语言处理能力提炼到更小但有效的 StruXGPT-7B 来执行结构化的可行性，解决了我们方法的实用性问题。代码将很快公开。</li>
</ul>

<h3>Title: Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kenza Benkirane (1), Laura Gongas (1), Shahar Pelles (1), Naomi Fuchs (1), Joshua Darmon (1), Pontus Stenetorp (1), David Ifeoluwa Adelani (1), Eduardo Sanchez (1 and 2) ((1) University College London, (2) Meta)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16470">https://arxiv.org/abs/2407.16470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16470">https://arxiv.org/pdf/2407.16470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16470]] Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models(https://arxiv.org/abs/2407.16470)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.</li>
<li><strong>摘要：</strong>大规模多语言机器翻译系统的最新进展显著提高了翻译准确性；然而，即使是表现最好的系统也会产生幻觉，严重影响用户的信任。检测机器翻译 (MT) 中的幻觉仍然是一项关键挑战，特别是因为现有方法在高资源语言 (HRL) 方面表现出色，但在应用于低资源语言 (LRL) 时表现出很大的局限性。本文评估了使用大型语言模型 (LLM) 和大规模多语言嵌入中的语义相似性的幻觉检测方法。我们的研究涵盖了 16 个语言方向，涵盖了 HRL、LRL 和各种脚本。我们发现模型的选择对于性能至关重要。平均而言，对于 HRL，Llama3-70B 的表现比之前最先进的水平高出 0.16 MCC（马修斯相关系数）。然而，对于 LRL，我们观察到 Claude Sonnet 的表现平均比其他 LLM 高出 0.03 MCC。我们研究的关键结论是，尽管 LLM 没有接受过任何机器翻译任务的明确训练，但它可以实现与之前提出的模型相当甚至更好的性能。然而，它们对 LRL 的优势并不那么明显。</li>
</ul>

<h3>Title: AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Chi, Lingjun Mao, Zineng Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16521">https://arxiv.org/abs/2407.16521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16521">https://arxiv.org/pdf/2407.16521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16521]] AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game(https://arxiv.org/abs/2407.16521)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming. This paper focuses on creating proxies of human behavior in simulated environments, with \textit{Among Us} utilized as a tool for studying simulated human behavior. The study introduces a text-based game environment, named AmongAgent, that mirrors the dynamics of \textit{Among Us}. Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew. Within this environment, the behavior of simulated language agents is analyzed. The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes. Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context. This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.</li>
<li><strong>摘要：</strong>战略性社交推理游戏是评估语言模型理解和推理能力的宝贵试验台，为社会科学、人工智能和战略游戏提供了重要见解。本文重点介绍如何在模拟环境中创建人类行为的代理，并使用 \textit{Among Us} 作为研究模拟人类行为的工具。该研究引入了一个名为 AmongAgent 的基于文本的游戏环境，它反映了 \textit{Among Us} 的动态。玩家扮演宇宙飞船上的船员，负责识别破坏飞船并消灭船员的冒名顶替者。在这个环境中，分析了模拟语言代理的行为。实验涉及具有不同配置的船员和冒名顶替者人格原型的各种游戏序列。我们的工作表明，最先进的大型语言模型 (LLM) 可以有效地掌握游戏规则并根据当前上下文做出决策。这项工作旨在促进对具有不完整信息和复杂动作空间的目标导向游戏中的 LLM 的进一步探索，因为这些设置为评估社会驱动场景中的语言模型性能提供了宝贵的机会。</li>
</ul>

<h3>Title: Quantifying the Role of Textual Predictability in Automatic Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sean Robertson, Gerald Penn, Ewan Dunbar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16537">https://arxiv.org/abs/2407.16537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16537">https://arxiv.org/pdf/2407.16537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16537]] Quantifying the Role of Textual Predictability in Automatic Speech Recognition(https://arxiv.org/abs/2407.16537)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A long-standing question in automatic speech recognition research is how to attribute errors to the ability of a model to model the acoustics, versus its ability to leverage higher-order context (lexicon, morphology, syntax, semantics). We validate a novel approach which models error rates as a function of relative textual predictability, and yields a single number, $k$, which measures the effect of textual predictability on the recognizer. We use this method to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use of textual context than a hybrid ASR model, in spite of not using an explicit language model, and also use it to shed light on recent results demonstrating poor performance of standard ASR systems on African-American English. We demonstrate that these mostly represent failures of acoustic--phonetic modelling. We show how this approach can be used straightforwardly in diagnosing and improving ASR.</li>
<li><strong>摘要：</strong>自动语音识别研究中一个长期存在的问题是，如何将错误归因于模型建模声学的能力，而不是利用高阶上下文（词汇、形态、语法、语义）的能力。我们验证了一种新方法，该方法将错误率建模为相对文本可预测性的函数，并得出一个数字 $k$，用于衡量文本可预测性对识别器的影响。我们使用这种方法来证明基于 Wav2Vec 2.0 的模型比混合 ASR 模型更能充分利用文本上下文，尽管没有使用显式语言模型，并且还用它来阐明最近的结果，这些结果表明标准 ASR 系统在非裔美国英语上表现不佳。我们证明这些主要代表了声学语音建模的失败。我们展示了如何直接使用这种方法来诊断和改进 ASR。</li>
</ul>

<h3>Title: Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ioana Buhnila, Aman Sinha, Mathieu Constant</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16565">https://arxiv.org/abs/2407.16565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16565">https://arxiv.org/pdf/2407.16565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16565]] Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models(https://arxiv.org/abs/2407.16565)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Recent surge in the accessibility of large language models (LLMs) to the general population can lead to untrackable use of such models for medical-related recommendations. Language generation via LLMs models has two key problems: firstly, they are prone to hallucination and therefore, for any medical purpose they require scientific and factual grounding; secondly, LLMs pose tremendous challenge to computational resources due to their gigantic model size. In this work, we introduce pRAGe, a pipeline for Retrieval Augmented Generation and evaluation of medical paraphrases generation using Small Language Models (SLM). We study the effectiveness of SLMs and the impact of external knowledge base for medical paraphrase generation in French.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 近期在普通人群中的普及度不断提升，这可能导致此类模型在医学相关建议方面的使用无法追踪。通过 LLM 模型生成语言存在两个关键问题：首先，它们容易产生幻觉，因此，对于任何医学目的，它们都需要科学和事实依据；其次，由于 LLM 的模型规模巨大，对计算资源构成了巨大挑战。在这项工作中，我们引入了 pRAGe，这是一种用于检索增强生成的管道，并评估使用小型语言模型 (SLM) 生成医学释义。我们研究了 SLM 的有效性以及外部知识库对法语医学释义生成的影响。</li>
</ul>

<h3>Title: TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Wontae Nam, Daejin Jo, Kyoung-Woon On, Mark A. Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16574">https://arxiv.org/abs/2407.16574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16574">https://arxiv.org/pdf/2407.16574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16574]] TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback(https://arxiv.org/abs/2407.16574)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence. These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model. Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token. To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context. Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks.</li>
<li><strong>摘要：</strong>人类反馈强化学习 (RLHF) 利用人类偏好数据来训练语言模型，使其更贴近人类本质。然而，这些人类偏好数据是在序列级别标记的，导致序列级别偏好标签与从语言模型自回归生成的标记不匹配。尽管最近有几种方法试图为每个单独的标记提供标记级别（即密集）奖励，但这些方法通常依赖于预定义的离散奖励值（例如，正面：+1、负面：-1、中性：0），无法解释每个标记固有的不同程度的偏好。为了解决这一限制，我们为 RLHF 引入了 TLCR（标记级别连续奖励），它结合了一个经过训练以区分正面和负面标记的鉴别器，并且使用鉴别器的置信度根据上下文为每个标记分配连续奖励。大量实验表明，我们提出的 TLCR 在开放式生成基准上比以前的序列级别或标记级别离散奖励带来了持续的性能改进。</li>
</ul>

<h3>Title: A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions</h3>
<ul>
<li><strong>Authors: </strong>Giorgos Lysandrou, Roma English Owen, Vanja Popovic, Grant Le Brun, Aryo Pradipta Gema, Beatrice Alex, Elizabeth A. L. Fairley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16593">https://arxiv.org/abs/2407.16593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16593">https://arxiv.org/pdf/2407.16593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16593]] A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions(https://arxiv.org/abs/2407.16593)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>There exists an invisible barrier between healthcare professionals' perception of a patient's clinical experience and the reality. This barrier may be induced by the environment that hinders patients from sharing their experiences openly with healthcare professionals. As patients are observed to discuss and exchange knowledge more candidly on social media, valuable insights can be leveraged from these platforms. However, the abundance of non-patient posts on social media necessitates filtering out such irrelevant content to distinguish the genuine voices of patients, a task we refer to as patient voice classification. In this study, we analyse the importance of linguistic characteristics in accurately classifying patient voices. Our findings underscore the essential role of linguistic and statistical text similarity analysis in identifying common patterns among patient groups. These results allude to even starker differences in the way patients express themselves at a disease level and across various therapeutic domains. Additionally, we fine-tuned a pre-trained Language Model on the combined datasets with similar linguistic patterns, resulting in a highly accurate automatic patient voice classification. Being the pioneering study on the topic, our focus on extracting authentic patient experiences from social media stands as a crucial step towards advancing healthcare standards and fostering a patient-centric approach.</li>
<li><strong>摘要：</strong>医疗专业人员对患者临床体验的认知与现实之间存在着一道无形的障碍。这种障碍可能是由阻碍患者与医疗专业人员公开分享其体验的环境引起的。随着患者在社交媒体上更坦诚地讨论和交流知识，可以从这些平台中获取有价值的见解。然而，社交媒体上有大量非患者帖子，因此必须过滤掉这些不相关的内容，以区分患者的真实声音，我们将这项任务称为患者声音分类。在本研究中，我们分析了语言特征在准确分类患者声音方面的重要性。我们的研究结果强调了语言和统计文本相似性分析在识别患者群体中的共同模式方面的重要作用。这些结果暗示患者在疾病层面和各个治疗领域的表达方式存在更明显的差异。此外，我们在具有相似语言模式的组合数据集上对预训练语言模型进行了微调，从而实现了高度准确的自动患者声音分类。作为该主题的先驱研究，我们专注于从社交媒体中提取真实的患者体验，这是提高医疗保健标准和促进以患者为中心的方法的关键一步。</li>
</ul>

<h3>Title: Shared Imagination: LLMs Hallucinate Alike</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zhou, Caiming Xiong, Silvio Savarese, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16604">https://arxiv.org/abs/2407.16604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16604">https://arxiv.org/pdf/2407.16604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16604]] Shared Imagination: LLMs Hallucinate Alike(https://arxiv.org/abs/2407.16604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Despite the recent proliferation of large language models (LLMs), their training recipes -- model architecture, pre-training data and optimization algorithm -- are often very similar. This naturally raises the question of the similarity among the resulting models. In this paper, we propose a novel setting, imaginary question answering (IQA), to better understand model similarity. In IQA, we ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer. Surprisingly, despite the total fictionality of these questions, all models can answer each other's questions with remarkable success, suggesting a "shared imagination space" in which these models operate during such hallucinations. We conduct a series of investigations into this phenomenon and discuss implications on model homogeneity, hallucination, and computational creativity.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 近年来数量激增，但它们的训练配方（模型架构、预训练数据和优化算法）通常非常相似。这自然引发了由此产生的模型之间的相似性问题。在本文中，我们提出了一种新颖的设置，即虚拟问答 (IQA)，以更好地理解模型相似性。在 IQA 中，我们要求一个模型生成纯粹虚构的问题（例如，完全虚构的物理学概念），并提示另一个模型回答。令人惊讶的是，尽管这些问题完全是虚构的，但所有模型都可以非常成功地回答彼此的问题，这表明这些模型在这种幻觉期间运行的“共享想象空间”。我们对这一现象进行了一系列调查，并讨论了对模型同质性、幻觉和计算创造力的影响。</li>
</ul>

<h3>Title: Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16607">https://arxiv.org/abs/2407.16607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16607">https://arxiv.org/pdf/2407.16607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16607]] Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?(https://arxiv.org/abs/2407.16607)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>The pretraining data of today's strongest language models is opaque. In particular, little is known about the proportions of various domains or languages represented. In this work, we tackle a task which we call data mixture inference, which aims to uncover the distributional make-up of training data. We introduce a novel attack based on a previously overlooked source of information -- byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. Our key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first merge is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest, we formulate a linear program that solves for the proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, we indirectly learn about the pretraining data. In controlled experiments, we show that our attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. We then apply our approach to off-the-shelf tokenizers released with recent LMs. We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o's tokenizer is much more multilingual than its predecessors, training on 39% non-English data; Llama3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs.</li>
<li><strong>摘要：</strong>当今最强大的语言模型的预训练数据是不透明的。特别是，人们对所代表的各种领域或语言的比例知之甚少。在这项工作中，我们解决了一项称为数据混合推理的任务，旨在揭示训练数据的分布构成。我们引入了一种基于以前被忽视的信息源的新攻击——字节对编码 (BPE) 标记器，绝大多数现代语言模型都在使用这种标记器。我们的主要见解是，BPE 标记器学习的合并规则的有序列表自然会揭示其训练数据中标记频率的信息：第一个合并是最常见的字节对，第二个合并是合并第一个标记后最常见的对，依此类推。给定标记器的合并列表以及每个感兴趣类别的数据样本，我们制定一个线性程序来求解标记器训练集中每个类别的比例。重要的是，在标记器训练数据代表预训练数据的程度上，我们可以间接了解预训练数据。在受控实验中，我们表明，对于在已知的自然语言、编程语言和数据源混合上训练的标记器，我们的攻击可以高精度地恢复混合比率。然后，我们将我们的方法应用于最近发布的 LM 的现成标记器。我们确认了有关这些模型的许多公开披露的信息，并做出了几个新的推论：GPT-4o 的标记器比其前代产品更加多语言，在 39% 的非英语数据上进行训练；Llama3 扩展了 GPT-3.5 的标记器，主要用于多语言（48%）；GPT-3.5 和 Claude 的标记器主要在代码上进行训练（约 60%）。我们希望我们的工作能够为预训练数据的当前设计实践提供启示，并激发对 LM 数据混合推理的持续研究。</li>
</ul>

<h3>Title: Lawma: The Power of Specialization for Legal Tasks</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Dominguez-Olmedo, Vedant Nanda, Rediet Abebe, Stefan Bechtold, Christoph Engel, Jens Frankenreiter, Krishna Gummadi, Moritz Hardt, Michael Livermore</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16615">https://arxiv.org/abs/2407.16615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16615">https://arxiv.org/pdf/2407.16615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16615]] Lawma: The Power of Specialization for Legal Tasks(https://arxiv.org/abs/2407.16615)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, our understanding of how to best utilize large language models for legal tasks remains limited. We conduct a comprehensive study of 260 legal text classification tasks, nearly all new to the machine learning community. Starting from GPT-4 as a baseline, we show that it has non-trivial but highly varied zero-shot accuracy, often exhibiting performance that may be insufficient for legal work. We then demonstrate that a lightly fine-tuned Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by double-digit percentage points. We find that larger models respond better to fine-tuning than smaller models. A few tens to hundreds of examples suffice to achieve high classification accuracy. Notably, we can fine-tune a single model on all 260 tasks simultaneously at a small loss in accuracy relative to having a separate model for each task. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal tasks with some available labeled data, researchers are better off using a fine-tuned open-source model.</li>
<li><strong>摘要：</strong>法律文本的注释和分类是实证法律研究的核心组成部分。传统上，这些任务通常委托给经过培训的研究助理。受语言建模进步的推动，实证法律学者越来越多地转向提示商业模型，希望它能减轻人工注释的巨大成本。尽管使用量不断增长，但我们对如何最好地利用大型语言模型完成法律任务的理解仍然有限。我们对 260 个法律文本分类任务进行了全面研究，这些任务几乎都是机器学习社区的新任务。从 GPT-4 作为基线开始，我们表明它具有不平凡但变化很大的零样本准确率，通常表现出的性能可能不足以完成法律工作。然后，我们证明，经过轻微微调的 Llama 3 模型在几乎所有任务上都远远优于 GPT-4，通常高出两位数的百分点。我们发现较大的模型比较小的模型对微调的反应更好。几十到几百个示例足以实现高分类准确率。值得注意的是，我们可以同时对所有 260 个任务微调一个模型，而准确度损失却很小，因为与为每个任务使用单独的模型相比。我们的工作为主流的提示商业模型的做法提供了一种可行的替代方案。对于具有一些可用标记数据的具体法律任务，研究人员最好使用经过微调的开源模型。</li>
</ul>

<h3>Title: Semantic Change Characterization with LLMs using Rhetorics</h3>
<ul>
<li><strong>Authors: </strong>Jader Martins Camboim de Sá, Marcos Da Silveira, Cédric Pruski</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16624">https://arxiv.org/abs/2407.16624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16624">https://arxiv.org/pdf/2407.16624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16624]] Semantic Change Characterization with LLMs using Rhetorics(https://arxiv.org/abs/2407.16624)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Languages continually evolve in response to societal events, resulting in new terms and shifts in meanings. These changes have significant implications for computer applications, including automatic translation and chatbots, making it essential to characterize them accurately. The recent development of LLMs has notably advanced natural language understanding, particularly in sense inference and reasoning. In this paper, we investigate the potential of LLMs in characterizing three types of semantic change: dimension, relation, and orientation. We achieve this by combining LLMs' Chain-of-Thought with rhetorical devices and conducting an experimental assessment of our approach using newly created datasets. Our results highlight the effectiveness of LLMs in capturing and analyzing semantic changes, providing valuable insights to improve computational linguistic applications.</li>
<li><strong>摘要：</strong>语言随着社会事件而不断发展，产生了新的术语和含义的变化。这些变化对计算机应用程序（包括自动翻译和聊天机器人）具有重要影响，因此准确描述它们至关重要。LLM 的最新发展显著提高了自然语言理解能力，特别是在意义推理和推理方面。在本文中，我们研究了 LLM 在描述三种语义变化方面的潜力：维度、关系和方向。我们通过将 LLM 的思路链与修辞手法相结合并使用新创建的数据集对我们的方法进行实验评估来实现这一点。我们的结果突出了 LLM 在捕捉和分析语义变化方面的有效性，为改进计算语言学应用提供了宝贵的见解。</li>
</ul>

<h3>Title: Course-Correction: Safety Alignment Using Synthetic Preferences</h3>
<ul>
<li><strong>Authors: </strong>Rongwu Xu, Yishuo Cai, Zhenhong Zhou, Renjie Gu, Haiqin Weng, Yan Liu, Tianwei Zhang, Wei Xu, Han Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16637">https://arxiv.org/abs/2407.16637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16637">https://arxiv.org/pdf/2407.16637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16637]] Course-Correction: Safety Alignment Using Synthetic Preferences(https://arxiv.org/abs/2407.16637)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>The risk of harmful content generated by large language models (LLMs) becomes a critical concern. This paper presents a systematic study on assessing and improving LLMs' capability to perform the task of \textbf{course-correction}, \ie, the model can steer away from generating harmful content autonomously. To start with, we introduce the \textsc{C$^2$-Eval} benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction. To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction. Using an automated pipeline, we create \textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven preference learning. Experiments on 2 LLMs, \textsc{Llama2-Chat 7B} and \textsc{Qwen2 7B}, show that our method effectively enhances course-correction skills without affecting general performance. Additionally, it effectively improves LLMs' safety, particularly in resisting jailbreak attacks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 生成有害内容的风险已成为一个关键问题。本文系统地研究了如何评估和改进 LLM 执行 \textbf{课程纠正} 任务的能力，即模型可以自主避免生成有害内容。首先，我们引入 \textsc{C$^2$-Eval} 基准进行定量评估，并分析了 10 个流行的 LLM，揭示了当前安全调整的 LLM 在课程纠正方面的不同熟练程度。为了改进，我们建议使用偏好学习对 LLM 进行微调，强调及时课程纠正的偏好。使用自动化管道，我们创建了 \textsc{C$^2$-Syn}，这是一个具有 750K 成对偏好的合成数据集，通过数据驱动的偏好学习向模型传授及时课程纠正的概念。在 \textsc{Llama2-Chat 7B} 和 \textsc{Qwen2 7B} 两个 LLM 上进行的实验表明，我们的方法有效地提高了课程纠正能力，而不会影响整体性能。此外，它还有效地提高了 LLM 的安全性，特别是在抵御越狱攻击方面。</li>
</ul>

<h3>Title: Can Large Language Models Automatically Jailbreak GPT-4V?</h3>
<ul>
<li><strong>Authors: </strong>Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16686">https://arxiv.org/abs/2407.16686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16686">https://arxiv.org/pdf/2407.16686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16686]] Can Large Language Models Automatically Jailbreak GPT-4V?(https://arxiv.org/abs/2407.16686)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information. At the same time, its ability of face recognition raises new safety concerns of privacy leakage. Despite researchers' efforts in safety alignment through RLHF or preprocessing filters, vulnerabilities might still be exploited. In our study, we introduce AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt optimization. We leverage Large Language Models (LLMs) for red-teaming to refine the jailbreak prompt and employ weak-to-strong in-context learning prompts to boost efficiency. Furthermore, we present an effective search method that incorporates early stopping to minimize optimization time and token expenditure. Our experiments demonstrate that AutoJailbreak significantly surpasses conventional methods, achieving an Attack Success Rate (ASR) exceeding 95.3\%. This research sheds light on strengthening GPT-4V security, underscoring the potential for LLMs to be exploited in compromising GPT-4V integrity.</li>
<li><strong>摘要：</strong>GPT-4V 因其出色的多模态信息集成和处理能力而备受关注。同时，其人脸识别能力也引发了隐私泄露等新的安全问题。尽管研究人员通过 RLHF 或预处理过滤器努力实现安全性调整，但仍可能存在漏洞被利用。在我们的研究中，我们引入了 AutoJailbreak，这是一种受提示优化启发的创新自动越狱技术。我们利用大型语言模型 (LLM) 进行红队演练来改进越狱提示，并采用从弱到强的上下文学习提示来提高效率。此外，我们提出了一种有效的搜索方法，该方法结合了早期停止以最大限度地减少优化时间和令牌消耗。我们的实验表明，AutoJailbreak 明显优于传统方法，攻击成功率 (ASR) 超过 95.3%。这项研究为加强 GPT-4V 安全性提供了启示，强调了 LLM 在破坏 GPT-4V 完整性方面被利用的潜力。</li>
</ul>

<h3>Title: Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyue Xu, Qinyuan Ye, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.16695">https://arxiv.org/abs/2407.16695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.16695">https://arxiv.org/pdf/2407.16695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.16695]] Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack(https://arxiv.org/abs/2407.16695)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn from a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expected to leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than the Single-task ICL baseline. Task Haystack draws inspiration from the widely-adopted "needle-in-a-haystack" (NIAH) evaluation, but presents new and unique challenges. It demands that models (1) utilize the contexts with deeper understanding, rather than resorting to simple copying and pasting; (2) navigate through long streams of evolving topics and tasks, which closely approximates the complexities of real-world usage of long-context LMs. Additionally, Task Haystack inherits the controllability aspect of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively. We benchmark 12 long-context LMs using Task Haystack. We find that state-of-the-art closed models such as GPT-4o still struggle in this setting, failing 15% of the cases on average, while all open-weight models we evaluate further lack behind by a large margin, failing up to 61% of the cases. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, we observe declines in performance when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of current long-context LMs.</li>
<li><strong>摘要：</strong>我们引入了 Lifelong ICL，这是一个挑战长上下文语言模型 (LM) 通过上下文学习 (ICL) 从一系列语言任务中学习的问题设置。我们进一步介绍了 Task Haystack，这是一个评估套件，专门用于评估和诊断长上下文语言模型如何在 Lifelong ICL 中利用上下文。当给出任务指令和测试输入时，长上下文语言模型有望利用 Lifelong ICL 提示中的相关演示，避免其他任务的干扰和干扰，并实现不比单任务 ICL 基线差很多的测试准确率。Task Haystack 的灵感来自广泛采用的“大海捞针”(NIAH) 评估，但提出了新的独特挑战。它要求模型 (1) 以更深入的理解利用上下文，而不是诉诸简单的复制和粘贴；(2) 浏览不断演变的主题和任务的长流，这与现实世界中使用长上下文语言模型的复杂性非常接近。此外，Task Haystack 继承了 NIAH 的可控性方面，为模型开发人员提供了工具和可视化效果，以有效识别模型漏洞。我们使用 Task Haystack 对 12 个长上下文 LM 进行了基准测试。我们发现，最先进的封闭模型（例如 GPT-4o）在这种情况下仍然表现不佳，平均有 15% 的案例失败，而我们进一步评估的所有开放权重模型都落后很多，失败率高达 61%。在我们的受控分析中，我们将干扰和近因偏差等因素确定为导致这些失败案例的因素。此外，我们观察到，当在测试时解释任务指令或过度重复 ICL 演示时，性能会下降，这引发了人们对当前长上下文 LM 的稳健性、指令理解和真实上下文利用率的担忧。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
