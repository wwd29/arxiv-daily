<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-31</h1>
<h3>Title: Informal Safety Guarantees for Simulated Optimizers Through  Extrapolation from Partial Simulations</h3>
<ul>
<li><strong>Authors: </strong>Luke Marks</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16426">https://arxiv.org/abs/2401.16426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16426">https://arxiv.org/pdf/2401.16426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16426]] Informal Safety Guarantees for Simulated Optimizers Through  Extrapolation from Partial Simulations(https://arxiv.org/abs/2401.16426)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag, agent</a></li>
<li><strong>Abstract: </strong>Self-supervised learning is the backbone of state of the art language modeling. It has been argued that training with predictive loss on a self-supervised dataset causes simulators: entities that internally represent possible configurations of real-world systems. Under this assumption, a mathematical model for simulators is built based in the Cartesian frames model of embedded agents, which is extended to multi-agent worlds through scaling a two-dimensional frame to arbitrary dimensions, where literature prior chooses to instead use operations on frames. This variant leveraging scaling dimensionality is named the Cartesian object, and is used to represent simulations (where individual simulacra are the agents and devices in that object). Around the Cartesian object, functions like token selection and simulation complexity are accounted for in formalizing the behavior of a simulator, and used to show (through the L\"obian obstacle) that a proof of alignment between simulacra by inspection of design is impossible in the simulator context. Following this, a scheme is proposed and termed Partial Simulation Extrapolation aimed at circumventing the L\"obian obstacle through the evaluation of low-complexity simulations.</li>
<li><strong>摘要：</strong>自监督学习是最先进的语言建模的支柱。有人认为，在自监督数据集上进行预测损失训练会产生模拟器：内部代表现实世界系统可能配置的实体。在此假设下，基于嵌入式代理的笛卡尔框架模型构建了模拟器的数学模型，该模型通过将二维框架缩放到任意维度而扩展到多代理世界，其中先前文献选择使用框架上的操作。这种利用缩放维度的变体被称为笛卡尔对象，用于表示模拟（其中单个拟像是该对象中的代理和设备）。围绕笛卡尔对象，在形式化模拟器的行为时考虑了诸如标记选择和模拟复杂性之类的功能，并用于表明（通过 L'obian 障碍）通过设计检查来证明拟像之间的对齐是不可能的在此之后，提出了一种称为部分模拟外推的方案，旨在通过评估低复杂度模拟来绕过 L'obian 障碍。</li>
</ul>

<h3>Title: Do deep neural networks utilize the weight space efficiently?</h3>
<ul>
<li><strong>Authors: </strong>Onur Can Koyun, Behçet Uğur Töreyin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16438">https://arxiv.org/abs/2401.16438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16438">https://arxiv.org/pdf/2401.16438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16438]] Do deep neural networks utilize the weight space efficiently?(https://arxiv.org/abs/2401.16438)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Deep learning models like Transformers and Convolutional Neural Networks (CNNs) have revolutionized various domains, but their parameter-intensive nature hampers deployment in resource-constrained settings. In this paper, we introduce a novel concept utilizes column space and row space of weight matrices, which allows for a substantial reduction in model parameters without compromising performance. Leveraging this paradigm, we achieve parameter-efficient deep learning models.. Our approach applies to both Bottleneck and Attention layers, effectively halving the parameters while incurring only minor performance degradation. Extensive experiments conducted on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of our method, showcasing competitive performance when compared to traditional models. This approach not only addresses the pressing demand for parameter efficient deep learning solutions but also holds great promise for practical deployment in real-world scenarios.</li>
<li><strong>摘要：</strong>像 Transformer 和卷积神经网络 (CNN) 这样的深度学习模型已经彻底改变了各个领域，但它们的参数密集型性质阻碍了在资源有限的环境中的部署。在本文中，我们引入了一种利用权重矩阵的列空间和行空间的新颖概念，它可以在不影响性能的情况下大幅减少模型参数。利用这种范例，我们实现了参数高效的深度学习模型。我们的方法适用于瓶颈层和注意力层，有效地将参数减半，同时仅导致轻微的性能下降。使用 ViT 和 ResNet50 在 ImageNet 数据集上进行的大量实验证明了我们方法的有效性，与传统模型相比，展示了具有竞争力的性能。这种方法不仅满足了对参数高效深度学习解决方案的迫切需求，而且为现实场景中的实际部署提供了广阔的前景。</li>
</ul>

<h3>Title: Polynomial time auditing of statistical subgroup fairness for Gaussian  data</h3>
<ul>
<li><strong>Authors: </strong>Daniel Hsu, Jizhou Huang, Brendan Juba</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16439">https://arxiv.org/abs/2401.16439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16439">https://arxiv.org/pdf/2401.16439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16439]] Polynomial time auditing of statistical subgroup fairness for Gaussian  data(https://arxiv.org/abs/2401.16439)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We study the problem of auditing classifiers with the notion of statistical subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing combinatorial subgroups fairness is as hard as agnostic learning. Essentially all work on remedying statistical measures of discrimination against subgroups assumes access to an oracle for this problem, despite the fact that no efficient algorithms are known for it. If we assume the data distribution is Gaussian, or even merely log-concave, then a recent line of work has discovered efficient agnostic learning algorithms for halfspaces. Unfortunately, the boosting-style reductions given by Kearns et al. required the agnostic learning algorithm to succeed on reweighted distributions that may not be log-concave, even if the original data distribution was. In this work, we give positive and negative results on auditing for the Gaussian distribution: On the positive side, we an alternative approach to leverage these advances in agnostic learning and thereby obtain the first polynomial-time approximation scheme (PTAS) for auditing nontrivial combinatorial subgroup fairness: we show how to audit statistical notions of fairness over homogeneous halfspace subgroups when the features are Gaussian. On the negative side, we find that under cryptographic assumptions, no polynomial-time algorithm can guarantee any nontrivial auditing, even under Gaussian feature distributions, for general halfspace subgroups.</li>
<li><strong>摘要：</strong>我们用统计子组公平性的概念研究审计分类器的问题。卡恩斯等人。 （2018）表明，审核组合子组公平性的问题与不可知学习一样困难。本质上，所有补救针对亚群体歧视的统计措施的工作都假设可以使用神谕来解决这个问题，尽管事实上还没有有效的算法。如果我们假设数据分布是高斯分布，或者甚至只是对数凹分布，那么最近的一系列工作已经发现了有效的半空间不可知学习算法。不幸的是，Kearns 等人给出的提升式缩减。要求不可知学习算法在可能不是对数凹的重新加权分布上取得成功，即使原始数据分布是。在这项工作中，我们对高斯分布的审计给出了积极和消极的结果：从积极的一面来看，我们采用了另一种方法来利用不可知学习中的这些进步，从而获得第一个用于审计非平凡组合的多项式时间近似方案（PTAS）子组公平性：我们展示了当特征为高斯分布时如何审核同质半空间子组的公平性统计概念。不利的一面是，我们发现在密码学假设下，即使在高斯特征分布下，对于一般半空间子组，没有多项式时间算法可以保证任何重要的审计。</li>
</ul>

<h3>Title: Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public  Records to Inform Action</h3>
<ul>
<li><strong>Authors: </strong>Tasfia Mashiat, Alex DiChristofano, Patrick J. Fowler, Sanmay Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16440">https://arxiv.org/abs/2401.16440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16440">https://arxiv.org/pdf/2401.16440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16440]] Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public  Records to Inform Action(https://arxiv.org/abs/2401.16440)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>There has been considerable recent interest in scoring properties on the basis of eviction risk. The success of methods for eviction prediction is typically evaluated using different measures of predictive accuracy. However, the underlying goal of such prediction is to direct appropriate assistance to households that may be at greater risk so they remain stably housed. Thus, we must ask the question of how useful such predictions are in targeting outreach efforts - informing action. In this paper, we investigate this question using a novel dataset that matches information on properties, evictions, and owners. We perform an eviction prediction task to produce risk scores and then use these risk scores to plan targeted outreach policies. We show that the risk scores are, in fact, useful, enabling a theoretical team of caseworkers to reach more eviction-prone properties in the same amount of time, compared to outreach policies that are either neighborhood-based or focus on buildings with a recent history of evictions. We also discuss the importance of neighborhood and ownership features in both risk prediction and targeted outreach.</li>
<li><strong>摘要：</strong>最近人们对根据驱逐风险对房产进行评分产生了相当大的兴趣。驱逐预测方法的成功通常是使用不同的预测准确性度量来评估的。然而，此类预测的根本目标是向可能面临更大风险的家庭提供适当的援助，使他们保持稳定的住房。因此，我们必须问这样的问题：此类预测对于确定外展工作（为行动提供信息）有多大用处。在本文中，我们使用一个新颖的数据集来研究这个问题，该数据集匹配有关财产、驱逐和所有者的信息。我们执行驱逐预测任务来生成风险评分，然后使用这些风险评分来规划有针对性的外展政策。我们表明，与基于社区或关注最近被驱逐的建筑物的外展政策相比，风险评分实际上是有用的，使案例工作者的理论团队能够在相同的时间内接触到更多容易被驱逐的房产。驱逐的历史。我们还讨论了社区和所有权特征在风险预测和有针对性的推广中的重要性。</li>
</ul>

<h3>Title: FaKnow: A Unified Library for Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Zhu, Yongjun Li, Jialiang Wang, Ming Gao, Jiali Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16441">https://arxiv.org/abs/2401.16441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16441">https://arxiv.org/pdf/2401.16441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16441]] FaKnow: A Unified Library for Fake News Detection(https://arxiv.org/abs/2401.16441)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Over the past years, a large number of fake news detection algorithms based on deep learning have emerged. However, they are often developed under different frameworks, each mandating distinct utilization methodologies, consequently hindering reproducibility. Additionally, a substantial amount of redundancy characterizes the code development of such fake news detection models. To address these concerns, we propose FaKnow, a unified and comprehensive fake news detection algorithm library. It encompasses a variety of widely used fake news detection models, categorized as content-based and social context-based approaches. This library covers the full spectrum of the model training and evaluation process, effectively organizing the data, models, and training procedures within a unified framework. Furthermore, it furnishes a series of auxiliary functionalities and tools, including visualization, and logging. Our work contributes to the standardization and unification of fake news detection research, concurrently facilitating the endeavors of researchers in this field. The open-source code and documentation can be accessed at https://github.com/NPURG/FaKnow and https://faknow.readthedocs.io, respectively.</li>
<li><strong>摘要：</strong>近年来，大量基于深度学习的假新闻检测算法涌现。然而，它们通常是在不同的框架下开发的，每个框架都要求不同的使用方法，从而阻碍了可重复性。此外，此类假新闻检测模型的代码开发具有大量冗余的特征。为了解决这些问题，我们提出了 FaKnow，一个统一且全面的假新闻检测算法库。它包含各种广泛使用的假新闻检测模型，分为基于内容和基于社交情境的方法。该库涵盖了模型训练和评估过程的全部范围，在统一的框架内有效地组织数据、模型和训练过程。此外，它还提供了一系列辅助功能和工具，包括可视化和日志记录。我们的工作有助于假新闻检测研究的标准化和统一，同时促进该领域研究人员的努力。开源代码和文档可以分别访问 https://github.com/NPURG/FaKnow 和 https://faknow.readthedocs.io。</li>
</ul>

<h3>Title: Context-Former: Stitching via Latent Conditioned Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Zhang, Jingzehua Xu, Zifeng Zhuang, Jinxin Liu, Donglin wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16452">https://arxiv.org/abs/2401.16452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16452">https://arxiv.org/pdf/2401.16452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16452]] Context-Former: Stitching via Latent Conditioned Sequence Modeling(https://arxiv.org/abs/2401.16452)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives: 1) We conduct extensive experiments on D4RL benchmarks under the settings of IL, and experimental results demonstrate ContextFormer can achieve competitive performance in multi-IL settings. 2) More importantly, we conduct a comparison of ContextFormer with diverse competitive DT variants using identical training datasets. The experimental results unveiled ContextFormer's superiority, as it outperformed all other variants, showcasing its remarkable performance.</li>
<li><strong>摘要：</strong>离线强化学习（RL）算法可以通过缝合次优轨迹以获得更优的轨迹来改善决策。这种能力是让强化学习能够学习优于行为策略的关键因素。另一方面，决策变换器（DT）将决策抽象为序列建模，在离线强化学习基准上展现出有竞争力的性能，然而，最近的研究表明DT缺乏拼接能力，因此利用DT的拼接能力对于进一步改进至关重要它的性能。为了赋予 DT 拼接能力，我们将轨迹拼接抽象为专家匹配，并引入我们的方法 ContextFormer，该方法集成了基于上下文信息的模仿学习（IL）和序列建模，通过模拟专家轨迹的数量有限。为了验证我们的主张，我们从两个角度进行实验：1）我们在IL设置下对D4RL基准进行了广泛的实验，实验结果表明ContextFormer可以在多IL设置下实现有竞争力的性能。 2）更重要的是，我们使用相同的训练数据集对 ContextFormer 与各种竞争性 DT 变体进行了比较。实验结果揭示了 ContextFormer 的优越性，它超越了所有其他变体，展示了其卓越的性能。</li>
</ul>

<h3>Title: Effective Controllable Bias Mitigation for Classification and Retrieval  using Gate Adapters</h3>
<ul>
<li><strong>Authors: </strong>Shahed Masoudian, Cornelia Volaucnik, Markus Schedl, Shahed Masoudian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16457">https://arxiv.org/abs/2401.16457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16457">https://arxiv.org/pdf/2401.16457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16457]] Effective Controllable Bias Mitigation for Classification and Retrieval  using Gate Adapters(https://arxiv.org/abs/2401.16457)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a trade-off between performance and fairness metrics. Our experiments on the classification tasks show that compared to baselines of the same caliber, ConGater can maintain higher task performance while containing less information regarding the attributes. Our results on the retrieval task show that the fully debiased ConGater can achieve the same fairness performance while maintaining more than twice as high task performance than recent strong baselines. Overall, besides strong performance ConGater enables the continuous transitioning between biased and debiased states of models, enhancing personalization of use and interpretability through controllability.</li>
<li><strong>摘要：</strong>语言模型的偏差缓解一直是许多研究的主题，最近的重点是学习单独的模块，例如用于按需去偏差的适配器。除了针对模块化去偏模型进行优化之外，在实践中控制推理时偏差减少的程度通常也很关键，例如，为了调整搜索结果中所需的性能与公平性权衡或控制去偏的强度在分类任务中。在本文中，我们介绍了可控门适配器（ConGater），这是一种具有可调灵敏度参数的新型模块化门控机制，它允许在推理时从模型的偏置状态逐渐过渡到完全去偏置的版本。我们通过以下方式展示 ConGater 的性能：(1) 使用三种不同模型对具有四个受保护属性的三个分类任务进行对抗性去偏差实验，以及 (2) 通过公平性列表正则化减少搜索结果的偏差，以调整性能之间的权衡和公平性指标。我们对分类任务的实验表明，与相同口径的基线相比，ConGater 可以保持更高的任务性能，同时包含更少的属性信息。我们在检索任务上的结果表明，完全除偏的 ConGater 可以实现相同的公平性能，同时保持比最近的强基线高两倍以上的任务性能。总体而言，除了强大的性能外，ConGater 还能够在模型的有偏差和无偏差状态之间持续转换，通过可控性增强使用的个性化和可解释性。</li>
</ul>

<h3>Title: Supervised Contrastive Learning based Dual-Mixer Model for Remaining  Useful Life Prediction</h3>
<ul>
<li><strong>Authors: </strong>En Fu, Yanyan Hu, Kaixiang Peng, Yuxin Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16462">https://arxiv.org/abs/2401.16462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16462">https://arxiv.org/pdf/2401.16462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16462]] Supervised Contrastive Learning based Dual-Mixer Model for Remaining  Useful Life Prediction(https://arxiv.org/abs/2401.16462)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>The problem of the Remaining Useful Life (RUL) prediction, aiming at providing an accurate estimate of the remaining time from the current predicting moment to the complete failure of the device, has gained significant attention from researchers in recent years. In this paper, to overcome the shortcomings of rigid combination for temporal and spatial features in most existing RUL prediction approaches, a spatial-temporal homogeneous feature extractor, named Dual-Mixer model, is firstly proposed. Flexible layer-wise progressive feature fusion is employed to ensure the homogeneity of spatial-temporal features and enhance the prediction accuracy. Secondly, the Feature Space Global Relationship Invariance (FSGRI) training method is introduced based on supervised contrastive learning. This method maintains the consistency of relationships among sample features with their degradation patterns during model training, simplifying the subsequently regression task in the output layer and improving the model's performance in RUL prediction. Finally, the effectiveness of the proposed method is validated through comparisons with other latest research works on the C-MAPSS dataset. The Dual-Mixer model demonstrates superiority across most metrics, while the FSGRI training method shows an average improvement of 7.00% and 2.41% in RMSE and MAPE, respectively, for all baseline models. Our experiments and model code are publicly available at https://github.com/fuen1590/PhmDeepLearningProjects.</li>
<li><strong>摘要：</strong>剩余使用寿命（RUL）预测问题旨在准确估计从当前预测时刻到设备完全失效的剩余时间，近年来引起了研究人员的极大关注。本文针对大多数现有 RUL 预测方法中时间和空间特征刚性组合的缺点，首先提出了一种时空同质特征提取器，称为 Dual-Mixer 模型。采用灵活的逐层渐进特征融合，保证时空特征的同质性，提高预测精度。其次，引入基于监督对比学习的特征空间全局关系不变性（FSGRI）训练方法。该方法在模型训练过程中保持了样本特征之间的关系与其退化模式的一致性，简化了后续输出层的回归任务，提高了模型在RUL预测中的性能。最后，通过与 C-MAPSS 数据集上其他最新研究工作的比较，验证了所提出方法的有效性。 Dual-Mixer 模型在大多数指标上都表现出优越性，而 FSGRI 训练方法显示所有基线模型的 RMSE 和 MAPE 平均提高了 7.00% 和 2.41%。我们的实验和模型代码可在 https://github.com/fuen1590/PhmDeepLearningProjects 上公开获取。</li>
</ul>

<h3>Title: InfoLossQA: Characterizing and Recovering Information Loss in Text  Simplification</h3>
<ul>
<li><strong>Authors: </strong>Jan Trienes, Sebastian Joseph, Jörg Schlötterer, Christin Seifert, Kyle Lo, Wei Xu, Byron C. Wallace, Junyi Jessy Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16475">https://arxiv.org/abs/2401.16475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16475">https://arxiv.org/pdf/2401.16475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16475]] InfoLossQA: Characterizing and Recovering Information Loss in Text  Simplification(https://arxiv.org/abs/2401.16475)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Question Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. We conduct a range of experiments with this framework. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of scientific abstracts of medical studies. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.</li>
<li><strong>摘要：</strong>文本简化旨在使技术文本更容易被外行人理解，但通常会导致信息删除和模糊。这项工作提出了 InfoLossQA，这是一个以问答（QA）对的形式描述和恢复简化引起的信息丢失的框架。问答对以“讨论中的问题”理论为基础，旨在帮助读者加深对文本的了解。我们用这个框架进行了一系列的实验。首先，我们收集了 1,000 个由语言学家策划的 QA 对的数据集，这些 QA 对源自 104 个法学硕士对医学研究科学摘要的简化。我们对这些数据的分析表明，信息丢失经常发生，并且 QA 对对丢失的信息进行了高级概述。其次，我们为这项任务设计了两种方法：开源和商业语言模型的端到端提示，以及自然语言推理管道。通过考虑 QA 对的正确性及其语言适用性的新颖评估框架，我们的专家评估表明，模型很难可靠地识别信息丢失，并在构成信息丢失的情况上应用与人类类似的标准。</li>
</ul>

<h3>Title: A Discriminative Bayesian Gaussian Process Latent Variable Model for  High-Dimensional Data</h3>
<ul>
<li><strong>Authors: </strong>Navid Ziaei, Behzad Nazari, Ali Yousefi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16497">https://arxiv.org/abs/2401.16497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16497">https://arxiv.org/pdf/2401.16497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16497]] A Discriminative Bayesian Gaussian Process Latent Variable Model for  High-Dimensional Data(https://arxiv.org/abs/2401.16497)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Extracting meaningful information from high-dimensional data poses a formidable modeling challenge, particularly when the data is obscured by noise or represented through different modalities. In this research, we propose a novel non-parametric modeling approach, leveraging the Gaussian Process (GP), to characterize high-dimensional data by mapping it to a latent low-dimensional manifold. This model, named the Latent Discriminative Generative Decoder (LDGD), utilizes both the data (or its features) and associated labels (such as category or stimulus) in the manifold discovery process. To infer the latent variables, we derive a Bayesian solution, allowing LDGD to effectively capture inherent uncertainties in the data while enhancing the model's predictive accuracy and robustness. We demonstrate the application of LDGD on both synthetic and benchmark datasets. Not only does LDGD infer the manifold accurately, but its prediction accuracy in anticipating labels surpasses state-of-the-art approaches. We have introduced inducing points to reduce the computational complexity of Gaussian Processes (GPs) for large datasets. This enhancement facilitates batch training, allowing for more efficient processing and scalability in handling extensive data collections. Additionally, we illustrate that LDGD achieves higher accuracy in predicting labels and operates effectively with a limited training dataset, underscoring its efficiency and effectiveness in scenarios where data availability is constrained. These attributes set the stage for the development of non-parametric modeling approaches in the analysis of high-dimensional data; especially in fields where data are both high-dimensional and complex.</li>
<li><strong>摘要：</strong>从高维数据中提取有意义的信息提出了巨大的建模挑战，特别是当数据被噪声掩盖或通过不同模式表示时。在这项研究中，我们提出了一种新颖的非参数建模方法，利用高斯过程（GP），通过将高维数据映射到潜在的低维流形来表征高维数据。该模型称为潜在判别生成解码器 (LDGD)，在多种发现过程中利用数据（或其特征）和相关标签（例如类别或刺激）。为了推断潜在变量，我们推导了贝叶斯解决方案，使 LDGD 能够有效捕获数据中固有的不确定性，同时增强模型的预测准确性和鲁棒性。我们演示了 LDGD 在合成数据集和基准数据集上的应用。 LDGD 不仅能够准确地推断出流形，而且它在预测标签方面的预测精度也超过了最先进的方法。我们引入了归纳点来降低大型数据集高斯过程（GP）的计算复杂性。此增强功能有利于批量训练，从而在处理大量数据收集时实现更高效的处理和可扩展性。此外，我们还说明 LDGD 在预测标签方面实现了更高的准确性，并在有限的训练数据集上有效运行，强调了其在数据可用性受限的场景中的效率和有效性。这些属性为高维数据分析中非参数建模方法的发展奠定了基础；尤其是在数据高维且复杂的领域。</li>
</ul>

<h3>Title: GuReT: Distinguishing Guilt and Regret related Text</h3>
<ul>
<li><strong>Authors: </strong>Sabur Butt, Fazlourrahman Balouchzahi, Abdul Gafar Manuel Meque, Maaz Amjad, Hector G. Ceballos Cancino, Grigori Sidorov, Alexander Gelbukh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16541">https://arxiv.org/abs/2401.16541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16541">https://arxiv.org/pdf/2401.16541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16541]] GuReT: Distinguishing Guilt and Regret related Text(https://arxiv.org/abs/2401.16541)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought, tree-of-thought</a></li>
<li><strong>Abstract: </strong>The intricate relationship between human decision-making and emotions, particularly guilt and regret, has significant implications on behavior and well-being. Yet, these emotions subtle distinctions and interplay are often overlooked in computational models. This paper introduces a dataset tailored to dissect the relationship between guilt and regret and their unique textual markers, filling a notable gap in affective computing research. Our approach treats guilt and regret recognition as a binary classification task and employs three machine learning and six transformer-based deep learning techniques to benchmark the newly created dataset. The study further implements innovative reasoning methods like chain-of-thought and tree-of-thought to assess the models interpretive logic. The results indicate a clear performance edge for transformer-based models, achieving a 90.4% macro F1 score compared to the 85.3% scored by the best machine learning classifier, demonstrating their superior capability in distinguishing complex emotional states.</li>
<li><strong>摘要：</strong>人类决策与情绪（尤其是内疚和后悔）之间错综复杂的关系，对行为和幸福感具有重大影响。然而，这些情绪的微妙区别和相互作用在计算模型中经常被忽视。本文介绍了一个专门用于剖析内疚和遗憾及其独特文本标记之间关系的数据集，填补了情感计算研究中的一个显着空白。我们的方法将内疚和遗憾识别视为二元分类任务，并采用三种机器学习和六种基于变压器的深度学习技术来对新创建的数据集进行基准测试。该研究进一步实施了思想链和思想树等创新推理方法来评估模型的解释逻辑。结果表明，基于 Transformer 的模型具有明显的性能优势，宏观 F1 得分为 90.4%，而最佳机器学习分类器的得分为 85.3%，这证明了它们在区分复杂情绪状态方面的卓越能力。</li>
</ul>

<h3>Title: Deep Learning for Multi-Label Learning: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Adane Nega Tarekegn, Mohib Ullah, Faouzi Alaya Cheikh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16549">https://arxiv.org/abs/2401.16549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16549">https://arxiv.org/pdf/2401.16549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16549]] Deep Learning for Multi-Label Learning: A Comprehensive Survey(https://arxiv.org/abs/2401.16549)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Multi-label learning is a rapidly growing research area that aims to predict multiple labels from a single input data point. In the era of big data, tasks involving multi-label classification (MLC) or ranking present significant and intricate challenges, capturing considerable attention in diverse domains. Inherent difficulties in MLC include dealing with high-dimensional data, addressing label correlations, and handling partial labels, for which conventional methods prove ineffective. Recent years have witnessed a notable increase in adopting deep learning (DL) techniques to address these challenges more effectively in MLC. Notably, there is a burgeoning effort to harness the robust learning capabilities of DL for improved modelling of label dependencies and other challenges in MLC. However, it is noteworthy that comprehensive studies specifically dedicated to DL for multi-label learning are limited. Thus, this survey aims to thoroughly review recent progress in DL for multi-label learning, along with a summary of open research problems in MLC. The review consolidates existing research efforts in DL for MLC,including deep neural networks, transformers, autoencoders, and convolutional and recurrent architectures. Finally, the study presents a comparative analysis of the existing methods to provide insightful observations and stimulate future research directions in this domain.</li>
<li><strong>摘要：</strong>多标签学习是一个快速发展的研究领域，旨在从单个输入数据点预测多个标签。在大数据时代，涉及多标签分类（MLC）或排序的任务提出了重大而复杂的挑战，引起了不同领域的广泛关注。 MLC 固有的困难包括处理高维数据、解决标签相关性以及处理部分标签，而传统方法对此无效。近年来，为了更有效地应对 MLC 中的这些挑战，深度学习 (DL) 技术的采用显着增加。值得注意的是，人们正在迅速努力利用深度学习强大的学习能力来改进标签依赖性和 MLC 中其他挑战的建模。然而，值得注意的是，专门针对多标签学习的深度学习的综合研究是有限的。因此，本次调查旨在全面回顾多标签学习 DL 的最新进展，并总结 MLC 中的开放研究问题。该综述整合了 MLC 深度学习的现有研究成果，包括深度神经网络、变压器、自动编码器以及卷积和循环架构。最后，该研究对现有方法进行了比较分析，以提供富有洞察力的观察结果并激发该领域的未来研究方向。</li>
</ul>

<h3>Title: SelectLLM: Can LLMs Select Important Instructions to Annotate?</h3>
<ul>
<li><strong>Authors: </strong>Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16553">https://arxiv.org/abs/2401.16553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16553">https://arxiv.org/pdf/2401.16553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16553]] SelectLLM: Can LLMs Select Important Instructions to Annotate?(https://arxiv.org/abs/2401.16553)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within each cluster. SelectLLM showed comparable or slightly better performance on the popular instruction benchmarks, compared to the recent state-of-the-art selection methods. All code and data are publicly available (https://github.com/minnesotanlp/select-llm).</li>
<li><strong>摘要：</strong>使用大型且多样化的指令数据集训练大型语言模型 (LLM)，使模型能够理解和遵循人类指令。最近的研究表明，使用一小组高质量指令的性能优于使用大量但噪声更大的指令。由于指令是未标记的，并且它们的响应是自然文本，因此具有模型置信度的传统主动学习方案不能直接应用于未标记指令的选择。在这项工作中，我们提出了一种新的指令选择方法，称为 SelectLLM，它利用 LLM 来选择高质量指令。我们的高级想法是使用法学硕士通过提示来估计每条指令的有用性和影响力，而无需相应的标签（即响应）。 SelectLLM涉及两个步骤：使用聚类算法（例如CoreSet）将未标记的指令划分为多个簇，然后提示LLM在每个簇内选择高质量的指令。与最近最先进的选择方法相比，SelectLLM 在流行的指令基准测试中表现出相当或略好的性能。所有代码和数据都是公开的（https://github.com/minnesotanlp/select-llm）。</li>
</ul>

<h3>Title: Autoencoder-Based Domain Learning for Semantic Communication with  Conceptual Spaces</h3>
<ul>
<li><strong>Authors: </strong>Dylan Wheeler, Balasubramaniam Natarajan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16569">https://arxiv.org/abs/2401.16569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16569">https://arxiv.org/pdf/2401.16569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16569]] Autoencoder-Based Domain Learning for Semantic Communication with  Conceptual Spaces(https://arxiv.org/abs/2401.16569)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Communication with the goal of accurately conveying meaning, rather than accurately transmitting symbols, has become an area of growing interest. This paradigm, termed semantic communication, typically leverages modern developments in artificial intelligence and machine learning to improve the efficiency and robustness of communication systems. However, a standard model for capturing and quantifying the details of "meaning" is lacking, with many leading approaches to semantic communication adopting a black-box framework with little understanding of what exactly the model is learning. One solution is to utilize the conceptual spaces framework, which models meaning explicitly in a geometric manner. Though prior work studying semantic communication with conceptual spaces has shown promising results, these previous attempts involve hand-crafting a conceptual space model, severely limiting the scalability and practicality of the approach. In this work, we develop a framework for learning a domain of a conceptual space model using only the raw data with high-level property labels. In experiments using the MNIST and CelebA datasets, we show that the domains learned using the framework maintain semantic similarity relations and possess interpretable dimensions.</li>
<li><strong>摘要：</strong>以准确传达意义而不是准确传输符号为目标的通信已成为人们日益关注的领域。这种范式被称为语义通信，通常利用人工智能和机器学习的现代发展来提高通信系统的效率和鲁棒性。然而，缺乏用于捕获和量化“意义”细节的标准模型，许多领先的语义通信方法采用黑盒框架，很少了解模型到底在学习什么。一种解决方案是利用概念空间框架，该框架以几何方式明确地模拟意义。尽管先前研究概念空间语义通信的工作已经显示出有希望的结果，但这些先前的尝试涉及手工制作概念空间模型，严重限制了该方法的可扩展性和实用性。在这项工作中，我们开发了一个框架，用于仅使用具有高级属性标签的原始数据来学习概念空间模型的域。在使用 MNIST 和 CelebA 数据集的实验中，我们表明使用该框架学习的领域保持语义相似性关系并具有可解释的维度。</li>
</ul>

<h3>Title: Beyond Image-Text Matching: Verb Understanding in Multimodal  Transformers Using Guided Masking</h3>
<ul>
<li><strong>Authors: </strong>Ivana Beňová, Jana Košecká, Michal Gregor, Martin Tamajka, Marcel Veselý, Marián Šimko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16575">https://arxiv.org/abs/2401.16575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16575">https://arxiv.org/pdf/2401.16575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16575]] Beyond Image-Text Matching: Verb Understanding in Multimodal  Transformers Using Guided Masking(https://arxiv.org/abs/2401.16575)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The dominant probing approaches rely on the zero-shot performance of image-text matching tasks to gain a finer-grained understanding of the representations learned by recent multimodal image-language transformer models. The evaluation is carried out on carefully curated datasets focusing on counting, relations, attributes, and others. This work introduces an alternative probing strategy called guided masking. The proposed approach ablates different modalities using masking and assesses the model's ability to predict the masked word with high accuracy. We focus on studying multimodal models that consider regions of interest (ROI) features obtained by object detectors as input tokens. We probe the understanding of verbs using guided masking on ViLBERT, LXMERT, UNITER, and VisualBERT and show that these models can predict the correct verb with high accuracy. This contrasts with previous conclusions drawn from image-text matching probing techniques that frequently fail in situations requiring verb understanding. The code for all experiments will be publicly available https://github.com/ivana-13/guided_masking.</li>
<li><strong>摘要：</strong>主要的探测方法依赖于图像文本匹配任务的零样本性能，以获得对最近多模态图像语言转换器模型学习的表示的更细粒度的理解。评估是在精心策划的数据集上进行的，重点关注计数、关系、属性等。这项工作引入了一种称为引导掩蔽的替代探测策略。所提出的方法使用掩蔽消除不同的模态，并评估模型高精度预测掩蔽词的能力。我们专注于研究多模态模型，该模型将目标检测器获得的感兴趣区域（ROI）特征视为输入标记。我们在 ViLBERT、LXMERT、UNTER 和 VisualBERT 上使用引导掩蔽来探索对动词的理解，并表明这些模型可以高精度地预测正确的动词。这与之前从图像文本匹配探测技术得出的结论形成鲜明对比，这些技术在需要动词理解的情况下经常失败。所有实验的代码将公开 https://github.com/ivana-13/guided_masking。</li>
</ul>

<h3>Title: LLMs as On-demand Customizable Service</h3>
<ul>
<li><strong>Authors: </strong>Souvika Sarkar, Mohammad Fakhruddin Babar, Monowar Hasan, Shubhra Kanti Karmaker (Santu)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16577">https://arxiv.org/abs/2401.16577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16577">https://arxiv.org/pdf/2401.16577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16577]] LLMs as On-demand Customizable Service(https://arxiv.org/abs/2401.16577)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable language understanding and generation capabilities. However, training, deploying, and accessing these models pose notable challenges, including resource-intensive demands, extended training durations, and scalability issues. To address these issues, we introduce a concept of hierarchical, distributed LLM architecture that aims at enhancing the accessibility and deployability of LLMs across heterogeneous computing platforms, including general-purpose computers (e.g., laptops) and IoT-style devices (e.g., embedded systems). By introducing a "layered" approach, the proposed architecture enables on-demand accessibility to LLMs as a customizable service. This approach also ensures optimal trade-offs between the available computational resources and the user's application needs. We envision that the concept of hierarchical LLM will empower extensive, crowd-sourced user bases to harness the capabilities of LLMs, thereby fostering advancements in AI technology in general.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出卓越的语言理解和生成能力。然而，训练、部署和访问这些模型带来了显着的挑战，包括资源密集型需求、延长的训练持续时间和可扩展性问题。为了解决这些问题，我们引入了分层、分布式 LLM 架构的概念，旨在增强 LLM 跨异构计算平台的可访问性和可部署性，包括通用计算机（例如笔记本电脑）和物联网式设备（例如嵌入式系统） ）。通过引入“分层”方法，所提出的架构可以按需访问法学硕士作为可定制的服务。这种方法还确保了可用计算资源和用户应用程序需求之间的最佳权衡。我们设想，分层法学硕士的概念将使广泛的众包用户群能够利用法学硕士的能力，从而总体上促进人工智能技术的进步。</li>
</ul>

<h3>Title: Leveraging Professional Radiologists' Expertise to Enhance LLMs'  Evaluation for Radiology Reports</h3>
<ul>
<li><strong>Authors: </strong>Qingqing Zhu, Xiuying Chen, Qiao Jin, Benjamin Hou, Tejas Sudharshan Mathai, Pritam Mukherjee, Xin Gao, Ronald M Summers, Zhiyong Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16578">https://arxiv.org/abs/2401.16578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16578">https://arxiv.org/pdf/2401.16578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16578]] Leveraging Professional Radiologists' Expertise to Enhance LLMs'  Evaluation for Radiology Reports(https://arxiv.org/abs/2401.16578)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric by 0.19, while our ''Regressed GPT-4'' model shows even greater alignment with expert evaluations, exceeding the best existing metric by a 0.35 margin. Moreover, the robustness of our explanations has been validated through a thorough iterative strategy. We plan to publicly release annotations from radiology experts, setting a new standard for accuracy in future assessments. This underscores the potential of our approach in enhancing the quality assessment of AI-driven medical reports.</li>
<li><strong>摘要：</strong>在放射学领域，人工智能 (AI) 显着改进了报告生成，但自动评估这些人工智能生成的报告仍然具有挑战性。当前的指标，例如传统自然语言生成 (NLG) 和临床疗效 (CE)，通常无法捕捉临床背景的语义复杂性或过分强调临床细节，从而损害了报告的清晰度。为了克服这些问题，我们提出的方法将专业放射科医生的专业知识与大型语言模型 (LLM)（例如 GPT-3.5 和 GPT-4）相结合 1. 利用上下文指令学习 (ICIL) 和思想链 (CoT) 推理，我们的方法使法学硕士评估与放射科医生标准保持一致，从而能够对人类和人工智能生成的报告进行详细比较。聚合句子评估分数的回归模型进一步增强了这一点。实验结果表明，我们的“Detailed GPT-4（5-shot）”模型获得了 0.48 分，比 METEOR 指标高出 0.19，而我们的“Regressed GPT-4”模型显示出与专家评估更加一致，超出现有最佳指标 0.35。此外，我们的解释的稳健性已通过彻底的迭代策略得到验证。我们计划公开发布放射学专家的注释，为未来评估的准确性设立新标准。这强调了我们的方法在增强人工智能驱动的医疗报告的质量评估方面的潜力。</li>
</ul>

<h3>Title: Massively Multilingual Text Translation For Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Zhong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16582">https://arxiv.org/abs/2401.16582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16582">https://arxiv.org/pdf/2401.16582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16582]] Massively Multilingual Text Translation For Low-Resource Languages(https://arxiv.org/abs/2401.16582)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Translation into severely low-resource languages has both the cultural goal of saving and reviving those languages and the humanitarian goal of assisting the everyday needs of local communities that are accelerated by the recent COVID-19 pandemic. In many humanitarian efforts, translation into severely low-resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, low-resource languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich-resource languages to efficiently produce best possible translation quality for well known texts, which are available in multiple languages, in a new, low-resource language. To reach this goal, we argue that in translating a closed text into low-resource languages, generalization to out-of-domain texts is not necessary, but generalization to new languages is. Performance gain comes from massive source parallelism by careful choice of close-by language families, style-consistent corpus-level paraphrases within the same language and strategic adaptation of existing large pretrained multilingual models to the domain first and then to the language. Such performance gain makes it possible for machine translation systems to collaborate with human translators to expedite the translation process into new, low-resource languages.</li>
<li><strong>摘要：</strong>翻译成资源严重匮乏的语言既具有拯救和复兴这些语言的文化目标，也具有满足当地社区日常需求的人道主义目标，而最近的 COVID-19 大流行加速了这种需求。在许多人道主义工作中，翻译成资源严重匮乏的语言通常不需要通用翻译引擎，而是专用的特定于文本的翻译引擎。例如，医疗记录、卫生程序、政府通讯、紧急程序和宗教文本都是有限文本。虽然不存在适用于所有语言的通用翻译引擎，但将多语言已知的有限文本翻译成新的、资源匮乏的语言是可能的，并减少人工翻译工作。我们尝试利用资源丰富的语言的翻译资源，以一种新的资源匮乏的语言，有效地为众所周知的文本提供最佳的翻译质量，这些文本有多种语言版本。为了实现这一目标，我们认为，在将封闭文本翻译成资源匮乏的语言时，不一定需要泛化到域外文本，但泛化到新语言是必要的。性能增益来自于大规模源并行性，通过仔细选择相近的语系、同一语言中风格一致的语料库级释义以及将现有的大型预训练多语言模型首先适应领域，然后适应语言。这种性能提升使得机器翻译系统可以与人工翻译人员协作，以加快翻译成新的、资源匮乏的语言的过程。</li>
</ul>

<h3>Title: A Linguistic Comparison between Human and ChatGPT-Generated  Conversations</h3>
<ul>
<li><strong>Authors: </strong>Morgan Sandler, Hyesun Choung, Arun Ross, Prabu David</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16587">https://arxiv.org/abs/2401.16587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16587">https://arxiv.org/pdf/2401.16587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16587]] A Linguistic Comparison between Human and ChatGPT-Generated  Conversations(https://arxiv.org/abs/2401.16587)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two independent chatbots, which were designed to replicate a corpus of human conversations available for open access and used widely in AI research on language modeling. Our findings increase understanding of ChatGPT's linguistic capabilities and inform ongoing efforts to distinguish between human and LLM-generated text, which is critical in detecting AI-generated fakes, misinformation, and disinformation.</li>
<li><strong>摘要：</strong>本研究使用 ChatGPT-3.5 生成的 19,500 个对话作为 EmpathicDialogues 数据集的伴侣，探讨了人类和 LLM 生成的对话之间的语言差异。该研究采用语言查询和字数统计 (LIWC) 分析，将 ChatGPT 生成的对话与 118 个语言类别的人类对话进行比较。结果显示，人类对话具有更大的可变性和真实性，但 ChatGPT 在社交过程、分析风格、认知、注意力集中和积极情绪基调等方面表现出色，这强化了法学硕士“比人类更人性化”的最新发现。然而，ChatGPT 和人类对话之间的积极或消极影响没有发现显着差异。对话嵌入的分类器分析表明，尽管对话中没有明确提及情感，但情感效价的隐式编码。该研究还贡献了一个由 ChatGPT 生成的新颖的两个独立聊天机器人之间的对话数据集，该数据集旨在复制可供开放访问的人类对话语料库，并广泛用于语言建模的人工智能研究。我们的研究结果增进了对 ChatGPT 语言能力的理解，并为区分人类和法学硕士生成的文本的持续努力提供了信息，这对于检测人工智能生成的假货、错误信息和虚假信息至关重要。</li>
</ul>

<h3>Title: ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence  Labeling Tasks</h3>
<ul>
<li><strong>Authors: </strong>Bolei Ma, Ercong Nie, Shuzhou Yuan, Helmut Schmid, Michael Färber, Frauke Kreuter, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16589">https://arxiv.org/abs/2401.16589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16589">https://arxiv.org/pdf/2401.16589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16589]] ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence  Labeling Tasks(https://arxiv.org/abs/2401.16589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, prompt</a></li>
<li><strong>Abstract: </strong>Prompt-based methods have been successfully applied to multilingual pretrained language models for zero-shot cross-lingual understanding. However, most previous studies primarily focused on sentence-level classification tasks, and only a few considered token-level labeling tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based method for token-level sequence labeling tasks. The ToPro method decomposes an input sentence into single tokens and applies one prompt template to each token. Our experiments on multilingual NER and POS tagging datasets demonstrate that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning in zero-shot cross-lingual transfer, especially for languages that are typologically different from the source language English. Our method also attains state-of-the-art performance when employed with the mT5 model. Besides, our exploratory study in multilingual large language models shows that ToPro performs much better than the current in-context learning method. Overall, the performance improvements show that ToPro could potentially serve as a novel and simple benchmarking method for sequence labeling tasks.</li>
<li><strong>摘要：</strong>基于提示的方法已成功应用于多语言预训练语言模型，以实现零样本跨语言理解。然而，之前的大多数研究主要集中在句子级分类任务，只有少数考虑了标记级标记任务，例如命名实体识别（NER）和词性（POS）标记。在本文中，我们提出了令牌级提示分解（ToPro），它有助于基于提示的方法进行令牌级序列标记任务。 ToPro 方法将输入句子分解为单个标记，并对每个标记应用一个提示模板。我们对多语言 NER 和 POS 标记数据集的实验表明，在零样本跨语言迁移中，基于 ToPro 的微调优于 Vanilla 微调和 Prompt-Tuning，特别是对于与源语言英语类型不同的语言。当与 mT5 模型一起使用时，我们的方法也获得了最先进的性能。此外，我们对多语言大语言模型的探索性研究表明，ToPro 的性能比当前的上下文学习方法要好得多。总体而言，性能改进表明 ToPro 有可能成为序列标记任务的一种新颖且简单的基准测试方法。</li>
</ul>

<h3>Title: Consistent algorithms for multi-label classification with macro-at-$k$  metrics</h3>
<ul>
<li><strong>Authors: </strong>Erik Schultheis, Wojciech Kotłowski, Marek Wydmuch, Rohit Babbar, Strom Borman, Krzysztof Dembczyński</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16594">https://arxiv.org/abs/2401.16594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16594">https://arxiv.org/pdf/2401.16594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16594]] Consistent algorithms for multi-label classification with macro-at-$k$  metrics(https://arxiv.org/abs/2401.16594)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These "macro-at-$k$" metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices. Empirical results provide evidence for the competitive performance of the proposed approach.</li>
<li><strong>摘要：</strong>我们考虑在群体效用框架下多标签分类中复杂性能指标的优化。我们主要关注可线性分解为单独应用于每个标签的二元分类实用程序总和的指标，并额外要求为每个实例预测准确的 $k$ 标签。这些“macro-at-$k$”指标具有长尾标签极端分类问题所需的属性。不幸的是，at-$k$ 约束耦合了原本独立的二元分类任务，导致比标准宏观平均值更具挑战性的优化问题。我们提供了一个统计框架来研究这个问题，证明了最优分类器的存在和形式，并提出了一种基于 Frank-Wolfe 方法的统计一致且实用的学习算法。有趣的是，我们的主要结果涉及更一般的指标，即标签混淆矩阵的非线性函数。实证结果为所提出的方法的竞争性能提供了证据。</li>
</ul>

<h3>Title: Improving Reinforcement Learning from Human Feedback with Efficient  Reward Model Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing Sun, Chuang Gan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16635">https://arxiv.org/abs/2401.16635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16635">https://arxiv.org/pdf/2401.16635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16635]] Improving Reinforcement Learning from Human Feedback with Efficient  Reward Model Ensemble(https://arxiv.org/abs/2401.16635)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.</li>
<li><strong>摘要：</strong>人类反馈强化学习 (RLHF) 是一种广泛采用的方法，用于使大型语言模型与人类价值观保持一致。然而，RLHF 依赖于用有限数量的人类偏好数据进行训练的奖励模型，这可能会导致预测不准确。因此，RLHF 可能会产生与人类价值观不一致的输出。为了缓解这个问题，我们提供了一种奖励集成方法，允许奖励模型做出更准确的预测。由于使用基于大型语言模型的奖励模型的集成可能需要大量的计算和资源，因此我们探索了有效的集成方法，包括线性层集成和基于 LoRA 的集成。根据经验，我们使用集成奖励模型运行 Best-of-$n$ 和邻近策略优化，并验证我们的集成方法是否有助于提高 RLHF 输出的对齐性能。</li>
</ul>

<h3>Title: Breaking Free Transformer Models: Task-specific Context Attribution  Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs</h3>
<ul>
<li><strong>Authors: </strong>Stepan Tytarenko, Mohammad Ruhul Amin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16638">https://arxiv.org/abs/2401.16638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16638">https://arxiv.org/pdf/2401.16638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16638]] Breaking Free Transformer Models: Task-specific Context Attribution  Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs(https://arxiv.org/abs/2401.16638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability. In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution. We show that a linear transformation of the text representation from any transformer model using the task-specific concept operator results in a projection onto the latent concept space, referred to as context attribution in this paper. The specific concept operator is optimized during the supervised learning stage via novel loss functions. The proposed framework demonstrates that context attribution of the text representation for each task objective can improve the capacity of the discriminator function and thus achieve better performance for the classification task. Experimental results on three datasets, namely HateXplain, IMDB reviews, and Social Media Attributions, illustrate that the proposed model attains superior accuracy and generalizability. Specifically, for the non-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in accuracy and 10% improvement in F1-score. Whereas for the IMDB dataset, fine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and F1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT fine-tuned on the IMDB dataset in conjunction with the proposed model improves the F1-score on the HateXplain dataset by 7%. For the Social Media Attributions dataset of YouTube comments, we observe 5.2% increase in F1-metric. The proposed framework is implemented with PyTorch and provided open-source on GitHub.</li>
<li><strong>摘要：</strong>在特定数据集上微调大型预训练语言模型 (LLM) 是自然语言处理 (NLP) 分类任务中常用的策略。然而，这种方法通常会导致模型普遍性的损失。在本文中，我们提出了一个框架，可以保持通用性，并通过利用特定于任务的上下文归因来增强下游任务的性能。我们证明，使用特定于任务的概念运算符对任何转换器模型的文本表示进行线性转换会导致投影到潜在概念空间，在本文中称为上下文归因。在监督学习阶段通过新颖的损失函数优化特定概念算子。所提出的框架表明，每个任务目标的文本表示的上下文归因可以提高鉴别器函数的能力，从而实现更好的分类任务性能。在 HateXplain、IMDB 评论和社交媒体归因这三个数据集上的实验结果表明，所提出的模型具有卓越的准确性和泛化性。具体来说，对于 HateXplain 数据集上的非微调 BERT，我们观察到准确度提高了 8%，F1 分数提高了 10%。而对于 IMDB 数据集，经过微调的最先进的 XLNet 在准确度和 F1 分数方面均优于 1%。此外，在域外跨数据集测试中，DistilBERT 结合所提出的模型对 IMDB 数据集进行微调，将 HateXplain 数据集上的 F1 分数提高了 7%。对于 YouTube 评论的社交媒体归因数据集，我们观察到 F1 指标增加了 5.2%。所提出的框架是用 PyTorch 实现的，并在 GitHub 上提供开源。</li>
</ul>

<h3>Title: TeenyTinyLlama: open-source tiny language models trained in Brazilian  Portuguese</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Kluge Corrêa, Sophia Falk, Shiza Fatimah, Aniket Sen, Nythamar de Oliveira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16640">https://arxiv.org/abs/2401.16640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16640">https://arxiv.org/pdf/2401.16640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16640]] TeenyTinyLlama: open-source tiny language models trained in Brazilian  Portuguese(https://arxiv.org/abs/2401.16640)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在自然语言处理方面取得了显着的进步，但其进步在不同语言中尚未达到同等水平。虽然大多数法学硕士都接受过英语等高资源语言的培训，但多语言模型的表现通常不如单语言模型。此外，其多语言基础的某些方面有时会限制它们产生的副产品，例如计算需求和许可制度。在这项研究中，我们记录了专为资源匮乏环境而设计的开放基础模型的开发、其局限性和好处。这是 TeenyTinyLlama 对：用于巴西葡萄牙语文本生成的两个紧凑模型。我们根据 Apache 2.0 许可在 GitHub 和 Hugging Face 上发布它们，以供社区使用和进一步开发。请参阅 https://github.com/Nkluge-correa/TeenyTinyLlama</li>
</ul>

<h3>Title: Incoherent Probability Judgments in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jian-Qiao Zhu, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16646">https://arxiv.org/abs/2401.16646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16646">https://arxiv.org/pdf/2401.16646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16646]] Incoherent Probability Judgments in Large Language Models(https://arxiv.org/abs/2401.16646)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.</li>
<li><strong>摘要：</strong>经过训练用于下一个单词预测的自回归大型语言模型 (LLM) 在生成连贯文本方面表现出了非凡的能力。但他们同样擅长形成连贯的概率判断吗？我们使用概率恒等式和重复判断来评估法学硕士做出的概率判断的一致性。我们的结果表明，这些模型产生的判断往往是不连贯的，表现出与概率论规则类似的人类系统性偏差。此外，当被提示对同一事件进行判断时，法学硕士产生的概率判断的均值-方差关系呈现出与人类相似的倒U形关系。我们提出，这些与理性的偏差可以通过将自回归法学硕士与隐式贝叶斯推理联系起来并与人类概率判断的贝叶斯采样器模型进行比较来解释。</li>
</ul>

<h3>Title: Using Motion Forecasting for Behavior-Based Virtual Reality (VR)  Authentication</h3>
<ul>
<li><strong>Authors: </strong>Mingjun Li, Natasha Kholgade Banerjee, Sean Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16649">https://arxiv.org/abs/2401.16649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16649">https://arxiv.org/pdf/2401.16649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16649]] Using Motion Forecasting for Behavior-Based Virtual Reality (VR)  Authentication(https://arxiv.org/abs/2401.16649)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Task-based behavioral biometric authentication of users interacting in virtual reality (VR) environments enables seamless continuous authentication by using only the motion trajectories of the person's body as a unique signature. Deep learning-based approaches for behavioral biometrics show high accuracy when using complete or near complete portions of the user trajectory, but show lower performance when using smaller segments from the start of the task. Thus, any systems designed with existing techniques are vulnerable while waiting for future segments of motion trajectories to become available. In this work, we present the first approach that predicts future user behavior using Transformer-based forecasting and using the forecasted trajectory to perform user authentication. Our work leverages the notion that given the current trajectory of a user in a task-based environment we can predict the future trajectory of the user as they are unlikely to dramatically shift their behavior since it would preclude the user from successfully completing their task goal. Using the publicly available 41-subject ball throwing dataset of Miller et al. we show improvement in user authentication when using forecasted data. When compared to no forecasting, our approach reduces the authentication equal error rate (EER) by an average of 23.85% and a maximum reduction of 36.14%.</li>
<li><strong>摘要：</strong>对虚拟现实 (VR) 环境中交互的用户进行基于任务的行为生物识别认证，仅使用人身体的运动轨迹作为唯一签名，从而实现无缝连续认证。基于深度学习的行为生物识别方法在使用用户轨迹的完整或接近完整部分时显示出较高的准确性，但在从任务开始时使用较小的部分时显示出较低的性能。因此，任何使用现有技术设计的系统在等待未来的运动轨迹片段变得可用时都容易受到攻击。在这项工作中，我们提出了第一种使用基于 Transformer 的预测来预测未来用户行为的方法，并使用预测的轨迹来执行用户身份验证。我们的工作利用了这样的概念：在基于任务的环境中给定用户的当前轨迹，我们可以预测用户的未来轨迹，因为他们不太可能显着改变他们的行为，因为这会妨碍用户成功完成他们的任务目标。使用 Miller 等人公开的 41 个对象的投球数据集。我们展示了使用预测数据时用户身份验证的改进。与没有预测相比，我们的方法平均降低了身份验证等错误率 (EER) 23.85%，最大降低了 36.14%。</li>
</ul>

<h3>Title: Augmenting Replay in World Models for Continual Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Luke Yang, Levin Kuhlmann, Gideon Kowadlo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16650">https://arxiv.org/abs/2401.16650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16650">https://arxiv.org/pdf/2401.16650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16650]] Augmenting Replay in World Models for Continual Reinforcement Learning(https://arxiv.org/abs/2401.16650)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and other failure modes such as the opposite -- lacking plasticity and being unable to learn a new task -- to be a potential limitation in continual learning systems.</li>
<li><strong>摘要：</strong>在持续强化学习中，强化学习 (RL) 代理的环境会发生变化。一个成功的系统应该适当地平衡保持智能体在已经学习的任务上的表现、稳定性，同时学习新任务、可塑性的相互冲突的要求。先进先出缓冲区通常用于增强此类设置中的学习，但需要大量内存。我们探索了对该缓冲区的增强应用，以减轻内存限制，并将其与基于世界模型的强化学习算法一起使用，以评估其促进持续学习的有效性。我们在 Procgen 和 Atari RL 基准测试中评估了我们的方法的有效性，并表明在潜在世界模型的上下文中使用的重放缓冲区的分布匹配增强可以成功防止灾难性遗忘，同时显着减少计算开销。然而，我们也发现这样的解决方案并非完全可靠，而其他失败模式（例如相反的缺乏可塑性和无法学习新任务）是持续学习系统的潜在限制。</li>
</ul>

<h3>Title: Gradient-Based Language Model Red Teaming</h3>
<ul>
<li><strong>Authors: </strong>Nevan Wichers, Carson Denison, Ahmad Beirami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16656">https://arxiv.org/abs/2401.16656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16656">https://arxiv.org/pdf/2401.16656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16656]] Gradient-Based Language Model Red Teaming(https://arxiv.org/abs/2401.16656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Red teaming is a common strategy for identifying weaknesses in generative language models (LMs), where adversarial prompts are produced that trigger an LM to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses than a strong reinforcement learning-based red teaming approach, and succeeds even when the LM has been fine-tuned to produce safer outputs.</li>
<li><strong>摘要：</strong>红队是识别生成语言模型 (LM) 弱点的常见策略，其中会产生对抗性提示，触发 LM 生成不安全的响应。红队对于模型对齐和评估都很有帮助，但如果由人类完成，则属于劳动密集型且难以扩展。在本文中，我们提出了基于梯度的红队（GBRT），这是一种红队方法，用于自动生成可能导致 LM 输出不安全响应的各种提示。 GBRT 是提示学习的一种形式，通过使用安全分类器对 LM 响应进行评分进行训练，然后通过冻结的安全分类器和 LM 进行反向传播以更新提示。为了提高输入提示的连贯性，我们引入了两种变体，它们增加了真实性损失，并对预训练模型进行微调以生成提示，而不是直接学习提示。我们的实验表明，与基于强化学习的红队方法相比，GBRT 在寻找触发 LM 生成不安全响应的提示方面更有效，并且即使 LM 已被微调以产生更安全的输出，GBRT 也能取得成功。</li>
</ul>

<h3>Title: Recovering Mental Representations from Large Language Models with Markov  Chain Monte Carlo</h3>
<ul>
<li><strong>Authors: </strong>Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16657">https://arxiv.org/abs/2401.16657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16657">https://arxiv.org/pdf/2401.16657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16657]] Recovering Mental Representations from Large Language Models with Markov  Chain Monte Carlo(https://arxiv.org/abs/2401.16657)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Simulating sampling algorithms with people has proven a useful method for efficiently probing and understanding their mental representations. We propose that the same methods can be used to study the representations of Large Language Models (LLMs). While one can always directly prompt either humans or LLMs to disclose their mental representations introspectively, we show that increased efficiency can be achieved by using LLMs as elements of a sampling algorithm. We explore the extent to which we recover human-like representations when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo (MCMC). We found a significant increase in efficiency and performance using adaptive sampling algorithms based on MCMC. We also highlight the potential of our method to yield a more general method of conducting Bayesian inference \textit{with} LLMs.</li>
<li><strong>摘要：</strong>事实证明，与人一起模拟采样算法是有效探测和理解他们的心理表征的有用方法。我们建议可以使用相同的方法来研究大型语言模型（LLM）的表示。虽然人们总是可以直接提示人类或法学硕士内省地披露他们的心理表征，但我们表明，通过使用法学硕士作为抽样算法的元素可以提高效率。我们探讨了当法学硕士接受直接采样和马尔可夫链蒙特卡罗 (MCMC) 询问时，我们在多大程度上恢复了类人表示。我们发现使用基于 MCMC 的自适应采样算法可以显着提高效率和性能。我们还强调了我们的方法的潜力，即产生一种更通用的方法来使用 LLM 进行贝叶斯推理 \textit{。</li>
</ul>

<h3>Title: OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on  E-Branchformer</h3>
<ul>
<li><strong>Authors: </strong>Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee-weon Jung, Shinji Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16658">https://arxiv.org/abs/2401.16658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16658">https://arxiv.org/pdf/2401.16658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16658]] OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on  E-Branchformer(https://arxiv.org/abs/2401.16658)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.</li>
<li><strong>摘要：</strong>最近的研究提倡完全开放的基金会模型，以促进透明度和开放科学。作为第一步，开放式 Whisper 式语音模型 (OWSM) 使用公开数据和开源工具包重现了 OpenAI 的 Whisper。为了重现 Whisper，之前的 OWSM v1 到 v3 模型仍然基于 Transformer，这可能会导致性能低于其他最先进的语音编码器。在这项工作中，我们的目标是在不需要额外训练数据的情况下提高 OWSM 的性能和效率。我们提出了两种尺度的基于 E-Branchformer 的 OWSM v3.1 模型，即 100M 和 1B。 1B 模型是已公开的最大的基于 E-Branchformer 的语音模型。它在绝大多数评估基准中都优于之前的 OWSM v3，同时推理速度提高了 25%。我们公开发布数据准备脚本、预训练模型和训练日志。</li>
</ul>

<h3>Title: Fast Dual-Regularized Autoencoder for Sparse Biological Data</h3>
<ul>
<li><strong>Authors: </strong>Aleksandar Poleksic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16664">https://arxiv.org/abs/2401.16664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16664">https://arxiv.org/pdf/2401.16664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16664]] Fast Dual-Regularized Autoencoder for Sparse Biological Data(https://arxiv.org/abs/2401.16664)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Relationship inference from sparse data is an important task with applications ranging from product recommendation to drug discovery. A recently proposed linear model for sparse matrix completion has demonstrated surprising advantage in speed and accuracy over more sophisticated recommender systems algorithms. Here we extend the linear model to develop a shallow autoencoder for the dual neighborhood-regularized matrix completion problem. We demonstrate the speed and accuracy advantage of our approach over the existing state-of-the-art in predicting drug-target interactions and drug-disease associations.</li>
<li><strong>摘要：</strong>从稀疏数据进行关系推断是一项重要的任务，其应用范围从产品推荐到药物发现。最近提出的稀疏矩阵补全线性模型在速度和准确性方面比更复杂的推荐系统算法表现出惊人的优势。在这里，我们扩展线性模型来开发用于双邻域正则化矩阵补全问题的浅层自动编码器。我们展示了我们的方法在预测药物-靶标相互作用和药物-疾病关联方面相对于现有最先进方法的速度和准确性优势。</li>
</ul>

<h3>Title: Is Artificial Intelligence Providing the Second Revolution for Weather  Forecasting?</h3>
<ul>
<li><strong>Authors: </strong>Fenghua Ling, Lin Ouyang, Boufeniza Redouane Larbi, Jing-Jia Luo, Tao Han, Xiaohui Zhong, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16669">https://arxiv.org/abs/2401.16669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16669">https://arxiv.org/pdf/2401.16669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16669]] Is Artificial Intelligence Providing the Second Revolution for Weather  Forecasting?(https://arxiv.org/abs/2401.16669)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The rapid advancement of artificial intelligence technologies, particularly in recent years, has led to the emergence of several large parameter artificial intelligence weather forecast models. These models represent a significant breakthrough, overcoming the limitations of traditional numerical weather prediction models and indicating a potential second revolution for weather forecast. This study explores the evolution of these advanced artificial intelligence forecast models, and based on the identified commonalities, proposes the "Three Large Rules" for their development. We discuss the potential of artificial intelligence in revolutionizing numerical weather prediction, briefly outlining the underlying reasons for this potential. Additionally, we explore key areas for future development prospects for large artificial intelligence weather forecast models, integrating the entire numerical prediction process. Through an example that combines a large artificial intelligence model with ocean wave forecasting, we illustrate how forecasters can adapt and leverage the advanced artificial intelligence model. While acknowledging the high accuracy, computational efficiency, and ease of deployment of large artificial intelligence forecast models, we emphasize the irreplaceable values of traditional numerical forecasts. We believe that the optimal future of weather forecasting lies in achieving a seamless integration of artificial intelligence and traditional numerical models. Such a synthesis is anticipated to offer a more comprehensive and reliable approach for future weather forecasting.</li>
<li><strong>摘要：</strong>人工智能技术的快速发展，特别是近年来，催生了多种大参数人工智能天气预报模型的出现。这些模型代表了重大突破，克服了传统数值天气预报模型的局限性，预示着天气预报潜在的第二次革命。本研究探讨了这些先进人工智能预测模型的演变，并基于发现的共性，提出了其发展的“三大规则”。我们讨论了人工智能在彻底改变数值天气预报方面的潜力，并简要概述了这种潜力的根本原因。此外，我们还探讨了大型人工智能天气预报模型未来发展前景的关键领域，整合了整个数值预测过程。通过一个将大型人工智能模型与海浪预报相结合的示例，我们说明了预报员如何适应和利用先进的人工智能模型。在承认大型人工智能预测模型的高精度、计算效率和易于部署的同时，我们强调传统数值预测的不可替代价值。我们相信天气预报的最佳未来在于实现人工智能与传统数值模型的无缝集成。这种综合预计将为未来的天气预报提供更全面、更可靠的方法。</li>
</ul>

<h3>Title: The Detection and Understanding of Fictional Discourse</h3>
<ul>
<li><strong>Authors: </strong>Andrew Piper, Haiqi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16678">https://arxiv.org/abs/2401.16678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16678">https://arxiv.org/pdf/2401.16678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16678]] The Detection and Understanding of Fictional Discourse(https://arxiv.org/abs/2401.16678)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>In this paper, we present a variety of classification experiments related to the task of fictional discourse detection. We utilize a diverse array of datasets, including contemporary professionally published fiction, historical fiction from the Hathi Trust, fanfiction, stories from Reddit, folk tales, GPT-generated stories, and anglophone world literature. Additionally, we introduce a new feature set of word "supersenses" that facilitate the goal of semantic generalization. The detection of fictional discourse can help enrich our knowledge of large cultural heritage archives and assist with the process of understanding the distinctive qualities of fictional storytelling more broadly.</li>
<li><strong>摘要：</strong>在本文中，我们提出了与虚构话语检测任务相关的各种分类实验。我们利用各种各样的数据集，包括当代专业出版的小说、Hathi Trust 的历史小说、同人小说、Reddit 的故事、民间故事、GPT 生成的故事和英语世界文学。此外，我们引入了单词“超意义”的新特征集，以促进语义泛化的目标。虚构话语的发现有助于丰富我们对大型文化遗产档案的了解，并有助于更广泛地理解虚构故事讲述的独特品质。</li>
</ul>

<h3>Title: EdgeOL: Efficient in-situ Online Learning on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Chao Wu, Alex K. Jones, Jingtong Hu, Yanzhi Wang, Xulong Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16694">https://arxiv.org/abs/2401.16694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16694">https://arxiv.org/pdf/2401.16694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16694]] EdgeOL: Efficient in-situ Online Learning on Edge Devices(https://arxiv.org/abs/2401.16694)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.</li>
<li><strong>摘要：</strong>机器人辅助老年护理和物体识别等新兴应用通常采用深度学习神经网络 (DNN) 模型，自然需要：i) 处理流式推理请求，ii) 适应可能的部署场景变化。在线模型微调被广泛采用来满足这些需求。然而，微调涉及大量能源消耗，使得在边缘设备上部署具有挑战性。在本文中，我们提出了 EdgeOL，这是一种边缘在线学习框架，可通过互调和内调优化来优化推理精度、微调执行时间和能源效率。实验结果表明，与即时在线学习策略相比，EdgeOL 平均降低整体微调执行时间 82%，能耗降低 74%，平均推理精度提高 1.70%。</li>
</ul>

<h3>Title: Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible  Cluster Shapes</h3>
<ul>
<li><strong>Authors: </strong>Yung-Peng Hsu, Hung-Hsuan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16708">https://arxiv.org/abs/2401.16708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16708">https://arxiv.org/pdf/2401.16708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16708]] Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible  Cluster Shapes(https://arxiv.org/abs/2401.16708)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability density function of the multivariate beta distribution. We introduce the properties of MBMM, describe the parameter learning procedure, and present the experimental results, showing that MBMM fits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at \url{https://github.com/hhchen1105/mbmm/}.</li>
<li><strong>摘要：</strong>本文介绍了多元 beta 混合模型（MBMM），这是一种新的软聚类概率模型。由于多元 beta 分布的灵活概率密度函数，MBMM 能够适应不同的簇形状。我们介绍了 MBMM 的特性，描述了参数学习过程，并展示了实验结果，表明 MBMM 适合合成数据集和真实数据集上的不同簇形状。该代码在 \url{https://github.com/hhchen1105/mbmm/} 上匿名发布。</li>
</ul>

<h3>Title: Recent Advances in Hate Speech Moderation: Multimodality and the Role of  Large Models</h3>
<ul>
<li><strong>Authors: </strong>Ming Shan Hee, Shivam Sharma, Rui Cao, Palash Nandi, Preslav Nakov, Tanmoy Chakraborty, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16727">https://arxiv.org/abs/2401.16727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16727">https://arxiv.org/pdf/2401.16727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16727]] Recent Advances in Hate Speech Moderation: Multimodality and the Role of  Large Models(https://arxiv.org/abs/2401.16727)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey concludes with a forward-looking perspective, outlining potential avenues for future research, including the exploration of novel AI methodologies, the ethical governance of AI in moderation, and the development of more nuanced, context-aware systems. This comprehensive overview aims to catalyze further research and foster a collaborative effort towards more sophisticated, responsible, and human-centric approaches to HS moderation in the digital era.\footnote{ \textcolor{red}{WARNING: This paper contains offensive examples.</li>
<li><strong>摘要：</strong>在不断发展的在线交流格局中，调节仇恨言论 (HS) 提出了一项复杂的挑战，而数字内容的多模式性质使这一挑战更加复杂。这项综合调查深入探讨了 HS 审核的最新进展，重点关注大型语言模型 (LLM) 和大型多模态模型 (LMM) 的新兴作用。我们的探索始于对当前文献的彻底分析，揭示了传播 HS 时文本、视觉和听觉元素之间微妙的相互作用。我们发现了整合这些模式的显着趋势，这主要是由于 HS 传播的复杂性和微妙性。重点强调法学硕士和 LMM 所推动的进步，它们已经开始重新定义检测和审核能力的边界。我们确定了研究中现有的差距，特别是在代表性不足的语言和文化的背景下，以及解决资源匮乏环境的解决方案的需求。该调查以前瞻性的视角结束，概述了未来研究的潜在途径，包括探索新颖的人工智能方法、适度的人工智能伦理治理以及开发更细致的情境感知系统。这份全面的概述旨在促进进一步的研究并促进合作，以实现数字时代更复杂、更负责任、以人为本的 HS 审核方法。\footnote{ \textcolor{red}{警告：本文包含令人反感的示例。</li>
</ul>

<h3>Title: Towards Generating Informative Textual Description for Neurons in  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shrayani Mondal, Rishabh Garodia, Arbaaz Qureshi, Taesung Lee, Youngja Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16731">https://arxiv.org/abs/2401.16731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16731">https://arxiv.org/pdf/2401.16731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16731]] Towards Generating Informative Textual Description for Neurons in  Language Models(https://arxiv.org/abs/2401.16731)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag</a></li>
<li><strong>Abstract: </strong>Recent developments in transformer-based language models have allowed them to capture a wide variety of world knowledge that can be adapted to downstream tasks with limited resources. However, what pieces of information are understood in these models is unclear, and neuron-level contributions in identifying them are largely unknown. Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model. In this paper, we take BERT as an example and we try to remove these constraints and propose a novel and scalable framework that ties textual descriptions to neurons. We leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset and use an unsupervised approach to explain neurons with these descriptors. Through various qualitative and quantitative analyses, we demonstrate the effectiveness of this framework in generating useful data-specific descriptors with little human involvement in identifying the neurons that encode these descriptors. In particular, our experiment shows that the proposed approach achieves 75% precision@2, and 50% recall@2</li>
<li><strong>摘要：</strong>基于变压器的语言模型的最新发展使他们能够捕获各种各样的世界知识，这些知识可以适应资源有限的下游任务。然而，这些模型中理解哪些信息尚不清楚，并且神经元水平在识别它们方面的贡献在很大程度上也是未知的。神经元可解释性的传统方法要么依赖于一组有限的预定义描述符，要么需要手动注释来训练辅助模型，然后该模型可以解释主要模型的神经元。在本文中，我们以 BERT 为例，尝试消除这些限制，并提出一种新颖且可扩展的框架，将文本描述与神经元联系起来。我们利用生成语言模型的潜力来发现数据集中存在的人类可解释的描述符，并使用无监督的方法用这些描述符来解释神经元。通过各种定性和定量分析，我们证明了该框架在生成有用的数据特定描述符方面的有效性，而几乎不需要人类参与识别编码这些描述符的神经元。特别是，我们的实验表明，所提出的方法实现了 75% 的精度@2 和 50% 的召回率@2</li>
</ul>

<h3>Title: Engineering A Large Language Model From Scratch</h3>
<ul>
<li><strong>Authors: </strong>Abiodun Finbarrs Oketunji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16736">https://arxiv.org/abs/2401.16736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16736">https://arxiv.org/pdf/2401.16736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16736]] Engineering A Large Language Model From Scratch(https://arxiv.org/abs/2401.16736)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical theory, the system achieves state-of-the-art results on natural language tasks whilst remaining interpretable and robust.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 领域深度学习的蓬勃发展导致了能够高度熟练地理解和生成人类语言的创新技术的开发和发布。 Atinuke 是一种基于 Transformer 的神经网络，通过利用独特的配置来优化各种语言任务的性能。该架构将用于处理顺序数据的层与注意机制交织在一起，以在输入和输出之间建立有意义的亲和力。由于其拓扑结构和超参数调整，它可以通过提取特征和学习复杂映射来模拟类人语言。 Atinuke 是模块化的、可扩展的，并且与现有的机器学习管道无缝集成。 Softmax、嵌入和多头注意力等高级矩阵运算可以对文本、声音和视觉信号进行细致入微的处理。通过将现代深度学习技术与软件设计原理和数学理论相结合，该系统在自然语言任务上取得了最先进的结果，同时保持可解释性和鲁棒性。</li>
</ul>

<h3>Title: MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16745">https://arxiv.org/abs/2401.16745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16745">https://arxiv.org/pdf/2401.16745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16745]] MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large  Language Models(https://arxiv.org/abs/2401.16745)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance. MT-Eval is released publicly to encourage future research towards more robust conversational models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越依赖于跨不同现实世界应用程序的复杂多轮对话。然而，现有的基准主要关注单轮评估，忽视了模型在多轮交互中的能力。为了解决这一差距，我们引入了 MT-Eval，这是一个旨在评估多轮对话能力的综合基准。通过分析人类与法学硕士的对话，我们将交互模式分为四种类型：回忆、扩展、细化和跟进。我们通过扩充现有数据集或使用 GPT-4 创建新示例来为每个类别构建多轮查询，以避免数据泄漏。为了研究影响多轮能力的因素，我们创建了 1170 多轮查询的单轮版本并比较性能。我们对 11 个知名 LLM 的评估表明，虽然闭源模型普遍超过开源模型，但某些开源模型在特定任务中超过了 GPT-3.5-Turbo。在大多数模型中，我们观察到与单匝设置相比，多匝设置的性能显着下降，这与模型的基本功能不相关。此外，我们将与相关内容的距离和对错误传播的敏感性确定为影响多轮性能的关键因素。 MT-Eval 公开发布是为了鼓励未来研究更强大的对话模型。</li>
</ul>

<h3>Title: Diffusion model for relational inference</h3>
<ul>
<li><strong>Authors: </strong>Shuhan Zheng, Ziqiang Li, Kantaro Fujiwara, Gouhei Tanaka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16755">https://arxiv.org/abs/2401.16755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16755">https://arxiv.org/pdf/2401.16755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16755]] Diffusion model for relational inference(https://arxiv.org/abs/2401.16755)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Dynamical behaviors of complex interacting systems, including brain activities, financial price movements, and physical collective phenomena, are associated with underlying interactions between the system's components. The issue of uncovering interaction relations in such systems using observable dynamics is called relational inference. In this study, we propose a Diffusion model for Relational Inference (DiffRI), inspired by a self-supervised method for probabilistic time series imputation. DiffRI learns to infer the probability of the presence of connections between components through conditional diffusion modeling. Experiments on both simulated and quasi-real datasets show that DiffRI is highly competent compared with other state-of-the-art models in discovering ground truth interactions in an unsupervised manner. Our code will be made public soon.</li>
<li><strong>摘要：</strong>复杂交互系统的动态行为，包括大脑活动、金融价格变动和物理集体现象，与系统组件之间的潜在交互相关。使用可观察的动力学揭示此类系统中的交互关系的问题称为关系推理。在本研究中，受概率时间序列插补的自监督方法的启发，我们提出了一种用于关系推理的扩散模型（DiffRI）。 DiffRI 学习通过条件扩散建模来推断组件之间存在连接的概率。对模拟数据集和准真实数据集的实验表明，与其他最先进的模型相比，DiffRI 在以无监督方式发现地面真实交互方面具有很强的能力。我们的代码很快就会公开。</li>
</ul>

<h3>Title: SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond  the Memory Budget</h3>
<ul>
<li><strong>Authors: </strong>Kun Wang, Jiani Cao, Zimu Zhou, Zhenjiang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16757">https://arxiv.org/abs/2401.16757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16757">https://arxiv.org/pdf/2401.16757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16757]] SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond  the Memory Budget(https://arxiv.org/abs/2401.16757)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Executing deep neural networks (DNNs) on edge artificial intelligence (AI) devices enables various autonomous mobile computing applications. However, the memory budget of edge AI devices restricts the number and complexity of DNNs allowed in such applications. Existing solutions, such as model compression or cloud offloading, reduce the memory footprint of DNN inference at the cost of decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN into blocks and swap them in and out in order, such that large DNNs can execute within a small memory budget. Nevertheless, naive swapping on edge AI devices induces significant delays due to the redundant memory operations in the DNN development ecosystem for edge AI devices. To this end, we develop SwapNet, an efficient DNN block swapping middleware for edge AI devices. We systematically eliminate the unnecessary memory operations during block swapping while retaining compatible with the deep learning frameworks, GPU backends, and hardware architectures of edge AI devices. We further showcase the utility of SwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference tasks in three applications demonstrate that SwapNet achieves almost the same latency as the case with sufficient memory even when DNNs demand 2.32x to 5.81x memory beyond the available budget. The design of SwapNet also provides novel and feasible insights for deploying large language models (LLMs) on edge AI devices in the future.</li>
<li><strong>摘要：</strong>在边缘人工智能 (AI) 设备上执行深度神经网络 (DNN) 可实现各种自主移动计算应用。然而，边缘 AI 设备的内存预算限制了此类应用中允许的 DNN 的数量和复杂性。现有的解决方案（例如模型压缩或云卸载）可以减少 DNN 推理的内存占用，但代价是模型准确性或自主性降低。为了避免这些缺点，我们将 DNN 分成块并按顺序交换它们，这样大型 DNN 就可以在较小的内存预算内执行。然而，由于边缘 AI 设备的 DNN 开发生态系统中存在冗余内存操作，边缘 AI 设备上的简单交换会导致严重的延迟。为此，我们开发了 SwapNet，这是一种用于边缘 AI 设备的高效 DNN 块交换中间件。我们系统地消除了块交换过程中不必要的内存操作，同时保持与边缘AI设备的深度学习框架、GPU后端和硬件架构的兼容。我们通过多 DNN 调度方案进一步展示了 SwapNet 的实用性。对三个应用程序中的 11 个 DNN 推理任务的评估表明，即使 DNN 需要超出可用预算 2.32 倍到 5.81 倍的内存，SwapNet 也能实现与具有足够内存的情况几乎相同的延迟。 SwapNet 的设计还为未来在边缘 AI 设备上部署大型语言模型 (LLM) 提供了新颖且可行的见解。</li>
</ul>

<h3>Title: MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups  and their Linker Joints</h3>
<ul>
<li><strong>Authors: </strong>Mogan Gim, Jueon Park, Soyon Park, Sanghoon Lee, Seungheun Baek, Junhyun Lee, Ngoc-Quang Nguyen, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16771">https://arxiv.org/abs/2401.16771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16771">https://arxiv.org/pdf/2401.16771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16771]] MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups  and their Linker Joints(https://arxiv.org/abs/2401.16771)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Molecular core structures and R-groups are essential concepts in drug development. Integration of these concepts with conventional graph pre-training approaches can promote deeper understanding in molecules. We propose MolPLA, a novel pre-training framework that employs masked graph contrastive learning in understanding the underlying decomposable parts inmolecules that implicate their core structure and peripheral R-groups. Furthermore, we formulate an additional framework that grants MolPLA the ability to help chemists find replaceable R-groups in lead optimization scenarios. Experimental results on molecular property prediction show that MolPLA exhibits predictability comparable to current state-of-the-art models. Qualitative analysis implicate that MolPLA is capable of distinguishing core and R-group sub-structures, identifying decomposable regions in molecules and contributing to lead optimization scenarios by rationally suggesting R-group replacements given various query core templates. The code implementation for MolPLA and its pre-trained model checkpoint is available at https://github.com/dmis-lab/MolPLA</li>
<li><strong>摘要：</strong>分子核心结构和 R 基团是药物开发中的基本概念。将这些概念与传统的图预训练方法相结合可以促进对分子的更深入理解。我们提出了 MolPLA，一种新颖的预训练框架，它采用掩模图对比学习来理解分子中涉及其核心结构和外围 R 基团的潜在可分解部分。此外，我们制定了一个额外的框架，使 MolPLA 能够帮助化学家在先导化合物优化场景中找到可替换的 R 基团。分子特性预测的实验结果表明，MolPLA 表现出与当前最先进模型相当的可预测性。定性分析表明，MolPLA 能够区分核心和 R 基团子结构，识别分子中的可分解区域，并通过在给定各种查询核心模板的情况下合理建议 R 基团替换来促进先导化合物优化方案。 MolPLA 及其预训练模型检查点的代码实现位于 https://github.com/dmis-lab/MolPLA</li>
</ul>

<h3>Title: Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator</h3>
<ul>
<li><strong>Authors: </strong>Ryoma Furuyama, Daiki Kuyoshi, Satoshi Yamane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16772">https://arxiv.org/abs/2401.16772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16772">https://arxiv.org/pdf/2401.16772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16772]] Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator(https://arxiv.org/abs/2401.16772)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial inverse reinforcement learning that rewards the agent for performing actions in status similar to the demo. We call this algorithm Discriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo environments.</li>
<li><strong>摘要：</strong>在奖励设计困难或奖励稀疏的环境中，除了强化学习之外，还经常使用模仿学习，但很难从少量专家数据和样本数据中很好地模仿未知状态。行为克隆等监督学习方法不需要采样数据，但通常会受到分布偏移的影响。基于强化学习的方法，例如逆强化学习和生成对抗模仿学习（GAIL），只能从少数专家数据中进行学习。然而，他们经常需要与环境互动。 Soft Q 模仿学习（SQIL）解决了这个问题，并且事实证明，它可以通过将行为克隆和软 Q 学习与恒定奖励相结合来有效地学习。为了使该算法对分布偏移更加鲁棒，我们提出了更高效、更鲁棒的算法，为此方法添加了基于对抗性逆强化学习的奖励函数，该函数奖励代理在与演示类似的状态下执行操作。我们将此算法称为鉴别器软 Q 模仿学习 (DSQIL)。我们在 MuJoCo 环境中对其进行了评估。</li>
</ul>

<h3>Title: Graph Fairness Learning under Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Yibo Li, Xiao Wang, Yujie Xing, Shaohua Fan, Ruijia Wang, Yaoqi Liu, Chuan Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16784">https://arxiv.org/abs/2401.16784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16784">https://arxiv.org/pdf/2401.16784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16784]] Graph Fairness Learning under Distribution Shifts(https://arxiv.org/abs/2401.16784)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. Motivated by our theoretical analysis, we propose our framework FatraGNN. Specifically, to guarantee fairness performance on unknown testing graphs, we propose a graph generator to produce numerous graphs with significant bias and under different distributions. Then we minimize the representation distances for each certain group between the training graph and generated graphs. This empowers our model to achieve high classification and fairness performance even on generated graphs with significant bias, thereby effectively handling unknown testing graphs. Experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of our model in terms of both accuracy and fairness.</li>
<li><strong>摘要：</strong>图神经网络（GNN）在图结构数据上取得了显着的性能。然而，GNN 可能会继承训练数据的偏见，并根据性别和种族等敏感属性做出歧视性预测。最近，人们对确保 GNN 的公平性越来越感兴趣，但所有这些都假设训练和测试数据处于相同的分布下，即训练数据和测试数据来自同一个图。在分布变化的情况下，图的公平性性能会下降吗？分布变化如何影响图公平学习？所有这些悬而未决的问题在很大程度上都没有从理论角度得到探索。为了回答这些问题，我们首先从理论上确定决定图表偏差的因素。随后，我们探讨了影响测试图公平性的因素，其中一个值得注意的因素是训练图和测试图之间某些组的表示距离。在理论分析的推动下，我们提出了 FatraGNN 框架。具体来说，为了保证未知测试图的公平性，我们提出了一个图生成器来生成大量具有显着偏差且在不同分布下的图。然后，我们最小化训练图和生成图之间每个特定组的表示距离。这使得我们的模型即使在具有显着偏差的生成图上也能实现高分类和公平性能，从而有效地处理未知的测试图。对现实世界和半合成数据集的实验证明了我们的模型在准确性和公平性方面的有效性。</li>
</ul>

<h3>Title: Enhancing Efficiency and Robustness in Support Vector Regression with  HawkEye Loss</h3>
<ul>
<li><strong>Authors: </strong>Mushir Akhtar, M. Tanveer, Mohd. Arshad</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16785">https://arxiv.org/abs/2401.16785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16785">https://arxiv.org/pdf/2401.16785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16785]] Enhancing Efficiency and Robustness in Support Vector Regression with  HawkEye Loss(https://arxiv.org/abs/2401.16785)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Support vector regression (SVR) has garnered significant popularity over the past two decades owing to its wide range of applications across various fields. Despite its versatility, SVR encounters challenges when confronted with outliers and noise, primarily due to the use of the $\varepsilon$-insensitive loss function. To address this limitation, SVR with bounded loss functions has emerged as an appealing alternative, offering enhanced generalization performance and robustness. Notably, recent developments focus on designing bounded loss functions with smooth characteristics, facilitating the adoption of gradient-based optimization algorithms. However, it's crucial to highlight that these bounded and smooth loss functions do not possess an insensitive zone. In this paper, we address the aforementioned constraints by introducing a novel symmetric loss function named the HawkEye loss function. It is worth noting that the HawkEye loss function stands out as the first loss function in SVR literature to be bounded, smooth, and simultaneously possess an insensitive zone. Leveraging this breakthrough, we integrate the HawkEye loss function into the least squares framework of SVR and yield a new fast and robust model termed HE-LSSVR. The optimization problem inherent to HE-LSSVR is addressed by harnessing the adaptive moment estimation (Adam) algorithm, known for its adaptive learning rate and efficacy in handling large-scale problems. To our knowledge, this is the first time Adam has been employed to solve an SVR problem. To empirically validate the proposed HE-LSSVR model, we evaluate it on UCI, synthetic, and time series datasets. The experimental outcomes unequivocally reveal the superiority of the HE-LSSVR model both in terms of its remarkable generalization performance and its efficiency in training time.</li>
<li><strong>摘要：</strong>支持向量回归（SVR）由于其在各个领域的广泛应用，在过去二十年中获得了极大的普及。尽管 SVR 具有多功能性，但在遇到异常值和噪声时会遇到挑战，这主要是由于使用了对 $\varepsilon$ 不敏感的损失函数。为了解决这一限制，具有有界损失函数的 SVR 已成为一种有吸引力的替代方案，提供增强的泛化性能和鲁棒性。值得注意的是，最近的发展重点是设计具有平滑特性的有界损失函数，促进基于梯度的优化算法的采用。然而，重要的是要强调这些有界且平滑的损失函数不具有不敏感区域。在本文中，我们通过引入一种新颖的对称损失函数（称为 HawkEye 损失函数）来解决上述约束。值得注意的是，HawkEye 损失函数是 SVR 文献中第一个有界、平滑且同时具有不敏感区域的损失函数。利用这一突破，我们将 HawkEye 损失函数集成到 SVR 的最小二乘框架中，并产生了一种新的快速且稳健的模型，称为 HE-LSSVR。 HE-LSSVR 固有的优化问题通过利用自适应矩估计 (Adam) 算法来解决，该算法以其自适应学习率和处理大规模问题的功效而闻名。据我们所知，这是 Adam 第一次被用来解决 SVR 问题。为了凭经验验证所提出的 HE-LSSVR 模型，我们在 UCI、合成和时间序列数据集上对其进行了评估。实验结果明确地揭示了 HE-LSSVR 模型在卓越的泛化性能和训练时间效率方面的优越性。</li>
</ul>

<h3>Title: Can Large Language Models be Trusted for Evaluation? Scalable  Meta-Evaluation of LLMs as Evaluators via Agent Debate</h3>
<ul>
<li><strong>Authors: </strong>Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16788">https://arxiv.org/abs/2401.16788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16788">https://arxiv.org/pdf/2401.16788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16788]] Can Large Language Models be Trusted for Evaluation? Scalable  Meta-Evaluation of LLMs as Evaluators via Agent Debate(https://arxiv.org/abs/2401.16788)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag, agent</a></li>
<li><strong>Abstract: </strong>Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \url{https://github.com/GAIR-NLP/scaleeval}.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 在各种任务和场景中都有实用性，但开发一种跨不同环境可靠评估 LLM 的方法仍然具有挑战性。现代评估方法通常使用法学硕士来评估法学硕士产生的反应。然而，为评估这些法学硕士作为评估者的有效性而进行的元评估通常受到现有基准覆盖范围的限制，或者需要大量的人工注释。这强调了可扩展元评估方法的紧迫性，这些方法可以有效、可靠和高效地评估法学硕士作为跨不同任务和场景的评估者的表现，特别是在潜在的新的、用户定义的场景中。为了填补这一空白，我们提出了 ScaleEval，这是一种代理辩论辅助的元评估框架，它利用了多个交流 LLM 代理的功能。该框架支持多轮讨论，以帮助人类注释者识别最有能力的法学硕士作为评估者，这在过去需要在元评估期间进行大规模注释的情况下显着减轻了他们的工作量。我们发布了框架的代码，可在以下位置公开获取：\url{https://github.com/GAIR-NLP/scaleeval}。</li>
</ul>

<h3>Title: Accelerated Cloud for Artificial Intelligence (ACAI)</h3>
<ul>
<li><strong>Authors: </strong>Dachi Chen, Weitian Ding, Chen Liang, Chang Xu, Junwei Zhang, Majd Sakr</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16791">https://arxiv.org/abs/2401.16791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16791">https://arxiv.org/pdf/2401.16791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16791]] Accelerated Cloud for Artificial Intelligence (ACAI)(https://arxiv.org/abs/2401.16791)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Training an effective Machine learning (ML) model is an iterative process that requires effort in multiple dimensions. Vertically, a single pipeline typically includes an initial ETL (Extract, Transform, Load) of raw datasets, a model training stage, and an evaluation stage where the practitioners obtain statistics of the model performance. Horizontally, many such pipelines may be required to find the best model within a search space of model configurations. Many practitioners resort to maintaining logs manually and writing simple glue code to automate the workflow. However, carrying out this process on the cloud is not a trivial task in terms of resource provisioning, data management, and bookkeeping of job histories to make sure the results are reproducible. We propose an end-to-end cloud-based machine learning platform, Accelerated Cloud for AI (ACAI), to help improve the productivity of ML practitioners. ACAI achieves this goal by enabling cloud-based storage of indexed, labeled, and searchable data, as well as automatic resource provisioning, job scheduling, and experiment tracking. Specifically, ACAI provides practitioners (1) a data lake for storing versioned datasets and their corresponding metadata, and (2) an execution engine for executing ML jobs on the cloud with automatic resource provisioning (auto-provision), logging and provenance tracking. To evaluate ACAI, we test the efficacy of our auto-provisioner on the MNIST handwritten digit classification task, and we study the usability of our system using experiments and interviews. We show that our auto-provisioner produces a 1.7x speed-up and 39% cost reduction, and our system reduces experiment time for ML scientists by 20% on typical ML use cases.</li>
<li><strong>摘要：</strong>训练有效的机器学习 (ML) 模型是一个迭代过程，需要多方面的努力。纵向上，单个管道通常包括原始数据集的初始 ETL（提取、转换、加载）、模型训练阶段以及从业者获取模型性能统计数据的评估阶段。在水平方向上，可能需要许多这样的管道来在模型配置的搜索空间中找到最佳模型。许多从业者诉诸手动维护日志并编写简单的粘合代码来自动化工作流程。然而，在云上执行此过程在资源配置、数据管理和作业历史记录以确保结果可重现方面并不是一项简单的任务。我们提出了一个基于云的端到端机器学习平台，即 Accelerated Cloud for AI (ACAI)，以帮助提高 ML 从业者的生产力。 ACAI 通过支持基于云的索引、标记和可搜索数据存储，以及自动资源配置、作业调度和实验跟踪来实现这一目标。具体来说，ACAI 为从业者提供了（1）一个用于存储版本化数据集及其相应元数据的数据湖，以及（2）一个用于在云上执行 ML 作业的执行引擎，具有自动资源配置（自动配置）、日志记录和来源跟踪功能。为了评估 ACAI，我们测试了自动配置器在 MNIST 手写数字分类任务上的功效，并通过实验和访谈研究了我们系统的可用性。我们表明，我们的自动配置器可实现 1.7 倍的加速和 39% 的成本降低，并且我们的系统将 ML 科学家在典型 ML 用例上的实验时间减少了 20%。</li>
</ul>

<h3>Title: Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of  Traditional EHR Data Imputation in Downstream Clinical Prediction</h3>
<ul>
<li><strong>Authors: </strong>Weibin Liao, Yinghao Zhu, Zixiang Wang, Xu Chu, Yasha Wang, Liantao Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16796">https://arxiv.org/abs/2401.16796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16796">https://arxiv.org/pdf/2401.16796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16796]] Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of  Traditional EHR Data Imputation in Downstream Clinical Prediction(https://arxiv.org/abs/2401.16796)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Analyzing the health status of patients based on Electronic Health Records (EHR) is a fundamental research problem in medical informatics. The presence of extensive missing values in EHR makes it challenging for deep neural networks to directly model the patient's health status based on EHR. Existing deep learning training protocols require the use of statistical information or imputation models to reconstruct missing values; however, the protocols inject non-realistic data into downstream EHR analysis models, significantly limiting model performance. This paper introduces Learnable Prompt as Pseudo Imputation (PAI) as a new training protocol. PAI no longer introduces any imputed data but constructs a learnable prompt to model the implicit preferences of the downstream model for missing values, resulting in a significant performance improvement for all EHR analysis models. Additionally, our experiments show that PAI exhibits higher robustness in situations of data insufficiency and high missing rates. More importantly, in a real-world application involving cross-institutional data with zero-shot evaluation, PAI demonstrates stronger model generalization capabilities for non-overlapping features.</li>
<li><strong>摘要：</strong>基于电子健康记录（EHR）分析患者的健康状况是医学信息学的一个基本研究问题。 EHR 中存在大量缺失值，这使得深度神经网络难以根据 EHR 直接对患者的健康状况进行建模。现有的深度学习训练协议需要使用统计信息或插补模型来重建缺失值；然而，这些协议将不真实的数据注入下游 EHR 分析模型，极大地限制了模型的性能。本文介绍了 Learnable Prompt as Pseudo Imputation (PAI) 作为一种新的训练协议。 PAI 不再引入任何估算数据，而是构建一个可学习的提示来对下游模型对缺失值的隐式偏好进行建模，从而显着提高所有 EHR 分析模型的性能。此外，我们的实验表明，PAI 在数据不足和高缺失率的情况下表现出更高的鲁棒性。更重要的是，在涉及零样本评估的跨机构数据的实际应用中，PAI 对于非重叠特征表现出了更强的模型泛化能力。</li>
</ul>

<h3>Title: Encoding Temporal Statistical-space Priors via Augmented Representation</h3>
<ul>
<li><strong>Authors: </strong>Insu Choi, Woosung Koh, Gimin Kang, Yuntae Jang, Woo Chang Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16808">https://arxiv.org/abs/2401.16808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16808">https://arxiv.org/pdf/2401.16808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16808]] Encoding Temporal Statistical-space Priors via Augmented Representation(https://arxiv.org/abs/2401.16808)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are available throughout the writing for a clear and rigorous understanding.</li>
<li><strong>摘要：</strong>时间序列数据建模仍然是一个普遍存在的问题，因为时间维度是许多领域所固有的。尽管时间序列预测取得了重大进展，但高噪声信号比、非正态性、非平稳性和数据缺乏仍然对从业者提出挑战。作为回应，我们利用简单的表示增强技术来克服这些挑战。我们的增强表示充当每个时间步骤的先验编码的统计空间。作为回应，我们将我们的方法命名为统计空间增强表示（SSAR）。底层的高维数据生成过程激发了我们的表示增强。我们使用两种下游时间学习算法严格检查两个数据集的经验泛化性能。我们的方法明显优于所有五个最新基线。此外，我们方法的高度模块化性质可以轻松应用于各种设置。最后，在整个写作中提供了成熟的理论观点，以实现清晰而严格的理解。</li>
</ul>

<h3>Title: H2O-Danube-1.8B Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Philipp Singer, Pascal Pfeiffer, Yauhen Babakhin, Maximilian Jeblick, Nischay Dhankhar, Gabor Fodor, Sri Satish Ambati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16818">https://arxiv.org/abs/2401.16818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16818">https://arxiv.org/pdf/2401.16818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16818]] H2O-Danube-1.8B Technical Report(https://arxiv.org/abs/2401.16818)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.</li>
<li><strong>摘要：</strong>我们提出了 H2O-Danube-1.8B，这是一种在 1T 代币上训练的 1.8B 语言模型，遵循 LLama 2 和 Mistral 的核心原则。我们利用和完善各种技术来预训练大型语言模型。尽管与类似大小的参考模型相比，我们的模型训练的总代币数量要少得多，但它在多个基准测试中表现出极具竞争力的指标。我们还发布了一个经过监督微调和直接偏好优化训练的聊天模型。我们在 Apache 2.0 许可证下公开提供 H2O-Danube-1.8B，进一步使法学硕士在经济上向更广泛的受众民主化。</li>
</ul>

<h3>Title: Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Juefei Chen, Longxiu Huang, Yimin Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16836">https://arxiv.org/abs/2401.16836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16836">https://arxiv.org/pdf/2401.16836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16836]] Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition(https://arxiv.org/abs/2401.16836)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos. The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF). In this work, we provide an alternating index selection method to select the coseparable core. Furthermore, we validate the t-CUR sampling theory and integrate it with the tensor Discrete Empirical Interpolation Method (t-DEIM) to introduce an alternative, randomized index selection process. These methods have been tested on both synthetic and facial analysis datasets. The results demonstrate the efficiency of coseparable NTF when compared to coseparable NMF.</li>
<li><strong>摘要：</strong>非负矩阵分解（NMF）是一种重要的无监督学习方法，用于从数据中提取有意义的特征。为了解决多项式时间框架内的 NMF 问题，研究人员引入了可分离性假设，该假设最近演变成可分离性的概念。这一进步为原始数据提供了更有效的核心表示。然而，在现实世界中，数据更自然地表示为多维数组，例如图像或视频。 NMF 对高维数据的应用涉及矢量化，这可能会丢失必要的多维相关性。为了保留数据中的这些固有相关性，我们转向张量（多维数组）并利用张量 t 乘积。这种方法将可分 NMF 扩展到张量设置，创建了我们所说的可分非负张量分解 (NTF)。在这项工作中，我们提供了一种交替索引选择方法来选择可分离核心。此外，我们验证了 t-CUR 采样理论，并将其与张量离散经验插值法 (t-DEIM) 相结合，以引入另一种随机索引选择过程。这些方法已经在合成数据集和面部分析数据集上进行了测试。结果证明了可分离 NTF 与可分离 NMF 相比的效率。</li>
</ul>

<h3>Title: Evaluating ML-Based Anomaly Detection Across Datasets of Varied  Integrity: A Case Study</h3>
<ul>
<li><strong>Authors: </strong>Adrian Pekar, Richard Jozsa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16843">https://arxiv.org/abs/2401.16843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16843">https://arxiv.org/pdf/2401.16843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16843]] Evaluating ML-Based Anomaly Detection Across Datasets of Varied  Integrity: A Case Study(https://arxiv.org/abs/2401.16843)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Cybersecurity remains a critical challenge in the digital age, with network traffic flow anomaly detection being a key pivotal instrument in the fight against cyber threats. In this study, we address the prevalent issue of data integrity in network traffic datasets, which are instrumental in developing machine learning (ML) models for anomaly detection. We introduce two refined versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed using NFStream to ensure methodologically sound flow expiration and labeling. Our research contrasts the performance of the Random Forest (RF) algorithm across the original CICIDS-2017, its refined counterparts WTMC-2021 and CRiSIS-2022, and our NFStream-generated datasets, in both binary and multi-class classification contexts. We observe that the RF model exhibits exceptional robustness, achieving consistent high-performance metrics irrespective of the underlying dataset quality, which prompts a critical discussion on the actual impact of data integrity on ML efficacy. Our study underscores the importance of continual refinement and methodological rigor in dataset generation for network security research. As the landscape of network threats evolves, so must the tools and techniques used to detect and analyze them.</li>
<li><strong>摘要：</strong>网络安全仍然是数字时代的一项严峻挑战，网络流量异常检测是对抗网络威胁的关键工具。在这项研究中，我们解决了网络流量数据集中普遍存在的数据完整性问题，这有助于开发用于异常检测的机器学习 (ML) 模型。我们引入了 CICIDS-2017 数据集的两个改进版本：NFS-2023-nTE 和 NFS-2023-TE，它们使用 NFStream 进行处理，以确保方法上合理的流过期和标记。我们的研究对比了原始 CICIDS-2017、其改进版 WTMC-2021 和 CRiSIS-2022 以及我们的 NFStream 生成的数据集在二元和多类分类上下文中的随机森林 (RF) 算法的性能。我们观察到 RF 模型表现出卓越的稳健性，无论底层数据集质量如何，都能实现一致的高性能指标，这引发了关于数据完整性对 ML 功效的实际影响的批判性讨论。我们的研究强调了网络安全研究数据集生成中持续改进和方法严谨的重要性。随着网络威胁形势的发展，用于检测和分析威胁的工具和技术也必须不断发展。</li>
</ul>

<h3>Title: State Value Generation with Prompt Learning and Self-Training for  Low-Resource Dialogue State Tracking</h3>
<ul>
<li><strong>Authors: </strong>Ming Gu, Yan Yang, Chengcai Chen, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16862">https://arxiv.org/abs/2401.16862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16862">https://arxiv.org/pdf/2401.16862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16862]] State Value Generation with Prompt Learning and Self-Training for  Low-Resource Dialogue State Tracking(https://arxiv.org/abs/2401.16862)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Recently, low-resource dialogue state tracking (DST) has received increasing attention. First obtaining state values then based on values to generate slot types has made great progress in this task. However, obtaining state values is still an under-studied problem. Existing extraction-based approaches cannot capture values that require the understanding of context and are not generalizable either. To address these issues, we propose a novel State VAlue Generation based framework (SVAG), decomposing DST into state value generation and domain slot generation. Specifically, we propose to generate state values and use self-training to further improve state value generation. Moreover, we design an estimator aiming at detecting incomplete generation and incorrect generation for pseudo-labeled data selection during self-training. Experimental results on the MultiWOZ 2.1 dataset show that our method which has only less than 1 billion parameters achieves state-of-the-art performance under the data ratio settings of 5%, 10%, and 25% when limited to models under 100 billion parameters. Compared to models with more than 100 billion parameters, SVAG still reaches competitive results.</li>
<li><strong>摘要：</strong>最近，低资源对话状态跟踪（DST）受到越来越多的关注。首先获取状态值，然后根据值生成槽类型，在这项任务中取得了很大进展。然而，获取状态值仍然是一个尚未充分研究的问题。现有的基于提取的方法无法捕获需要理解上下文的值，并且也无法推广。为了解决这些问题，我们提出了一种新颖的基于状态值生成的框架（SVAG），将 DST 分解为状态值生成和域槽生成。具体来说，我们建议生成状态值并使用自我训练来进一步改进状态值生成。此外，我们设计了一个估计器，旨在检测自训练期间伪标记数据选择的不完整生成和错误生成。 MultiWOZ 2.1数据集上的实验结果表明，当限制在1000亿以下的模型时，我们的方法只有不到10亿个参数，在5%、10%和25%的数据比率设置下实现了state-of-the-art的性能参数。与超过1000亿个参数的模型相比，SVAG仍然达到了有竞争力的结果。</li>
</ul>

<h3>Title: Cross-Lingual Transfer from Related Languages: Treating Low-Resource  Maltese as Multilingual Code-Switching</h3>
<ul>
<li><strong>Authors: </strong>Kurt Micallef, Nizar Habash, Claudia Borg, Fadhl Eryani, Houda Bouamor</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16895">https://arxiv.org/abs/2401.16895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16895">https://arxiv.org/pdf/2401.16895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16895]] Cross-Lingual Transfer from Related Languages: Treating Low-Resource  Maltese as Multilingual Code-Switching(https://arxiv.org/abs/2401.16895)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model's pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language, thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese language. We contrast indiscriminate transliteration or translation to mixing processing pipelines that only transliterate words of Arabic origin, thereby resulting in text with a mixture of scripts. We fine-tune the processed data on four downstream tasks and show that conditional transliteration based on word etymology yields the best results, surpassing fine-tuning with raw Maltese or Maltese processed with non-selective pipelines.</li>
<li><strong>摘要：</strong>尽管多语言模型在未见过的语言上表现出令人印象深刻的跨语言传输能力，但当多语言模型的预训练数据中使用的语言存在脚本差异时，下游任务的性能会受到影响。使用音译提供了一种简单而有效的方法，将资源丰富的语言的脚本与目标语言对齐，从而增强跨语言传输能力。然而，对于混合语言，这种方法并不是最理想的，因为只有一部分语言从跨语言迁移中受益，而其余语言则受到阻碍。在这项工作中，我们重点关注马耳他语，这是一种闪族语言，受到阿拉伯语、意大利语和英语的重大影响，特别是用拉丁文字书写。我们提出了一个用词级词源注释的新颖数据集。我们使用该数据集来训练分类器，使我们能够就马耳他语言中每个标记的适当处理做出明智的决策。我们将不加区别的音译或翻译与仅音译阿拉伯语来源的单词的混合处理管道进行对比，从而产生混合脚本的文本。我们对四个下游任务的处理数据进行了微调，并表明基于词源的条件音译产生了最佳结果，超过了对原始马耳他语或使用非选择性管道处理的马耳他语的微调。</li>
</ul>

<h3>Title: Energy-conserving equivariant GNN for elasticity of lattice architected  metamaterials</h3>
<ul>
<li><strong>Authors: </strong>Ivan Grega, Ilyes Batatia, Gábor Csányi, Sri Karlapati, Vikram S. Deshpande</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16914">https://arxiv.org/abs/2401.16914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16914">https://arxiv.org/pdf/2401.16914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16914]] Energy-conserving equivariant GNN for elasticity of lattice architected  metamaterials(https://arxiv.org/abs/2401.16914)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modelling. In this work we present a higher-order GNN model trained to predict the fourth-order stiffness tensor of periodic strut-based lattices. The key features of the model are (i) SE(3) equivariance, and (ii) consistency with the thermodynamic law of conservation of energy. We compare the model to non-equivariant models based on a number of error metrics and demonstrate the benefits of the encoded equivariance and energy conservation in terms of predictive performance and reduced training requirements.</li>
<li><strong>摘要：</strong>晶格是一种超材料，其特性很大程度上取决于其几何设计。与有限元建模等传统方法相比，晶格和图之间的类比使得图神经网络 (GNN) 能够用作更快的代理模型。在这项工作中，我们提出了一个经过训练的高阶 GNN 模型，用于预测基于周期支柱的晶格的四阶刚度张量。该模型的主要特征是 (i) SE(3) 等方差，以及 (ii) 与热力学能量守恒定律的一致性。我们根据许多误差指标将该模型与非等变模型进行比较，并证明编码等变性和节能在预测性能和减少训练要求方面的好处。</li>
</ul>

<h3>Title: Online Resource Allocation with Non-Stationary Customers</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyue Zhang, Hanzhang Qin, Mabel C. Chou</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16945">https://arxiv.org/abs/2401.16945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16945">https://arxiv.org/pdf/2401.16945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16945]] Online Resource Allocation with Non-Stationary Customers(https://arxiv.org/abs/2401.16945)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We propose a novel algorithm for online resource allocation with non-stationary customer arrivals and unknown click-through rates. We assume multiple types of customers arrive in a nonstationary stochastic fashion, with unknown arrival rates in each period, and that customers' click-through rates are unknown and can only be learned online. By leveraging results from the stochastic contextual bandit with knapsack and online matching with adversarial arrivals, we develop an online scheme to allocate the resources to nonstationary customers. We prove that under mild conditions, our scheme achieves a ``best-of-both-world'' result: the scheme has a sublinear regret when the customer arrivals are near-stationary, and enjoys an optimal competitive ratio under general (non-stationary) customer arrival distributions. Finally, we conduct extensive numerical experiments to show our approach generates near-optimal revenues for all different customer scenarios.</li>
<li><strong>摘要：</strong>我们提出了一种新颖的算法，用于在非固定客户到达和未知点击率的情况下进行在线资源分配。我们假设多种类型的客户以非平稳随机方式到达，每个时期的到达率未知，并且客户的点击率未知，只能在线了解。通过利用背包随机上下文强盗的结果以及与对抗性到达的在线匹配，我们开发了一种在线方案，将资源分配给非固定客户。我们证明，在温和的条件下，我们的方案达到了“两全其美”的结果：当顾客到达接近平稳时，该方案具有亚线性遗憾，并且在一般（非非）条件下享有最佳竞争比。固定）顾客到达分布。最后，我们进行了广泛的数值实验，以证明我们的方法可以为所有不同的客户场景带来接近最佳的收入。</li>
</ul>

<h3>Title: Two Heads Are Better Than One: Integrating Knowledge from Knowledge  Graphs and Large Language Models for Entity Alignment</h3>
<ul>
<li><strong>Authors: </strong>Linyao Yang, Hongyang Chen, Xiao Wang, Jing Yang, Fei-Yue Wang, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16960">https://arxiv.org/abs/2401.16960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16960">https://arxiv.org/pdf/2401.16960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16960]] Two Heads Are Better Than One: Integrating Knowledge from Knowledge  Graphs and Large Language Models for Entity Alignment(https://arxiv.org/abs/2401.16960)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Entity alignment, which is a prerequisite for creating a more comprehensive Knowledge Graph (KG), involves pinpointing equivalent entities across disparate KGs. Contemporary methods for entity alignment have predominantly utilized knowledge embedding models to procure entity embeddings that encapsulate various similarities-structural, relational, and attributive. These embeddings are then integrated through attention-based information fusion mechanisms. Despite this progress, effectively harnessing multifaceted information remains challenging due to inherent heterogeneity. Moreover, while Large Language Models (LLMs) have exhibited exceptional performance across diverse downstream tasks by implicitly capturing entity semantics, this implicit knowledge has yet to be exploited for entity alignment. In this study, we propose a Large Language Model-enhanced Entity Alignment framework (LLMEA), integrating structural knowledge from KGs with semantic knowledge from LLMs to enhance entity alignment. Specifically, LLMEA identifies candidate alignments for a given entity by considering both embedding similarities between entities across KGs and edit distances to a virtual equivalent entity. It then engages an LLM iteratively, posing multiple multi-choice questions to draw upon the LLM's inference capability. The final prediction of the equivalent entity is derived from the LLM's output. Experiments conducted on three public datasets reveal that LLMEA surpasses leading baseline models. Additional ablation studies underscore the efficacy of our proposed framework.</li>
<li><strong>摘要：</strong>实体对齐是创建更全面的知识图 (KG) 的先决条件，涉及在不同的知识图谱中精确定位等效实体。当代的实体对齐方法主要利用知识嵌入模型来获取封装各种相似性（结构、关系和属性）的实体嵌入。然后通过基于注意力的信息融合机制整合这些嵌入。尽管取得了这些进展，但由于固有的异质性，有效利用多方面的信息仍然具有挑战性。此外，虽然大型语言模型（LLM）通过隐式捕获实体语义在不同的下游任务中表现出了卓越的性能，但这种隐式知识尚未用于实体对齐。在这项研究中，我们提出了一个大型语言模型增强的实体对齐框架（LLMEA），将知识图谱的结构知识与法学硕士的语义知识相结合，以增强实体对齐。具体来说，LLMEA 通过考虑跨 KG 的实体之间的嵌入相似性以及到虚拟等效实体的编辑距离来识别给定实体的候选对齐。然后，它反复与法学硕士互动，提出多个多项选择题，以利用法学硕士的推理能力。等效实体的最终预测源自法学硕士的输出。在三个公共数据集上进行的实验表明，LLMEA 超越了领先的基线模型。其他消融研究强调了我们提出的框架的有效性。</li>
</ul>

<h3>Title: CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement  Learning</h3>
<ul>
<li><strong>Authors: </strong>Andreas W.M. Sauter, Nicolò Botteghi, Erman Acar, Aske Plaat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.16974">https://arxiv.org/abs/2401.16974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.16974">https://arxiv.org/pdf/2401.16974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.16974]] CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement  Learning(https://arxiv.org/abs/2401.16974)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Causal discovery is the challenging task of inferring causal structure from data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research. Reinforcement learning provides a convenient framework for such an active approach to learning. This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning. CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions. Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures. Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency. All relevant code and supplementary material can be found at https://github.com/sa-and/CORE</li>
<li><strong>摘要：</strong>因果发现是从数据推断因果结构的一项具有挑战性的任务。珀尔的因果层次结构（PCH）告诉我们，仅被动观察不足以区分相关性和因果关系，在它的推动下，最近有人在推动将干预措施纳入机器学习研究中。强化学习为这种主动学习方法提供了一个方便的框架。本文提出了 CORE，一种基于深度强化学习的方法，用于因果发现和干预规划。 CORE 学习从数据中顺序重建因果图，同时学习执行信息干预。我们的结果表明，CORE 可以推广到看不见的图并有效地揭示因果结构。此外，CORE 可扩展到包含多达 10 个变量的更大图表，并且在结构估计准确性和样本效率方面优于现有方法。所有相关代码和补充材料可以在 https://github.com/sa-and/CORE 找到</li>
</ul>

<h3>Title: Evaluation of Out-of-Distribution Detection Performance on Autonomous  Driving Datasets</h3>
<ul>
<li><strong>Authors: </strong>Jens Henriksson, Christian Berger, Stig Ursing, Markus Borg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17013">https://arxiv.org/abs/2401.17013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17013">https://arxiv.org/pdf/2401.17013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17013]] Evaluation of Out-of-Distribution Detection Performance on Autonomous  Driving Datasets(https://arxiv.org/abs/2401.17013)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications. Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples. This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score. The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets. The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception.</li>
<li><strong>摘要：</strong>需要系统地研究安全措施，以评估关键应用的深度神经网络 (DNN) 的预期性能。由于缺乏高维 DNN 的验证方法，需要在可接受的性能和分布外 (OOD) 样本的处理之间进行权衡。这项工作通过应用基于预测类的最可能类条件高斯分布的马哈拉诺比斯距离 (MD) 作为 OOD 分数来评估语义分割 DNN 的拒绝输出。该评估遵循在 Cityscapes 数据集上训练的三个 DNN 并在四个汽车数据集上进行测试，发现即使应用于看不见的数据集，也可以以像素覆盖为代价大幅降低分类风险。我们的研究结果的适用性将支持安全措施的合法化，并在争论汽车感知中安全使用 DNN 时激励其使用。</li>
</ul>

<h3>Title: Robust Kernel Sparse Subspace Clustering</h3>
<ul>
<li><strong>Authors: </strong>Ivica Kopriva</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17035">https://arxiv.org/abs/2401.17035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17035">https://arxiv.org/pdf/2401.17035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17035]] Robust Kernel Sparse Subspace Clustering(https://arxiv.org/abs/2401.17035)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Kernel methods are applied to many problems in pattern recognition, including subspace clustering (SC). That way, nonlinear problems in the input data space become linear in mapped high-dimensional feature space. Thereby, computationally tractable nonlinear algorithms are enabled through implicit mapping by the virtue of kernel trick. However, kernelization of linear algorithms is possible only if square of the Froebenious norm of the error term is used in related optimization problem. That, however, implies normal distribution of the error. That is not appropriate for non-Gaussian errors such as gross sparse corruptions that are modeled by -norm. Herein, to the best of our knowledge, we propose for the first time robust kernel sparse SC (RKSSC) algorithm for data with gross sparse corruptions. The concept, in principle, can be applied to other SC algorithms to achieve robustness to the presence of such type of corruption. We validated proposed approach on two well-known datasets with linear robust SSC algorithm as a baseline model. According to Wilcoxon test, clustering performance obtained by the RKSSC algorithm is statistically significantly better than corresponding performance obtained by the robust SSC algorithm. MATLAB code of proposed RKSSC algorithm is posted on https://github.com/ikopriva/RKSSC.</li>
<li><strong>摘要：</strong>核方法应用于模式识别中的许多问题，包括子空间聚类（SC）。这样，输入数据空间中的非线性问题在映射的高维特征空间中变成线性问题。因此，借助核技巧，通过隐式映射可以实现计算上易于处理的非线性算法。然而，只有在相关优化问题中使用误差项的Froebenious范数的平方时，线性算法的核化才有可能。然而，这意味着误差呈正态分布。这不适用于非高斯误差，例如由 -norm 建模的严重稀疏损坏。在此，据我们所知，我们首次提出了针对严重稀疏损坏数据的鲁棒内核稀疏SC（RKSSC）算法。原则上，这个概念可以应用于其他 SC 算法，以实现对此类损坏的鲁棒性。我们以线性稳健的 SSC 算法作为基线模型，在两个著名的数据集上验证了所提出的方法。根据 Wilcoxon 测试，RKSSC 算法获得的聚类性能在统计上显着优于鲁棒 SSC 算法获得的相应性能。所提出的 RKSSC 算法的 MATLAB 代码发布在 https://github.com/ikopriva/RKSSC 上。</li>
</ul>

<h3>Title: Bayesian Optimization with Noise-Free Observations: Improved Regret  Bounds via Random Exploration</h3>
<ul>
<li><strong>Authors: </strong>Hwanwoo Kim, Daniel Sanz-Alonso</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17037">https://arxiv.org/abs/2401.17037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17037">https://arxiv.org/pdf/2401.17037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17037]] Bayesian Optimization with Noise-Free Observations: Improved Regret  Bounds via Random Exploration(https://arxiv.org/abs/2401.17037)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>This paper studies Bayesian optimization with noise-free observations. We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate. Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem. Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples.</li>
<li><strong>摘要：</strong>本文研究了无噪声观测的贝叶斯优化。我们引入了植根于分散数据近似的新算法，该算法依赖于随机探索步骤来确保查询点的填充距离以接近最佳的速率衰减。我们的算法保留了经典 GP-UCB 算法的易于实现性，并满足与 arXiv:2002.05096 中推测的几乎匹配的累积遗憾边界，从而解决了 COLT 开放问题。此外，新算法在几个例子中优于 GP-UCB 和其他流行的贝叶斯优化策略。</li>
</ul>

<h3>Title: Forecasting VIX using Bayesian Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Héctor J. Hortúa, Andrés Mora-Valencia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17042">https://arxiv.org/abs/2401.17042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17042">https://arxiv.org/pdf/2401.17042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17042]] Forecasting VIX using Bayesian Deep Learning(https://arxiv.org/abs/2401.17042)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recently, deep learning techniques are gradually replacing traditional statistical and machine learning models as the first choice for price forecasting tasks. In this paper, we leverage probabilistic deep learning for inferring the volatility index VIX. We employ the probabilistic counterpart of WaveNet, Temporal Convolutional Network (TCN), and Transformers. We show that TCN outperforms all models with an RMSE around 0.189. In addition, it has been well known that modern neural networks provide inaccurate uncertainty estimates. For solving this problem, we use the standard deviation scaling to calibrate the networks. Furthermore, we found out that MNF with Gaussian prior outperforms Reparameterization Trick and Flipout models in terms of precision and uncertainty predictions. Finally, we claim that MNF with Cauchy and LogUniform prior distributions yield well calibrated TCN and WaveNet networks being the former that best infer the VIX values.</li>
<li><strong>摘要：</strong>近年来，深度学习技术逐渐取代传统的统计和机器学习模型，成为价格预测任务的首选。在本文中，我们利用概率深度学习来推断波动率指数 VIX。我们采用了 WaveNet、时间卷积网络 (TCN) 和 Transformers 的概率对应部分。我们表明 TCN 优于所有模型，RMSE 约为 0.189。此外，众所周知，现代神经网络提供的不确定性估计不准确。为了解决这个问题，我们使用标准偏差缩放来校准网络。此外，我们发现具有高斯先验的 MNF 在精度和不确定性预测方面优于重新参数化 Trick 和 Flipout 模型。最后，我们声称具有柯西和 LogUniform 先验分布的 MNF 产生经过良好校准的 TCN 和 WaveNet 网络，前者最能推断 VIX 值。</li>
</ul>

<h3>Title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented  Generation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17043">https://arxiv.org/abs/2401.17043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17043">https://arxiv.org/pdf/2401.17043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17043]] CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented  Generation of Large Language Models(https://arxiv.org/abs/2401.17043)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the range of RAG applications into four distinct types-Create, Read, Update, and Delete (CRUD), each representing a unique use case. "Create" refers to scenarios requiring the generation of original, varied content. "Read" involves responding to intricate questions in knowledge-intensive situations. "Update" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. "Delete" pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed comprehensive datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, the context length, the knowledge base construction, and the LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 是一种通过整合外部知识源来增强大型语言模型 (LLM) 功能的技术。该方法解决了法学硕士的常见限制，包括过时的信息和产生不准确的“幻觉”内容的倾向。然而，RAG 系统的评估具有挑战性，因为现有基准的范围和多样性有限。当前大多数基准测试主要评估问答应用程序，而忽略了 RAG 可能具有优势的更广泛的情况。而且，他们在实验中仅评估RAG管道的LLM组件的性能，而忽略了检索组件和外部知识数据库的影响。为了解决这些问题，本文构建了一个大规模、更全面的基准，并在各种 RAG 应用场景中评估 RAG 系统的所有组件。具体来说，我们将 RAG 应用程序范围分为四种不同的类型 - 创建、读取、更新和删除 (CRUD)，每种类型代表一个独特的用例。 “创造”是指需要生成原创的、多样化的内容的场景。 “阅读”涉及在知识密集的情况下回答复杂的问题。 “更新”侧重于修改和纠正现有文本中的不准确或不一致之处。 “删除”涉及将大量文本总结为更简洁形式的任务。对于每个 CRUD 类别，我们开发了全面的数据集来评估 RAG 系统的性能。我们还分析了 RAG 系统各个组件的影响，例如检索器、上下文长度、知识库构建和法学硕士。最后，我们提供了针对不同场景优化 RAG 技术的有用见解。</li>
</ul>

<h3>Title: Scalable Mechanism Design for Multi-Agent Path Finding</h3>
<ul>
<li><strong>Authors: </strong>Paul Friedrich, Yulun Zhang, Michael Curry, Ludwig Dierks, Stephen McAleer, Jiaoyang Li, Tuomas Sandholm, Sven Seuken</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.GT, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17044">https://arxiv.org/abs/2401.17044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17044">https://arxiv.org/pdf/2401.17044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17044]] Scalable Mechanism Design for Multi-Agent Path Finding(https://arxiv.org/abs/2401.17044)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Multi-Agent Path Finding (MAPF) involves determining paths for multiple agents to travel simultaneously through a shared area toward particular goal locations. This problem is computationally complex, especially when dealing with large numbers of agents, as is common in realistic applications like autonomous vehicle coordination. Finding an optimal solution is often computationally infeasible, making the use of approximate algorithms essential. Adding to the complexity, agents might act in a self-interested and strategic way, possibly misrepresenting their goals to the MAPF algorithm if it benefits them. Although the field of mechanism design offers tools to align incentives, using these tools without careful consideration can fail when only having access to approximately optimal outcomes. Since approximations are crucial for scalable MAPF algorithms, this poses a significant challenge. In this work, we introduce the problem of scalable mechanism design for MAPF and propose three strategyproof mechanisms, two of which even use approximate MAPF algorithms. We test our mechanisms on realistic MAPF domains with problem sizes ranging from dozens to hundreds of agents. Our findings indicate that they improve welfare beyond a simple baseline.</li>
<li><strong>摘要：</strong>多智能体路径查找 (MAPF) 涉及确定多个智能体同时穿过共享区域到达特定目标位置的路径。这个问题在计算上很复杂，特别是在处理大量代理时，这在自动车辆协调等实际应用中很常见。寻找最佳解决方案通常在计算上是不可行的，因此必须使用近似算法。增加复杂性的是，代理可能会以自利和战略的方式行事，如果对他们有利的话，可能会向 MAPF 算法歪曲他们的目标。尽管机制设计领域提供了调整激励措施的工具，但如果不仔细考虑就使用这些工具，当只能获得近似最佳的结果时，可能会失败。由于近似对于可扩展 MAPF 算法至关重要，因此这构成了重大挑战。在这项工作中，我们介绍了 MAPF 的可扩展机制设计问题，并提出了三种策略证明机制，其中两种甚至使用近似 MAPF 算法。我们在现实的 MAPF 域上测试我们的机制，问题规模从数十个到数百个智能体不等。我们的研究结果表明，它们改善的福利超出了简单的基线。</li>
</ul>

<h3>Title: SemScore: Automated Evaluation of Instruction-Tuned LLMs based on  Semantic Textual Similarity</h3>
<ul>
<li><strong>Authors: </strong>Ansar Aynetdinov, Alan Akbik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17072">https://arxiv.org/abs/2401.17072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17072">https://arxiv.org/pdf/2401.17072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17072]] SemScore: Automated Evaluation of Instruction-Tuned LLMs based on  Semantic Textual Similarity(https://arxiv.org/abs/2401.17072)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for the evaluation of instruction-tuned LLMs.</li>
<li><strong>摘要：</strong>指令调整的大型语言模型 (LLM) 最近在生成对自然语言指令的合适响应的能力方面取得了显着的进步。然而，当前的许多工作依赖于手动评估来判断生成的响应的质量。由于这种手动评估非常耗时，因此它不容易扩展到多个模型和模型变体的评估。在这篇简短的论文中，我们提出了一种简单但非常有效的评估指标，称为 SemScore，其中我们使用语义文本相似性 (STS) 直接将模型输出与黄金目标响应进行比较。我们使用 8 个广泛使用的文本生成评估指标，对 12 个著名的指令调整法学硕士的模型输出进行了比较评估。我们发现，就与人类评估的相关性而言，我们提出的 SemScore 指标优于所有其他（在许多情况下更复杂）评估指标。这些发现表明我们提出的指标对于评估指令调整的法学硕士的效用。</li>
</ul>

<h3>Title: NNOSE: Nearest Neighbor Occupational Skill Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mike Zhang, Rob van der Goot, Min-Yen Kan, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17092">https://arxiv.org/abs/2401.17092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17092">https://arxiv.org/pdf/2401.17092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17092]] NNOSE: Nearest Neighbor Occupational Skill Extraction(https://arxiv.org/abs/2401.17092)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, rag</a></li>
<li><strong>Abstract: </strong>The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text. With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well. We tackle the complexity in occupational skill datasets tasks -- combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets. In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner. Our proposed method, \textbf{N}earest \textbf{N}eighbor \textbf{O}ccupational \textbf{S}kill \textbf{E}xtraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore. This improves skill extraction \emph{without} additional fine-tuning. Crucially, we observe a performance gain in predicting infrequent patterns, with substantial gains of up to 30\% span-F1 in cross-dataset settings.</li>
<li><strong>摘要：</strong>劳动力市场正在迅速变化，促使人们对从文本中自动提取职业技能的兴趣日益浓厚。随着英语基准职位描述数据集的出现，需要能够很好地处理其多样性的系统。我们解决职业技能数据集任务的复杂性——组合和利用多个数据集进行技能提取，识别数据集中很少观察到的技能，并克服跨数据集技能的稀缺性。特别是，我们研究了语言模型的检索增强，使用外部数据存储以数据集统一的方式检索类似的技能。我们提出的方法 \textbf{N}earest \textbf{N}eighbor \textbf{O}ccupational \textbf{S}kill \textbf{E}xtraction (NNOSE) 通过从其他数据集中检索相邻技能来有效地利用多个数据集数据存储。这提高了技能提取\emph{无需}额外的微调。至关重要的是，我们观察到预测不常见模式的性能提升，在跨数据集设置中 Span-F1 的大幅提升高达 30%。</li>
</ul>

<h3>Title: Traffic estimation in unobserved network locations using data-driven  macroscopic models</h3>
<ul>
<li><strong>Authors: </strong>Pablo Guarda, Sean Qian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17095">https://arxiv.org/abs/2401.17095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17095">https://arxiv.org/pdf/2401.17095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17095]] Traffic estimation in unobserved network locations using data-driven  macroscopic models(https://arxiv.org/abs/2401.17095)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable. This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts. The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities. Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable. The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time. Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect to the model parameters. This property facilitates the application of computational graphs to learn parameters from vast amounts of spatiotemporal data. We also integrate neural networks and polynomial kernel functions to capture link flow interactions and enrich the mapping of traffic flows into travel times. MaTE also adds a destination choice model and a trip generation model that uses historical data on the number of trips generated by location. Experiments on synthetic data show that the model can accurately estimate travel time and traffic flow in out-of-sample links. Results obtained using real-world multi-source data from a large-scale transportation network suggest that MaTE outperforms data-driven benchmarks, especially in travel time estimation. The estimated parameters of MaTE are also informative about the hourly change in travel demand and supply characteristics of the transportation network.</li>
<li><strong>摘要：</strong>本文利用宏观模型和从自动交通计数器和探测车辆收集的多源时空数据来准确估计无法获得这些测量数据的路段的交通流量和旅行时间。这个问题在交通规划应用中至关重要，因为传感器覆盖范围较低，并且计划的干预措施会对网络范围产生影响。所提出的模型被称为宏观交通估计器（MaTE），可以仅使用这些量的一组观察到的测量值来执行网络范围内的交通流和行程时间估计。由于 MaTE 以宏观流动理论为基础，因此所有参数和变量都是可解释的。估计的交通流量满足基本的流量守恒约束，并且与估计的行程时间呈现出递增的单调关系。使用基于 logit 的随机流量分配作为路由流行为的原则使得模型在模型参数方面完全可微。这一特性促进了计算图的应用，以从大量时空数据中学习参数。我们还集成神经网络和多项式核函数来捕获链路流交互并丰富交通流到旅行时间的映射。 MaTE 还添加了目的地选择模型和行程生成模型，该模型使用有关位置生成的行程数量的历史数据。综合数据实验表明，该模型可以准确估计样本外路段的出行时间和交通流量。使用来自大型交通网络的真实多源数据获得的结果表明，MaTE 的性能优于数据驱动的基准，尤其是在行程时间估计方面。 MaTE 的估计参数还提供了有关交通网络的出行需求和供给特征的每小时变化的信息。</li>
</ul>

<h3>Title: Explainable data-driven modeling via mixture of experts: towards  effective blending of grey and black-box models</h3>
<ul>
<li><strong>Authors: </strong>Jessica Leoni, Valentina Breschi, Simone Formentin, Mara Tanelli</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17118">https://arxiv.org/abs/2401.17118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17118">https://arxiv.org/pdf/2401.17118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17118]] Explainable data-driven modeling via mixture of experts: towards  effective blending of grey and black-box models(https://arxiv.org/abs/2401.17118)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Traditional models grounded in first principles often struggle with accuracy as the system's complexity increases. Conversely, machine learning approaches, while powerful, face challenges in interpretability and in handling physical constraints. Efforts to combine these models often often stumble upon difficulties in finding a balance between accuracy and complexity. To address these issues, we propose a comprehensive framework based on a "mixture of experts" rationale. This approach enables the data-based fusion of diverse local models, leveraging the full potential of first-principle-based priors. Our solution allows independent training of experts, drawing on techniques from both machine learning and system identification, and it supports both collaborative and competitive learning paradigms. To enhance interpretability, we penalize abrupt variations in the expert's combination. Experimental results validate the effectiveness of our approach in producing an interpretable combination of models closely resembling the target phenomena.</li>
<li><strong>摘要：</strong>随着系统复杂性的增加，基于第一性原理的传统模型常常难以保证准确性。相反，机器学习方法虽然强大，但在可解释性和处理物理约束方面面临挑战。组合这些模型的努力常常会遇到在准确性和复杂性之间找到平衡的困难。为了解决这些问题，我们提出了一个基于“专家组合”原理的综合框架。这种方法能够实现不同本地模型的基于数据的融合，充分利用基于第一原理的先验的潜力。我们的解决方案允许对专家进行独立培训，利用机器学习和系统识别技术，并且支持协作和竞争学习范例。为了增强可解释性，我们对专家组合中的突然变化进行惩罚。实验结果验证了我们的方法在生成与目标现象非常相似的可解释模型组合方面的有效性。</li>
</ul>

<h3>Title: Large Language Model Evaluation via Matrix Entropy</h3>
<ul>
<li><strong>Authors: </strong>Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, Weiran Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17139">https://arxiv.org/abs/2401.17139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17139">https://arxiv.org/pdf/2401.17139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17139]] Large Language Model Evaluation via Matrix Entropy(https://arxiv.org/abs/2401.17139)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains. Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs. In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs. It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability. Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings. For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law. For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing alignment quality and we find that modern large multi-modal models exhibit great alignment performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了自然语言处理领域，将其强大的功能扩展到多模态领域。因此，为LLM的评估制定适当且多样化的指标至关重要。在本文中，我们介绍了矩阵熵，这是一种植根于信息论和几何原理的新颖度量，用于量化法学硕士的数据压缩能力。它反映了模型提取相关信息并消除不必要元素的能力，从而提供对语言模型内在能力的洞察。具体来说，我们展示了它在单模式（语言）和多模式设置中的适用性。对于语言模型，我们的研究结果表明，当模型按比例放大时，表示的矩阵熵遵循比例律类型减少，作为传统损失比例律的补充。对于多模态设置，我们还提出了一种基于矩阵熵的评估方法来评估对齐质量，我们发现现代大型多模态模型表现出出色的对齐性能。</li>
</ul>

<h3>Title: Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zhengyang Lu, Stefan Siemer, Piyush Jha, Joel Day, Florin Manea, Vijay Ganesh</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LO, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17159">https://arxiv.org/abs/2401.17159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17159">https://arxiv.org/pdf/2401.17159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17159]] Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis(https://arxiv.org/abs/2401.17159)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Modern SMT solvers, such as Z3, offer user-controllable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike. In this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decision-making process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-of-the-art (SOTA) SMT solvers. We implement our method, dubbed Z3alpha, as part of the Z3 SMT solver. Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT, the default Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging QF_BV benchmark set, Z3alpha solves 42.7% more instances than the default strategy in the Z3 SMT solver.</li>
<li><strong>摘要：</strong>现代 SMT 求解器（例如 Z3）提供用户可控的策略，使用户能够根据自己独特的实例集进行定制，从而显着提高其用例的求解器性能。然而，这种策略定制方法提出了一个重大挑战：对于求解器开发人员和用户来说，为一类 SMT 实例手工制定优化策略仍然是一项复杂且艰巨的任务。在本文中，我们通过一种新颖的基于蒙特卡罗树搜索（MCTS）的方法解决了自动 SMT 策略合成的问题。我们的方法将策略综合视为一个顺序决策过程，其搜索树对应于策略空间，并采用 MCTS 来导航这个巨大的搜索空间。使我们的方法能够识别有效策略并同时保持低成本的关键创新是分层和分阶段 MCTS 搜索的思想。这些新颖的方法允许对策略空间进行更深入、更有效的探索，使我们能够合成比最先进 (SOTA) SMT 求解器中的默认策略更有效的策略。我们实现了我们的方法，称为 Z3alpha，作为 Z3 SMT 求解器的一部分。通过对 6 个重要 SMT 逻辑的广泛评估，Z3alpha 在大多数基准测试中表现出比 SOTA 综合工具 FastSMT、默认 Z3 求解器和 CVC5 求解器更优越的性能。值得注意的是，在具有挑战性的 QF_BV 基准集上，Z3alpha 比 Z3 SMT 求解器中的默认策略多解决了 42.7% 的实例。</li>
</ul>

<h3>Title: Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool  Utilization in Real-World Complex Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, Qun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17167">https://arxiv.org/abs/2401.17167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17167">https://arxiv.org/pdf/2401.17167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17167]] Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool  Utilization in Real-World Complex Scenarios(https://arxiv.org/abs/2401.17167)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The recent trend of using Large Language Models (LLMs) as intelligent agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by mapping out the intermediate steps. Thus, unlike previous work, it eliminates the restriction of pre-defined toolset during planning. Through extensive experiments on various LLMs, we offer novel insights into the evaluation of capabilities of LLMs in tool utilization, thereby contributing a fresh perspective to this rapidly evolving field. The benchmark is publicly available at https://github.com/JoeYing1019/UltraTool.</li>
<li><strong>摘要：</strong>最近在现实应用中使用大型语言模型（LLM）作为智能代理的趋势强调了对其能力进行全面评估的必要性，特别是在涉及规划、创建和使用工具的复杂场景中。然而，现有的基准测试通常侧重于简单的综合查询，不能反映现实世界的复杂性，从而在评估工具利用率方面提供的视角有限。为了解决这个问题，我们推出了 UltraTool，这是一种新颖的基准测试，旨在提高和评估法学硕士在现实场景中使用工具的能力。 UltraTool 专注于使用工具的整个过程 - 从规划和创建到在复杂任务中应用它们。它强调现实世界的复杂性，要​​求准确、多步骤的规划才能有效解决问题。 UltraTool 的一个关键功能是它使用自然语言对规划进行独立评估，该评估发生在工具使用之前，并通过绘制中间步骤来简化任务解决。因此，与以前的工作不同，它消除了规划过程中预定义工具集的限制。通过对各种法学硕士的广泛实验，我们对法学硕士在工具利用方面的能力评估提供了新颖的见解，从而为这个快速发展的领域提供了新的视角。该基准测试可在 https://github.com/JoeYing1019/UltraTool 上公开获取。</li>
</ul>

<h3>Title: Conditional and Modal Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wesley H. Holliday, Matthew Mandelkern</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17169">https://arxiv.org/abs/2401.17169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17169">https://arxiv.org/pdf/2401.17169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17169]] Conditional and Modal Reasoning in Large Language Models(https://arxiv.org/abs/2401.17169)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的推理能力是人工智能和认知科学领域越来越多研究的主题。在本文中，我们探讨了十几个法学硕士能够区分逻辑正确的推论和逻辑错误的推论的程度。我们专注于涉及条件句的推理模式（例如，“如果安有皇后，那么鲍勃有杰克”）和认知情态（例如，“安可能有一张王牌”，“鲍勃必须有一个国王”）。这些推理模式引起了逻辑学家、哲学家和语言学家的特别兴趣，因为它们似乎在人类推理中发挥着核心作用。因此，根据这些推理模式评估法学硕士与法学硕士的推理能力与人类推理能力的匹配程度高度相关。在我们测试的法学硕士中，除了 GPT-4 之外的所有法学硕士都经常在条件句方面犯一些基本错误。此外，即使 GPT-4 在涉及认知模态的推理模式中也显示出逻辑上不一致的判断。</li>
</ul>

<h3>Title: Zero-Shot Reinforcement Learning via Function Encoders</h3>
<ul>
<li><strong>Authors: </strong>Tyler Ingebrand, Amy Zhang, Ufuk Topcu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17173">https://arxiv.org/abs/2401.17173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17173">https://arxiv.org/pdf/2401.17173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17173]] Zero-Shot Reinforcement Learning via Function Encoders(https://arxiv.org/abs/2401.17173)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encoder task representation.</li>
<li><strong>摘要：</strong>尽管强化学习（RL）可以解决许多具有挑战性的顺序决策问题，但实现跨相关任务的零样本迁移仍然是一个挑战。困难在于找到当前任务的良好表示，以便代理了解它与之前看到的任务之间的关系。为了实现零样本迁移，我们引入了函数编码器，这是一种表示学习算法，它将函数表示为学习的非线性基函数的加权组合。通过使用函数编码器来表示奖励函数或转换函数，代理可以通过连贯的向量表示来了解当前任务如何与先前看到的任务相关联。因此，代理能够在运行时实现相关任务之间的转移，而无需额外的训练。通过使用函数编码器任务表示增强基本 RL 算法，我们在三个 RL 领域展示了最先进的数据效率、渐近性能和训练稳定性。</li>
</ul>

<h3>Title: Transfer Learning for Text Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Fiedel, Noah Constant</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17181">https://arxiv.org/abs/2401.17181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17181">https://arxiv.org/pdf/2401.17181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17181]] Transfer Learning for Text Diffusion Models(https://arxiv.org/abs/2401.17181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>In this report, we explore the potential for text diffusion to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs). We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff''. We begin by establishing a strong baseline setup for training text diffusion models. Comparing across multiple architectures and pretraining objectives, we find that training a decoder-only model with a prefix LM objective is best or near-best across several tasks. Building on this finding, we test various transfer learning setups for text diffusion models. On machine translation, we find that text diffusion underperforms the standard AR approach. However, on code synthesis and extractive QA, we find diffusion models trained from scratch outperform AR models in many cases. We also observe quality gains from AR2Diff -- adapting AR models to use diffusion decoding. These results are promising given that text diffusion is relatively underexplored and can be significantly faster than AR decoding for long text generation.</li>
<li><strong>摘要：</strong>在本报告中，我们探讨了文本扩散取代自回归 (AR) 解码以训练和部署大型语言模型 (LLM) 的潜力。我们特别感兴趣的是，是否可以通过我们称为“AR2Diff”的轻量级适应过程将预训练的 AR 模型转换为文本扩散模型。我们首先为训练文本传播模型建立强大的基线设置。比较多种架构和预训练目标，我们发现使用前缀 LM 目标训练仅解码器模型在多个任务中是最佳或接近最佳的。基于这一发现，我们测试了文本传播模型的各种迁移学习设置。在机器翻译方面，我们发现文本扩散的表现不如标准 AR 方法。然而，在代码合成和提取 QA 方面，我们发现从头开始训练的扩散模型在许多情况下优于 AR 模型。我们还观察到 AR2Diff 的质量提升——调整 AR 模型以使用扩散解码。鉴于文本扩散的研究相对较少，而且对于长文本生成来说，文本扩散的速度明显快于 AR 解码，因此这些结果很有希望。</li>
</ul>

<h3>Title: NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble  Techniques</h3>
<ul>
<li><strong>Authors: </strong>Weronika Hryniewska-Guzik, Bartosz Sawicki, Przemysław Biecek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17200">https://arxiv.org/abs/2401.17200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17200">https://arxiv.org/pdf/2401.17200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17200]] NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble  Techniques(https://arxiv.org/abs/2401.17200)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive comparative analysis of explainable artificial intelligence (XAI) ensembling methods. Our research brings three significant contributions. Firstly, we introduce a novel ensembling method, NormEnsembleXAI, that leverages minimum, maximum, and average functions in conjunction with normalization techniques to enhance interpretability. Secondly, we offer insights into the strengths and weaknesses of XAI ensemble methods. Lastly, we provide a library, facilitating the practical implementation of XAI ensembling, thus promoting the adoption of transparent and interpretable deep learning models.</li>
<li><strong>摘要：</strong>本文对可解释人工智能（XAI）集成方法进行了全面的比较分析。我们的研究带来了三项重大贡献。首先，我们介绍了一种新颖的集成方法 NormEnsembleXAI，该方法利用最小、最大和平均函数以及归一化技术来增强可解释性。其次，我们深入了解 XAI 集成方法的优点和缺点。最后，我们提供了一个库，促进 XAI 集成的实际实施，从而促进透明和可解释的深度学习模型的采用。</li>
</ul>

<h3>Title: Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding  Space using Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Jeongwoo Park, Enrico Liscio, Pradeep K. Murukannaiah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17228">https://arxiv.org/abs/2401.17228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17228">https://arxiv.org/pdf/2401.17228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17228]] Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding  Space using Contrastive Learning(https://arxiv.org/abs/2401.17228)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms. However, existing works often treat morality as binary, ranging from right to wrong. This simplistic view does not capture the nuances of moral judgment. Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment. In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach. We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively. Our results show that a pluralist approach to morality can be captured in an embedding space. However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.</li>
<li><strong>摘要：</strong>NLP 的最新进展表明，语言模型在义务论伦理和道德规范方面保留了可辨别的知识水平。然而，现有的作品往往将道德视为二元的，从正确到错误。这种简单化的观点并没有抓住道德判断的细微差别。多元主义道德哲学家认为，人类道德可以被解构为有限数量的元素，尊重道德判断中的个体差异。根据这一观点，我们通过最先进的对比学习方法构建了一个多元化的道德句子嵌入空间。我们通过定量和定性研究道德要素之间关系的出现来系统地研究嵌入空间。我们的结果表明，可以在嵌入空间中捕捉到道德的多元化方法。然而，道德多元化很难仅通过自我监督来推断，需要采用带有人类标签的监督方法。</li>
</ul>

<h3>Title: LLaMP: Large Language Model Made Powerful for High-fidelity Materials  Knowledge Retrieval and Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yuan Chiang, Chia-Hong Chou, Janosh Riebesell</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17244">https://arxiv.org/abs/2401.17244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17244">https://arxiv.org/pdf/2401.17244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17244]] LLaMP: Large Language Model Made Powerful for High-fidelity Materials  Knowledge Retrieval and Distillation(https://arxiv.org/abs/2401.17244)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, code, retrieval-augmented generation, rag, agent</a></li>
<li><strong>Abstract: </strong>Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from mixed data sources. Additionally, LLaMP substantially reduces the hallucinated volumetric strain in a diamond cubic silicon structure from 66.3% to 0. The proposed framework offers an intuitive and nearly hallucination-free approach to exploring materials informatics and establishes a pathway for knowledge distillation and fine-tuning other language models. We envision the framework as a valuable component for scientific hypotheses and a foundation for future autonomous laboratories where multiple LLM agents communicate and cooperate with robotics to drive material synthesis and chemical reactions without hard-coded human logic and intervention.</li>
<li><strong>摘要：</strong>减少大型语言模型（LLM）的幻觉对于在可重复性至关重要的科学中的使用至关重要。然而，法学硕士本质上缺乏长期记忆，这使得根据特定领域的文献和数据对其进行微调成为一项不平凡的、临时的、不可避免地存在偏见的任务。在这里，我们介绍 LLaMP，这是一种多模式检索增强生成 (RAG) 框架，由多个数据感知推理与行动 (ReAct) 代理组成，可与材料项目 (MP) 上的计算和实验数据动态交互。无需微调，LLaMP 就能展现出理解和集成材料科学概念的各种模式、动态获取相关数据存储、处理高阶数据（例如晶体结构和弹性张量）以及总结多步骤过程的能力。固态合成。我们表明，LLaMP 有效地纠正了 GPT-3.5 内在知识中的错误，将频繁记录的带隙的 MAPE 降低了 5.21%，并将形成能的 MAPE 显着降低了 1103.54%——GPT-3.5 似乎源自混合数据源的错误。此外，LLaMP 将金刚石立方硅结构中的幻觉体积应变从 66.3% 大幅降低至 0。所提出的框架提供了一种直观且几乎无幻觉的方法来探索材料信息学，并建立了知识蒸馏和微调其他语言的途径楷模。我们设想该框架是科学假设的重要组成部分，也是未来自主实验室的基础，在该实验室中，多个法学硕士代理与机器人进行通信和合作，以驱动材料合成和化学反应，而无需硬编码的人类逻辑和干预。</li>
</ul>

<h3>Title: Weak-to-Strong Jailbreaking on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17256">https://arxiv.org/abs/2401.17256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17256">https://arxiv.org/pdf/2401.17256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17256]] Weak-to-Strong Jailbreaking on Large Language Models(https://arxiv.org/abs/2401.17256)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding. Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations. This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs. The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations. Our study reveals a previously unnoticed yet efficient way of jailbreaking, exposing an urgent safety issue that needs to be considered when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong</li>
<li><strong>摘要：</strong>尽管在调整大型语言模型 (LLM) 方面付出了巨大的努力，但红队报告表明，这些精心调整的 LLM 仍然可以通过对抗性提示、调整或解码来越狱。在检查对齐法学硕士的越狱漏洞后，我们观察到越狱模型和对齐模型的解码分布仅在最初几代中有所不同。这一观察促使我们提出从弱到强的越狱攻击，其中对手可以利用较小的不安全/对齐的 LLM（例如 7B）来指导针对更大的对齐 LLM（例如 70B）的越狱。要越狱，只需额外解码两个较小的 LLM 一次，与解码较大的 LLM 相比，这涉及最少的计算和延迟。通过对来自三个不同组织的五个模型进行的实验证明了这种攻击的有效性。我们的研究揭示了一种以前未被注意到但有效的越狱方法，暴露了在调整法学硕士时需要考虑的紧急安全问题。作为初步尝试，我们提出了一种防御策略来防御此类攻击，但创建更先进的防御仍然具有挑战性。复制该方法的代码位于 https://github.com/XuandongZhao/weak-to-strong</li>
</ul>

<h3>Title: Robust Prompt Optimization for Defending Language Models Against  Jailbreaking Attacks</h3>
<ul>
<li><strong>Authors: </strong>Andy Zhou, Bo Li, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17263">https://arxiv.org/abs/2401.17263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17263">https://arxiv.org/pdf/2401.17263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17263]] Robust Prompt Optimization for Defending Language Models Against  Jailbreaking Attacks(https://arxiv.org/abs/2401.17263)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success rate of the strongest attack on GPT-4 from 92% to 6%.</li>
<li><strong>摘要：</strong>尽管人工智能对齐取得了进步，但语言模型（LM）仍然容易受到对抗性攻击或越狱，其中对手会修改输入提示以引发有害行为。虽然已经提出了一些防御措施，但它们侧重于狭隘的威胁模型，缺乏强大的防御能力，我们认为这种防御措施应该是有效的、普遍的和实用的。为了实现这一目标，我们提出了第一个对抗性目标来保护 LM 免受越狱攻击，并提出了一种算法，即鲁棒提示优化（RPO），该算法使用基于梯度的令牌优化来强制执行无害的输出。这产生了一个易于访问的后缀，显着提高了对优化过程中出现的越狱和未知的、持续越狱的鲁棒性，将 Starling-7B 在 20 次越狱中的攻击成功率从 84% 降低到 8.66%。此外，我们发现RPO对正常LM使用影响较小，在自适应攻击下成功，并且可以转移到黑盒模型，将GPT-4最强攻击的成功率从92%降低到6%。</li>
</ul>

<h3>Title: ReacLLaMA: Merging chemical and textual information in chemical  reactivity AI models</h3>
<ul>
<li><strong>Authors: </strong>Aline Hartgers, Ramil Nugmanov, Kostiantyn Chernichenko, Joerg Kurt Wegner</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17267">https://arxiv.org/abs/2401.17267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17267">https://arxiv.org/pdf/2401.17267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17267]] ReacLLaMA: Merging chemical and textual information in chemical  reactivity AI models(https://arxiv.org/abs/2401.17267)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, agent</a></li>
<li><strong>Abstract: </strong>Chemical reactivity models are developed to predict chemical reaction outcomes in the form of classification (success/failure) or regression (product yield) tasks. The vast majority of the reported models are trained solely on chemical information such as reactants, products, reagents, and solvents, but not on the details of a synthetic protocol. Herein incorporation of procedural text with the aim to augment the Graphormer reactivity model and improve its accuracy is presented. Two major approaches are used: training an adapter Graphormer model that is provided with a GPT-2-derived latent representation of the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a dataset with the LLaMA 2 model followed by training the Graphormer on an extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the discernment of unpromising reactions, thereby providing more accurate models with improved specificity.</li>
<li><strong>摘要：</strong>开发化学反应模型是为了以分类（成功/失败）或回归（产物产量）任务的形式预测化学反应结果。绝大多数报告的模型仅针对化学信息（例如反应物、产物、试剂和溶剂）进行训练，而不是针对合成方案的细节进行训练。本文提出了程序文本的合并，旨在增强 Graphomer 反应性模型并提高其准确性。使用两种主要方法：训练适配器 Graphomer 模型，该模型提供 GPT-2 派生的文本过程的潜在表示 (ReacLLaMA-Adapter)，并使用 LLaMA 2 模型标记数据集的未标记部分，然后训练 Graphomer在扩展数据集上（零样本标记 ReacLLaMA）。这两种方法都增强了对无希望反应的识别能力，从而提供了具有更高特异性的更准确的模型。</li>
</ul>

<h3>Title: Weaver: Foundation Models for Creative Writing</h3>
<ul>
<li><strong>Authors: </strong>Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, Wangchunshu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17268">https://arxiv.org/abs/2401.17268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17268">https://arxiv.org/pdf/2401.17268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17268]] Weaver: Foundation Models for Creative Writing(https://arxiv.org/abs/2401.17268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval-augmented generation, rag, agent</a></li>
<li><strong>Abstract: </strong>This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.</li>
<li><strong>摘要：</strong>这项工作介绍了 Weaver，这是我们第一个致力于内容创建的大型语言模型 (LLM) 系列。 Weaver 在精心挑选的语料库上进行了预训练，该语料库专注于提高大型语言模型的写作能力。然后，我们针对创意和专业写作目的对 Weaver 进行微调，并使用一套新颖的指令数据合成和 LLM 对齐方法使其符合专业作家的偏好，使其能够生成更接近人类的文本并遵循更多样化的指令用于内容创作。 Weaver系列包括Weaver Mini（1.8B）、Weaver Base（6B）、Weaver Pro（14B）和Weaver Ultra（34B）尺寸型号，适合不同的应用，并且可以由路由代理根据查询动态调度平衡响应质量和计算成本的复杂性。对精心策划的用于评估法学硕士写作能力的基准进行的评估表明，各种规模的韦弗模型的表现都比其规模大几倍的通才法学硕士要好几倍。值得注意的是，我们最强大的 Weaver Ultra 模型在各种写作场景上都超越了最先进的通才 LLM GPT-4，展示了为写作目的训练专业 LLM 的优势。此外，Weaver 本身支持检索增强生成（RAG）和函数调用（工具使用）。我们展示了这些能力的各种用例，用于改进人工智能辅助写作系统，包括集成外部知识库、工具或 API，以及提供个性化写作帮助。此外，我们讨论并总结了预训练和微调特定领域法学硕士的指南和最佳实践。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
