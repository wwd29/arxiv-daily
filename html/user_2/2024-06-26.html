<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-06-26</h1>
<h3>Title: Title:
          TextAge: A Curated and Diverse Text Dataset for Age Classification</h3>
<ul>
<li><strong>Authors: </strong>Shravan Cheekati, Mridul Gupta, Vibha Raghu, Pranav Raj</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          TextAge: A Curated and Diverse Text Dataset for Age Classification(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Age-related language patterns play a crucial role in understanding linguistic differences and developing age-appropriate communication strategies. However, the lack of comprehensive and diverse datasets has hindered the progress of research in this area. To address this issue, we present TextAge, a curated text dataset that maps sentences to the age and age group of the producer, as well as an underage (under 13) label. TextAge covers a wide range of ages and includes both spoken and written data from various sources such as CHILDES, Meta, Poki Poems-by-kids, JUSThink, and the TV show "Survivor." The dataset undergoes extensive cleaning and preprocessing to ensure data quality and consistency. We demonstrate the utility of TextAge through two applications: Underage Detection and Generational Classification. For Underage Detection, we train a Naive Bayes classifier, fine-tuned RoBERTa, and XLNet models to differentiate between language patterns of minors and young-adults and over. For Generational Classification, the models classify language patterns into different age groups (kids, teens, twenties, etc.). The models excel at classifying the "kids" group but struggle with older age groups, particularly "fifties," "sixties," and "seventies," likely due to limited data samples and less pronounced linguistic differences. TextAge offers a valuable resource for studying age-related language patterns and developing age-sensitive language models. The dataset's diverse composition and the promising results of the classification tasks highlight its potential for various applications, such as content moderation, targeted advertising, and age-appropriate communication. Future work aims to expand the dataset further and explore advanced modeling techniques to improve performance on older age groups.</li>
<li><strong>摘要：</strong>与年龄相关的语言模式在理解语言差异和制定适合年龄的沟通策略方面起着至关重要的作用。然而，缺乏全面而多样化的数据集阻碍了该领域的研究进展。为了解决这个问题，我们推出了 TextAge，这是一个精选的文本数据集，它将句子映射到制作者的年龄和年龄组，以及未成年人（13 岁以下）标签。TextAge 涵盖广泛的年龄范围，包括来自各种来源的口头和书面数据，例如 CHILDES、Meta、Poki Poems-by-kids、JUSThink 和电视节目“幸存者”。该数据集经过大量清理和预处理，以确保数据质量和一致性。我们通过两个应用展示了 TextAge 的实用性：未成年人检测和世代分类。对于未成年人检测，我们训练了一个朴素贝叶斯分类器、微调的 RoBERTa 和 XLNet 模型，以区分未成年人和青年成年人及以上的语言模式。对于代际分类，模型将语言模式分为不同的年龄组（儿童、青少年、二十几岁等）。这些模型擅长对“儿童”组进行分类，但对年龄较大的群体，尤其是“五十多岁”、“六十多岁”和“七十多岁”的群体，则表现不佳，这可能是由于数据样本有限且语言差异不太明显。TextAge 为研究与年龄相关的语言模式和开发年龄敏感的语言模型提供了宝贵的资源。数据集的多样化组成和分类任务的良好结果凸显了其在各种应用中的潜力，例如内容审核、定向广告和适合年龄的沟通。未来的工作旨在进一步扩展数据集并探索先进的建模技术，以提高对年龄较大群体的表现。</li>
</ul>

<h3>Title: Title:
          Survey on Reasoning Capabilities and Accessibility of Large Language Models Using Biology-related Questions</h3>
<ul>
<li><strong>Authors: </strong>Michael Ackerman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Survey on Reasoning Capabilities and Accessibility of Large Language Models Using Biology-related Questions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This research paper discusses the advances made in the past decade in biomedicine and Large Language Models. To understand how the advances have been made hand-in-hand with one another, the paper also discusses the integration of Natural Language Processing techniques and tools into biomedicine. Finally, the goal of this paper is to expand on a survey conducted last year (2023) by introducing a new list of questions and prompts for the top two language models. Through this survey, this paper seeks to quantify the improvement made in the reasoning abilities in LLMs and to what extent those improvements are felt by the average user. Additionally, this paper seeks to extend research on retrieval of biological literature by prompting the LLM to answer open-ended questions in great depth.</li>
<li><strong>摘要：</strong>本研究论文讨论了过去十年生物医学和大型语言模型的进展。为了了解这些进展是如何齐头并进的，本文还讨论了自然语言处理技术和工具与生物医学的整合。最后，本文的目标是通过引入针对前两种语言模型的新问题和提示列表来扩展去年（2023 年）进行的一项调查。通过这项调查，本文试图量化 LLM 推理能力的提高以及普通用户在多大程度上感受到这些改进。此外，本文还试图通过促使 LLM 深入回答开放式问题来扩展对生物文献检索的研究。</li>
</ul>

<h3>Title: Title:
          InstructPatentGPT: Training patent language models to follow instructions with human feedback</h3>
<ul>
<li><strong>Authors: </strong>Jieh-Sheng Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          InstructPatentGPT: Training patent language models to follow instructions with human feedback(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In this research, patent prosecution is conceptualized as a system of reinforcement learning from human feedback. The objective of the system is to increase the likelihood for a language model to generate patent claims that have a higher chance of being granted. To showcase the controllability of the language model, the system learns from granted patents and pre-grant applications with different rewards. The status of "granted" and "pre-grant" are perceived as labeled human feedback implicitly. In addition, specific to patent drafting, the experiments in this research demonstrate the model's capability to learn from adjusting claim length and inclusion of limiting terms for narrowing claim scope. As proof of concept, the experiments focus on claim ones only and the training data originates from a patent dataset tailored specifically for artificial intelligence. Although the available human feedback in patent prosecution are limited and the quality of generated patent text requires improvement, the experiments following the 3-stage reinforcement learning from human feedback have demonstrated that generative language models are capable of reflecting the human feedback or intent in patent prosecution. To enhance the usability of language models, the implementation in this research utilizes modern techniques that enable execution on a single consumer-grade GPU. The demonstrated proof of concept, which reduces hardware requirements, will prove valuable in the future as more human feedback in patent prosecution become available for broader use, either within patent offices or in the public domain.</li>
<li><strong>摘要：</strong>在本研究中，专利诉讼被概念化为一种从人工反馈中进行强化学习的系统。该系统的目标是增加语言模型生成更有可能被授予的专利权利要求的可能性。为了展示语言模型的可控性，该系统从已授予的专利和授予前申请中学习，并给予不同的奖励。“已授予”和“授予前”的状态被视为隐含的人工反馈。此外，针对专利撰写，本研究中的实验证明了该模型能够通过调整权利要求长度和包含限制性术语来缩小权利要求范围。作为概念验证，实验仅关注权利要求，训练数据来自专门为人工智能量身定制的专利数据集。尽管专利诉讼中可用的人工反馈有限，生成的专利文本的质量需要改进，但遵循从人工反馈进行 3 阶段强化学习的实验表明，生成语言模型能够反映专利诉讼中的人工反馈或意图。为了提高语言模型的可用性，本研究的实施采用了现代技术，可在单个消费级 GPU 上执行。经过验证的概念验证降低了硬件要求，随着专利审查中越来越多的人工反馈在专利局或公共领域得到更广泛使用，这一概念在未来将大有裨益。</li>
</ul>

<h3>Title: Title:
          Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification</h3>
<ul>
<li><strong>Authors: </strong>Yuni Susanti, Nina Holsmoelle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>This work aims toward an application of natural language processing (NLP) technology for automatic verification of causal graphs using text sources. A causal graph is often derived from unsupervised causal discovery methods and requires manual evaluation from human experts. NLP technologies, i.e., Large Language Models (LLMs) such as BERT and ChatGPT, can potentially be used to verify the resulted causal graph by predicting if causal relation can be observed between node pairs based on the textual context. In this work, we compare the performance of two types of NLP models: (1) Pre-trained language models fine-tuned for causal relation classification task and, (2) prompt-based LLMs. Contrasted to previous studies where prompt-based LLMs work relatively well over a set of diverse tasks, preliminary experiments on biomedical and open-domain datasets suggest that the fine-tuned models far outperform the prompt-based LLMs, up to 20.5 points improvement of F1 score. We shared the code and the pre-processed datasets in our repository.</li>
<li><strong>摘要：</strong>这项工作旨在应用自然语言处理 (NLP) 技术使用文本源自动验证因果图。因果图通常来自无监督的因果发现方法，需要人类专家的手动评估。NLP 技术，即大型语言模型 (LLM)，例如 BERT 和 ChatGPT，可用于通过基于文本上下文预测是否可以在节点对之间观察到因果关系来验证生成的因果图。在这项工作中，我们比较了两种类型的 NLP 模型的性能：（1）针对因果关系分类任务进行了微调的预训练语言模型；（2）基于提示的 LLM。与先前研究中基于提示的 LLM 在一系列不同任务上表现相对较好相比，在生物医学和开放域数据集上的初步实验表明，微调模型的表现远远优于基于提示的 LLM，F1 分数提高了 20.5 分。我们在我们的存储库中共享了代码和预处理的数据集。</li>
</ul>

<h3>Title: Title:
          A Complete Survey on LLM-based AI Chatbots</h3>
<ul>
<li><strong>Authors: </strong>Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, Chaoning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Complete Survey on LLM-based AI Chatbots(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>The past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.</li>
<li><strong>摘要：</strong>过去几十年见证了数据量的激增，为数据密集型、基于学习的 AI 技术奠定了基础。对话代理（通常称为 AI 聊天机器人）严重依赖此类数据来训练大型语言模型 (LLM) 并根据用户提示生成新内容（知识）。随着 OpenAI 的 ChatGPT 的出现，基于 LLM 的聊天机器人为 AI 社区树立了新标准。本文全面介绍了基于 LLM 的聊天机器人在各个领域的发展和部署。我们首先总结了基础聊天机器人的发展，然后总结了 LLM 的发展，然后概述了目前正在使用的基于 LLM 的聊天机器人和处于开发阶段的聊天机器人。我们将 AI 聊天机器人视为生成新知识的工具，探索它们在各个行业的各种应用。然后，我们讨论了尚未解决的挑战，并考虑了用于训练 LLM 的数据以及对生成的知识的滥用如何导致一些问题。最后，我们探讨了未来前景，以增强它们在众多应用中的效率和可靠性。通过解决关键里程碑和基于法学硕士的聊天机器人的当今背景，我们的调查邀请读者更深入地研究这个领域，思考他们的下一代将如何重塑对话式人工智能。</li>
</ul>

<h3>Title: Title:
          modeLing: A Novel Dataset for Testing Linguistic Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nathan A. Chi, Teodor Malchev, Riley Kong, Ryan A. Chi, Lucas Huang, Ethan A. Chi, R. Thomas McCoy, Dragomir Radev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          modeLing: A Novel Dataset for Testing Linguistic Reasoning in Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We introduce modeLing, a novel benchmark of Linguistics Olympiad-style puzzles which tests few-shot reasoning in AI systems. Solving these puzzles necessitates inferring aspects of a language's grammatical structure from a small number of examples. Such puzzles provide a natural testbed for language models, as they require compositional generalization and few-shot inductive reasoning. Consisting solely of new puzzles written specifically for this work, modeLing has no risk of appearing in the training data of existing AI systems: this ameliorates the risk of data leakage, a potential confounder for many prior evaluations of reasoning. Evaluating several large open source language models and GPT on our benchmark, we observe non-negligible accuracy, demonstrating few-shot emergent reasoning ability which cannot merely be attributed to shallow memorization. However, imperfect model performance suggests that modeLing can be used to measure further progress in linguistic reasoning.</li>
<li><strong>摘要：</strong>我们引入了 modelLing，这是语言学奥林匹克式谜题的新基准，用于测试 AI 系统中的少样本推理。解决这些谜题需要从少量示例中推断出语言语法结构的各个方面。此类谜题为语言模型提供了天然的试验台，因为它们需要组合泛化和少样本归纳推理。modelLing 仅由专门为这项工作编写的新谜题组成，因此不存在出现在现有 AI 系统的训练数据中的风险：这降低了数据泄露的风险，而数据泄露是许多先前推理评估的潜在混杂因素。在我们的基准上评估了几个大型开源语言模型和 GPT，我们观察到不可忽略的准确性，展示了少样本的突发推理能力，而这不能仅仅归因于浅层记忆。然而，不完美的模型性能表明 modelLing 可用于衡量语言推理的进一步进展。</li>
</ul>

<h3>Title: Title:
          Large Language Models Assume People are More Rational than We Really are</h3>
<ul>
<li><strong>Authors: </strong>Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models Assume People are More Rational than We Really are(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate -- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice -- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.</li>
<li><strong>摘要：</strong>为了让人工智能系统能够有效地与人类沟通，它们必须了解我们如何做出决策。然而，人们的决策并不总是理性的，因此大型语言模型 (LLM) 中人类决策的隐式内部模型必须考虑到这一点。先前的经验证据似乎表明这些隐式模型是准确的——LLM 提供了人类行为的可信代理，其行为方式与我们预期的人类在日常互动中的行为方式相同。然而，通过将 LLM 行为和预测与大量人类决策数据集进行比较，我们发现事实并非如此：在模拟和预测人们的选择时，一套前沿的 LLM（GPT-4o 和 4-Turbo、Llama-3-8B 和 70B、Claude 3 Opus）假设人们比我们实际更理性。具体来说，这些模型偏离了人类行为，更接近理性选择的经典模型——期望值理论。有趣的是，人们在解释他人的行为时也倾向于假设其他人是理性的。因此，当我们使用另一个心理数据集比较法学硕士和人们从他人决策中得出的推论时，我们发现这些推论高度相关。因此，法学硕士的隐性决策模型似乎与人类对其他人会理性行事的期望相一致，而不是与人们的实际行为相一致。</li>
</ul>

<h3>Title: Title:
          Attention Instruction: Amplifying Attention in the Middle via Prompting</h3>
<ul>
<li><strong>Authors: </strong>Meiru Zhang, Zaiqiao Meng, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Attention Instruction: Amplifying Attention in the Middle via Prompting(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The context window of large language models has been extended to 128k tokens or more. However, language models still suffer from position bias and have difficulty in accessing and using the middle part of the context due to the lack of attention. We examine the relative position awareness of LLMs and the feasibility of mitigating disproportional attention through prompting. We augment the original task instruction with $\texttt{attention instructions}$ that direct language models to allocate more attention towards a selected segment of the context. We conduct a comprehensive investigation on multi-document question answering task with both position-based and index-based instructions. We find that language models do not have relative position awareness of the context. Nevertheless, they demonstrate the capacity to adapt attention to a specific segment using matching indexes. Our analysis contributes to a deeper understanding of position bias in LLMs and provides a pathway to mitigate this bias by instruction, thus benefiting LLMs in locating and utilizing relevant information from retrieved documents in RAG applications.</li>
<li><strong>摘要：</strong>大型语言模型的上下文窗口已扩展到 128k 个 token 或更多。然而，由于缺乏注意力，语言模型仍然受到位置偏差的影响，并且难以访问和使用上下文的中间部分。我们研究了 LLM 的相对位置意识以及通过提示缓解不成比例的注意力的可行性。我们在原始任务指令中添加了 $\texttt{注意力指令}$，指示语言模型将更多注意力分配给选定的上下文片段。我们对基于位置和基于索引的指令的多文档问答任务进行了全面调查。我们发现语言模型不具有上下文的相对位置意识。尽管如此，它们还是展示了使用匹配索引将注意力调整到特定片段的能力。我们的分析有助于更深入地理解 LLM 中的位置偏差，并提供了一种通过指令缓解这种偏差的途径，从而使 LLM 能够在 RAG 应用程序中定位和利用从检索到的文档中获取的相关信息。</li>
</ul>

<h3>Title: Title:
          Automated Adversarial Discovery for Safety Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Yash Kumar Lal, Preethi Lahoti, Aradhana Sinha, Yao Qin, Ananth Balashankar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Automated Adversarial Discovery for Safety Classifiers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Safety classifiers are critical in mitigating toxicity on online forums such as social media and in chatbots. Still, they continue to be vulnerable to emergent, and often innumerable, adversarial attacks. Traditional automated adversarial data generation methods, however, tend to produce attacks that are not diverse, but variations of previously observed harm types. We formalize the task of automated adversarial discovery for safety classifiers - to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier. We measure progress on this task along two key axes (1) adversarial success: does the attack fool the classifier? and (2) dimensional diversity: does the attack represent a previously unseen harm type? Our evaluation of existing attack generation methods on the CivilComments toxicity task reveals their limitations: Word perturbation attacks fail to fool classifiers, while prompt-based LLM attacks have more adversarial success, but lack dimensional diversity. Even our best-performing prompt-based method finds new successful attacks on unseen harm dimensions of attacks only 5\% of the time. Automatically finding new harmful dimensions of attack is crucial and there is substantial headroom for future research on our new task.</li>
<li><strong>摘要：</strong>安全分类器对于减轻社交媒体和聊天机器人等在线论坛上的恶意行为至关重要。尽管如此，它们仍然容易受到不断涌现的、往往是无数的对抗性攻击。然而，传统的自动对抗性数据生成方法往往会产生一些攻击，这些攻击并不是多样化的，而是以前观察到的伤害类型的变体。我们将安全分类器的自动对抗性发现任务形式化——沿着以前看不见的伤害维度寻找新的攻击，从而暴露分类器中的新弱点。我们沿着两个关键轴来衡量这项任务的进展 (1) 对抗性成功：攻击是否欺骗了分类器？和 (2) 维度多样性：攻击是否代表了以前看不见的伤害类型？我们对 CivilComments 恶意性任务上现有攻击生成方法的评估揭示了它们的局限性：词扰动攻击无法欺骗分类器，而基于提示的 LLM 攻击具有更多的对抗性成功，但缺乏维度多样性。即使我们表现最佳的基于提示的方法也只有 5% 的时间能够发现针对未知危害维度的新成功攻击。自动发现新的攻击危害维度至关重要，我们的新任务在未来的研究方面还有很大的发展空间。</li>
</ul>

<h3>Title: Title:
          DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Venktesh V. Deepali Prabhu, Avishek Anand</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While retrieval performance for classical QA tasks is well explored, their capabilities for heterogeneous complex retrieval tasks, especially in an open-domain setting, and the impact on downstream QA performance, are relatively unexplored. To address this, in this work, we propose a benchmark composing diverse complex QA tasks and provide a toolkit to evaluate state-of-the-art pre-trained dense and sparse retrieval models in an open-domain setting. We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre-trained dense retrieval models. In addition, since context-based reasoning is critical for solving complex QA tasks, we also evaluate the reasoning capabilities of LLMs and the impact of retrieval performance on their reasoning capabilities. Through experiments, we observe that much progress is to be made in retrieval for complex QA to improve downstream QA performance. Our software and related data can be accessed at this https URL</li>
<li><strong>摘要：</strong>开放域复杂问答 (QA) 是一项艰巨的任务，在证据检索和推理方面面临挑战。此类问题的复杂性可能源于问题的组合性、混合证据或问题的歧义性。虽然经典 QA 任务的检索性能已得到充分探索，但它们在异构复杂检索任务（尤其是在开放域环境中）中的能力以及对下游 QA 性能的影响相对尚未得到探索。为了解决这个问题，在这项工作中，我们提出了一个由各种复杂 QA 任务组成的基准，并提供了一个工具包来评估开放域环境中最先进的预训练密集和稀疏检索模型。我们观察到，与其他预训练的密集检索模型相比，后期交互模型和令人惊讶的词汇模型（如 BM25）表现良好。此外，由于基于上下文的推理对于解决复杂的 QA 任务至关重要，我们还评估了 LLM 的推理能力以及检索性能对其推理能力的影响。通过实验，我们发现在复杂 QA 的检索方面还有很大的进步空间，可以提高下游 QA 的性能。我们的软件和相关数据可以通过此 https URL 访问</li>
</ul>

<h3>Title: Title:
          Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors</h3>
<ul>
<li><strong>Authors: </strong>Vikas Yadav, Zheng Tang, Vijay Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) have achieved remarkable success in natural language generation but lesser focus has been given to their applicability in decision making tasks such as classification. We show that LLMs like LLaMa can achieve high performance on large multi-class classification tasks but still make classification errors and worse, generate out-of-vocabulary class labels. To address these critical issues, we introduce Paraphrase and AGgregate (PAG)-LLM approach wherein an LLM generates multiple paraphrases of the input query (parallel queries), performs multi-class classification for the original query and each paraphrase, and at the end aggregate all the classification labels based on their confidence scores. We evaluate PAG-LLM on two large multi-class classication datasets: CLINC, and Banking and show 22.7% and 15.1% error reduction. We show that PAG-LLM is especially effective for hard examples where LLM is uncertain, and reduces the critical misclassification and hallucinated label generation errors</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言生成方面取得了显著的成功，但人们对其在分类等决策任务中的适用性关注较少。我们表明，像 LLaMa 这样的 LLM 可以在大型多类分类任务上实现高性能，但仍然会产生分类错误，更糟糕的是，会生成超出词汇量的类标签。为了解决这些关键问题，我们引入了释义和聚合 (PAG)-LLM 方法，其中 LLM 生成输入查询的多个释义（并行查询），对原始查询和每个释义执行多类分类，最后根据它们的置信度分数聚合所有分类标签。我们在两个大型多类分类数据集上评估了 PAG-LLM：CLINC 和 Banking，结果显示错误减少了 22.7% 和 15.1%。我们表明 PAG-LLM 对于 LLM 不确定的困难示例特别有效，并减少了关键的错误分类和幻觉标签生成错误</li>
</ul>

<h3>Title: Title:
          Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) continue to exhibit remarkable performance in natural language understanding tasks, there is a crucial need to measure their ability for human-like multi-step logical reasoning. Existing logical reasoning evaluation benchmarks often focus primarily on simplistic single-step or multi-step reasoning with a limited set of inference rules. Furthermore, the lack of datasets for evaluating non-monotonic reasoning represents a crucial gap since it aligns more closely with human-like reasoning. To address these limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset encompassing multi-step logical reasoning with various inference rules and depths. Multi-LogiEval covers three logic types--propositional, first-order, and non-monotonic--consisting of more than 30 inference rules and more than 60 of their combinations with various depths. Leveraging this dataset, we conduct evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca, and Mistral, employing a zero-shot chain-of-thought. Experimental results show that there is a significant drop in the performance of LLMs as the reasoning steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5). We further conduct a thorough investigation of reasoning chains generated by LLMs which reveals several important findings. We believe that Multi-LogiEval facilitates future research for evaluating and enhancing the logical reasoning ability of LLMs. Data is available at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在自然语言理解任务中继续展现出卓越的性能，迫切需要衡量它们进行类似人类的多步逻辑推理的能力。现有的逻辑推理评估基准通常主要关注具有有限推理规则的简单单步或多步推理。此外，缺乏用于评估非单调推理的数据集代表着一个关键的差距，因为它与类似人类的推理更接近。为了解决这些限制，我们提出了 Multi-LogiEval，这是一个全面的评估数据集，涵盖具有各种推理规则和深度的多步逻辑推理。Multi-LogiEval 涵盖三种逻辑类型——命题、一阶和非单调——由 30 多个推理规则和 60 多个具有不同深度的组合组成。利用此数据集，我们采用零样本思维链对一系列 LLM 进行了评估，包括 GPT-4、ChatGPT、Gemini-Pro、Yi、Orca 和 Mistral。实验结果表明，随着推理步骤/深度的增加，LLM 的性能显著下降（深度 1 时的平均准确率约为 68%，深度 5 时的平均准确率约为 43%）。我们进一步对 LLM 生成的推理链进行了彻底调查，发现了几个重要发现。我们相信 Multi-LogiEval 有助于未来评估和增强 LLM 逻辑推理能力的研究。数据可在此 https URL 上获取。</li>
</ul>

<h3>Title: Title:
          CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented Analysis Generation</h3>
<ul>
<li><strong>Authors: </strong>Abe Bohan Hou, Orion Weller, Guanghui Qin, Eugene Yang, Dawn Lawrie, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented Analysis Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Legal professionals need to write analyses that rely on citations to relevant precedents, i.e., previous case decisions. Intelligent systems assisting legal professionals in writing such documents provide great benefits but are challenging to design. Such systems need to help locate, summarize, and reason over salient precedents in order to be useful. To enable systems for such tasks, we work with legal professionals to transform a large open-source legal corpus into a dataset supporting two important backbone tasks: information retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC (Case Law Evaluation Retrieval Corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. We benchmark state-of-the-art models on CLERC, showing that current approaches still struggle: GPT-4o generates analyses with the highest ROUGE F-scores but hallucinates the most, while zero-shot IR models only achieve 48.3% recall@1000.</li>
<li><strong>摘要：</strong>法律专业人士需要撰写依赖相关先例（即先前的案例判决）引证的分析。协助法律专业人士撰写此类文件的智能系统提供了巨大的好处，但设计​​起来却具有挑战性。这样的系统需要帮助查找、总结和推理突出的先例才能发挥作用。为了让系统能够完成此类任务，我们与法律专业人士合作，将一个大型开源法律语料库转换成一个支持两个重要骨干任务的数据集：信息检索 (IR) 和检索增强生成 (RAG)。这个数据集 CLERC（判例法评估检索语料库）是为训练和评估模型而构建的，以评估它们 (1) 为给定的法律分析找到相应引证的能力，以及 (2) 将这些引证的文本（以及先前的背景）编译成支持推理目标的令人信服的分析。我们对 CLERC 上最先进的模型进行了基准测试，结果表明当前的方法仍然存在问题：GPT-4o 生成的分析具有最高的 ROUGE F 分数，但产生了最多的幻觉，而零样本 IR 模型在 1000 时的召回率仅为 48.3%。</li>
</ul>

<h3>Title: Title:
          CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available.</li>
<li><strong>摘要：</strong>尽管大型语言模型容易产生幻觉和事实不准确的内容，但它们已成为问答应用程序不可或缺的一部分。查询知识图谱以减少 LLM 中的幻觉面临着知识图谱中知识覆盖不完整的挑战。另一方面，通过信息提取和知识图谱补全来更新知识图谱面临着知识更新错位问题。在这项工作中，我们引入了一个协作增强框架 CogMG，利用知识图谱来解决 LLM 在 QA 场景中的局限性，明确针对知识覆盖不完整和知识更新错位的问题。LLM 识别和分解 KG 中不存在的所需知识三元组，丰富它们并使更新与现实世界的需求保持一致。我们通过代理框架内的监督微调 LLM 证明了这种方法的有效性，显示出在减少幻觉和提高 QA 响应的事实准确性方面有显著的改善。我们的代码和视频是公开的。</li>
</ul>

<h3>Title: Title:
          Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks</h3>
<ul>
<li><strong>Authors: </strong>Yun-Shiuan Chuang, Zach Studdiford, Krirk Nirunwiroj, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 18 topics loading on two non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.</li>
<li><strong>摘要：</strong>创建类似人类的大型语言模型 (LLM) 代理对于忠实的社会模拟至关重要。让 LLM 根据人口统计信息进行角色扮演有时会提高人类相似性，但通常不会。这项研究评估了是否可以通过整合来自经验得出的人类信念网络的信息来改善 LLM 与人类行为的一致性。使用来自人类调查的数据，我们估计了一个信念网络，该网络包含 18 个主题，加载在两个不重叠的潜在因素上。然后，我们为基于 LLM 的代理播下对一个主题的意见，并评估其对剩余测试主题表达的意见与相应人类数据的一致性。仅基于人口统计信息的角色扮演并不能使 LLM 和人类意见保持一致，但为代理播下单一信念会大大改善信念网络中相关主题的一致性，而不是网络外主题的一致性。这些结果为试图模拟和理解社会信念分布模式的工作中人类-LLM 信念一致性提供了一种新途径。</li>
</ul>

<h3>Title: Title:
          What Do the Circuits Mean? A Knowledge Edit View</h3>
<ul>
<li><strong>Authors: </strong>Huaizhi Ge, Frank Rudzicz, Zining Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          What Do the Circuits Mean? A Knowledge Edit View(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In the field of language model interpretability, circuit discovery is gaining popularity. Despite this, the true meaning of these circuits remain largely unanswered. We introduce a novel method to learn their meanings as a holistic object through the lens of knowledge editing. We extract circuits in the GPT2-XL model using diverse text classification datasets, and use hierarchical relations datasets to explore knowledge editing in the circuits. Our findings indicate that these circuits contain entity knowledge but resist new knowledge more than complementary circuits during knowledge editing. Additionally, we examine the impact of circuit size, discovering that an ideal "theoretical circuit" where essential knowledge is concentrated likely incorporates more than 5% but less than 50% of the model's parameters. We also assess the overlap between circuits from different datasets, finding moderate similarities. What constitutes these circuits, then? We find that up to 60% of the circuits consist of layer normalization modules rather than attention or MLP modules, adding evidence to the ongoing debates regarding knowledge localization. In summary, our findings offer new insights into the functions of the circuits, and introduce research directions for further interpretability and safety research of language models.</li>
<li><strong>摘要：</strong>在语言模型可解释性领域，电路发现越来越受欢迎。尽管如此，这些电路的真正含义在很大程度上仍未得到解答。我们介绍了一种新方法，通过知识编辑的视角来学习它们作为整体对象的含义。我们使用不同的文本分类数据集提取 GPT2-XL 模型中的电路，并使用层次关系数据集探索电路中的知识编辑。我们的研究结果表明，这些电路包含实体知识，但在知识编辑过程中比互补电路更能抵抗新知识。此外，我们研究了电路大小的影响，发现一个理想的“理论电路”，其中集中了基本知识，可能包含超过 5% 但不到 50% 的模型参数。我们还评估了来自不同数据集的电路之间的重叠，发现中等相似性。那么，这些电路是由什么构成的呢？我们发现多达 60% 的电路由层规范化模块而不是注意力或 MLP 模块组成，这为正在进行的知识本地化争论提供了证据。总之，我们的发现为电路的功能提供了新的见解，并为语言模型的进一步可解释性和安全性研究提供了研究方向。</li>
</ul>

<h3>Title: Title:
          How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?</h3>
<ul>
<li><strong>Authors: </strong>Huaizhi Ge, Frank Rudzicz, Zining Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are widely deployed, targeted editing of their knowledge has become a critical challenge. Recently, advancements in model editing techniques, such as Rank-One Model Editing (ROME), have paved the way for updating LLMs with new knowledge. However, the efficacy of these methods varies across different types of knowledge. This study investigates the capability of knowledge editing methods to incorporate new knowledge with varying degrees of "perplexingness", a term we use to describe the initial difficulty LLMs have in understanding new concepts. We begin by quantifying the "perplexingness" of target knowledge using pre-edit conditional probabilities, and assess the efficacy of edits through post-edit conditional probabilities. Utilizing the widely-used CounterFact dataset, we find significant negative correlations between the "perplexingness" of the new knowledge and the edit efficacy across all 12 scenarios. To dive deeper into this phenomenon, we introduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym pairs across diverse categories. Our analysis reveal that more abstract concepts (hypernyms) tend to be more perplexing than their specific counterparts (hyponyms). Further exploration into the influence of knowledge hierarchy on editing outcomes indicates that knowledge positioned at higher hierarchical levels is more challenging to modify in some scenarios. Our research highlights a previously overlooked aspect of LLM editing: the variable efficacy of editing methods in handling perplexing knowledge. By revealing how hierarchical relationships can influence editing outcomes, our findings offer new insights into the challenges of updating LLMs and pave the way for more nuanced approaches to model editing in the future.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的广泛部署，有针对性地编辑其知识已成为一项关键挑战。最近，模型编辑技术（例如 Rank-One 模型编辑 (ROME)）的进步为使用新知识更新 LLM 铺平了道路。然而，这些方法的有效性因不同类型的知识而异。本研究调查了知识编辑方法整合不同程度“困惑度”的新知识的能力，我们用这个术语来描述 LLM 在理解新概念时遇到的初始困难。我们首先使用编辑前条件概率量化目标知识的“困惑度”，并通过编辑后条件概率评估编辑的有效性。利用广泛使用的 CounterFact 数据集，我们发现在所有 12 个场景中，新知识的“困惑度”与编辑有效性之间存在显著的负相关性。为了更深入地研究这一现象，我们引入了一个新数据集 HierarchyData，它由不同类别的 99 个上下义词对组成。我们的分析表明，更抽象的概念（上位词）往往比其具体对应物（下位词）更令人困惑。进一步探究知识层次对编辑结果的影响表明，在某些情况下，位于更高层次的知识更难修改。我们的研究突出了 LLM 编辑中以前被忽视的一个方面：编辑方法在处理令人困惑的知识方面的不同功效。通过揭示层次关系如何影响编辑结果，我们的研究结果为更新 LLM 的挑战提供了新的见解，并为未来更细致的模型编辑方法铺平了道路。</li>
</ul>

<h3>Title: Title:
          MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhenlong Dai, Chang Yao, WenKang Han, Ying Yuan, Zhipeng Gao, Jingyuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在协助开发人员日常开发方面表现出了巨大的潜力。然而，大多数研究都集中在生成正确的代码上，如何使用 LLM 生成个性化代码却很少被研究。为了弥补这一空白，我们提出了 MPCoder（多用户个性化代码生成器）来为多个用户生成个性化代码。为了更好地学习编码风格特征，我们利用显式编码风格残差学习来捕获语法代码风格标准，利用隐式风格学习来捕获语义代码风格约定。我们训练一个多用户风格适配器，通过对比学习更好地区分不同用户的隐式特征表示，最终实现为多个用户生成个性化代码。我们进一步提出了一种新的评估指标来估计不同编码风格的代码之间的相似性。实验结果证明了我们的方法对这项新任务的有效性。</li>
</ul>

<h3>Title: Title:
          Mitigating Hallucination in Fictional Character Role-Play</h3>
<ul>
<li><strong>Authors: </strong>Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mitigating Hallucination in Fictional Character Role-Play(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Role-playing has wide-ranging applications in customer support, embodied agents, computational social science, etc. The influence of parametric world knowledge of large language models (LLMs) often causes role-playing characters to act out of character and hallucinate about things outside the scope of their knowledge. In this work, we focus on the evaluation and mitigation of hallucination in fictional character role-play. We introduce a dataset with more than 2,000 characters and 72,000 interviews, including 18,000 adversarial questions. We propose RoleFact, a role-playing method that mitigates hallucination by modulating the influence of parametric knowledge using a pre-calibrated confidence threshold. Experiments show that the proposed method improves the factual precision of generated responses by 18% for adversarial questions with a 44% reduction in temporal hallucination for time-sensitive interviews. The code and the dataset will be available at this https URL.</li>
<li><strong>摘要：</strong>角色扮演在客户支持、具身代理、计算社会科学等领域有着广泛的应用。大型语言模型 (LLM) 的参数世界知识的影响常常导致角色扮演角色做出不符合角色性格的行为，并对超出其知识范围的事物产生幻觉。在这项工作中，我们专注于评估和缓解虚构角色角色扮演中的幻觉。我们引入了一个包含 2,000 多个角色和 72,000 次访谈的数据集，其中包括 18,000 个对抗性问题。我们提出了 RoleFact，这是一种角色扮演方法，它通过使用预校准的置信度阈值调节参数知识的影响来缓解幻觉。实验表明，对于对抗性问题，所提出的方法将生成响应的事实精度提高了 18%，对于时间敏感的访谈，时间幻觉减少了 44%。代码和数据集将在此 https URL 上提供。</li>
</ul>

<h3>Title: Title:
          TRAWL: Tensor Reduced and Approximated Weights for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiran Luo, Het Patel, Yu Fu, Dawon Ahn, Jia Chen, Yue Dong, Evangelos E. Papalexakis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          TRAWL: Tensor Reduced and Approximated Weights for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have fundamentally transformed artificial intelligence, catalyzing recent advancements while imposing substantial environmental and computational burdens. We introduce TRAWL (Tensor Reduced and Approximated Weights for Large Language Models), a novel methodology for optimizing LLMs through tensor decomposition. TRAWL leverages diverse strategies to exploit matrices within transformer-based architectures, realizing notable performance enhancements without necessitating retraining. The most significant improvements were observed through a layer-by-layer intervention strategy, particularly when applied to fully connected weights of the final layers, yielding up to 16% enhancement in accuracy without the need for additional data or fine-tuning. These results underscore the importance of targeted and adaptive techniques in increasing the efficiency and effectiveness of large language model optimization, thereby promoting the development of more sustainable and accessible AI systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 从根本上改变了人工智能，催化了最近的进步，同时也带来了巨大的环境和计算负担。我们引入了 TRAWL（大型语言模型的张量减少和近似权重），这是一种通过张量分解优化 LLM 的新方法。TRAWL 利用多种策略来利用基于转换器的架构中的矩阵，无需重新训练即可实现显着的性能提升。通过逐层干预策略观察到最显着的改进，特别是当应用于最终层的完全连接权重时，无需额外数据或微调即可将准确度提高 16%。这些结果强调了有针对性和自适应技术在提高大型语言模型优化效率和有效性方面的重要性，从而促进更可持续和更易于访问的 AI 系统的开发。</li>
</ul>

<h3>Title: Title:
          D2LLM: Decomposed and Distilled Large Language Models for Semantic Search</h3>
<ul>
<li><strong>Authors: </strong>Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          D2LLM: Decomposed and Distilled Large Language Models for Semantic Search(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The key challenge in semantic search is to create models that are both accurate and efficient in pinpointing relevant sentences for queries. While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications. In this paper, we present D2LLMs-Decomposed and Distilled LLMs for semantic search-that combines the best of both worlds. We decompose a cross-encoder into an efficient bi-encoder integrated with Pooling by Multihead Attention and an Interaction Emulation Module, achieving nuanced understanding and pre-computability. Knowledge from the LLM is distilled into this model using contrastive, rank, and feature imitation techniques. Our experiments show that D2LLM surpasses five leading baselines in terms of all metrics across three tasks, particularly improving NLI task performance by at least 6.45%. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>语义搜索的关键挑战是创建既准确又高效的模型，以精确定位查询的相关句子。虽然 BERT 风格的双编码器在预计算嵌入方面效率更高，但它们通常会错过搜索任务中的细微差别。相反，具有交叉编码器设计的 GPT 风格的 LLM 可以捕捉这些细微差别，但计算量大，阻碍了实时应用。在本文中，我们提出了 D2LLM（用于语义搜索的分解和提炼 LLM），结合了两全其美。我们将交叉编码器分解为高效的双编码器，该编码器集成了多头注意力池和交互模拟模块，实现了细致入微的理解和预计算性。使用对比、排名和特征模仿技术将 LLM 中的知识提炼到此模型中。我们的实验表明，D2LLM 在三个任务的所有指标方面都超过了五个领先的基线，尤其是将 NLI 任务性能提高了至少 6.45%。源代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph</h3>
<ul>
<li><strong>Authors: </strong>Zhehao Zhang, Jiaao Chen, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs. Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed. In this work, we introduce Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity. Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data. Such newly generated test samples can have different levels of complexity while maintaining linguistic diversity similar to the original benchmarks. We further use a code-augmented LLM to ensure the label correctness of newly generated data. We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs. Experimental results show that almost all LLMs experience a performance decrease with increased complexity and certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit more biases when being evaluated via the data generated by DARG with higher complexity levels. These observations provide useful insights into how to dynamically and adaptively evaluate LLMs. The code is available at this https URL.</li>
<li><strong>摘要：</strong>当前通过静态基准测试评估大型语言模型 (LLM) 的范式存在重大局限性，例如易受数据污染以及对 LLM 不断发展的能力缺乏适应性。因此，迫切需要能够适应并生成具有受控复杂度的评估数据的评估方法。在这项工作中，我们引入了通过自适应推理图演化 (DARG) 对 LLM 进行动态评估，以动态扩展具有受控复杂度和多样性的当前基准测试。具体而言，我们首先提取当前基准测试中数据点的推理图，然后扰动推理图以生成新的测试数据。这种新生成的测试样本可以具有不同的复杂度级别，同时保持与原始基准测试相似的语言多样性。我们进一步使用代码增强的 LLM 来确保新生成数据的标签正确性。我们将我们的 DARG 框架应用于四个领域的各种推理任务，其中包括 15 个最先进的 LLM。实验结果表明，几乎所有 LLM 的性能都会随着复杂度的增加而下降，某些 LLM 的性能下降幅度显著。此外，我们发现，当通过复杂度更高的 DARG 生成的数据进行评估时，LLM 会表现出更多偏差。这些观察结果为如何动态和自适应地评估 LLM 提供了有用的见解。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?</h3>
<ul>
<li><strong>Authors: </strong>Jianfeng He, Runing Yang, Linlin Yu, Changbin Li, Ruoxi Jia, Feng Chen, Ming Jin, Chang-Tien Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Text summarization, a key natural language generation (NLG) task, is vital in various domains. However, the high cost of inaccurate summaries in risk-critical applications, particularly those involving human-in-the-loop decision-making, raises concerns about the reliability of uncertainty estimation on text summarization (UE-TS) evaluation methods. This concern stems from the dependency of uncertainty model metrics on diverse and potentially conflicting NLG metrics. To address this issue, we introduce a comprehensive UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The benchmark evaluates the uncertainty estimation capabilities of two large language models and one pre-trained language model on three datasets, with human-annotation analysis incorporated where applicable. We also assess the performance of 14 common uncertainty estimation methods within this benchmark. Our findings emphasize the importance of considering multiple uncorrelated NLG metrics and diverse uncertainty estimation methods to ensure reliable and efficient evaluation of UE-TS techniques.</li>
<li><strong>摘要：</strong>文本摘要是一项重要的自然语言生成 (NLG) 任务，在各个领域都至关重要。然而，在风险关键型应用中，特别是涉及人在环决策的应用中，不准确的摘要成本很高，这引发了人们对文本摘要 (UE-TS) 评估方法中不确定性估计的可靠性的担忧。这种担忧源于不确定性模型指标对各种可能相互冲突的 NLG 指标的依赖。为了解决这个问题，我们引入了一个全面的 UE-TS 基准，该基准结合了四个维度的 31 个 NLG 指标。该基准评估了两个大型语言模型和一个预训练语言模型在三个数据集上的不确定性估计能力，并在适用的情况下结合了人工注释分析。我们还评估了该基准中 14 种常见不确定性估计方法的性能。我们的研究结果强调了考虑多个不相关的 NLG 指标和多种不确定性估计方法的重要性，以确保可靠、有效地评估 UE-TS 技术。</li>
</ul>

<h3>Title: Title:
          OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure</h3>
<ul>
<li><strong>Authors: </strong>Jikai Wang, Yi Su, Juntao Li, Qinrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a "draft and then verify" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>自回归语言模型在各种场景下都表现出色，但其一步一个词的生成模式限制了推理效率，随着模型规模的不断扩大，这一问题也日益凸显。推测解码采用“先草稿再验证”的机制，允许一步生成多个 token，实现无损加速。现有方法主要采用固定的启发式草稿结构，无法适应不同情况以最大化验证过程中的接受长度。为了缓解这一困境，我们提出了一种自适应可扩展的草稿树构建算法 OPT-Tree，该算法在每个解码步骤中搜索最大化接受长度数学期望的最优树结构。实验结果表明，OPT-Tree 优于现有草稿结构，与自回归解码相比，加速比高达 3.2。如果草稿模型足够强大且节点预算充足，它可以在一步生成十多个 token。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          BERT, Neural Information Retrieval, Boolean Retrieval, Negation Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Quan Mai, Susan Gauch, Douglas Adams</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BERT, Neural Information Retrieval, Boolean Retrieval, Negation Retrieval(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query embeddings for set operations and Boolean logic queries, such as Intersection (AND), Difference (NOT), and Union (OR). SetBERT significantly improves retrieval performance for logic-structured queries, an area where both traditional and neural retrieval methods typically underperform. We propose an innovative use of inversed-contrastive loss, focusing on identifying the negative sentence, and fine-tuning BERT with a dataset generated via prompt GPT. Furthermore, we demonstrate that, unlike other BERT-based models, fine-tuning with triplet loss actually degrades performance for this specific task. Our experiments reveal that SetBERT-base not only significantly outperforms BERT-base (up to a 63% improvement in Recall) but also achieves performance comparable to the much larger BERT-large model, despite being only one-third the size.</li>
<li><strong>摘要：</strong>我们推出了 SetBERT，这是一种基于 BERT 的微调模型，旨在增强集合运算和布尔逻辑查询的查询嵌入，例如交集 (AND)、差集 (NOT) 和并集 (OR)。SetBERT 显著提高了逻辑结构化查询的检索性能，而传统和神经检索方法通常都表现不佳。我们提出了一种创新的逆对比损失用法，重点是识别否定句，并使用通过即时 GPT 生成的数据集对 BERT 进行微调。此外，我们证明，与其他基于 BERT 的模型不同，使用三重态损失进行微调实际上会降低此特定任务的性能。我们的实验表明，SetBERT-base 不仅显著优于 BERT-base（召回率提高了 63%），而且尽管只有三分之一的大小，但其性能却与更大的 BERT-large 模型相当。</li>
</ul>

<h3>Title: Title:
          Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Yan, Lizhi Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Accurate assessment of personality traits is crucial for effective psycho-counseling, yet traditional methods like self-report questionnaires are time-consuming and biased. This study exams whether Large Language Models (LLMs) can predict the Big Five personality traits directly from counseling dialogues and introduces an innovative framework to perform the task. Our framework applies role-play and questionnaire-based prompting to condition LLMs on counseling sessions, simulating client responses to the Big Five Inventory. We evaluated our framework on 853 real-world counseling sessions, finding a significant correlation between LLM-predicted and actual Big Five traits, proving the validity of framework. Moreover, ablation studies highlight the importance of role-play simulations and task simplification via questionnaires in enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model, utilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves a 130.95\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\% in personality prediction validity. In conclusion, LLMs can predict personality based on counseling dialogues. Our code and model are publicly available at \url{this https URL}, providing a valuable tool for future research in computational psychometrics.</li>
<li><strong>摘要：</strong>准确评估人格特质对于有效的心理咨询至关重要，但自我报告问卷等传统方法既耗时又有偏见。本研究检验大型语言模型 (LLM) 是否能直接从咨询对话中预测大五人格特质，并引入一个创新框架来执行该任务。我们的框架应用角色扮演和基于问卷的提示来调节 LLM 的咨询会话，模拟客户对大五量表的反应。我们在 853 个现实世界的咨询会话中评估了我们的框架，发现 LLM 预测的大五特质与实际的大五特质之间存在显著相关性，证明了框架的有效性。此外，消融研究强调了角色扮演模拟和通过问卷简化任务对提高预测准确性的重要性。同时，我们经过微调的 Llama3-8B 模型利用直接偏好优化和监督微调，实现了 130.95% 的改进，在性格预测有效性方面比最先进的 Qwen1.5-110B 高出 36.94%。总之，LLM 可以根据咨询对话预测性格。我们的代码和模型在 \url{此 https URL} 上公开提供，为未来计算心理测量学研究提供了宝贵的工具。</li>
</ul>

<h3>Title: Title:
          Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出令人印象深刻的推理能力，尤其是在文本数学问题解决方面。然而，现有的开源图像指令微调数据集，每幅图像包含有限的问答对，没有充分利用视觉信息来增强多模态 LLM (MLLM) 的多模态数学推理能力。为了弥补这一差距，我们从 24 个现有数据集中收集了 40K 张带有问答对的高质量图像，并合成了 320K 张新问答对，创建了 MathV360K 数据集，从而解决了缺乏高质量、多样化的多模态数学数据集的问题，该数据集增强了多模态数学问题的广度和深度。我们推出了 Math-LLaVA，这是一个基于 LLaVA-1.5 的模型，使用 MathV360K 进行了微调。这种新方法显著提高了 LLaVA-1.5 的多模态数学推理能力，在 MathVista 的 minitest split 上实现了 19 分的提升，性能与 GPT-4V 相当。此外，Math-LLaVA 表现出增强的通用性，在 MMMU 基准上显示出显着的改进。我们的研究强调了数据集多样性和综合性对于提高 MLLM 数学推理能力的重要性。代码和数据可在以下位置获取：\url{此 https URL}。</li>
</ul>

<h3>Title: Title:
          Leveraging LLMs for Dialogue Quality Measurement</h3>
<ul>
<li><strong>Authors: </strong>Jinghan Jia, Abi Komma, Timothy Leffel, Xujun Peng, Ajay Nagesh, Tamer Soliman, Aram Galstyan, Anoop Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Leveraging LLMs for Dialogue Quality Measurement(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In task-oriented conversational AI evaluation, unsupervised methods poorly correlate with human judgments, and supervised approaches lack generalization. Recent advances in large language models (LLMs) show robust zeroshot and few-shot capabilities across NLP tasks. This paper explores using LLMs for automated dialogue quality evaluation, experimenting with various configurations on public and proprietary datasets. Manipulating factors such as model size, in-context examples, and selection techniques, we examine "chain-of-thought" (CoT) reasoning and label extraction procedures. Our results show that (1) larger models yield more accurate dialogue labels; (2) algorithmic selection of in-context examples outperforms random selection; (3) CoT reasoning where an LLM is asked to provide justifications before outputting final labels improves performance; and (4) fine-tuned LLMs outperform out-of-the-box ones. Our results indicate that LLMs that are suitably fine-tuned and have sufficient reasoning capabilities can be leveraged for automated dialogue evaluation.</li>
<li><strong>摘要：</strong>在面向任务的对话式 AI 评估中，无监督方法与人类判断的相关性较差，而监督方法缺乏泛化能力。大型语言模型 (LLM) 的最新进展表明，它在 NLP 任务中具有强大的零样本和少样本能力。本文探讨了使用 LLM 进行自动对话质量评估，并在公共和专有数据集上尝试了各种配置。通过操纵模型大小、上下文示例和选择技术等因素，我们研究了“思路链”(CoT) 推理和标签提取程序。我们的结果表明：(1) 更大的模型可以产生更准确的对话标签；(2) 上下文示例的算法选择优于随机选择；(3) CoT 推理（要求 LLM 在输出最终标签之前提供理由）可以提高性能；(4) 经过微调的 LLM 优于开箱即用的 LLM。我们的结果表明，经过适当微调且具有足够推理能力的 LLM 可用于自动对话评估。</li>
</ul>

<h3>Title: Title:
          Retrieval Augmented Instruction Tuning for Open NER with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tingyu Xie, Jian Zhang, Yan Zhang, Yuanyuan Liang, Qi Li, Hongwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Retrieval Augmented Instruction Tuning for Open NER with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The strong capability of large language models (LLMs) has been applied to information extraction (IE) through either retrieval augmented prompting or instruction tuning (IT). However, the best way to incorporate information with LLMs for IE remains an open question. In this paper, we explore Retrieval Augmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named entity recognition (NER). Specifically, for each training sample, we retrieve semantically similar examples from the training dataset as the context and prepend them to the input of the original instruction. To evaluate our RA-IT approach more thoroughly, we construct a Chinese IT dataset for open NER and evaluate RA-IT in both English and Chinese scenarios. Experimental results verify the effectiveness of RA-IT across various data sizes and in both English and Chinese scenarios. We also conduct thorough studies to explore the impacts of various retrieval strategies in the proposed RA-IT framework. Code and data are available at: this https URL</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的强大功能已通过检索增强提示或指令调整 (IT) 应用于信息提取 (IE)。然而，将信息与 LLM 结合用于 IE 的最佳方式仍是一个悬而未决的问题。在本文中，我们探索了用于 IE 的检索增强指令调整 (RA-IT)，重点关注开放式命名实体识别 (NER) 任务。具体来说，对于每个训练样本，我们从训练数据集中检索语义相似的示例作为上下文，并将它们添加到原始指令的输入中。为了更彻底地评估我们的 RA-IT 方法，我们为开放式 NER 构建了一个中文 IT 数据集，并在英文和中文场景中评估 RA-IT。实验结果验证了 RA-IT 在不同数据大小以及英文和中文场景中的有效性。我们还进行了深入研究，以探索各种检索策略对所提出的 RA-IT 框架的影响。代码和数据可从以下网址获取：此 https URL</li>
</ul>

<h3>Title: Title:
          Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy</h3>
<ul>
<li><strong>Authors: </strong>Simone Astarita, Sandor Kruk, Jan Reerink, Pablo Gómez</a></li>
<li><strong>Subjects: </strong>cs.CL, astro-ph.IM, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Rapid progress in the capabilities of machine learning approaches in natural language processing has culminated in the rise of large language models over the last two years. Recent works have shown unprecedented adoption of these for academic writing, especially in some fields, but their pervasiveness in astronomy has not been studied sufficiently. To remedy this, we extract words that ChatGPT uses more often than humans when generating academic text and search a total of 1 million articles for them. This way, we assess the frequency of word occurrence in published works in astronomy tracked by the NASA Astrophysics Data System since 2000. We then perform a statistical analysis of the occurrences. We identify a list of words favoured by ChatGPT and find a statistically significant increase for these words against a control group in 2024, which matches the trend in other disciplines. These results suggest a widespread adoption of these models in the writing of astronomy papers. We encourage organisations, publishers, and researchers to work together to identify ethical and pragmatic guidelines to maximise the benefits of these systems while maintaining scientific rigour.</li>
<li><strong>摘要：</strong>在过去两年中，机器学习方法在自然语言处理方面的能力迅速进步，最终导致了大型语言模型的兴起。最近的研究表明，这些模型在学术写作中得到了前所未有的采用，尤其是在某些领域，但它们在天文学中的普遍性尚未得到充分研究。为了解决这个问题，我们提取了 ChatGPT 在生成学术文本时比人类更常用的单词，并在总共 100 万篇文章中搜索它们。通过这种方式，我们评估了自 2000 年以来 NASA 天体物理数据系统跟踪的天文学出版作品中单词出现的频率。然后，我们对这些出现进行统计分析。我们确定了 ChatGPT 偏爱的单词列表，并发现这些单词在 2024 年与对照组相比有显著的增加，这与其他学科的趋势相符。这些结果表明，这些模型在天文学论文的写作中得到了广泛采用。我们鼓励组织、出版商和研究人员共同努力，确定道德和务实的指导方针，以最大限度地发挥这些系统的优势，同时保持科学严谨性。</li>
</ul>

<h3>Title: Title:
          Dual-Space Knowledge Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Dual-Space Knowledge Distillation for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is known as a promising solution to compress large language models (LLMs) via transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the two models so that more knowledge can be transferred. However, in the current white-box KD framework, the output distributions are from the respective output spaces of the two models, using their own prediction heads. We argue that the space discrepancy will lead to low similarity between the teacher model and the student model on both representation and distribution levels. Furthermore, this discrepancy also hinders the KD process between models with different vocabularies, which is common for current LLMs. To address these issues, we propose a dual-space knowledge distillation (DSKD) framework that unifies the output spaces of the two models for KD. On the basis of DSKD, we further develop a cross-model attention mechanism, which can automatically align the representations of the two models with different vocabularies. Thus, our framework is not only compatible with various distance functions for KD (e.g., KL divergence) like the current framework, but also supports KD between any two LLMs regardless of their vocabularies. Experiments on task-agnostic instruction-following benchmarks show that DSKD significantly outperforms the current white-box KD framework with various distance functions, and also surpasses existing KD methods for LLMs with different vocabularies.</li>
<li><strong>摘要：</strong>知识蒸馏 (KD) 是一种很有前途的解决方案，它通过将大型语言模型 (LLM) 的知识转移到较小的模型中来压缩大型语言模型 (LLM)。在此过程中，白盒知识蒸馏方法通常会最小化两个模型的输出分布之间的距离，以便可以迁移更多的知识。然而，在当前的白盒知识蒸馏框架中，输出分布来自两个模型各自的输出空间，使用它们自己的预测头。我们认为空间差异将导致教师模型和学生模型在表示和分布层面上的相似度较低。此外，这种差异还阻碍了具有不同词汇的模型之间的知识蒸馏过程，这在当前的 LLM 中很常见。为了解决这些问题，我们提出了一个双空间知识蒸馏 (DSKD) 框架，将两个模型的输出空间统一起来进行知识蒸馏。在 DSKD 的基础上，我们进一步开发了一种跨模型注意机制，可以自动对齐具有不同词汇的两个模型的表示。因此，我们的框架不仅与当前框架一样兼容各种 KD 距离函数（例如 KL 散度），而且还支持任意两个 LLM 之间的 KD，无论它们的词汇如何。在与任务无关的指令跟踪基准上进行的实验表明，DSKD 明显优于具有各种距离函数的当前白盒 KD 框架，并且也超越了具有不同词汇的 LLM 的现有 KD 方法。</li>
</ul>

<h3>Title: Title:
          An Empirical Study on the Characteristics of Bias upon Context Length Variation for Bangla</h3>
<ul>
<li><strong>Authors: </strong>Jayanta Sadhu, Ayan Antik Khan, Abhik Bhattacharjee, Rifat Shahriyar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          An Empirical Study on the Characteristics of Bias upon Context Length Variation for Bangla(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Pretrained language models inherently exhibit various social biases, prompting a crucial examination of their social impact across various linguistic contexts due to their widespread usage. Previous studies have provided numerous methods for intrinsic bias measurements, predominantly focused on high-resource languages. In this work, we aim to extend these investigations to Bangla, a low-resource language. Specifically, in this study, we (1) create a dataset for intrinsic gender bias measurement in Bangla, (2) discuss necessary adaptations to apply existing bias measurement methods for Bangla, and (3) examine the impact of context length variation on bias measurement, a factor that has been overlooked in previous studies. Through our experiments, we demonstrate a clear dependency of bias metrics on context length, highlighting the need for nuanced considerations in Bangla bias analysis. We consider our work as a stepping stone for bias measurement in the Bangla Language and make all of our resources publicly available to support future research.</li>
<li><strong>摘要：</strong>预训练语言模型本身就表现出各种社会偏见，由于其广泛使用，因此需要对其在各种语言环境中的社会影响进行关键检查。先前的研究提供了许多用于测量内在偏见的方法，主要侧重于资源丰富的语言。在这项工作中，我们旨在将这些调查扩展到资源匮乏的语言孟加拉语。具体来说，在本研究中，我们 (1) 创建了一个用于测量孟加拉语内在性别偏见的数据集，(2) 讨论了将现有偏见测量方法应用于孟加拉语所需的调整，以及 (3) 研究上下文长度变化对偏见测量的影响，这是先前研究中忽视的一个因素。通过我们的实验，我们展示了偏见指标对上下文长度的明显依赖性，强调了在孟加拉语偏见分析中需要进行细致的考虑。我们将我们的工作视为测量孟加拉语偏见的垫脚石，并将我们的所有资源公开以支持未来的研究。</li>
</ul>

<h3>Title: Title:
          A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs</h3>
<ul>
<li><strong>Authors: </strong>Vaibhav Singh, Amrith Krishna, Karthika NJ, Ganesh Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Low-resource languages, by its very definition, tend to be under represented in the pre-training corpora of Large Language Models. In this work, we investigate three low-resource cross-lingual approaches that enable an LLM adapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic languages, among many other language families, contribute to less than $0.005\%$ of the total $2$ trillion token pre-training corpora. In this work, we experiment with the English-dominated Llama-2 for cross-lingual transfer to three Indic languages, Bengali, Hindi, and Tamil as target languages. We study three approaches for cross-lingual transfer, under ICL and fine-tuning. One, we find that adding additional supervisory signals via a dominant language in the LLM, leads to improvements, both under in-context learning and fine-tuning. Two, adapting the target languages to word reordering may be beneficial under ICL, but its impact diminishes with fine tuning. Finally, continued pre-training in one low-resource language can improve model performance for other related low-resource languages.</li>
<li><strong>摘要：</strong>低资源语言，顾名思义，在大型语言模型的预训练语料库中往往代表性不足。在这项工作中，我们研究了三种低资源跨语言方法，使 LLM 能够适应以前从未见过的语言的任务。Llama-2 是一个 LLM，其中印度语和许多其他语系占总 $2$ 万亿标记预训练语料库的不到 $0.005\%$。在这项工作中，我们尝试使用以英语为主的 Llama-2 进行跨语言迁移，以三种印度语（孟加拉语、印地语和泰米尔语）作为目标语言。我们研究了 ICL 和微调下的三种跨语言迁移方法。第一，我们发现通过 LLM 中的主导语言添加额外的监督信号，可以带来改进，无论是在上下文学习还是微调下。第二，在 ICL 下，使目标语言适应单词重新排序可能有益，但其影响会随着微调而减弱。最后，继续在一种低资源语言中进行预训练可以提高其他相关低资源语言的模型性能。</li>
</ul>

<h3>Title: Title:
          A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens</h3>
<ul>
<li><strong>Authors: </strong>Zhijie Nie, Richong Zhang, Zhanyu Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Text embeddings from large language models (LLMs) have achieved excellent results in tasks such as information retrieval, semantic textual similarity, etc. In this work, we show an interesting finding: when feeding a text into the embedding LLMs, the obtained text embedding will be able to be aligned with the key tokens in the input text. We first fully analyze this phenomenon on eight embedding LLMs and show that this phenomenon is universal and is not affected by model architecture, training strategy, and embedding method. With a deeper analysis, we then find that the main change in embedding space between the embedding LLMs and their original generative LLMs is in the first principal component. By adjusting the first principal component, we can align text embedding with the key tokens. Finally, we give several examples to demonstrate the vast application potential of this finding: (1) we propose a simple and practical sparse retrieval method based on the aligned tokens, which can achieve 80\% of the dense retrieval effect of the same model while reducing the computation significantly; (2) we show that our findings provide a fresh perspective to help understand fuzzy concepts (e.g., semantic relatedness vs. semantic similarity) and emerging technologies (e.g., instruction-following embedding) in this field.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的文本嵌入在信息检索、语义文本相似性等任务中取得了优异的效果。在这项工作中，我们展示了一个有趣的发现：当将文本输入嵌入 LLM 时，获得的文本嵌入将能够与输入文本中的关键标记对齐。我们首先在八个嵌入 LLM 上充分分析了这一现象，并表明这种现象具有普遍性，不受模型架构、训练策略和嵌入方法的影响。经过更深入的分析，我们发现嵌入 LLM 与其原始生成 LLM 之间的嵌入空间的主要变化在于第一个主成分。通过调整第一个主成分，我们可以将文本嵌入与关键标记对齐。最后，我们给出几个例子来证明这一发现的巨大应用潜力：（1）我们提出了一种基于对齐标记的简单实用的稀疏检索方法，该方法可以达到同一模型 80％的密集检索效果，同时显着减少计算量；（2）我们表明我们的发现提供了一个新的视角来帮助理解该领域的模糊概念（例如，语义相关性与语义相似性）和新兴技术（例如，指令跟踪嵌入）。</li>
</ul>

<h3>Title: Title:
          Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance</h3>
<ul>
<li><strong>Authors: </strong>Manon Reusens, Philipp Borchert, Jochen De Weerdt, Bart Baesens</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at providing information acquired during pretraining on large-scale corpora and following instructions through user prompts. This study investigates whether the quality of LLM responses varies depending on the demographic profile of users. Considering English as the global lingua franca, along with the diversity of its dialects among speakers of different native languages, we explore whether non-native English speakers receive lower-quality or even factually incorrect responses from LLMs more frequently. Our results show that performance discrepancies occur when LLMs are prompted by native versus non-native English speakers and persist when comparing native speakers from Western countries with others. Additionally, we find a strong anchoring effect when the model recognizes or is made aware of the user's nativeness, which further degrades the response quality when interacting with non-native speakers. Our analysis is based on a newly collected dataset with over 12,000 unique annotations from 124 annotators, including information on their native language and English proficiency.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 擅长提供在大型语料库上进行预训练期间获得的信息以及通过用户提示遵循指令。本研究调查了 LLM 响应的质量是否因用户的人口统计资料而异。考虑到英语是全球通用语，并且不同母语使用者的方言各不相同，我们探讨非英语母语使用者是否更频繁地收到来自 LLM 的低质量甚至事实错误的响应。我们的结果表明，当 LLM 由英语母语使用者和非英语母语使用者提示时，性能差异会出现，并且在将西方国家的母语使用者与其他国家的母语使用者进行比较时，这种差异仍然存在。此外，我们发现，当模型识别或意识到用户的母语时，锚定效应很强，这进一步降低了与非母语使用者互动时的响应质量。我们的分析基于一个新收集的数据集，其中包含来自 124 位注释者的 12,000 多个独特注释，包括有关他们的母语和英语水平的信息。</li>
</ul>

<h3>Title: Title:
          Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Existing speculative decoding methods typically require additional model structure and training processes to assist the model for draft token generation. This makes the migration of acceleration methods to the new model more costly and more demanding on device memory. To address this problem, we propose the Make Some Noise (MSN) training framework as a replacement for the supervised fine-tuning stage of the large language model. The training method simply introduces some noise at the input for the model to learn the denoising task. It significantly enhances the parallel decoding capability of the model without affecting the original task capability. In addition, we propose a tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further improve the inference speed of MSN models. Experiments in both the general and code domains have shown that MSN can improve inference speed by 2.3-2.7x times without compromising model performance. The MSN model also achieves comparable acceleration ratios to the SOTA model with additional model structure on Spec-Bench.</li>
<li><strong>摘要：</strong>现有的推测解码方法通常需要额外的模型结构和训练过程来辅助模型进行草稿 token 生成，这使得将加速方法迁移到新模型的成本更高，对设备内存的要求也更高。针对这一问题，我们提出了 Make Some Noise (MSN) 训练框架来替代大型语言模型的监督微调阶段，该训练方法只是在输入端引入一些噪音，让模型学习去噪任务，在不影响原有任务能力的情况下，显著提升了模型的并行解码能力。此外，我们提出了一种基于树的检索增强雅可比 (TR-Jacobi) 解码策略，进一步提升 MSN 模型的推理速度。在通用和代码领域的实验都表明，MSN 可以在不影响模型性能的情况下将推理速度提高 2.3-2.7 倍。MSN 模型在 Spec-Bench 上也实现了与增加模型结构的 SOTA 模型相当的加速比。</li>
</ul>

<h3>Title: Title:
          Variable Layer-Wise Quantization: A Simple and Effective Approach to Quantize LLMs</h3>
<ul>
<li><strong>Authors: </strong>Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Variable Layer-Wise Quantization: A Simple and Effective Approach to Quantize LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present a simple variable quantization approach that quantizes different layers of a large language model (LLM) at different bit levels. Specifically, we quantize the most important layers to higher bit precision and less important layers to lower bits to achieve floating point quantization levels. We propose two effective strategies to measure the importance of layers within LLMs: the first measures the importance of a layer based on how different its output embeddings are from the input embeddings (the higher the better); the second estimates the importance of a layer using the number of layer weights that are much larger than average (the smaller the better). We show that quantizing different layers at varying bits according to our importance scores results in minimal performance drop with a far more compressed model size. Finally, we present several practical key takeaways from our variable layer-wise quantization experiments: (a) LLM performance under variable quantization remains close to the original model until 25-50% of layers are moved in lower quantization using our proposed ordering but only until 5-10% if moved using no specific ordering; (b) Quantizing LLMs to lower bits performs substantially better than pruning unless extreme quantization (2-bit) is used; and (c) Layer-wise quantization to lower bits works better in the case of larger LLMs with more layers compared to smaller LLMs with fewer layers. The code used to run the experiments is available at: this https URL.</li>
<li><strong>摘要：</strong>我们提出了一种简单的变量量化方法，该方法以不同的位级别量化大型语言模型 (LLM) 的不同层。具体而言，我们将最重要的层量化为更高的位精度，将不太重要的层量化为更低的位，以实现浮点量化级别。我们提出了两种有效的策略来衡量 LLM 中层的重要性：第一种策略根据层的输出嵌入与输入嵌入的差异来衡量层的重要性（越高越好）；第二种策略使用远大于平均值的层权重数量来估计层的重要性（越小越好）。我们表明，根据我们的重要性分数以不同的位量化不同的层会导致性能下降最小，同时模型大小会大大压缩。最后，我们从变量逐层量化实验中得出几个实用的关键结论：(a) 在变量量化下，LLM 性能保持接近原始模型，直到使用我们提出的排序以较低的量化方式移动 25-50% 的层，但如果不使用特定排序移动，则只有 5-10%； (b) 除非使用极端量化（2 位），否则将 LLM 量化为较低位比修剪效果要好得多；(c) 与层数较少的较小 LLM 相比，层数较多的较大 LLM 的逐层量化为较低位效果更好。用于运行实验的代码可从以下网址获取：此 https URL。</li>
</ul>

<h3>Title: Title:
          Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA</h3>
<ul>
<li><strong>Authors: </strong>Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Long-context modeling capabilities have garnered widespread attention, leading to the emergence of Large Language Models (LLMs) with ultra-context windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong's test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model's long-context modeling capabilities.</li>
<li><strong>摘要：</strong>长上下文建模能力得到了广泛关注，并催生了具有超上下文窗口的大型语言模型（LLM）。与此同时，用于评估长上下文 LLM 的基准也在逐渐赶上。然而，现有的基准使用不相关的噪声文本来人为地延长测试用例的长度，偏离了长上下文应用的真实场景。为了弥补这一差距，我们提出了一个新颖的长上下文基准 Loong，通过扩展多文档问答 (QA) 来贴近现实场景。与典型的文档问答不同，在 Loong 的测试用例中，每个文档都与最终答案相关，忽略任何文档都会导致答案失败。此外，Loong 引入了四类具有不同上下文长度范围的任务：焦点定位、比较、聚类和推理链，以便更真实、更全面地评估长上下文理解。大量实验表明，现有的长上下文语言模型仍具有相当大的增强潜力。检索增强生成 (RAG) 的性能较差，表明 Loong 可以可靠地评估模型的长上下文建模能力。</li>
</ul>

<h3>Title: Title:
          Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain</h3>
<ul>
<li><strong>Authors: </strong>Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Questions are essential tools for acquiring the necessary information to complete information-seeking tasks. However, large language models (LLMs), especially open-source models, often perform poorly in generating informative questions, as measured by expected information gain (EIG). In this paper, we propose a method to enhance the informativeness of LLM-generated questions in 20-question game dialogues. We sample multiple questions from the same model (LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG questions to apply a Direct Preference Optimization (DPO) algorithm. Our results show that this method produces more effective questions (in terms of EIG), even in domains different from those used to train the DPO model.</li>
<li><strong>摘要：</strong>问题是获取完成信息搜索任务所需信息的重要工具。然而，大型语言模型 (LLM)，尤其是开源模型，在生成信息量丰富的问题​​方面往往表现不佳，以预期信息增益 (EIG) 来衡量。在本文中，我们提出了一种方法来增强 20 个问题游戏对话中 LLM 生成问题的信息量。我们为每个游戏从同一模型 (LLAMA 2-CHAT 7B) 中抽取多个问题，并创建低 EIG 和高 EIG 问题对以应用直接偏好优化 (DPO) 算法。我们的结果表明，即使在与用于训练 DPO 模型的领域不同的领域中，这种方法也能产生更有效的问题（就 EIG 而言）。</li>
</ul>

<h3>Title: Title:
          Enhancing Tool Retrieval with Iterative Feedback from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qiancheng Xu, Yongqi Li, Heming Xia, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Enhancing Tool Retrieval with Iterative Feedback from Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Tool learning aims to enhance and expand large language models' (LLMs) capabilities with external tools, which has gained significant attention recently. Current methods have shown that LLMs can effectively handle a certain amount of tools through in-context learning or fine-tuning. However, in real-world scenarios, the number of tools is typically extensive and irregularly updated, emphasizing the necessity for a dedicated tool retrieval component. Tool retrieval is nontrivial due to the following challenges: 1) complex user instructions and tool descriptions; 2) misalignment between tool retrieval and tool usage models. To address the above issues, we propose to enhance tool retrieval with iterative feedback from the large language model. Specifically, we prompt the tool usage model, i.e., the LLM, to provide feedback for the tool retriever model in multi-round, which could progressively improve the tool retriever's understanding of instructions and tools and reduce the gap between the two standalone components. We build a unified and comprehensive benchmark to evaluate tool retrieval models. The extensive experiments indicate that our proposed approach achieves advanced performance in both in-domain evaluation and out-of-domain evaluation.</li>
<li><strong>摘要：</strong>工具学习旨在通过外部工具增强和扩展大型语言模型 (LLM) 的功能，这在最近引起了广泛关注。当前的方法表明，LLM 可以通过上下文学习或微调有效地处理一定数量的工具。然而，在现实世界中，工具的数量通常很大且不定期更新，这强调了专用工具检索组件的必要性。工具检索并非易事，因为存在以下挑战：1) 复杂的用户指令和工具描述；2) 工具检索和工具使用模型之间的不一致。为了解决上述问题，我们建议使用大型语言模型的迭代反馈来增强工具检索。具体来说，我们提示工具使用模型，即 LLM，在多轮中为工具检索器模型提供反馈，这可以逐步提高工具检索器对指令和工具的理解，并缩小两个独立组件之间的差距。我们建立了一个统一而全面的基准来评估工具检索模型。大量实验表明，我们提出的方法在域内评估和域外评估中都取得了先进的性能。</li>
</ul>

<h3>Title: Title:
          MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown substantial progress in natural language understanding and generation, proving valuable especially in the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks, which can be categorized as knowledge-intensive tasks and alignment-required tasks. Previous approaches either ignore the latter task or focus on a minority of tasks and hence lose generalization. To address these drawbacks, we propose a progressive fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise aggregator to encode diverse knowledge in the first stage and filter out detrimental information. In the second stage, we drop the Noise Aggregator to avoid the interference of suboptimal representation and leverage an additional alignment module optimized towards an orthogonal direction to the knowledge space to mitigate knowledge forgetting. Based on this two-stage paradigm, we proposed a Medical LLM through decoupling Clinical Alignment and Knowledge Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA) performance on over 20 medical tasks, as well as SOTA results on specific medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all demonstrate significant improvements over existing models with similar model sizes.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言理解和生成方面取得了实质性进展，尤其在医学领域很有价值。尽管取得了进展，但由于医疗任务固有的复杂性和多样性，挑战依然存在，医疗任务可分为知识密集型任务和需要对齐的任务。以前的方法要么忽略后一项任务，要么专注于少数任务，因此失去了泛化能力。为了解决这些缺点，我们提出了一种渐进式微调流程。该流程采用知识聚合器和噪声聚合器在第一阶段对各种知识进行编码并过滤掉有害信息。在第二阶段，我们放弃噪声聚合器以避免次优表示的干扰，并利用一个朝着知识空间正交方向优化的额外对齐模块来减轻知识遗忘。基于这一两阶段范式，我们提出了一种通过分离临床对齐和知识聚合 (MedCare) 实现的医学 LLM，旨在实现 20 多项医学任务的 SOTA 性能，以及特定医学对齐任务的 SOTA 结果。MedCare 的各种模型大小 (1.8B、7B、14B) 均比现有具有类似模型大小的模型有显著改进。</li>
</ul>

<h3>Title: Title:
          Benchmarking Mental State Representations in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Benchmarking Mental State Representations in Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>While numerous works have assessed the generative performance of language models (LMs) on tasks requiring Theory of Mind reasoning, research into the models' internal representation of mental states remains limited. Recent work has used probing to demonstrate that LMs can represent beliefs of themselves and others. However, these claims are accompanied by limited evaluation, making it difficult to assess how mental state representations are affected by model design and training choices. We report an extensive benchmark with various LM types with different model sizes, fine-tuning approaches, and prompt designs to study the robustness of mental state representations and memorisation issues within the probes. Our results show that the quality of models' internal representations of the beliefs of others increases with model size and, more crucially, with fine-tuning. We are the first to study how prompt variations impact probing performance on theory of mind tasks. We demonstrate that models' representations are sensitive to prompt variations, even when such variations should be beneficial. Finally, we complement previous activation editing experiments on Theory of Mind tasks and show that it is possible to improve models' reasoning performance by steering their activations without the need to train any probe.</li>
<li><strong>摘要：</strong>虽然许多研究都评估了语言模型 (LM) 在需要心智理论推理的任务中的生成性能，但对模型对心理状态的内部表征的研究仍然有限。最​​近的研究使用探索来证明 LM 可以代表自己和他人的信念。然而，这些说法伴随着有限的评估，因此很难评估心理状态表征如何受到模型设计和训练选择的影响。我们报告了一个广泛的基准，其中包含具有不同模型大小、微调方法和提示设计的各种 LM 类型，以研究探测中的心理状态表征和记忆问题的稳健性。我们的结果表明，模型对他人信念的内部表征的质量随着模型大小的增加而增加，更重要的是，随着微调而增加。我们是第一个研究提示变化如何影响心智理论任务的探索性能的人。我们证明模型的表征对提示变化很敏感，即使这种变化应该是有益的。最后，我们补充了之前关于心智理论任务的激活编辑实验，并表明可以通过控制模型的激活来提高模型的推理性能，而无需训练任何探测器。</li>
</ul>

<h3>Title: Title:
          Entropy-Based Decoding for Retrieval-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Entropy-Based Decoding for Retrieval-Augmented Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the generated responses are negatively influenced by noise from both external and internal knowledge sources. In this paper, we introduce a novel, training-free decoding method guided by entropy considerations to mitigate this issue. Our approach utilizes entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents, thereby enhancing the extraction of relevant information of context. Additionally, it incorporates a contrastive decoding mechanism that contrasts the obtained low-entropy ensemble distribution with the high-entropy distribution derived from the model's internal knowledge across layers, which ensures a greater emphasis on reliable external information. Extensive experiments on open-domain question answering datasets demonstrate the superiority of our method.</li>
<li><strong>摘要：</strong>事实证明，使用检索到的外部知识增强大型语言模型 (LLM) 可有效提高生成响应的事实准确性。尽管检索增强型 LLM 取得了成功，但它仍然面临注意力分散问题，即生成的响应会受到来自外部和内部知识源的噪声的负面影响。在本文中，我们介绍了一种新颖的、无需训练的解码方法，该方法以熵为指导，以缓解此问题。我们的方法利用基于熵的文档并行集成解码来优先处理检索到的文档中的低熵分布，从而增强对上下文相关信息的提取。此外，它还结合了一种对比解码机制，将获得的低熵集成分布与跨层模型内部知识得出的高熵分布进行对比，从而确保更加重视可靠的外部信息。在开放域问答数据集上进行的大量实验证明了我们方法的优越性。</li>
</ul>

<h3>Title: Title:
          LumberChunker: Long-Form Narrative Document Segmentation</h3>
<ul>
<li><strong>Authors: </strong>André V. Duarte, João Marques, Miguel Graça, Miguel Freire, Lei Li, Arlindo L. Oliveira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LumberChunker: Long-Form Narrative Document Segmentation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Modern NLP tasks increasingly rely on dense retrieval methods to access up-to-date and relevant contextual information. We are motivated by the premise that retrieval benefits from segments that can vary in size such that a content's semantic independence is better captured. We propose LumberChunker, a method leveraging an LLM to dynamically segment documents, which iteratively prompts the LLM to identify the point within a group of sequential passages where the content begins to shift. To evaluate our method, we introduce GutenQA, a benchmark with 3000 "needle in a haystack" type of question-answer pairs derived from 100 public domain narrative books available on Project Gutenberg. Our experiments show that LumberChunker not only outperforms the most competitive baseline by 7.37% in retrieval performance (DCG@20) but also that, when integrated into a RAG pipeline, LumberChunker proves to be more effective than other chunking methods and competitive baselines, such as the Gemini 1.5M Pro. Our Code and Data are available at this https URL</li>
<li><strong>摘要：</strong>现代 NLP 任务越来越依赖于密集检索方法来访问最新和相关的上下文信息。我们受到这样一个前提的激励：检索受益于大小可以变化的段，这样可以更好地捕捉内容的语义独立性。我们提出了 LumberChunker，这是一种利用 LLM 动态分段文档的方法，它以迭代方式提示 LLM 识别一组连续段落中内容开始转变的点。为了评估我们的方法，我们引入了 GutenQA，这是一个基准，其中包含 3000 个“大海捞针”类型的问答对，这些问答对来自 Project Gutenberg 上提供的 100 本公共领域叙事书籍。我们的实验表明，LumberChunker 不仅在检索性能 (DCG@20) 方面比最具竞争力的基线高出 7.37%，而且当集成到 RAG 管道中时，LumberChunker 被证明比其他分块方法和竞争基线（如 Gemini 1.5M Pro）更有效。我们的代码和数据可在此 https URL 上获取</li>
</ul>

<h3>Title: Title:
          Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Hierarchical text classification (HTC) is an important task with broad applications, while few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely-ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically-similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.</li>
<li><strong>摘要：</strong>分层文本分类 (HTC) 是一项具有广泛应用的重要任务，而小样本 HTC 最近引起了越来越多的关注。虽然使用大型语言模型 (LLM) 的上下文学习 (ICL) 在小样本学习中取得了显著成功，但由于扩展的分层标签集和极其模糊的标签，它对于 HTC 并不那么有效。在这项工作中，我们引入了第一个基于 ICL 的带有 LLM 的少样本 HTC 框架。我们利用检索数据库来识别相关的演示，并使用迭代策略来管理多层分层标签。具体而言，我们为检索数据库配备了用于输入文本的 HTC 标签感知表示，这是通过对具有掩码语言建模 (MLM)、分层分类 (CLS，专门用于 HTC) 的预训练语言模型进行持续训练来实现的，以及一种新颖的发散对比学习 (DCL，主要针对相邻的语义相似标签) 目标。在三个基准数据集上的实验结果证明了我们方法的卓越性能，并且我们可以在少样本 HTC 中取得最先进的结果。</li>
</ul>

<h3>Title: Title:
          Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Fabio Mercorio, Mario Mezzanzanica, Daniele Potertì, Antonio Serino, Andrea Seveso</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to generate and manipulate human language, highlighting their potential across various applications. Evaluating LLMs in languages other than English is crucial for ensuring their linguistic versatility, cultural relevance, and applicability in diverse global contexts, thus broadening their usability and effectiveness. We tackle this challenge by introducing a structured benchmark using the INVALSI tests, a set of well-established assessments designed to measure educational competencies across Italy. Our study makes three primary contributions: Firstly, we adapt the INVALSI benchmark for automated LLM evaluation, which involves rigorous adaptation of the test format to suit automated processing while retaining the essence of the original tests. Secondly, we provide a detailed assessment of current LLMs, offering a crucial reference point for the academic community. Finally, we visually compare the performance of these models against human results. Additionally, researchers are invited to submit their models for ongoing evaluation, ensuring the benchmark remains a current and valuable resource.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展显著增强了它们生成和操纵人类语言的能力，凸显了它们在各种应用中的潜力。评估非英语语言的 LLM 对于确保其语言的多功能性、文化相关性和在不同全球背景下的适用性至关重要，从而扩大其可用性和有效性。我们通过使用 INVALSI 测试引入结构化基准来应对这一挑战，INVALSI 测试是一套成熟的评估方法，旨在衡量意大利各地的教育能力。我们的研究做出了三个主要贡献：首先，我们调整了 INVALSI 基准以进行自动 LLM 评估，这涉及严格调整测试格式以适应自动化处理，同时保留原始测试的本质。其次，我们对当前的 LLM 进行了详细评估，为学术界提供了一个重要的参考点。最后，我们直观地比较了这些模型的性能与人类结果。此外，研究人员受邀提交他们的模型进行持续评估，确保基准仍然是一个最新且宝贵的资源。</li>
</ul>

<h3>Title: Title:
          Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft</h3>
<ul>
<li><strong>Authors: </strong>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In the Minecraft Collaborative Building Task, two players collaborate: an Architect (A) provides instructions to a Builder (B) to assemble a specified structure using 3D blocks. In this work, we investigate the use of large language models (LLMs) to predict the sequence of actions taken by the Builder. Leveraging LLMs' in-context learning abilities, we use few-shot prompting techniques, that significantly improve performance over baseline methods. Additionally, we present a detailed analysis of the gaps in performance for future work</li>
<li><strong>摘要：</strong>在 Minecraft 协作建造任务中，两名玩家合作：建筑师 (A) 向建造者 (B) 提供指令，使用 3D 积木组装指定结构。在这项工作中，我们研究了使用大型语言模型 (LLM) 来预测建造者采取的行动顺序。利用 LLM 的上下文学习能力，我们使用少量提示技术，与基线方法相比，性能显著提高。此外，我们还对未来工作中的表现差距进行了详细分析</li>
</ul>

<h3>Title: Title:
          The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale</h3>
<ul>
<li><strong>Authors: </strong>Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的性能在很大程度上取决于其预训练数据集的质量和大小。然而，像 Llama 3 和 Mixtral 这样的最先进的开放式 LLM 的预训练数据集并不公开，而且人们对它们的创建方式知之甚少。在这项工作中，我们引入了 FineWeb，这是一个 15 万亿个标记的数据集，源自 96 个 Common Crawl 快照，它产生的 LLM 比其他开放式预训练数据集性能更好。为了加深对如何最好地策划高质量预训练数据集的理解，我们仔细记录并消除了 FineWeb 中使用的所有设计选择，包括对重复数据删除和过滤策略的深入研究。此外，我们引入了 FineWeb-Edu，这是一个从 FineWeb 过滤的 1.3 万亿个教育文本标记集合。在 FineWeb-Edu 上预训练的 LLM 在知识和推理密集型基准（如 MMLU 和 ARC）上表现出显著更好的性能。除了我们的数据集之外，我们还公开发布了我们的数据管理代码库和在消融实验期间训练的所有模型。</li>
</ul>

<h3>Title: Title:
          Multi-property Steering of Large Language Models with Dynamic Activation Composition</h3>
<ul>
<li><strong>Authors: </strong>Daniel Scalena, Gabriele Sarti, Malvina Nissim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multi-property Steering of Large Language Models with Dynamic Activation Composition(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Activation steering methods were shown to be effective in conditioning language model generation by additively intervening over models' intermediate representations. However, the evaluation of these techniques has so far been limited to single conditioning properties and synthetic settings. In this work, we conduct a comprehensive evaluation of various activation steering strategies, highlighting the property-dependent nature of optimal parameters to ensure a robust effect throughout generation. To address this issue, we propose Dynamic Activation Composition, an information-theoretic approach to modulate the steering intensity of one or more properties throughout generation. Our experiments on multi-property steering show that our method successfully maintains high conditioning while minimizing the impact of conditioning on generation fluency.</li>
<li><strong>摘要：</strong>激活控制方法已被证明能够有效地调节语言模型生成，方法是通过对模型的中间表示进行附加干预。然而，到目前为止，对这些技术的评估仅限于单一的调节属性和合成设置。在这项工作中，我们对各种激活控制策略进行了全面评估，强调了最佳参数的属性依赖性，以确保在整个生成过程中产生稳健的效果。为了解决这个问题，我们提出了动态激活组合，这是一种信息理论方法，用于在整个生成过程中调节一个或多个属性的控制强度。我们对多属性控制的实验表明，我们的方法成功地保持了高调节性，同时最大限度地减少了调节对生成流畅性的影响。</li>
</ul>

<h3>Title: Title:
          FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity in French Texts</h3>
<ul>
<li><strong>Authors: </strong>Caroline Brun, Vassilina Nikoulina</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity in French Texts(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly popular but are also prone to generating bias, toxic or harmful language, which can have detrimental effects on individuals and communities. Although most efforts is put to assess and mitigate toxicity in generated content, it is primarily concentrated on English, while it's essential to consider other languages as well. For addressing this issue, we create and release FrenchToxicityPrompts, a dataset of 50K naturally occurring French prompts and their continuations, annotated with toxicity scores from a widely used toxicity classifier. We evaluate 14 different models from four prevalent open-sourced families of LLMs against our dataset to assess their potential toxicity across various dimensions. We hope that our contribution will foster future research on toxicity detection and mitigation beyond Englis</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越受欢迎，但也容易产生偏见、有毒或有害的语言，这可能会对个人和社区产生不利影响。尽管大多数努力都是为了评估和减轻生成内容中的毒性，但主要集中在英语上，但也必须考虑其他语言。为了解决这个问题，我们创建并发布了 FrenchToxicityPrompts，这是一个包含 50K 个自然发生的法语提示及其延续的数据集，并标注了广泛使用的毒性分类器的毒性分数。我们根据我们的数据集评估了来自四个流行的开源 LLM 系列的 14 种不同模型，以评估它们在各个维度上的潜在毒性。我们希望我们的贡献能够促进未来对英语以外的毒性检测和缓解的研究</li>
</ul>

<h3>Title: Title:
          Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for Querying and Classifying IoT Threats</h3>
<ul>
<li><strong>Authors: </strong>Ryan Pavlich, Nima Ebadi, Richard Tarbell, Billy Linares, Adrian Tan, Rachael Humphreys, Jayanta Kumar Das, Rambod Ghandiparsi, Hannah Haley, Jerris George, Rocky Slavin, Kim-Kwang Raymond Choo, Glenn Dietrich, Anthony Rios</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for Querying and Classifying IoT Threats(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recognizing the promise of natural language interfaces to databases, prior studies have emphasized the development of text-to-SQL systems. While substantial progress has been made in this field, existing research has concentrated on generating SQL statements from text queries. The broader challenge, however, lies in inferring new information about the returned data. Our research makes two major contributions to address this gap. First, we introduce a novel Internet-of-Things (IoT) text-to-SQL dataset comprising 10,985 text-SQL pairs and 239,398 rows of network traffic activity. The dataset contains additional query types limited in prior text-to-SQL datasets, notably temporal-related queries. Our dataset is sourced from a smart building's IoT ecosystem exploring sensor read and network traffic data. Second, our dataset allows two-stage processing, where the returned data (network traffic) from a generated SQL can be categorized as malicious or not. Our results show that joint training to query and infer information about the data can improve overall text-to-SQL performance, nearly matching substantially larger models. We also show that current large language models (e.g., GPT3.5) struggle to infer new information about returned data, thus our dataset provides a novel test bed for integrating complex domain-specific reasoning into LLMs.</li>
<li><strong>摘要：</strong>认识到自然语言接口与数据库的前景，先前的研究强调了文本到 SQL 系统的开发。虽然该领域已经取得了实质性进展，但现有研究集中在从文本查询生成 SQL 语句。然而，更广泛的挑战在于推断有关返回数据的新信息。我们的研究为解决这一差距做出了两大贡献。首先，我们引入了一个新颖的物联网 (IoT) 文本到 SQL 数据集，包含 10,985 个文本-SQL 对和 239,398 行网络流量活动。该数据集包含先前文本到 SQL 数据集中有限的其他查询类型，特别是与时间相关的查询。我们的数据集来自智能建筑的 IoT 生态系统，探索传感器读取和网络流量数据。其次，我们的数据集允许两阶段处理，其中生成的 SQL 返回的数据（网络流量）可以归类为恶意或非恶意。我们的结果表明，联合训练查询和推断有关数据的信息可以提高整体文本到 SQL 性能，几乎可以匹配更大的模型。我们还表明，当前的大型语言模型（例如 GPT3.5）难以推断有关返回数据的新信息，因此我们的数据集为将复杂的特定领域推理集成到 LLM 中提供了一个新的测试平台。</li>
</ul>

<h3>Title: Title:
          LongIns: A Challenging Long-context Instruction-based Exam for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, Ge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LongIns: A Challenging Long-context Instruction-based Exam for LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的长上下文能力是近年来的热门话题。为了评估 LLM 在不同场景下的表现，出现了各种评估基准。然而，由于这些基准大多侧重于识别回答问题的关键信息，这主要需要 LLM 的检索能力，因此这些基准只能部分代表 LLM 从大量信息中进行推理的性能。同时，尽管 LLM 通常声称具有 32k、128k、200k 甚至更长的上下文窗口，但这些基准未能揭示这些 LLM 实际支持的长度。为了解决这些问题，我们提出了 LongIns 基准数据集，这是一个具有挑战性的 LLM 长上下文基于指令的考试，它基于现有的指令数据集构建。具体来说，在我们的 LongIns 中，我们引入了三种评估设置：全局指令和单任务 (GIST)、局部指令和单任务 (LIST) 以及局部指令和多任务 (LIMT)。基于 LongIns，我们对现有的 LLM 进行了全面的评估，并得到以下重要发现：（1）。表现最好的 GPT-4 具有 128k 上下文长度，在我们的 LongIns 中 16k 的评估上下文窗口上表现不佳。（2）。对于许多现有 LLM 的多跳推理能力，在短上下文窗口（小于 4k）下仍需要做出巨大努力。</li>
</ul>

<h3>Title: Title:
          "Seeing the Big through the Small": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?</h3>
<ul>
<li><strong>Authors: </strong>Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          "Seeing the Big through the Small": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Human label variation (HLV) is a valuable source of information that arises when multiple human annotators provide different labels for valid reasons. In Natural Language Inference (NLI) earlier approaches to capturing HLV involve either collecting annotations from many crowd workers to represent human judgment distribution (HJD) or use expert linguists to provide detailed explanations for their chosen labels. While the former method provides denser HJD information, obtaining it is resource-intensive. In contrast, the latter offers richer textual information but it is challenging to scale up to many human judges. Besides, large language models (LLMs) are increasingly used as evaluators (``LLM judges'') but with mixed results, and few works aim to study HJDs. This study proposes to exploit LLMs to approximate HJDs using a small number of expert labels and explanations. Our experiments show that a few explanations significantly improve LLMs' ability to approximate HJDs with and without explicit labels, thereby providing a solution to scale up annotations for HJD. However, fine-tuning smaller soft-label aware models with the LLM-generated model judgment distributions (MJDs) presents partially inconsistent results: while similar in distance, their resulting fine-tuned models and visualized distributions differ substantially. We show the importance of complementing instance-level distance measures with a global-level shape metric and visualization to more effectively evaluate MJDs against human judgment distributions.</li>
<li><strong>摘要：</strong>人类标签变异 (HLV) 是一种宝贵的信息来源，当多个人类注释者出于正当理由提供不同的标签时，就会出现这种变异。在自然语言推理 (NLI) 中，早期捕获 HLV 的方法包括从许多众包工作者那里收集注释以表示人类判断分布 (HJD)，或者使用专业语言学家为他们选择的标签提供详细解释。虽然前一种方法提供了更密集的 HJD 信息，但获取这些信息需要大量资源。相比之下，后者提供了更丰富的文本信息，但很难扩展到许多人类判断者。此外，大型语言模型 (LLM) 越来越多地被用作评估器（“LLM 判断者”），但结果好坏参半，很少有研究旨在研究 HJD。本研究建议利用 LLM 来近似 HJD，使用少量专家标签和解释。我们的实验表明，一些解释可以显著提高 LLM 近似有无明确标签的 HJD 的能力，从而为扩大 HJD 的注释提供了一种解决方案。然而，使用 LLM 生成的模型判断分布 (MJD) 对较小的软标签感知模型进行微调会产生部分不一致的结果：虽然距离相似，但它们产生的微调模型和可视化分布却大不相同。我们展示了使用全局级形状度量和可视化来补充实例级距离度量的重要性，以便更有效地根据人类判断分布评估 MJD。</li>
</ul>

<h3>Title: Title:
          Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wen, Yu Yang, Jiannong Cao, Haoming Sun, Ruosong Yang, Shuaiqi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) appear to behave increasingly human-like in text-based interactions, more and more researchers become interested in investigating personality in LLMs. However, the diversity of psychological personality research and the rapid development of LLMs have led to a broad yet fragmented landscape of studies in this interdisciplinary field. Extensive studies across different research focuses, different personality psychometrics, and different LLMs make it challenging to have a holistic overview and further pose difficulties in applying findings to real-world applications. In this paper, we present a comprehensive review by categorizing current studies into three research problems: self-assessment, exhibition, and recognition, based on the intrinsic characteristics and external manifestations of personality in LLMs. For each problem, we provide a thorough analysis and conduct in-depth comparisons of their corresponding solutions. Besides, we summarize research findings and open challenges from current studies and further discuss their underlying causes. We also collect extensive publicly available resources to facilitate interested researchers and developers. Lastly, we discuss the potential future research directions and application scenarios. Our paper is the first comprehensive survey of up-to-date literature on personality in LLMs. By presenting a clear taxonomy, in-depth analysis, promising future directions, and extensive resource collections, we aim to provide a better understanding and facilitate further advancements in this emerging field.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在基于文本的交互中表现得越来越像人类，越来越多的研究人员对研究 LLM 中的个性产生了兴趣。然而，心理人格研究的多样性和 LLM 的快速发展导致这一跨学科领域的研究范围广泛而又分散。跨不同研究重点、不同人格心理测量和不同 LLM 的广泛研究使得很难进行整体概述，并进一步给将研究结果应用于现实世界的应用带来困难。在本文中，我们根据 LLM 中个性的内在特征和外在表现，将当前的研究分为三个研究问题：自我评估、表现和识别，进行了全面的综述。对于每个问题，我们都进行了彻底的分析，并对其相应的解决方案进行了深入的比较。此外，我们总结了当前研究的发现和未解决的挑战，并进一步讨论了其根本原因。我们还收集了大量的公开资源，以方便感兴趣的研究人员和开发人员。最后，我们讨论了未来的潜在研究方向和应用场景。本文首次全面调查了法学硕士人格研究的最新文献。通过提供清晰的分类、深入的分析、有前景的未来方向和广泛的资源集合，我们旨在更好地理解这一新兴领域并促进其进一步发展。</li>
</ul>

<h3>Title: Title:
          CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference</h3>
<ul>
<li><strong>Authors: </strong>Erxin Yu, Jing Li, Ming Liao, Siqi Wang, Zuchen Gao, Fei Mi, Lanqing Hong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的不断发展，确保其安全性仍然是一个关键的研究问题。以前针对 LLM 安全性的红队方法主要侧重于单一提示攻击或目标劫持。据我们所知，我们是第一个研究多轮对话共指中 LLM 安全性的人。我们创建了一个包含 14 个类别的 1,400 个问题的数据集，每个类别都包含多轮共指安全攻击。然后，我们对五个广泛使用的开源 LLM 进行了详细评估。结果表明，在多轮共指安全攻击下，LLaMA2-Chat-7b 模型的攻击成功率最高为 56%，而 Mistral-7B-Instruct 模型的攻击成功率最低为 13.9%。这些发现凸显了对话共指交互过程中 LLM 的安全漏洞。</li>
</ul>

<h3>Title: Title:
          Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Pangakis, Samuel Wolken</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Computational social science (CSS) practitioners often rely on human-labeled data to fine-tune supervised text classifiers. We assess the potential for researchers to augment or replace human-generated training data with surrogate training labels from generative large language models (LLMs). We introduce a recommended workflow and test this LLM application by replicating 14 classification tasks and measuring performance. We employ a novel corpus of English-language text classification data sets from recent CSS articles in high-impact journals. Because these data sets are stored in password-protected archives, our analyses are less prone to issues of contamination. For each task, we compare supervised classifiers fine-tuned using GPT-4 labels against classifiers fine-tuned with human annotations and against labels from GPT-4 and Mistral-7B with few-shot in-context learning. Our findings indicate that supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators. Fine-tuning models using LLM-generated labels can be a fast, efficient and cost-effective method of building supervised text classifiers.</li>
<li><strong>摘要：</strong>计算社会科学 (CSS) 从业者通常依靠人工标记的数据来微调监督文本分类器。我们评估了研究人员使用来自生成式大型语言模型 (LLM) 的替代训练标签来增强或替换人工生成的训练数据的可能性。我们引入了推荐的工作流程，并通过复制 14 个分类任务并测量性能来测试此 LLM 应用程序。我们使用了来自高影响力期刊中近期 CSS 文章的英语文本分类数据集的新语料库。由于这些数据集存储在受密码保护的档案中，因此我们的分析不太容易受到污染问题的影响。对于每个任务，我们将使用 GPT-4 标签微调的监督分类器与使用人工注释微调的分类器以及使用少量上下文学习的 GPT-4 和 Mistral-7B 的标签进行比较。我们的研究结果表明，在 LLM 生成的标签上微调的监督分类模型的表现与使用人工注释者的标签微调的模型相当。使用 LLM 生成的标签进行微调模型可以成为构建监督文本分类器的一种快速、高效且经济的方法。</li>
</ul>

<h3>Title: Title:
          Banishing LLM Hallucinations Requires Rethinking Generalization</h3>
<ul>
<li><strong>Authors: </strong>Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, Gregory Diamos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Banishing LLM Hallucinations Requires Rethinking Generalization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 具有强大的聊天、编码和推理能力，但它们经常会产生幻觉。传统观点认为，幻觉是创造力与事实性之间平衡的结果，通过将 LLM 置于外部知识源中，可以减轻但不能消除这种平衡。通过大量系统实验，我们表明这些传统方法无法解释 LLM 在实践中产生幻觉的原因。具体而言，我们表明，通过大量记忆专家混合 (MoME) 增强的 LLM 可以轻松记住大量随机数数据集。我们通过理论构建证实了这些实验结果，该理论表明，当训练损失超过阈值时，经过训练以预测下一个标记的简单神经网络会产生幻觉，就像在互联网规模数据上进行训练时通常所做的那样。我们通过与传统的缓解幻觉的检索方法进行比较来解释我们的发现。我们利用我们的发现设计了第一代消除幻觉的模型——Lamini-1——该模型将事实存储在数百万个记忆专家的大量混合中，并动态检索这些专家。</li>
</ul>

<h3>Title: Title:
          LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic</h3>
<ul>
<li><strong>Authors: </strong>Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, Jennifer Chu-Carroll, David Melville, David Ferrucci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the logical reasoning capabilities of Large Language Models (LLMs), by combining them with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic method where the LLM Actor generates declarative logic programs along with tests for semantic correctness, while the Automated Reasoning Critic evaluates the code, runs the tests and provides feedback on test failures for iterative refinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a new state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests complex logical reasoning capabilities. Our experiments demonstrate significant improvements over LLM-only baselines, highlighting the importance of logic test generation and iterative self-refinement. We achieve our best result using a fully automated self-supervised training loop where the Actor is trained on end-to-end dialog traces with Critic feedback. We discuss potential enhancements and provide a detailed error analysis, showcasing the robustness and efficacy of LLM-ARC for complex natural language reasoning tasks.</li>
<li><strong>摘要：</strong>我们引入了 LLM-ARC，这是一种神经符号框架，旨在通过将其与自动推理评论家 (ARC) 相结合来增强大型语言模型 (LLM) 的逻辑推理能力。LLM-ARC 采用 Actor-Critic 方法，其中 LLM Actor 生成声明性逻辑程序以及语义正确性测试，而自动推理评论家则评估代码、运行测试并提供测试失败的反馈以进行迭代改进。使用答案集编程 (ASP) 实现的 LLM-ARC 在测试复杂逻辑推理能力的 FOLIO 基准上实现了 88.32% 的最新准确率。我们的实验表明，与仅使用 LLM 的基线相比，有显着的改进，突出了逻辑测试生成和迭代自我改进的重要性。我们使用全自动自监督训练循环实现了最佳结果，其中 Actor 在端到端对话跟踪上进行训练，并提供评论家反馈。我们讨论了潜在的增强功能并提供了详细的错误分析，展示了 LLM-ARC 对于复杂自然语言推理任务的稳健性和有效性。</li>
</ul>

<h3>Title: Title:
          Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuan Li, Yue Huang, Hongyi Wang, Xiangliang Zhang, James Zou, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出卓越的任务解决能力，越来越多地扮演类似人类助手的角色。LLM 更广泛地融入社会，引发了人们对它们是否表现出心理属性以及这些属性是否是可以加深对其行为理解的稳定探究的兴趣。受心理测量学的启发，本文提出了一个研究 LLM 心理学的框架，包括心理维度识别、评估数据集管理和结果验证评估。根据这个框架，我们为 LLM 引入了一个全面的心理测量基准，涵盖六个心理维度：性格、价值观、情感、心智理论、动机和智力。该基准包括十三个数据集，具有不同的场景和项目类型。我们的研究结果表明，LLM 表现出广泛的心理属性。我们还发现了 LLM 自我报告的特征与其在现实场景中的行为之间的差异。本文展示了对 LLM 的全面心理测量评估，为可靠的评估和人工智能和社会科学中的潜在应用提供了见解。</li>
</ul>

<h3>Title: Title:
          VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem.</li>
<li><strong>摘要：</strong>随着大型语言模型在传统基准测试中取得令人印象深刻的成绩，越来越多的研究人员开始担心预训练期间的基准数据泄露，即通常所说的数据污染问题。为了确保公平评估，最近的基准测试只发布训练集和验证集，测试集标签保持闭源。他们要求任何希望评估其语言模型的人提交模型的预测以供集中处理，然后在他们的排行榜上发布模型的结果。然而，这种提交过程效率低下，阻碍了有效的错误分析。为了解决这个问题，我们建议对基准进行变量化并动态评估语言模型。具体来说，我们从每个测试用例中提取变量并为每个变量定义一个值范围。对于每次评估，我们从这些值范围中抽取新值以创建唯一的测试用例，从而确保每次都有新的评估。我们将这种变量扰动方法应用于四个数据集：GSM8K、ARC、CommonsenseQA 和 TruthfulQA，涵盖数学生成和多项选择任务。我们的实验结果表明，这种方法可以更准确地评估语言模型的真实能力，有效地缓解污染问题。</li>
</ul>

<h3>Title: Title:
          From Distributional to Overton Pluralism: Investigating Large Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Thom Lake, Eunsol Choi, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          From Distributional to Overton Pluralism: Investigating Large Language Model Alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at this https URL.</li>
<li><strong>摘要：</strong>对齐过程会改变大型语言模型 (LLM) 输出分布的几个属性。我们分析了对齐后 LLM 响应分布变化的两个方面。首先，我们重新检查了之前报告的对齐后响应多样性的减少。我们的分析表明，响应多样性的明显下降主要归因于质量控制和信息聚合。对齐会抑制不相关和无用的内容，同时将输出分布转向更长的响应，这些响应涵盖了来自基础 LLM 的多个响应的信息，本质上是在单个响应中呈现不同的信息。由于没有发现对齐会抑制有用信息的证据，因此很自然地会问相反的问题：对齐模型是否会显示无法从基础模型中恢复的信息？我们的第二项调查表明情况并非如此，对齐模型的行为可以从基础模型中恢复，而无需进行微调。上下文示例和关于响应内容的低分辨率语义提示的组合可以从基础 LLM 中引出响应，这些响应与对齐调整后的 LLM 响应一样相似，就像对齐调整后的 LLM 响应彼此一样。综合起来，这些结果表明，当前的对齐技术可以捕捉到但无法扩展类似于助手的基本 LLM 行为的有用子集，这为表面对齐假说提供了进一步的证据。它们还表明，上下文对齐可以作为一种无需微调即可模仿对齐 LLM 的策略，效果出奇地好。我们的代码和数据可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users</h3>
<ul>
<li><strong>Authors: </strong>Elinor Poole-Dayan, Deb Roy, Jad Kabbara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>While state-of-the-art Large Language Models (LLMs) have shown impressive performance on many tasks, there has been extensive research on undesirable model behavior such as hallucinations and bias. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users.</li>
<li><strong>摘要：</strong>虽然最先进的大型语言模型 (LLM) 在许多任务上都表现出色，但人们对模型的不良行为（如幻觉和偏见）进行了广泛的研究。在这项工作中，我们研究了 LLM 响应质量在信息准确性、真实性和拒绝方面如何根据三个用户特征而变化：英语水平、教育水平和原籍国。我们对三个最先进的 LLM 和两个不同的数据集进行了广泛的实验，以真实性和事实性为目标。我们的研究结果表明，最先进的 LLM 中的不良行为更多地发生在英语水平较低、教育水平较低和来自美国以外的用户身上，这使得这些模型成为最脆弱用户的不可靠信息来源。</li>
</ul>

<h3>Title: Title:
          Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model</h3>
<ul>
<li><strong>Authors: </strong>Fei Xia, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Taxonomies, which organize domain concepts into hierarchical structures, are crucial for building knowledge systems and downstream applications. As domain knowledge evolves, taxonomies need to be continuously updated to include new concepts. Previous approaches have mainly focused on adding concepts to the leaf nodes of the existing hierarchical tree, which does not fully utilize the taxonomy's knowledge and is unable to update the original taxonomy structure (usually involving non-leaf nodes). In this paper, we propose a two-stage method called ATTEMPT for taxonomy completion. Our method inserts new concepts into the correct position by finding a parent node and labeling child nodes. Specifically, by combining local nodes with prompts to generate natural sentences, we take advantage of pre-trained language models for hypernym/hyponymy recognition. Experimental results on two public datasets (including six domains) show that ATTEMPT performs best on both taxonomy completion and extension tasks, surpassing existing methods.</li>
<li><strong>摘要：</strong>分类法将领域概念组织成层级结构，对于知识体系和下游应用的构建至关重要。随着领域知识的发展，分类法需要不断更新以纳入新概念。之前的方法主要侧重于将概念添加到现有层级树的叶节点，这种方法没有充分利用分类法的知识，也无法更新原有的分类结构（通常涉及非叶节点）。在本文中，我们提出了一种两阶段的分类法补全方法 ATTEMPT。我们的方法通过寻找父节点和标记子节点将新概念插入到正确的位置。具体而言，通过将本地节点与提示相结合来生成自然句子，我们利用预训练的语言模型进行上下义词识别。在两个公开数据集（包含六个领域）上的实验结果表明，ATTEMPT 在分类法补全和扩展任务上均表现最佳，超越了现有方法。</li>
</ul>

<h3>Title: Title:
          Following Length Constraints in Instructions</h3>
<ul>
<li><strong>Authors: </strong>Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, Jing Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Following Length Constraints in Instructions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Aligned instruction following models can better fulfill user requests than their unaligned counterparts. However, it has been shown that there is a length bias in evaluation of such models, and that training algorithms tend to exploit this bias by learning longer responses. In this work we show how to train models that can be controlled at inference time with instructions containing desired length constraints. Such models are superior in length instructed evaluations, outperforming standard instruction following models such as GPT4, Llama 3 and Mixtral.</li>
<li><strong>摘要：</strong>与未对齐的模型相比，对齐的指令遵循模型可以更好地满足用户请求。然而，事实证明，在评估此类模型时存在长度偏差，并且训练算法倾向于通过学习更长的响应来利用这种偏差。在这项工作中，我们展示了如何训练可以在推理时使用包含所需长度约束的指令进行控制的模型。此类模型在长度指导评估方面表现出色，优于 GPT4、Llama 3 和 Mixtral 等标准指令遵循模型。</li>
</ul>

<h3>Title: Title:
          Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon</h3>
<ul>
<li><strong>Authors: </strong>USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir S V, Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, Naomi Saphra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, we break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. We demonstrate the usefulness of our taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, we find that different factors influence the likelihood of memorization differently depending on the taxonomic category.</li>
<li><strong>摘要：</strong>语言模型中的记忆通常被视为一种同质现象，忽略了记忆数据的细节。相反，我们将记忆建模为一组复杂因素的影响，这些因素描述每个样本并将其与模型和语料库联系起来。为了围绕这些因素建立直觉，我们将记忆分解为一个分类法：背诵高度重复的序列、重建固有可预测的序列以及回忆两者皆非的序列。我们通过使用我们的分类法构建记忆预测模型来证明其实用性。通过分析依赖关系并检查预测模型的权重，我们发现不同的因素对记忆可能性的影响取决于分类类别的不同。</li>
</ul>

<h3>Title: Title:
          Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language</h3>
<ul>
<li><strong>Authors: </strong>Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda - all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive text - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase. To this end, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of a short text and of a text rewritten by an LLM to amplify or diminish persuasive language. We multi-annotate the pairs on a relative scale for persuasive language. This data is not only a valuable resource in itself, but we also show that it can be used to train a regression model to predict a score of persuasive language between text pairs. This model can score and benchmark new LLMs across domains, thereby facilitating the comparison of different LLMs. Finally, we discuss effects observed for different system prompts. Notably, we find that different 'personas' in the system prompt of LLaMA3 change the persuasive language in the text substantially, even when only instructed to paraphrase. These findings underscore the importance of investigating persuasive language in LLM generated text.</li>
<li><strong>摘要：</strong>我们接触到许多试图影响我们的信息，例如挑逗性消息、辩论、政治性新闻和宣传——所有这些都使用了说服性语言。随着最近对大型语言模型 (LLM) 的兴趣，我们研究了 LLM 生成说服性文本的能力。与专注于特定领域或说服类型的先前研究相反，我们在各个领域进行了一项一般性研究，以衡量和评估 LLM 生成说服性文本的程度——无论是明确指示重写文本以增加或减少说服力，还是仅指示改写。为此，我们构建了一个新的数据集 Persuasive-Pairs，每个对都由一个短文本和一个由 LLM 重写的文本组成，以扩大或削弱说服性语言。我们根据说服性语言的相对比例对这些对进行多重注释。这些数据不仅本身是一种宝贵的资源，我们还表明，它可用于训练回归模型，以预测文本对之间的说服性语言得分。该模型可以对跨领域的新 LLM 进行评分和基准测试，从而促进不同 LLM 的比较。最后，我们讨论了不同系统提示所观察到的效果。值得注意的是，我们发现 LLaMA3 系统提示中的不同“角色”会显著改变文本中的说服性语言，即使仅指示改述。这些发现强调了研究 LLM 生成文本中的说服性语言的重要性。</li>
</ul>

<h3>Title: Title:
          Accelerating Clinical Evidence Synthesis with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zifeng Wang, Lang Cao, Benjamin Danek, Yichi Zhang, Qiao Jin, Zhiyong Lu, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Accelerating Clinical Evidence Synthesis with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Automatic medical discovery by AI is a dream of many. One step toward that goal is to create an AI model to understand clinical studies and synthesize clinical evidence from the literature. Clinical evidence synthesis currently relies on systematic reviews of clinical trials and retrospective analyses from medical literature. However, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating evidence. We introduce TrialMind, a generative AI-based pipeline for conducting medical systematic reviews, encompassing study search, screening, and data extraction phases. We utilize large language models (LLMs) to drive each pipeline component while incorporating human expert oversight to minimize errors. To facilitate evaluation, we also create a benchmark dataset TrialReviewBench, a custom dataset with 870 annotated clinical studies from 25 meta-analysis papers across various medical treatments. Our results demonstrate that TrialMind significantly improves the literature review process, achieving high recall rates (0.897-1.000) in study searching from over 20 million PubMed studies and outperforming traditional language model embeddings-based methods in screening (Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses direct GPT-4 performance in result extraction, with accuracy ranging from 0.65 to 0.84. We also support clinical evidence synthesis in forest plots, as validated by eight human annotators who preferred TrialMind over the GPT-4 baseline with a winning rate of 62.5%-100% across the involved reviews. Our findings suggest that an LLM-based clinical evidence synthesis approach, such as TrialMind, can enable reliable and high-quality clinical evidence synthesis to improve clinical research efficiency.</li>
<li><strong>摘要：</strong>通过人工智能自动进行医学发现是许多人的梦想。实现这一目标的一步是创建一个人工智能模型来理解临床研究并从文献中综合临床证据。临床证据的综合目前依赖于临床试验的系统评价和医学文献的回顾性分析。然而，出版物的迅速扩张对有效识别、总结和更新证据提出了挑战。我们推出了 TrialMind，这是一个基于人工智能的生成式流程，用于进行医学系统评价，包括研究搜索​​、筛选和数据提取阶段。我们利用大型语言模型 (LLM) 来驱动每个流程组件，同时结合人类专家的监督以最大限度地减少错误。为了便于评估，我们还创建了一个基准数据集 TrialReviewBench，这是一个自定义数据集，包含来自 25 篇荟萃分析论文的 870 项带注释的临床研究，涉及各种医疗治疗。我们的结果表明，TrialMind 显著改善了文献综述流程，在从 2000 多万篇 PubMed 研究中搜索时实现了高召回率（0.897-1.000），并且在筛选方面优于传统的基于语言模型嵌入的方法（Recall@20 为 0.227-0.246 vs. 0.000-0.102）。此外，我们的方法在结果提取方面超越了直接 GPT-4 的性能，准确率范围从 0.65 到 0.84。我们还支持森林图中的临床证据合成，经八位人类注释者验证，他们更喜欢 TrialMind 而不是 GPT-4 基线，在涉及的评审中胜率为 62.5%-100%。我们的研究结果表明，基于 LLM 的临床证据合成方法（如 TrialMind）可以实现可靠且高质量的临床证据合成，从而提高临床研究效率。</li>
</ul>

<h3>Title: Title:
          CaLMQA: Exploring culturally specific long-form question answering across 23 languages</h3>
<ul>
<li><strong>Authors: </strong>Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CaLMQA: Exploring culturally specific long-form question answering across 23 languages(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are commonly used for long-form question answering, which requires them to generate paragraph-length answers to complex questions. While long-form QA has been well-studied in English via many different datasets and evaluation metrics, this research has not been extended to cover most other languages. To bridge this gap, we introduce CaLMQA, a collection of 2.6K complex questions spanning 23 languages, including under-resourced, rarely-studied languages such as Fijian and Kirundi. Our dataset includes both naturally-occurring questions collected from community web forums as well as questions written by native speakers, whom we hire for this purpose. Our process yields diverse, complex questions that reflect cultural topics (e.g. traditions, laws, news) and the language usage of native speakers. We conduct automatic evaluation across a suite of open- and closed-source models using our novel metric CaLMScore, which detects incorrect language and token repetitions in answers, and observe that the quality of LLM-generated answers degrades significantly for some low-resource languages. We perform human evaluation on a subset of models and see that model performance is significantly worse for culturally specific questions than for culturally agnostic questions. Our findings highlight the need for further research in LLM multilingual capabilities and non-English LFQA evaluation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常用于长篇问答，这要求它们为复杂问题生成段落长度的答案。虽然长篇问答在英语中已经通过许多不同的数据集和评估指标得到了很好的研究，但这项研究尚未扩展到大多数其他语言。为了弥补这一差距，我们推出了 CaLMQA，这是一个包含 2.6K 个复杂问题的集合，涵盖 23 种语言，包括资源不足、很少研究的语言，如斐济语和基隆迪语。我们的数据集包括从社区网络论坛收集的自然发生的问题以及我们为此目的聘请的母语人士编写的问题。我们的流程产生了各种复杂的问题，反映了文化主题（例如传统、法律、新闻）和母语人士的语言使用情况。我们使用我们新颖的指标 CaLMScore 对一套开源和闭源模型进行自动评估，该指标可检测答案中的不正确语言和标记重复，并观察到 ​​LLM 生成的答案的质量对于某些资源不足的语言会显著下降。我们对部分模型进行了人工评估，发现模型在文化特定问题上的表现明显比在文化无关问题上的表现差。我们的研究结果强调了进一步研究 LLM 多语言能力和非英语 LFQA 评估的必要性。</li>
</ul>

<h3>Title: Title:
          BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) possess extensive parametric knowledge, but this knowledge is difficult to update with new information because retraining is very expensive and infeasible for closed-source models. Knowledge editing (KE) has emerged as a viable solution for updating the knowledge of LLMs without compromising their overall performance. On-the-fly KE methods, inspired by in-context learning (ICL), have shown great promise and allow LLMs to be treated as black boxes. In the past, KE was primarily employed in English contexts, whereas the potential for cross-lingual KE in current English-centric LLMs has not been fully explored. To foster more research in this direction, we introduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse languages across three KE task types. We also propose a gradient-free KE method called Multilingual In-context Knowledge Editing (MIKE) and evaluate it on BMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms of reliability, generality, locality, and portability, offering valuable insights and a framework for future research in cross-lingual KE. Our code and data are publicly accessible via the anonymous repository at https://anonymous.4open.science/r/MIKE.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 拥有广泛的参数知识，但这些知识很难用新信息更新，因为重新训练非常昂贵，并且对于闭源模型来说是不可行的。知识编辑 (KE) 已成为一种可行的解决方案，可在不影响其整体性能的情况下更新 LLM 的知识。受上下文学习 (ICL) 启发的即时 KE 方法已显示出巨大的前景，并允许将 LLM 视为黑匣子。过去，KE 主要用于英语环境，而当前以英语为中心的 LLM 中跨语言 KE 的潜力尚未得到充分探索。为了促进这方面的更多研究，我们引入了 BMIKE-53 基准，用于评估三种 KE 任务类型中 53 种不同语言的跨语言 KE。我们还提出了一种无梯度 KE 方法，称为多语言上下文知识编辑 (MIKE)，并在 BMIKE-53 上对其进行评估。我们的评估侧重于跨语言知识转移的可靠性、通用性、局部性和可移植性，为跨语言知识转移的未来研究提供了宝贵的见解和框架。我们的代码和数据可通过匿名存储库 https://anonymous.4open.science/r/MIKE 公开访问。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
