<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-13</h1>
<h3>Title: Exposing Product Bias in LLM Investment Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Zhi, Xiaoyu Zhang, Longtian Wang, Shumin Jiang, Shiqing Ma, Xiaohong Guan, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08750">https://arxiv.org/abs/2503.08750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08750">https://arxiv.org/pdf/2503.08750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08750]] Exposing Product Bias in LLM Investment Recommendation(https://arxiv.org/abs/2503.08750)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), as a new generation of recommendation engines, possess powerful summarization and data analysis capabilities, surpassing traditional recommendation systems in both scope and performance. One promising application is investment recommendation. In this paper, we reveal a novel product bias in LLM investment recommendation, where LLMs exhibit systematic preferences for specific products. Such preferences can subtly influence user investment decisions, potentially leading to inflated valuations of products and financial bubbles, posing risks to both individual investors and market stability. To comprehensively study the product bias, we develop an automated pipeline to create a dataset of 567,000 samples across five asset classes (stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this dataset, we present the bf first study on product bias in LLM investment recommendations. Our findings reveal that LLMs exhibit clear product preferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from Microsoft). Notably, this bias persists even after applying debiasing techniques. We urge AI researchers to take heed of the product bias in LLM investment recommendations and its implications, ensuring fairness and security in the digital space and market.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）作为新一代推荐引擎具有强大的摘要和数据分析功能，超过了范围和性能的传统推荐系统。一个有希望的申请是投资建议。在本文中，我们在LLM投资建议中揭示了一种新的产品偏见，其中LLMS对特定产品表现出系统性的偏好。这样的偏好可以巧妙地影响用户投资决策，这可能导致产品和财务泡沫的估值膨胀，从而对个人投资者和市场稳定构成风险。为了全面研究产品偏见，我们开发了一条自动管道，以在五个资产类别（股票，共同基金，加密货币，储蓄和投资组合）的数据集中创建567,000个样本的数据集。使用此数据集，我们介绍了BF在LLM投资建议中有关产品偏见的首次研究。我们的发现表明，LLMS表现出清晰的产品偏好，例如某些股票（例如，来自Apple的AAPL'和Microsoft的“ MSFT”）。值得注意的是，即使在应用了辩护技术之后，这种偏见仍然存在。我们敦促AI研究人员注意LLM投资建议及其含义中的产品偏见，以确保数字空间和市场的公平性和安全性。</li>
</ul>

<h3>Title: Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations</h3>
<ul>
<li><strong>Authors: </strong>Danielle Villa, Maria Chang, Keerthiram Murugesan, Rosario Uceda-Sosa, Karthikeyan Natesan Ramamurthy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08815">https://arxiv.org/abs/2503.08815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08815">https://arxiv.org/pdf/2503.08815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08815]] Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations(https://arxiv.org/abs/2503.08815)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are often asked to explain their outputs to enhance accuracy and transparency. However, evidence suggests that these explanations can misrepresent the models' true reasoning processes. One effective way to identify inaccuracies or omissions in these explanations is through consistency checking, which typically involves asking follow-up questions. This paper introduces, cross-examiner, a new method for generating follow-up questions based on a model's explanation of an initial question. Our method combines symbolic information extraction with language model-driven question generation, resulting in better follow-up questions than those produced by LLMs alone. Additionally, this approach is more flexible than other methods and can generate a wider variety of follow-up questions.</li>
<li><strong>摘要：</strong>通常要求大型语言模型（LLM）解释其输出以提高准确性和透明度。但是，有证据表明，这些解释可能会歪曲模型的真实推理过程。在这些解释中确定不准确或遗漏的一种有效方法是通过一致性检查，这通常涉及提出后续问题。本文介绍了一种盘问，这是一种基于模型对初始问题的解释来生成后续问题的新方法。我们的方法将符号信息提取与语言模型驱动的问题的产生相结合，从而比仅LLMS产生的问题提出了更好的后续问题。此外，这种方法比其他方法更灵活，并且可以产生更广泛的后续问题。</li>
</ul>

<h3>Title: Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Sun, Kun Qian, Wenhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08842">https://arxiv.org/abs/2503.08842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08842">https://arxiv.org/pdf/2503.08842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08842]] Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation with LLMs(https://arxiv.org/abs/2503.08842)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multi-party dialogue generation presents significant challenges due to the complex interplay of multiple speakers and interwoven conversational threads. Traditional approaches often fall short in capturing these complexities, particularly when relying on manually annotated dialogue relations. This paper introduces Speaker-Attentive LLM (SA-LLM), a novel generative model that leverages pre-trained Large Language Models (LLMs) and a speaker-aware contrastive learning strategy to address these challenges. SA-LLM incorporates a speaker-attributed input encoding and a contrastive learning objective to implicitly learn contextual coherence and speaker roles without explicit relation annotations. Extensive experiments on the Ubuntu IRC and Movie Dialogues datasets demonstrate that SA-LLM significantly outperforms state-of-the-art baselines in automatic and human evaluations, achieving superior performance in fluency, coherence, informativeness, and response diversity. Ablation studies and detailed error analyses further validate the effectiveness of the proposed speaker-attentive training approach, highlighting its robustness across different speaker roles and context lengths. The results underscore the potential of SA-LLM as a powerful and annotation-free solution for high-quality multi-party dialogue generation.</li>
<li><strong>摘要：</strong>由于多个扬声器和交织的对话线程的复杂相互作用，多方对话生成提出了重大挑战。传统方法通常在捕获这些复杂性方面差不多，尤其是在依靠手动注释的对话关系时。本文介绍了演讲者指导LLM（SA-LLM），这是一种新颖的生成模型，利用预先训练的大型语言模型（LLM）和一种说话者意识到的对比学习策略来应对这些挑战。 SA-LLM结合了讲话者的输入编码和一个对比度学习目标，以隐式学习上下文连贯性和说话者角色而没有明确的关系注释。关于Ubuntu IRC和电影对话数据集的广泛实验表明，SA-LLM在自动和人类评估中的最先进基线表现明显优于最先进的基线，从而在流利性，连贯性，信息性和响应多样性方面取得了出色的表现。消融研究和详细的错误分析进一步验证了拟议的说话者训练方法的有效性，从而强调了其在不同的说话者角色和上下文长度之间的鲁棒性。结果强调了SA-LLM作为高质量多方对话生成的强大且无注释的解决方案的潜力。</li>
</ul>

<h3>Title: Interpretable and Robust Dialogue State Tracking via Natural Language Summarization with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rafael Carranza, Mateo Alejandro Rojas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08857">https://arxiv.org/abs/2503.08857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08857">https://arxiv.org/pdf/2503.08857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08857]] Interpretable and Robust Dialogue State Tracking via Natural Language Summarization with LLMs(https://arxiv.org/abs/2503.08857)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to Dialogue State Tracking (DST) that leverages Large Language Models (LLMs) to generate natural language descriptions of dialogue states, moving beyond traditional slot-value representations. Conventional DST methods struggle with open-domain dialogues and noisy inputs. Motivated by the generative capabilities of LLMs, our Natural Language DST (NL-DST) framework trains an LLM to directly synthesize human-readable state descriptions. We demonstrate through extensive experiments on MultiWOZ 2.1 and Taskmaster-1 datasets that NL-DST significantly outperforms rule-based and discriminative BERT-based DST baselines, as well as generative slot-filling GPT-2 DST models, in both Joint Goal Accuracy and Slot Accuracy. Ablation studies and human evaluations further validate the effectiveness of natural language state generation, highlighting its robustness to noise and enhanced interpretability. Our findings suggest that NL-DST offers a more flexible, accurate, and human-understandable approach to dialogue state tracking, paving the way for more robust and adaptable task-oriented dialogue systems.</li>
<li><strong>摘要：</strong>本文介绍了一种新颖的对话状态跟踪方法（DST），该方法利用大型语言模型（LLMS）生成对话状态的自然语言描述，而不是传统的插槽值表示。传统的DST方法与开放域对话和嘈杂的输入相加。我们的自然语言DST（NL-DST）框架是由LLM的生成能力的动机训练LLM，以直接综合人类可读的状态描述。我们通过对多沃兹2.1和TaskMaster-1数据集进行的广泛实验来证明NL-DST显着胜过基于规则的基于规则的基于BERT的DST基准，以及生成的插槽填充GPT-2 DST模型，在关节目标准确性和SLOT精度中均具有。消融研究和人类评估进一步验证了自然语言状态产生的有效性，突出了其对噪声的稳健性和增强的解释性。我们的发现表明，NL-DST为对话状态跟踪提供了一种更灵活，准确和人为理解的方法，为更健壮和适应性的任务对话系统铺平了道路。</li>
</ul>

<h3>Title: LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Guangtao Wang, Shubhangi Upasani, Chen Wu, Darshan Gandhi, Jonathan Li, Changran Hu, Bo Li, Urmish Thakker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08879">https://arxiv.org/abs/2503.08879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08879">https://arxiv.org/pdf/2503.08879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08879]] LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference(https://arxiv.org/abs/2503.08879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Efficient long-context inference is critical as large language models (LLMs) adopt context windows of ranging from 128K to 1M tokens. However, the growing key-value (KV) cache and the high computational complexity of attention create significant bottlenecks in memory usage and latency. In this paper, we find that attention in diverse long-context tasks exhibits sparsity, and LLMs implicitly "know" which tokens can be dropped or evicted at the head level after the pre-filling stage. Based on this insight, we propose Self-Attention Guided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for long-context inference. After prefilling, our method performs a one-time top-k selection at both the token and head levels to compress the KV cache, enabling efficient inference with the reduced cache. Evaluations on LongBench and three long-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct, and Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable to full attention while significantly improving efficiency. Specifically, SAGE-KV achieves 4x higher memory efficiency with improved accuracy over the static KV cache selection method StreamLLM, and 2x higher memory efficiency with better accuracy than the dynamic KV cache selection method Quest.</li>
<li><strong>摘要：</strong>高效的长篇小说推理至关重要，因为大语言模型（LLMS）采用上下文窗口，范围为128K到1M令牌。但是，不断增长的键值（KV）缓存以及注意力的高计算复杂性在记忆使用和延迟中创造了重要的瓶颈。在本文中，我们发现各种长篇文章任务中的注意力表现出稀疏性，而LLMS隐含地“知道”了哪些令牌可以在预填充阶段后在头部降低或驱逐。基于这种见解，我们提出了自我发挥的指导驱逐〜（Sage-kv），这是一种简单有效的KV驱逐缓存方法，用于长篇小说推断。预填充后，我们的方法在令牌和头部级别上都进行了一次top-K选择，以压缩KV缓存，从而有效地推断了减少的缓存。对Longbench和三个长篇小说LLM（Llama3.1-8B-Instruct-128K，Llama3-8B-Prolong-512K-Instruct和Qwen2.5-7B-Instruct-1128K）的评估表明，Sage-kv保持了与全部注意力相当的准确性，同时提高了效率。具体而言，SAGE-KV比静态KV缓存选择方法的精度提高了4倍的内存效率，并且比动态KV缓存选择方法任务更高，并具有更高精度的2倍内存效率。</li>
</ul>

<h3>Title: PlainQAFact: Automatic Factuality Evaluation Metric for Biomedical Plain Language Summaries Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen You, Yue Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08890">https://arxiv.org/abs/2503.08890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08890">https://arxiv.org/pdf/2503.08890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08890]] PlainQAFact: Automatic Factuality Evaluation Metric for Biomedical Plain Language Summaries Generation(https://arxiv.org/abs/2503.08890)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Hallucinated outputs from language models pose risks in the medical domain, especially for lay audiences making health-related decisions. Existing factuality evaluation methods, such as entailment- and question-answering-based (QA), struggle with plain language summary (PLS) generation due to elaborative explanation phenomenon, which introduces external content (e.g., definitions, background, examples) absent from the source document to enhance comprehension. To address this, we introduce PlainQAFact, a framework trained on a fine-grained, human-annotated dataset PlainFact, to evaluate the factuality of both source-simplified and elaboratively explained sentences. PlainQAFact first classifies factuality type and then assesses factuality using a retrieval-augmented QA-based scoring method. Our approach is lightweight and computationally efficient. Empirical results show that existing factuality metrics fail to effectively evaluate factuality in PLS, especially for elaborative explanations, whereas PlainQAFact achieves state-of-the-art performance. We further analyze its effectiveness across external knowledge sources, answer extraction strategies, overlap measures, and document granularity levels, refining its overall factuality assessment.</li>
<li><strong>摘要：</strong>语言模型的幻觉输出在医疗领域中构成风险，尤其是对于做出健康相关决定的外行观众而言。现有的事实评估方法，例如基于问题和提问的基于问题的（QA），由于精明的解释现象而与普通语言摘要（PLS）生成斗争，该现象引入了外部内容（例如，定义，背景，示例，示例），从源文档中缺乏来提高理解。为了解决这个问题，我们介绍了PlainQafact，该框架是在细粒度的，人类通知的plainfact上训练的框架，以评估源简化和精心解释的句子的事实。 PlainQafact首先将事实类型分类，然后使用基于QA的检索QA评分方法评估事实。我们的方法轻巧且计算上有效。经验结果表明，现有的事实指标无法有效地评估PLS中的事实，尤其是对于精明的解释，而PlainQafact实现了最新的表现。我们进一步分析了其跨外部知识来源，回答提取策略，重叠措施和记录粒度水平的有效性，从而完善了其整体事实评估。</li>
</ul>

<h3>Title: EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Zeng, Yizhong Wang, Hannaneh Hajishirzi, Pang Wei Koh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08893">https://arxiv.org/abs/2503.08893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08893">https://arxiv.org/pdf/2503.08893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08893]] EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees(https://arxiv.org/abs/2503.08893)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for Language Model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also propose a weakness profiling method EvalTree. It constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we release our code and an interface that allows practitioners to interactively explore the capability trees built by EvalTree.</li>
<li><strong>摘要：</strong>理想的模型评估应实现两个目标：确定模型失败的位置并提供可行的改进指导。朝着这些语言模型（LM）评估的目标，我们制定了产生弱点概况的问题，鉴于LM在基准中的每个实例上的表现，都以自然语言表达的一组弱点。我们引入了一套定量评估，以比较不同的弱点分析方法。我们还提出了一个弱点分析方法评估者。它构建了一个能力树，其中每个节点代表自然语言中描述的能力，并与专门评估该能力的基准实例的子集相连。然后，它提取LM表现不佳以产生弱点的节点。在数学和Wildchat的基准上，我们表明，通过更精确，更全面地识别弱点，评估者的表现优于基线弱点方法。弱点概况进一步使弱点引导的数据收集和以评估者识别的弱点为指导的培训数据收集比其他数据收集策略更能提高LM的性能。我们还展示了评估者如何在Chatbot Arena的基于人类投票的评估实践中暴露缺陷。为了促进未来的工作，我们发布了代码和一个界面，该界面允许从业者交互式探索评估者构建的功能树。</li>
</ul>

<h3>Title: Backtracking for Safety</h3>
<ul>
<li><strong>Authors: </strong>Bilgehan Sel, Dingcheng Li, Phillip Wallis, Vaishakh Keshava, Ming Jin, Siddhartha Reddy Jonnalagadda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08919">https://arxiv.org/abs/2503.08919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08919">https://arxiv.org/pdf/2503.08919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08919]] Backtracking for Safety(https://arxiv.org/abs/2503.08919)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across various tasks, but ensuring their safety and alignment with human values remains crucial. Current safety alignment methods, such as supervised fine-tuning and reinforcement learning-based approaches, can exhibit vulnerabilities to adversarial attacks and often result in shallow safety alignment, primarily focusing on preventing harmful content in the initial tokens of the generated output. While methods like resetting can help recover from unsafe generations by discarding previous tokens and restarting the generation process, they are not well-suited for addressing nuanced safety violations like toxicity that may arise within otherwise benign and lengthy generations. In this paper, we propose a novel backtracking method designed to address these limitations. Our method allows the model to revert to a safer generation state, not necessarily at the beginning, when safety violations occur during generation. This approach enables targeted correction of problematic segments without discarding the entire generated text, thereby preserving efficiency. We demonstrate that our method dramatically reduces toxicity appearing through the generation process with minimal impact to efficiency.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在各种任务中都表现出了出色的功能，但是确保其安全性和与人类价值观的一致性仍然至关重要。当前的安全一致性方法，例如受监督的微调和基于加强学习的方法，可以表现出对对抗性攻击的脆弱性，并且通常会导致较浅的安全对准，主要集中于在生成的产出的初始标记中防止有害内容。尽管重置等方法可以通过丢弃前代币并重新启动生成过程来帮助从不安全的世代中恢复过来，但它们不适合解决细微的安全违规行为，例如在否则良性和漫长的一代中可能出现的毒性。在本文中，我们提出了一种旨在解决这些局限性的新型回溯方法。我们的方法使该模型可以恢复到更安全的一代状态，而不一定是在发电期间发生安全违规的情况下。这种方法可以针对有问题的细分市场进行有针对性的纠正，而无需丢弃整个生成的文本，从而确保了效率。我们证明，我们的方法大大降低了通过生成过程出现的毒性，对效率的影响很小。</li>
</ul>

<h3>Title: Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Jiaxin Zhang, Xiang Gao, Wendi Cui, Peng Li, Kamalika Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08963">https://arxiv.org/abs/2503.08963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08963">https://arxiv.org/pdf/2503.08963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08963]] Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation(https://arxiv.org/abs/2503.08963)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>In tasks like summarization and open-book question answering (QA), Large Language Models (LLMs) often encounter "contextual hallucination", where they produce irrelevant or incorrect responses despite having access to accurate source information. This typically occurs because these models tend to prioritize self-generated content over the input context, causing them to disregard pertinent details. To address this challenge, we introduce a novel method called "Guided Attention Map Editing" (GAME), which dynamically adjusts attention maps to improve contextual relevance. During inference, GAME employs a trained classifier to identify attention maps prone to inducing hallucinations and executes targeted interventions. These interventions, guided by gradient-informed "edit directions'', strategically redistribute attention weights across various heads to effectively reduce hallucination. Comprehensive evaluations on challenging summarization and open-book QA tasks show that GAME consistently reduces hallucinations across a variety of open-source models. Specifically, GAME reduces hallucinations by 10% in the XSum summarization task while achieving a 7X speed-up in computational efficiency compared to the state-of-the-art baselines.</li>
<li><strong>摘要：</strong>在诸如摘要和开放式问题回答（QA）之类的任务中，大型语言模型（LLMS）经常遇到“上下文幻觉”，尽管可以访问准确的源信息，但它们会产生无关紧要或不正确的答复。这通常是因为这些模型倾向于在输入上下文中优先考虑自我生成的内容，从而导致它们无视相关的细节。为了应对这一挑战，我们介绍了一种名为“指导注意力图编辑”（游戏）的新方法，该方法会动态调整注意力图以提高上下文相关性。在推论期间，Game采用训练有素的分类器来确定容易引起幻觉和执行目标干预措施的注意力图。这些干预措施以梯度信息的“编辑指示”为指导，从战略上重新分配了各种头部的注意力，以有效地减少幻觉。对挑战性汇总和QA任务的全面评估表明，游戏一致地表明，游戏会始终如一地降低各种开放式模型的幻觉。专门为X型降低了10％的XPONTIONS。与最先进的基线相比，效率。</li>
</ul>

<h3>Title: Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08979">https://arxiv.org/abs/2503.08979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08979">https://arxiv.org/pdf/2503.08979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08979]] Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions(https://arxiv.org/abs/2503.08979)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The integration of Agentic AI into scientific discovery marks a new frontier in research automation. These AI systems, capable of reasoning, planning, and autonomous decision-making, are transforming how scientists perform literature review, generate hypotheses, conduct experiments, and analyze results. This survey provides a comprehensive overview of Agentic AI for scientific discovery, categorizing existing systems and tools, and highlighting recent progress across fields such as chemistry, biology, and materials science. We discuss key evaluation metrics, implementation frameworks, and commonly used datasets to offer a detailed understanding of the current state of the field. Finally, we address critical challenges, such as literature review automation, system reliability, and ethical concerns, while outlining future research directions that emphasize human-AI collaboration and enhanced system calibration.</li>
<li><strong>摘要：</strong>将代理AI集成到科学发现中标志着研究自动化的新领域。这些能够推理，计划和自主决策的AI系统正在改变科学家如何执行文献综述，产生假设，进行实验和分析结果。这项调查提供了针对科学发现，对现有系统和工具进行分类的代理AI的全面概述，并强调了化学，生物学和材料科学等领域的最新进展。我们讨论关键评估指标，实施框架以及常用的数据集，以详细了解该领域的现状。最后，我们应对关键挑战，例如文献审查自动化，系统可靠性和道德问题，同时概述了强调人类协作和增强系统校准的未来研究方向。</li>
</ul>

<h3>Title: Aligning to What? Limits to RLHF Based Alignment</h3>
<ul>
<li><strong>Authors: </strong>Logan Barnhart, Reza Akbarian Bafghi, Stephen Becker, Maziar Raissi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09025">https://arxiv.org/abs/2503.09025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09025">https://arxiv.org/pdf/2503.09025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09025]] Aligning to What? Limits to RLHF Based Alignment(https://arxiv.org/abs/2503.09025)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is increasingly used to align large language models (LLMs) with human preferences. However, the effectiveness of RLHF in addressing underlying biases remains unclear. This study investigates the relationship between RLHF and both covert and overt biases in LLMs, particularly focusing on biases against African Americans. We applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and evaluated the covert and overt biases of the resulting models using matched-guise probing and explicit bias testing. We performed additional tests with DPO on different base models and datasets; among several implications, we found that SFT before RLHF calcifies model biases. Additionally, we extend the tools for measuring biases to multi-modal models. Through our experiments we collect evidence that indicates that current alignment techniques are inadequate for nebulous tasks such as mitigating covert biases, highlighting the need for capable datasets, data curating techniques, or alignment tools.</li>
<li><strong>摘要：</strong>从人类反馈（RLHF）中学习的强化学习越来越多地用于使大型语言模型（LLM）与人类的偏好保持一致。但是，RLHF在解决潜在偏见方面的有效性尚不清楚。这项研究调查了LLM中RLHF与秘密偏见之间的关系，尤其是针对非裔美国人的偏见。我们将各种RLHF技术（DPO，ORPO和RLOO）应用于Llama 3 8B，并使用匹配的旋转探测和显式偏置测试评估了所得模型的秘密和明显偏见。我们在不同的基本模型和数据集上使用DPO进行了其他测试。在几种含义中，我们发现RLHF之前的SFT会钙化模型偏差。此外，我们扩展了测量多模式模型偏见的工具。通过我们的实验，我们收集了证据表明，目前的一致性技术不足以用于减轻秘密偏见等模糊任务，突出了对功能强大的数据集，数据策划技术或对齐工具的需求。</li>
</ul>

<h3>Title: DAST: Difficulty-Aware Self-Training on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boyang Xue, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Hongling Xu, Fei Mi, Yasheng Wang, Lifeng Shang, Qun Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09029">https://arxiv.org/abs/2503.09029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09029">https://arxiv.org/pdf/2503.09029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09029]] DAST: Difficulty-Aware Self-Training on Large Language Models(https://arxiv.org/abs/2503.09029)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training (DAST) framework that focuses on improving both the quantity and quality of self-generated responses on challenging queries during self-training. DAST is specified in three components: 1) sampling-based difficulty level estimation, 2) difficulty-aware data augmentation, and 3) the self-training algorithm using SFT and DPO respectively. Experiments on mathematical tasks demonstrate the effectiveness and generalization of DAST, highlighting the critical role of difficulty-aware strategies in advancing LLM self-training.</li>
<li><strong>摘要：</strong>目前，大型语言模型（LLM）自我训练方法总是在具有挑战性的查询上样本不足，从而导致对限制LLMS能力的困难问题的学习不足。因此，这项工作提出了一个困难的自我训练（DAST）框架，重点是改善自我训练期间有挑战性查询的自我生成反应的数量和质量。 DAST在三个组件中指定：1）基于抽样的难度水平估计，2）难以感知的数据增强和3）分别使用SFT和DPO的自我训练算法。关于数学任务的实验证明了DAST的有效性和概括，强调了困扰策略在推进LLM自我训练中的关键作用。</li>
</ul>

<h3>Title: VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for Detecting LLM-Generated Vaccine Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Syed Talal Ahmad, Haohui Lu, Sidong Liu, Annie Lau, Amin Beheshti, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09103">https://arxiv.org/abs/2503.09103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09103">https://arxiv.org/pdf/2503.09103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09103]] VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for Detecting LLM-Generated Vaccine Misinformation(https://arxiv.org/abs/2503.09103)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly improved text generation capabilities. However, they also present challenges, particularly in generating vaccine-related misinformation, which poses risks to public health. Despite research on human-authored misinformation, a notable gap remains in understanding how LLMs contribute to vaccine misinformation and how best to detect it. Existing benchmarks often overlook vaccine-specific misinformation and the diverse roles of misinformation spreaders. This paper introduces VaxGuard, a novel dataset designed to address these challenges. VaxGuard includes vaccine-related misinformation generated by multiple LLMs and provides a comprehensive framework for detecting misinformation across various roles. Our findings show that GPT-3.5 and GPT-4o consistently outperform other LLMs in detecting misinformation, especially when dealing with subtle or emotionally charged narratives. On the other hand, PHI3 and Mistral show lower performance, struggling with precision and recall in fear-driven contexts. Additionally, detection performance tends to decline as input text length increases, indicating the need for improved methods to handle larger content. These results highlight the importance of role-specific detection strategies and suggest that VaxGuard can serve as a key resource for improving the detection of LLM-generated vaccine misinformation.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的最新进展已大大提高了文本生成功能。但是，它们也提出了挑战，特别是在产生与疫苗有关的错误信息方面，这给公共卫生带来了风险。尽管研究了人为造成的错误信息，但仍有显着的差距在理解LLM如何促进疫苗错误信息以及如何最好地检测其。现有的基准通常忽略了特定于疫苗的错误信息以及散布散布器的不同作用。本文介绍了VaxGuard，这是一个旨在应对这些挑战的新型数据集。 VaxGuard包括由多个LLM产生的与疫苗相关的错误信息，并提供了一个全面的框架，用于检测各种角色的错误信息。我们的发现表明，GPT-3.5和GPT-4O在检测错误信息方面始终优于其他LLM，尤其是在处理微妙或充满情感的叙述时。另一方面，Phi3和Mistral表现出较低的表现，在恐惧驱动的环境中精确地挣扎和回忆。此外，随着输入文本长度的增加，检测性能往往会下降，这表明需要改进方法来处理较大的内容。这些结果强调了特定角色检测策略的重要性，并表明VaxGuard可以作为改善LLM生成的疫苗错误信息检测的关键资源。</li>
</ul>

<h3>Title: Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Chaowei Zhang, Zongling Feng, Zewei Zhang, Jipeng Qiang, Guandong Xu, Yun Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09153">https://arxiv.org/abs/2503.09153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09153">https://arxiv.org/pdf/2503.09153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09153]] Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection(https://arxiv.org/abs/2503.09153)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>The questionable responses caused by knowledge hallucination may lead to LLMs' unstable ability in decision-making. However, it has never been investigated whether the LLMs' hallucination is possibly usable to generate negative reasoning for facilitating the detection of fake news. This study proposes a novel supervised self-reinforced reasoning rectification approach - SR$^3$ that yields both common reasonable reasoning and wrong understandings (negative reasoning) for news via LLMs reflection for semantic consistency learning. Upon that, we construct a negative reasoning-based news learning model called - \emph{NRFE}, which leverages positive or negative news-reasoning pairs for learning the semantic consistency between them. To avoid the impact of label-implicated reasoning, we deploy a student model - \emph{NRFE-D} that only takes news content as input to inspect the performance of our method by distilling the knowledge from \emph{NRFE}. The experimental results verified on three popular fake news datasets demonstrate the superiority of our method compared with three kinds of baselines including prompting on LLMs, fine-tuning on pre-trained SLMs, and other representative fake news detection methods.</li>
<li><strong>摘要：</strong>知识幻觉引起的可疑反应可能会导致LLMS在决策中的不稳定能力。但是，从未调查过LLMS的幻觉是否可以用于促进假新闻的检测产生负面推理。这项研究提出了一种新颖的监督自我增强的推理纠正方法-SR $^3 $，该方法既可以通过LLMS反思，既可以通过LLMS反思来获得常见的合理推理和错误的理解（负面推理），以实现语义一致性学习。因此，我们构建了一种基于负面推理的新闻学习模型 -  \ emph {nrfe}，该模型利用正面或负面的新闻策划对学习它们之间的语义一致性。为了避免标签 - 刻度推理的影响，我们部署了学生模型 -  \ emph {nrfe-d}，该模型仅将新闻内容作为输入来检查我们方法的性能，以从\ emph {nrfe}中提取知识。与三种基线相比，在三个流行的假新闻数据集上进行了验证的实验结果证明了我们方法的优越性，包括提示LLMS，对预训练的SLM进行微调以及其他代表性的假新闻检测方法。</li>
</ul>

<h3>Title: Token Weighting for Long-Range Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Falko Helm, Nico Daheim, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09202">https://arxiv.org/abs/2503.09202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09202">https://arxiv.org/pdf/2503.09202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09202]] Token Weighting for Long-Range Language Modeling(https://arxiv.org/abs/2503.09202)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Many applications of large language models (LLMs) require long-context understanding, but models continue to struggle with such tasks. We hypothesize that conventional next-token prediction training could contribute to this, because each token is assigned equal weight. Yet, intuitively, the amount of context needed to predict the next token accurately varies greatly across different data. To reflect this, we propose various novel token-weighting schemes that assign different weights to each training token in the loss, thereby generalizing existing works. For this, we categorize token-weighting methods using a two-step framework which compares the confidences of a long-context and short-context model to score tokens. We evaluate all methods on multiple long-context understanding tasks and show that non-uniform loss weights are helpful to improve the long-context abilities of LLMs. Different short-context models can be used effectively for token scoring, including models that are much smaller than the long-context model that is trained. All in all, this work contributes to a better understanding of the trade-offs long-context language modeling faces and provides guidelines for model steering via loss-weighting based on empirical evidence. The code can be found on Github.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的许多应用都需要长期的理解，但是模型继续在此类任务上挣扎。我们假设传统的下一步预测训练可能会为此做出贡献，因为每个令牌均分配了相等的权重。但是，直觉上，预测接下来的令牌所需的上下文数量在不同数据之间准确地差异很大。为了反映这一点，我们提出了各种新颖的令牌加权方案，这些方案为损失的每个培训令牌分配了不同的权重，从而概括了现有作品。为此，我们使用两个步骤的框架对令牌加权方法进行了分类，该框架比较了长篇文章和短篇小写模型的信心与代价令牌。我们评估了多个长篇文化理解任务的所有方法，并表明非均匀的损耗权重有助于提高LLMS的长篇文化能力。可以有效地将不同的短篇小写模型用于令牌评分，其中包括比训练有素的长篇小写模型小得多。总而言之，这项工作有助于更好地理解折衷的长篇文章建模面孔，并为基于经验证据的损失加权提供模型转向的指南。该代码可以在GitHub上找到。</li>
</ul>

<h3>Title: N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jie He, Simon Yu, Deyi Xiong, Víctor Gutiérrez-Basulto, Jeff Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09218">https://arxiv.org/abs/2503.09218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09218">https://arxiv.org/pdf/2503.09218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09218]] N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual In-Context Learning(https://arxiv.org/abs/2503.09218)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements of in-context learning (ICL) show language models can significantly improve their performance when demonstrations are provided. However, little attention has been paid to model calibration and prediction confidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a thorough analysis of ICL for cross-lingual sentiment classification. Our findings suggest that ICL performs poorly in cross-lingual scenarios, exhibiting low accuracy and presenting high calibration errors. In response, we propose a novel approach, N2C2, which employs a -nearest neighbors augmented classifier for prediction confidence calibration. N2C2 narrows the prediction gap by leveraging a datastore of cached few-shot instances. Specifically, N2C2 integrates the predictions from the datastore and incorporates confidence-aware distribution, semantically consistent retrieval representation, and adaptive neighbor combination modules to effectively utilize the limited number of supporting instances. Evaluation on two multilingual sentiment classification datasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine tuning, prompt tuning and recent state-of-the-art methods in terms of accuracy and calibration errors.</li>
<li><strong>摘要：</strong>在提供示范时，在文化学习（ICL）的最新进展表明语言模型可以显着提高其性能。但是，很少有人注意建模ICL在跨语性方案中的校准和预测信心。为了弥合这一差距，我们对ICL进行了详尽的分析，以进行跨语义情感分类。我们的发现表明，ICL在跨语性场景中的表现较差，表现出较低的精度并出现高校准误差。作为响应，我们提出了一种新型方法N2C2，该方法采用了最近的邻居增强分类器进行预测置信度校准。 N2C2通过利用缓存的少数播放实例的数据存储来缩小预测差距。具体而言，N2C2整合了数据存储的预测，并结合了置信度分布，语义一致的检索表示和自适应邻居组合模块，以有效利用有限数量的支持实例。对两个多语言情感分类数据集的评估表明，N2C2的表现优于传统ICL。就准确性和校准错误而言，它超过了微调，及时的调整和最新的最新方法。</li>
</ul>

<h3>Title: Rethinking Prompt-based Debiasing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Yang, Runzhe Zhan, Derek F. Wong, Shu Yang, Junchao Wu, Lidia S. Chao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09219">https://arxiv.org/abs/2503.09219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09219">https://arxiv.org/pdf/2503.09219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09219]] Rethinking Prompt-based Debiasing in Large Language Models(https://arxiv.org/abs/2503.09219)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Investigating bias in large language models (LLMs) is crucial for developing trustworthy AI. While prompt-based through prompt engineering is common, its effectiveness relies on the assumption that models inherently understand biases. Our study systematically analyzed this assumption using the BBQ and StereoSet benchmarks on both open-source models as well as commercial GPT model. Experimental results indicate that prompt-based is often superficial; for instance, the Llama2-7B-Chat model misclassified over 90% of unbiased content as biased, despite achieving high accuracy in identifying bias issues on the BBQ dataset. Additionally, specific evaluation and question settings in bias benchmarks often lead LLMs to choose "evasive answers", disregarding the core of the question and the relevance of the response to the context. Moreover, the apparent success of previous methods may stem from flawed evaluation metrics. Our research highlights a potential "false prosperity" in prompt-base efforts and emphasizes the need to rethink bias metrics to ensure truly trustworthy AI.</li>
<li><strong>摘要：</strong>在大型语言模型（LLM）中调查偏见对于发展值得信赖的AI至关重要。尽管基于及时的及时工程的迅速，但其有效性取决于模型固有地理解偏见的假设。我们的研究系统地使用开源模型和商业GPT模型上的烧烤和立体声基准来系统地分析了这一假设。实验结果表明，基于及时通常是肤浅的。例如，尽管在识别烧烤数据集中的偏见问题方面达到了很高的准确性，但Llama2-7B-Chat模型将超过90％的无偏见内容误分类为有偏见。此外，偏置基准中的特定评估和问题设置通常会导致LLMS选择“回避答案”，无视问题的核心以及对上下文的回答的相关性。此外，先前方法的明显成功可能源于评估指标的缺陷。我们的研究强调了潜在的“虚假繁荣”在迅速的努力中，并强调需要重新思考偏见指标，以确保真正值得信赖的AI。</li>
</ul>

<h3>Title: xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Elio Musacchio, Lucia Siciliani, Pierpaolo Basile, Giovanni Semeraro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09313">https://arxiv.org/abs/2503.09313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09313">https://arxiv.org/pdf/2503.09313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09313]] xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge Distillation(https://arxiv.org/abs/2503.09313)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the current literature, most embedding models are based on the encoder-only transformer architecture to extract a dense and meaningful representation of the given input, which can be a text, an image, and more. With the recent advances in language modeling thanks to the introduction of Large Language Models, the possibility of extracting embeddings from these large and extensively trained models has been explored. However, current studies focus on textual embeddings in English, which is also the main language on which these models have been trained. Furthermore, there are very few models that consider multimodal and multilingual input. In light of this, we propose an adaptation methodology for Large Vision-Language Models trained on English language data to improve their performance in extracting multilingual and multimodal embeddings. Finally, we design and introduce a benchmark to evaluate the effectiveness of multilingual and multimodal embedding models.</li>
<li><strong>摘要：</strong>在当前文献中，大多数嵌入模型基于仅编码的变压器体系结构，以提取给定输入的密集且有意义的表示，这可以是文本，图像等。随着语言建模的最新进展，由于引入了大型语言模型，已经探索了从这些大型训练的模型中提取嵌入的可能性。但是，当前的研究集中于英语中的文本嵌入，这也是对这些模型进行培训的主要语言。此外，很少有模型考虑多模式和多语言输入。鉴于这一点，我们提出了一种适应方法，用于对英语数据进行培训的大型视觉语言模型，以提高其在提取多语言和多模式嵌入方面的性能。最后，我们设计并引入了一个基准，以评估多语言和多模式嵌入模型的有效性。</li>
</ul>

<h3>Title: A Survey on Enhancing Causal Reasoning Ability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Zhuo Cai, Shoujin Wang, Kun Yu, Fang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09326">https://arxiv.org/abs/2503.09326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09326">https://arxiv.org/pdf/2503.09326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09326]] A Survey on Enhancing Causal Reasoning Ability of Large Language Models(https://arxiv.org/abs/2503.09326)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently shown remarkable performance in language tasks and beyond. However, due to their limited inherent causal reasoning ability, LLMs still face challenges in handling tasks that require robust causal reasoning ability, such as health-care and economic analysis. As a result, a growing body of research has focused on enhancing the causal reasoning ability of LLMs. Despite the booming research, there lacks a survey to well review the challenges, progress and future directions in this area. To bridge this significant gap, we systematically review literature on how to strengthen LLMs' causal reasoning ability in this paper. We start from the introduction of background and motivations of this topic, followed by the summarisation of key challenges in this area. Thereafter, we propose a novel taxonomy to systematically categorise existing methods, together with detailed comparisons within and between classes of methods. Furthermore, we summarise existing benchmarks and evaluation metrics for assessing LLMs' causal reasoning ability. Finally, we outline future research directions for this emerging field, offering insights and inspiration to researchers and practitioners in the area.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）最近在语言任务及其他方面表现出色。但是，由于其固有的因果推理能力有限，LLMS仍然面临着需要强大因果推理能力的任务（例如医疗保健和经济分析）的挑战。结果，越来越多的研究重点是增强LLM的因果推理能力。尽管进行了蓬勃发展的研究，但仍缺乏对该领域的挑战，进步和未来方向进行充分审查的调查。为了弥合这一重大差距，我们系统地回顾了有关如何在本文中增强LLMS的因果推理能力的文献。我们从引入该主题的背景和动机的引入，然后摘要该领域的主要挑战。此后，我们提出了一种新颖的分类法，以系统地对现有方法进行分类，并在方法之间和类别之间进行详细的比较。此外，我们总结了现有的基准和评估指标，以评估LLMS的因果推理能力。最后，我们概述了这个新兴领域的未来研究指示，为该地区的研究人员和从业人员提供了见解和灵感。</li>
</ul>

<h3>Title: An Evaluation of LLMs for Detecting Harmful Computing Terms</h3>
<ul>
<li><strong>Authors: </strong>Joshua Jacas, Hana Winchester, Alicia Boyd, Brittany Johnson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09341">https://arxiv.org/abs/2503.09341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09341">https://arxiv.org/pdf/2503.09341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09341]] An Evaluation of LLMs for Detecting Harmful Computing Terms(https://arxiv.org/abs/2503.09341)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Detecting harmful and non-inclusive terminology in technical contexts is critical for fostering inclusive environments in computing. This study explores the impact of model architecture on harmful language detection by evaluating a curated database of technical terms, each paired with specific use cases. We tested a range of encoder, decoder, and encoder-decoder language models, including BERT-base-uncased, RoBERTa large-mnli, Gemini Flash 1.5 and 2.0, GPT-4, Claude AI Sonnet 3.5, T5-large, and BART-large-mnli. Each model was presented with a standardized prompt to identify harmful and non-inclusive language across 64 terms. Results reveal that decoder models, particularly Gemini Flash 2.0 and Claude AI, excel in nuanced contextual analysis, while encoder models like BERT exhibit strong pattern recognition but struggle with classification certainty. We discuss the implications of these findings for improving automated detection tools and highlight model-specific strengths and limitations in fostering inclusive communication in technical domains.</li>
<li><strong>摘要：</strong>在技​​术环境中检测有害和非包含的术语对于培养计算中的包容性环境至关重要。这项研究通过评估策划的技术术语数据库，探索模型体系结构对有害语言检测的影响，每个数据库与特定用例配对。我们测试了一系列编码器，解码器和编码器 - 模型模型，包括基于Bert-Base的Roberta Groot-Mnli，Gemini Flash 1.5和2.0，GPT-4，Claude AI SONNET 3.5，T5-Large，T5-Large和Bart-Large-Mnli。每个模型都有标准化提示，以识别64个学期的有害和非包含语言。结果表明，解码器模型，尤其是Gemini Flash 2.0和Claude AI，在细微的上下文分析中表现出色，而伯特（Bert）等编码器模型则表现出强烈的模式识别，但在分类确定性方面挣扎。我们讨论了这些发现对改善自动检测工具的含义，并突出显示了模型特定的优势和局限性在促进技术领域的包容性交流中。</li>
</ul>

<h3>Title: Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Chen, Seraphina Goldfarb-Tarrant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09347">https://arxiv.org/abs/2503.09347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09347">https://arxiv.org/pdf/2503.09347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09347]] Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts(https://arxiv.org/abs/2503.09347)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用作自动化评估者来评估生成的内容的安全性，但是它们在此角色中的可靠性仍然不确定。这项研究评估了跨关键安全领域的一组11个LLM法官模型，研究了三个关键方面：反复判断任务的自洽性，与人类判断的一致性以及对输入诸如道歉或冗长词句等输入文物的敏感性。我们的发现表明，LLM法官中的偏见可能会严重扭曲最终判决哪些内容来源更安全，从而破坏了比较评估的有效性。值得注意的是，仅道歉的语言文物就可以偏向评估者的偏好，最多可达98 \％。与期望相反，较大的模型不会始终显示出更大的鲁棒性，而较小的模型有时会显示出对特定伪像的耐药性。为了减轻LLM评估者的鲁棒性问题，我们研究了基于陪审团的评估，汇总了来自多个模型的决策。尽管这种方法既可以提高鲁棒性并增强对人类判断的一致性，但即使使用最佳陪审团的配置，人工敏感性仍然存在。这些结果凸显了对多元化的抗伪影方法的迫切需求，以确保可靠的安全评估。</li>
</ul>

<h3>Title: RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports</h3>
<ul>
<li><strong>Authors: </strong>Jiushen Cai, Weihang Zhang, Hanruo Liu, Ningli Wang, Huiqi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09358">https://arxiv.org/abs/2503.09358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09358">https://arxiv.org/pdf/2503.09358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09358]] RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports(https://arxiv.org/abs/2503.09358)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard terminology, containing fundus clinical terms and commonly used descriptions in clinical diagnosis. Then, we establish two models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented dataset simulating clinical scenarios, demonstrates powerful standardization behaviors. However, it encounters a challenge of limitation to cover a wider range of diseases. To further enhance standardization performance, we build RetSTA-7B, which integrates a substantial amount of standardized data generated by RetSTA-7B-Zero along with corresponding English data, covering diverse complex clinical scenarios and achieving report-level standardization for the first time. Experimental results demonstrate that RetSTA-7B outperforms other compared LLMs in bilingual standardization task, which validates its superior performance and generalizability. The checkpoints are available at this https URL.</li>
<li><strong>摘要：</strong>临床报告的标准化对于提高医疗保健质量和促进数据整合至关重要。缺乏统一标准（包括格式，术语和样式）是临床底底诊断报告中的一个巨大挑战，这增加了大语言模型（LLMS）难以理解数据的困难。为了解决这个问题，我们构建了双语标准术语，其中包含眼底临床术语和临床诊断中常用的描述。然后，我们建立了两个模型，分别是retsta-7b-Zero和retsta-7b。 retsta-7b-Zero在模拟临床方案的增强数据集上进行了微调，表现出强大的标准化行为。但是，它遇到了涵盖更广泛疾病的限制挑战。为了进一步提高标准化的性能，我们构建了RETSTA-7B，该retsta-7b集成了大量由Retsta-7b-Zero生成的标准化数据以及相应的英语数据，涵盖了首次涵盖各种复杂的临床方案并实现报告级别的标准化。实验结果表明，RETSA-7B在双语标准化任务中比其他LLM的表现优于其他LLM，从而验证了其出色的性能和概括性。该检查点可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Got Compute, but No Data: Lessons From Post-training a Finnish LLM</h3>
<ul>
<li><strong>Authors: </strong>Elaine Zosa, Ville Komulainen, Sampo Pyysalo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09407">https://arxiv.org/abs/2503.09407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09407">https://arxiv.org/pdf/2503.09407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09407]] Got Compute, but No Data: Lessons From Post-training a Finnish LLM(https://arxiv.org/abs/2503.09407)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, chat</a></li>
<li><strong>Abstract: </strong>As LLMs gain more popularity as chatbots and general assistants, methods have been developed to enable LLMs to follow instructions and align with human preferences. These methods have found success in the field, but their effectiveness has not been demonstrated outside of high-resource languages. In this work, we discuss our experiences in post-training an LLM for instruction-following for English and Finnish. We use a multilingual LLM to translate instruction and preference datasets from English to Finnish. We perform instruction tuning and preference optimization in English and Finnish and evaluate the instruction-following capabilities of the model in both languages. Our results show that with a few hundred Finnish instruction samples we can obtain competitive performance in Finnish instruction-following. We also found that although preference optimization in English offers some cross-lingual benefits, we obtain our best results by using preference data from both languages. We release our model, datasets, and recipes under open licenses at this https URL</li>
<li><strong>摘要：</strong>随着聊天机器人和普通助理的发展，LLM的受欢迎程度越来越高，因此已经开发出可以使LLMS遵循指示并与人类偏好保持一致的方法。这些方法在该领域取得了成功，但是在高资源语言之外，它们的有效性尚未得到证明。在这项工作中，我们讨论了我们在培训LLM后进行培训的经验，以遵守英语和芬兰语。我们使用多语言LLM将指令和偏好数据集从英语转换为芬兰语。我们在英语和芬兰语中执行教学调整和偏好优化，并评估两种语言中模型的指令遵循功能。我们的结果表明，通过几百个芬兰的指令样本，我们可以在芬兰教学中获得竞争性表现。我们还发现，尽管英语的偏好优化提供了一些跨语性的好处，但我们通过使用两种语言的偏好数据获得了最佳结果。我们在此HTTPS URL上发布了模型，数据集和食谱</li>
</ul>

<h3>Title: Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Julian Spravil, Sebastian Houben, Sven Behnke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09443">https://arxiv.org/abs/2503.09443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09443">https://arxiv.org/pdf/2503.09443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09443]] Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models(https://arxiv.org/abs/2503.09443)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on a synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to a scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in a specific language even when only data for the translation task is available. Fine-tuning on a mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).</li>
<li><strong>摘要：</strong>跨语言转移使视觉语言模型（VLM）仅使用一种语言培训数据以各种语言执行视觉任务。当前的方法依赖于大型预训练的多语言模型。但是，他们面临多语言的诅咒，为多语言能力牺牲了下游任务绩效，与词汇歧义斗争，并落后于最近的进步。在这项工作中，我们研究了用于多语言任务的单语VLMS系统概括的缩放定律，重点是模型大小和可见的训练样本的影响。我们提出了弗洛伦兹（Florenz），这是一种单语编码器decoder vlm，具有0.4b至11.2b参数，结合了预先训练的VLM Florence-2和大型语言模型Gemma-2。弗洛伦兹（Florenz）在合成数据集中接受了不同计算预算的培训，该数据集具有故意不完整的图像字幕语言覆盖范围，因此，从完全覆盖的翻译任务中测试了概括。我们表明，不仅可以间接学习看不见的任务语言对遵守缩放定律，而且在我们的数据生成管道和拟议的弗洛伦兹模型家族的情况下，即使只有翻译任务的数据可用，图像字幕能力也可以以特定的语言出现。在下游数据集的混合中进行微调会产生竞争性能，并证明了多模式机器翻译（Multi30k，通勤），词汇歧义（通勤）和图像字幕（Multi30K，XM3600，Coco Karpathy）的有希望的缩放趋势。</li>
</ul>

<h3>Title: Explicit Learning and the LLM in Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Malik Marmonier, Rachel Bawden, Benoît Sagot</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09454">https://arxiv.org/abs/2503.09454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09454">https://arxiv.org/pdf/2503.09454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09454]] Explicit Learning and the LLM in Machine Translation(https://arxiv.org/abs/2503.09454)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study explores the capacity of large language models (LLMs) for explicit learning, a process involving the assimilation of metalinguistic explanations to carry out language tasks. Using constructed languages generated by cryptographic means as controlled test environments, we designed experiments to assess an LLM's ability to explicitly learn and apply grammar rules. Our results demonstrate that while LLMs possess a measurable capacity for explicit learning, this ability diminishes as the complexity of the linguistic phenomena at hand increases. Supervised fine-tuning on chains of thought significantly enhances LLM performance but struggles to generalize to typologically novel or more complex linguistic features. These findings point to the need for more diverse training sets and alternative fine-tuning strategies to further improve explicit learning by LLMs.</li>
<li><strong>摘要：</strong>这项研究探讨了大语言模型（LLMS）在显式学习中的能力，这是一个涉及对元语言解释同化以执行语言任务的过程。我们使用密码手段作为受控测试环境生成的构造语言，我们设计了实验，以评估LLM明确学习和应用语法规则的能力。我们的结果表明，尽管LLM具有可衡量的显式学习能力，但随着手头语言现象的复杂性的增加，这种能力会降低。对思想链的监督微调显着提高了LLM的性能，但努力概括为类型的新颖或更复杂的语言特征。这些发现表明，需要更多样化的培训集和替代性微调策略，以进一步改善LLMS的明确学习。</li>
</ul>

<h3>Title: BAMBI: Developing Baby Language Models for Italian</h3>
<ul>
<li><strong>Authors: </strong>Alice Suozzi, Luca Capone, Gianluca E. Lebani, Alessandro Lenci</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09481">https://arxiv.org/abs/2503.09481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09481">https://arxiv.org/pdf/2503.09481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09481]] BAMBI: Developing Baby Language Models for Italian(https://arxiv.org/abs/2503.09481)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper presents BAMBI (BAby language Models Boostrapped for Italian), a series of Baby Language Models (BabyLMs) trained on data that mimic the linguistic input received by a five-year-old Italian-speaking child. The BAMBI models are tested using a benchmark specifically designed to evaluate language models, which takes into account the amount of training input the models received. The BAMBI models are compared against a large language model (LLM) and a multimodal language model (VLM) to study the contribution of extralinguistic information for language acquisition. The results of our evaluation align with the existing literature on English language models, confirming that while reduced training data support the development of relatively robust syntactic competence, they are insufficient for fostering semantic understanding. However, the gap between the training resources (data and computation) of the BAMBI models and the LLMs is not fully reflected in their performance: despite LLMs' massive training, their performance is not much better than that of BAMBI models. This suggests that strategies beyond scaling training resources, such as data curation, inclusion of multimodal input, and other training strategies such as curriculum learning, could play a crucial role in shaping model performance.</li>
<li><strong>摘要：</strong>本文介绍了Bambi（婴儿语言模型为意大利语Boostry），这是一系列的婴儿语言模型（BABYLMS），该模型培训了模仿一个讲意大利语五岁的意大利儿童的语言输入的数据。使用专门设计用于评估语言模型的基准测试BAMBI模型，该基准考虑了所收到的模型的培训输入量。将BAMBI模型与大型语言模型（LLM）和多模式模型（VLM）进行比较，以研究语言信息对语言获取的贡献。我们的评估结果与有关英语模型的现有文献保持一致，证实虽然培训数据减少支持了相对强大的句法能力的发展，但它们不足以促进语义理解。但是，BAMBI模型的培训资源（数据和计算）之间的差距并未完全反映在其性能中：尽管LLMS的巨大培训，但其性能并不比BAMBI模型的差异要好得多。这表明，除了扩展培训资源（例如数据策划，多模式输入的包含）以及其他培训策略（例如课程学习）之外的策略可能在塑造模型性能中起着至关重要的作用。</li>
</ul>

<h3>Title: TRACE: Real-Time Multimodal Common Ground Tracking in Situated Collaborative Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Hannah VanderHoeven, Brady Bhalla, Ibrahim Khebour, Austin Youngren, Videep Venkatesha, Mariah Bradford, Jack Fitzgerald, Carlos Mabrey, Jingxuan Tu, Yifan Zhu, Kenneth Lai, Changsoo Jung, James Pustejovsky, Nikhil Krishnaswamy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09511">https://arxiv.org/abs/2503.09511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09511">https://arxiv.org/pdf/2503.09511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09511]] TRACE: Real-Time Multimodal Common Ground Tracking in Situated Collaborative Dialogues(https://arxiv.org/abs/2503.09511)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We present TRACE, a novel system for live *common ground* tracking in situated collaborative tasks. With a focus on fast, real-time performance, TRACE tracks the speech, actions, gestures, and visual attention of participants, uses these multimodal inputs to determine the set of task-relevant propositions that have been raised as the dialogue progresses, and tracks the group's epistemic position and beliefs toward them as the task unfolds. Amid increased interest in AI systems that can mediate collaborations, TRACE represents an important step forward for agents that can engage with multiparty, multimodal discourse.</li>
<li><strong>摘要：</strong>我们介绍了Trace，这是一种实时 *共同基础 *跟踪的新型系统。通过关注快速，实时的表现，跟踪参与者的语音，动作，手势和视觉关注，使用这些多模式输入来确定随着对话的进行随着对话的进行，并跟踪集团对他们的认知位置和对他们的信念，这些命题已被提出，并跟踪了他们的认识论。在对可以调节协作的AI系统的兴趣增加的同时，Trace代表了可以参与多方，多模式话语的代理商迈出的重要一步。</li>
</ul>

<h3>Title: Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09516">https://arxiv.org/abs/2503.09516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09516">https://arxiv.org/pdf/2503.09516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09516]] Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning(https://arxiv.org/abs/2503.09516)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at this https URL.</li>
<li><strong>摘要：</strong>有效获取外部知识和最新信息对于大型语言模型（LLM）中的有效推理和文本生成至关重要。检索增强和工具使用培训方法，其中将搜索引擎视为工具缺乏复杂的多转弯检索灵活性或需要大规模监督数据。在推理过程中提示具有推理能力的高级LLM使用搜索引擎并不是最佳的，因为LLM无法学习如何与搜索引擎进行最佳互动。本文介绍了search-r1，这是deepSeek-r1模型的扩展，其中LLM仅通过增强学习（RL）学习，以自主生成（多个）搜索查询，并在实时检索中逐步推理。 Search-R1通过多转弯搜索交互优化LLM的推出，利用将令牌掩盖检索进行稳定的RL培训和简单的基于结果的奖励功能。在七个问题的数据集上进行的实验表明，Search-R1在SOTA碱基上提高了26％（QWEN2.5-7B），21％（QWEN2.5-3B）和10％（Llama3.2-3B）。本文进一步提供了对RL优化方法，LLM选择和响应长度动力学的经验见解。代码和模型检查点可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs</h3>
<ul>
<li><strong>Authors: </strong>Oskar van der Wal, Pietro Lesci, Max Muller-Eberstein, Naomi Saphra, Hailey Schoelkopf, Willem Zuidema, Stella Biderman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09543">https://arxiv.org/abs/2503.09543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09543">https://arxiv.org/pdf/2503.09543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09543]] PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs(https://arxiv.org/abs/2503.09543)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research community still lacks sufficient resources and tools to systematically investigate pre-training stability, particularly for decoder-only language models. We introduce the PolyPythias, a set of 45 new training runs for the Pythia model suite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting in about 7k new checkpoints that we release. Using these new 45 training runs, in addition to the 5 already available, we study the effects of different initial conditions determined by the seed -- i.e., parameters' initialisation and data order -- on (i) downstream performance, (ii) learned linguistic representations, and (iii) emergence of training phases. In addition to common scaling behaviours, our analyses generally reveal highly consistent training dynamics across both model sizes and initial conditions. Further, the new seeds for each model allow us to identify outlier training runs and delineate their characteristics. Our findings show the potential of using these methods to predict training stability.</li>
<li><strong>摘要：</strong>语言模型预训练及其对下游性能的影响的稳定性仍在研究中。先前的工作表明，训练过程可以响应初始条件（例如随机种子）而产生明显不同的结果。至关重要的是，研究界仍然缺乏足够的资源和工具来系统地研究训练前的稳定性，尤其是对于仅解码器语言模型而言。我们介绍了Polypythias，这是Pythia模型套件的45套新训练运行：5个型号尺寸的9种新种子，从14m到410m的参数，导致我们发布的大约7K新检查点。使用这些新的45个训练运行，除了已经可用的5个训练外，我们还研究了由种子确定的不同初始条件的影响 - 即参数的初始化和数据顺序 - （i）下游性能，（ii）学习的语言表示以及（iii）训练阶段的出现。除了常见的缩放行为外，我们的分析通常揭示了模型大小和初始条件的高度一致的训练动力。此外，每种型号的新种子使我们能够识别出异常训练的运行并描述其特征。我们的发现表明了使用这些方法预测训练稳定性的潜力。</li>
</ul>

<h3>Title: Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</h3>
<ul>
<li><strong>Authors: </strong>Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, Amir Gholami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09572">https://arxiv.org/abs/2503.09572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09572">https://arxiv.org/pdf/2503.09572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09572]] Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks(https://arxiv.org/abs/2503.09572)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of the-art 54% success rate on the WebArena-Lite benchmark.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在使语言代理能够处理简单任务方面显示出了显着的进步。但是，将它们应用于复杂的多步，长马的任务仍然是一个挑战。最近的工作通过将高级计划与低级执行分开，这使得模型能够有效地平衡高级计划目标和低级执行细节。但是，由于LLM并未固有地执行此任务，因此生成准确的计划仍然很困难。为了解决这个问题，我们提出了计划和效果，该计划将新颖的框架纳入了基于LLM的代理中，并引入了一种可扩展的方法，以通过新型的合成数据生成方法来增强计划生成。计划与操作由一个计划模型组成，该模型生成结构化的高级计划以实现用户目标，并将这些计划转化为特定环境动作的执行者模型。为了有效地培训计划者，我们引入了一种合成数据生成方法，该方法通过可行的计划注释地面真实轨迹，并以各种各样的广泛示例增强以增强概括。我们使用Web导航作为代表性的长途计划环境评估计划和行动，证明了Webarena-Lite基准的最先进的54％的成功率。</li>
</ul>

<h3>Title: Cost-Optimal Grouped-Query Attention for Long-Context LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yingfa Chen, Yutong Wu, Xu Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09579">https://arxiv.org/abs/2503.09579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09579">https://arxiv.org/pdf/2503.09579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09579]] Cost-Optimal Grouped-Query Attention for Long-Context LLMs(https://arxiv.org/abs/2503.09579)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.</li>
<li><strong>摘要：</strong>建立有效，高效的基于变压器的大型语言模型（LLM）最近已成为研究重点，需要最大程度地提高模型语言能力并最大程度地降低培训和部署成本。现有的努力主要描述了模型性能，参数大小和数据大小之间的复杂关系，并搜索了训练LLM的最佳计算分配。但是，他们忽略了上下文长度和注意力头配置的影响（在分组疑问注意力中的查询和键值头的数量）对训练和推理的影响。在本文中，我们系统地比较具有不同参数大小，上下文长度和注意力头配置的模型，以模型性能，计算成本和内存成本进行比较。然后，我们扩展了仅基于参数规模和训练计算的现有缩放方法，以指导培训和推理期间成本优势LLM的构建。我们的定量缩放研究表明，当处理足够长的序列时，具有较少注意力头的较大模型可以实现较低的损失，同时产生较低的计算和记忆成本。我们的发现为开发实用的LLM提供了宝贵的见解，尤其是在长期处理方案中。我们将公开发布我们的代码和数据。</li>
</ul>

<h3>Title: How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Ruohao Guo, Wei Xu, Alan Ritter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09598">https://arxiv.org/abs/2503.09598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09598">https://arxiv.org/pdf/2503.09598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09598]] How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation(https://arxiv.org/abs/2503.09598)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are widely deployed in diverse scenarios, the extent to which they could tacitly spread misinformation emerges as a critical safety concern. Current research primarily evaluates LLMs on explicit false statements, overlooking how misinformation often manifests subtly as unchallenged premises in real-world user interactions. We curated ECHOMIST, the first comprehensive benchmark for implicit misinformation, where the misinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based on rigorous selection criteria and carefully curated data from diverse sources, including real-world human-AI conversations and social media interactions. We also introduce a new evaluation metric to measure whether LLMs can recognize and counter false information rather than amplify users' misconceptions. Through an extensive empirical study on a wide range of LLMs, including GPT-4, Claude, and Llama, we find that current models perform alarmingly poorly on this task, often failing to detect false premises and generating misleading explanations. Our findings underscore the critical need for an increased focus on implicit misinformation in LLM safety research.</li>
<li><strong>摘要：</strong>由于大型语言模型（LLM）被广泛部署在各种情况下，因此它们可以默认传播错误信息的程度是一个关键的安全问题。当前的研究主要评估LLMS在明确的错误陈述上，忽略了错误信息通常在现实世界用户交互中巧妙地表现为无挑战的前提。我们策划了Echomist，这是隐式错误信息的第一个综合基准，其中错误的假设嵌入到LLMS的用户查询中。 Echomist基于严格的选择标准和来自不同来源的精心策划数据，包括现实世界中的人类对话和社交媒体互动。我们还引入了一个新的评估指标，以衡量LLM是否可以识别和反对虚假信息，而不是放大用户的误解。通过对包括GPT-4，Claude和Llama在内的广泛LLM的广泛实证研究，我们发现当前模型在这项任务上的表现较差，通常无法检测到错误的前提并产生误导性的解释。我们的发现强调了对LLM安全研究中隐含错误信息的关注的关键需求。</li>
</ul>

<h3>Title: MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System</h3>
<ul>
<li><strong>Authors: </strong>Jihao Zhao, Zhiyuan Ji, Zhaoxin Fan, Hanyu Wang, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09600">https://arxiv.org/abs/2503.09600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09600">https://arxiv.org/pdf/2503.09600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09600]] MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System(https://arxiv.org/abs/2503.09600)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.</li>
<li><strong>摘要：</strong>在作为大型语言模型（LLMS）的可行补充时，检索授权的一代（RAG）经常忽略其管道中文本块的关键方面。本文最初引入了一种双重评估方法，包括边界清晰度和块状粘性，以实现直接量化质量的量化。利用这种评估方法，我们强调了传统和语义块在处理复杂的上下文细微差别中的固有局限性，从而证实了将LLMS整合到块状过程中的必要性。为了解决基于LLM的方法中的计算效率和分块精度之间的固有权衡，我们设计了由三个阶段的处理机制组成的嵌入式粒度感知的混合物（MOC）框架。值得注意的是，我们的目标是指导块制品生成结构化的块列表，随后被用来从原始文本中提取块。广泛的实验表明，我们提出的指标和MOC框架都有效地解决了块状任务的挑战，从而揭示了块状内核的同时，同时增强了抹布系统的性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
