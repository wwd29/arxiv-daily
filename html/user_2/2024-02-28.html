<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-28</h1>
<h3>Title: EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math  Languages</h3>
<ul>
<li><strong>Authors: </strong>Johnathan Mercer</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16878">https://arxiv.org/abs/2402.16878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16878">https://arxiv.org/pdf/2402.16878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16878]] EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math  Languages(https://arxiv.org/abs/2402.16878)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Formal mathematics is the discipline of translating mathematics into a programming language in which any statement can be unequivocally checked by a computer. Mathematicians and computer scientists have spent decades of painstaking formalization efforts developing languages such as Coq, HOL, and Lean. Machine learning research has converged on these formal math corpora and given rise to an assortment of methodologies to aid in interactive and automated theorem proving. However, these papers have primarily focused on one method, for one proof task, in one language. This paper introduces EvoGPT-f: a novel evolutionary framework for the first systematic quantitative analysis of the differential machine learnability of five formal math corpora (Lean 3, Lean 4, Coq, HOL 4, HOL Light) using four tokenization methods (character, word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not put to rest the question of the "best" or "easiest" language to learn. Rather, this framework and preliminary findings begin to illuminate the differential machine learnability of these languages, offering a foundation to forge more systematic quantitative and qualitative comparative research across communities.</li>
<li><strong>摘要：</strong>形式数学是将数学转化为编程语言的学科，其中任何语句都可以由计算机明确地检查。数学家和计算机科学家花费了数十年的艰苦形式化努力来开发 Coq、HOL 和 Lean 等语言。机器学习研究已经集中在这些正式的数学语料库上，并产生了各种方法来帮助交互式和自动化的定理证明。然而，这些论文主要关注一种方法、一种语言、一种证明任务。本文介绍了 EvoGPT-f：一种新颖的进化框架，用于使用四种标记化方法（字符、单词）对五个正式数学语料库（Lean 3、Lean 4、Coq、HOL 4、HOL Light）的差异机器学习性进行首次系统定量分析-level、字节对编码和 StarCoder 分词器）。本文并没有解决“最好”或“最容易”学习的语言的问题。相反，这个框架和初步研究结果开始阐明这些语言的不同机器学习能力，为跨社区建立更系统的定量和定性比较研究提供了基础。</li>
</ul>

<h3>Title: BESA: Pruning Large Language Models with Blockwise Parameter-Efficient  Sparsity Allocation</h3>
<ul>
<li><strong>Authors: </strong>Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16880">https://arxiv.org/abs/2402.16880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16880">https://arxiv.org/pdf/2402.16880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16880]] BESA: Pruning Large Language Models with Blockwise Parameter-Efficient  Sparsity Allocation(https://arxiv.org/abs/2402.16880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at \href{https://github.com/OpenGVLab/LLMPrune-BESA}{here}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中表现出了出色的性能，例如文本摘要、文本问答等。虽然它们的性能令人印象深刻，但由于参数数量庞大，计算占用量可能令人望而却步。 SparseGPT 和 Wanda 等现有解决方案试图通过权重修剪来缓解这个问题。然而，它们的分层方法会对模型的输出产生显着的扰动，并且需要细致的超参数调整，例如剪枝率，这可能会对模型的整体性能产生不利影响。为了解决这个问题，本文通过应用块式重建损失引入了一种新颖的 LLM 剪枝技术，称为块式参数高效稀疏分配（BESA）。与典型的逐层剪枝技术相比，BESA 具有两个独特的属性：i）它针对各个变压器块的总体剪枝误差，ii）它以可微分的方式分配特定于层的稀疏性，两者都确保减少修剪后的性能下降。我们的实验表明，BESA 实现了最先进的性能，在短短五个小时内在单个 A100 GPU 上高效地修剪了 LLaMA1 和 LLaMA2 等具有 7B 到 70B 参数的 LLM。代码可在 \href{https://github.com/OpenGVLab/LLMPrune-BESA}{here} 获取。</li>
</ul>

<h3>Title: A prior Estimates for Deep Residual Network in Continuous-time  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16899">https://arxiv.org/abs/2402.16899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16899">https://arxiv.org/pdf/2402.16899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16899]] A prior Estimates for Deep Residual Network in Continuous-time  Reinforcement Learning(https://arxiv.org/abs/2402.16899)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dimensionality.</li>
<li><strong>摘要：</strong>深度强化学习在众多大规模实际应用中表现出色。然而，现有的性能分析忽略了连续时间控制问题的独特特征，无法直接估计贝尔曼最优损失的泛化误差，并且需要有界假设。我们的工作重点是连续时间控制问题，并提出了一种适用于所有此类问题的方法，其中过渡函数满足半群和 Lipschitz 性质。在这种方法下，我们可以直接分析贝尔曼最优损失的\emph{先验}泛化误差。该方法的核心在于损失函数的两次变换。为了完成转换，我们提出了最大算子的分解方法。此外，该分析方法不需要有界假设。最后，我们获得了\emph{先验}泛化误差，而没有维数灾难。</li>
</ul>

<h3>Title: PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA</h3>
<ul>
<li><strong>Authors: </strong>Sheng Wang, Boyang Xue, Jiacheng Ye, Jiyue Jiang, Liheng Chen, Lingpeng Kong, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16902">https://arxiv.org/abs/2402.16902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16902">https://arxiv.org/pdf/2402.16902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16902]] PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA(https://arxiv.org/abs/2402.16902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple instruction tuning datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRA as a resource-friendly alternative to LoRA.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的快速扩展，同时服务多个 LoRA 变得越来越不切实际，导致成本难以承受，并且需要更高效的参数微调方法。在这项工作中，我们引入了部分旋转增强低秩自适应（PRoLoRA），这是一种层内共享机制，包含四个基本组成部分：广播减少、旋转增强、部分共享细化和纠正初始化策略。作为 LoRA 的超集，PRoLoRA 继承了 LoRA 的优点，并有效规避了对等参数共享方法的弊端，具有优越的模型容量、实际可行性和广泛的适用性。实证实验证明了 PRoLoRA 在特定参数预算和性能目标场景中显着提高的参数效率，以及其可扩展至更大的 LLM。值得注意的是，在可训练参数减少一倍的情况下，PRoLoRA 在多指令调整数据集上仍然优于 LoRA。随后，进行了消融研究，以验证各个组件的必要性，并强调 PRoLoRA 相对于三种潜在变体的优越性。希望明显更高的参数效率能够使 PRoLoRA 成为 LoRA 的资源友好型替代方案。</li>
</ul>

<h3>Title: Enforcing Temporal Constraints on Generative Agent Behavior with  Reactive Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Raven Rothkopf, Hannah Tongxin Zeng, Mark Santolucito</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16905">https://arxiv.org/abs/2402.16905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16905">https://arxiv.org/pdf/2402.16905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16905]] Enforcing Temporal Constraints on Generative Agent Behavior with  Reactive Synthesis(https://arxiv.org/abs/2402.16905)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The surge in popularity of Large Language Models (LLMs) has opened doors for new approaches to the creation of interactive agents. However, managing the temporal behavior of such agents over the course of an interaction remains challenging. The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm. We propose a combination of formal logic-based program synthesis and LLM content generation to create generative agents that adhere to temporal constraints. Our approach uses Temporal Stream Logic (TSL) to generate an automaton that enforces a temporal structure on an agent and leaves the details of each action for a moment in time to an LLM. By using TSL, we are able to augment the generative agent where users have a higher level of guarantees on behavior, better interpretability of the system, and more ability to build agents in a modular way. We evaluate our approach on different tasks involved in creating a coherent interactive agent specialized for various application domains. We found that over all of the tasks, our approach using TSL achieves at least 96% adherence, whereas the pure LLM-based approach demonstrates as low as 14.67% adherence.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的流行为创建交互式代理的新方法打开了大门。然而，在交互过程中管理此类代理的时间行为仍然具有挑战性。连贯代理行为所需的有状态、长期视野和定量推理并不适合法学硕士范式。我们提出将基于形式逻辑的程序合成和法学硕士内容生成相结合，以创建遵守时间约束的生成代理。我们的方法使用时间流逻辑（TSL）来生成一个自动机，该自动机在代理上强制执行时间结构，并将每个动作的细节及时留给 LLM。通过使用 TSL，我们能够增强生成代理，使用户对行为有更高级别的保证、更好的系统可解释性以及更多以模块化方式构建代理的能力。我们评估了我们在创建专门针对各种应用程序领域的连贯交互式代理所涉及的不同任务上的方法。我们发现，在所有任务中，我们使用 TSL 的方法实现了至少 96% 的遵守率，而纯粹基于 LLM 的方法的遵守率低至 14.67%。</li>
</ul>

<h3>Title: Personalized Federated Instruction Tuning via Neural Architecture Search</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Zhang, Yingbo Zhou, Ming Hu, Junxian Feng, Jiawen Weng, Mingsong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16919">https://arxiv.org/abs/2402.16919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16919">https://arxiv.org/pdf/2402.16919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16919]] Personalized Federated Instruction Tuning via Neural Architecture Search(https://arxiv.org/abs/2402.16919)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Federated Instruction Tuning (FIT) has shown the ability to achieve collaborative model instruction tuning among massive data owners without sharing private data. However, it still faces two key challenges, i.e., data and resource heterogeneity. Due to the varying data distribution and preferences among data owners, FIT cannot adapt to the personalized data of individual owners. Moreover, clients with superior computational abilities are constrained since they need to maintain the same fine-tuning architecture as the weaker clients. To address these issues, we propose a novel Personalized Federated Instruction Tuning (PerFIT) framework based on architecture search. Specifically, PerFIT allows each client to search for a personalized architecture by expanding the trainable parameter space of the global model followed by pruning the parameters to the original state. This procedure allows personalized instruction fine-tuning within expanded parameter spaces, concurrently preserving the same number of trainable parameters. Furthermore, to release the abilities of heterogeneous computational resources and enhance the performance of personalization on local data, we exploit personalized parameter-wise aggregation. The evaluation with multiple LLMs non-IID scenarios demonstrates that compared to the state-of-the-art FIT methods, our approach can achieve up to a 23% decrease in perplexity.</li>
<li><strong>摘要：</strong>联合指令调优（FIT）已经展示了在不共享私有数据的情况下在海量数据所有者之间实现协作模型指令调优的能力。然而，它仍然面临着数据和资源异构性两个关键挑战。由于数据所有者之间的数据分布和偏好不同，FIT无法适应个体所有者的个性化数据。此外，具有卓越计算能力的客户端受到限制，因为它们需要与较弱的客户端保持相同的微调架构。为了解决这些问题，我们提出了一种基于架构搜索的新型个性化联合指令调优（PerFIT）框架。具体来说，PerFIT 允许每个客户通过扩展全局模型的可训练参数空间，然后将参数修剪到原始状态来搜索个性化架构。该过程允许在扩展的参数空间内进行个性化指令微调，同时保留相同数量的可训练参数。此外，为了释放异构计算资源的能力并增强本地数据的个性化性能，我们利用个性化参数方式聚合。对多个法学硕士非独立同分布场景的评估表明，与最先进的 FIT 方法相比，我们的方法可以将困惑度降低高达 23%。</li>
</ul>

<h3>Title: Successfully Guiding Humans with Imperfect Instructions by Highlighting  Potential Errors and Suggesting Corrections</h3>
<ul>
<li><strong>Authors: </strong>Lingjun Zhao, Khanh Nguyen, Hal Daumé III</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16973">https://arxiv.org/abs/2402.16973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16973">https://arxiv.org/pdf/2402.16973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16973]] Successfully Guiding Humans with Imperfect Instructions by Highlighting  Potential Errors and Suggesting Corrections(https://arxiv.org/abs/2402.16973)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.</li>
<li><strong>摘要：</strong>本文解决了在接地导航任务的背景下利用不完善的语言模型来指导人类决策的挑战。我们证明，不完美的指令生成模型可以通过有效的沟通机制来补充，以更成功地指导人类。我们构建的通信机制包括可以检测指令中潜在幻觉并提出实用替代方案的模型，以及向用户呈现该信息的直观界面。我们的研究表明，这种方法可将人类导航误差降低高达 29%，且不会带来额外的认知负担。这一结果强调了将多种通信渠道整合到人工智能系统中的潜力，以弥补其缺陷并增强其对人类的效用。</li>
</ul>

<h3>Title: Long Dialog Summarization: An Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ankan Mullick, Ayan Kumar Bhowmick, Raghav R, Ravi Kokku, Prasenjit Dey, Pawan Goyal, Niloy Ganguly</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16986">https://arxiv.org/abs/2402.16986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16986">https://arxiv.org/pdf/2402.16986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16986]] Long Dialog Summarization: An Analysis(https://arxiv.org/abs/2402.16986)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Dialog summarization has become increasingly important in managing and comprehending large-scale conversations across various domains. This task presents unique challenges in capturing the key points, context, and nuances of multi-turn long conversations for summarization. It is worth noting that the summarization techniques may vary based on specific requirements such as in a shopping-chatbot scenario, the dialog summary helps to learn user preferences, whereas in the case of a customer call center, the summary may involve the problem attributes that a user specified, and the final resolution provided. This work emphasizes the significance of creating coherent and contextually rich summaries for effective communication in various applications. We explore current state-of-the-art approaches for long dialog summarization in different domains and benchmark metrics based evaluations show that one single model does not perform well across various areas for distinct summarization tasks.</li>
<li><strong>摘要：</strong>对话摘要在管理和理解跨各个领域的大规模对话方面变得越来越重要。这项任务在捕捉多轮长对话的要点、背景和细微差别以进行总结方面提出了独特的挑战。值得注意的是，摘要技术可能会根据具体要求而有所不同，例如在购物聊天机器人场景中，对话摘要有助于了解用户偏好，而在客户呼叫中心的情况下，摘要可能涉及以下问题属性：用户指定，并提供最终解决方案。这项工作强调了为各种应用程序中的有效沟通创建连贯且上下文丰富的摘要的重要性。我们探索了当前在不同领域进行长对话摘要的最先进方法，并且基于基准指标的评估表明，单一模型在不同领域的不同摘要任务中表现不佳。</li>
</ul>

<h3>Title: What Do Language Models Hear? Probing for Auditory Representations in  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jerry Ngo, Yoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16998">https://arxiv.org/abs/2402.16998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16998">https://arxiv.org/pdf/2402.16998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16998]] What Do Language Models Hear? Probing for Auditory Representations in  Language Models(https://arxiv.org/abs/2402.16998)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.</li>
<li><strong>摘要：</strong>这项工作探讨了语言模型是否编码了物体声音的有意义的基础表征。我们学习一个线性探针，它可以在给定与该对象相关的音频片段的情况下检索该对象的正确文本表示，其中声音表示由预训练的音频模型给出。该探针通过对比损失进行训练，使对象的语言表示和声音表示彼此接近。训练后，测试探针泛化到训练期间未看到的对象的能力。在不同的语言模型和音频模型中，我们发现在许多情况下探测泛化高于偶然性，这表明尽管仅在原始文本上进行训练，但语言模型编码了某些对象的声音的基础知识。</li>
</ul>

<h3>Title: Benchmarking LLMs on the Semantic Overlap Summarization Task</h3>
<ul>
<li><strong>Authors: </strong>John Salvador, Naman Bansal, Mousumi Akter, Souvika Sarkar, Anupam Das, Shubhra Kanti Karmaker ("Santu")</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17008">https://arxiv.org/abs/2402.17008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17008">https://arxiv.org/pdf/2402.17008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17008]] Benchmarking LLMs on the Semantic Overlap Summarization Task(https://arxiv.org/abs/2402.17008)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Semantic Overlap Summarization (SOS) is a constrained multi-document summarization task, where the constraint is to capture the common/overlapping information between two alternative narratives. While recent advancements in Large Language Models (LLMs) have achieved superior performance in numerous summarization tasks, a benchmarking study of the SOS task using LLMs is yet to be performed. As LLMs' responses are sensitive to slight variations in prompt design, a major challenge in conducting such a benchmarking study is to systematically explore a variety of prompts before drawing a reliable conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed which can be used to design and explore various prompts for LLMs. Using this TELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs on the SOS Task, assessing their ability to summarize overlapping information from multiple alternative narratives. For evaluation, we report well-established metrics like ROUGE, BERTscore, and SEM-F1$ on two different datasets of alternative narratives. We conclude the paper by analyzing the strengths and limitations of various LLMs in terms of their capabilities in capturing overlapping information The code and datasets used to conduct this study are available at https://anonymous.4open.science/r/llm_eval-E16D.</li>
<li><strong>摘要：</strong>语义重叠摘要（SOS）是一种受约束的多文档摘要任务，其中约束是捕获两个替代叙述之间的共同/重叠信息。虽然大型语言模型 (LLM) 的最新进展在众多摘要任务中取得了卓越的性能，但使用 LLM 的 SOS 任务的基准研究尚未进行。由于法学硕士的反应对提示设计中的细微变化很敏感，因此进行此类基准研究的一个主要挑战是在得出可靠的结论之前系统地探索各种提示。幸运的是，最近，TELeR 分类法被提出，可用于设计和探索法学硕士的各种提示。本文使用 TELeR 分类法和 15 个流行的法学硕士，全面评估了 SOS 任务的法学硕士，评估他们从多种替代叙述中总结重叠信息的能力。为了进行评估，我们在两个不同的替代叙述数据集上报告了成熟的指标，例如 ROUGE、BERTscore 和 SEM-F1$。我们通过分析各种法学硕士在捕获重叠信息的能力方面的优势和局限性来总结本文。用于进行这项研究的代码和数据集可在 https://anonymous.4open.science/r/llm_eval-E16D 上找到。</li>
</ul>

<h3>Title: Can Large Language Models Recall Reference Location Like Humans?</h3>
<ul>
<li><strong>Authors: </strong>Ye Wang, Xinrun Xu, Rui Xie, Wenxin Hu, Wei Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17010">https://arxiv.org/abs/2402.17010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17010">https://arxiv.org/pdf/2402.17010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17010]] Can Large Language Models Recall Reference Location Like Humans?(https://arxiv.org/abs/2402.17010)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks.</li>
<li><strong>摘要：</strong>人类在完成知识密集型任务时，有时不仅需要一个答案，还需要相应的参考文章来辅助阅读。以前的方法需要通过额外的检索模型获取预先分割的文章块。本文探索利用大型语言模型 (LLM) 预训练阶段存储的参数化知识，从任何起始位置独立回忆参考段落。我们提出了一个两阶段框架，模拟人类回忆容易忘记的参考文献的场景。最初，法学硕士会被提示回忆文档标题标识符以获得粗粒度的文档集。然后，根据获取的粗粒度文档集，回忆出细粒度的段落。在两阶段召回过程中，我们使用约束解码来确保不会生成存储文档之外的内容。为了提高速度，我们在第二阶段只回忆一个短前缀，然后找到它的位置来检索完整的段落。 KILT知识敏感任务的实验验证了LLM可以在各种任务形式中独立回忆参考段落位置，并且获得的参考显着辅助下游任务。</li>
</ul>

<h3>Title: Leveraging Large Language Models for Learning Complex Legal Concepts  through Storytelling</h3>
<ul>
<li><strong>Authors: </strong>Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex 'Sandy' Pentland, Yoon Kim, Jad Kabbara, Deb Roy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17019">https://arxiv.org/abs/2402.17019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17019">https://arxiv.org/pdf/2402.17019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17019]] Leveraging Large Language Models for Learning Complex Legal Concepts  through Storytelling(https://arxiv.org/abs/2402.17019)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an RCT experiment with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.</li>
<li><strong>摘要：</strong>向非专家提供法律知识对于提高一般法律素养和鼓励公民参与民主至关重要。然而，对于没有法律背景的人来说，法律文件往往难以理解。在本文中，我们提出了大语言模型（LLM）在法律教育中的新颖应用，以帮助非专家通过讲故事来学习复杂的法律概念，这是传达复杂和抽象概念的有效教学工具。我们还引入了一个新的数据集 LegalStories，它由 295 个复杂的法律原则组成，每个原则都附有一个故事和一组由法学硕士生成的多项选择题。为了构建数据集，我们尝试了各种法学硕士来生成解释这些概念的法律故事。此外，我们使用专家循环方法来迭代设计多项选择题。然后，我们通过对法律新手对数据集中的 10 个样本进行 RCT 实验来评估法学硕士讲故事的有效性。我们发现，与仅定义相比，法学硕士生成的故事增强了非母语人士对法律概念的理解和对法律的兴趣。此外，故事不断帮助参与者将法律概念与他们的生活联系起来。最后，我们发现在后续评估中，通过故事学习对于非母语人士来说具有更高的保留率。我们的工作对于利用法学硕士促进法律领域及其他领域的教学和学习具有重要意义。</li>
</ul>

<h3>Title: Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM  Responses</h3>
<ul>
<li><strong>Authors: </strong>Juyeon Kim, Jeongeun Lee, Yoonho Chang, Chanyeol Choi, Junseong Kim, Jy-yong Sohn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17097">https://arxiv.org/abs/2402.17097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17097">https://arxiv.org/pdf/2402.17097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17097]] Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM  Responses(https://arxiv.org/abs/2402.17097)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required for the response revision process. Compared with existing methods including Factool, CoVE, and RARR, Re-Ex provides better revision performance with less time and fewer tokens in multiple benchmarks.</li>
<li><strong>摘要：</strong>减轻幻觉问题是我们需要克服的法学硕士的主要挑战之一，以便在现实世界中可靠地使用它们。最近，人们提出了各种方法来检查LLM生成的文本中的事实错误并进行相应的修改，以减少幻觉问题。在本文中，我们提出了 Re-Ex，一种修改 LLM 生成文本的方法，它引入了一个被称为事实错误解释步骤的新颖步骤。 Re-Ex通过三个步骤修改LLM的初始回复：首先，使用外部工具获取回复中事实错误的证据；其次，要求法学硕士根据第一步收集的证据解释答复中存在问题的部分；最后，法学硕士使用第二步中获得的解释修改答案。除了解释步骤之外，我们还提出了新的提示技术，以减少响应修订过程所需的令牌数量和挂钟时间。与 Factool、CoVE 和 RARR 等现有方法相比，Re-Ex 在多个基准测试中以更少的时间和更少的标记提供了更好的修订性能。</li>
</ul>

<h3>Title: Sinkhorn Distance Minimization for Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Cui, Yulei Qin, Yuting Gao, Enwei Zhang, Zihan Xu, Tong Wu, Ke Li, Xing Sun, Wengang Zhou, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17110">https://arxiv.org/abs/2402.17110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17110">https://arxiv.org/pdf/2402.17110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17110]] Sinkhorn Distance Minimization for Knowledge Distillation(https://arxiv.org/abs/2402.17110)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid of sample-wise KD that restricts the perception of divergence in each teacher-student sample pair. Instead, we propose a batch-wise reformulation to capture geometric intricacies of distributions across samples in the high-dimensional space. Comprehensive evaluation on GLUE and SuperGLUE, in terms of comparability, validity, and generalizability, highlights our superiority over state-of-the-art methods on all kinds of LLMs with encoder-only, encoder-decoder, and decoder-only architectures.</li>
<li><strong>摘要：</strong>知识蒸馏（KD）已被广泛采用来压缩大型语言模型（LLM）。现有的 KD 方法研究各种散度度量，包括 Kullback-Leibler (KL)、反向 Kullback-Leibler (RKL) 和 Jensen-Shannon (JS) 散度。然而，由于其假设和定义固有的局限性，当教师和学生之间很少存在分配重叠时，这些措施无法提供有效的监督。在本文中，我们表明上述 KL、RKL 和 JS 散度分别存在模式平均、模式崩溃和模式低估问题，这会恶化各种 NLP 任务的基于 logits 的 KD。我们提出了 Sinkhorn 知识蒸馏（SinKD），它利用 Sinkhorn 距离来确保对教师和学生分布之间的差异进行细致而精确的评估。此外，得益于 Sinkhorn 度量的特性，我们可以摆脱限制每个师生样本对的分歧感知的样本 KD。相反，我们提出了一种分批重构来捕获高维空间中样本分布的几何复杂性。对 GLUE 和 SuperGLUE 在可比性、有效性和通用性方面的综合评估，凸显了我们相对于各种仅编码器、编码器-解码器和仅解码器架构的 LLM 的最先进方法的优越性。</li>
</ul>

<h3>Title: Creating Suspenseful Stories: Iterative Planning with Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Kaige Xie, Mark Riedl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17119">https://arxiv.org/abs/2402.17119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17119">https://arxiv.org/pdf/2402.17119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17119]] Creating Suspenseful Stories: Iterative Planning with Large Language  Models(https://arxiv.org/abs/2402.17119)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.</li>
<li><strong>摘要：</strong>自动故事生成一直是 NLP 领域长期存在的挑战之一。在故事的各个维度中，悬念在人类编写的故事中非常常见，但在人工智能生成的故事中相对较少被探索。虽然大型语言模型 (LLM) 的最新进展总体上极大地促进了语言生成，但在生成悬疑故事时，最先进的 LLM 仍然不可靠。我们提出了一种新颖的基于迭代提示的规划方法，该方法基于认知心理学和叙事学的故事悬念两个理论基础。这种基于理论的方法以完全零样本的方式工作，并且不依赖于任何有监督的故事语料库。据我们所知，本文是法学硕士首次尝试生成悬疑故事。对生成的悬疑故事进行广泛的人类评估证明了我们方法的有效性。</li>
</ul>

<h3>Title: Fact-and-Reflection (FaR) Improves Confidence Calibration of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Tongshuang Wu, Jianshu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17124">https://arxiv.org/abs/2402.17124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17124">https://arxiv.org/pdf/2402.17124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17124]] Fact-and-Reflection (FaR) Improves Confidence Calibration of Large  Language Models(https://arxiv.org/abs/2402.17124)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known "facts" that are relevant to the input prompt from the LLM. And then it asks the model to "reflect" over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.</li>
<li><strong>摘要：</strong>对于一个值得信赖的法学硕士来说，其置信水平应该与其实际表现进行良好的校准。虽然现在众所周知，法学硕士的表现很大程度上受到提示的影响，但提示法学硕士的信心校准尚未得到彻底探索。在本文中，我们探讨了不同的提示策略如何影响 LLM 信心校准以及如何改进。我们对问答环境中的六种提示方法进行了广泛的实验，我们观察到，虽然这些方法有助于提高预期的法学硕士校准，但它们也会导致法学硕士在应对某些情况时过于自信。受人类认知的启发，我们提出事实与反思（FaR）提示，分两步改进了 LLM 校准。首先，FaR 引出与 LLM 的输入提示相关的已知“事实”。然后它要求模型“反思”它们以生成最终答案。实验表明FaR提示取得了明显更好的校准效果；它使我们的多用途 QA 任务的预期校准误差降低了 23.5%。值得注意的是，FaR 提示甚至可以在不太自信的场景中引发口头表达担忧的能力，这有助于触发检索增强来解决这些更困难的情况。</li>
</ul>

<h3>Title: Unsupervised Zero-Shot Reinforcement Learning via Functional Reward  Encodings</h3>
<ul>
<li><strong>Authors: </strong>Kevin Frans, Seohong Park, Pieter Abbeel, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17135">https://arxiv.org/abs/2402.17135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17135">https://arxiv.org/pdf/2402.17135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17135]] Unsupervised Zero-Shot Reinforcement Learning via Functional Reward  Encodings(https://arxiv.org/abs/2402.17135)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: https://github.com/kvfrans/fre</li>
<li><strong>摘要：</strong>我们能否从大量未标记的离线轨迹中预训练一个多面手智能体，使其能够以零样本的方式立即适应任何新的下游任务？在这项工作中，我们提出了一种功能奖励编码（FRE）作为零样本强化学习问题的通用、可扩展的解决方案。我们的主要想法是通过使用基于变压器的变分自动编码器对状态奖励样本进行编码来学习任意任务的功能表示。这种功能编码不仅能够从各种通用无监督奖励函数中对代理进行预训练，而且还提供了一种在给定少量奖励注释样本的情况下以零样本方式解决任何新下游任务的方法。我们凭经验表明，在各种随机无监督奖励函数上训练的 FRE 代理可以泛化以解决一系列模拟机器人基准中的新任务，通常优于以前的零样本 RL 和离线 RL 方法。该项目的代码位于：https://github.com/kvfrans/fre</li>
</ul>

<h3>Title: Actions Speak Louder than Words: Trillion-Parameter Sequential  Transducers for Generative Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, Yu Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17152">https://arxiv.org/abs/2402.17152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17152">https://arxiv.org/pdf/2402.17152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17152]] Actions Speak Louder than Words: Trillion-Parameter Sequential  Transducers for Generative Recommendations(https://arxiv.org/abs/2402.17152)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4\% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.</li>
<li><strong>摘要：</strong>大规模推荐系统的特点是依赖高基数、异构特征以及每天需要处理数百亿个用户操作。尽管经过了具有数千个特征的海量数据的训练，但业界大多数深度学习推荐模型（DLRM）都无法随着计算而扩展。受到 Transformers 在语言和视觉领域取得的成功的启发，我们重新审视推荐系统中的基本设计选择。我们将推荐问题重新表述为生成建模框架（“生成推荐器”）内的顺序转换任务，并提出了一种新的架构 HSTU，专为高基数、非平稳流推荐数据而设计。在 NDCG 中，HSTU 的性能比合成数据集和公共数据集的基线高出 65.8%，并且在 8192 长度序列上比基于 FlashAttention2 的 Transformer 快 5.3 倍到 15.2 倍。基于 HSTU 的生成推荐器拥有 1.5 万亿个参数，将在线 A/B 测试的指标提高了 12.4%，并已部署在拥有数十亿用户的大型互联网平台的多个表面上。更重要的是，生成推荐器的模型质量根据经验可扩展为三个数量级的训练计算幂律，最高可达 GPT-3/LLaMa-2 规模，这减少了未来模型开发所需的碳足迹，并进一步为未来的模型开发奠定了基础。建议中第一个基础模型的方式。</li>
</ul>

<h3>Title: Large Language Model for Participatory Urban Planning</h3>
<ul>
<li><strong>Authors: </strong>Zhilun Zhou, Yuming Lin, Depeng Jin, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17161">https://arxiv.org/abs/2402.17161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17161">https://arxiv.org/pdf/2402.17161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17161]] Large Language Model for Participatory Urban Planning(https://arxiv.org/abs/2402.17161)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Participatory urban planning is the mainstream of modern urban planning that involves the active engagement of residents. However, the traditional participatory paradigm requires experienced planning experts and is often time-consuming and costly. Fortunately, the emerging Large Language Models (LLMs) have shown considerable ability to simulate human-like agents, which can be used to emulate the participatory process easily. In this work, we introduce an LLM-based multi-agent collaboration framework for participatory urban planning, which can generate land-use plans for urban regions considering the diverse needs of residents. Specifically, we construct LLM agents to simulate a planner and thousands of residents with diverse profiles and backgrounds. We first ask the planner to carry out an initial land-use plan. To deal with the different facilities needs of residents, we initiate a discussion among the residents in each community about the plan, where residents provide feedback based on their profiles. Furthermore, to improve the efficiency of discussion, we adopt a fishbowl discussion mechanism, where part of the residents discuss and the rest of them act as listeners in each round. Finally, we let the planner modify the plan based on residents' feedback. We deploy our method on two real-world regions in Beijing. Experiments show that our method achieves state-of-the-art performance in residents satisfaction and inclusion metrics, and also outperforms human experts in terms of service accessibility and ecology metrics.</li>
<li><strong>摘要：</strong>参与式城市规划是现代城市规划的主流，需要居民的积极参与。然而，传统的参与范式需要经验丰富的规划专家，而且往往耗时且成本高昂。幸运的是，新兴的大型语言模型（LLM）已经显示出相当大的模拟类人代理的能力，可以用来轻松模拟参与过程。在这项工作中，我们引入了一种基于法学硕士的参与式城市规划多主体协作框架，该框架可以考虑居民的多样化需求，为城市地区制定土地利用规划。具体来说，我们构建了 LLM 代理来模拟规划者和数千名具有不同背景和背景的居民。我们首先要求规划师进行初步的土地利用规划。针对居民不同的设施需求，我们在各个社区发起了居民讨论，居民根据自己的情况提出反馈。此外，为了提高讨论效率，我们采用鱼缸讨论机制，每轮由部分居民进行讨论，其余居民作为倾听者。最后，我们让规划师根据居民的反馈修改方案。我们将我们的方法部署在北京的两个现实世界区域。实验表明，我们的方法在居民满意度和包容性指标方面实现了最先进的性能，并且在服务可及性和生态指标方面也优于人类专家。</li>
</ul>

<h3>Title: Benchmarking Data Science Agents</h3>
<ul>
<li><strong>Authors: </strong>Yuge Zhang, Qiyang Jiang, Xingyu Han, Nan Chen, Yuqing Yang, Kan Ren</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17168">https://arxiv.org/abs/2402.17168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17168">https://arxiv.org/pdf/2402.17168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17168]] Benchmarking Data Science Agents(https://arxiv.org/abs/2402.17168)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.</li>
<li><strong>摘要：</strong>在数据驱动决策的时代，数据分析的复杂性需要先进的数据科学专业知识和工具，这对专家来说也提出了巨大的挑战。大型语言模型 (LLM) 已成为数据科学代理的有前途的辅助工具，可帮助人类进行数据分析和处理。然而，它们的实际功效仍然受到现实应用的不同需求和复杂的分析过程的限制。在本文中，我们介绍了 DSEval——一种新颖的评估范式，以及一系列为评估这些代理在整个数据科学生命周期中的性能而定制的创新基准。结合一种新颖的引导注释方法，我们简化了数据集准备，提高了评估覆盖率，并扩展了基准测试的全面性。我们的研究结果揭示了普遍存在的障碍，并为该领域未来的进步提供了重要的见解。</li>
</ul>

<h3>Title: Dual-Space Optimization: Improved Molecule Sequence Design by Latent  Prompt Transformer</h3>
<ul>
<li><strong>Authors: </strong>Deqian Kong, Yuhao Huang, Jianwen Xie, Edouardo Honig, Ming Xu, Shuanghong Xue, Pei Lin, Sanping Zhou, Sheng Zhong, Nanning Zheng, Ying Nian Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17179">https://arxiv.org/abs/2402.17179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17179">https://arxiv.org/pdf/2402.17179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17179]] Dual-Space Optimization: Improved Molecule Sequence Design by Latent  Prompt Transformer(https://arxiv.org/abs/2402.17179)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer. Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks.</li>
<li><strong>摘要：</strong>设计具有所需特性的分子，例如药物相似性和对蛋白质靶标的高结合亲和力，是一个具有挑战性的问题。在本文中，我们提出了集成潜在空间采样和数据空间选择的双空间优化（DSO）方法来解决这个问题。 DSO 在优化过程中迭代更新潜在空间生成模型和合成数据集，逐渐将生成模型和合成数据移向所需属性值的区域。我们的生成模型采用潜在提示变换器（LPT）的形式，其中潜在向量充当因果变换器的提示。我们广泛的实验证明了所提出方法的有效性，该方法为单目标、多目标和约束分子设计任务设定了新的性能基准。</li>
</ul>

<h3>Title: When Scaling Meets LLM Finetuning: The Effect of Data, Model and  Finetuning Method</h3>
<ul>
<li><strong>Authors: </strong>Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17193">https://arxiv.org/abs/2402.17193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17193">https://arxiv.org/pdf/2402.17193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17193]] When Scaling Meets LLM Finetuning: The Effect of Data, Model and  Finetuning Method(https://arxiv.org/abs/2402.17193)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.</li>
<li><strong>摘要：</strong>虽然大型语言模型（LLM）经常采用微调来释放其下游应用程序的功能，但我们对不同微调方法的归纳偏差（特别是缩放属性）的理解仍然有限。为了填补这一空白，我们进行了系统的实验，研究不同的缩放因素（包括LLM模型大小、预训练数据大小、新的微调参数大小和微调数据大小）是否以及如何影响微调性能。我们考虑两种类型的微调——全模型调优（FMT）和参数高效调优（PET，包括即时调优和 LoRA），并探索它们在数据有限的情况下的缩放行为，其中 LLM 模型大小大大超过了微调数据尺寸。基于两组预训练的 1B 到 16B 的双语 LLM 以及双语机器翻译和多语言摘要基准的实验，我们发现 1）LLM 微调遵循微调数据大小和各个缩放因子之间基于幂的乘法联合缩放定律； 2）LLM微调比预训练数据缩放更受益于LLM模型缩放，而PET参数缩放通常无效； 3）最佳微调方法高度依赖于任务和微调数据。我们希望我们的研究结果能够为理解、选择和开发法学硕士微调方法提供启发。</li>
</ul>

<h3>Title: Measuring Vision-Language STEM Skills of Neural Models</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17205">https://arxiv.org/abs/2402.17205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17205">https://arxiv.org/pdf/2402.17205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17205]] Measuring Vision-Language STEM Skills of Neural Models(https://arxiv.org/abs/2402.17205)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset. Even though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.</li>
<li><strong>摘要：</strong>我们引入了一个新的挑战来测试神经模型的 STEM 技能。现实世界中的问题通常需要结合 STEM（科学、技术、工程和数学）知识来解决。与现有数据集不同，我们的数据集需要理解 STEM 的多模态视觉语言信息。我们的数据集是应对挑战的最大、最全面的数据集之一。它包含涵盖所有 STEM 科目的 448 项技能和 1,073,146 个问题。与通常侧重于检查专家级能力的现有数据集相比，我们的数据集包括基于 K-12 课程设计的基本技能和问题。我们还在基准测试中添加了最先进的基础模型，例如 CLIP 和 GPT-3.5-Turbo。结果表明，最近的模型进展仅有助于掌握我们数据集中非常有限数量的较低年级技能（三年级中的 2.5%）。事实上，这些模型仍然远低于小学生的表现（平均 54.7%），更不用说接近专家水平的表现了。为了理解并提高数据集的性能，我们在数据集的训练分割上教授模型。尽管我们观察到表现有所改善，但与普通小学生相比，模型表现仍然相对较低。为了解决 STEM 问题，我们需要社区的新颖算法创新。</li>
</ul>

<h3>Title: Temporal Logic Specification-Conditioned Decision Transformer for  Offline Safe Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zijian Guo, Weichao Zhou, Wenchao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17217">https://arxiv.org/abs/2402.17217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17217">https://arxiv.org/pdf/2402.17217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17217]] Temporal Logic Specification-Conditioned Decision Transformer for  Offline Safe Reinforcement Learning(https://arxiv.org/abs/2402.17217)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is conditioned on.</li>
<li><strong>摘要：</strong>离线安全强化学习（RL）旨在从固定数据集训练约束满足策略。当前最先进的方法基于具有条件策略的监督学习。然而，这些方法在涉及具有丰富时间和逻辑结构的复杂任务的实际应用中存在不足。在本文中，我们提出了时序逻辑规范条件决策变换器（SDT），这是一种新颖的框架，它利用信号时序逻辑（STL）的表达能力来指定代理应遵循的复杂时序规则以及决策变换器的顺序建模能力（DT）。对 DSRL 基准的实证评估表明，与现有方法相比，SDT 在学习安全和高回报策略方面具有更好的能力。此外，SDT 对其所依赖的 STL 规范的不同期望满足程度表现出良好的一致性。</li>
</ul>

<h3>Title: Reasoning in Conversation: Solving Subjective Tasks through Dialogue  Simulation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Wang, Yile Wang, Yuanchi Zhang, Fuwen Luo, Peng Li, Maosong Sun, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17226">https://arxiv.org/abs/2402.17226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17226">https://arxiv.org/pdf/2402.17226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17226]] Reasoning in Conversation: Solving Subjective Tasks through Dialogue  Simulation for Large Language Models(https://arxiv.org/abs/2402.17226)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在开放领域问答和数学推理等客观任务中取得了显着的表现，这些任务通常可以通过回忆学到的事实知识或思维链式推理来解决。然而，我们发现法学硕士在主观任务中的表现仍然不能令人满意，例如隐喻识别、黑色幽默检测等。与客观任务相比，主观任务更注重解释或情绪反应，而不是普遍接受的推理路径。基于任务的特点和法学硕士强大的对话生成能力，我们提出了RiC（Reasoning in Conversation），一种专注于通过对话模拟解决主观任务的方法。 RiC 的动机是通过模拟对话来挖掘有用的上下文信息，而不是提供思维链式的基本原理，从而提供对话背后潜在的有用知识以给出最终答案。我们在 12 项任务中评估基于 API 的 LLM 和开源 LLM，包括 GPT-4、ChatGPT 和 OpenChat。实验结果表明，与各种基线相比，RiC 可以产生显着的改进。</li>
</ul>

<h3>Title: MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical  Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17231">https://arxiv.org/abs/2402.17231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17231">https://arxiv.org/pdf/2402.17231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17231]] MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical  Reasoning(https://arxiv.org/abs/2402.17231)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical disciplines. We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance. MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with chain-of-thought on the MATH dataset. We further observe that TALMs are not as effective for simpler math word problems (in GSM-8k), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH). The code and data are available at https://github.com/Debrup-61/MathSensei.</li>
<li><strong>摘要：</strong>众所周知，工具增强大型语言模型 (TALM) 可以增强大型语言模型 (LLM) 的技能，从而提高其在许多任务中的推理能力。虽然 TALM 已成功应用于不同的问答基准，但它们在复杂数学推理基准上的功效，以及知识检索和数学方程求解工具提供的潜在互补优势，都是开放的研究问题。在这项工作中，我们提出了 MATHSENSEI，一种用于数学推理的工具增强大型语言模型。通过知识检索（Bing Web 搜索）、程序执行（Python）和符号方程求解（Wolfram-Alpha）工具的增强，我们通过对数学推理数据集的评估来研究这些工具的互补优势。我们对 MATH 进行了详尽的消融，这是一个用于评估不同数学学科的数学推理的流行数据集。我们还进行了涉及知名工具规划者的实验，以研究工具排序对模型性能的影响。 MATHSENSEI 在 MATH 数据集上的思想链比 gpt-3.5-turbo 的准确度提高了 13.5%。我们进一步观察到，TALM 对于更简单的数学应用题（在 GSM-8k 中）并不那么有效，并且随着复杂性和所需知识的增加，其好处也会增加（逐渐超过 AQuA、MMLU-Math 和数学中更高级别的复杂问题）。代码和数据可在 https://github.com/Debrup-61/MathSensei 获取。</li>
</ul>

<h3>Title: Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent  Detection</h3>
<ul>
<li><strong>Authors: </strong>Pei Wang, Keqing He, Yejie Wang, Xiaoshuai Song, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17256">https://arxiv.org/abs/2402.17256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17256">https://arxiv.org/pdf/2402.17256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17256]] Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent  Detection(https://arxiv.org/abs/2402.17256)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Out-of-domain (OOD) intent detection aims to examine whether the user's query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by fine-tuning discriminative models. Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD, and understanding long instructions.</li>
<li><strong>摘要：</strong>域外（OOD）意图检测旨在检查用户的查询是否超出系统的预定义域，这对于面向任务的对话（TOD）系统的正常运行至关重要。以前的方法通过微调判别模型来解决这个问题。最近，一些研究一直在探索以ChatGPT为代表的大语言模型（LLM）在各种下游任务中的应用，但其在OOD检测任务上的能力仍不清楚。本文对各种实验设置下的LLM进行了综合评估，然后概述法学硕士的优点和缺点。我们发现LLM表现出强大的零样本和少样本能力，但与全资源微调的模型相比仍然处于劣势。更深入地说，通过一系列额外的分析实验，我们讨论和总结了LLM面临的挑战，并为未来的工作提供指导，包括注入领域知识、加强从IND（域内）到OOD的知识转移以及理解长指令。</li>
</ul>

<h3>Title: Speak Out of Turn: Safety Vulnerability of Large Language Models in  Multi-turn Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17262">https://arxiv.org/abs/2402.17262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17262">https://arxiv.org/pdf/2402.17262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17262]] Speak Out of Turn: Safety Vulnerability of Large Language Models in  Multi-turn Dialogue(https://arxiv.org/abs/2402.17262)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to "jailbreak." Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已被证明会产生非法或不道德的响应，特别是在“越狱”时。对越狱的研究凸显了法学硕士的安全问题。然而，先前的研究主要集中在单轮对话，忽略了多轮对话所带来的潜在复杂性和风险，而多轮对话是人类从法学硕士中获取信息的关键模式。在本文中，我们认为人类可以利用多轮对话来诱导法学硕士产生有害信息。法学硕士可能不打算拒绝警告性或边缘不安全的查询，即使在多轮对话中每一轮都紧密服务于一个恶意目的。因此，通过将不安全的查询分解为多个子查询以进行多轮对话，我们诱导法学硕士逐步回答有害的子问题，最终产生总体有害的响应。我们在广泛的法学硕士中进行的实验表明，目前法学硕士在多轮对话中的安全机制存在不足。我们的研究结果暴露了法学硕士在涉及多轮对话的复杂场景中的脆弱性，为法学硕士的安全提出了新的挑战。</li>
</ul>

<h3>Title: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten de Rijke, Zhumin Chen, Jiahuan Pei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17263">https://arxiv.org/abs/2402.17263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17263">https://arxiv.org/pdf/2402.17263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17263]] Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2402.17263)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.</li>
<li><strong>摘要：</strong>参数高效微调 (PEFT) 是一种定制预训练大型语言模型 (LLM) 的流行方法，尤其是随着模型规模和任务多样性的增加。低秩自适应（LoRA）基于自适应过程本质上是低维的思想，即可以用相对较少的参数来表示显着的模型变化。然而，与全参数微调相比，降低排名会遇到特定任务泛化错误的挑战。我们提出了 MELoRA，一种迷你集成低秩适配器，它使用更少的可训练参数，同时保持较高的秩，从而提供改进的性能潜力。其核心思想是冻结原有的预训练权重，只用少量参数训练一组迷你LoRA。这可以捕获迷你 LoRA 之间的显着程度的多样性，从而提高泛化能力。我们对各种 NLP 任务进行理论分析和实证研究。我们的实验结果表明，与 LoRA 相比，MELoRA 取得了更好的性能，在自然语言理解任务上可训练参数减少了 8 倍，在指令跟随任务上可训练参数减少了 36 倍，这证明了 MELoRA 的有效性。</li>
</ul>

<h3>Title: Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social  Dilemmas</h3>
<ul>
<li><strong>Authors: </strong>Hao Guo, Chunjiang Mu, Yang Chen, Chen Shen, Shuyue Hu, Zhen Wang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.GT, cs.HC, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17270">https://arxiv.org/abs/2402.17270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17270">https://arxiv.org/pdf/2402.17270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17270]] Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social  Dilemmas(https://arxiv.org/abs/2402.17270)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as using large language models, establishing unified theoretical frameworks, revisiting existing theories of human cooperation, and exploring multiple real-world applications.</li>
<li><strong>摘要：</strong>长期以来，对社会困境中的合作的研究一直是包括计算机科学和社会科学在内的各个学科的基本主题。人工智能 (AI) 的最新进展极大地重塑了这一领域，为理解和加强合作提供了新的见解。这项调查研究了人工智能与社会困境中的合作交叉点的三个关键领域。首先，我们关注多智能体合作，回顾了支持理性智能体之间合作的内在和外在动机，以及针对不同对手制定有效策略的方法。其次，研究人类与智能体的合作，我们讨论了当前与人类合作的人工智能算法以及人类对人工智能体的偏见。第三，我们回顾了利用人工智能代理加强人类之间合作的新兴领域。最后，我们讨论了未来的研究途径，例如使用大型语言模型、建立统一的理论框架、重新审视现有的人类合作理论以及探索多种现实世界的应用。</li>
</ul>

<h3>Title: Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in  Indonesian and Sundanese</h3>
<ul>
<li><strong>Authors: </strong>Rifki Afina Putri, Faiz Ghifari Haznitrama, Dea Adhista, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17302">https://arxiv.org/abs/2402.17302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17302">https://arxiv.org/pdf/2402.17302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17302]] Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in  Indonesian and Sundanese(https://arxiv.org/abs/2402.17302)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators. Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages. We also benchmark various LLMs on our generated datasets and find that they perform better on the LLM-generated datasets compared to those created by humans.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于生成用于训练和评估模型的合成数据。然而，尚不清楚他们是否可以生成高质量的问答（QA）数据集，将知识和文化细微差别嵌入到语言中，特别是对于资源匮乏的语言。在这项研究中，我们研究了使用法学硕士为印度尼西亚语和巽他语生成文化相关常识问答数据集的有效性。为此，我们使用涉及法学硕士和人工注释者的各种方法为这些语言创建数据集。我们的实验表明，当前表现最好的法学硕士 GPT-4 Turbo 能够用印度尼西亚语生成具有足够知识的问题，但不能用巽他语生成问题，这凸显了中等资源语言和低资源语言之间的性能差异。我们还在我们生成的数据集上对各种法学硕士进行了基准测试，发现与人类创建的数据集相比，它们在法学硕士生成的数据集上表现更好。</li>
</ul>

<h3>Title: Probing Multimodal Large Language Models for Global and Local Semantic  Representation</h3>
<ul>
<li><strong>Authors: </strong>Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17304">https://arxiv.org/abs/2402.17304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17304">https://arxiv.org/pdf/2402.17304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17304]] Probing Multimodal Large Language Models for Global and Local Semantic  Representation(https://arxiv.org/abs/2402.17304)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information.</li>
<li><strong>摘要：</strong>大型语言模型的成功激发了研究人员将其卓越的表示能力转移到其他模式。最近的几项工作利用图像标题对齐数据集来训练多模态大语言模型（MLLM），该模型在图像到文本任务上实现了最先进的性能。然而，很少有研究探讨 MLLM 是否真正理解完整的图像信息，即全局信息，或者它们是否只能捕获一些局部目标信息。在这项研究中，我们发现模型的中间层可以编码更多的全局语义信息，其表示向量在视觉语言蕴涵任务上表现得更好，而不是最顶层。我们通过对象检测任务进一步探索局部语义表示的模型。我们得出的结论是，最顶层可能过度关注局部信息，导致编码全局信息的能力下降。</li>
</ul>

<h3>Title: LocalGCL: Local-aware Contrastive Learning for Graphs</h3>
<ul>
<li><strong>Authors: </strong>Haojun Jiang, Jiawei Sun, Jie Li, Chentao Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17345">https://arxiv.org/abs/2402.17345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17345">https://arxiv.org/pdf/2402.17345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17345]] LocalGCL: Local-aware Contrastive Learning for Graphs(https://arxiv.org/abs/2402.17345)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Graph representation learning (GRL) makes considerable progress recently, which encodes graphs with topological structures into low-dimensional embeddings. Meanwhile, the time-consuming and costly process of annotating graph labels manually prompts the growth of self-supervised learning (SSL) techniques. As a dominant approach of SSL, Contrastive learning (CL) learns discriminative representations by differentiating between positive and negative samples. However, when applied to graph data, it overemphasizes global patterns while neglecting local structures. To tackle the above issue, we propose \underline{Local}-aware \underline{G}raph \underline{C}ontrastive \underline{L}earning (\textbf{\methnametrim}), a self-supervised learning framework that supplementarily captures local graph information with masking-based modeling compared with vanilla contrastive learning. Extensive experiments validate the superiority of \methname against state-of-the-art methods, demonstrating its promise as a comprehensive graph representation learner.</li>
<li><strong>摘要：</strong>图表示学习（GRL）最近取得了相当大的进展，它将具有拓扑结构的图编码为低维嵌入。与此同时，手动注释图标签的耗时且昂贵的过程促进了自监督学习（SSL）技术的发展。作为 SSL 的主要方法，对比学习（CL）通过区分正样本和负样本来学习判别表示。然而，当应用于图数据时，它过分强调全局模式而忽略局部结构。为了解决上述问题，我们提出了 \underline{Local}-aware \underline{G}raph \underline{C}contrastive \underline{L}earning (\textbf{\methnametrim})，一个自我监督的学习框架，补充与普通对比学习相比，通过基于掩蔽的建模捕获局部图形信息。大量的实验验证了 \methname 相对于最先进方法的优越性，证明了它作为全面的图形表示学习器的前景。</li>
</ul>

<h3>Title: RECOST: External Knowledge Guided Data-efficient Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhang, Yiming Zhang, Haobo Wang, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17355">https://arxiv.org/abs/2402.17355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17355">https://arxiv.org/pdf/2402.17355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17355]] RECOST: External Knowledge Guided Data-efficient Instruction Tuning(https://arxiv.org/abs/2402.17355)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which integrates external-knowledge-base re-ranking and diversity-consistent sampling into a single pipeline. Through extensive experiments on several synthetic datasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our method and achieve even better results with only \textbf{1\%} of the full dataset.</li>
<li><strong>摘要：</strong>在当前的大型语言模型（LLM）领域，指令调优过程是一个重要的步骤。考虑到较高的计算能力开销，提出了数据高效的指令调优，以减少该过程中的训练数据大小，旨在选择高质量的指令数据。尽管如此，我们认为大多数当前数据高效的指令调优方法高度依赖于原始指令调优数据集的质量。当涉及到LLM合成的数据集时，这是该领域的常见场景，脏样本甚至会比其他样本以更高的概率被选择。为了应对这些挑战，我们利用外部知识（相关示例或段落）来评估法学硕士合成的样本，并使用基于上下文的相对预测熵。基于新的指标，我们提出了一个名为 \textbf{RECOST} 的框架，它将外部知识库重排序和多样性一致采样集成到单个管道中。通过对几个合成数据集（Alpaca 和 Alpaca-gpt4）的广泛实验，我们证明了我们方法的有效性，并且仅使用完整数据集的 \textbf{1\%} 即可获得更好的结果。</li>
</ul>

<h3>Title: SoFA: Shielded On-the-fly Alignment via Priority Rule Following</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Lu, Bowen Yu, Yaojie Lu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17358">https://arxiv.org/abs/2402.17358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17358">https://arxiv.org/pdf/2402.17358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17358]] SoFA: Shielded On-the-fly Alignment via Priority Rule Following(https://arxiv.org/abs/2402.17358)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的对齐问题涉及使其适应广泛的人类价值观。由于偏好和监管标准的多样性，这一要求对现有的调整方法提出了挑战。本文介绍了一种新颖的对齐范例，即优先规则遵循，它将规则定义为每个对话框中的主要控制机制，优先于用户指令。我们的初步分析表明，即使是高级法学硕士，例如 GPT-4，在理解规则和确定规则优先级方面也存在缺陷。因此，我们提出了 PriorityDistill，这是一种半自动化方法，用于从 LLM 模拟中提取优先级跟踪信号，以确保稳健的规则集成和遵守。我们的实验表明，这种方法不仅仅利用一个通用规则有效地最小化错位，而且还可以平滑地适应各种看不见的规则，确保它们免受劫持并且模型做出适当的响应。</li>
</ul>

<h3>Title: KoDialogBench: Evaluating Conversational Understanding of Language  Models with Korean Dialogue Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Seongbo Jang, Seonghyeon Lee, Hwanjo Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17377">https://arxiv.org/abs/2402.17377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17377">https://arxiv.org/pdf/2402.17377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17377]] KoDialogBench: Evaluating Conversational Understanding of Language  Models with Korean Dialogue Benchmark(https://arxiv.org/abs/2402.17377)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improvement in models' conversation skills. Furthermore, our in-depth comparisons across different language models highlight the effectiveness of recent training techniques in enhancing conversational proficiency. We anticipate that KoDialogBench will promote the progress towards conversation-aware Korean language models.</li>
<li><strong>摘要：</strong>由于语言模型通常被部署为聊天机器人助手，因此模型以用户的第一语言进行对话成为一种美德。虽然这些模型接受了多种语言的训练，但缺乏对其对韩语等资源匮乏语言的熟练程度的综合评估。在这项工作中，我们引入了 KoDialogBench，这是一个旨在评估语言模型的韩语对话能力的基准。为此，我们从公共来源收集有关日常话题的韩语本土对话，或翻译其他语言的对话。然后，我们将这些对话构建成不同的测试数据集，涵盖从对话理解到响应选择任务。利用所提出的基准，我们对各种语言模型进行了广泛的评估和分析，以衡量对韩语对话的基本理解。实验结果表明模型的对话技巧还有很大的改进空间。此外，我们对不同语言模型的深入比较突显了最新训练技术在提高会话熟练程度方面的有效性。我们预计 KoDialogBench 将促进对话感知韩语模型的进展。</li>
</ul>

<h3>Title: Determinants of LLM-assisted Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Eva Eigner, Thorsten Händler</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17385">https://arxiv.org/abs/2402.17385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17385">https://arxiv.org/pdf/2402.17385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17385]] Determinants of LLM-assisted Decision-Making(https://arxiv.org/abs/2402.17385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Decision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision-specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.</li>
<li><strong>摘要：</strong>决策是日常生活中的一项基本能力。大型语言模型 (LLM) 为增强人类决策过程提供多方面的支持。然而，了解LLM辅助决策的影响因素对于使个人能够利用LLM提供的优势并最大程度地减少相关风险以做出更明智和更好的决策至关重要。本研究呈现了综合文献分析的结果，在法学硕士的支持下，对影响决策的决定因素进行了结构概述和详细分析。我们特别探讨了法学硕士技术方面的影响，包括透明度和及时工程、情绪和决策风格等心理因素，以及任务难度和责任等具体决策决定因素。此外，还通过多个应用场景说明了决定因素对决策过程的影响。根据我们的分析，我们开发了一个依赖框架，该框架根据这些决定因素之间的相互依赖关系将可能的相互作用系统化。我们的研究表明，由于与各种决定因素的多方面相互作用，对法学硕士的信任或依赖、用户的心理模型以及信息处理的特征等因素被认为是影响法学硕士辅助决策过程的重要方面。我们的研究结果对于提高人类与人工智能协作的决策质量、增强用户和组织的能力以及设计更有效的法学硕士界面至关重要。此外，我们的工作为未来对法学硕士协助的决策决定因素进行实证研究奠定了基础。</li>
</ul>

<h3>Title: FairBelief - Assessing Harmful Beliefs in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mattia Setzu, Marta Marchiori Manerba, Pasquale Minervini, Debora Nozza</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17389">https://arxiv.org/abs/2402.17389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17389">https://arxiv.org/pdf/2402.17389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17389]] FairBelief - Assessing Harmful Beliefs in Language Models(https://arxiv.org/abs/2402.17389)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.</li>
<li><strong>摘要：</strong>语言模型 (LM) 已被证明会继承不良偏见，如果在没有仔细的公平性审核的情况下将此类系统集成到现实世界的应用程序中，可能会伤害少数群体和代表性不足的群体。本文提出了 FairBelief，一种捕获和评估信念的分析方法，即 LM 可能以不同程度的置信度嵌入并秘密影响其预测的命题。借助 FairBelief，我们利用提示来研究几个最先进的 LM 在不同的先前被忽视的轴上的行为，例如模型规模和可能性，评估专门设计用于量化 LM 输出伤害性的公平数据集的预测。最后，我们对模型所发出的信念进行了深入的定性评估。我们将 FairBelief 应用于英语 LM，结果表明，尽管这些架构能够在不同的自然语言处理任务上实现高性能，但它们表现出对特定性别的有害信念。有趣的是，训练程序和数据集、模型规模和架构会引发不同程度的伤害信念。</li>
</ul>

<h3>Title: Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of  Prompting Strategies</h3>
<ul>
<li><strong>Authors: </strong>Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17396">https://arxiv.org/abs/2402.17396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17396">https://arxiv.org/pdf/2402.17396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17396]] Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of  Prompting Strategies(https://arxiv.org/abs/2402.17396)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了自然语言处理领域，因为它们能够在各种下游任务中重用从大量文本语料库中获得的知识，并且只需最少的（如果有的话）调整步骤。与此同时，研究一再表明，法学硕士缺乏系统的泛化能力，这使得可以在训练分布之外推断出学到的统计规律。在这项工作中，我们提供了 GPT-4（最先进的法学硕士之一）的系统基准测试，针对三个算法任务，其特征是可以使用两个参数控制问题难度。我们将 GPT-4 与其前身 (GPT-3.5) 以及最近推出的用于解决类似任务的 Transformer-Encoder 架构变体（神经数据路由器）的性能进行了比较。我们发现，先进提示技术的部署使 GPT-4 在所有任务上都能达到卓越的准确性，这表明最先进的 LLM 在需要系统泛化的挑战性任务中也构成了非常强大的基线。</li>
</ul>

<h3>Title: Investigating Continual Pretraining in Large Language Models: Insights  and Implications</h3>
<ul>
<li><strong>Authors: </strong>Çağatay Yıldız, Nishaanth Kanna Ravichandran, Prishruit Punia, Matthias Bethge, Beyza Ermis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17400">https://arxiv.org/abs/2402.17400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17400">https://arxiv.org/pdf/2402.17400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17400]] Investigating Continual Pretraining in Large Language Models: Insights  and Implications(https://arxiv.org/abs/2402.17400)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning. We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field.</li>
<li><strong>摘要：</strong>本文研究了大型语言模型 (LLM) 中持续学习 (CL) 的不断发展的领域，重点是制定高效和可持续培训的策略。我们的主要重点是持续的领域自适应预训练，这一过程旨在使法学硕士能够整合来自各个领域的新信息，同时保留以前学到的知识并增强跨领域知识转移，而无需依赖于特定领域的识别。与以前的研究主要集中在有限的任务或领域选择并主要旨在解决遗忘问题不同，我们的研究评估了法学硕士在实际场景中不断变化的数据环境的适应性和能力。为此，我们引入了一个新的基准，旨在衡量法学硕士对这些不断变化的数据环境的适应性，提供全面的评估框架。我们研究了模型大小对学习效率和遗忘的影响，以及新兴领域的进展和相似性如何影响这些模型内的知识转移。我们的研究结果揭示了几个关键见解：（i）当领域序列显示语义相似性时，与独立微调相比，持续预训练使法学硕士能够更好地专注于当前领域，（ii）跨不同领域的培训增强向后和向前的知识转移，以及（iii）较小的模型对持续预训练特别敏感，表现出最显着的遗忘率和学习率。我们认为，我们的研究标志着为研究法学硕士的 CL 建立更现实的基准的转变，并有可能在指导该领域未来研究方向方面发挥关键作用。</li>
</ul>

<h3>Title: Consistency Matters: Explore LLMs Consistency From a Black-Box  Perspective</h3>
<ul>
<li><strong>Authors: </strong>Fufangchen Zhao, Guoqiang Jin, Jiaheng Huang, Rui Zhao, Fei Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17411">https://arxiv.org/abs/2402.17411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17411">https://arxiv.org/pdf/2402.17411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17411]] Consistency Matters: Explore LLMs Consistency From a Black-Box  Perspective(https://arxiv.org/abs/2402.17411)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Nowadays both commercial and open-source academic LLM have become the mainstream models of NLP. However, there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive, and there is also an additional cost of secondary deployment, resulting in economic and time losses. To fill this gap, we build an LLM consistency task dataset and design several baselines. Additionally, we choose models of diverse scales for the main experiments. Specifically, in the LightGBM experiment, we used traditional NLG metrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training. The final result exceeds the manual evaluation and GPT3.5 as well as other models in the main experiment, achieving the best performance. In the end, we use the best performing LightGBM model as the base model to build the evaluation tool, which can effectively assist in the deployment of business models. Our code and tool demo are available at https://github.com/heavenhellchen/Consistency.git</li>
<li><strong>摘要：</strong>如今商业LLM和开源学术LLM都已成为NLP的主流模式。然而，目前还缺乏对LLM一致性的研究，这意味着在LLM研究和部署的各个阶段，其内部参数和能力应该保持不变。这个问题在工业界和学术界都存在。这一问题的解决往往耗时耗力，而且还存在二次部署的额外成本，造成经济和时间的损失。为了填补这一空白，我们构建了 LLM 一致性任务数据集并设计了多个基线。此外，我们选择不同尺度的模型进行主要实验。具体来说，在LightGBM实验中，我们使用传统的NLG指标（即ROUGE、BLEU、METEOR）作为模型训练所需的特征。最终结果超过了人工评测和GPT3.5以及主实验中的其他模型，达到了最好的性能。最后，我们使用性能最好的LightGBM模型作为基础模型来构建评估工具，可以有效辅助业务模型的部署。我们的代码和工具演示可在 https://github.com/heavenhellchen/Consistency.git 获取</li>
</ul>

<h3>Title: Enhancing EEG-to-Text Decoding through Transferable Representations from  Pre-trained Contrastive EEG-Text Masked Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, Zhiguo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17433">https://arxiv.org/abs/2402.17433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17433">https://arxiv.org/pdf/2402.17433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17433]] Enhancing EEG-to-Text Decoding through Transferable Representations from  Pre-trained Contrastive EEG-Text Masked Autoencoder(https://arxiv.org/abs/2402.17433)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. These results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread BCI applications.</li>
<li><strong>摘要：</strong>从非侵入性脑电图 (EEG) 重建自然语言作为脑机接口 (BCI) 的语言解码技术具有广阔的前景。然而，基于脑电图的语言解码仍处于起步阶段，面临一些技术问题，例如：1）缺乏能够有效地将跨模态（脑电图和文本之间）自学习与模内自学习结合起来的混合策略。重建脑电图特征或文本序列； 2）未充分利用大型语言模型（LLM）来增强基于脑电图的语言解码。为了解决上述问题，我们提出了对比脑电图文本屏蔽自动编码器（CET-MAE），这是一种新颖的模型，可以通过专用的多流编码器协调脑电图和文本之间和内部的复合自监督学习。此外，我们开发了一个名为 E2T-PTR（使用预训练可转移表示的脑电图到文本解码）的框架，该框架利用预训练模块以及来自 CET-MAE 的脑电图流，并进一步使 LLM（特别是 BART）能够解码来自脑电图序列。在流行的文本诱发脑电图数据库 ZuCo 上进行的综合实验证明了 E2T-PTR 的优越性，其在 RO​​UGE-1 F1 和 BLEU-4 分数上分别优于现有技术 8.34% 和 32.21% 。这些结果表明该领域取得了重大进展，并强调了所提出的框架实现更强大和更广泛的 BCI 应用的潜力。</li>
</ul>

<h3>Title: Deep Learning Based Named Entity Recognition Models for Recipes</h3>
<ul>
<li><strong>Authors: </strong>Mansi Goel, Ayush Agarwal, Shubham Agrawal, Janak Kapuriya, Akhil Vamshi Konam, Rishabh Gupta, Shrey Rastogi, Niharika, Ganesh Bagler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17447">https://arxiv.org/abs/2402.17447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17447">https://arxiv.org/pdf/2402.17447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17447]] Deep Learning Based Named Entity Recognition Models for Recipes(https://arxiv.org/abs/2402.17447)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset. A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights. We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.</li>
<li><strong>摘要：</strong>食物通过各种努力影响着我们的生活，包括风味、营养、健康和可持续性。食谱是通过非结构化文本代代相传的文化胶囊。用于识别命名实体（菜谱文本的构建块）的自动化协议对于从信息提取到新菜谱生成等各种应用具有巨大的价值。命名实体识别是一种从具有已知标签的非结构化或半结构化数据中提取信息的技术。从 6,611 个成分短语的手动注释数据开始，我们累计创建了包含 26,445 个短语的增强数据集。同时，我们系统地清理和分析了黄金标准配方数据库 RecipeDB 中的成分短语，并使用斯坦福 NER 对其进行了注释。根据分析，我们使用基于聚类的方法对 88,526 个短语的子集进行了采样，同时保留了多样性以创建机器注释的数据集。对这三个数据集的 NER 方法进行了彻底的研究，涉及统计、基于深度学习的语言模型的微调以及大型语言模型 (LLM) 上的小样本提示，提供了深刻的见解。我们得出的结论是，LLM 上的少样本提示具有糟糕的性能，而经过微调的 spaCy-transformer 成为最佳模型，对于手动注释、增强和增强的模型，宏 F1 分数分别为 95.9%、96.04% 和 95.71%。分别是机器注释的数据集。</li>
</ul>

<h3>Title: DS-Agent: Automated Data Science by Empowering Large Language Models  with Case-Based Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17453">https://arxiv.org/abs/2402.17453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17453">https://arxiv.org/pdf/2402.17453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17453]] DS-Agent: Automated Data Science by Empowering Large Language Models  with Case-Based Reasoning(https://arxiv.org/abs/2402.17453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per run with GPT-4, respectively.</li>
<li><strong>摘要：</strong>在这项工作中，我们研究了基于大型语言模型（LLM）的代理自动化数据科学任务的潜力，目标是理解任务要求，然后构建和训练最适合的机器学习模型。尽管取得了广泛的成功，现有的法学硕士代理人却因在这种情况下制定不合理的实验计划而受到阻碍。为此，我们提出了 DS-Agent，这是一种利用 LLM 代理和基于案例的推理 (CBR) 的新型自动框架。在开发阶段，DS-Agent遵循CBR框架构建自动迭代管道，可以灵活利用Kaggle的专家知识，并通过反馈机制促进持续的性能改进。此外，DS-Agent通过简化的CBR范例实现了低资源部署阶段，以将过去成功的解决方案从开发阶段改编为直接代码生成，从而显着降低了对LLM基础能力的需求。根据经验，采用 GPT-4 的 DS-Agent 在开发阶段实现了前所未有的 100% 成功率，同时在部署阶段替代法学硕士的平均一次通过率提高了 36%。在这两个阶段中，DS-Agent 均取得了最佳性能排名，GPT-4 每次运行的成本分别为 1.60 美元和 0.13 美元。</li>
</ul>

<h3>Title: Why do Learning Rates Transfer? Reconciling Optimization and Scaling  Limits for Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, Antonio Orvieto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17457">https://arxiv.org/abs/2402.17457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17457">https://arxiv.org/pdf/2402.17457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17457]] Why do Learning Rates Transfer? Reconciling Optimization and Scaling  Limits for Deep Learning(https://arxiv.org/abs/2402.17457)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at different scales, thus preventing learning rate transfer. But what causes these differences in the sharpness dynamics? Through a connection between the spectra of the Hessian and the NTK matrix, we argue that the cause lies in the presence (for $\mu$P) or progressive absence (for the NTK regime) of feature learning, which results in a different evolution of the NTK, and thus of the sharpness. We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText</li>
<li><strong>摘要：</strong>最近，越来越多的证据表明，如果神经网络的宽度和深度向着所谓的丰富特征学习极限（$\mu$P 及其深度扩展）缩放，那么一些超参数 - 例如学习率 -展示从小模型到非常大模型的迁移，从而降低超参数调整的成本。从优化的角度来看，这种现象令人费解，因为它意味着损失情况在不同的模型大小上是非常一致的。在这项工作中，我们发现经验证据表明，学习率迁移可归因于以下事实：在 $\mu$P 及其深度扩展下，训练损失 Hessian 的最大特征值（即锐度）在很大程度上与宽度和宽度无关。持续一段训练时间的网络深度。另一方面，我们表明，在神经正切核（NTK）机制下，清晰度在不同尺度上表现出非常不同的动态，从而阻止了学习率转移。但是什么导致了锐度动态的这些差异呢？通过 Hessian 矩阵和 NTK 矩阵的谱之间的联系，我们认为原因在于特征学习的存在（对于 $\mu$P）或逐渐缺失（对于 NTK 体系），这导致了不同的演化NTK 的影响，以及清晰度的影响。我们通过大量实验证实了我们的主张，这些实验涵盖了广泛的数据集和架构：从在基准视觉数据集上训练的 ResNet 和 Vision Transformers 到在 WikiText 上训练的基于 Transformers 的语言模型</li>
</ul>

<h3>Title: Training-Free Long-Context Scaling of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17463">https://arxiv.org/abs/2402.17463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17463">https://arxiv.org/pdf/2402.17463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17463]] Training-Free Long-Context Scaling of Large Language Models(https://arxiv.org/abs/2402.17463)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \url{https://github.com/HKUNLP/ChunkLlama}.</li>
<li><strong>摘要：</strong>当输入标记的数量超过其预训练长度时，大型语言模型（LLM）处理和生成连贯文本的能力会明显减弱。考虑到微调具有较长序列的大型模型的昂贵开销，我们提出了双块注意力（DCA），它使 Llama2 70B 能够支持超过 100k 个标记的上下文窗口，而无需持续训练。通过将长序列的注意力计算分解为基于块的模块，DCA 能够有效地捕获同一块内（块内）和不同块（块间）之间标记的相对位置信息，并与闪光注意。除了令人印象深刻的外推能力之外，DCA 在实际长上下文任务上的性能可与微调模型相媲美甚至更好。与专有模型相比，我们的免训练 70B 模型的性能达到了 gpt-3.5-16k 的 94%，这表明它是一个可行的开源替代方案。这项工作中使用的所有代码和数据均在 \url{https://github.com/HKUNLP/ChunkLlama} 发布。</li>
</ul>

<h3>Title: Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda  Spans in News Articles</h3>
<ul>
<li><strong>Authors: </strong>Maram Hasanain, Fatema Ahmed, Firoj Alam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17478">https://arxiv.org/abs/2402.17478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17478">https://arxiv.org/pdf/2402.17478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17478]] Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda  Spans in News Articles(https://arxiv.org/abs/2402.17478)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The use of propaganda has spiked on mainstream and social media, aiming to manipulate or mislead users. While efforts to automatically detect propaganda techniques in textual, visual, or multimodal content have increased, most of them primarily focus on English content. The majority of the recent initiatives targeting medium to low-resource languages produced relatively small annotated datasets, with a skewed distribution, posing challenges for the development of sophisticated propaganda detection models. To address this challenge, we carefully develop the largest propaganda dataset to date, ArPro, comprised of 8K paragraphs from newspaper articles, labeled at the text span level following a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the first attempt to understand the performance of large language models (LLMs), using GPT-4, for fine-grained propaganda detection from text. Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a dataset consisting of six other languages for span detection, and results suggest that the model struggles with the task across languages. Our dataset and resources will be released to the community.</li>
<li><strong>摘要：</strong>主流媒体和社交媒体上的宣传活动激增，旨在操纵或误导用户。虽然自动检测文本、视觉或多模式内容中的宣传技术的努力有所增加，但其中大多数主要集中在英语内容上。最近针对中低资源语言的大多数举措都产生了相对较小的注释数据集，且分布不均匀，这给复杂的宣传检测模型的开发带来了挑战。为了应对这一挑战，我们精心开发了迄今为止最大的宣传数据集 ArPro，它由报纸文章中的 8K 段落组成，按照 23 种宣传技术的分类法在文本跨度级别进行标记。此外，我们的工作首次尝试使用 GPT-4 来了解大型语言模型 (LLM) 的性能，以从文本中进行细粒度的宣传检测。结果表明，随着任务从简单地将段落分类为宣传性或非宣传性，转变为检测宣传技术及其在文本中的表现形式的细粒度任务，GPT-4 的性能会下降。与在数据集上微调的不同分类粒度的宣传检测模型相比，GPT-4 仍然远远落后。最后，我们在由其他六种语言组成的数据集上评估 GPT-4 以进行跨度检测，结果表明该模型难以完成跨语言的任务。我们的数据集和资源将发布给社区。</li>
</ul>

<h3>Title: Prescribing Large Language Models for Perioperative Care: What's The  Right Dose for Pre-trained Models?</h3>
<ul>
<li><strong>Authors: </strong>Bing Xue, Charles Alba, Joanna Abraham, Thomas Kannampallil, Chenyang Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17493">https://arxiv.org/abs/2402.17493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17493">https://arxiv.org/pdf/2402.17493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17493]] Prescribing Large Language Models for Perioperative Care: What's The  Right Dose for Pre-trained Models?(https://arxiv.org/abs/2402.17493)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC. Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care.</li>
<li><strong>摘要：</strong>术后风险预测可以为有效的围手术期护理管理和规划提供信息。我们的目的是评估临床大语言模型（LLM）是否可以使用具有各种训练策略的临床文本来预测术后风险。主要队列涉及 2018 年至 2021 年间巴恩斯犹太医院 (BJH) 系统的 84,875 条记录。方法在 Beth Israel Deaconess 的 MIMIC 数据集上进行了复制。两项研究的平均随访时间均基于术后 ICU 住院时间少于 7 天。对于 BJH 数据集，结果包括 30 天死亡率、肺栓塞 (PE) 和肺炎。 BioGPT、ClinicalBERT 和 BioClinicalBERT 实施了三种领域适应和微调策略：自我监督目标；将标签与半监督微调结合起来；以及通过多任务学习进行基础建模。使用分类任务的受试者工作特征曲线下面积 (AUROC) 和精确召回曲线下面积 (AUPRC) 以及回归任务的均方误差 (MSE) 和 R2 来比较模型性能。预训练的 LLM 的性能优于传统的词嵌入，AUROC 的绝对最大增益为 38.3%，AUPRC 的绝对最大增益为 14%。调整模型进一步提高性能：（1）自监督微调AUROC 3.2%，AUPRC 1.5%； (2) 与自监督微调相比，半监督微调 AUROC 提高了 1.8%，AUPRC 提高了 2%； (3) 与自监督微调相比，AUROC 的基础建模提高了 3.6%，AUPRC 的基础建模提高了 2.6%。预先训练的临床法学硕士为不可预见的数据中的术后风险预测提供了机会，基础模型的峰值表明与任务无关的学习对于法学硕士在围手术期护理中的普遍性具有潜力。</li>
</ul>

<h3>Title: REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain  Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17497">https://arxiv.org/abs/2402.17497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17497">https://arxiv.org/pdf/2402.17497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17497]] REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain  Question Answering(https://arxiv.org/abs/2402.17497)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our code and data can be accessed at https://github.com/RUCAIBox/REAR.</li>
<li><strong>摘要：</strong>考虑到内部参数知识有限，检索增强生成（RAG）已被广泛用于扩展大型语言模型（LLM）的知识范围。尽管在 RAG 研究方面做出了广泛的努力，但在现有方法中，法学硕士无法精确评估检索到的文档的相关性，因此可能导致外部知识（即检索到的文档）的误导甚至不正确的利用。为了解决这个问题，在本文中，我们提出了 REAR，一种用于开放域问答（QA）的 RElevance-Aware 检索增强方法。作为关键动机，我们的目标是增强法学硕士对来源相关性的自我意识，以便在 RAG 系统中适应性地利用外部知识。特别是，我们为基于 LLM 的 RAG 系统开发了一种新架构，通过结合专门设计的排名头来精确评估检索到的文档的相关性。此外，我们提出了一种基于双粒度相关性融合和抗噪声训练的改进训练方法。通过结合架构和训练方面的改进，我们提出的 REAR 可以通过有效感知检索到的文档的相关性来更好地利用外部知识。对四个开放域 QA 任务的实验表明，REAR 显着优于之前的许多竞争性 RAG 方法。我们的代码和数据可以在 https://github.com/RUCAIBox/REAR 访问。</li>
</ul>

<h3>Title: Intensive Care as One Big Sequence Modeling Problem</h3>
<ul>
<li><strong>Authors: </strong>Vadim Liventsev, Tobias Fritz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17501">https://arxiv.org/abs/2402.17501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17501">https://arxiv.org/pdf/2402.17501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17501]] Intensive Care as One Big Sequence Modeling Problem(https://arxiv.org/abs/2402.17501)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.</li>
<li><strong>摘要：</strong>医疗保健中的强化学习通常涉及狭隘的独立任务，例如败血症预测或麻醉控制。然而，之前的研究已经证明，通才模型（主要的例子是大型语言模型）由于其隐式迁移学习的能力而有超越特定任务方法的潜力。为了能够训练医疗保健基础模型并利用最先进的 Transformer 架构的功能，我们提出了医疗保健作为序列建模的范式，其中患者和医疗保健提供者之间的交互表示为事件流和任务像诊断和治疗选择一样被建模为流中未来事件的预测。为了通过实验探索这一范式，我们开发了 MIMIC-SEQ，这是一种序列建模基准，通过将 MIMIC-IV 数据集中的异质临床记录转换为统一的事件流格式，训练基线模型并探索其功能。</li>
</ul>

<h3>Title: Predict the Next Word: <Humans exhibit uncertainty in this task and  language models _____></h3>
<ul>
<li><strong>Authors: </strong>Evgenia Ilia, Wilker Aziz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17527">https://arxiv.org/abs/2402.17527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17527">https://arxiv.org/pdf/2402.17527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17527]] Predict the Next Word: <Humans exhibit uncertainty in this task and  language models _____>(https://arxiv.org/abs/2402.17527)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty. We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting.</li>
<li><strong>摘要：</strong>语言模型 (LM) 是经过训练的统计模型，用于为人类生成的文本分配概率。因此，有理由质疑它们是否很好地近似了人类所表现出的语言变异性。这种形式的统计评估很难在通过水平上执行，因为它需要可接受性判断（即人类评估）或强大的自动化代理（这很重要）。然而，在单词级别，给定一些上下文，可以通过与可用上下文的替代单单词延续的预先记录的数据集进行精确匹配来评估来自 LM 的样本。我们利用这一事实并评估 LM 重现人类（特别是说英语的人群）在“下一个单词预测”任务中表现出的变异性的能力。这可以被视为评估一种校准形式，在文本分类的背景下，Baan 等人。 (2022) 称为对人类不确定性的校准。我们评估了 GPT2、BLOOM 和 ChatGPT，发现它们对人类不确定性的校准相当低。我们还验证了预期校准误差 (ECE) 是否未能反映这一点，因此建议社区不要在这种情况下依赖它。</li>
</ul>

<h3>Title: Retrieval is Accurate Generation</h3>
<ul>
<li><strong>Authors: </strong>Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou, Shuming Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17532">https://arxiv.org/abs/2402.17532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17532">https://arxiv.org/pdf/2402.17532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17532]] Retrieval is Accurate Generation(https://arxiv.org/abs/2402.17532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.</li>
<li><strong>摘要：</strong>标准语言模型通过从固定、有限和独立的词汇表中选择标记来生成文本。我们引入了一种新颖的方法，可以从支持文档集合中选择上下文感知短语。这种范式转变的最重大挑战之一是确定训练预言，因为可以以各种方式对文本字符串进行分段，并且可以从许多可能的文档中检索每个分段。为了解决这个问题，我们建议使用语言启发法来初始化训练预言机，更重要的是，通过迭代自我强化来引导预言机。大量的实验表明，我们的模型不仅在各种知识密集型任务上优于标准语言模型，而且还证明了开放式文本生成中生成质量的提高。例如，与标准语言模型相比，我们的模型在 OpenbookQA 上的准确率从 23.47% 提高到 36.27%，并将开放式文本生成的 MAUVE 分数从 42.61% 提高到 81.58%。值得注意的是，我们的模型在多个检索增强基线中还实现了最佳性能和最低延迟。总之，我们断言检索是更准确的生成，并希望我们的工作将鼓励对这种新范式转变的进一步研究。</li>
</ul>

<h3>Title: COCOA: CBT-based Conversational Counseling Agent using Memory  Specialized in Cognitive Distortions and Dynamic Prompt</h3>
<ul>
<li><strong>Authors: </strong>Suyeon Lee, Jieun Kang, Harim Kim, Kyoung-Mee Chung, Dongha Lee, Jinyoung Yeo</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17546">https://arxiv.org/abs/2402.17546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17546">https://arxiv.org/pdf/2402.17546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17546]] COCOA: CBT-based Conversational Counseling Agent using Memory  Specialized in Cognitive Distortions and Dynamic Prompt(https://arxiv.org/abs/2402.17546)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, agent</a></li>
<li><strong>Abstract: </strong>The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from other models.</li>
<li><strong>摘要：</strong>对提供心理健康护理的对话代理的需求不断增加。在这项工作中，我们开发了一种心理咨询代理，称为 CoCoA，它应用认知行为疗法 (CBT) 技术来识别和解决客户陈述中固有的认知扭曲。具体来说，我们构建了一个记忆系统，以有效管理咨询所需的信息，同时从客户的话语中提取有关客户的高级见解。此外，为了确保咨询代理产生适当的反应，我们引入动态提示来灵活应用CBT技术并促进适当的信息检索。我们在 CoCoA 和 Character.ai 的角色之间进行对话，创建用于评估的数据集。然后，我们要求 GPT 评估构建的咨询数据集，我们的模型与其他模型表现出统计上的显着差异。</li>
</ul>

<h3>Title: OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist  Autonomous Agents for Desktop and Web</h3>
<ul>
<li><strong>Authors: </strong>Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17553">https://arxiv.org/abs/2402.17553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17553">https://arxiv.org/pdf/2402.17553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17553]] OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist  Autonomous Agents for Desktop and Web(https://arxiv.org/abs/2402.17553)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such as "Send an email to John Doe mentioning the time and place to meet". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.</li>
<li><strong>摘要：</strong>几十年来，人机交互基本上都是手动的。即使在今天，几乎所有在计算机上完成的生产性工作都需要人工在每一步进行输入。自主虚拟代理代表了许多这些琐碎任务自动化的令人兴奋的一步。虚拟代理将使技术能力有限的用户能够充分利用计算机系统的可能性。它们还可以有效地简化大量计算机任务，从日历管理到复杂的旅行预订，而无需人工干预。在本文中，我们介绍了 OmniACT，这是第一个用于评估代理生成可执行程序以完成计算机任务的能力的数据集和基准。我们的范围超出了传统的网络自动化，涵盖了各种桌面应用程序。该数据集包含“播放下一首歌曲”等基本任务，以及“向 John Doe 发送电子邮件，提及见面的时间和地点”等长期任务。具体来说，给定一对屏幕图像和一个基于视觉的自然语言任务，目标是生成能够完全执行该任务的脚本。我们在基准测试中运行了几个强大的基线语言模型代理。最强的基线 GPT-4 在我们的基准测试中表现最好，但是，在生成能够完成任务的可执行脚本方面，其性能水平仍然仅达到人类熟练程度的 15%，这表明我们的任务对传统 Web 代理的挑战。我们的基准测试提供了一个平台来衡量和评估语言模型代理在自动化计算机任务方面的进展，并激励未来的工作构建连接大型语言模型和计算机屏幕视觉基础的多模态模型。</li>
</ul>

<h3>Title: Unleashing the Potential of Large Language Models as Prompt Optimizers:  An Analogical Analysis with Gradient-based Model Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17564">https://arxiv.org/abs/2402.17564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17564">https://arxiv.org/pdf/2402.17564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17564]] Unleashing the Potential of Large Language Models as Prompt Optimizers:  An Analogical Analysis with Gradient-based Model Optimizers(https://arxiv.org/abs/2402.17564)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the optimization trajectory as the update direction. Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 55.3% on MMLU compared to baseline methods.</li>
<li><strong>摘要：</strong>自动提示优化是提高大型语言模型（LLM）性能的重要方法。最近的研究证明了使用法学硕士作为提示优化器的潜力，它可以通过迭代细化生成改进的任务提示。在本文中，我们通过与基于梯度的模型优化器进行类比，提出了一种新颖的视角来研究基于 LLM 的提示优化器的设计。为了连接这两种方法，我们确定了模型参数学习中的两个关键因素：更新方向和更新方法。围绕这两方面，我们借鉴基于梯​​度的优化的理论框架和学习方法，为基于LLM的提示优化器设计改进策略。通过系统地分析一组丰富的改进策略，我们进一步开发了一个强大的基于梯度的 LLM 提示优化器，称为 GPO。在每一步中，它首先从优化轨迹中检索相关提示作为更新方向。然后，它利用基于生成的细化策略来执行更新，同时通过基于余弦的衰减策略控制编辑距离。大量实验证明了 GPO 的有效性和效率。特别是，与基线方法相比，GPO 在 Big-Bench Hard 上带来了高达 56.8% 的额外改进，在 MMLU 上带来了 55.3% 的额外改进。</li>
</ul>

<h3>Title: Agent-Pro: Learning to Evolve via Policy-Level Reflection and  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17574">https://arxiv.org/abs/2402.17574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17574">https://arxiv.org/pdf/2402.17574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17574]] Agent-Pro: Learning to Evolve via Policy-Level Reflection and  Optimization(https://arxiv.org/abs/2402.17574)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.</li>
<li><strong>摘要：</strong>大型语言模型表现出针对不同任务的强大的问题解决能力。然而，大多数基于 LLM 的代理被设计为具有复杂提示工程的特定任务解决器，而不是能够通过交互学习和进化的代理。这些任务解决器需要手动制作提示来告知任务规则并规范 LLM 行为，本质上无法解决复杂的动态场景，例如大型互动游戏。有鉴于此，我们提出了 Agent-Pro：一种基于 LLM 的 Agent，具有策略级反射和优化功能，可以从交互体验中学习丰富的专业知识，并逐步提升其行为策略。具体来说，它涉及政策演变的动态信念生成和反思过程。 Agent-Pro 不是进行行动层面的反思，而是迭代地反思过去的轨迹和信念，微调其非理性信念以获得更好的政策。此外，采用深度优先搜索进行策略优化，确保策略收益不断增强。 Agent-Pro 在两种游戏中进行了评估：二十一点和德州扑克，其表现优于普通 LLM 和专业模型。我们的结果表明 Agent-Pro 可以在复杂和动态的场景中学习和发展，这也有利于众多基于 LLM 的应用程序。</li>
</ul>

<h3>Title: Variational Learning is Effective for Large Deep Networks</h3>
<ul>
<li><strong>Authors: </strong>Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, Thomas Möllenhoff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17641">https://arxiv.org/abs/2402.17641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17641">https://arxiv.org/pdf/2402.17641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17641]] Variational Learning is Effective for Large Deep Networks(https://arxiv.org/abs/2402.17641)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.</li>
<li><strong>摘要：</strong>我们提供了大量的经验证据，反对变分学习对于大型神经网络无效的普遍看法。我们证明，在从头开始训练 GPT-2 和 ResNet 等大型网络时，一种名为改进变分在线牛顿 (IVON) 的优化器始终匹配或优于 Adam。 IVON 的计算成本几乎与 Adam 相同，但其预测不确定性更好。我们展示了 IVON 的几个新用例，其中我们改进了大型语言模型中的微调和模型合并，准确预测泛化误差，并如实地估计对数据的敏感性。我们发现压倒性的证据支持变分学习的有效性。</li>
</ul>

<h3>Title: Are LLMs Capable of Data-based Statistical and Causal Reasoning?  Benchmarking Advanced Quantitative Reasoning with Data</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17644">https://arxiv.org/abs/2402.17644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17644">https://arxiv.org/pdf/2402.17644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17644]] Are LLMs Capable of Data-based Statistical and Causal Reasoning?  Benchmarking Advanced Quantitative Reasoning with Data(https://arxiv.org/abs/2402.17644)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.</li>
<li><strong>摘要：</strong>定量推理是分析数据的一项关键技能，但对这种能力的评估仍然有限。为了弥补这一差距，我们引入了数据定量推理（QRData）基准，旨在评估大型语言模型在使用现实世界数据进行统计和因果推理方面的能力。该基准包括精心构建的包含 411 个问题的数据集，以及来自教科书、在线学习材料和学术论文的数据表。为了比较模型对数据和文本的定量推理能力，我们用 290 个纯文本问题的辅助集（即 QRText）丰富了基准。我们在不同的模型上评估自然语言推理、基于程序的推理和代理推理方法，包括思想链、思想程序、ReAct 和代码解释器助手。最强模型GPT-4达到了58%的准确率，还有很大的提升空间。在开源模型中，Deepseek-coder-instruct（一种在 2T 代币上预训练的代码 LLM）获得了 37% 的最高准确率。分析表明，模型在数据分析和因果推理方面遇到困难，并且难以同时使用因果知识和提供的数据。代码和数据位于https://github.com/xxxiaol/QRData。</li>
</ul>

<h3>Title: Beyond prompt brittleness: Evaluating the reliability and consistency of  political worldviews in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tanise Ceron, Neele Falk, Ana Barić, Dmitry Nikolaev, Sebastian Padó</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17649">https://arxiv.org/abs/2402.17649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17649">https://arxiv.org/pdf/2402.17649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17649]] Beyond prompt brittleness: Evaluating the reliability and consistency of  political worldviews in LLMs(https://arxiv.org/abs/2402.17649)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy.</li>
<li><strong>摘要：</strong>由于大型语言模型（LLM）在无处不在的系统中广泛使用，我们需要了解它们是否嵌入了特定的世界观以及这些观点反映了什么。最近的研究报告称，在政治调查问卷的提示下，法学硕士表现出左翼自由主义倾向。然而，目前尚不清楚这些倾向是否可靠（对迅速变化具有鲁棒性）以及这些倾向在政策和政治倾向之间是否一致。我们提出了一系列测试，根据从七个欧盟国家收集并针对政策领域进行注释的投票建议调查问卷数据集，评估法学硕士对政治声明立场的可靠性和一致性。我们研究了参数大小从 7B 到 70B 不等的法学硕士，发现它们的可靠性随着参数数量的增加而增加。较大的模型总体上显示出与左倾政党的一致性更强，但政策方案有所不同：它们对环境保护、社会福利以及（右翼）法律和秩序表现出（左翼）积极立场，但在外国方面没有一致的偏好政策、移民和经济。</li>
</ul>

<h3>Title: Multi-Agent Deep Reinforcement Learning for Distributed Satellite  Routing</h3>
<ul>
<li><strong>Authors: </strong>Federico Lozano-Cuadra, Beatriz Soret</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17666">https://arxiv.org/abs/2402.17666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17666">https://arxiv.org/pdf/2402.17666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17666]] Multi-Agent Deep Reinforcement Learning for Distributed Satellite  Routing(https://arxiv.org/abs/2402.17666)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.</li>
<li><strong>摘要：</strong>本文介绍了一种用于近地轨道卫星星座 (LSatC) 路由的多智能体深度强化学习 (MA-DRL) 方法。每颗卫星都是一个独立的决策主体，对环境有部分了解，并得到附近主体反馈的支持。在我们之前介绍 Q 路由解决方案的工作的基础上，本文的贡献是将其扩展到能够快速适应网络和流量变化的深度学习框架，并基于两个阶段：（1）离线探索学习阶段，依靠全局深度神经网络（DNN）来学习每个可能位置和拥塞程度的最佳路径； (2) 使用本地、机载、预训练的 DNN 进行在线开发阶段。结果表明，MA-DRL 可以有效地离线学习最佳路由，然后将其加载到在线以实现高效的分布式路由。</li>
</ul>

<h3>Title: Securing Reliability: A Brief Overview on Enhancing In-Context Learning  for Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, Xiaoxing Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17671">https://arxiv.org/abs/2402.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17671">https://arxiv.org/pdf/2402.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17671]] Securing Reliability: A Brief Overview on Enhancing In-Context Learning  for Foundation Models(https://arxiv.org/abs/2402.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.</li>
<li><strong>摘要：</strong>随着基础模型（FM）继续塑造人工智能的格局，情境学习（ICL）范式蓬勃发展，但也遇到了毒性、幻觉、差异、对抗性脆弱性和不一致等问题。确保FM的可靠性和责任感对于AI生态系统的可持续发展至关重要。在这篇简明概述中，我们研究了 ICL 框架内增强 FM 可靠性和可信度方面的最新进展，重点关注四种关键方法，每种方法都有相应的子目标。我们真诚地希望本文能够为致力于构建安全可靠的 FM 并培育稳定一致的 ICL 环境的研究人员和从业者提供宝贵的见解，从而释放其巨大的潜力。</li>
</ul>

<h3>Title: NextLevelBERT: Investigating Masked Language Modeling with Higher-Level  Representations for Long Documents</h3>
<ul>
<li><strong>Authors: </strong>Tamara Czinczoll, Christoph Hönes, Maximilian Schall, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17682">https://arxiv.org/abs/2402.17682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17682">https://arxiv.org/pdf/2402.17682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17682]] NextLevelBERT: Investigating Masked Language Modeling with Higher-Level  Representations for Long Documents(https://arxiv.org/abs/2402.17682)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code available.</li>
<li><strong>摘要：</strong>虽然（大型）语言模型在过去几年中取得了显着改进，但由于底层注意力机制的二次缩放，它们仍然难以明智地处理例如在书籍中发现的长序列。为了解决这个问题，我们提出了 NextLevelBERT，一种掩码语言模型，它不运行在标记上，而是运行在文本嵌入形式的更高级别的语义表示上。我们预训练 NextLevelBERT 来预测整个屏蔽文本块的向量表示，并评估生成的文档向量在三种任务类型上的有效性：1）通过零样本文档嵌入实现语义文本相似性，2）长文档分类，3）多项选择问题回答。我们发现，下一级掩码语言建模是处理长文档用例的有效技术，并且只要所需的细节级别不太高，就可以胜过更大的嵌入模型。我们提供模型和代码。</li>
</ul>

<h3>Title: RAVEL: Evaluating Interpretability Methods on Disentangling Language  Model Representations</h3>
<ul>
<li><strong>Authors: </strong>Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17700">https://arxiv.org/abs/2402.17700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17700">https://arxiv.org/pdf/2402.17700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17700]] RAVEL: Evaluating Interpretability Methods on Disentangling Language  Model Representations(https://arxiv.org/abs/2402.17700)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.</li>
<li><strong>摘要：</strong>单个神经元参与多个高级概念的表示。不同的解释方法可以在多大程度上成功地解开这些角色？为了帮助解决这个问题，我们引入了 RAVEL（解决语言模型中的属性值纠缠），这是一个数据集，可以在各种现有的可解释性方法之间进行严格控制的定量比较。我们使用由此产生的概念框架来定义多任务分布式对齐搜索（MDAS）的新方法，它使我们能够找到满足多个因果标准的分布式表示。以 Llama2-7B 作为目标语言模型，MDAS 在 RAVEL 上取得了最先进的结果，证明了超越神经元级分析来识别跨激活分布的特征的重要性。我们在 https://github.com/explanare/ravel 发布了我们的基准测试。</li>
</ul>

<h3>Title: Case-Based or Rule-Based: How Do Transformers Do the Math?</h3>
<ul>
<li><strong>Authors: </strong>Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17709">https://arxiv.org/abs/2402.17709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17709">https://arxiv.org/pdf/2402.17709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17709]] Case-Based or Rule-Based: How Do Transformers Do the Math?(https://arxiv.org/abs/2402.17709)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to explicitly use rules helps them learn rule-based reasoning and generalize better in length.</li>
<li><strong>摘要：</strong>尽管在各种复杂任务中表现出色，但现代大型语言模型 (LLM) 在处理一些对人类来说简单直观的数学问题（例如加法）时仍然遇到困难。虽然我们可以轻松学习加法的基本规则并将其应用于任何长度的新问题，但法学硕士却很难做到这一点。相反，他们可能会依靠训练语料库中看到的类似“案例”来寻求帮助。我们将这两种不同的推理机制定义为“基于规则的推理”和“基于案例的推理”。由于基于规则的推理对于获得系统泛化能力至关重要，因此我们的目标是准确探索 Transformer 是否使用基于规则的推理或基于案例的推理来解决数学问题。通过对五个数学任务精心设计的干预实验，我们确认 Transformer 正在执行基于案例的推理，无论是否使用暂存器，这与之前的观察一致，即 Transformer 使用子图匹配/快捷学习进行推理。为了缓解此类问题，我们提出了一种规则遵循微调（RFFT）技术来教导 Transformer 执行基于规则的推理。具体来说，我们在输入中提供明确的规则，然后指导 Transformer 逐步背诵并遵循规则。通过 RFFT，我们成功地使法学硕士能够在 1-5 位加法上进行微调，以推广到 12 位加法，准确率超过 95%，比暂存器高出 40% 以上。这一显着改进表明，教导法学硕士明确使用规则有助于他们学习基于规则的推理并在长度上更好地概括。</li>
</ul>

<h3>Title: AmbigNLG: Addressing Task Ambiguity in Instruction for NLG</h3>
<ul>
<li><strong>Authors: </strong>Ayana Niwa, Hayate Iso</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17717">https://arxiv.org/abs/2402.17717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17717">https://arxiv.org/pdf/2402.17717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17717]] AmbigNLG: Addressing Task Ambiguity in Instruction for NLG(https://arxiv.org/abs/2402.17717)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks. Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions. To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better. We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities. Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks.</li>
<li><strong>摘要：</strong>在这项研究中，我们引入了 AmbigNLG，这是一项新任务，旨在解决自然语言生成（NLG）任务指令中任务模糊性的挑战。尽管大型语言模型（LLM）在通过自然语言交互理解和执行各种任务方面具有令人印象深刻的能力，但它们的性能受到现实世界指令中存在的歧义性的严重阻碍。为了解决这个问题，AmbigNLG 试图识别并减少此类歧义，旨在完善指令以更好地满足用户的期望。我们引入了一个由 2,500 个实例组成的数据集 AmbigSNI-NLG，并开发了一个用于对指令歧义进行分类和注释的歧义分类法。我们的方法展示了文本生成质量的显着改进，强调了清晰而具体的指令在提高 NLG 任务中的 LLM 性能方面的关键作用。</li>
</ul>

<h3>Title: Tower: An Open Multilingual Large Language Model for Translation-Related  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Duarte M. Alves, José Pombal, Nuno M. Guerreiro, Pedro H. Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G.C. de Souza, André F.T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17733">https://arxiv.org/abs/2402.17733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17733">https://arxiv.org/pdf/2402.17733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17733]] Tower: An Open Multilingual Large Language Model for Translation-Related  Tasks(https://arxiv.org/abs/2402.17733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.</li>
<li><strong>摘要：</strong>虽然通用大型语言模型 (LLM) 表现出对翻译领域内多项任务的熟练程度，但基于开放式 LLM 的方法只有在专注于单个任务时才具有竞争力。在本文中，我们提出了一种针对翻译工作流程中存在的多个任务定制法学硕士的方法。我们对单语和并行数据的多语言混合进行持续的预训练，创建 TowerBase，然后对与翻译过程相关的指令进行微调，创建 TowerInstruct。我们的最终模型在与翻译工作流程相关的多项任务上超越了开放替代方案，并且与通用封闭式法学硕士具有竞争力。为了促进未来的研究，我们发布了 Tower 模型、我们的专业化数据集、专注于翻译生态系统的法学硕士评估框架，以及一系列模型生成（包括我们的模型生成）。</li>
</ul>

<h3>Title: Evaluating Very Long-Term Conversational Memory of LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17753">https://arxiv.org/abs/2402.17753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17753">https://arxiv.org/pdf/2402.17753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17753]] Evaluating Very Long-Term Conversational Memory of LLM Agents(https://arxiv.org/abs/2402.17753)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, retrieval augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.</li>
<li><strong>摘要：</strong>现有的长期开放域对话工作侧重于评估不超过五个聊天会话的上下文中的模型响应。尽管长上下文大语言模型（LLM）和检索增强生成（RAG）技术取得了进步，但它们在超长期对话中的功效仍未得到探索。为了解决这一研究差距，我们引入了机器与人类的管道，通过利用基于 LLM 的代理架构并将其对话建立在人物角色和时间事件图的基础上，来生成高质量、长期的对话。此外，我们为每个代理配备了共享图像和对图像做出反应的能力。生成的对话由人工注释者验证和编辑，以实现远程一致性和事件图的基础。使用此管道，我们收集 LoCoMo，这是一个非常长期对话的数据集，每个对话包含 300 个回合和平均 9K 代币，最多 35 个会话。基于LoCoMo，我们提出了一个综合评估基准来衡量模型中的长期记忆，包括问答、事件总结和多模态对话生成任务。我们的实验结果表明，法学硕士在理解冗长的对话和理解对话中的长期时间和因果动态方面表现出挑战。采用长背景 LLM 或 RAG 等策略可以提供改进，但这些模型仍然大大落后于人类表现。</li>
</ul>

<h3>Title: Towards Optimal Learning of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17759">https://arxiv.org/abs/2402.17759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17759">https://arxiv.org/pdf/2402.17759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17759]] Towards Optimal Learning of Language Models(https://arxiv.org/abs/2402.17759)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an "LM-training-as-lossless-compression" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.</li>
<li><strong>摘要：</strong>这项工作研究了改进语言模型（LM）学习的一般原则，旨在减少实现卓越性能所需的训练步骤。具体来说，我们提出了 LM 最优学习的理论。我们首先提出一个目标，通过在“LM 训练无损压缩”视图中最大化数据压缩比来优化 LM 学习。然后，我们推导出一个名为学习定律的定理，以揭示我们目标下的最优学习过程的动态特性。然后通过线性分类和现实世界语言建模任务的实验验证该定理。最后，我们通过实证验证了 LM 的最优学习本质上源于 LM 标度律中系数的改进，这对于设计实用的学习加速方法具有巨大的前景和意义。我们的代码可以在 https://aka.ms/LearningLaw 找到。</li>
</ul>

<h3>Title: Massive Activations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17762">https://arxiv.org/abs/2402.17762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17762">https://arxiv.org/pdf/2402.17762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17762]] Massive Activations in Large Language Models(https://arxiv.org/abs/2402.17762)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.</li>
<li><strong>摘要：</strong>我们在大型语言模型 (LLM) 中观察到一个经验现象——很少有激活表现出比其他激活显着更大的值（例如，大 100,000 倍）。我们称之为大规模激活。首先，我们证明了各种法学硕士中大规模激活的广泛存在，并描述了它们的位置。其次，我们发现无论输入如何，它们的值基本上保持不变，并且它们在法学硕士中起着不可或缺的偏差项的作用。第三，这些大量的激活导致注意力概率集中到相应的标记上，进而导致自注意力输出中的隐含偏差项。最后，我们还研究了视觉变形金刚中的大规模激活。</li>
</ul>

<h3>Title: The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</h3>
<ul>
<li><strong>Authors: </strong>Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17764">https://arxiv.org/abs/2402.17764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17764">https://arxiv.org/pdf/2402.17764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17764]] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits(https://arxiv.org/abs/2402.17764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.</li>
<li><strong>摘要：</strong>最近的研究，例如 BitNet，正在为 1 位大型语言模型 (LLM) 的新时代铺平道路。在这项工作中，我们引入了一个 1 位 LLM 变体，即 BitNet b1.58，其中 LLM 的每个参数（或权重）都是三元的 {-1, 0, 1}。它在困惑度和最终任务性能方面与具有相同模型大小和训练令牌的全精度（即 FP16 或 BF16）Transformer LLM 相匹配，同时在延迟、内存、吞吐量、和能源消耗。更深刻的是，1.58 位法学硕士定义了新的扩展法则和配方，用于培训高性能且经济高效的新一代法学硕士。此外，它还实现了一种新的计算范例，并为设计针对 1 位 LLM 优化的特定硬件打开了大门。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
