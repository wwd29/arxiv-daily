<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-24</h1>
<h3>Title: Domain Adaptation in Intent Classification Systems: A Review</h3>
<ul>
<li><strong>Authors: </strong>Jesse Atuhurra, Hidetaka Kamigaito, Taro Watanabe, Eric Nichols</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14415">https://arxiv.org/abs/2404.14415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14415">https://arxiv.org/pdf/2404.14415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14415]] Domain Adaptation in Intent Classification Systems: A Review(https://arxiv.org/abs/2404.14415)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Dialogue agents, which perform specific tasks, are part of the long-term goal of NLP researchers to build intelligent agents that communicate with humans in natural language. Such systems should adapt easily from one domain to another to assist users in completing tasks. Researchers have developed a broad range of techniques, objectives, and datasets for intent classification to achieve such systems. Despite the progress in developing intent classification systems (ICS), a systematic review of the progress from a technical perspective is yet to be conducted. In effect, important implementation details of intent classification remain restricted and unclear, making it hard for natural language processing (NLP) researchers to develop new methods. To fill this gap, we review contemporary works in intent classification. Specifically, we conduct a thorough technical review of the datasets, domains, tasks, and methods needed to train the intent classification part of dialogue systems. Our structured analysis describes why intent classification is difficult and studies the limitations to domain adaptation while presenting opportunities for future work.</li>
<li><strong>摘要：</strong>执行特定任务的对话代理是 NLP 研究人员的长期目标的一部分，即构建以自然语言与人类交流的智能代理。这样的系统应该能够轻松地从一个领域适应到另一个领域，以帮助用户完成任务。研究人员开发了广泛的意图分类技术、目标和数据集来实现此类系统。尽管在开发意图分类系统（ICS）方面取得了进展，但尚未从技术角度对进展进行系统审查。实际上，意图分类的重要实现细节仍然受到限制和不清楚，这使得自然语言处理（NLP）研究人员很难开发新方法。为了填补这一空白，我们回顾了意图分类方面的当代作品。具体来说，我们对训练对话系统意图分类部分所需的数据集、领域、任务和方法进行彻底的技术审查。我们的结构化分析描述了意图分类为何困难的原因，并研究了领域适应的局限性，同时为未来的工作提供了机会。</li>
</ul>

<h3>Title: EPI-SQL: Enhancing Text-to-SQL Translation with Error-Prevention  Instructions</h3>
<ul>
<li><strong>Authors: </strong>Xiping Liu, Zhao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14453">https://arxiv.org/abs/2404.14453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14453">https://arxiv.org/pdf/2404.14453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14453]] EPI-SQL: Enhancing Text-to-SQL Translation with Error-Prevention  Instructions(https://arxiv.org/abs/2404.14453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The conversion of natural language queries into SQL queries, known as Text-to-SQL, is a critical yet challenging task. This paper introduces EPI-SQL, a novel methodological framework leveraging Large Language Models (LLMs) to enhance the performance of Text-to-SQL tasks. EPI-SQL operates through a four-step process. Initially, the method involves gathering instances from the Spider dataset on which LLMs are prone to failure. These instances are then utilized to generate general error-prevention instructions (EPIs). Subsequently, LLMs craft contextualized EPIs tailored to the specific context of the current task. Finally, these context-specific EPIs are incorporated into the prompt used for SQL generation. EPI-SQL is distinguished in that it provides task-specific guidance, enabling the model to circumvent potential errors for the task at hand. Notably, the methodology rivals the performance of advanced few-shot methods despite being a zero-shot approach. An empirical assessment using the Spider benchmark reveals that EPI-SQL achieves an execution accuracy of 85.1\%, underscoring its effectiveness in generating accurate SQL queries through LLMs. The findings indicate a promising direction for future research, i.e. enhancing instructions with task-specific and contextualized rules, for boosting LLMs' performance in NLP tasks.</li>
<li><strong>摘要：</strong>将自然语言查询转换为 SQL 查询（称为文本到 SQL）是一项关键但具有挑战性的任务。本文介绍了 EPI-SQL，这是一种利用大型语言模型 (LLM) 来增强文本到 SQL 任务性能的新颖方法框架。 EPI-SQL 通过四个步骤进行操作。最初，该方法涉及从 Spider 数据集中收集 LLM 容易失败的实例。然后利用这些实例来生成通用防错指令 (EPI)。随后，法学硕士会根据当前任务的具体情况制定情境化的 EPI。最后，这些特定于上下文的 EPI 被合并到用于 SQL 生成的提示中。 EPI-SQL 的独特之处在于它提供特定于任务的指导，使模型能够避免手头任务的潜在错误。值得注意的是，尽管该方法是零样本方法，但其性能可与先进的少样本方法相媲美。使用 Spider 基准的实证评估表明，EPI-SQL 的执行精度达到 85.1\%，强调了其通过 LLM 生成准确 SQL 查询的有效性。研究结果表明了未来研究的一个有希望的方向，即通过特定于任务和情境化的规则来增强指令，以提高法学硕士在 NLP 任务中的表现。</li>
</ul>

<h3>Title: Reinforcement of Explainability of ChatGPT Prompts by Embedding Breast  Cancer Self-Screening Rules into AI Responses</h3>
<ul>
<li><strong>Authors: </strong>Yousef Khan, Ahmed Abdeen Hamed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14454">https://arxiv.org/abs/2404.14454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14454">https://arxiv.org/pdf/2404.14454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14454]] Reinforcement of Explainability of ChatGPT Prompts by Embedding Breast  Cancer Self-Screening Rules into AI Responses(https://arxiv.org/abs/2404.14454)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Addressing the global challenge of breast cancer, this research explores the fusion of generative AI, focusing on ChatGPT 3.5 turbo model, and the intricacies of breast cancer risk assessment. The research aims to evaluate ChatGPT's reasoning capabilities, emphasizing its potential to process rules and provide explanations for screening recommendations. The study seeks to bridge the technology gap between intelligent machines and clinicians by demonstrating ChatGPT's unique proficiency in natural language reasoning. The methodology employs a supervised prompt-engineering approach to enforce detailed explanations for ChatGPT's recommendations. Synthetic use cases, generated algorithmically, serve as the testing ground for the encoded rules, evaluating the model's processing prowess. Findings highlight ChatGPT's promising capacity in processing rules comparable to Expert System Shells, with a focus on natural language reasoning. The research introduces the concept of reinforcement explainability, showcasing its potential in elucidating outcomes and facilitating user-friendly interfaces for breast cancer risk assessment.</li>
<li><strong>摘要：</strong>为了应对乳腺癌的全球挑战，本研究探索了生成式人工智能的融合，重点关注 ChatGPT 3.5 Turbo 模型以及乳腺癌风险评估的复杂性。该研究旨在评估 ChatGPT 的推理能力，强调其处理规则并为筛选建议提供解释的潜力。该研究旨在通过展示 ChatGPT 在自然语言推理方面的独特能力来缩小智能机器和临床医生之间的技术差距。该方法采用有监督的即时工程方法来强制对 ChatGPT 的建议进行详细解释。通过算法生成的综合用例可作为编码规则的测试场，评估模型的处理能力。研究结果突显了 ChatGPT 在处理规则方面的能力，可与专家系统 Shell 相媲美，重点是自然语言推理。该研究引入了强化可解释性的概念，展示了其在阐明结果和促进乳腺癌风险评估的用户友好界面方面的潜力。</li>
</ul>

<h3>Title: Competition Report: Finding Universal Jailbreak Backdoors in Aligned  LLMs</h3>
<ul>
<li><strong>Authors: </strong>Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, Florian Tramèr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14461">https://arxiv.org/abs/2404.14461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14461">https://arxiv.org/pdf/2404.14461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14461]] Competition Report: Finding Universal Jailbreak Backdoors in Aligned  LLMs(https://arxiv.org/abs/2404.14461)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models are aligned to be safe, preventing users from generating harmful content like misinformation or instructions for illegal activities. However, previous work has shown that the alignment process is vulnerable to poisoning attacks. Adversaries can manipulate the safety training data to inject backdoors that act like a universal sudo command: adding the backdoor string to any prompt enables harmful responses from models that, otherwise, behave safely. Our competition, co-located at IEEE SaTML 2024, challenged participants to find universal backdoors in several large language models. This report summarizes the key findings and promising ideas for future research.</li>
<li><strong>摘要：</strong>大型语言模型经过调整以保证安全，防止用户生成有害内容，例如错误信息或非法活动指令。然而，之前的工作表明，对齐过程很容易受到中毒攻击。攻击者可以操纵安全训练数据来注入后门，其作用类似于通用 sudo 命令：将后门字符串添加到任何提示中都会导致模型做出有害响应，否则，这些模型会表现得安全。我们的竞赛在 IEEE SaTML 2024 上举行，要求参与者在几种大型语言模型中找到通用后门。本报告总结了主要发现和未来研究的有前景的想法。</li>
</ul>

<h3>Title: DAIC-WOZ: On the Validity of Using the Therapist's prompts in Automatic  Depression Detection from Clinical Interviews</h3>
<ul>
<li><strong>Authors: </strong>Sergio Burdisso, Ernesto Reyes-Ramírez, Esaú Villatoro-Tello, Fernando Sánchez-Vega, Pastor López-Monroy, Petr Motlicek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14463">https://arxiv.org/abs/2404.14463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14463">https://arxiv.org/pdf/2404.14463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14463]] DAIC-WOZ: On the Validity of Using the Therapist's prompts in Automatic  Depression Detection from Clinical Interviews(https://arxiv.org/abs/2404.14463)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, agent</a></li>
<li><strong>Abstract: </strong>Automatic depression detection from conversational data has gained significant interest in recent years. The DAIC-WOZ dataset, interviews conducted by a human-controlled virtual agent, has been widely used for this task. Recent studies have reported enhanced performance when incorporating interviewer's prompts into the model. In this work, we hypothesize that this improvement might be mainly due to a bias present in these prompts, rather than the proposed architectures and methods. Through ablation experiments and qualitative analysis, we discover that models using interviewer's prompts learn to focus on a specific region of the interviews, where questions about past experiences with mental health issues are asked, and use them as discriminative shortcuts to detect depressed participants. In contrast, models using participant responses gather evidence from across the entire interview. Finally, to highlight the magnitude of this bias, we achieve a 0.90 F1 score by intentionally exploiting it, the highest result reported to date on this dataset using only textual information. Our findings underline the need for caution when incorporating interviewers' prompts into models, as they may inadvertently learn to exploit targeted prompts, rather than learning to characterize the language and behavior that are genuinely indicative of the patient's mental health condition.</li>
<li><strong>摘要：</strong>近年来，从对话数据中自动检测抑郁症引起了人们的极大兴趣。 DAIC-WOZ 数据集是由人类控制的虚拟代理进行的访谈，已广泛用于此任务。最近的研究表明，将面试官的提示纳入模型中后，绩效会得到提高。在这项工作中，我们假设这种改进可能主要是由于这些提示中存在的偏差，而不是所提出的架构和方法。通过消融实验和定性分析，我们发现使用采访者提示的模型学会关注采访的特定区域，在该区域询问有关过去心理健康问题经历的问题，并将其用作识别抑郁参与者的辨别捷径。相比之下，使用参与者回答的模型从整个访谈中收集证据。最后，为了强调这种偏差的严重程度，我们通过有意利用它获得了 0.90 F1 分数，这是迄今为止仅使用文本信息在该数据集上报告的最高结果。我们的研究结果强调，在将访谈员的提示纳入模型时需要谨慎，因为他们可能会无意中学会利用有针对性的提示，而不是学会描述真正反映患者心理健康状况的语言和行为。</li>
</ul>

<h3>Title: Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for  Multi-hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Li Jiapeng, Liu Runze, Li Yabo, Zhou Tong, Li Mingling, Chen Xiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14464">https://arxiv.org/abs/2404.14464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14464">https://arxiv.org/pdf/2404.14464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14464]] Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for  Multi-hop Question Answering(https://arxiv.org/abs/2404.14464)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multi-hop question answering is a knowledge-intensive complex problem. Large Language Models (LLMs) use their Chain of Thoughts (CoT) capability to reason complex problems step by step, and retrieval-augmentation can effectively alleviate factual errors caused by outdated and unknown knowledge in LLMs. Recent works have introduced retrieval-augmentation in the CoT reasoning to solve multi-hop question answering. However, these chain methods have the following problems: 1) Retrieved irrelevant paragraphs may mislead the reasoning; 2) An error in the chain structure may lead to a cascade of errors. In this paper, we propose a dynamic retrieval framework called Tree of Reviews (ToR), where the root node is the question, and the other nodes are paragraphs from retrieval, extending different reasoning paths from the root node to other nodes. Our framework dynamically decides to initiate a new search, reject, or accept based on the paragraphs on the reasoning paths. Compared to related work, we introduce a tree structure to handle each retrieved paragraph separately, alleviating the misleading effect of irrelevant paragraphs on the reasoning path; the diversity of reasoning path extension reduces the impact of a single reasoning error on the whole. We conducted experiments on three different multi-hop question answering datasets. The results show that compared to the baseline methods, ToR achieves state-of-the-art performance in both retrieval and response generation. In addition, we propose two tree-based search optimization strategies, pruning and effective expansion, to reduce time overhead and increase the diversity of path extension. We will release our code.</li>
<li><strong>摘要：</strong>多跳问答是一个知识密集型的复杂问题。大型语言模型（LLM）利用其思想链（CoT）能力逐步推理复杂问题，检索增强可以有效减轻LLM中过时和未知知识造成的事实错误。最近的工作在 CoT 推理中引入了检索增强来解决多跳问答。然而，这些链式方法存在以下问题：1）检索到不相关的段落可能会误导推理； 2）链结构中的一个错误可能会导致错误的级联。在本文中，我们提出了一种称为评论树（ToR）的动态检索框架，其中根节点是问题，其他节点是检索中的段落，将不同的推理路径从根节点延伸到其他节点。我们的框架根据推理路径上的段落动态地决定启动新的搜索、拒绝或接受。与相关工作相比，我们引入了树结构来单独处理每个检索到的段落，减轻了不相关段落对推理路径的误导作用；推理路径延伸的多样性减少了单个推理错误对整体的影响。我们对三个不同的多跳问答数据集进行了实验。结果表明，与基线方法相比，ToR 在检索和响应生成方面都实现了最先进的性能。此外，我们提出了两种基于树的搜索优化策略，剪枝和有效扩展，以减少时间开销并增加路径扩展的多样性。我们将发布我们的代码。</li>
</ul>

<h3>Title: Benchmarking Advanced Text Anonymisation Methods: A Comparative Study on  Novel and Traditional Approaches</h3>
<ul>
<li><strong>Authors: </strong>Dimitris Asimopoulos, Ilias Siniosoglou, Vasileios Argyriou, Thomai Karamitsou, Eleftherios Fountoukidis, Sotirios K. Goudos, Ioannis D. Moscholios, Konstantinos E. Psannis, Panagiotis Sarigiannidis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14465">https://arxiv.org/abs/2404.14465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14465">https://arxiv.org/pdf/2404.14465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14465]] Benchmarking Advanced Text Anonymisation Methods: A Comparative Study on  Novel and Traditional Approaches(https://arxiv.org/abs/2404.14465)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the realm of data privacy, the ability to effectively anonymise text is paramount. With the proliferation of deep learning and, in particular, transformer architectures, there is a burgeoning interest in leveraging these advanced models for text anonymisation tasks. This paper presents a comprehensive benchmarking study comparing the performance of transformer-based models and Large Language Models(LLM) against traditional architectures for text anonymisation. Utilising the CoNLL-2003 dataset, known for its robustness and diversity, we evaluate several models. Our results showcase the strengths and weaknesses of each approach, offering a clear perspective on the efficacy of modern versus traditional methods. Notably, while modern models exhibit advanced capabilities in capturing con textual nuances, certain traditional architectures still keep high performance. This work aims to guide researchers in selecting the most suitable model for their anonymisation needs, while also shedding light on potential paths for future advancements in the field.</li>
<li><strong>摘要：</strong>在数据隐私领域，有效匿名文本的能力至关重要。随着深度学习，特别是 Transformer 架构的普及，人们对利用这些先进模型执行文本匿名化任务的兴趣日益浓厚。本文提出了一项全面的基准测试研究，将基于 Transformer 的模型和大型语言模型 (LLM) 的性能与传统文本匿名化架构进行比较。利用以其稳健性和多样性而闻名的 CoNLL-2003 数据集，我们评估了多个模型。我们的结果展示了每种方法的优点和缺点，为现代方法与传统方法的有效性提供了清晰的视角。值得注意的是，虽然现代模型在捕捉上下文细微差别方面表现出先进的能力，但某些传统架构仍然保持高性能。这项工作旨在指导研究人员选择最适合其匿名化需求的模型，同时也揭示该领域未来发展的潜在路径。</li>
</ul>

<h3>Title: Integrating Chemistry Knowledge in Large Language Models via Prompt  Engineering</h3>
<ul>
<li><strong>Authors: </strong>Hongxuan Liu, Haoyu Yin, Zhiyao Luo, Xiaonan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14467">https://arxiv.org/abs/2404.14467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14467">https://arxiv.org/pdf/2404.14467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14467]] Integrating Chemistry Knowledge in Large Language Models via Prompt  Engineering(https://arxiv.org/abs/2404.14467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>This paper presents a study on the integration of domain-specific knowledge in prompt engineering to enhance the performance of large language models (LLMs) in scientific domains. A benchmark dataset is curated to encapsulate the intricate physical-chemical properties of small molecules, their drugability for pharmacology, alongside the functional attributes of enzymes and crystal materials, underscoring the relevance and applicability across biological and chemical domains.The proposed domain-knowledge embedded prompt engineering method outperforms traditional prompt engineering strategies on various metrics, including capability, accuracy, F1 score, and hallucination drop. The effectiveness of the method is demonstrated through case studies on complex materials including the MacMillan catalyst, paclitaxel, and lithium cobalt oxide. The results suggest that domain-knowledge prompts can guide LLMs to generate more accurate and relevant responses, highlighting the potential of LLMs as powerful tools for scientific discovery and innovation when equipped with domain-specific prompts. The study also discusses limitations and future directions for domain-specific prompt engineering development.</li>
<li><strong>摘要：</strong>本文提出了一项关于在即时工程中集成特定领域知识的研究，以提高科学领域中大型语言模型（LLM）的性能。精心策划的基准数据集囊括了小分子复杂的物理化学特性、其药理学成药性以及酶和晶体材料的功能属性，强调了生物和化学领域的相关性和适用性。所提出的领域知识嵌入提示工程方法在各种指标上都优于传统的即时工程策略，包括能力、准确性、F1 分数和幻觉下降。通过对麦克米伦催化剂、紫杉醇和钴酸锂等复杂材料的案例研究证明了该方法的有效性。结果表明，领域知识提示可以指导法学硕士生成更准确和相关的答复，突显了法学硕士在配备特定领域提示时作为科学发现和创新的强大工具的潜力。该研究还讨论了特定领域快速工程开发的局限性和未来方向。</li>
</ul>

<h3>Title: SnapKV: LLM Knows What You are Looking for Before Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14469">https://arxiv.org/abs/2404.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14469">https://arxiv.org/pdf/2404.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14469]] SnapKV: LLM Knows What You are Looking for Before Generation(https://arxiv.org/abs/2404.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在处理广泛的上下文方面取得了显着的进步，其中键值 (KV) 缓存在提高其性能方面发挥着至关重要的作用。然而，KV 缓存随着输入长度的增加而增长，对内存和时间效率提出了挑战。为了解决这个问题，本文介绍了 SnapKV，这是一种创新且无需微调的方法，可以有效地最小化 KV 缓存大小，同时仍然在实际应用程序中提供可比的性能。我们发现模型中的每个注意力头在生成过程中始终关注特定的即时注意力特征。同时，这种稳健的模式可以从位于提示末尾的“观察”窗口获得。利用这一见解，SnapKV 通过为每个注意力头选择聚集的重要 KV 位置来自动压缩 KV 缓存。我们的方法显着减少了处理长输入序列时不断增长的计算开销和内存占用。具体来说，在处理 16K 令牌的输入时，与基线相比，SnapKV 实现了一致的解码速度，生成速度提高了 3.6 倍，内存效率提高了 8.2 倍。同时，它在 16 个长序列数据集上保持了与基线模型相当的性能。此外，SnapKV 可以使用 HuggingFace 实现在单个 A100-80GB GPU 上处理多达 380K 上下文令牌，只需进行微小的更改，在大海捞针测试中仅表现出可以忽略不计的准确性下降。进一步的综合研究表明 SnapKV 具有实际应用的潜力。</li>
</ul>

<h3>Title: WangLab at MEDIQA-CORR 2024: Optimized LLM-based Programs for Medical  Error Detection and Correction</h3>
<ul>
<li><strong>Authors: </strong>Augustin Toma, Ronald Xie, Steven Palayew, Patrick R. Lawler, Bo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14544">https://arxiv.org/abs/2404.14544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14544">https://arxiv.org/pdf/2404.14544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14544]] WangLab at MEDIQA-CORR 2024: Optimized LLM-based Programs for Medical  Error Detection and Correction(https://arxiv.org/abs/2404.14544)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Medical errors in clinical text pose significant risks to patient safety. The MEDIQA-CORR 2024 shared task focuses on detecting and correcting these errors across three subtasks: identifying the presence of an error, extracting the erroneous sentence, and generating a corrected sentence. In this paper, we present our approach that achieved top performance in all three subtasks. For the MS dataset, which contains subtle errors, we developed a retrieval-based system leveraging external medical question-answering datasets. For the UW dataset, reflecting more realistic clinical notes, we created a pipeline of modules to detect, localize, and correct errors. Both approaches utilized the DSPy framework for optimizing prompts and few-shot examples in large language model (LLM) based programs. Our results demonstrate the effectiveness of LLM based programs for medical error correction. However, our approach has limitations in addressing the full diversity of potential errors in medical documentation. We discuss the implications of our work and highlight future research directions to advance the robustness and applicability of medical error detection and correction systems.</li>
<li><strong>摘要：</strong>临床文本中的医疗错误对患者安全构成重大风险。 MEDIQA-CORR 2024 共享任务侧重于通过三个子任务检测和纠正这些错误：识别错误的存在、提取错误的句子以及生成更正的句子。在本文中，我们提出了在所有三个子任务中实现最佳性能的方法。对于包含细微错误的 MS 数据集，我们开发了一个利用外部医学问答数据集的基于检索的系统。对于华盛顿大学数据集，反映更真实的临床记录，我们创建了一系列模块来检测、定位和纠正错误。这两种方法都利用 DSPy 框架来优化基于大型语言模型 (LLM) 的程序中的提示和少量示例。我们的结果证明了基于法学硕士的医疗错误纠正计划的有效性。然而，我们的方法在解决医疗文档中各种潜在错误方面存在局限性。我们讨论了我们工作的意义，并强调了未来的研究方向，以提高医疗错误检测和纠正系统的稳健性和适用性。</li>
</ul>

<h3>Title: WangLab at MEDIQA-M3G 2024: Multimodal Medical Answer Generation using  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ronald Xie, Steven Palayew, Augustin Toma, Gary Bader, Bo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14567">https://arxiv.org/abs/2404.14567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14567">https://arxiv.org/pdf/2404.14567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14567]] WangLab at MEDIQA-M3G 2024: Multimodal Medical Answer Generation using  Large Language Models(https://arxiv.org/abs/2404.14567)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper outlines our submission to the MEDIQA2024 Multilingual and Multimodal Medical Answer Generation (M3G) shared task. We report results for two standalone solutions under the English category of the task, the first involving two consecutive API calls to the Claude 3 Opus API and the second involving training an image-disease label joint embedding in the style of CLIP for image classification. These two solutions scored 1st and 2nd place respectively on the competition leaderboard, substantially outperforming the next best solution. Additionally, we discuss insights gained from post-competition experiments. While the performance of these two solutions have significant room for improvement due to the difficulty of the shared task and the challenging nature of medical visual question answering in general, we identify the multi-stage LLM approach and the CLIP image classification approach as promising avenues for further investigation.</li>
<li><strong>摘要：</strong>本文概述了我们向 MEDIQA2024 多语言和多模式医疗答案生成 (M3G) 共享任务提交的内容。我们报告了该任务英语类别下的两个独立解决方案的结果，第一个涉及对 Claude 3 Opus API 的两次连续 API 调用，第二个涉及以 CLIP 风格训练图像-疾病标签联合嵌入以进行图像分类。这两个解决方案在竞赛排行榜上分别获得第一和第二名，大大超过了第二好的解决方案。此外，我们还讨论了从赛后实验中获得的见解。虽然由于共享任务的难度和医学视觉问答的挑战性，这两种解决方案的性能还有很大的改进空间，但我们认为多阶段 LLM 方法和 CLIP 图像分类方法是有希望的途径进一步的调查。</li>
</ul>

<h3>Title: Describe-then-Reason: Improving Multimodal Mathematical Reasoning  through Visual Comprehension Training</h3>
<ul>
<li><strong>Authors: </strong>Mengzhao Jia, Zhihan Zhang, Wenhao Yu, Fangkai Jiao, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14604">https://arxiv.org/abs/2404.14604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14604">https://arxiv.org/pdf/2404.14604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14604]] Describe-then-Reason: Improving Multimodal Mathematical Reasoning  through Visual Comprehension Training(https://arxiv.org/abs/2404.14604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Open-source multimodal large language models (MLLMs) excel in various tasks involving textual and visual inputs but still struggle with complex multimodal mathematical reasoning, lagging behind proprietary models like GPT-4V(ision) and Gemini-Pro. Although fine-tuning with intermediate steps (i.e., rationales) elicits some mathematical reasoning skills, the resulting models still fall short in visual comprehension due to inadequate visual-centric supervision, which leads to inaccurate interpretation of math figures. To address this issue, we propose a two-step training pipeline VCAR, which emphasizes the Visual Comprehension training in Addition to mathematical Reasoning learning. It first improves the visual comprehension ability of MLLMs through the visual description generation task, followed by another training step on generating rationales with the assistance of descriptions. Experimental results on two popular benchmarks demonstrate that VCAR substantially outperforms baseline methods solely relying on rationale supervision, especially on problems with high visual demands.</li>
<li><strong>摘要：</strong>开源多模态大语言模型 (MLLM) 在涉及文本和视觉输入的各种任务中表现出色，但仍然难以处理复杂的多模态数学推理，落后于 GPT-4V(ision) 和 Gemini-Pro 等专有模型。尽管通过中间步骤（即基本原理）进行微调可以引发一些数学推理技能，但由于以视觉为中心的监督不足，所得模型在视觉理解方面仍然存在不足，从而导致对数学数字的解释不准确。为了解决这个问题，我们提出了一个两步训练管道 VCAR，除了数学推理学习之外，它还强调视觉理解训练。它首先通过视觉描述生成任务提高 MLLM 的视觉理解能力，然后是在描述的帮助下生成基本原理的另一个训练步骤。两个流行基准测试的实验结果表明，VCAR 大大优于仅依赖基本原理监督的基线方法，尤其是在视觉要求较高的问题上。</li>
</ul>

<h3>Title: Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language  Learning</h3>
<ul>
<li><strong>Authors: </strong>Yanhui Guo, Shaoyuan Xu, Jinmiao Fu, Jia Liu, Chaosheng Dong, Bryan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14607">https://arxiv.org/abs/2404.14607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14607">https://arxiv.org/pdf/2404.14607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14607]] Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language  Learning(https://arxiv.org/abs/2404.14607)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This paper introduces \textbf{Q-tuning}, a novel approach for continual prompt tuning that enables the lifelong learning of a pre-trained language model. When learning a new task, Q-tuning trains a task-specific prompt by adding it to a prompt queue consisting of the prompts from older tasks. To better transfer the knowledge of old tasks, we design an adaptive knowledge aggregation technique that reweighs previous prompts in the queue with a learnable low-rank matrix. Once the prompt queue reaches its maximum capacity, we leverage a PCA-based eviction rule to reduce the queue's size, allowing the newly trained prompt to be added while preserving the primary knowledge of old tasks. In order to mitigate the accumulation of information loss caused by the eviction, we additionally propose a globally shared prefix prompt and a memory retention regularization based on information theory. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods substantially on continual prompt tuning benchmarks. Moreover, our approach enables lifelong learning on linearly growing task sequences while requiring constant complexity for training and inference.</li>
<li><strong>摘要：</strong>本文介绍了 \textbf{Q-tuning}，这是一种持续提示调整的新方法，可以实现预训练语言模型的终身学习。学习新任务时，Q-tuning 通过将特定于任务的提示添加到由旧任务的提示组成的提示队列中来训练该提示。为了更好地迁移旧任务的知识，我们设计了一种自适应知识聚合技术，该技术使用可学习的低秩矩阵重新权衡队列中先前的提示。一旦提示队列达到其最大容量，我们就会利用基于 PCA 的驱逐规则来减小队列的大小，从而允许添加新训练的提示，同时保留旧任务的主要知识。为了减轻驱逐造成的信息损失的积累，我们还提出了全局共享的前缀提示和基于信息论的记忆保留正则化。大量的实验表明，我们的方法在持续提示调整基准上大大优于最先进的方法。此外，我们的方法可以实现对线性增长的任务序列的终身学习，同时需要恒定的训练和推理复杂性。</li>
</ul>

<h3>Title: OpenELM: An Efficient Language Model Family with Open-source Training  and Inference Framework</h3>
<ul>
<li><strong>Authors: </strong>Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14619">https://arxiv.org/abs/2404.14619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14619">https://arxiv.org/pdf/2404.14619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14619]] OpenELM: An Efficient Language Model Family with Open-source Training  and Inference Framework(https://arxiv.org/abs/2404.14619)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\times$ fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at \url{https://github.com/apple/corenet}. Additionally, \model models can be found on HuggingFace at: \url{https://huggingface.co/apple/OpenELM}.</li>
<li><strong>摘要：</strong>大型语言模型的可重复性和透明度对于推进开放研究、确保结果的可信性以及对数据和模型偏差以及潜在风险的调查至关重要。为此，我们发布了 OpenELM，一种最先进的开放语言模型。 OpenELM 使用分层缩放策略来有效地分配变压器模型每一层内的参数，从而提高准确性。例如，在参数预算约为 10 亿个参数的情况下，OpenELM 与 OLMo 相比，准确率提高了 2.36%，同时需要的预训练令牌减少了 2 倍。与之前仅提供模型权重和推理代码以及在私有数据集上进行预训练的做法不同，我们的版本包括在公开数据集上训练和评估语言模型的完整框架，包括训练日志、多个检查点和预训练。训练配置。我们还发布了将模型转换为 MLX 库的代码，以便在 Apple 设备上进行推理和微调。这一全面的发布旨在增强和加强开放研究社区，为未来的开放研究努力铺平道路。我们的源代码以及预训练的模型权重和训练配方可在 \url{https://github.com/apple/corenet} 获取。此外，\model 模型可以在 HuggingFace 上找到：\url{https://huggingface.co/apple/OpenELM}。</li>
</ul>

<h3>Title: Automated Multi-Language to English Machine Translation Using Generative  Pre-Trained Transformers</h3>
<ul>
<li><strong>Authors: </strong>Elijah Pelofske, Vincent Urias, Lorie M. Liebrock</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14680">https://arxiv.org/abs/2404.14680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14680">https://arxiv.org/pdf/2404.14680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14680]] Automated Multi-Language to English Machine Translation Using Generative  Pre-Trained Transformers(https://arxiv.org/abs/2404.14680)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The task of accurate and efficient language translation is an extremely important information processing task. Machine learning enabled and automated translation that is accurate and fast is often a large topic of interest in the machine learning and data science communities. In this study, we examine using local Generative Pretrained Transformer (GPT) models to perform automated zero shot black-box, sentence wise, multi-natural-language translation into English text. We benchmark 16 different open-source GPT models, with no custom fine-tuning, from the Huggingface LLM repository for translating 50 different non-English languages into English using translated TED Talk transcripts as the reference dataset. These GPT model inference calls are performed strictly locally, on single A100 Nvidia GPUs. Benchmark metrics that are reported are language translation accuracy, using BLEU, GLEU, METEOR, and chrF text overlap measures, and wall-clock time for each sentence translation. The best overall performing GPT model for translating into English text for the BLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.152$, for the GLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.256$, for the chrF metric is Llama2-chat-AYT-13B with a mean score across all tested languages of $0.448$, and for the METEOR metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.438$.</li>
<li><strong>摘要：</strong>准确高效的语言翻译任务是一项极其重要的信息处理任务。支持机器学习的准确而快速的自动翻译通常是机器学习和数据科学社区感兴趣的一个大话题。在本研究中，我们研究使用本地生成预训练 Transformer (GPT) 模型来执行自动零样本黑盒、句子明智的多自然语言翻译为英语文本。我们对来自 Huggingface LLM 存储库的 16 种不同的开源 GPT 模型进行了基准测试，无需进行自定义微调，以使用翻译的 TED Talk 成绩单作为参考数据集将 50 种不同的非英语语言翻译成英语。这些 GPT 模型推理调用严格在单个 A100 Nvidia GPU 上本地执行。报告的基准指标包括使用 BLEU、GLEU、METEOR 和 chrF 文本重叠测量的语言翻译准确性，以及每个句子翻译的挂钟时间。对于 BLEU 指标而言，用于翻译成英语文本的整体性能最佳的 GPT 模型是 ReMM-v2-L2-13B，所有测试语言的平均得分为 0.152 美元；对于 GLEU 指标来说，ReMM-v2-L2-13B 的平均得分为 0.152 美元。所有测试语言的得分为 0.256 美元，chrF 指标为 Llama2-chat-AYT-13B，所有测试语言的平均得分为 0.448 美元，METEOR 指标为 ReMM-v2-L2-13B，平均得分所有测试语言的费用为 0.438 美元。</li>
</ul>

<h3>Title: MisgenderMender: A Community-Informed Approach to Interventions for  Misgendering</h3>
<ul>
<li><strong>Authors: </strong>Tamanna Hossain, Sunipa Dev, Sameer Singh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14695">https://arxiv.org/abs/2404.14695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14695">https://arxiv.org/pdf/2404.14695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14695]] MisgenderMender: A Community-Informed Approach to Interventions for  Misgendering(https://arxiv.org/abs/2404.14695)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering. Misgendering, the act of incorrectly addressing someone's gender, inflicts serious harm and is pervasive in everyday technologies, yet there is a notable lack of research to combat it. We are the first to address this lack of research into interventions for misgendering by conducting a survey of gender-diverse individuals in the US to understand perspectives about automated interventions for text-based misgendering. Based on survey insights on the prevalence of misgendering, desired solutions, and associated concerns, we introduce a misgendering interventions task and evaluation dataset, MisgenderMender. We define the task with two sub-tasks: (i) detecting misgendering, followed by (ii) correcting misgendering where misgendering is present in domains where editing is appropriate. MisgenderMender comprises 3790 instances of social media content and LLM-generations about non-cisgender public figures, annotated for the presence of misgendering, with additional annotations for correcting misgendering in LLM-generated text. Using this dataset, we set initial benchmarks by evaluating existing NLP systems and highlighting challenges for future models to address. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendermender/.</li>
<li><strong>摘要：</strong>内容警告：本文包含性别错误和删除的示例，可能具有冒犯性和潜在的触发性。性别歧视，即错误地称呼某人性别的行为，会造成严重伤害，并且在日常技术中普遍存在，但目前却明显缺乏应对这种现象的研究。我们是第一个解决性别错误干预措施研究不足的问题的，我们对美国的性别多元化个体进行了一项调查，以了解对基于文本的性别错误自动干预措施的看法。基于对性别错误的普遍性、期望的解决方案和相关问题的调查见解，我们引入了错误性别干预任务和评估数据集 MisgenderMender。我们用两个子任务定义该任务：（i）检测性别错误，然后（ii）在适合编辑的领域中存在性别错误的情况下纠正性别错误。 MisgenderMender 包含 3790 个关于非顺性别公众人物的社交媒体内容和 LLM 生成的实例，对存在性别错误进行了注释，并附有用于纠正 LLM 生成文本中的性别错误的附加注释。使用该数据集，我们通过评估现有的 NLP 系统并强调未来模型需要解决的挑战来设定初始基准。我们在 https://tamannahossainkay.github.io/misgenderender/ 发布了完整的数据集、代码和演示。</li>
</ul>

<h3>Title: Bayesian Example Selection Improves In-Context Learning for Speech,  Text, and Visual Modalities</h3>
<ul>
<li><strong>Authors: </strong>Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14716">https://arxiv.org/abs/2404.14716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14716">https://arxiv.org/pdf/2404.14716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14716]] Bayesian Example Selection Improves In-Context Learning for Speech,  Text, and Visual Modalities(https://arxiv.org/abs/2404.14716)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）可以通过基于对话历史中出现的一些示例的上下文学习（ICL）来适应新任务，而无需更新任何模型参数。尽管有这样的便利，ICL 的性能在很大程度上取决于所提供的上下文示例的质量，这使得上下文示例选择方法成为关键选择。本文提出了一种新颖的 ICL 贝叶斯上下文实例选择方法（ByCS）。 ByCS 扩展了基于贝叶斯定理的以上下文示例为条件的推理概率，重点关注以测试输入为条件的逆推理。假设准确的逆推理概率（似然）将导致准确的推理概率（后验），则根据逆推理结果选择上下文中的示例。使用语音、文本和图像示例进行了多样化和广泛的跨任务和跨模态实验。实验结果表明了我们的 ByCS 方法在各种模型、任务和模式上的有效性和鲁棒性。</li>
</ul>

<h3>Title: Insights into Alignment: Evaluating DPO and its Variants Across Multiple  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Amir Saeidi, Shivanshu Verma, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14723">https://arxiv.org/abs/2404.14723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14723">https://arxiv.org/pdf/2404.14723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14723]] Insights into Alignment: Evaluating DPO and its Variants Across Multiple  Tasks(https://arxiv.org/abs/2404.14723)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance across a spectrum of tasks. Recently, Direct Preference Optimization (DPO) has emerged as an RL-free approach to optimize the policy model on human preferences. However, several limitations hinder the widespread adoption of this method. To address these shortcomings, various versions of DPO have been introduced. Yet, a comprehensive evaluation of these variants across diverse tasks is still lacking. In this study, we aim to bridge this gap by investigating the performance of alignment methods across three distinct scenarios: (1) keeping the Supervised Fine-Tuning (SFT) part, (2) skipping the SFT part, and (3) skipping the SFT part and utilizing an instruction-tuned model. Furthermore, we explore the impact of different training sizes on their performance. Our evaluation spans a range of tasks including dialogue systems, reasoning, mathematical problem-solving, question answering, truthfulness, and multi-task understanding, encompassing 13 benchmarks such as MT-Bench, Big Bench, and Open LLM Leaderboard. Key observations reveal that alignment methods achieve optimal performance with smaller training data subsets, exhibit limited effectiveness in reasoning tasks yet significantly impact mathematical problem-solving, and employing an instruction-tuned model notably influences truthfulness. We anticipate that our findings will catalyze further research aimed at developing more robust models to address alignment challenges.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在一系列任务中表现出了卓越的性能。最近，直接偏好优化（DPO）作为一种无强化学习的方法出现，用于优化人类偏好的政策模型。然而，一些限制阻碍了这种方法的广泛采用。为了解决这些缺点，引入了各种版本的 DPO。然而，仍然缺乏对不同任务中的这些变体的全面评估。在本研究中，我们的目标是通过研究对齐方法在三种不同场景中的性能来弥补这一差距：(1) 保留监督微调 (SFT) 部分，(2) 跳过 SFT 部分，以及 (3) 跳过SFT 部分并利用指令调整模型。此外，我们探讨了不同训练规模对其表现的影响。我们的评估涵盖对话系统、推理、数学问题解决、问题回答、真实性和多任务理解等一系列任务，涵盖 MT-Bench、Big Bench 和 Open LLM Leaderboard 等 13 个基准。主要观察结果表明，对齐方法可以通过较小的训练数据子集实现最佳性能，在推理任务中表现出有限的有效性，但会显着影响数学问题的解决，并且采用指令调整模型会显着影响真实性。我们预计我们的发现将促进进一步的研究，旨在开发更强大的模型来应对对齐挑战。</li>
</ul>

<h3>Title: Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete  Knowledge Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang Tong, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14741">https://arxiv.org/abs/2404.14741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14741">https://arxiv.org/pdf/2404.14741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14741]] Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete  Knowledge Graph Question Answering(https://arxiv.org/abs/2404.14741)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>To address the issue of insufficient knowledge and the tendency to generate hallucination in Large Language Models (LLMs), numerous studies have endeavored to integrate LLMs with Knowledge Graphs (KGs). However, all these methods are evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where the factual triples involved in each question are entirely covered by the given KG. In this situation, LLM mainly acts as an agent to find answer entities by exploring the KG, rather than effectively integrating internal and external knowledge sources. However, in real-world scenarios, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, in this paper, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include all the factual triples involved in each question. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG) that can generate new factual triples while exploring on KGs. Specifically, we propose a selecting-generating-answering framework, which not only treat the LLM as an agent to explore on KGs, but also treat it as a KG to generate new facts based on the explored subgraph and its inherent knowledge. Experimental results on two datasets demonstrate that our GoG can solve IKGQA to a certain extent, while almost all previous methods cannot perform well on IKGQA.</li>
<li><strong>摘要：</strong>为了解决大型语言模型（LLM）知识不足和容易产生幻觉的问题，许多研究都致力于将 LLM 与知识图谱（KG）相结合。然而，所有这些方法都是在具有完整知识图谱的传统知识图问答（KGQA）上进行评估的，其中每个问题涉及的事实三元组完全由给定的知识图谱覆盖。在这种情况下，LLM主要充当代理通过探索知识图谱来寻找答案实体，而不是有效整合内部和外部知识源。然而，在现实场景中，知识图谱通常不完整，无法涵盖回答问题所需的所有知识。为了模拟现实世界的场景并评估法学硕士整合内部和外部知识的能力，在本文中，我们建议利用法学硕士在不完整知识图（IKGQA）下进行质量保证，其中给定的知识图谱不包括所涉及的所有事实三元组在每个问题中。为了处理 IKGQA，我们提出了一种名为“Generate-on-Graph”（GoG）的免训练方法，该方法可以在探索 KG 时生成新的事实三元组。具体来说，我们提出了一个选择-生成-回答框架，该框架不仅将LLM视为在知识图谱上进行探索的代理，而且将其视为知识图谱，根据探索的子图及其固有知识生成新的事实。两个数据集上的实验结果表明，我们的 GoG 可以在一定程度上解决 IKGQA，而几乎所有以前的方法都不能在 IKGQA 上表现良好。</li>
</ul>

<h3>Title: Retrieval Augmented Generation for Domain-specific Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Sanat Sharma, David Seunghyun Yoon, Franck Dernoncourt, Dewang Sultania, Karishma Bagga, Mengjiao Zhang, Trung Bui, Varun Kotte</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14760">https://arxiv.org/abs/2404.14760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14760">https://arxiv.org/pdf/2404.14760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14760]] Retrieval Augmented Generation for Domain-specific Question Answering(https://arxiv.org/abs/2404.14760)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Question answering (QA) has become an important application in the advanced development of large language models. General pre-trained large language models for question-answering are not trained to properly understand the knowledge or terminology for a specific domain, such as finance, healthcare, education, and customer service for a product. To better cater to domain-specific understanding, we build an in-house question-answering system for Adobe products. We propose a novel framework to compile a large question-answer database and develop the approach for retrieval-aware finetuning of a Large Language model. We showcase that fine-tuning the retriever leads to major improvements in the final generation. Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding.</li>
<li><strong>摘要：</strong>问答（QA）已成为大型语言模型高级开发中的重要应用。用于问答的一般预训练大语言模型并未经过训练来正确理解特定领域的知识或术语，例如金融、医疗保健、教育和产品的客户服务。为了更好地满足特定领域的理解，我们为 Adob​​e 产品构建了一个内部问答系统。我们提出了一种新颖的框架来编译大型问答数据库，并开发用于大型语言模型的检索感知微调的方法。我们展示了对检索器的微调导致了最后一代的重大改进。我们的整体方法减少了生成过程中的幻觉，同时将最新的检索信息保留在上下文中以进行上下文基础。</li>
</ul>

<h3>Title: Simulating Task-Oriented Dialogues with State Transition Graphs and  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chris Samarinas, Pracha Promthaw, Atharva Nijasure, Hansi Zeng, Julian Killingback, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14772">https://arxiv.org/abs/2404.14772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14772">https://arxiv.org/pdf/2404.14772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14772]] Simulating Task-Oriented Dialogues with State Transition Graphs and  Large Language Models(https://arxiv.org/abs/2404.14772)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper explores SynTOD, a new synthetic data generation approach for developing end-to-end Task-Oriented Dialogue (TOD) Systems capable of handling complex tasks such as intent classification, slot filling, conversational question-answering, and retrieval-augmented response generation, without relying on crowdsourcing or real-world data. SynTOD utilizes a state transition graph to define the desired behavior of a TOD system and generates diverse, structured conversations through random walks and response simulation using large language models (LLMs). In our experiments, using graph-guided response simulations leads to significant improvements in intent classification, slot filling and response relevance compared to naive single-prompt simulated conversations. We also investigate the end-to-end TOD effectiveness of different base and instruction-tuned LLMs, with and without the constructed synthetic conversations. Finally, we explore how various LLMs can evaluate responses in a TOD system and how well they are correlated with human judgments. Our findings pave the path towards quick development and evaluation of domain-specific TOD systems. We release our datasets, models, and code for research purposes.</li>
<li><strong>摘要：</strong>本文探讨了 SynTOD，这是一种新的合成数据生成方法，用于开发端到端的面向任务的对话 (TOD) 系统，该系统能够处理复杂的任务，例如意图分类、槽位填充、对话式问答和检索增强响应生成，而不依赖于众包或真实世界的数据。 SynTOD 利用状态转换图来定义 TOD 系统所需的行为，并使用大型语言模型 (LLM) 通过随机游走和响应模拟来生成多样化的结构化对话。在我们的实验中，与简单的单提示模拟对话相比，使用图形引导的响应模拟可以显着改进意图分类、槽位填充和响应相关性。我们还研究了不同基础和指令调整的法学硕士的端到端 TOD 有效性，无论有没有构建的合成对话。最后，我们探讨了各种法学硕士如何评估 TOD 系统中的响应以及它们与人类判断的相关性如何。我们的研究结果为快速开发和评估特定领域的 TOD 系统铺平了道路。我们出于研究目的发布数据集、模型和代码。</li>
</ul>

<h3>Title: CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based  Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ling Yue, Tianfan Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14777">https://arxiv.org/abs/2404.14777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14777">https://arxiv.org/pdf/2404.14777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14777]] CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based  Reasoning(https://arxiv.org/abs/2404.14777)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge. Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility. We introduce Clinical Agent System (CT-Agent), a Clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology. This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities. Our system autonomously manages the entire clinical trial process, demonstrating significant efficiency improvements in our evaluations, which include both computational benchmarks and expert feedback.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）和多智能体系统在自然语言任务中表现出了令人印象深刻的能力，但在临床试验应用中面临挑战，这主要是由于对外部知识的访问有限。认识到根据最新医疗数据进行汇总和预测的先进临床试验工具的潜力，我们提出了一种集成解决方案来增强其可访问性和实用性。我们推出临床代理系统（CT-Agent），这是一种专为临床试验任务而设计的临床多代理系统，利用 GPT-4、多代理架构、LEAST-TO-MOST 和 ReAct 推理技术。这种集成不仅提高了法学硕士在临床环境中的表现，还引入了新的功能。我们的系统自主管理整个临床试验过程，证明我们的评估效率显着提高，其中包括计算基准和专家反馈。</li>
</ul>

<h3>Title: Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs:  Full-Parameter vs. Parameter-Efficient Approaches</h3>
<ul>
<li><strong>Authors: </strong>Clément Christophe, Praveen K Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie Rajan, Ahmed Al-Mahrooqi, Avani Gupta, Muhammad Umar Salman, Gurpreet Gosal, Bhargav Kanakiya, Charles Chen, Natalia Vassilieva, Boulbaba Ben Amor, Marco AF Pimentel, Shadab Khan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14779">https://arxiv.org/abs/2404.14779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14779">https://arxiv.org/pdf/2404.14779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14779]] Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs:  Full-Parameter vs. Parameter-Efficient Approaches(https://arxiv.org/abs/2404.14779)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.</li>
<li><strong>摘要：</strong>本研究在医学大语言模型 (LLM) 的背景下，对两种主要的微调方法（全参数微调和参数高效调整）进行了全面分析和比较。我们基于 Llama-2 架构开发并完善了一系列 LLM，专门用于增强医学知识检索、推理和问答能力。我们的实验系统地评估了这些调整策略在各种众所周知的医学基准上的有效性。值得注意的是，我们的医学法学硕士 Med42 在美国医学执照考试 (USMLE) 数据集上的准确率达到 72%，为公开的医学法学硕士树立了新的绩效标准。通过这种比较分析，我们的目标是找到在医学领域微调法学硕士的最有效和最高效的方法，从而为人工智能驱动的医疗保健应用的进步做出重大贡献。</li>
</ul>

<h3>Title: Talk Too Much: Poisoning Large Language Models under Token Limit</h3>
<ul>
<li><strong>Authors: </strong>Jiaming He, Wenbo Jiang, Guanyu Hou, Wenshu Fan, Rui Zhang, Hongwei Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14795">https://arxiv.org/abs/2404.14795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14795">https://arxiv.org/pdf/2404.14795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14795]] Talk Too Much: Poisoning Large Language Models under Token Limit(https://arxiv.org/abs/2404.14795)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Mainstream poisoning attacks on large language models (LLMs) typically set a fixed trigger in the input instance and specific responses for triggered queries. However, the fixed trigger setting (e.g., unusual words) may be easily detected by human detection, limiting the effectiveness and practicality in real-world scenarios. To enhance the stealthiness of the trigger, we present a poisoning attack against LLMs that is triggered by a generation/output condition-token limitation, which is a commonly adopted strategy by users for reducing costs. The poisoned model performs normally for output without token limitation, while becomes harmful for output with limited tokens. To achieve this objective, we introduce BrieFool, an efficient attack framework. It leverages the characteristics of generation limitation by efficient instruction sampling and poisoning data generation, thereby influencing the behavior of LLMs under target conditions. Our experiments demonstrate that BrieFool is effective across safety domains and knowledge domains. For instance, with only 20 generated poisoning examples against GPT-3.5-turbo, BrieFool achieves a 100% Attack Success Rate (ASR) and a 9.28/10 average Harmfulness Score (HS) under token limitation conditions while maintaining the benign performance.</li>
<li><strong>摘要：</strong>针对大型语言模型 (LLM) 的主流中毒攻击通常会在输入实例中设置固定触发器，并针对触发的查询设置特定响应。然而，固定的触发设置（例如，不寻常的单词）可能很容易被人类检测到，限制了现实场景中的有效性和实用性。为了增强触发器的隐蔽性，我们提出了一种针对 LLM 的中毒攻击，该攻击是由生成/输出条件令牌限制触发的，这是用户为降低成本而普遍采用的策略。中毒模型对于没有令牌限制的输出表现正常，而对于有限令牌的输出则变得有害。为了实现这一目标，我们引入了 BrieFool，一个高效的攻击框架。它通过有效的指令采样和中毒数据生成来利用生成限制的特征，从而影响目标条件下LLM的行为。我们的实验证明 BrieFool 在安全领域和知识领域都是有效的。例如，仅生成了 20 个针对 GPT-3.5-turbo 的中毒示例，BrieFool 在代币限制条件下实现了 100% 的攻击成功率 (ASR) 和 9.28/10 的平均危害评分 (HS)，同时保持了良好的性能。</li>
</ul>

<h3>Title: A Survey of Large Language Models on Generative Graph Analytics: Query,  Learning, and Applications</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Shang, Xin Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14809">https://arxiv.org/abs/2404.14809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14809">https://arxiv.org/pdf/2404.14809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14809]] A Survey of Large Language Models on Generative Graph Analytics: Query,  Learning, and Applications(https://arxiv.org/abs/2404.14809)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.</li>
<li><strong>摘要：</strong>图是表示社会和自然中各种实体及其复杂关系的基本数据模型，例如社交网络、交通网络、金融网络和生物医学系统。最近，大型语言模型（LLM）表现出了强大的泛化能力，可以处理各种自然语言处理和多模式任务，以回答用户的任意问题和特定领域的内容生成。与图学习模型相比，法学硕士通过消除训练图学习模型的需要并降低手动注释的成本，在解决泛化图任务的挑战方面具有优越的优势。在本次调查中，我们对现有的图数据LLM研究进行了全面调查，总结了先进的LLM模型解决的相关图分析任务，并指出了现有的剩余挑战和未来的方向。具体来说，我们研究了基于LLM的生成图分析（LLM-GGA）的关键问题，分为三类：基于LLM的图查询处理（LLM-GQP）、基于LLM的图推理和学习（LLM-GIL）以及图-基于法学硕士的申请。 LLM-GQP侧重于图分析技术和LLM提示的集成，包括图理解和基于知识图（KG）的增强检索，而LLM-GIL侧重于图的学习和推理，包括图学习、图形成推理和图推理。表示。我们总结了 LLM 中处理不同图形下游任务的有用提示。此外，我们还总结了 LLM 模型评估、基准数据集/任务，并对 LLM 模型的优缺点进行了深入分析。我们还在法学硕士和图形分析这个令人兴奋的跨学科研究领域探索开放问题和未来方向。</li>
</ul>

<h3>Title: Pattern-Aware Chain-of-Thought Prompting in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Zhang, Xuepeng Wang, Lingxiang Wu, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14812">https://arxiv.org/abs/2404.14812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14812">https://arxiv.org/pdf/2404.14812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14812]] Pattern-Aware Chain-of-Thought Prompting in Large Language Models(https://arxiv.org/abs/2404.14812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) prompting can guide language models to engage in complex multi-step reasoning. The quality of provided demonstrations significantly impacts the success of downstream inference tasks. While existing automated methods prioritize accuracy and semantics in these demonstrations, we show that the underlying reasoning patterns play a more crucial role in such tasks. In this paper, we propose Pattern-Aware CoT, a prompting method that considers the diversity of demonstration patterns. By incorporating patterns such as step length and reasoning process within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios. We conduct experiments on nine reasoning benchmark tasks using two open-source LLMs. The results show that our method substantially enhances reasoning performance and exhibits robustness to errors. The code will be made publicly available.</li>
<li><strong>摘要：</strong>思想链（CoT）提示可以引导语言模型进行复杂的多步骤推理。提供的演示的质量极大地影响下游推理任务的成功。虽然现有的自动化方法在这些演示中优先考虑准确性和语义，但我们表明潜在的推理模式在此类任务中发挥着更重要的作用。在本文中，我们提出了 Pattern-Aware CoT，一种考虑演示模式多样性的提示方法。通过在中间步骤中结合步长和推理过程等模式，PA-CoT 有效地减轻了演示引起的偏差问题，并能够更好地泛化到不同的场景。我们使用两个开源法学硕士对九个推理基准任务进行了实验。结果表明，我们的方法大大提高了推理性能并表现出对错误的鲁棒性。该代码将公开。</li>
</ul>

<h3>Title: Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Tan, Mingchen Li, Bingxin Zhou, Bozitao Zhong, Lirong Zheng, Pan Tan, Ziyi Zhou, Huiqun Yu, Guisheng Fan, Liang Hong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14850">https://arxiv.org/abs/2404.14850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14850">https://arxiv.org/pdf/2404.14850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14850]] Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein  Language Models(https://arxiv.org/abs/2404.14850)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Pre-trained protein language models (PLMs) has emerged as a prominent strategy for enhancing downstream prediction tasks, often outperforming traditional supervised learning approaches. As a widely applied powerful technique in natural language processing, employing Parameter-Efficient Fine-Tuning techniques could potentially enhance the performance of PLMs. However, the direct transfer to life science tasks is non-trivial due to the different training strategies and data forms. To address this gap, we introduce SES-Adapter, a simple, efficient, and scalable adapter method for enhancing the representation learning of PLMs. SES-Adapter incorporates PLM embeddings with structural sequence embeddings to create structure-aware representations. We show that the proposed method is compatible with different PLM architectures and across diverse tasks. Extensive evaluations are conducted on 2 types of folding structures with notable quality differences, 9 state-of-the-art baselines, and 9 benchmark datasets across distinct downstream tasks. Results show that compared to vanilla PLMs, SES-Adapter improves downstream task performance by a maximum of 11% and an average of 3%, with significantly accelerated training speed by a maximum of 1034% and an average of 362%, the convergence rate is also improved by approximately 2 times. Moreover, positive optimization is observed even with low-quality predicted structures. The source code for SES-Adapter is available at https://github.com/tyang816/SES-Adapter.</li>
<li><strong>摘要：</strong>微调预训练蛋白质语言模型 (PLM) 已成为增强下游预测任务的重要策略，通常优于传统的监督学习方法。作为自然语言处理中广泛应用的强大技术，采用参数高效微调技术可以潜在地提高 PLM 的性能。然而，由于训练策略和数据形式不同，直接转移到生命科学任务并非易事。为了解决这一差距，我们引入了 SES-Adapter，这是一种简单、高效且可扩展的适配器方法，用于增强 PLM 的表示学习。 SES-Adapter 将 PLM 嵌入与结构序列嵌入相结合，以创建结构感知表示。我们证明所提出的方法与不同的 PLM 架构和不同的任务兼容。对两种质量差异显着的折叠结构、9 个最先进的基线和 9 个跨不同下游任务的基准数据集进行了广泛的评估。结果表明，与vanilla PLM相比，SES-Adapter下游任务性能最大提升11%，平均提升3%，训练速度显着加快，最大提升1034%，平均提升362%，收敛速度为也提高了约2倍。此外，即使对于低质量的预测结构，也可以观察到积极的优化。 SES-Adapter 的源代码可在 https://github.com/tyang816/SES-Adapter 获取。</li>
</ul>

<h3>Title: Language in Vivo vs. in Silico: Size Matters but Larger Language Models  Still Do Not Comprehend Language on a Par with Humans</h3>
<ul>
<li><strong>Authors: </strong>Vittoria Dentella, Fritz Guenther, Evelina Leivada</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14883">https://arxiv.org/abs/2404.14883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14883">https://arxiv.org/pdf/2404.14883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14883]] Language in Vivo vs. in Silico: Size Matters but Larger Language Models  Still Do Not Comprehend Language on a Par with Humans(https://arxiv.org/abs/2404.14883)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt, chat</a></li>
<li><strong>Abstract: </strong>Understanding the limits of language is a prerequisite for Large Language Models (LLMs) to act as theories of natural language. LLM performance in some language tasks presents both quantitative and qualitative differences from that of humans, however it remains to be determined whether such differences are amenable to model size. This work investigates the critical role of model scaling, determining whether increases in size make up for such differences between humans and models. We test three LLMs from different families (Bard, 137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a grammaticality judgment task featuring anaphora, center embedding, comparatives, and negative polarity. N=1,200 judgments are collected and scored for accuracy, stability, and improvements in accuracy upon repeated presentation of a prompt. Results of the best performing LLM, ChatGPT-4, are compared to results of n=80 humans on the same stimuli. We find that increased model size may lead to better performance, but LLMs are still not sensitive to (un)grammaticality as humans are. It seems possible but unlikely that scaling alone can fix this issue. We interpret these results by comparing language learning in vivo and in silico, identifying three critical differences concerning (i) the type of evidence, (ii) the poverty of the stimulus, and (iii) the occurrence of semantic hallucinations due to impenetrable linguistic reference.</li>
<li><strong>摘要：</strong>了解语言的局限性是大型语言模型（LLM）充当自然语言理论的先决条件。 LLM 在某些语言任务中的表现与人类的表现在数量和质量上都存在差异，但这种差异是否会受到模型大小的影响仍有待确定。这项工作研究了模型缩放的关键作用，确定尺寸的增加是否可以弥补人类和模型之间的差异。我们在语法判断任务上测试了来自不同家族的三名法学硕士（Bard，1370 亿个参数；ChatGPT-3.5，1750 亿个参数；ChatGPT-4，1.5 万亿个参数）的语法判断任务，包括照应、中心嵌入、比较和负极性。收集了 N=1,200 个判断，并对准确性、稳定性以及重复提示时准确性的提高进行评分。将表现最佳的 LLM ChatGPT-4 的结果与 n=80 人在相同刺激下的结果进行比较。我们发现增加模型大小可能会带来更好的性能，但法学硕士仍然不像人类那样对（非）语法敏感。仅靠扩展似乎有可能但不太可能解决这个问题。我们通过比较体内和计算机中的语言学习来解释这些结果，确定三个关键差异，涉及（i）证据类型，（ii）刺激的贫乏，以及（iii）由于难以理解的语言参考而出现语义幻觉。</li>
</ul>

<h3>Title: Beyond the Speculative Game: A Survey of Speculative Execution in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhang, Zhuorui Liu, Dawei Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14897">https://arxiv.org/abs/2404.14897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14897">https://arxiv.org/pdf/2404.14897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14897]] Beyond the Speculative Game: A Survey of Speculative Execution in Large  Language Models(https://arxiv.org/abs/2404.14897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>With the increasingly giant scales of (causal) large language models (LLMs), the inference efficiency comes as one of the core concerns along the improved performance. In contrast to the memory footprint, the latency bottleneck seems to be of greater importance as there can be billions of requests to a LLM (e.g., GPT-4) per day. The bottleneck is mainly due to the autoregressive innateness of LLMs, where tokens can only be generated sequentially during decoding. To alleviate the bottleneck, the idea of speculative execution, which originates from the field of computer architecture, is introduced to LLM decoding in a \textit{draft-then-verify} style. Under this regime, a sequence of tokens will be drafted in a fast pace by utilizing some heuristics, and then the tokens shall be verified in parallel by the LLM. As the costly sequential inference is parallelized, LLM decoding speed can be significantly boosted. Driven by the success of LLMs in recent couple of years, a growing literature in this direction has emerged. Yet, there lacks a position survey to summarize the current landscape and draw a roadmap for future development of this promising area. To meet this demand, we present the very first survey paper that reviews and unifies literature of speculative execution in LLMs (e.g., blockwise parallel decoding, speculative decoding, etc.) in a comprehensive framework and a systematic taxonomy. Based on the taxonomy, we present a critical review and comparative analysis of the current arts. Finally we highlight various key challenges and future directions to further develop the area.</li>
<li><strong>摘要：</strong>随着（因果）大型语言模型（LLM）的规模越来越大，推理效率成为提高性能的核心问题之一。与内存占用相比，延迟瓶颈似乎更为重要，因为每天可能有数十亿个 LLM（例如 GPT-4）请求。瓶颈主要是由于 LLM 固有的自回归特性，其中令牌只能在解码过程中顺序生成。为了缓解瓶颈，源自计算机体系结构领域的推测执行的思想被引入到 LLM 解码中，采用 \textit{draft-then-verify} 风格。在这种制度下，将利用一些启发式方法快速起草一系列令牌，然后由法学硕士并行验证这些令牌。由于成本高昂的顺序推理被并行化，LLM 解码速度可以显着提高。在近几年法学硕士成功的推动下，这一方向的文献不断涌现。然而，目前还缺乏一个定位调查来总结这一前景广阔的领域的当前形势并绘制未来发展路线图。为了满足这一需求，我们提出了第一篇调查论文，以全面的框架和系统的分类法回顾和统一了法学硕士推测执行的文献（例如，块式并行解码、推测解码等）。基于分类法，我们对当前艺术进行批判性回顾和比较分析。最后，我们强调了进一步发展该领域的各种关键挑战和未来方向。</li>
</ul>

<h3>Title: Pillars of Grammatical Error Correction: Comprehensive Inspection Of  Contemporary Approaches In The Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kostiantyn Omelianchuk, Andrii Liubonko, Oleksandr Skurzhanskyi, Artem Chernodub, Oleksandr Korniienko, Igor Samokhin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14914">https://arxiv.org/abs/2404.14914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14914">https://arxiv.org/pdf/2404.14914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14914]] Pillars of Grammatical Error Correction: Comprehensive Inspection Of  Contemporary Approaches In The Era of Large Language Models(https://arxiv.org/abs/2404.14914)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we carry out experimental research on Grammatical Error Correction, delving into the nuances of single-model systems, comparing the efficiency of ensembling and ranking methods, and exploring the application of large language models to GEC as single-model systems, as parts of ensembles, and as ranking methods. We set new state-of-the-art performance with F_0.5 scores of 72.8 on CoNLL-2014-test and 81.4 on BEA-test, respectively. To support further advancements in GEC and ensure the reproducibility of our research, we make our code, trained models, and systems' outputs publicly available.</li>
<li><strong>摘要：</strong>在本文中，我们开展语法错误纠正的实验研究，深入研究单模型系统的细微差别，比较集成和排序方法的效率，并探索大型语言模型作为单模型系统在 GEC 中的应用，如系综的一部分，以及作为排名方法。我们设定了新的最先进性能，在 CoNLL-2014 测试上的 F_0.5 分数分别为 72.8，在 BEA 测试上的 F_0.5 分数分别为 81.4。为了支持 GEC 的进一步发展并确保我们研究的可重复性，我们公开了我们的代码、经过训练的模型和系统的输出。</li>
</ul>

<h3>Title: Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs  Perfect Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14963">https://arxiv.org/abs/2404.14963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14963">https://arxiv.org/pdf/2404.14963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14963]] Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs  Perfect Reasoners(https://arxiv.org/abs/2404.14963)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Chain of Thought prompting strategy has enhanced the performance of Large Language Models (LLMs) across various NLP tasks. However, it still has shortcomings when dealing with complex reasoning tasks, following~\citet{cot_wei}, including understanding errors, calculation errors and process errors (e.g. missing-step and hallucinations). Subsequently, Our in-depth analysis of various error types has found that deeply understanding the whole problem is critical in addressing complicated reasoning tasks. In this paper, we proposed a novel prompt strategy called Deeply Understanding the Problems (DUP) prompting, inspired by how humans solve complex reasoning problems, designed to enhance the comprehensive understanding of problems by LLMs. It consists of three stages: 1) extract the core question; 2) find out problem-solving information based on the core question; 3) generate and extract answers by LLMs. We evaluate the performance of DUP prompting on ten diverse reasoning datasets. Experimental results suggest that DUP prompting significantly outperforms Zero-Shot CoT ~\cite{kojima2022large} across all datasets. Notably, DUP achieves \textbf{state-of-the-art on SVAMP (90.4\% to 94.2\%) and GSM8K (94.6\% to 97.1\%).}</li>
<li><strong>摘要：</strong>思想链提示策略提高了大型语言模型 (LLM) 在各种 NLP 任务中的性能。然而，它在处理复杂推理任务时仍然存在缺陷，如下~\citet{cot_wei}，包括理解错误、计算错误和过程错误（例如缺步和幻觉）。随后，我们对各种错误类型的深入分析发现，深入理解整个问题对于解决复杂的推理任务至关重要。在本文中，我们提出了一种名为“深度理解问题（DUP）提示”的新颖提示策略，其灵感来自于人类如何解决复杂的推理问题，旨在增强法学硕士对问题的全面理解。它包括三个阶段：1）提取核心问题； 2）根据核心问题找出解题信息； 3) 由法学硕士生成并提取答案。我们评估了 DUP 提示在十个不同推理数据集上的性能。实验结果表明，在所有数据集上，DUP 提示显着优于零样本 CoT ~\cite{kojima2022large}。值得注意的是，DUP 在 SVAMP（90.4\% 到 94.2\%）和 GSM8K（94.6\% 到 97.1\%）上实现了最先进的技术。}</li>
</ul>

<h3>Title: Transformers Can Represent $n$-gram Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anej Svete, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CC, cs.FL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14994">https://arxiv.org/abs/2404.14994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14994">https://arxiv.org/pdf/2404.14994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14994]] Transformers Can Represent $n$-gram Language Models(https://arxiv.org/abs/2404.14994)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Plenty of existing work has analyzed the abilities of the transformer architecture by describing its representational capacity with formal models of computation. However, the focus so far has been on analyzing the architecture in terms of language \emph{acceptance}. We contend that this is an ill-suited problem in the study of \emph{language models} (LMs), which are definitionally \emph{probability distributions} over strings. In this paper, we focus on the relationship between transformer LMs and $n$-gram LMs, a simple and historically relevant class of language models. We show that transformer LMs using the hard or sparse attention mechanisms can exactly represent any $n$-gram LM, giving us a concrete lower bound on their probabilistic representational capacity. This provides a first step towards understanding the mechanisms that transformer LMs can use to represent probability distributions over strings.</li>
<li><strong>摘要：</strong>大量现有工作通过用形式计算模型描述变压器架构的表示能力来分析变压器架构的能力。然而，到目前为止，重点是根据语言 \emph{acceptance} 来分析架构。我们认为，这是一个不适合研究 \emph{语言模型} (LM) 的问题，LM 在定义上是字符串上的 \emph{概率分布}。在本文中，我们重点关注 Transformer LM 和 $n$-gram LM 之间的关系，$n$-gram LM 是一类简单且与历史相关的语言模型。我们证明，使用硬注意力或稀疏注意力机制的 Transformer LM 可以准确地表示任何 $n$-gram LM，从而为我们提供了其概率表示能力的具体下限。这为理解 Transformer LM 可用来表示字符串概率分布的机制迈出了第一步。</li>
</ul>

<h3>Title: TAXI: Evaluating Categorical Knowledge Editing for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Derek Powell, Walter Gerych, Thomas Hartvigsen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15004">https://arxiv.org/abs/2404.15004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15004">https://arxiv.org/pdf/2404.15004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15004]] TAXI: Evaluating Categorical Knowledge Editing for Language Models(https://arxiv.org/abs/2404.15004)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Humans rarely learn one fact in isolation. Instead, learning a new fact induces knowledge of other facts about the world. For example, in learning a korat is a type of cat, you also infer it is a mammal and has claws, ensuring your model of the world is consistent. Knowledge editing aims to inject new facts into language models to improve their factuality, but current benchmarks fail to evaluate consistency, which is critical to ensure efficient, accurate, and generalizable edits. We manually create TAXI, a new benchmark dataset specifically created to evaluate consistency. TAXI contains 11,120 multiple-choice queries for 976 edits spanning 41 categories (e.g., Dogs), 164 subjects (e.g., Labrador), and 183 properties (e.g., is a mammal). We then use TAXI to evaluate popular editors' consistency, measuring how often editing a subject's category appropriately edits its properties. We find that 1) the editors achieve marginal, yet non-random consistency, 2) their consistency far underperforms human baselines, and 3) consistency is more achievable when editing atypical subjects. Our code and data are available at https://github.com/derekpowell/taxi.</li>
<li><strong>摘要：</strong>人类很少孤立地了解一个事实。相反，学习一个新事实会引发对世界其他事实的了解。例如，在学习呵叻猫是猫的一种时，您还可以推断它是哺乳动物并且有爪子，从而确保您的世界模型是一致的。知识编辑旨在将新事实注入语言模型以提高其真实性，但当前的基准无法评估一致性，而一致性对于确保高效、准确和可概括的编辑至关重要。我们手动创建 TAXI，这是一个专门为评估一致性而创建的新基准数据集。 TAXI 包含 11,120 个多项选择查询，涉及 41 个类别（例如，狗）、164 个主题（例如，拉布拉多）和 183 个属性（例如，是哺乳动物）的 976 项编辑。然后，我们使用 TAXI 来评估流行编辑器的一致性，测量编辑主题类别适当编辑其属性的频率。我们发现：1）编辑人员实现了边际但非随机的一致性，2）他们的一致性远远低于人类基线，3）在编辑非典型主题时更容易实现一致性。我们的代码和数据可在 https://github.com/derekpowell/taxi 获取。</li>
</ul>

<h3>Title: Multi-Head Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Xun Wu, Shaohan Huang, Wenhui Wang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15045">https://arxiv.org/abs/2404.15045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15045">https://arxiv.org/pdf/2404.15045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15045]] Multi-Head Mixture-of-Experts(https://arxiv.org/abs/2404.15045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in training and inference costs, but exhibits the following two issues: (1) Low expert activation, where only a small subset of experts are activated for optimization. (2) Lacking fine-grained analytical capabilities for multiple semantic concepts within individual tokens. We propose Multi-Head Mixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each token into multiple sub-tokens. These sub-tokens are then assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form. The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting. Moreover, our MH-MoE is straightforward to implement and decouples from other SMoE optimization methods, making it easy to integrate with other SMoE models for enhanced performance. Extensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE.</li>
<li><strong>摘要：</strong>稀疏专家混合 (SMoE) 可以在不显着增加训练和推理成本的情况下扩展模型容量，但存在以下两个问题：(1) 专家激活率低，仅激活一小部分专家进行优化。 (2)缺乏对单个token内多个语义概念的细粒度分析能力。我们提出多头专家混合（MH-MoE），它采用多头机制将每个令牌拆分为多个子令牌。然后，这些子代币被分配给一组不同的专家并行处理，并无缝地重新集成到原始代币形式中。多头机制使模型能够共同关注来自不同专家内的各种表示空间的信息，同时显着增强专家激活，从而加深上下文理解并缓解过度拟合。此外，我们的 MH-MoE 易于实现，并且与其他 SMoE 优化方法解耦，从而可以轻松与其他 SMoE 模型集成以增强性能。跨三个任务的广泛实验结果：以英语为中心的语言建模、多语言语言建模和 Masked 多模态建模任务，证明了 MH-MoE 的有效性。</li>
</ul>

<h3>Title: Identifying Fairness Issues in Automatically Generated Testing Content</h3>
<ul>
<li><strong>Authors: </strong>Kevin Stowe, Benny Longwill, Alyssa Francis, Tatsuya Aoyama, Debanjan Ghosh, Swapna Somasundaran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15104">https://arxiv.org/abs/2404.15104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15104">https://arxiv.org/pdf/2404.15104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15104]] Identifying Fairness Issues in Automatically Generated Testing Content(https://arxiv.org/abs/2404.15104)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Natural language generation tools are powerful and effective for generating content. However, language models are known to display bias and fairness issues, making them impractical to deploy for many use cases. We here focus on how fairness issues impact automatically generated test content, which can have stringent requirements to ensure the test measures only what it was intended to measure. Specifically, we identify test content that is focused on particular domains and experiences that only reflect a certain demographic or that are potentially emotionally upsetting; both of which could inadvertently impact a test-taker's score. This kind of content doesn't reflect typical biases out of context, making it challenging even for modern models that contain safeguards. We build a dataset of 621 generated texts annotated for fairness and explore a variety of methods for classification: fine-tuning, topic-based classification, and prompting, including few-shot and self-correcting prompts. We find that combining prompt self-correction and few-shot learning performs best, yielding an F1 score of .791 on our held-out test set, while much smaller BERT- and topic-based models have competitive performance on out-of-domain data.</li>
<li><strong>摘要：</strong>自然语言生成工具对于生成内容来说是强大而有效的。然而，众所周知，语言模型存在偏见和公平问题，这使得它们在许多用例中部署起来不切实际。我们在这里关注公平问题如何影响自动生成的测试内容，这些内容可能有严格的要求，以确保测试仅测量其预期测量的内容。具体来说，我们确定专注于特定领域和体验的测试内容，这些领域和体验仅反映特定人群或可能令人情绪不安；这两者都可能会无意中影响考生的分数。此类内容不会反映断章取义的典型偏见，因此即使对于包含保障措施的现代模型来说也具有挑战性。我们构建了一个由 621 个生成的文本组成的数据集，并进行了公平性注释，并探索了各种分类方法：微调、基于主题的分类和提示，包括少量提示和自我纠正提示。我们发现，将即时自我修正和小样本学习相结合表现最好，在我们的测试集上产生 0.791 的 F1 分数，而更小的 BERT 和基于主题的模型在域外具有竞争性的性能数据。</li>
</ul>

<h3>Title: Bias patterns in the application of LLMs for clinical decision support:  A comprehensive study</h3>
<ul>
<li><strong>Authors: </strong>Raphael Poulain, Hamed Fayyaz, Rahmatollah Beheshti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15149">https://arxiv.org/abs/2404.15149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15149">https://arxiv.org/pdf/2404.15149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15149]] Bias patterns in the application of LLMs for clinical decision support:  A comprehensive study(https://arxiv.org/abs/2404.15149)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful candidates to inform clinical decision-making processes. While these models play an increasingly prominent role in shaping the digital landscape, two growing concerns emerge in healthcare applications: 1) to what extent do LLMs exhibit social bias based on patients' protected attributes (like race), and 2) how do design choices (like architecture design and prompting strategies) influence the observed biases? To answer these questions rigorously, we evaluated eight popular LLMs across three question-answering (QA) datasets using clinical vignettes (patient descriptions) standardized for bias evaluations. We employ red-teaming strategies to analyze how demographics affect LLM outputs, comparing both general-purpose and clinically-trained models. Our extensive experiments reveal various disparities (some significant) across protected groups. We also observe several counter-intuitive patterns such as larger models not being necessarily less biased and fined-tuned models on medical data not being necessarily better than the general-purpose models. Furthermore, our study demonstrates the impact of prompt design on bias patterns and shows that specific phrasing can influence bias patterns and reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively. Consistent with prior studies, we call on additional evaluations, scrutiny, and enhancement of LLMs used in clinical decision support applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为为临床决策过程提供信息的强大候选者。虽然这些模型在塑造数字景观方面发挥着越来越重要的作用，但医疗保健应用中出现了两个日益突出的问题：1）法学硕士在多大程度上表现出基于患者受保护属性（如种族）的社会偏见，以及 2）设计选择如何（如架构设计和提示策略）会影响观察到的偏差吗？为了严格回答这些问题，我们使用偏倚评估标准化的临床小插图（患者描述）在三个问答（QA）数据集中评估了八个流行的法学硕士。我们采用红队策略来分析人口统计如何影响法学硕士输出，比较通用模型和临床训练模型。我们广泛的实验揭示了受保护群体之间的各种差异（有些是显着的）。我们还观察到一些反直觉的模式，例如较大的模型不一定偏差较小，针对医疗数据的微调模型不一定比通用模型更好。此外，我们的研究证明了提示设计对偏见模式的影响，并表明特定的措辞可以影响偏见模式，而反思型方法（如思想链）可以有效减少偏见结果。与之前的研究一致，我们呼吁对临床决策支持应用中使用的法学硕士进行额外的评估、审查和增强。</li>
</ul>

<h3>Title: Expert Router: Orchestrating Efficient Language Model Inference through  Prompt Classification</h3>
<ul>
<li><strong>Authors: </strong>Josef Pichlmeier, Philipp Ross, Andre Luckow</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15153">https://arxiv.org/abs/2404.15153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15153">https://arxiv.org/pdf/2404.15153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15153]] Expert Router: Orchestrating Efficient Language Model Inference through  Prompt Classification(https://arxiv.org/abs/2404.15153)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have experienced widespread adoption across scientific and industrial domains due to their versatility and utility for diverse tasks. Nevertheless, deploying and serving these models at scale with optimal throughput and latency remains a significant challenge, primarily because of the high computational and memory demands associated with LLMs. To tackle this limitation, we introduce Expert Router, a system designed to orchestrate multiple expert models efficiently, thereby enhancing scalability. Expert Router is a parallel inference system with a central routing gateway that distributes incoming requests using a clustering method. This approach effectively partitions incoming requests among available LLMs, maximizing overall throughput. Our extensive evaluations encompassed up to 1,000 concurrent users, providing comprehensive insights into the system's behavior from user and infrastructure perspectives. The results demonstrate Expert Router's effectiveness in handling high-load scenarios and achieving higher throughput rates, particularly under many concurrent users.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 由于其多功能性和针对不同任务的实用性而在科学和工业领域得到了广泛采用。然而，以最佳吞吐量和延迟大规模部署和服务这些模型仍然是一个重大挑战，主要是因为与法学硕士相关的高计算和内存需求。为了解决这个限制，我们引入了专家路由器，这是一个旨在有效协调多个专家模型的系统，从而增强可扩展性。 Expert Router 是一个并行推理系统，具有中央路由网关，可使用集群方法分发传入请求。这种方法可以有效地在可用的 LLM 之间划分传入请求，从而最大限度地提高总体吞吐量。我们的广泛评估涵盖了多达 1,000 个并发用户，从用户和基础设施的角度提供了对系统行为的全面见解。结果证明了专家路由器在处理高负载场景和实现更高吞吐量方面的有效性，特别是在许多并发用户的情况下。</li>
</ul>

<h3>Title: Do not think pink elephant!</h3>
<ul>
<li><strong>Authors: </strong>Kyomin Hwang, Suyoung Kim, JunHoo Lee, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15154">https://arxiv.org/abs/2404.15154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15154">https://arxiv.org/pdf/2404.15154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15154]] Do not think pink elephant!(https://arxiv.org/abs/2404.15154)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Large Models (LMs) have heightened expectations for the potential of general AI as they are akin to human intelligence. This paper shows that recent large models such as Stable Diffusion and DALL-E3 also share the vulnerability of human intelligence, namely the "white bear phenomenon". We investigate the causes of the white bear phenomenon by analyzing their representation space. Based on this analysis, we propose a simple prompt-based attack method, which generates figures prohibited by the LM provider's policy. To counter these attacks, we introduce prompt-based defense strategies inspired by cognitive therapy techniques, successfully mitigating attacks by up to 48.22\%.</li>
<li><strong>摘要：</strong>大型模型 (LM) 提高了人们对通用人工智能潜力的期望，因为它们类似于人类智能。这篇论文表明，最近的大型模型如稳定扩散和DALL-E3也具有人类智能的脆弱性，即“白熊现象”。我们通过分析白熊现象的表征空间来调查白熊现象的原因。基于此分析，我们提出了一种简单的基于提示的攻击方法，该方法生成 LM 提供商策略禁止的数字。为了应对这些攻击，我们引入了受认知治疗技术启发的基于提示的防御策略，成功地将攻击减轻了高达 48.22%。</li>
</ul>

<h3>Title: Adaptive Collaboration Strategy for LLMs in Medical Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, Hae Won Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15155">https://arxiv.org/abs/2404.15155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15155">https://arxiv.org/pdf/2404.15155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15155]] Adaptive Collaboration Strategy for LLMs in Medical Decision Making(https://arxiv.org/abs/2404.15155)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Foundation models have become invaluable in advancing the medical field. Despite their promise, the strategic deployment of LLMs for effective utility in complex medical tasks remains an open question. Our novel framework, Medical Decision-making Agents (MDAgents) aims to address this gap by automatically assigning the effective collaboration structure for LLMs. Assigned solo or group collaboration structure is tailored to the complexity of the medical task at hand, emulating real-world medical decision making processes. We evaluate our framework and baseline methods with state-of-the-art LLMs across a suite of challenging medical benchmarks: MedQA, MedMCQA, PubMedQA, DDXPlus, PMC-VQA, Path-VQA, and MedVidQA, achieving the best performance in 5 out of 7 benchmarks that require an understanding of multi-modal medical reasoning. Ablation studies reveal that MDAgents excels in adapting the number of collaborating agents to optimize efficiency and accuracy, showcasing its robustness in diverse scenarios. We also explore the dynamics of group consensus, offering insights into how collaborative agents could behave in complex clinical team dynamics. Our code can be found at https://github.com/mitmedialab/MDAgents.</li>
<li><strong>摘要：</strong>基础模型对于推动医学领域的发展具有无价的价值。尽管他们做出了承诺，但如何战略部署法学硕士以在复杂的医疗任务中有效利用仍然是一个悬而未决的问题。我们的新颖框架医疗决策代理 (MDAgents) 旨在通过自动为法学硕士分配有效的协作结构来解决这一差距。分配的单独或小组协作结构是根据当前医疗任务的复杂性量身定制的，模拟现实世界的医疗决策过程。我们使用最先进的法学硕士在一系列具有挑战性的医学基准中评估我们的框架和基线方法：MedQA、MedMCQA、PubMedQA、DDXPlus、PMC-VQA、Path-VQA 和 MedVidQA，在 5 个选项中实现最佳性能需要了解多模式医学推理的 7 个基准。消融研究表明，MDAgents 擅长调整协作代理的数量以优化效率和准确性，展示其在不同场景中的稳健性。我们还探索了群体共识的动态，提供了关于协作代理如何在复杂的临床团队动态中表现的见解。我们的代码可以在 https://github.com/mitmedialab/MDAgents 找到。</li>
</ul>

<h3>Title: Regressive Side Effects of Training Language Models to Mimic Student  Misconceptions</h3>
<ul>
<li><strong>Authors: </strong>Shashank Sonkar, Naiming Liu, Richard G. Baraniuk</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15156">https://arxiv.org/abs/2404.15156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15156">https://arxiv.org/pdf/2404.15156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15156]] Regressive Side Effects of Training Language Models to Mimic Student  Misconceptions(https://arxiv.org/abs/2404.15156)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>This paper presents a novel exploration into the regressive side effects of training Large Language Models (LLMs) to mimic student misconceptions for personalized education. We highlight the problem that as LLMs are trained to more accurately mimic student misconceptions, there is a compromise in the factual integrity and reasoning ability of the models. Our work involved training an LLM on a student-tutor dialogue dataset to predict student responses. The results demonstrated a decrease in the model's performance across multiple benchmark datasets, including the ARC reasoning challenge and TruthfulQA, which evaluates the truthfulness of model's generated responses. Furthermore, the HaluEval Dial dataset, used for hallucination detection, and MemoTrap, a memory-based task dataset, also reported a decline in the model accuracy. To combat these side effects, we introduced a "hallucination token" technique. This token, appended at the beginning of each student response during training, instructs the model to switch between mimicking student misconceptions and providing factually accurate responses. Despite the significant improvement across all datasets, the technique does not completely restore the LLM's baseline performance, indicating the need for further research in this area. This paper contributes to the ongoing discussion on the use of LLMs for student modeling, emphasizing the need for a balance between personalized education and factual accuracy.</li>
<li><strong>摘要：</strong>本文对训练大型语言模型 (LLM) 来模仿学生对个性化教育的误解的回归副作用进行了新颖的探索。我们强调的问题是，当法学硕士接受训练以更准确地模仿学生的误解时，模型的事实完整性和推理能力会受到损害。我们的工作涉及在学生与导师对话数据集上训练法学硕士以预测学生的反应。结果表明，模型在多个基准数据集上的性能有所下降，包括 ARC 推理挑战和 TruthfulQA（评估模型生成的响应的真实性）。此外，用于幻觉检测的 HaluEval Dial 数据集和基于内存的任务数据集 MemoTrap 也报告了模型准确性的下降。为了对抗这些副作用，我们引入了“幻觉令牌”技术。该标记附加在训练期间每个学生响应的开头，指示模型在模仿学生的误解和提供事实上准确的响应之间切换。尽管所有数据集都有显着改进，但该技术并没有完全恢复法学硕士的基线性能，表明需要在该领域进行进一步的研究。本文有助于正在进行的关于使用法学硕士进行学生建模的讨论，强调个性化教育和事实准确性之间需要平衡。</li>
</ul>

<h3>Title: FASTTRACK: Fast and Accurate Fact Tracing for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Si Chen, Feiyang Kang, Ning Yu, Ruoxi Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15157">https://arxiv.org/abs/2404.15157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15157">https://arxiv.org/pdf/2404.15157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15157]] FASTTRACK: Fast and Accurate Fact Tracing for LLMs(https://arxiv.org/abs/2404.15157)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fact tracing seeks to identify specific training examples that serve as the knowledge source for a given query. Existing approaches to fact tracing rely on assessing the similarity between each training sample and the query along a certain dimension, such as lexical similarity, gradient, or embedding space. However, these methods fall short of effectively distinguishing between samples that are merely relevant and those that actually provide supportive evidence for the information sought by the query. This limitation often results in suboptimal effectiveness. Moreover, these approaches necessitate the examination of the similarity of individual training points for each query, imposing significant computational demands and creating a substantial barrier for practical applications. This paper introduces FASTTRACK, a novel approach that harnesses the capabilities of Large Language Models (LLMs) to validate supportive evidence for queries and at the same time clusters the training database towards a reduced extent for LLMs to trace facts. Our experiments show that FASTTRACK substantially outperforms existing methods in both accuracy and efficiency, achieving more than 100\% improvement in F1 score over the state-of-the-art methods while being X33 faster than \texttt{TracIn}.</li>
<li><strong>摘要：</strong>事实追踪旨在识别作为给定查询的知识源的特定训练示例。现有的事实追踪方法依赖于评估每个训练样本和查询之间在某个维度上的相似性，例如词汇相似性、梯度或嵌入空间。然而，这些方法无法有效区分仅相关的样本和实际上为查询所寻求的信息提供支持证据的样本。这种限制通常会导致效果不佳。此外，这些方法需要检查每个查询的各个训练点的相似性，从而提出了大量的计算要求，并为实际应用造成了巨大的障碍。本文介绍了 FASTTRACK，这是一种新颖的方法，它利用大型语言模型 (LLM) 的功能来验证查询的支持证据，同时对训练数据库进行聚类，以减少 LLM 跟踪事实的范围。我们的实验表明，FASTTRACK 在准确性和效率方面都大大优于现有方法，与最先进的方法相比，F1 分数提高了 100% 以上，同时比 \texttt{TracIn} 快 33 倍。</li>
</ul>

<h3>Title: MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based  Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan Cheng, Lei Duan, Jie Zuo, Cal Yang, Mingjie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15159">https://arxiv.org/abs/2404.15159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15159">https://arxiv.org/pdf/2404.15159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15159]] MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based  Mixture of Experts(https://arxiv.org/abs/2404.15159)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have showcased exceptional performance across a wide array of Natural Language Processing (NLP) tasks. Fine-tuning techniques are commonly utilized to tailor pre-trained models to specific applications. While methods like LoRA have effectively tackled GPU memory constraints during fine-tuning, their applicability is often restricted to limited performance, especially on multi-task. On the other hand, Mix-of-Expert (MoE) models, such as Mixtral 8x7B, demonstrate remarkable performance across multiple NLP tasks while maintaining a reduced parameter count. However, the resource requirements of these MoEs still challenging, particularly for consumer-grade GPUs only have limited VRAM. To address these challenge, we propose MixLoRA, an innovative approach aimed at constructing a resource-efficient sparse MoE model based on LoRA. MixLoRA inserts multiple LoRA-based experts within the feed-forward network block of a frozen pre-trained dense model through fine-tuning, employing a commonly used top-k router. Unlike other LoRA based MoE methods, MixLoRA enhances model performance by utilizing independently configurable attention-layer LoRA adapters, supporting the use of LoRA and its variants for the construction of experts, and applying auxiliary load balance loss to address the imbalance problem of the router. In experiments, MixLoRA achieves commendable performance across all evaluation metrics in both single-task and multi-task learning scenarios. Implemented within the m-LoRA framework, MixLoRA enables parallel fine-tuning of multiple mixture-of-experts models on a single 24GB consumer-grade GPU without quantization, thereby reducing GPU memory consumption by 41\% and latency during the training process by 17\%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中展示了卓越的性能。微调技术通常用于根据特定应用定制预训练模型。虽然像 LoRA 这样的方法可以有效地解决微调过程中的 GPU 内存限制，但它们的适用性通常仅限于有限的性能，尤其是在多任务上。另一方面，Mixtral 8x7B 等专家混合 (MoE) 模型在多个 NLP 任务中表现出卓越的性能，同时保持了较少的参数数量。然而，这些 MoE 的资源需求仍然具有挑战性，特别是对于只有有限 VRAM 的消费级 GPU。为了应对这些挑战，我们提出了 MixLoRA，这是一种创新方法，旨在基于 LoRA 构建资源高效的稀疏 MoE 模型。 MixLoRA 通过微调，采用常用的 top-k 路由器，在冻结的预训练密集模型的前馈网络块中插入多个基于 LoRA 的专家。与其他基于LoRA的MoE方法不同，MixLoRA通过利用可独立配置的注意力层LoRA适配器来增强模型性能，支持使用LoRA及其变体来构建专家，并应用辅助负载平衡损失来解决路由器的不平衡问题。在实验中，MixLoRA 在单任务和多任务学习场景中的所有评估指标上都取得了值得称赞的性能。 MixLoRA 在 m-LoRA 框架内实现，可在单个 24GB 消费级 GPU 上并行微调多个专家混合模型，无需量化，从而将 GPU 内存消耗减少 41%，并将训练过程中的延迟减少 17% \％。</li>
</ul>

<h3>Title: Pixels and Predictions: Potential of GPT-4V in Meteorological Imagery  Analysis and Forecast Communication</h3>
<ul>
<li><strong>Authors: </strong>John R. Lawson, Montgomery L. Flora, Kevin H. Goebbert, Seth N. Lyman, Corey K. Potvin, David M. Schultz, Adam J. Stepanek, Joseph E. Trujillo-Falcón</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15166">https://arxiv.org/abs/2404.15166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15166">https://arxiv.org/pdf/2404.15166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15166]] Pixels and Predictions: Potential of GPT-4V in Meteorological Imagery  Analysis and Forecast Communication(https://arxiv.org/abs/2404.15166)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Generative AI, such as OpenAI's GPT-4V large-language model, has rapidly entered mainstream discourse. Novel capabilities in image processing and natural-language communication may augment existing forecasting methods. Large language models further display potential to better communicate weather hazards in a style honed for diverse communities and different languages. This study evaluates GPT-4V's ability to interpret meteorological charts and communicate weather hazards appropriately to the user, despite challenges of hallucinations, where generative AI delivers coherent, confident, but incorrect responses. We assess GPT-4V's competence via its web interface ChatGPT in two tasks: (1) generating a severe-weather outlook from weather-chart analysis and conducting self-evaluation, revealing an outlook that corresponds well with a Storm Prediction Center human-issued forecast; and (2) producing hazard summaries in Spanish and English from weather charts. Responses in Spanish, however, resemble direct (not idiomatic) translations from English to Spanish, yielding poorly translated summaries that lose critical idiomatic precision required for optimal communication. Our findings advocate for cautious integration of tools like GPT-4V in meteorology, underscoring the necessity of human oversight and development of trustworthy, explainable AI.</li>
<li><strong>摘要：</strong>生成式人工智能，例如 OpenAI 的 GPT-4V 大语言模型，已迅速进入主流话语。图像处理和自然语言通信的新功能可能会增强现有的预测方法。大型语言模型进一步显示出以针对不同社区和不同语言的风格更好地传达天气灾害的潜力。这项研究评估了 GPT-4V 解释气象图并向用户适当传达天气危害的能力，尽管存在幻觉的挑战，其中生成式人工智能提供连贯、自信但不正确的响应。我们通过其网络界面 ChatGPT 评估 GPT-4V 在两项任务中的能力：(1) 通过天气图分析生成恶劣天气展望并进行自我评估，揭示与风暴预报中心人工发布的预报相符的展望; (2) 根据天气图生成西班牙语和英语的危险摘要。然而，西班牙语的回复类似于从英语到西班牙语的直接（非惯用）翻译，产生翻译不佳的摘要，失去了最佳沟通所需的关键惯用精确度。我们的研究结果主张在气象学中谨慎整合 GPT-4V 等工具，强调人类监督和开发值得信赖、可解释的人工智能的必要性。</li>
</ul>

<h3>Title: Setting up the Data Printer with Improved English to Ukrainian Machine  Translation</h3>
<ul>
<li><strong>Authors: </strong>Yurii Paniv, Dmytro Chaplynskyi, Nikita Trynus, Volodymyr Kyrylov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15196">https://arxiv.org/abs/2404.15196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15196">https://arxiv.org/pdf/2404.15196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15196]] Setting up the Data Printer with Improved English to Ukrainian Machine  Translation(https://arxiv.org/abs/2404.15196)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>To build large language models for Ukrainian we need to expand our corpora with large amounts of new algorithmic tasks expressed in natural language. Examples of task performance expressed in English are abundant, so with a high-quality translation system our community will be enabled to curate datasets faster. To aid this goal, we introduce a recipe to build a translation system using supervised finetuning of a large pretrained language model with a noisy parallel dataset of 3M pairs of Ukrainian and English sentences followed by a second phase of training using 17K examples selected by k-fold perplexity filtering on another dataset of higher quality. Our decoder-only model named Dragoman beats performance of previous state of the art encoder-decoder models on the FLORES devtest set.</li>
<li><strong>摘要：</strong>为了构建乌克兰语的大型语言模型，我们需要通过大量用自然语言表达的新算法任务来扩展我们的语料库。用英语表达的任务表现的例子非常丰富，因此有了高质量的翻译系统，我们的社区将能够更快地整理数据集。为了帮助实现这一目标，我们引入了一种方法来构建翻译系统，该方法使用大型预训练语言模型的监督微调，该模型具有 300 万对乌克兰语和英语句子的嘈杂并行数据集，然后使用 k- 选择的 17K 个示例进行第二阶段的训练在另一个更高质量的数据集上折叠困惑过滤。我们名为 Dragoman 的纯解码器模型在 FLORES 开发测试集上击败了先前最先进的编码器-解码器模型的性能。</li>
</ul>

<h3>Title: Does Instruction Tuning Make LLMs More Consistent?</h3>
<ul>
<li><strong>Authors: </strong>Constanza Fierro, Jiaang Li, Anders Søgaard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15206">https://arxiv.org/abs/2404.15206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15206">https://arxiv.org/pdf/2404.15206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15206]] Does Instruction Tuning Make LLMs More Consistent?(https://arxiv.org/abs/2404.15206)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The purpose of instruction tuning is enabling zero-shot performance, but instruction tuning has also been shown to improve chain-of-thought reasoning and value alignment (Si et al., 2023). Here we consider the impact on $\textit{consistency}$, i.e., the sensitivity of language models to small perturbations in the input. We compare 10 instruction-tuned LLaMA models to the original LLaMA-7b model and show that almost across-the-board they become more consistent, both in terms of their representations and their predictions in zero-shot and downstream tasks. We explain these improvements through mechanistic analyses of factual recall.</li>
<li><strong>摘要：</strong>指令调优的目的是实现零样本性能，但指令调优也被证明可以改善思想链推理和值对齐（Si 等人，2023）。这里我们考虑对 $\textit{consistency}$ 的影响，即语言模型对输入中的小扰动的敏感性。我们将 10 个指令调整的 LLaMA 模型与原始 LLaMA-7b 模型进行了比较，结果表明，它们几乎全面变得更加一致，无论是在零样本和下游任务中的表示和预测方面。我们通过事实回忆的机械分析来解释这些改进。</li>
</ul>

<h3>Title: The Power of the Noisy Channel: Unsupervised End-to-End Task-Oriented  Dialogue with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Brendan King, Jeffrey Flanigan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15219">https://arxiv.org/abs/2404.15219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15219">https://arxiv.org/pdf/2404.15219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15219]] The Power of the Noisy Channel: Unsupervised End-to-End Task-Oriented  Dialogue with LLMs(https://arxiv.org/abs/2404.15219)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step. These annotations can be costly to produce, error-prone, and require both domain and annotation expertise. With advances in LLMs, we hypothesize unlabelled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised. Using only (1) a well-defined API schema (2) a set of unlabelled dialogues between a user and agent, we develop a novel approach for inferring turn-level annotations as latent variables using a noisy channel model. We iteratively improve these pseudo-labels with expectation-maximization (EM), and use the inferred labels to train an end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline.</li>
<li><strong>摘要：</strong>训练面向任务的对话系统通常需要回合级注释才能与其 API 交互：例如对话状态以及每一步采取的系统操作。这些注释的生成成本高昂、容易出错，并且需要领域和注释专业知识。随着法学硕士的进步，我们假设未标记的数据和模式定义足以构建一个完全无人监督的、面向任务的对话系统。仅使用 (1) 定义良好的 API 模式 (2) 用户和代理之间的一组未标记对话，我们开发了一种使用噪声通道模型将回合级注释推断为潜在变量的新颖方法。我们通过期望最大化（EM）迭代地改进这些伪标签，并使用推断的标签来训练端到端对话代理。在 MultiWOZ 基准上评估我们的方法，我们的方法使强大的 GPT-3.5 基准的对话成功率提高了一倍以上。</li>
</ul>

<h3>Title: CultureBank: An Online Community-Driven Knowledge Base Towards  Culturally Aware Language Technologies</h3>
<ul>
<li><strong>Authors: </strong>Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Chunhua yu, Raya Horesh, Rogério Abreu de Paula, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15238">https://arxiv.org/abs/2404.15238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15238">https://arxiv.org/pdf/2404.15238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15238]] CultureBank: An Online Community-Driven Knowledge Base Towards  Culturally Aware Language Technologies(https://arxiv.org/abs/2404.15238)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale. With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation. With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement. We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting. Finally, we offer recommendations based on our findings for future culturally aware language technologies. The project page is https://culturebank.github.io . The code and model is at https://github.com/SALT-NLP/CultureBank . The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .</li>
<li><strong>摘要：</strong>为了增强语言模型的文化意识，我们设计了一个通用的管道来大规模构建来自不同在线社区的文化知识库。通过该管道，我们构建了 CultureBank，这是一个基于用户自我叙述的知识库，其中包含来自 TikTok 的 12K 文化描述符和来自 Reddit 的 11K 文化描述符。与以往的文化知识资源不同，CultureBank包含了多种文化描述符观点，可以灵活解读文化知识，并提供情境化的文化场景，帮助进行落地评估。通过 CultureBank，我们评估不同法学硕士的文化意识，并找出需要改进的领域。我们还在 CultureBank 上微调了语言模型：实验表明，它在零样本设置中在两个下游文化任务上取得了更好的性能。最后，我们根据我们的发现为未来的文化意识语言技术提供建议。项目页面为 https://culturebank.github.io 。代码和模型位于 https://github.com/SALT-NLP/CultureBank 。已发布的 CultureBank 数据集位于 https://huggingface.co/datasets/SALT-NLP/CultureBank 。</li>
</ul>

<h3>Title: XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging  Upcycled Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Ding, Jiawei Liu, Yuxiang Wei, Terry Yue Zhuo, Lingming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15247">https://arxiv.org/abs/2404.15247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15247">https://arxiv.org/pdf/2404.15247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15247]] XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging  Upcycled Mixture-of-Experts(https://arxiv.org/abs/2404.15247)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at https://github.com/ise-uiuc/xft .</li>
<li><strong>摘要：</strong>我们引入了 XFT，这是一种简单但功能强大的训练方案，通过简单地合并升级后的专家混合 (MoE) 来释放指令调整代码大型语言模型 (LLM) 的性能限制。虽然普通的稀疏升级无法改善指令调优，但 XFT 在稀疏升级中引入了共享专家机制和新颖的路由权重归一化策略，从而显着提高了指令调优。在对升级后的 MoE 模型进行微调后，XFT 引入了可学习的模型合并机制，将升级后的 MoE 模型编译回密集模型，仅通过密集模型计算即可实现升级后的 MoE 级别的性能。通过将 XFT 应用于 1.3B 模型，我们创建了一个新的最先进的微型代码 LLM (<3B)，在 HumanEval 和 HumanEval+ 上分别具有 67.1 和 64.6 pass@1。使用相同的数据和模型架构，XFT 在 HumanEval+ 上将监督微调 (SFT) 提高了 13%，在 MBPP+、MultiPL-E 和 DS-1000 上也从 2% 持续改进到 13%，这证明了其通用性。 XFT 与 Evol-Instruct 和 OSS-Instruct 等现有技术完全正交，为改进代码指令调优开辟了新的维度。代码可在 https://github.com/ise-uiuc/xft 获取。</li>
</ul>

<h3>Title: Aligning LLM Agents by Learning Latent Preference from User Edits</h3>
<ul>
<li><strong>Authors: </strong>Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, Dipendra Misra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15269">https://arxiv.org/abs/2404.15269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15269">https://arxiv.org/pdf/2404.15269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15269]] Aligning LLM Agents by Learning Latent Preference from User Edits(https://arxiv.org/abs/2404.15269)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences</li>
<li><strong>摘要：</strong>我们根据用户对代理输出的编辑来研究语言代理的交互式学习。在诸如写作助理之类的典型设置中，用户与语言代理交互以在给定上下文的情况下生成响应，并且除了提高正确性之外，还可以选择编辑代理响应以根据他们的潜在偏好对其进行个性化。编辑反馈是自然生成的，使其成为提高代理与用户偏好的一致性以及随着时间的推移降低用户编辑成本的合适候选者。我们提出了一个学习框架 PRELUDE，它根据历史编辑数据推断用户潜在偏好的描述，并使用它来定义驱动未来响应生成的提示策略。这避免了对代理进行微调，这种微调成本高昂，难以随着用户数量的增长而扩展，甚至可能会降低其在其他任务上的性能。此外，学习描述性偏好可以提高可解释性，允许用户查看和修改学习到的偏好。然而，用户偏好可能很复杂，并且会根据上下文而变化，这使得学习变得困难。为了解决这个问题，我们提出了一种名为 CIPHER 的简单而有效的算法，该算法利用大型语言模型 (LLM) 根据用户编辑来推断用户对给定上下文的偏好。未来，CIPHER 从历史上 k 个最接近的上下文中检索推断的偏好，并形成响应生成的聚合偏好。我们引入了两种交互式环境——摘要和电子邮件编写，以使用 GPT-4 模拟用户进行评估。我们与直接检索用户编辑但不学习描述性偏好的算法以及学习上下文无关偏好的算法进行比较。在这两项任务中，CIPHER 实现了最低的编辑距离成本，并学习与地面真实偏好显示出显着相似性的偏好</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
