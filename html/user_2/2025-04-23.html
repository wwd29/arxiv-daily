<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-23</h1>
<h3>Title: Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection</h3>
<ul>
<li><strong>Authors: </strong>Myrthe Reuver, Indira Sen, Matteo Melis, Gabriella Lapesa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15392">https://arxiv.org/abs/2504.15392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15392">https://arxiv.org/pdf/2504.15392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15392]] Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection(https://arxiv.org/abs/2504.15392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper investigates hybrid intelligence and collaboration between researchers of sexism and Large Language Models (LLMs), with a four-component pipeline. First, nine sexism researchers answer questions about their knowledge of sexism and of LLMs. They then participate in two interactive experiments involving an LLM (GPT3.5). The first experiment has experts assessing the model's knowledge about sexism and suitability for use in research. The second experiment tasks them with creating three different definitions of sexism: an expert-written definition, an LLM-written one, and a co-created definition. Lastly, zero-shot classification experiments use the three definitions from each expert in a prompt template for sexism detection, evaluating GPT4o on 2.500 texts sampled from five sexism benchmarks. We then analyze the resulting 67.500 classification decisions. The LLM interactions lead to longer and more complex definitions of sexism. Expert-written definitions on average perform poorly compared to LLM-generated definitions. However, some experts do improve classification performance with their co-created definitions of sexism, also experts who are inexperienced in using LLMs.</li>
<li><strong>摘要：</strong>本文通过四组分管道调查了性别歧视与大语言模型（LLMS）研究人员之间的混合情报和合作。首先，九名性别歧视研究人员回答了有关他们对性别歧视和LLM的了解的问题。然后，他们参加了两个涉及LLM的交互式实验（GPT3.5）。第一个实验的专家评估了该模型对性别歧视的知识和在研究中使用的适用性。第二个实验使他们创建了三个不同的性别歧视定义：专家写的定义，一个LLM编写的定义和共同创建的定义。最后，零射击分类实验在及时模板中使用每个专家的三个定义进行性别歧视检测，评估了从五个性别歧视基准采样的2.500文本上的GPT4O。然后，我们分析结果的67.500分类决策。 LLM相互作用导致性别歧视的更长，更复杂的定义。与LLM生成的定义相比，专家写的定义平均表现差。但是，一些专家确实通过共同创建的性别歧视定义提高了分类绩效，也没有使用LLM的经验的专家。</li>
</ul>

<h3>Title: Trillion 7B Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Sungjun Han, Juyoung Suk, Suyeong An, Hyungguk Kim, Kyuseok Kim, Wonsuk Yang, Seungtaek Choi, Jamin Shin (Trillion Labs)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15431">https://arxiv.org/abs/2504.15431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15431">https://arxiv.org/pdf/2504.15431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15431]] Trillion 7B Technical Report(https://arxiv.org/abs/2504.15431)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency.</li>
<li><strong>摘要：</strong>我们介绍了万亿名7B，这是最有效的韩国以韩国为中心的多语言LLM。我们新颖的跨语性文档关注（XLDA）机制可以使高效有效的知识转移到韩语和日语等目标语言。结合优化的数据混合物，特定于语言的过滤和量身定制的令牌构建，数万亿-7B可以实现竞争性能，同时仅将其2T培训代币的10 \％专用于多语言数据，仅需要59.4K H100 GPU小时（\ $ 148K）才能进行全面培训。四种语言的27个基准测试的全面评估表明，数万亿美元的强大多种表现和出色的跨语性一致性。</li>
</ul>

<h3>Title: Feeding LLM Annotations to BERT Classifiers at Your Own Risk</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Lu, Kazimier Smith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15432">https://arxiv.org/abs/2504.15432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15432">https://arxiv.org/pdf/2504.15432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15432]] Feeding LLM Annotations to BERT Classifiers at Your Own Risk(https://arxiv.org/abs/2504.15432)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Using LLM-generated labels to fine-tune smaller encoder-only models for text classification has gained popularity in various settings. While this approach may be justified in simple and low-stakes applications, we conduct empirical analysis to demonstrate how the perennial curse of training on synthetic data manifests itself in this specific setup. Compared to models trained on gold labels, we observe not only the expected performance degradation in accuracy and F1 score, but also increased instability across training runs and premature performance plateaus. These findings cast doubts on the reliability of such approaches in real-world applications. We contextualize the observed phenomena through the lens of error propagation and offer several practical mitigation strategies, including entropy-based filtering and ensemble techniques. Although these heuristics offer partial relief, they do not fully resolve the inherent risks of propagating non-random errors from LLM annotations to smaller classifiers, underscoring the need for caution when applying this workflow in high-stakes text classification tasks.</li>
<li><strong>摘要：</strong>使用LLM生成的标签来微调仅用于文本分类的较小编码模型，已在各种设置中越来越受欢迎。尽管这种方法可以在简单且低风险的应用中证明是合理的，但我们进行了经验分析，以证明对合成数据的多年生训练如何在这种特定的设置中表现出来。与在黄金标签上训练的模型相比，我们不仅观察到准确性和F1得分的预期性能降低，而且还增加了训练跑步和早产高原的不稳定性。这些发现对实际应用中此类方法的可靠性产生了怀疑。我们通过误差传播的镜头将观察到的现象与观察到的现象相关，并提供了几种实际的缓解策略，包括基于熵的过滤和整体技术。尽管这些启发式方法提供了部分缓解，但它们并未完全解决从LLM注释到较小分类器传播非随机错误的固有风险，从而强调了在高风险文本分类任务中应用此工作流程时需要谨慎的需求。</li>
</ul>

<h3>Title: Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tyler A. Chang, Benjamin K. Bergen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15471">https://arxiv.org/abs/2504.15471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15471">https://arxiv.org/pdf/2504.15471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15471]] Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models(https://arxiv.org/abs/2504.15471)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying language model circuits by building up from a minimal circuit rather than the traditional approach of ablating circuits from a full model.</li>
<li><strong>摘要：</strong>在变压器语言模型中，激活向量从当前的令牌嵌入到通过模型时的下一个令牌预测。为了隔离这种转换的最小形式，我们确定了进行大型预测的语言模型子网，仅基于当前令牌而天真的令牌预测。我们发现，BigRAM子网络可以在最高1B参数的完全训练的语言模型中找到，并且这些子网对于模型性能至关重要，即使它们不包括模型参数的0.2％。 Bigram子网集中在第一个变压器MLP层中，它们与经过训练以最佳修剪给定模型的子网显着重叠。从机械上讲，Bigram子网通常从完整模型中重新创建了一个模式，在整个模型中，第一层会引起急剧的变化，从而使激活与接下来的令牌预测而不是当前的令牌表示。我们的结果表明，BigRAM子网组成的参数的最小子集既需要语言模型中的基本隔壁预测，又足够，它们有助于驱动残差流中从当前到近代的令牌激活的转换。这些子网络可以通过从最小电路而不是从完整模型中消除电路的传统方法来研究语言模型电路的基础。</li>
</ul>

<h3>Title: Speculative Sampling via Exponential Races</h3>
<ul>
<li><strong>Authors: </strong>Szymon Kobus, Deniz Gündüz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15475">https://arxiv.org/abs/2504.15475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15475">https://arxiv.org/pdf/2504.15475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15475]] Speculative Sampling via Exponential Races(https://arxiv.org/abs/2504.15475)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding accelerates large language model inference using a smaller draft model. In this paper, we establish a surprising connection between speculative decoding and channel simulation, which aims at simulating a noisy channel using as few bits as possible. This connection allows us to provide an information-theoretic analysis of the speed up that can be achieved by speculative decoding. Leveraging this link, we derive an explicit relation between generation speed-up and the number of tokens $k$ generated by the draft model for large $k$, which serves as an upper bound for all $k$. We also propose a novel speculative decoding method via exponential race ERSD that matches state-of-the-art performance.</li>
<li><strong>摘要：</strong>投机解码使用较小的草稿模型加速了大语言模型。在本文中，我们建立了投机解码与通道模拟之间的令人惊讶的联系，该连接旨在使用尽可能少的位模拟嘈杂的通道。该连接使我们能够对可以通过投机解码来实现的速度进行信息理论分析。利用此链接，我们得出了生成加速与大型$ K $生成的令牌$ K $之间的明确关系，该$ k $对于所有$ k $来说都是上限。我们还通过指数种族ERSD提出了一种新型的投机解码方法，该方法与最先进的性能相匹配。</li>
</ul>

<h3>Title: SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation</h3>
<ul>
<li><strong>Authors: </strong>Keqi Deng, Wenxi Chen, Xie Chen, Philip C. Woodland</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15509">https://arxiv.org/abs/2504.15509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15509">https://arxiv.org/pdf/2504.15509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15509]] SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation(https://arxiv.org/abs/2504.15509)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.</li>
<li><strong>摘要：</strong>同时语音翻译（SST）与流语音输入，平衡翻译质量和延迟并行输出翻译。尽管大型语言模型（LLM）已扩展以处理语音模式，但随着语音被培养作为整个生成过程的提示，流媒体仍然具有挑战性。为了解锁LLM流媒体功能，本文提出了Simuls2S-LLM，该simuls2s-llm逐渐训练语音LLMS，并采用测试时间政策来指导同时推断。 SIMULS2S-LLM通过提取边界感知的语音提示来减轻训练与推理之间的不匹配，从而使其与文本输入数据更好地匹配。 SIMULS2S-LLM通过预测离散的输出语音令牌，然后使用预训练的Vocoder来综合输出语音，从而实现同时的语音到语音翻译（SIMUL-S2ST）。增量梁搜索旨在扩大语音令牌预测的搜索空间，而不会增加延迟。 CVSS语音数据上的实验表明，SIMULS2S-LLM比使用相同培训数据的现有方法提供了更好的翻译质量延迟权衡，例如在相似的延迟下将ASR-Bleu得分提高3分。</li>
</ul>

<h3>Title: The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Minghao Wu, Weixuan Wang, Sinuo Liu, Huifeng Yin, Xintong Wang, Yu Zhao, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15521">https://arxiv.org/abs/2504.15521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15521">https://arxiv.org/pdf/2504.15521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15521]] The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks(https://arxiv.org/abs/2504.15521)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLMS）在语言能力上继续提高，强大的多语言评估对于促进公平的技术进步至关重要。该立场论文审查了来自2021年至2024年发表的148个国家 /地区的2,000多个多语言（非英语）基准，以评估过去，现在和未来的多语言基准测试实践。我们的发现表明，尽管大量投资总计数千美元，但在这些基准测试中，英语仍然大大超为代表。此外，大多数基准测试依赖原始语言内容而不是翻译，其中大多数来自中国，印度，德国，英国和美国等高资源国家。此外，基准表现与人类判断的比较突出了显着的差异。与STEM相关的任务表现出与人类评估（0.70至0.85）的密切相关性，而传统的NLP任务（例如问答（例如，Xquad））的相关性较弱（0.11至0.30）。此外，将英语基准转换为其他语言被证明不足，因为本地化的基准表现出与当地人类判断的一致性（0.68）的比对高于其翻译的对应物（0.47）。这强调了在文化和语言上量身定制的基准而不是仅依靠翻译的重要性。通过这项全面的分析，我们重点介绍了当前多语言评估实践中的六个关键局限性，提出了指导原则以进行有效的多语言基准测试，并概述了五个关键研究方向以推动该领域的进步。最后，我们呼吁进行全球协作努力，以开发优先考虑现实应用程序的人类一致的基准。</li>
</ul>

<h3>Title: IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property</h3>
<ul>
<li><strong>Authors: </strong>Qiyao Wang, Guhong Chen, Hongbo Wang, Huaren Liu, Minghui Zhu, Zhifei Qin, Linwei Li, Yilin Yue, Shiqiang Wang, Jiayan Li, Yihang Wu, Ziqiang Liu, Longze Chen, Run Luo, Liyang Fan, Jiaming Li, Lei Zhang, Kan Xu, Hongfei Lin, Hamid Alinejad-Rokny, Shiwen Ni, Yuan Lin, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15524">https://arxiv.org/abs/2504.15524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15524">https://arxiv.org/pdf/2504.15524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15524]] IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property(https://arxiv.org/abs/2504.15524)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Intellectual Property (IP) is a unique domain that integrates technical and legal knowledge, making it inherently complex and knowledge-intensive. As large language models (LLMs) continue to advance, they show great potential for processing IP tasks, enabling more efficient analysis, understanding, and generation of IP-related content. However, existing datasets and benchmarks either focus narrowly on patents or cover limited aspects of the IP field, lacking alignment with real-world scenarios. To bridge this gap, we introduce the first comprehensive IP task taxonomy and a large, diverse bilingual benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is designed to evaluate LLMs in real-world intellectual property applications, encompassing both understanding and generation. We benchmark 16 LLMs, ranging from general-purpose to domain-specific models, and find that even the best-performing model achieves only 75.8% accuracy, revealing substantial room for improvement. Notably, open-source IP and law-oriented models lag behind closed-source general-purpose models. We publicly release all data and code of IPBench and will continue to update it with additional IP-related tasks to better reflect real-world challenges in the intellectual property domain.</li>
<li><strong>摘要：</strong>知识产权（IP）是一个整合技术和法律知识的独特领域，使其本质上是复杂且知识量的。随着大型语言模型（LLMS）继续发展，它们显示出处理IP任务的巨大潜力，从而可以更有效地分析，理解和与IP相关的内容的生成。但是，现有的数据集和基准要么关注专利，要么涵盖IP字段的有限方面，因此缺乏与现实世界情景的一致性。为了弥合这一差距，我们介绍了第一个全面的IP任务分类法和大型，多样化的双语基准，IPBench，涵盖8个IP机制和20个任务。该基准旨在评估现实世界知识产权应用程序中的LLM，包括理解和产生。我们基准16 LLM，从通用到特定模型的范围不等，发现即使表现最佳的模型也只能达到75.8％的精度，从而揭示了很大的改进空间。值得注意的是，开源IP和面向法律的模型落后于封闭源通用模型。我们将公开发布IPBENCH的所有数据和代码，并将继续使用其他与IP相关的任务进行更新，以更好地反映知识产权域中的现实世界挑战。</li>
</ul>

<h3>Title: Compass-V2 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Sophia Maria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15527">https://arxiv.org/abs/2504.15527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15527">https://arxiv.org/pdf/2504.15527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15527]] Compass-V2 Technical Report(https://arxiv.org/abs/2504.15527)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Predominant LLMs focus on high-resource languages while leaving low-resource languages, particularly those in Southeast Asia (SEA), underrepresented. In addition, those models are general-purpose and pay limited attention to the e-commerce domain. To overcome these limitations, we introduce Compass-v2, a lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast Asian languages and e-commerce applications. To balance model performance and inference cost, the model is designed with 30B total parameters and 5B active parameters, incorporating both fine-grained and shared expert modules. To enhance multilingual performance, we curated and constructed a high-quality, industry-leading SEA dataset, to the best of our knowledge. To boost performance in the e-commerce domain, we built a dataset comprising hundreds of billions of tokens, sourced through external data mining and internal platform collection. Besides, we pioneered a hybrid reasoning model that supports both fast thinking and deep thinking within a unified framework to enhance the reasoning capabilities, diverging from the conventional industry practice of deploying two separate models. Through extensive experimental evaluations, our model demonstrates state-of-the-art SEA multilingual and e-commerce performance among sub-30B models, while maintaining significantly lower inference cost.</li>
<li><strong>摘要：</strong>主要的LLM专注于高资源语言，同时留下低资源语言，尤其是在东南亚（SEA）的语言，人数不足。此外，这些模型是通用的，并且对电子商务领域的关注有限。为了克服这些限制，我们引入了Compass-V2，这是专门为东南亚语言和电子商务应用设计的轻量级混合物（MOE）模型。为了平衡模型性能和推理成本，该模型的设计使用了30B总参数和5B活动参数，并结合了细粒和共享的专家模块。为了提高多语言性能，我们掌握了一个高质量的，行业领先的海洋数据集并掌握了我们的最佳知识。为了提高电子商务领域的性能，我们构建了一个数据集，其中包括数百亿个令牌，该数据集是通过外部数据挖掘和内部平台集合来源的。此外，我们开创了一个混合推理模型，该模型在统一框架内支持快速思考和深入思考，以增强推理能力，这与传统行业的实践不同，即部署两个单独的模型。通过广泛的实验评估，我们的模型展示了低下30B模型中最先进的海上多语言和电子商务性能，同时保持了明显降低的推理成本。</li>
</ul>

<h3>Title: llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length</h3>
<ul>
<li><strong>Authors: </strong>Issa Sugiura, Kouta Nakayama, Yusuke Oda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15544">https://arxiv.org/abs/2504.15544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15544">https://arxiv.org/pdf/2504.15544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15544]] llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length(https://arxiv.org/abs/2504.15544)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, long context</a></li>
<li><strong>Abstract: </strong>Encoder-only transformer models like BERT are widely adopted as a pre-trained backbone for tasks like sentence classification and retrieval. However, pretraining of encoder models with large-scale corpora and long contexts has been relatively underexplored compared to decoder-only transformers. In this work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly available, massive Japanese corpus with a context length of 8192 tokens. While our model does not surpass existing baselines on downstream tasks, it achieves good results on fill-mask test evaluations. We also analyze the effect of context length expansion through pseudo-perplexity experiments. Furthermore, we investigate sentence embeddings in detail, analyzing their transitions during training and comparing them with those from other existing models, confirming similar trends with models sharing the same architecture. To support reproducibility and foster the development of long-context BERT, we release our model, along with the training and evaluation code.</li>
<li><strong>摘要：</strong>诸如BERT（例如BERT）的仅编码变压器模型被广泛用作句子分类和检索等任务的预训练的骨干。但是，与仅解码器的变压器相比，具有大规模语料库和较长背景的编码器模型相对毫无疑问。在这项工作中，我们介绍了LLM-JP-Modernbert，这是一种现代模型，该模型接受了公开可用的日本语料库，其上下文长度为8192令牌。尽管我们的模型没有超过下游任务上的现有基准，但它在填充掩盖测试评估上取得了良好的结果。我们还通过伪质感实验分析了上下文长度扩展的影响。此外，我们详细研究了句子的嵌入，分析了它们在培训期间的过渡，并将其与其他现有模型的过渡进行了比较，从而证实了类似的趋势与模型共享相同的体系结构。为了支持可重复性并促进了长篇小说Bert的发展，我们发布了模型，以及培训和评估法。</li>
</ul>

<h3>Title: LLM-based Semantic Augmentation for Harmful Content Detection</h3>
<ul>
<li><strong>Authors: </strong>Elyas Meguellati, Assaad Zeghina, Shazia Sadiq, Gianluca Demartini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15548">https://arxiv.org/abs/2504.15548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15548">https://arxiv.org/pdf/2504.15548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15548]] LLM-based Semantic Augmentation for Harmful Content Detection(https://arxiv.org/abs/2504.15548)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated strong performance on simple text classification tasks, frequently under zero-shot settings. However, their efficacy declines when tackling complex social media challenges such as propaganda detection, hateful meme classification, and toxicity identification. Much of the existing work has focused on using LLMs to generate synthetic training data, overlooking the potential of LLM-based text preprocessing and semantic augmentation. In this paper, we introduce an approach that prompts LLMs to clean noisy text and provide context-rich explanations, thereby enhancing training sets without substantial increases in data volume. We systematically evaluate on the SemEval 2024 multi-label Persuasive Meme dataset and further validate on the Google Jigsaw toxic comments and Facebook hateful memes datasets to assess generalizability. Our results reveal that zero-shot LLM classification underperforms on these high-context tasks compared to supervised models. In contrast, integrating LLM-based semantic augmentation yields performance on par with approaches that rely on human-annotated data, at a fraction of the cost. These findings underscore the importance of strategically incorporating LLMs into machine learning (ML) pipeline for social media classification tasks, offering broad implications for combating harmful content online.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的最新进展已在简单的文本分类任务上表现出很强的性能，通常在零击设置下。但是，在应对复杂的社交媒体挑战（例如宣传检测，可恨模因分类和毒性识别）等复杂的社交媒体挑战时，它们的功效会下降。现有的许多工作都集中在使用LLMS生成合成训练数据，从而忽视了基于LLM的文本预处理和语义增强的潜力。在本文中，我们介绍了一种方法，该方法促使LLMS清洁嘈杂的文本并提供上下文丰富的解释，从而增强训练集而不大量增加数据量。我们在Semeval 2024多标签有说服力的模因数据集中进行系统地评估，并在Google拼图中进一步验证有毒评论和Facebook仇恨模因数据集，以评估可推广性。我们的结果表明，与监督模型相比，在这些高文本任务上的零射门LLM分类表现不佳。相比之下，基于LLM的语义增强的整合与依赖于人类通知数据的方法（成本的一小部分）相同。这些发现强调了将LLM策略性地纳入机器学习（ML）管道中的重要性，以实现社交媒体分类任务，这对在线打击有害内容具有广泛的影响。</li>
</ul>

<h3>Title: Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Jiang, Yufei Wang, Chuhan Wu, Xinyi Dai, Yan Xu, Weinan Gan, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15573">https://arxiv.org/abs/2504.15573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15573">https://arxiv.org/pdf/2504.15573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15573]] Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction(https://arxiv.org/abs/2504.15573)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The improvement of LLMs' instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigm--Web as Instruction and Web as Response--where each web document is designated as either an instruction or a response to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort. The data and code are publicly available at this https URL.</li>
<li><strong>摘要：</strong>LLMS遵循指导能力的提高取决于高质量的指令 - 响应对的可用性。尽管现有的自动数据综合方法减轻了手动策划的负担，但它们通常严重依赖种子数据的质量或关于Web文档的结构和内容的强有力的假设。为了应对这些挑战，我们提出了Web重建（WEBR），这是一个完全自动化的框架，用于直接从具有最小假设的原始Web文档中综合高质量指导调查（IT）数据。利用原始Web内容的固有多样性，我们将Web重构概念化为指令调查数据综合任务，这是通过新颖的双重观点范式 -  Web作为指令和Web作为响应 - 每个Web文档都将每个Web文档指定为指令或触发重构过程的响应。全面的实验表明，Webr胜过最先进的基线生成的数据集在四个指令遵循的基准中最多可达16.65％。值得注意的是，WebR表现出了卓越的兼容性，数据效率和可伸缩性，从而使域的适应性最小。数据和代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models</h3>
<ul>
<li><strong>Authors: </strong>Pavan Yadav, Nikhil Khandalkar, Krishna Shinde, Lokesh B. Ramegowda, Rajarshi Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15604">https://arxiv.org/abs/2504.15604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15604">https://arxiv.org/pdf/2504.15604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15604]] Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models(https://arxiv.org/abs/2504.15604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Language models have made significant progress in generating coherent text and predicting next tokens based on input prompts. This study compares the next-token prediction performance of two well-known models: OpenAI's GPT-2 and Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their capabilities, we built a dataset from 10 short stories sourced from the Explore ToM Dataset. We enhanced these stories by programmatically inserting additional sentences (infills) using GPT-4, creating variations that introduce different levels of contextual complexity. This setup enables analysis of how increasing context affects model performance. We tested both models under four temperature settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next token across three reasoning levels. Zero-order reasoning involves tracking the state, either current (ground truth) or past (memory). First-order reasoning concerns understanding another's mental state (e.g., "Does Anne know the apple is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think that Charles knows the apple is salted?"). Our results show that adding more infill sentences slightly reduces prediction accuracy, as added context increases complexity and ambiguity. Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at lower temperatures, demonstrating greater confidence in selecting the most probable token. As reasoning complexity rises, model responses diverge more. Notably, GPT-2 and Llama-2 display greater variability in predictions during first- and second-order reasoning tasks. These findings illustrate how model architecture, temperature, and contextual complexity influence next-token prediction, contributing to a better understanding of the strengths and limitations of current language models.</li>
<li><strong>摘要：</strong>语言模型在生成连贯的文本和根据输入提示中预测旁边的令牌方面取得了重大进展。这项研究比较了两个著名模型的下一步预测性能：OpenAI的GPT-2和Meta的Llama-2-7b-Chat-HF在心理理论（TOM）任务上。为了评估他们的功能，我们从探索汤姆数据集采购的10个短篇小说中构建了一个数据集。我们通过使用GPT-4编程插入其他句子（填充）来增强这些故事，从而创造了引入不同级别上下文复杂性的变化。该设置使上下文如何影响模型性能分析。我们在四个温度设置（0.01、0.5、1.0、2.0）之下测试了两个模型，并评估了它们在三个推理水平上预测下一代币的能力。零级推理涉及跟踪状态，即当前（地面真相）或过去（内存）。一阶推理涉及理解他人的心理状态（例如，“安妮知道苹果被腌制吗？”）。二阶推理增加了递归（例如，“安妮认为查尔斯知道苹果被腌制了？”）。我们的结果表明，随着增加的上下文增加了复杂性和模棱两可，增加更多的填充句子会稍微降低预测准确性。 Llama-2在预测准确性上始终优于GPT-2，尤其是在较低的温度下，在选择最可能的令牌方面表现出更大的信心。随着推理复杂性的上升，模型响应的差异更大。值得注意的是，GPT-2和Llama-2在一阶和二阶推理任务中显示出更大的预测可变性。这些发现说明了模型架构，温度和上下文复杂性如何影响下一步的预测，从而更好地理解当前语言模型的优势和局限性。</li>
</ul>

<h3>Title: Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xiaowei Yuan, Zhao Yang, Ziyang Huang, Yequan Wang, Siqi Fan, Yiming Ju, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15630">https://arxiv.org/abs/2504.15630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15630">https://arxiv.org/pdf/2504.15630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15630]] Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement(https://arxiv.org/abs/2504.15630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet they often struggle with context-faithfulness generations that properly reflect contextual knowledge. While existing approaches focus on enhancing the decoding strategies, they ignore the fundamental mechanism of how contextual information is processed within LLMs' internal states. As a result, LLMs remain limited in their ability to fully leverage contextual knowledge. In this paper, we propose Context-aware Layer Enhancement (CaLE), a novel intervention method that enhances the utilization of contextual knowledge within LLMs' internal representations. By employing V-usable information analysis, CaLE strategically amplifies the growth of contextual information at an optimal layer, thereby enriching representations in the final layer. Our experiments demonstrate that CaLE effectively improves context-faithful generation in Question-Answering tasks, particularly in scenarios involving unknown or conflicting contextual knowledge.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在各种任务中都表现出了非凡的能力，但是它们经常与正确反映上下文知识的上下文信仰世代斗争。尽管现有方法着重于增强解码策略，但它们忽略了LLMS内部状态中情境信息如何处理的基本机制。结果，LLM在充分利用上下文知识的能力上仍然有限。在本文中，我们提出了上下文感知层增强（CALE），这是一种新颖的干预方法，可增强LLMS内部表示中情境知识的利用。通过采用V型信息分析，Cale从战略上可以在最佳层上放大上下文信息的增长，从而丰富最终层中的表示形式。我们的实验表明，Cale有效地改善了在提问的任务中的上下文信仰，尤其是在涉及未知或相互矛盾的上下文知识的情况下。</li>
</ul>

<h3>Title: Cost-Effective Text Clustering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongtao Wang, Taiyan Zhang, Renchi Yang, Jianliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15640">https://arxiv.org/abs/2504.15640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15640">https://arxiv.org/pdf/2504.15640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15640]] Cost-Effective Text Clustering with Large Language Models(https://arxiv.org/abs/2504.15640)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Text clustering aims to automatically partition a collection of text documents into distinct clusters based on linguistic features. In the literature, this task is usually framed as metric clustering based on text embeddings from pre-trained encoders or a graph clustering problem upon pairwise similarities from an oracle, e.g., a large ML model. Recently, large language models (LLMs) bring significant advancement in this field by offering contextualized text embeddings and highly accurate similarity scores, but meanwhile, present grand challenges to cope with substantial computational and/or financial overhead caused by numerous API-based queries or inference calls to the models. In response, this paper proposes TECL, a cost-effective framework that taps into the feedback from LLMs for accurate text clustering within a limited budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or TriangleLLM to construct must-link/cannot-link constraints for text pairs, and further leverages such constraints as supervision signals input to our weighted constrained clustering approach to generate clusters. Particularly, EdgeLLM (resp. TriangleLLM) enables the identification of informative text pairs (resp. triplets) for querying LLMs via well-thought-out greedy algorithms and accurate extraction of pairwise constraints through carefully-crafted prompting techniques. Our experiments on multiple benchmark datasets exhibit that TECL consistently and considerably outperforms existing solutions in unsupervised text clustering under the same query cost for LLMs.</li>
<li><strong>摘要：</strong>文本聚类旨在根据语言特征自动将文本文档集合分为不同的群集。在文献中，该任务通常基于预训练编码器的文本嵌入或甲骨文的成对相似性（例如大型ML模型）的图形聚类问题的文本嵌入或图形聚类问题。最近，大型语言模型（LLMS）通过提供上下文化的文本嵌入和高度准确的相似性得分来带来重大进步，但同时，面临着巨大的挑战，以应对由许多基于API的查询或对模型的推理引起的实质性计算和/或财务间接费用。作为回应，本文提出了TECL，TECL是一个具有成本效益的框架，它涉及LLMS的反馈，以在有限的查询预算对LLMS的有限预算中进行准确的文本群集。在引擎盖下，TECL采用我们的Edgellm或Trianglellm来构建文本对的必链链接/无法链接约束，并进一步利用了对加权约束聚类方法的监督信号输入等约束，以生成簇。尤其是，Edgellm（三角形分别）可以通过精心策划的贪婪算法来识别信息丰富的文本对（分别三重态），并通过精心制作的促进技术来查询LLM。我们在多个基准数据集上进行的实验表明，在LLMS相同的查询成本下，TECL始终如一地优于无监督文本聚类的现有解决方案。</li>
</ul>

<h3>Title: Tina: Tiny Reasoning Models via LoRA</h3>
<ul>
<li><strong>Authors: </strong>Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Willie Neiswanger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15777">https://arxiv.org/abs/2504.15777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15777">https://arxiv.org/pdf/2504.15777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15777]] Tina: Tiny Reasoning Models via LoRA(https://arxiv.org/abs/2504.15777)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints.</li>
<li><strong>摘要：</strong>在语言模型中，如何实现强大的推理能力？在这个基本问题的驱动下，我们提出了蒂娜（Tina），这是一个以高成本效率实现的微小推理模型的家族。值得注意的是，蒂娜（Tina）证明，只能使用最小资源来开发实质性的推理性能，即通过在增强学习过程中应用参数有效的更新（RL），使用低级别适应性（LORA），并将其用于已经很小的1.5B参数基础模型。这种极简主义的方法产生的模型可以实现与同一基础模型建立的SOTA RL推理​​模型具有竞争力的推理性能。至关重要的是，这是在现有SOTA模型所采用的计算后培训成本的一小部分中实现的。实际上，最佳的TINA模型可实现> 20 \％的推理性能提高，而AIME24上的A级准确性为43.33 \％PASS@1，仅\ $ 9 USD培训和评估成本（即估计的260倍降低成本）。我们的工作揭示了通过Lora有效RL推理的令人惊讶的有效性。我们通过多个开源推理数据集和各种消融设置来验证这一点，从一组固定的超参数开始。此外，我们假设这种有效性和效率源于洛拉（Lora）迅速将模型调整为RL奖励的推理的结构形式，同时很大程度上保留了基本模型的基本知识。为了服务可访问性和开放研究，我们将所有代码，培训日志和模型权重\＆Checkpoints完全开放。</li>
</ul>

<h3>Title: Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Ruizhe Li, Chiwei Zhu, Benfeng Xu, Xiaorui Wang, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15784">https://arxiv.org/abs/2504.15784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15784">https://arxiv.org/pdf/2504.15784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15784]] Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach(https://arxiv.org/abs/2504.15784)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Creative writing is a key capability of Large Language Models (LLMs), with potential applications in literature, storytelling, and various creative domains. However, evaluating the creativity of machine-generated texts remains a significant challenge, as existing methods either rely on costly manual annotations or fail to align closely with human assessments. In this paper, we propose an effective automated evaluation method based on the Torrance Test of Creative Writing (TTCW), which evaluates creativity as product. Our method employs a reference-based Likert-style approach, scoring generated creative texts relative to high-quality reference texts across various tests. Experimental results demonstrate that our method significantly improves the alignment between LLM evaluations and human assessments, achieving a pairwise accuracy of 0.75 (+15\%).</li>
<li><strong>摘要：</strong>创意写作是大型语言模型（LLM）的关键能力，具有文学，讲故事和各种创意领域的潜在应用。但是，评估机器生成的文本的创造力仍然是一个重大挑战，因为现有方法要么依赖于昂贵的手动注释，要么无法与人类评估紧密保持一致。在本文中，我们提出了一种基于创意写作测试（TTCW）的有效自动化评估方法，该方法将创造力评估为产品。我们的方法采用了基于参考的李克特式方法，对各种测试中高质量参考文本进行了评分。实验结果表明，我们的方法显着提高了LLM评估和人类评估之间的比对，成对精度为0.75（+15 \％）。</li>
</ul>

<h3>Title: A closer look at how large language models trust humans: patterns and biases</h3>
<ul>
<li><strong>Authors: </strong>Valeria Lerman, Yaniv Dover</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15801">https://arxiv.org/abs/2504.15801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15801">https://arxiv.org/pdf/2504.15801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15801]] A closer look at how large language models trust humans: patterns and biases(https://arxiv.org/abs/2504.15801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）和基于LLM的代理在决策环境中越来越多地与人类相互作用，了解人类和AI代理之间的信任动态成为核心问题。尽管大量文献研究了人类如何信任AI代理人，但基于LLM的代理商如何对人类产生有效的信任是不了解的。基于LLM的代理商可能依靠某种隐含的有效信任对信任相关的上下文（例如，评估单个贷款申请）来协助和影响决策。使用既定的行为理论，我们开发了一种研究LLMS信任是否取决于三个主要的可信度维度的方法：人类主题的能力，仁慈和完整性。我们还研究人口变量如何影响有效的信任。在43,200个模拟实验中，对于五种流行语言模型，在五个不同的情况下，我们发现LLM Trust开发显示出与人类信任发展的总体相似之处。我们发现，在大多数但并非所有案例中，LLM信任都会通过可信赖性来强烈预测，在某些情况下也因年龄，宗教和性别而偏见，尤其是在财务情况下。对于文献和新模型中常见的情况尤其如此。尽管整体模式与有效信任形成的人类样式机制保持一致，但不同的模型在估计信任方面表现出差异。在某些情况下，可信度和人口因素是有效信任的预测指标。这些发现要求更好地了解AI到人类信任的动态，并监视偏见和信任开发模式，以防止AI信任应用程序中意外且潜在的有害结果。</li>
</ul>

<h3>Title: What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns</h3>
<ul>
<li><strong>Authors: </strong>Michael A. Hedderich, Anyi Wang, Raoyuan Zhao, Florian Eichin, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15815">https://arxiv.org/abs/2504.15815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15815">https://arxiv.org/pdf/2504.15815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15815]] What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns(https://arxiv.org/abs/2504.15815)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompt and model changes efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs, and we are able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.</li>
<li><strong>摘要：</strong>大型语言模型的及时工程具有挑战性，因为即使是较小的迅速扰动或模型更改也会显着影响生成的输出文本。现有的评估方法，无论是自动指标还是人类评估，都有局限性，例如提供有限的见解或劳动密集型。我们提出了Spotlight，这是一种结合自动化和人类分析的新方法。基于数据挖掘技术，我们会自动区分语言模型输出的随机（解码）变化和系统差异。此过程提供了代币模式，可描述系统的差异并指导用户手动分析其及时的影响并有效地模型更改。我们创建三个基准，以定量测试令牌模式提取方法的可靠性，并证明我们的方法为已建立的及时数据提供了新的见解。从以人为中心的角度来看，通过演示研究和用户研究，我们表明我们的令牌模式方法有助于用户了解语言模型输出的系统差异，并且我们能够发现由迅速和模型变化引起的相关差异（例如，与性别或文化有关），从而支持及时的工程工程过程和以人为中心的模型行为研究。</li>
</ul>

<h3>Title: Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model</h3>
<ul>
<li><strong>Authors: </strong>Junshu Pan, Wei Shen, Shulin Huang, Qiji Zhou, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15843">https://arxiv.org/abs/2504.15843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15843">https://arxiv.org/pdf/2504.15843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15843]] Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model(https://arxiv.org/abs/2504.15843)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.</li>
<li><strong>摘要：</strong>直接偏好优化（DPO）通过直接在没有明确奖励模型的情况下直接优化人类的偏好来简化对大语言模型（LLM）的强化学习。我们发现，在DPO培训期间，参考模型扮演了数据权重调节器的角色。但是，在DPO中相同初始化策略和参考模型的常见实践可能导致数据利用率降低并施加性能上限。同时，缺乏简单偏好优化（SIMPO）的参考模型会降低训练的鲁棒性，并需要更严格的条件以防止灾难性遗忘。在这项工作中，我们提出了Pre-DPO，这是一种简单但有效的基于DPO的训练范式，可通过利用指导参考模型来增强偏好优化性能。该参考模型可以远见，可以通过培训偏好数据实现最佳的政策状态，并作为一种指导机制，可将更高的权重分配给更适合该模型的样品，并将权重降低到不适合的样品。关于Alpacaeval 2.0和Arena-Hard V0.1基准的广泛实验表明，DEPO始终在不依赖外部模型或其他数据的情况下始终提高DPO和SIMPO的性能。</li>
</ul>

<h3>Title: Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Luwei Xiao, Rui Mao, Shuai Zhao, Qika Lin, Yanhao Jia, Liang He, Erik Cambria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15848">https://arxiv.org/abs/2504.15848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15848">https://arxiv.org/pdf/2504.15848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15848]] Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis(https://arxiv.org/abs/2504.15848)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Multimodal aspect-based sentiment classification (MASC) is an emerging task due to an increase in user-generated multimodal content on social platforms, aimed at predicting sentiment polarity toward specific aspect targets (i.e., entities or attributes explicitly mentioned in text-image pairs). Despite extensive efforts and significant achievements in existing MASC, substantial gaps remain in understanding fine-grained visual content and the cognitive rationales derived from semantic content and impressions (cognitive interpretations of emotions evoked by image content). In this study, we present Chimera: a cognitive and aesthetic sentiment causality understanding framework to derive fine-grained holistic features of aspects and infer the fundamental drivers of sentiment expression from both semantic perspectives and affective-cognitive resonance (the synergistic effect between emotional responses and cognitive interpretations). Specifically, this framework first incorporates visual patch features for patch-word alignment. Meanwhile, it extracts coarse-grained visual features (e.g., overall image representation) and fine-grained visual regions (e.g., aspect-related regions) and translates them into corresponding textual descriptions (e.g., facial, aesthetic). Finally, we leverage the sentimental causes and impressions generated by a large language model (LLM) to enhance the model's awareness of sentimental cues evoked by semantic content and affective-cognitive resonance. Experimental results on standard MASC datasets demonstrate the effectiveness of the proposed model, which also exhibits greater flexibility to MASC compared to LLMs such as GPT-4o. We have publicly released the complete implementation and dataset at this https URL</li>
<li><strong>摘要：</strong>基于多模式方面的情感分类（MASC）是一项新的任务，这是因为用户生成的社交平台上的多模式内容的增加，旨在预测对特定方面目标的情感极性（即，在文本图像对中明确提到的实体或属性）。尽管在现有的MASC中进行了广泛的努力和重大成就，但在理解细粒度的视觉内容以及从语义内容和印象（图像内容引起的情绪的认知解释）中得出的认知理由仍然存在很大的差距。在这项研究中，我们提出了嵌合体：一种认知和美学情感因果关系理解框架，以从语义的角度和情感认知共鸣（情绪响应之间的微音效应和认知解释之间的微微效果）来得出各个方面的精细整体特征，并推断出情感表达的基本驱动力）。具体而言，该框架首先包含用于补丁字对齐的视觉补丁功能。同时，它提取粗粒的视觉特征（例如，整体图像表示）和细粒度的视觉区域（例如，与方面相关区域），并将其转化为相应的文本描述（例如，面部，美学）。最后，我们利用大语言模型（LLM）产生的情感原因和印象，以增强模型对语义内容和情感认知共鸣所引起的情感提示的认识。标准MASC数据集的实验结果证明了该模型的有效性，与GPT-4O等LLM相比，该模型对MASC的灵活性也更大。我们已在此HTTPS URL上公开发布了完整的实现和数据集</li>
</ul>

<h3>Title: Dynamic Early Exit in Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15895">https://arxiv.org/abs/2504.15895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15895">https://arxiv.org/pdf/2504.15895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15895]] Dynamic Early Exit in Reasoning Models(https://arxiv.org/abs/2504.15895)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,"Wait" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024 show that the proposed method is consistently effective on deepseek-series reasoning LLMs, reducing the length of CoT sequences by an average of 31% to 43% while improving accuracy by 1.7% to 5.7%.</li>
<li><strong>摘要：</strong>大型推理语言模型（LRLMS）的最新进展依赖于测试时间缩放，这扩展了长期的经营链（COT）生成以解决复杂的任务。但是，长期以来，长期以来的思考不仅会减慢解决问题的效率，而且由于非常详细或冗余的推理步骤而导致的准确性损失有风险。我们提出了一种简单而有效的方法，该方法允许LLMS通过生成期间的早期出口来自我截断COT序列。提出的方法不依赖固定的启发式方法，而是在潜在的推理过渡点（例如，“等待”令牌）监测模型行为，并在模型对试验答案表现出较高的信心时，动态终止了下一个推理链的生成。我们的方法不需要额外的培训，并且可以无缝集成到现有的类似O1的推理LLM中。在多个推理基准中进行的实验Math-500，AMC 2023，GPQA Diamond和Aime 2024表明，该提出的方法始终在DeepSeek系列推理LLMS上有效，将COT序列的长度降低了31％至43％，同时将准确的准确性提高了1.7％至5.7％。</li>
</ul>

<h3>Title: SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15900">https://arxiv.org/abs/2504.15900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15900">https://arxiv.org/pdf/2504.15900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15900]] SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning(https://arxiv.org/abs/2504.15900)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to "think before answering." Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.</li>
<li><strong>摘要：</strong>最近的工作表明，加强学习（RL）可以通过促使他们“在回答之前思考”来显着提高大语模型（LLMS）的推理能力。然而，这些收益是否以及如何转移到音频推理仍然很大程度上没有探索。我们将群体相关策略优化（GRPO）框架从DeepSeek-R1扩展到大型音频模型（LALM），并构建一个32K样本多项选择语料库。我们使用对结构化和非结构化链的两阶段监督进行微调，然后进行课程指导的GRPO，我们系统地比较了隐式与显式与显式，以及在相同架构下的结构化与自由形式的推理。我们的结构化音频推理模型SARI（通过课程引导的强化学习结构化音频推理）在基本模型QWEN2-ADIO-7B-7B教学中的平均准确性提高了16.35％。此外，基于QWEN2.5-OMNI建立的变体在MMAU测试MINI基准中达到67.08％的最先进性能。消融实验表明，在我们使用的基本模型上：（i）SFT热身对于稳定的RL训练非常重要，（ii）结构化链比非组织链产生更强的概括性，以及（iii）易于硬化的课程加速融合并提高最终性能。这些发现表明，明确的，结构化的推理和课程学习可以显着增强音频的理解。</li>
</ul>

<h3>Title: FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity</h3>
<ul>
<li><strong>Authors: </strong>Fanny Jourdan, Yannick Chevalier, Cécile Favre</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15941">https://arxiv.org/abs/2504.15941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15941">https://arxiv.org/pdf/2504.15941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15941]] FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity(https://arxiv.org/abs/2504.15941)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly leveraged for translation tasks but often fall short when translating inclusive language -- such as texts containing the singular 'they' pronoun or otherwise reflecting fair linguistic protocols. Because these challenges span both computational and societal domains, it is imperative to critically evaluate how well LLMs handle inclusive translation with a well-founded framework. This paper presents FairTranslate, a novel, fully human-annotated dataset designed to evaluate non-binary gender biases in machine translation systems from English to French. FairTranslate includes 2418 English-French sentence pairs related to occupations, annotated with rich metadata such as the stereotypical alignment of the occupation, grammatical gender indicator ambiguity, and the ground-truth gender label (male, female, or inclusive). We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B, Llama3.3-70B) on this dataset under different prompting procedures. Our results reveal substantial biases in gender representation across LLMs, highlighting persistent challenges in achieving equitable outcomes in machine translation. These findings underscore the need for focused strategies and interventions aimed at ensuring fair and inclusive language usage in LLM-based translation systems. We make the FairTranslate dataset publicly available on Hugging Face, and disclose the code for all experiments on GitHub.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）越来越多地用于翻译任务，但在翻译包容性语言时通常会缺乏 - 例如包含单数“他们”代词或以其他方式反映公平语言协议的文本。由于这些挑战涵盖了计算和社会领域，因此必须批判性地评估LLM使用有充分的框架处理包容性翻译的能力。本文介绍了Fairtranslate，这是一种新颖的，完全人为宣传的数据集，旨在评估从英语到法语的机器翻译系统中的非二元性别偏见。 Fairtranslate包括与职业相关的2418个英语句子对，并用丰富的元数据注释，例如职业的刻板印象对准，语法性别指标歧义和基本真相的性别标签（男性，女性，女性或包容性）。我们在此数据集上评估了四个领先的LLM（Gemma2-2B，Mistral-7b，Llama3.1-8B，Llama3.3-70B）在不同的提示程序下。我们的结果揭示了跨LLM的性别代表性的实质性偏见，这突出了在机器翻译中实现公平结果时持续的挑战。这些发现强调了旨在确保基于LLM的翻译系统中的公平和包容性语言使用的重点策略和干预措施的必要性。我们将Fairtranslate数据集在拥抱面上公开可用，并在GitHub上披露所有实验的代码。</li>
</ul>

<h3>Title: W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15983">https://arxiv.org/abs/2504.15983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15983">https://arxiv.org/pdf/2504.15983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15983]] W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models(https://arxiv.org/abs/2504.15983)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The demand for efficient natural language processing (NLP) systems has led to the development of lightweight language models. Previous work in this area has primarily focused on manual design or training-based neural architecture search (NAS) methods. Recently, zero-shot NAS methods have been proposed for evaluating language models without the need for training. However, prevailing approaches to zero-shot NAS often face challenges such as biased evaluation metrics and computational inefficiencies. In this paper, we introduce weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored for lightweight language models. Our approach utilizes two evaluation proxies: the parameter count and the number of principal components with cumulative contribution exceeding $\eta$ in the feed-forward neural (FFN) layer. Additionally, by eliminating the need for gradient computations, we optimize the evaluation time, thus enhancing the efficiency of designing and evaluating lightweight language models. We conduct a comparative analysis on the GLUE and SQuAD datasets to evaluate our approach. The results demonstrate that our method significantly reduces training time compared to one-shot NAS methods and achieves higher scores in the testing phase compared to previous state-of-the-art training-based methods. Furthermore, we perform ranking evaluations on a dataset sampled from the FlexiBERT search space. Our approach exhibits superior ranking correlation and further reduces solving time compared to other zero-shot NAS methods that require gradient computation.</li>
<li><strong>摘要：</strong>对有效的自然语言处理（NLP）系统的需求导致了轻型语言模型的发展。该领域的先前工作主要集中于手动设计或基于培训的神经体系结构搜索（NAS）方法。最近，已经提出了用于评估语言模型而无需培训的NAS方法。但是，零射门NAS的普遍方法通常面临诸如偏见的评估指标和计算效率低下的挑战。在本文中，我们介绍了体重加权PCA（W-PCA），这是一种专门针对轻质语言模型量身定制的新型零弹药NAS方法。我们的方法利用了两个评估代理：参数计数和累积贡献的主要成分数量超过了前馈神经（FFN）层中的$ \ eta $。此外，通过消除对梯度计算的需求，我们优化了评估时间，从而提高了设计和评估轻量级语言模型的效率。我们对胶水和小队数据集进行了比较分析，以评估我们的方法。结果表明，与先前的基于最新的训练方法相比，与单发NAS方法相比，我们的方法显着减少了训练时间，并且在测试阶段获得更高的分数。此外，我们在Flexibert搜索空间采样的数据集上执行排名评估。与其他需要梯度计算的NAS方法相比，我们的方法表现出较高的排名相关性，并进一步减少了解决时间。</li>
</ul>

<h3>Title: Few-shot Hate Speech Detection Based on the MindSpore Framework</h3>
<ul>
<li><strong>Authors: </strong>Zhenkai Qin, Dongze Wu, Yuxin Liu, Guifang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.15987">https://arxiv.org/abs/2504.15987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.15987">https://arxiv.org/pdf/2504.15987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.15987]] Few-shot Hate Speech Detection Based on the MindSpore Framework(https://arxiv.org/abs/2504.15987)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>The proliferation of hate speech on social media poses a significant threat to online communities, requiring effective detection systems. While deep learning models have shown promise, their performance often deteriorates in few-shot or low-resource settings due to reliance on large annotated corpora. To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for few-shot hate speech detection implemented on the MindSpore deep learning platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM backbone with attention pooling, and synonym-based adversarial data augmentation to improve generalization. Experimental results on two benchmark datasets-HateXplain and HSOL-demonstrate that our approach outperforms competitive baselines in precision, recall, and F1-score. Additionally, the framework shows high efficiency and scalability, suggesting its suitability for deployment in resource-constrained environments. These findings highlight the potential of combining prompt-based learning with adversarial augmentation for robust and adaptable hate speech detection in few-shot scenarios.</li>
<li><strong>摘要：</strong>社交媒体上仇恨言论的扩散对在线社区构成了重大威胁，需要有效的检测系统。尽管深度学习模型已经表现出了希望，但由于依赖大型注释的语料库，它们的性能通常在几次或低资源设置中恶化。为了解决这个问题，我们提出了MS-FSLHATE，这是一个迅速增强的神经框架，用于在Mindspore深度学习平台上实施的几声仇恨言论检测。该模型集成了可学习的提示嵌入，CNN-BILSTM主链具有注意力集合，以及基于同义词的对抗数据增强，以改善概括。两个基准数据集Hatexplain和HSOL示出的实验结果表明，我们的方法在精确，回忆和F1得分方面优于竞争基准。此外，该框架显示出很高的效率和可扩展性，这表明其在资源受限环境中的部署适用性。这些发现突出了将基于及时的学习与对抗性增强结合起来的潜力，以在几个场景中以鲁棒和适应性的仇恨言论检测。</li>
</ul>

<h3>Title: CAPO: Cost-Aware Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.NE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16005">https://arxiv.org/abs/2504.16005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16005">https://arxiv.org/pdf/2504.16005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16005]] CAPO: Cost-Aware Prompt Optimization(https://arxiv.org/abs/2504.16005)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通过解决了一个提示指导的广泛任务，彻底改变了自然语言处理。然而，他们的性能对及时配方非常敏感。尽管自动提示优化通过找到最佳提示来解决此挑战，但当前的方法需要大量的LLM调用和输入令牌，从而使迅速优化昂贵。我们介绍CAPO（成本吸引及时优化），该算法通过集成自动化技术来提高及时的优化效率。 CAPO是LLM作为运营商的进化方法，并结合了赛车以节省评估和多目标优化，以平衡性能与迅速的长度。它共同优化说明和少量示例，同时利用任务说明以提高鲁棒性。我们在各种数据集和LLMS上进行的广泛实验表明，在11/15个案例中，CAPO优于最先进的离散及时及时优化方法，改善了高达21％p。我们的算法在预算较小的情况下已经取得了更好的性能，通过赛车节省评估，并通过长度罚款来降低平均及时及时长度，使其既成本效率又具有成本效益。即使没有几次示例，CAPO的表现也超过了竞争对手，并且通常在初始提示方面仍然保持强劲。 CAPO代表了通过提高成本效益来提高功能更强大和访问的重要一步。</li>
</ul>

<h3>Title: Certified Mitigation of Worst-Case LLM Copyright Infringement</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Zhang, Jiacan Yu, Marc Marone, Benjamin Van Durme, Daniel Khashabi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16046">https://arxiv.org/abs/2504.16046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16046">https://arxiv.org/pdf/2504.16046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16046]] Certified Mitigation of Worst-Case LLM Copyright Infringement(https://arxiv.org/abs/2504.16046)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of "copyright takedown" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.</li>
<li><strong>摘要：</strong>在培训前，大型语言模型（LLM）暴露于受版权保护的材料引起了人们对部署后无意侵权侵权的担忧。这促进了“版权所有”方法的开发，该方法旨在防止模型生成与受版权保护的内容基本相似的内容。尽管当前的缓解方法对于平均案例风险有效，但我们证明，它们忽略了最糟糕的版权风险，这表现出了从版权来源的长期逐字引用的存在。我们提出了BloomsCrub，这是一种非常简单但非常有效的推理时间方法，可提供经过认证的版权。我们的方法反复交织了引用检测的重写技术，以改变潜在的侵权段。通过利用有效的数据草图（BLOOM过滤器），我们的方法甚至可以针对大型现实世界中的COLLAING，也可以进行可扩展的版权筛选。当无法删除超过长度阈值的报价时，系统可以弃权，从而提供认证的风险降低。实验结果表明，Bloomscrub降低了侵权风险，保留了效用，并适应自适应弃权的不同水平的执行严格。我们的结果表明，轻量级，推理时间方法对于预防版权可能非常有效。</li>
</ul>

<h3>Title: LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement</h3>
<ul>
<li><strong>Authors: </strong>Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan Kautz, Pavlo Molchanov, Yingyan Celine Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16053">https://arxiv.org/abs/2504.16053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16053">https://arxiv.org/pdf/2504.16053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16053]] LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement(https://arxiv.org/abs/2504.16053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context</a></li>
<li><strong>Abstract: </strong>State space models (SSMs) have emerged as an efficient alternative to Transformer models for language modeling, offering linear computational complexity and constant memory usage as context length increases. However, despite their efficiency in handling long contexts, recent studies have shown that SSMs, such as Mamba models, generally underperform compared to Transformers in long-context understanding tasks. To address this significant shortfall and achieve both efficient and accurate long-context understanding, we propose LongMamba, a training-free technique that significantly enhances the long-context capabilities of Mamba models. LongMamba builds on our discovery that the hidden channels in Mamba can be categorized into local and global channels based on their receptive field lengths, with global channels primarily responsible for long-context capability. These global channels can become the key bottleneck as the input context lengthens. Specifically, when input lengths largely exceed the training sequence length, global channels exhibit limitations in adaptively extend their receptive fields, leading to Mamba's poor long-context performance. The key idea of LongMamba is to mitigate the hidden state memory decay in these global channels by preventing the accumulation of unimportant tokens in their memory. This is achieved by first identifying critical tokens in the global channels and then applying token filtering to accumulate only those critical tokens. Through extensive benchmarking across synthetic and real-world long-context scenarios, LongMamba sets a new standard for Mamba's long-context performance, significantly extending its operational range without requiring additional training. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>状态空间模型（SSM）已成为用于语言建模的变压器模型的有效替代方法，随着上下文长度的增加，线性计算复杂性和恒定内存使用情况。然而，尽管它们在处理较长的背景方面的效率，但最近的研究表明，与长篇小说中的变压器相比，SSM（例如MAMBA模型）通常表现不佳。为了解决这一严重的缺口并达到有效和准确的长期理解，我们提出了Longmamba，这是一种无训练的技术，可显着增强MAMBA模型的长期文化功能。 Longmamba建立在我们发现的基础上，即MAMBA中的隐藏渠道可以根据其接受场的长度归类为本地和全球渠道，其全球渠道主要负责长期文化的能力。随着输入上下文的延长，这些全局通道可以成为关键瓶颈。具体而言，当输入长度在很大程度上超过训练序列长度时，全局通道在适应性地扩展其接受场的局限性，导致Mamba的长期持久性能不佳。 Longmamba的关键思想是通过防止在其内存中积累不重要的令牌来减轻这些全局通道中隐藏的状态内存衰减。这是通过首先识别全局通道中的关键令牌而实现的，然后应用令牌过滤以仅积累这些关键令牌。通过跨合成和现实世界的长篇小说方案进行广泛的基准测试，Longmamba为Mamba的长篇小说性能设定了新的标准，大大扩展了其操作范围而无需额外的培训。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability</h3>
<ul>
<li><strong>Authors: </strong>Daniel Hendriks, Philipp Spitzer, Niklas Kühl, Gerhard Satzger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16056">https://arxiv.org/abs/2504.16056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16056">https://arxiv.org/pdf/2504.16056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16056]] Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability(https://arxiv.org/abs/2504.16056)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) has increasingly influenced modern society, recently in particular through significant advancements in Large Language Models (LLMs). However, high computational and storage demands of LLMs still limit their deployment in resource-constrained environments. Knowledge distillation addresses this challenge by training a small student model from a larger teacher model. Previous research has introduced several distillation methods for both generating training data and for training the student model. Despite their relevance, the effects of state-of-the-art distillation methods on model performance and explainability have not been thoroughly investigated and compared. In this work, we enlarge the set of available methods by applying critique-revision prompting to distillation for data generation and by synthesizing existing methods for training. For these methods, we provide a systematic comparison based on the widely used Commonsense Question-Answering (CQA) dataset. While we measure performance via student model accuracy, we employ a human-grounded study to evaluate explainability. We contribute new distillation methods and their comparison in terms of both performance and explainability. This should further advance the distillation of small language models and, thus, contribute to broader applicability and faster diffusion of LLM technology.</li>
<li><strong>摘要：</strong>人工智能（AI）越来越多地影响了现代社会，特别是通过大型语言模型（LLM）的重大进步。但是，LLMS的高计算和存储需求仍然限制其在资源受限环境中的部署。知识蒸馏通过培训大型教师模型的小型学生模型来应对这一挑战。先前的研究引入了几种生成培训数据和培训学生模型的蒸馏方法。尽管它们相关，但最新的蒸馏方法对模型性能和解释性的影响尚未得到彻底研究和比较。在这项工作中，我们通过应用批判性革命提示来提示数据生成并综合现有方法进行培训，从而扩大了可用方法的集合。对于这些方法，我们提供了基于广泛使用的常识性提问（CQA）数据集的系统比较。尽管我们通过学生模型的准确性来衡量绩效，但我们采用了人类的研究来评估解释性。我们在性能和解释性方面贡献了新的蒸馏方法及其比较。这应该进一步推进小语言模型的蒸馏，从而有助于更广泛的适用性和更快的LLM技术扩散。</li>
</ul>

<h3>Title: Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16060">https://arxiv.org/abs/2504.16060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16060">https://arxiv.org/pdf/2504.16060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16060]] Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation(https://arxiv.org/abs/2504.16060)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.</li>
<li><strong>摘要：</strong>参考表达产生（REG）是评估视觉系统实用能力的核心任务，不仅需要准确的语义基础，而且需要遵守合作沟通原则（Grice，1975）。但是，当前对视觉模型（VLM）的评估通常忽略了务实的维度，将REG降低到基于区域的字幕任务并忽略了Gricean Maxims。在这项工作中，我们从务实的角度重新访问了Reg，引入了用书面和口语引用表达式注释的1.5k图像的新数据集（Refoi）。通过对最先进的VLM的系统评估，我们确定了务实能力的三个关键失败：（1）未能唯一地识别指参考的信息，（2）包含过度或无关的信息，以及（3）与人类务实偏好的未对准，例如最小的空间线索不足。我们还表明，标准自动评估无法捕获这些务实的违规行为，增强了表面提示，而不是真正的参考成功。我们的发现要求重点关注务实知情的模型和与实际人类交流一致的评估框架。</li>
</ul>

<h3>Title: A Python Tool for Reconstructing Full News Text from GDELT</h3>
<ul>
<li><strong>Authors: </strong>A. Fronzetti Colladon, R. Vestrelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16063">https://arxiv.org/abs/2504.16063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16063">https://arxiv.org/pdf/2504.16063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16063]] A Python Tool for Reconstructing Full News Text from GDELT(https://arxiv.org/abs/2504.16063)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>News data have become an essential resource across various disciplines, including economics, finance, management, social sciences, and computer science. Researchers leverage newspaper articles to study economic trends, market dynamics, corporate strategies, public perception, political discourse, and the evolution of public opinion. Additionally, news datasets have been instrumental in training large-scale language models, with applications in sentiment analysis, fake news detection, and automated news summarization. Despite their significance, access to comprehensive news corpora remains a key challenge. Many full-text news providers, such as Factiva and LexisNexis, require costly subscriptions, while free alternatives often suffer from incomplete data and transparency issues. This paper presents a novel approach to obtaining full-text newspaper articles at near-zero cost by leveraging data from the Global Database of Events, Language, and Tone (GDELT). Specifically, we focus on the GDELT Web News NGrams 3.0 dataset, which provides high-frequency updates of n-grams extracted from global online news sources. We provide Python code to reconstruct full-text articles from these n-grams by identifying overlapping textual fragments and intelligently merging them. Our method enables researchers to access structured, large-scale newspaper data for text analysis while overcoming the limitations of existing proprietary datasets. The proposed approach enhances the accessibility of news data for empirical research, facilitating applications in economic forecasting, computational social science, and natural language processing.</li>
<li><strong>摘要：</strong>新闻数据已成为各个学科的重要资源，包括经济学，金融，管理，社会科学和计算机科学。研究人员利用报纸文章来研究经济趋势，市场动态，公司战略，公众看法，政治话语以及公众舆论的演变。此外，新闻数据集在培训大规模语言模型中发挥了重要作用，并在情感分析，假新闻检测和自动化新闻摘要中进行了应用。尽管它们具有重要意义，但获得综合新闻机构仍然是一个关键挑战。许多全文新闻提供商，例如Factiva和Lexisnexis，都需要昂贵的订阅，而免费替代方案通常会遭受不完整的数据和透明度问题。本文提出了一种新的方法，可以通过利用全球事件，语言和音调数据库（GDELT）的数据来获取全文报纸文章以接近零的成本。具体来说，我们专注于GDELT Web News Ngrams 3.0数据集，该数据集提供了从全球在线新闻来源中提取的N-Grams的高频更新。我们通过识别重叠的文本片段并智能合并它们来提供Python代码来重建这些N-Grams的全文文章。我们的方法使研究人员能够访问结构化的大规模报纸数据进行文本分析，同时克服现有专有数据集的局限性。提出的方法增强了新闻数据的实证研究，促进经济预测，计算社会科学和自然语言处理的应用。</li>
</ul>

<h3>Title: Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Hu, Shiyun Xiong, Yifan Zhang, See-Kiong Ng, Anh Tuan Luu, Bo An, Shuicheng Yan, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16073">https://arxiv.org/abs/2504.16073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16073">https://arxiv.org/pdf/2504.16073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16073]] Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation(https://arxiv.org/abs/2504.16073)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in three GUI navigation tasks, achieving a 3.4% improvement in single step action accuracy for static environments, along with a around 33% increase in task success rate in one dynamic environment. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）的最新进展显着增强了其处理复杂的图形用户界面（GUI）交互任务的能力。尽管有这些改进，但当前的框架通常很难在挑战性的GUI环境中产生正确的行动。最先进的商业VLM是黑盒，用于GUI任务的微调开源VLM需要大量资源。此外，由于反馈和局部优化问题，现有的轨迹级别评估和改进技术经常缺乏。为了应对这些挑战，我们提出了一种方法，该方法可以指导VLM代理在推理时通过GUI导航和控制期间通过奖励模型进行过程监督的方法。该指南使VLM代理可以在每个推理步骤中优化动作，从而改善静态和动态环境中的性能。特别是，我们的方法在三个GUI导航任务中表现出显着的性能提高，在静态环境中，单步操作精度提高了3.4％，并且在一个动态环境中，任务成功率提高了约33％。随着轨迹反射和重试机制的进一步整合，我们还证明了任务成功的增强。</li>
</ul>

<h3>Title: PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming-xing Luo, Muhan Zhang, Hua Xing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16074">https://arxiv.org/abs/2504.16074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16074">https://arxiv.org/pdf/2504.16074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16074]] PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models(https://arxiv.org/abs/2504.16074)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at this https URL.</li>
<li><strong>摘要：</strong>我们介绍了Phybench，这是一种新颖的高质量基准测试，旨在评估物理环境中大语​​言模型（LLM）的推理能力。 Phybench由基于现实世界的物理场景精心策划的物理问题组成，旨在评估模型理解和推理现实物理过程的能力。涵盖力学，电磁，热力学，光学，现代物理和高级物理学，基准跨越了从高中练习到本科问题和物理学奥林匹克挑战的难度水平。此外，我们提出了表达式编辑距离（EED）评分，这是一种基于数学表达式之间的编辑距离的新颖评估度量，该距离有效地捕获了模型推理过程中的差异以及传统二进制评分方法以外的结果。我们评估了Phybench上的各种LLM，并将其与人类专家进行比较。我们的结果表明，即使是最先进的推理模型也会显着落后于人类专家，强调了他们的局限性以及在复杂的物理推理方案中的改善。我们的基准结果和数据集可在此HTTPS URL上公开获得。</li>
</ul>

<h3>Title: TTRL: Test-Time Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.16084">https://arxiv.org/abs/2504.16084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.16084">https://arxiv.org/pdf/2504.16084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.16084]] TTRL: Test-Time Reinforcement Learning(https://arxiv.org/abs/2504.16084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: this https URL</li>
<li><strong>摘要：</strong>本文在没有明确标签的大型语言模型（LLMS）的推理任务的数据上研究了加强学习（RL）。该问题的核心挑战是推理期间的奖励估计，而无法访问地面信息。尽管这种设置似乎难以捉摸，但我们发现测试时间缩放（TTS）（例如多数投票）中的常见做法令人惊讶地获得适合推动RL培训的有效奖励。在这项工作中，我们介绍了测试时间增强学习（TTRL），这是一种使用RL在未标记数据上训练LLM的新方法。 TTRL通过在预训练的模型中利用先验来实现LLM的自我进化。我们的实验表明，TTRL始终提高各种任务和模型的性能。值得注意的是，TTRL在AIME 2024上仅使用未标记的测试数据将QWEN-2.5-MATH-7B的PASS@1的性能提高了约159％。此外，尽管TTRL仅由Maj@N Metric监督，但TTRL表现出了绩效，以始终超过初始模型的上限，并接近直接在测试数据上使用地面真实标签训练的模型的性能。我们的实验发现验证了TTRL在各种任务中的总体有效性，并突出了TTRL的更广泛的任务和领域的潜力。 github：此HTTPS URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
