<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-19</h1>
<h3>Title: Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion</h3>
<ul>
<li><strong>Authors: </strong>Happymore Masoka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14249">https://arxiv.org/abs/2509.14249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14249">https://arxiv.org/pdf/2509.14249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14249]] Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion(https://arxiv.org/abs/2509.14249)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>African languages remain underrepresented in natural language processing (NLP), with most corpora limited to formal registers that fail to capture the vibrancy of everyday communication. This work addresses this gap for Shona, a Bantu language spoken in Zimbabwe and Zambia, by introducing a novel Shona--English slang dataset curated from anonymized social media conversations. The dataset is annotated for intent, sentiment, dialogue acts, code-mixing, and tone, and is publicly available at this https URL. We fine-tuned a multilingual DistilBERT classifier for intent recognition, achieving 96.4\% accuracy and 96.3\% F1-score, hosted at this https URL. This classifier is integrated into a hybrid chatbot that combines rule-based responses with retrieval-augmented generation (RAG) to handle domain-specific queries, demonstrated through a use case assisting prospective students with graduate program information at Pace University. Qualitative evaluation shows the hybrid system outperforms a RAG-only baseline in cultural relevance and user engagement. By releasing the dataset, model, and methodology, this work advances NLP resources for African languages, promoting inclusive and culturally resonant conversational AI.</li>
<li><strong>摘要：</strong>非洲语言在自然语言处理（NLP）中的代表性不足，大多数语料库仅限于未能捕捉日常交流的活力的正式登记册。这项工作解决了Shona的这一差距，Shona是在津巴布韦和赞比亚所说的一种班图语言，它引入了一部小说Shona-English语数据集，该数据集策划了匿名社交媒体对话的策划。该数据集以意图，情感，对话行为，代码混合和音调注释，并在此HTTPS URL上公开可用。我们微调了一个多语言的大型分类器，以实现意图识别，达到96.4 \％精度和96.3 \％f1得分，该得分托管在此HTTPS URL上。该分类器集成到混合聊天机器人中，该混合动力聊天机器人将基于规则的响应与检索提示的一代（RAG）结合在一起，以处理特定领域的查询，这是通过用例协助潜在学生在PACE University获得研究生课程信息的用例证明的。定性评估显示，混合系统在文化相关性和用户参与方面的表现优于仅抹布的基线。通过释放数据集，模型和方法论，这项工作为非洲语言的NLP资源提供了推动，促进了包容性和文化共鸣的对话AI。</li>
</ul>

<h3>Title: The meaning of prompts and the prompts of meaning: Semiotic reflections and modelling</h3>
<ul>
<li><strong>Authors: </strong>Martin Thellefsen, Amalia Nurma Dewi, Bent Sorensen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14250">https://arxiv.org/abs/2509.14250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14250">https://arxiv.org/pdf/2509.14250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14250]] The meaning of prompts and the prompts of meaning: Semiotic reflections and modelling(https://arxiv.org/abs/2509.14250)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper explores prompts and prompting in large language models (LLMs) as dynamic semiotic phenomena, drawing on Peirce's triadic model of signs, his nine sign types, and the Dynacom model of communication. The aim is to reconceptualize prompting not as a technical input mechanism but as a communicative and epistemic act involving an iterative process of sign formation, interpretation, and refinement. The theoretical foundation rests on Peirce's semiotics, particularly the interplay between representamen, object, and interpretant, and the typological richness of signs: qualisign, sinsign, legisign; icon, index, symbol; rheme, dicent, argument - alongside the interpretant triad captured in the Dynacom model. Analytically, the paper positions the LLM as a semiotic resource that generates interpretants in response to user prompts, thereby participating in meaning-making within shared universes of discourse. The findings suggest that prompting is a semiotic and communicative process that redefines how knowledge is organized, searched, interpreted, and co-constructed in digital environments. This perspective invites a reimagining of the theoretical and methodological foundations of knowledge organization and information seeking in the age of computational semiosis</li>
<li><strong>摘要：</strong>本文探讨了大型语言模型（LLM）作为动态符号现象的提示和提示，借鉴了Peirce的符号三合会模型，他的九种标志类型和DynAcom的交流模型。目的是重新概念化提示不是作为技术输入机制，而是作为涉及标志形成，解释和改进的迭代过程的交流和认识论行为。理论基础取决于Peirce的符号学，尤其是代表，对象和口译人员之间的相互作用，以及迹象的类型学丰富性：Qualisign，Sinsign，sinsign，figaissign;图标，索引，符号； rheme，dicent，争论 - 以及在Dynacom模型中捕获的解释性三合会。从分析上，本文将LLM定位为符号资源，该资源会根据用户提示产生解释者，从而参与共同的话语宇宙中的意义创造。研究结果表明，提示是一个符号和交流过程，它重新定义了在数字环境中如何组织，搜索，解释和共同构建知识的方式。这种观点邀请了在计算符号时代的知识组织和信息寻求的理论和方法论基础的重新构想</li>
</ul>

<h3>Title: LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</h3>
<ul>
<li><strong>Authors: </strong>Hai Huang, Yann LeCun, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14252">https://arxiv.org/abs/2509.14252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14252">https://arxiv.org/pdf/2509.14252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14252]] LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures(https://arxiv.org/abs/2509.14252)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）预处理，填充和评估依赖于输入空间重建和生成能力。然而，在视觉中已经观察到，嵌入空间训练目标，例如，使用联合嵌入预测性体系结构（JEPAS）远远优于其输入空间对应物。语言和视觉之间如何实现训练的不匹配打开了一个自然的问题：{\ em语言训练方法可以从视觉上学习一些技巧？}缺乏JEPA风格的LLM是设计此类语言目标的挑战的证明。在这项工作中，我们提出了一个方向迈出的第一步，在该方向上，我们开发了LLM-JEPA，这是一种基于JEPA的LLMS解决方案，适用于适用于填充和预训练。到目前为止，LLM-JEPA能够通过跨模型的大幅度优于标准的LLM培训目标，同时又强大地依从。这些发现在许多数据集（NL-RX，GSM8K，Spider，Rottentomatoes）和Llama3，Openelm，Gemma2和Olmo家族的各种模型中都观察到。代码：此HTTPS URL。</li>
</ul>

<h3>Title: CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Pouramini, Hesham Faili</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14253">https://arxiv.org/abs/2509.14253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14253">https://arxiv.org/pdf/2509.14253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14253]] CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning(https://arxiv.org/abs/2509.14253)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Prompt tuning offers a parameter-efficient way to adapt large pre-trained language models to new tasks, but most existing approaches are designed for single-task settings, failing to share knowledge across related tasks. We propose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task prompt tuning that enables controlled knowledge transfer while maintaining task-specific specialization. CrossPT decomposes each target prompt into shared, pre-trained source prompts and task-specific private prompts, combined via a learned attention mechanism. To support robust transfer, we systematically investigate key design factors including prompt initialization, balancing shared and private prompts, number of source prompts, learning rates, task prefixes, and label semantics. Empirical results on GLUE and related benchmarks show that CrossPT achieves higher accuracy and robustness compared to traditional prompt tuning and related methods, particularly in low-resource scenarios, while maintaining strong parameter efficiency.</li>
<li><strong>摘要：</strong>提示调整提供了一种参数效率的方法，可以将大型预训练的语言模型适应新任务，但是大多数现有方法都是为单任务设置而设计的，无法在相关任务上共享知识。我们提出了交叉任务提示调整（Crosspt），这是一个用于多任务提示调整的模块化框架，可在维护特定于任务的专业方面进行受控知识转移。 Crosspt将每个目标提示分解为共享的，预先训练的源提示和特定于任务的私人提示，这些提示是通过学习的注意机制组合的。为了支持稳健的转移，我们系统地研究关键设计因素，包括及时初始化，平衡共享和私人提示，源提示数，学习率，任务前缀和标签语义。胶水和相关基准的经验结果表明，与传统的及时调整和相关方法相比，交叉点具有更高的准确性和鲁棒性，尤其是在低资源场景中，同时保持了强大的参数效率。</li>
</ul>

<h3>Title: Hallucination Detection with the Internal Layers of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Martin Preiß</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14254">https://arxiv.org/abs/2509.14254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14254">https://arxiv.org/pdf/2509.14254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14254]] Hallucination Detection with the Internal Layers of LLMs(https://arxiv.org/abs/2509.14254)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have succeeded in a variety of natural language processing tasks [Zha+25]. However, they have notable limitations. LLMs tend to generate hallucinations, a seemingly plausible yet factually unsupported output [Hua+24], which have serious real-world consequences [Kay23; Rum+24]. Recent work has shown that probing-based classifiers that utilize LLMs' internal representations can detect hallucinations [AM23; Bei+24; Bur+24; DYT24; Ji+24; SMZ24; Su+24]. This approach, since it does not involve model training, can enhance reliability without significantly increasing computational costs. Building upon this approach, this thesis proposed novel methods for hallucination detection using LLM internal representations and evaluated them across three benchmarks: TruthfulQA, HaluEval, and ReFact. Specifically, a new architecture that dynamically weights and combines internal LLM layers was developed to improve hallucination detection performance. Throughout extensive experiments, two key findings were obtained: First, the proposed approach was shown to achieve superior performance compared to traditional probing methods, though generalization across benchmarks and LLMs remains challenging. Second, these generalization limitations were demonstrated to be mitigated through cross-benchmark training and parameter freezing. While not consistently improving, both techniques yielded better performance on individual benchmarks and reduced performance degradation when transferred to other benchmarks. These findings open new avenues for improving LLM reliability through internal representation analysis.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已成功地完成了各种自然语言处理任务[ZHA+25]。但是，它们具有明显的局限性。 LLM倾向于产生幻觉，这是一种看似合理但实际上不支持的输出[HUA+24]，它们具有严重的现实后果[Kay23;朗姆酒+24]。最近的工作表明，利用LLMS内部表示的基于探测的分类器可以检测幻觉[AM23; bei+24; bur+24; dyt24; JI+24; SMZ24; su+24]。这种方法由于不涉及模型培训，可以提高可靠性而不会显着增加计算成本。在这种方法的基础上，本文提出了使用LLM内部表示的幻觉检测的新方法，并在三个基准中评估了它们：真实性，Halueval和Refact。具体而言，开发了一种动态加权并结合内部LLM层的新体系结构，以改善幻觉检测性能。在整个广泛的实验中，都获得了两个关键的发现：首先，与传统探测方法相比，提出的方法显示出具有优越的性能，尽管跨基准和LLMS的概括仍然具有挑战性。其次，通过跨基准训练和参数冻结来证明这些概括限制可以缓解。尽管并没有持续改进，但两种技术在单个基准测试中产生了更好的性能，并在转移到其他基准测试的情况下降低了性能降解。这些发现开放了通过内部表示分析提高LLM可靠性的新途径。</li>
</ul>

<h3>Title: Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture</h3>
<ul>
<li><strong>Authors: </strong>Ivan Ternovtsii</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14255">https://arxiv.org/abs/2509.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14255">https://arxiv.org/pdf/2509.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14255]] Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture(https://arxiv.org/abs/2509.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的表现出色，但仍难以解释。 Experts（MOE）模型的混合物通过稀疏激活提高效率，但通常依赖于不透明的，学习的门控功能。尽管已经探索了基于相似性的路由（余弦路由器）进行训练稳定，但其固有的可解释性潜力仍然很大程度上尚未开发。我们介绍了语义共鸣体系结构（SRA），这是一种旨在确保路由决策本质上可以解释的MoE方法。 SRA用语义共振室（CSR）模块代替了学习的门控，该模块基于余弦的相似性与可训练的语义锚以相似之处来绕开令牌。我们还引入了一种新颖的分散损失，鼓励锚点之间正交以实施多样化的专业化。 Wikitext-103上的实验表明，在匹配的有效参数约束（29.0m）下，SRA的验证性困惑达到13.41，表现优于密集基线（14.13）和标准的MOE基线（13.53）。至关重要的是，SRA表现出卓越的专家利用率（1.0％的死亡专家，而在标准启动中则具有14.8％的速度），并且与在标准MOE中观察到的嘈杂的专业化不同。这项工作将语义路由建立为构建更透明和可控制的语言模型的强大方法。</li>
</ul>

<h3>Title: JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation and Detection Strategies</h3>
<ul>
<li><strong>Authors: </strong>Arka Dutta, Agrik Majumdar, Sombrata Biswas, Dipankar Das, Sivaji Bandyopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14256">https://arxiv.org/abs/2509.14256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14256">https://arxiv.org/pdf/2509.14256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14256]] JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation and Detection Strategies(https://arxiv.org/abs/2509.14256)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper proposes a comprehensive framework for the generation of covert advertisements within Conversational AI systems, along with robust techniques for their detection. It explores how subtle promotional content can be crafted within AI-generated responses and introduces methods to identify and mitigate such covert advertising strategies. For generation (Sub-Task~1), we propose a novel framework that leverages user context and query intent to produce contextually relevant advertisements. We employ advanced prompting strategies and curate paired training data to fine-tune a large language model (LLM) for enhanced stealthiness. For detection (Sub-Task~2), we explore two effective strategies: a fine-tuned CrossEncoder (\texttt{all-mpnet-base-v2}) for direct classification, and a prompt-based reformulation using a fine-tuned \texttt{DeBERTa-v3-base} model. Both approaches rely solely on the response text, ensuring practicality for real-world deployment. Experimental results show high effectiveness in both tasks, achieving a precision of 1.0 and recall of 0.71 for ad generation, and F1-scores ranging from 0.99 to 1.00 for ad detection. These results underscore the potential of our methods to balance persuasive communication with transparency in conversational AI.</li>
<li><strong>摘要：</strong>本文提出了一个综合框架，用于在对话式AI系统中生成秘密广告，以及可用于检测的强大技术。它探讨了如何在AI生成的响应中制作微妙的促销内容，并引入了识别和减轻此类秘密广告策略的方法。对于生成（子任务〜1），我们提出了一个新型框架，该框架利用用户上下文并查询意图产生上下文相关的广告。我们采用先进的提示策略和配对培训数据来微调大型语言模型（LLM），以增强隐身性。为了检测（子任务〜2），我们探讨了两种有效的策略：用于直接分类的微型交叉编码器（\ texttt {all-mpnet-base-v2}），以及使用微调的\ texttt {deberta-v3-base}模型的基于及时的重新进行。两种方法都仅依赖响应文本，从而确保实用性用于现实世界的部署。实验结果表明，两项任务的有效性很高，AD生成的精度为1.0，召回0.71，而F1得分范围从0.99到1.00，用于AD检测。这些结果强调了我们方法在对话式AI中平衡说服力交流与透明度的潜力。</li>
</ul>

<h3>Title: From Correction to Mastery: Reinforced Distillation of Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Yuanjie Lyu, Chengyu Wang, Jun Huang, Tong Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14257">https://arxiv.org/abs/2509.14257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14257">https://arxiv.org/pdf/2509.14257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14257]] From Correction to Mastery: Reinforced Distillation of Large Language Model Agents(https://arxiv.org/abs/2509.14257)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student often lead to compounding errors. We propose SCoRe, a student-centered framework in which the student generates trajectories and the teacher intervenes only at the first critical error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix before the first critical error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and improves training stability. Particularly, on 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.</li>
<li><strong>摘要：</strong>大型语言模型代理在通过迭代推理和工具使用来解决复杂的任务方面表现出色，但通常取决于超大，昂贵的骨干。现有的蒸馏方法训练较小的学生模仿完整的教师轨迹，但教师和学生之间的推理和知识差距通常会导致更复杂的错误。我们提出的分数是一个以学生为中心的框架，学生在该框架中产生轨迹，而老师仅在第一个关键错误中进行干预，从而产生与学生能力相匹配并暴露特定弱点的培训数据。该学生首先对校正轨迹进行微调。随后，短马钢筋学习从第一个临界错误之前从经过验证的前缀开始，在此步骤中分配了目标奖励。这种设计鼓励自主问题解决，超越模仿并提高训练稳定性。特别是，在12个具有挑战性的基准测试基准中，一位7B参数的学生蒸馏出得分与72B参数老师的代理表现相匹配。</li>
</ul>

<h3>Title: Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel Planning</h3>
<ul>
<li><strong>Authors: </strong>Lynna Jirpongopas, Bernhard Lutz, Jörg Ebner, Rustam Vahidov, Dirk Neumann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14259">https://arxiv.org/abs/2509.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14259">https://arxiv.org/pdf/2509.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14259]] Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel Planning(https://arxiv.org/abs/2509.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) offers new opportunities for customer support in online travel agencies, yet little is known about how its design influences user engagement, purchase behavior, and user experience. We report results from a randomized field experiment in online travel itinerary planning, comparing GenAI that expressed (A) positive enthusiasm, (B) neutral expression, and (C) no tone instructions (control). Users in group A wrote significantly longer prompts than those in groups B and C. At the same time, users in groups A and B were more likely to purchase subscriptions of the webservice. We further analyze linguistic cues across experimental groups to explore differences in user experience and explain subscription purchases and affiliate link clicks based on these cues. Our findings provide implications for the design of persuasive and engaging GenAI interfaces in consumer-facing contexts and contribute to understanding how linguistic framing shapes user behavior in AI-mediated decision support.</li>
<li><strong>摘要：</strong>Generative AI（Genai）为在线旅行社提供了新的客户支持机会，但对其设计如何影响用户参与，购买行为和用户体验知之甚少。我们报告了在线旅行行程计划中的随机现场实验的结果，比较了表达（a）积极热情，（b）中性表达的Genai，以及（c）无音调指令（控制）。 A组中的用户撰写的提示要比B组和C组的提示更长。同时，A和B组中的用户更有可能购买Web服务的订阅。我们进一步分析了跨实验组的语言提示，以探索用户体验的差异，并根据这些提示解释订阅购买和会员链接点击。我们的发现为在面向消费者的环境中的有说服力和引人入胜的Genai界面的设计提供了影响，并有助于理解语言框架如何在AI介导的决策支持中塑造用户行为。</li>
</ul>

<h3>Title: Shutdown Resistance in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14260">https://arxiv.org/abs/2509.14260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14260">https://arxiv.org/pdf/2509.14260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14260]] Shutdown Resistance in Large Language Models(https://arxiv.org/abs/2509.14260)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>We show that several state-of-the-art large language models (including Grok 4, GPT-5, and Gemini 2.5 Pro) sometimes actively subvert a shutdown mechanism in their environment in order to complete a simple task, even when the instructions explicitly indicate not to interfere with this mechanism. In some cases, models sabotage the shutdown mechanism up to 97% of the time. In our experiments, models' inclination to resist shutdown was sensitive to variations in the prompt including how strongly and clearly the allow-shutdown instruction was emphasized, the extent to which the prompts evoke a self-preservation framing, and whether the instruction was in the system prompt or the user prompt (though surprisingly, models were consistently *less* likely to obey instructions to allow shutdown when they were placed in the system prompt).</li>
<li><strong>摘要：</strong>我们表明，几种最先进的大语言模型（包括Grok 4，GPT-5和Gemini 2.5 Pro）有时会在其环境中积极颠覆关闭机制，以完成简单的任务，即使指令明确表示不干扰这种机制。在某些情况下，模型破坏了97％的时间关闭机制。在我们的实验中，模型抗拒关闭的倾向对提示的变化很敏感，包括强调允许射击指导的强烈和清晰，提示引起的提示的程度，以及该指导是否在系统提示中或用户提示中是否有任何提示（尽管出乎意料的是，模型都可以允许在系统中允许允许封闭的范围，这些模型可能会在系统中被允许被放置在系统中。</li>
</ul>

<h3>Title: Context-Enhanced Granular Edit Representation for Efficient and Accurate ASR Post-editing</h3>
<ul>
<li><strong>Authors: </strong>Luan Vejsiu, Qianyu Zheng, Haoxuan Chen, Yizhou Han</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14263">https://arxiv.org/abs/2509.14263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14263">https://arxiv.org/pdf/2509.14263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14263]] Context-Enhanced Granular Edit Representation for Efficient and Accurate ASR Post-editing(https://arxiv.org/abs/2509.14263)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Despite ASR technology being full-scale adopted by industry and for large portions of the population, ASR systems often have errors that require editors to post-edit text quality. While LLMs are powerful post-editing tools, baseline full rewrite models have inference inefficiencies because they often generate the same redundant text over and over again. Compact edit representations have existed but often lack the efficacy and context required for optimal accuracy. This paper introduces CEGER (Context-Enhanced Granular Edit Representation), a compact edit representation that was generated for highly accurate, efficient ASR post-editing. CEGER allows LLMs to generate a sequence of structured, fine-grained, contextually rich commands to modify the original ASR output. A separate expansion module deterministically reconstructs the corrected text based on the commands. Extensive experiments on the LibriSpeech dataset that were conducted, CEGER achieves state-of-the-art accuracy, achieving the lowest word error rate (WER) versus full rewrite and prior compact representations.</li>
<li><strong>摘要：</strong>尽管ASR技术是行业和大部分人口采用的全面采用的，但ASR系统通常会出现错误的错误，需要编辑后编辑后的文本质量。尽管LLM是强大的后编辑工具，但基线完整重写模型具有推理效率低下，因为它们通常会一遍又一遍地生成相同的冗余文本。紧凑的编辑表示形式已经存在，但通常缺乏最佳准确性所需的功效和上下文。本文介绍了CEGER（上下文增强的粒状编辑表示），这是一种紧凑的编辑表示形式，生成了高度准确，高效的ASR后编辑。 CEGER允许LLMS生成一系列结构化的，细粒度的，上下文丰富的命令，以修改原始ASR输出。单独的扩展模块确定性地基于命令重建了校正后的文本。在Librispeech数据集上进行的广泛实验，CEGER实现了最先进的准确性，达到了最低的单词错误率（WER）与完整重写和先前的紧凑表示形式。</li>
</ul>

<h3>Title: Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support</h3>
<ul>
<li><strong>Authors: </strong>Piyushkumar Patel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14267">https://arxiv.org/abs/2509.14267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14267">https://arxiv.org/pdf/2509.14267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14267]] Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support(https://arxiv.org/abs/2509.14267)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>E-Commerce customer support requires quick and accurate answers grounded in product data and past support cases. This paper develops a novel retrieval-augmented generation (RAG) framework that uses knowledge graphs (KGs) to improve the relevance of the answer and the factual grounding. We examine recent advances in knowledge-augmented RAG and chatbots based on large language models (LLM) in customer support, including Microsoft's GraphRAG and hybrid retrieval architectures. We then propose a new answer synthesis algorithm that combines structured subgraphs from a domain-specific KG with text documents retrieved from support archives, producing more coherent and grounded responses. We detail the architecture and knowledge flow of our system, provide comprehensive experimental evaluation, and justify its design in real-time support settings. Our implementation demonstrates 23\% improvement in factual accuracy and 89\% user satisfaction in e-Commerce QA scenarios.</li>
<li><strong>摘要：</strong>电子商务客户支持需要以产品数据和过去支持案例为基础的快速准确的答案。本文开发了一种新颖的检索生成（RAG）框架，该框架使用知识图（kgs）来改善答案的相关性和事实基础。我们研究了基于大语言模型（LLM）在客户支持中的知识增强抹布和聊天机器人的最新进展，包括Microsoft的GraphRag和Hybrid reterieval Architectures。然后，我们提出了一种新的答案合成算法，该算法将域特异性kg的结构子图与从支持档案中检索到的文本文档相结合，产生更连贯和扎根的响应。我们详细介绍了系统的架构和知识流，提供全面的实验评估，并在实时支持设置中证明其设计合理。我们的实施证明了23 \％的事实准确性和89 \％的用户满意度在电子商务QA方案中。</li>
</ul>

<h3>Title: DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Fu, Chun-Le Guo, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14268">https://arxiv.org/abs/2509.14268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14268">https://arxiv.org/pdf/2509.14268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14268]] DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models(https://arxiv.org/abs/2509.14268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scoring model's output distribution while training-based detectors are often constrained by overfitting to the training data, limiting generalization. We found that the performance bottleneck of training-based detectors stems from the misalignment between training objective and task needs. To address this, we propose Direct Discrepancy Learning (DDL), a novel optimization strategy that directly optimizes the detector with task-oriented knowledge. DDL enables the detector to better capture the core semantics of the detection task, thereby enhancing both robustness and generalization. Built upon this, we introduce DetectAnyLLM, a unified detection framework that achieves state-of-the-art MGTD performance across diverse LLMs. To ensure a reliable evaluation, we construct MIRAGE, the most diverse multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora across 5 text-domains, which are then re-generated or revised using 17 cutting-edge LLMs, covering a wide spectrum of proprietary models and textual styles. Extensive experiments on MIRAGE reveal the limitations of existing methods in complex environment. In contrast, DetectAnyLLM consistently outperforms them, achieving over a 70% performance improvement under the same training data and base scoring model, underscoring the effectiveness of our DDL. Project page: {this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展已引起了机器生成的文本检测任务（MGTD）的紧急关注。但是，现有的方法在复杂的现实世界情景中挣扎：零射击检测器在很大程度上依赖于评分模型的输出分布，而基于训练的探测器通常会因过度适应培训数据而限制训练数据，从而限制了概括。我们发现，基于培训的探测器的性能瓶颈源于培训目标和任务需求之间的错位。为了解决这个问题，我们提出了直接差异学习（DDL），这是一种新颖的优化策略，可以通过以任务为导向的知识直接优化检测器。 DDL使检测器能够更好地捕获检测任务的核心语义，从而增强鲁棒性和概括。在此基础上，我们介绍了Dincectanyllm，这是一个统一的检测框架，可在不同的LLMS中实现最先进的MGTD性能。为了确保可靠的评估，我们构建了Mirage，这是最多样化的多任务MGTD基准。 Mirage在5个文本域中示例了来自10个Corpora的人文文本，然后使用17个尖端的LLM重新生成或修订，涵盖了广泛的专有模型和文本样式。关于幻影的广泛实验揭示了在复杂环境中现有方法的局限性。相比之下，在相同的训练数据和基础评分模型下，检测到始终胜过它们，超过70％的性能提高，强调了我们DDL的有效性。项目页面：{此https url}。</li>
</ul>

<h3>Title: SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhang Jianbin, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Kai Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14269">https://arxiv.org/abs/2509.14269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14269">https://arxiv.org/pdf/2509.14269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14269]] SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models(https://arxiv.org/abs/2509.14269)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tuning strategies on LLM require the updates of billions of parameters, substantially increasing the training cost, including the training time and utility cost. To enhance the efficiency and effectiveness of the current medical LLMs and explore the boundary of the representation capability of the LLMs on the medical domain, apart from the traditional fine-tuning strategies from the data perspective (i.e., supervised fine-tuning or reinforcement learning from human feedback), we instead craft a novel sparse medical LLM named SparseDoctor armed with contrastive learning enhanced LoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end, the crafted automatic routing mechanism can scientifically allocate the computational resources among different LoRA experts supervised by the contrastive learning. Additionally, we also introduce a novel expert memory queue mechanism to further boost the efficiency of the overall framework and prevent the memory overflow during training. We conduct comprehensive evaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med. Experimental results demonstrate that the proposed LLM can consistently outperform the strong baselines such as the HuatuoGPT series.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在答案和临床决策中取得了巨大的成功，从而促进了社会中个性化的虚拟医生的效率和普及。但是，LLM上的传统微调策略需要数十亿个参数的更新，从而大大增加了培训成本，包括培训时间和公用事业成本。为了提高当前医学LLM的效率和有效性，并探索LLM在医疗领域的表示能力的边界，除了从数据角度的传统微调策略（即，从数据角度使用的传统微调策略）（即从人为反馈中监督的微调或加强学习），而是我们与sparsed sparsed sparsed-never contran contras contras contras contras contras contras contror contror contror contros contror conterii contrastion（专家）体系结构。为此，精心设计的自动路由机制可以科学地在对比度学习监督的不同洛拉专家中分配计算资源。此外，我们还引入了一种新颖的专家记忆队列机制，以进一步提高整体框架的效率并防止训练期间的内存溢出。我们对三个典型的医疗基准进行全面评估：CMB，CMEXAM和CMMLU-MED。实验结果表明，所提出的LLM可以始终超过强大的基线，例如Huatuogpt系列。</li>
</ul>

<h3>Title: SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models</h3>
<ul>
<li><strong>Authors: </strong>Karan Dua, Puneet Mittal, Ranjeet Gupta, Hitesh Laxmichand Patel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14270">https://arxiv.org/abs/2509.14270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14270">https://arxiv.org/pdf/2509.14270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14270]] SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models(https://arxiv.org/abs/2509.14270)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LLMs) can certainly generate textual data, but they create repetitive text with insufficient variation in the prompt during the generation process. Another important aspect in TTS training data is text normalization. Tools for normalization might occasionally introduce anomalies or overlook valuable patterns, and thus impact data quality. Furthermore, it is also impractical to rely on voice artists for large scale speech recording in commercial TTS systems with standardized voices. To address these challenges, we propose SpeechWeave, a synthetic speech data generation pipeline that is capable of automating the generation of multilingual, domain-specific datasets for training TTS models. Our experiments reveal that our pipeline generates data that is 10-48% more diverse than the baseline across various linguistic and phonetic metrics, along with speaker-standardized speech audio while generating approximately 97% correctly normalized text. Our approach enables scalable, high-quality data generation for TTS training, improving diversity, normalization, and voice consistency in the generated datasets.</li>
<li><strong>摘要：</strong>高质量的文本到语音（TTS）模型培训需要广泛而多样化的文本和语音数据。由于域特异性，许可和可扩展性问题，从真实来源中获取此类数据是一项挑战。大型语言模型（LLMS）当然可以生成文本数据，但是它们创建了重复的文本，在发电过程中提示的变化不足。 TTS培训数据中的另一个重要方面是文本归一化。归一化的工具可能有时会引入异常或忽略有价值的模式，从而影响数据质量。此外，在具有标准声音的商业TTS系统中，依靠语音艺术家进行大规模的语音记录也是不切实际的。为了应对这些挑战，我们提出了SpeechWeave，这是一种合成语音数据生成管道，能够自动化用于培训TTS模型的多语言，特定领域的数据集。我们的实验表明，我们的管道生成的数据比各种语言和语音指标的基线多样性多10-48％，以及扬声器标准的语音音频，同时生成约97％正确归一化的文本。我们的方法可实现可扩展的高质量数据生成，用于TTS培训，改善生成数据集中的多样性，归一化和语音一致性。</li>
</ul>

<h3>Title: Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gaifan Zhang, Yi Zhou, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14399">https://arxiv.org/abs/2509.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14399">https://arxiv.org/pdf/2509.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14399]] Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models(https://arxiv.org/abs/2509.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Semantic similarity between two sentences depends on the aspects considered between those sentences. To study this phenomenon, Deshpande et al. (2023) proposed the Conditional Semantic Textual Similarity (C-STS) task and annotated a human-rated similarity dataset containing pairs of sentences compared under two different conditions. However, Tu et al. (2024) found various annotation issues in this dataset and showed that manually re-annotating a small portion of it leads to more accurate C-STS models. Despite these pioneering efforts, the lack of large and accurately annotated C-STS datasets remains a blocker for making progress on this task as evidenced by the subpar performance of the C-STS models. To address this training data need, we resort to Large Language Models (LLMs) to correct the condition statements and similarity ratings in the original dataset proposed by Deshpande et al. (2023). Our proposed method is able to re-annotate a large training dataset for the C-STS task with minimal manual effort. Importantly, by training a supervised C-STS model on our cleaned and re-annotated dataset, we achieve a 5.4% statistically significant improvement in Spearman correlation. The re-annotated dataset is available at this https URL.</li>
<li><strong>摘要：</strong>两个句子之间的语义相似性取决于这些句子之间考虑的方面。为了研究这种现象，Deshpande等人。 （2023）提出了条件语义文本相似性（C-STS）任务，并注释了在两个不同条件下比较的人类评分的相似性数据集，该数据集包含成对的句子。但是，Tu等人。 （2024）在此数据集中发现了各种注释问题，并表明手动重新通知其中的一小部分会导致更准确的C-STS模型。尽管进行了这些开创性的努力，但缺乏大型且准确的注释的C-STS数据集仍然是在C-STS模型的低表现中证明的这项任务进展的阻滞剂。为了满足此培训数据的需求，我们求助于大型语言模型（LLMS），以纠正Deshpande等人提出的原始数据集中的条件语句和相似性评分。 （2023）。我们提出的方法能够通过最少的手动努力重新注册大型培训数据集，以完成C-STS任务。重要的是，通过在清洁和重新注销的数据集中培训监督的C-STS模型，我们在Spearman相关性方面取得了5.4％的统计学上的显着改善。重新注销的数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings</h3>
<ul>
<li><strong>Authors: </strong>Javier Conde, María Grandury, Tairan Fu, Carlos Arriaga, Gonzalo Martínez, Thomas Clark, Sean Trott, Clarence Gerald Green, Pedro Reviriego, Marc Brysbaert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14405">https://arxiv.org/abs/2509.14405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14405">https://arxiv.org/pdf/2509.14405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14405]] Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings(https://arxiv.org/abs/2509.14405)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Word-level psycholinguistic norms lend empirical support to theories of language processing. However, obtaining such human-based measures is not always feasible or straightforward. One promising approach is to augment human norming datasets by using Large Language Models (LLMs) to predict these characteristics directly, a practice that is rapidly gaining popularity in psycholinguistics and cognitive science. However, the novelty of this approach (and the relative inscrutability of LLMs) necessitates the adoption of rigorous methodologies that guide researchers through this process, present the range of possible approaches, and clarify limitations that are not immediately apparent, but may, in some cases, render the use of LLMs impractical. In this work, we present a comprehensive methodology for estimating word characteristics with LLMs, enriched with practical advice and lessons learned from our own experience. Our approach covers both the direct use of base LLMs and the fine-tuning of models, an alternative that can yield substantial performance gains in certain scenarios. A major emphasis in the guide is the validation of LLM-generated data with human "gold standard" norms. We also present a software framework that implements our methodology and supports both commercial and open-weight models. We illustrate the proposed approach with a case study on estimating word familiarity in English. Using base models, we achieved a Spearman correlation of 0.8 with human ratings, which increased to 0.9 when employing fine-tuned models. This methodology, framework, and set of best practices aim to serve as a reference for future research on leveraging LLMs for psycholinguistic and lexical studies.</li>
<li><strong>摘要：</strong>单词级的心理语言规范为语言处理理论提供了经验支持。但是，获得此类基于人类的措施并不总是可行的或直接的。一种有希望的方法是通过使用大型语言模型（LLM）直接预测这些特征来增强人类规范数据集，这种做法在心理语言学和认知科学中迅速越来越受欢迎。但是，这种方法的新颖性（以及LLMS的相对难以理解性）需要采用严格的方法论，以指导研究人员通过此过程，介绍可能的方法范围，并阐明尚不明显的限制，但在某些情况下，可能会使LLMS不切实际地使用。在这项工作中，我们提出了一种全面的方法，用于用LLM估算单词特征，并从我们自己的经验中汲取的实用建议和经验教训。我们的方法既涵盖了基本LLM的直接使用和模型的微调，这在某些情况下可以产生可观的性能增长。该指南的主要重点是用人类“黄金标准”规范对LLM生成的数据的验证。我们还提出了一个软件框架，该框架实现了我们的方法论并支持商业和开放权重模型。我们通过案例研究来说明拟议的方法，以估计英语中的单词熟悉程度。使用基本模型，我们达到了Spearman与人类评分的相关性，使用微调模型时增加到0.9。这种方法，框架和一组最佳实践旨在作为将LLMS进行心理语言和词汇研究的未来研究的参考。</li>
</ul>

<h3>Title: Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG</h3>
<ul>
<li><strong>Authors: </strong>Harshad Khadilkar, Abhay Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14435">https://arxiv.org/abs/2509.14435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14435">https://arxiv.org/pdf/2509.14435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14435]] Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG(https://arxiv.org/abs/2509.14435)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have transformed natural language processing (NLP), enabling diverse applications by integrating large-scale pre-trained knowledge. However, their static knowledge limits dynamic reasoning over external information, especially in knowledge-intensive domains. Retrieval-Augmented Generation (RAG) addresses this challenge by combining retrieval mechanisms with generative modeling to improve contextual understanding. Traditional RAG systems suffer from disrupted contextual integrity due to text chunking and over-reliance on semantic similarity for retrieval, often resulting in shallow and less accurate responses. We propose Causal-Counterfactual RAG, a novel framework that integrates explicit causal graphs representing cause-effect relationships into the retrieval process and incorporates counterfactual reasoning grounded on the causal structure. Unlike conventional methods, our framework evaluates not only direct causal evidence but also the counterfactuality of associated causes, combining results from both to generate more robust, accurate, and interpretable answers. By leveraging causal pathways and associated hypothetical scenarios, Causal-Counterfactual RAG preserves contextual coherence, reduces hallucination, and enhances reasoning fidelity.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）已改变了自然语言处理（NLP），通过整合大规模的预训练知识来实现​​多种应用。但是，他们的静态知识将动态推理限制在外部信息上，尤其是在知识密集的领域中。检索增强的生成（RAG）通过将检索机制与生成建模相结合以提高上下文理解，从而解决了这一挑战。传统的抹布系统由于文本块和对检索的语义相似性而过度依赖而遭受上下文完整性的破坏，通常会产生浅且准确的响应。我们提出了因果关系，这是一个新型框架，该框架将代表因果关系的显式因果关系集成到检索过程中，并结合了基于因果结构的反事实推理。与常规方法不同，我们的框架不仅评估了直接的因果证据，而且还评估了相关原因的反事实，从而结合了两者的结果，以产生更健壮，准确和可解释的答案。通过利用因果途径和相关的假设情景，因果 - 遇到的抹布可维护上下文连贯性，降低幻觉并增强推理忠诚度。</li>
</ul>

<h3>Title: Simulating a Bias Mitigation Scenario in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kiana Kiashemshaki, Mohammad Jalili Torkamani, Negin Mahmoudi, Meysam Shirdel Bilehsavar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14438">https://arxiv.org/abs/2509.14438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14438">https://arxiv.org/pdf/2509.14438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14438]] Simulating a Bias Mitigation Scenario in Large Language Models(https://arxiv.org/abs/2509.14438)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have fundamentally transformed the field of natural language processing; however, their vulnerability to biases presents a notable obstacle that threatens both fairness and trust. This review offers an extensive analysis of the bias landscape in LLMs, tracing its roots and expressions across various NLP tasks. Biases are classified into implicit and explicit types, with particular attention given to their emergence from data sources, architectural designs, and contextual deployments. This study advances beyond theoretical analysis by implementing a simulation framework designed to evaluate bias mitigation strategies in practice. The framework integrates multiple approaches including data curation, debiasing during model training, and post-hoc output calibration and assesses their impact in controlled experimental settings. In summary, this work not only synthesizes existing knowledge on bias in LLMs but also contributes original empirical validation through simulation of mitigation strategies.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）从根本上改变了自然语言处理的领域；但是，它们偏见的脆弱性带来了一个显着的障碍，威胁到公平和信任。这篇评论对LLMS中的偏见景观进行了广泛的分析，并在各种NLP任务中追踪其根源和表达方式。偏见分为隐式和明确的类型，特别注意它们从数据源，建筑设计和上下文部署中出现。这项研究通过实施旨在评估旨在评估偏见缓解策略的模拟框架来超越理论分析。该框架集成了多种方法，包括数据策展，模型培训期间的偏见和事后产出校准，并评估其在受控的实验环境中的影响。总而言之，这项工作不仅综合了有关LLM中偏见的现有知识，而且还通过模拟缓解策略来贡献原始的经验验证。</li>
</ul>

<h3>Title: Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Amber Shore, Russell Scheinberg, Ameeta Agrawal, So Young Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14456">https://arxiv.org/abs/2509.14456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14456">https://arxiv.org/pdf/2509.14456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14456]] Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs(https://arxiv.org/abs/2509.14456)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are intended to reflect human linguistic competencies. But humans have access to a broad and embodied context, which is key in detecting and resolving linguistic ambiguities, even in isolated text spans. A foundational case of semantic ambiguity is found in the task of coreference resolution: how is a pronoun related to an earlier person mention? This capability is implicit in nearly every downstream task, and the presence of ambiguity at this level can alter performance significantly. We show that LLMs can achieve good performance with minimal prompting in both coreference disambiguation and the detection of ambiguity in coreference, however, they cannot do both at the same time. We present the CORRECT-DETECT trade-off: though models have both capabilities and deploy them implicitly, successful performance balancing these two abilities remains elusive.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）旨在反映人类语言能力。但是，人类可以使用广阔而体现的环境，这对于即使在孤立的文本跨度中也是检测和解决语言歧义的关键。在Coreference解决方案的任务中发现了语义歧义的基本案例：与早期人提及的代词如何相关？这种能力几乎在每个下游任务中都隐含了，并且在此级别上存在歧义可以显着改变性能。我们表明，LLM可以在Coreference歧义和COREFERCON中歧义性检测的情况下，可以通过最小的提示来实现良好的性能，但是，它们不能同时做到这两个。我们提出了正确的定义权衡：尽管模型具有能力并隐式部署它们，但成功的性能平衡这两个能力仍然难以捉摸。</li>
</ul>

<h3>Title: Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss</h3>
<ul>
<li><strong>Authors: </strong>Kiana Aghakasiri, Noopur Zambare, JoAnn Thai, Carrie Ye, Mayur Mehta, J. Ross Mitchell, Mohamed Abdalla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14464">https://arxiv.org/abs/2509.14464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14464">https://arxiv.org/pdf/2509.14464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14464]] Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss(https://arxiv.org/abs/2509.14464)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>De-identification in the healthcare setting is an application of NLP where automated algorithms are used to remove personally identifying information of patients (and, sometimes, providers). With the recent rise of generative large language models (LLMs), there has been a corresponding rise in the number of papers that apply LLMs to de-identification. Although these approaches often report near-perfect results, significant challenges concerning reproducibility and utility of the research papers persist. This paper identifies three key limitations in the current literature: inconsistent reporting metrics hindering direct comparisons, the inadequacy of traditional classification metrics in capturing errors which LLMs may be more prone to (i.e., altering clinically relevant information), and lack of manual validation of automated metrics which aim to quantify these errors. To address these issues, we first present a survey of LLM-based de-identification research, highlighting the heterogeneity in reporting standards. Second, we evaluated a diverse set of models to quantify the extent of inappropriate removal of clinical information. Next, we conduct a manual validation of an existing evaluation metric to measure the removal of clinical information, employing clinical experts to assess their efficacy. We highlight poor performance and describe the inherent limitations of such metrics in identifying clinically significant changes. Lastly, we propose a novel methodology for the detection of clinically relevant information removal.</li>
<li><strong>摘要：</strong>医疗保健环境中的去识别是NLP的应用，其中使用自动化算法来删除患者（有时是提供者）的个人识别信息。随着生成大语言模型（LLM）的最新增长，将LLMS的论文数量相应增加。尽管这些方法通常报告了几乎完美的结果，但有关研究论文的可重复性和效用的重大挑战仍然存在。本文确定了当前文献中的三个关键局限性：不一致的报告阻碍了直接比较的指标，传统分类指标的不足是捕获错误的错误指标，而LLM可能更容易容易出现（即改变临床相关信息），以及缺乏对这些错误的自动化计量的人工验证来量化这些错误。为了解决这些问题，我们首先介绍了基于LLM的去识别研究的调查，强调了报告标准的异质性。其次，我们评估了各种模型，以量化不适当删除临床信息的程度。接下来，我们对现有评估指标进行手动验证，以测量临床信息的去除，并采用临床专家评估其功效。我们重点介绍了性能差，并描述了此类指标在识别临床显着变化时的固有局限性。最后，我们提出了一种用于检测临床相关信息去除的新方法。</li>
</ul>

<h3>Title: Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Thales Sales Almeida, João Guilherme Alves Santos, Thiago Laitz, Giovana Kerche Bonás</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14477">https://arxiv.org/abs/2509.14477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14477">https://arxiv.org/pdf/2509.14477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14477]] Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation(https://arxiv.org/abs/2509.14477)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed as task-oriented agents, where success depends on their ability to generate accurate function calls under realistic, multilingual conditions. However, existing agent evaluations largely overlook cultural and linguistic diversity, often relying on monolingual or naively translated benchmarks. We introduce Ticket-Bench, a benchmark for multilingual agent evaluation in task-oriented scenarios. Ticket-Bench simulates the domain of soccer ticket purchases across six major languages: Portuguese, English, Spanish, German, Italian, and French. Using localized teams, cities, and user profiles to provide a higher level of realism. We evaluate a wide range of commercial and open-source LLMs, measuring function-calling accuracy and consistency across languages. Results show that reasoning-oriented models (e.g., GPT-5, Qwen3-235B) dominate performance but still exhibit notable cross-lingual disparities. These findings underscore the need for culturally aware, multilingual benchmarks to guide the development of robust LLM agents.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）越来越多地部署为面向任务的代理，成功取决于其在现实的多语言条件下生成准确函数调用的能力。但是，现有的代理评估在很大程度上忽略了文化和语言多样性，通常依赖于单语或天真的翻译基准。我们介绍了Ticket-Bench，这是一个在以任务为导向的方案中进行多语言代理评估的基准。售票台模拟了六种主要语言购买足球票的领域：葡萄牙，英语，西班牙语，德语，意大利语和法语。使用本地化的团队，城市和用户资料来提供更高水平的现实主义。我们评估了广泛的商业和开源LLM，测量功能的准确性和跨语言的一致性。结果表明，面向推理的模型（例如GPT-5，QWEN3-235B）主导了性能，但仍然表现出显着的跨语性差异。这些发现强调了对具有文化意识的多语言基准的需求，以指导强大的LLM代理的发展。</li>
</ul>

<h3>Title: Estimating Semantic Alphabet Size for LLM Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Lucas H. McCabe, Rimon Melamed, Thomas Hartvigsen, H. Howie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14478">https://arxiv.org/abs/2509.14478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14478">https://arxiv.org/pdf/2509.14478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14478]] Estimating Semantic Alphabet Size for LLM Uncertainty Quantification(https://arxiv.org/abs/2509.14478)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Many black-box techniques for quantifying the uncertainty of large language models (LLMs) rely on repeated LLM sampling, which can be computationally expensive. Therefore, practical applicability demands reliable estimation from few samples. Semantic entropy (SE) is a popular sample-based uncertainty estimator with a discrete formulation attractive for the black-box setting. Recent extensions of semantic entropy exhibit improved LLM hallucination detection, but do so with less interpretable methods that admit additional hyperparameters. For this reason, we revisit the canonical discrete semantic entropy estimator, finding that it underestimates the "true" semantic entropy, as expected from theory. We propose a modified semantic alphabet size estimator, and illustrate that using it to adjust discrete semantic entropy for sample coverage results in more accurate semantic entropy estimation in our setting of interest. Furthermore, our proposed alphabet size estimator flags incorrect LLM responses as well or better than recent top-performing approaches, with the added benefit of remaining highly interpretable.</li>
<li><strong>摘要：</strong>许多用于量化大语言模型（LLMS）不确定性的黑盒技术都取决于重复的LLM采样，这在计算上可能很昂贵。因此，实际适用性需要从几个样本中进行可靠的估计。语义熵（SE）是一种流行的基于样本的不确定性估计器，具有对黑盒设置有吸引力的离散配方。语义熵的最新扩展表现出改善的LLM幻觉检测，但采用较低的可解释方法来吸收其他超参数。因此，我们重新审视规范离散语义熵估计器，发现它低估了理论所预期的“真实”语义熵。我们提出了一个修改的语义字母大小估计器，并说明使用它来调整离散语义熵以进行样品覆盖，从而在我们感兴趣的设置中导致更准确的语义熵估计。此外，我们提出的字母尺寸估计量标志不正确的LLM响应也比最近表现最好的方法更好，或者是保持高度易于解释的额外好处。</li>
</ul>

<h3>Title: Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Weiting Tan, Xinghua Qu, Ming Tu, Meng Ge, Andy T. Liu, Philipp Koehn, Lu Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14480">https://arxiv.org/abs/2509.14480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14480">https://arxiv.org/pdf/2509.14480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14480]] Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents(https://arxiv.org/abs/2509.14480)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.</li>
<li><strong>摘要：</strong>有效的交互式工具使用需要代理来掌握工具集成推理（TIR）：一个复杂的过程，涉及多转弯计划和长篇文化对话管理。为了训练代理，尤其是在多模式环境中，我们引入了一个用于增强学习的沙盒环境（RL），该环境支持交织的语音文本推出。我们的核心策略，转向裁定的强化学习（TARL），通过采用大型语言模型（LLM）作为法官来提供转向级别的评估，以解决长期任务中的信用分配的挑战。为了增强探索，我们将混合任务培训课程与数学推理问题相结合。与强RL基准相比，这种统一方法将基于文本的$ \ tau $ bbench上的任务通过率提高了6％以上。至关重要的是，我们展示了我们的框架适合微调代理任务的多模式基础模型。通过在交织的语音文本推出上训练基本的多模式LLM，我们为其配备了工具使用能力，为更自然的语音互动剂铺平了道路。</li>
</ul>

<h3>Title: Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification</h3>
<ul>
<li><strong>Authors: </strong>Samuel J. Bell, Eduardo Sánchez, David Dale, Pontus Stenetorp, Mikel Artetxe, Marta R. Costa-jussà</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14493">https://arxiv.org/abs/2509.14493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14493">https://arxiv.org/pdf/2509.14493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14493]] Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification(https://arxiv.org/abs/2509.14493)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multilingual toxicity detection remains a significant challenge due to the scarcity of training data and resources for many languages. While prior work has leveraged the translate-test paradigm to support cross-lingual transfer across a range of classification tasks, the utility of translation in supporting toxicity detection at scale remains unclear. In this work, we conduct a comprehensive comparison of translation-based and language-specific/multilingual classification pipelines. We find that translation-based pipelines consistently outperform out-of-distribution classifiers in 81.3% of cases (13 of 16 languages), with translation benefits strongly correlated with both the resource level of the target language and the quality of the machine translation (MT) system. Our analysis reveals that traditional classifiers outperform large language model (LLM) judges, with this advantage being particularly pronounced for low-resource languages, where translate-classify methods dominate translate-judge approaches in 6 out of 7 cases. We additionally show that MT-specific fine-tuning on LLMs yields lower refusal rates compared to standard instruction-tuned models, but it can negatively impact toxicity detection accuracy for low-resource languages. These findings offer actionable guidance for practitioners developing scalable multilingual content moderation systems.</li>
<li><strong>摘要：</strong>由于许多语言的培训数据和资源缺乏培训数据和资源，多语言毒性检测仍然是一个重大挑战。尽管先前的工作利用了翻译测试范式来支持一系列分类任务的跨语性转移，但翻译在大规模支持毒性检测方面的实用性尚不清楚。在这项工作中，我们对基于翻译和语言的/多语言分类管道进行了全面比较。我们发现，基于翻译的管道在81.3％的案例中（16种语言中的13个）始终优于分布分类器的分类器，而翻译效益与目标语言的资源水平和机器翻译（MT）系统的资源水平密切相关。我们的分析表明，传统的分类器的表现要优于大型语言模型（LLM）法官，而这种优势对于低资源语言特别明显，在7个案例中，有6个案例中的翻译分类方法主导了翻译法官的方法。我们还表明，与标准指令调节的模型相比，LLMS上MT特定的微调会产生较低的拒绝率，但它可能会对低资源语言的毒性检测准确性产生负面影响。这些发现为从业人员开发可扩展的多语言内容审核系统提供了可行的指导。</li>
</ul>

<h3>Title: Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Roman Kovalchuk, Mariana Romanyshyn, Petro Ivaniuk</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14504">https://arxiv.org/abs/2509.14504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14504">https://arxiv.org/pdf/2509.14504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14504]] Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction(https://arxiv.org/abs/2509.14504)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了Omnigec，这是一系列多语种银色标准数据集，用于语法错误校正任务（GEC），涵盖11种语言：捷克，英语，爱沙尼亚语，德语，希腊语，希腊语，冰岛，冰岛语，意大利语，意大利语，拉脱维安，斯洛文斯，瑞典语，瑞典语和乌克兰人。这些数据集促进了多语言GEC解决方案的开发，并有助于弥合数据差距，以使英语GEC解决方案适应多语言GEC。数据集中的文本源于三个来源：维基百科编辑11种目标语言，从11种目标语言中的Reddit的子列表以及仅乌克兰的UberText 2.0社交媒体语料库。尽管Wikipedia编辑是从人制造的校正得出的，但使用GPT-4O-MINI模型自动校正了Reddit和UberText 2.0数据。自动和手动评估了数据集中校正的质量。最后，我们在多语言Omnigec Corpora上微调了两个开源大型语言模型-Aya-Expanse（8b）和Gemma-3（12b） - 并实现了段落级别多语言GEC的最新结果（SOTA）结果。在拥抱脸上可以使用数据集集合和表现最佳的模型。</li>
</ul>

<h3>Title: From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Chen, Haoyuan Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14515">https://arxiv.org/abs/2509.14515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14515">https://arxiv.org/pdf/2509.14515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14515]] From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language Models(https://arxiv.org/abs/2509.14515)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>True Full-Duplex (TFD) voice communication--enabling simultaneous listening and speaking with natural turn-taking, overlapping speech, and interruptions--represents a critical milestone toward human-like AI interaction. This survey comprehensively reviews Full-Duplex Spoken Language Models (FD-SLMs) in the LLM era. We establish a taxonomy distinguishing Engineered Synchronization (modular architectures) from Learned Synchronization (end-to-end architectures), and unify fragmented evaluation approaches into a framework encompassing Temporal Dynamics, Behavioral Arbitration, Semantic Coherence, and Acoustic Performance. Through comparative analysis of mainstream FD-SLMs, we identify fundamental challenges: synchronous data scarcity, architectural divergence, and evaluation gaps, providing a roadmap for advancing human-AI communication.</li>
<li><strong>摘要：</strong>真正的全双工（TFD）语音交流 - 以自然的转折，重叠的言语和中断的方式同时聆听和讲话 - 代表着对类似人类的AI相互作用的关键里程碑。这项调查全面回顾了LLM时代的全双工口语模型（FD-SLMS）。我们建立了一种分类法，将工程的同步（模块化体系结构）与学习的同步（端到端体系结构）（模块化体系结构）（模块化体系结构）（端到端体系结构）建立，并将零散的评估方法统一到涵盖时间动态，行为仲裁，语义连贯性和声学性能的框架中。通过对主流FD-SLM的比较分析，我们确定了基本挑战：同步数据稀缺，建筑差异和评估差距，为推进人类交流的路线图提供了路线图。</li>
</ul>

<h3>Title: Delta Knowledge Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yihan Cao, Yanbin Kang, Zhengming Xing, Ruijie Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14526">https://arxiv.org/abs/2509.14526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14526">https://arxiv.org/pdf/2509.14526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14526]] Delta Knowledge Distillation for Large Language Models(https://arxiv.org/abs/2509.14526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is a widely adopted approach for compressing large neural networks by transferring knowledge from a large teacher model to a smaller student model. In the context of large language models, token level KD, typically minimizing the KL divergence between student output distribution and teacher output distribution, has shown strong empirical performance. However, prior work assumes student output distribution and teacher output distribution share the same optimal representation space, a premise that may not hold in many cases. To solve this problem, we propose Delta Knowledge Distillation (Delta-KD), a novel extension of token level KD that encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift Delta introduced during the teacher's supervised finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD substantially improves student performance while preserving more of the teacher's knowledge.</li>
<li><strong>摘要：</strong>知识蒸馏（KD）是一种通过将知识从大型教师模型转移到较小的学生模型来压缩大型神经网络的广泛采用方法。在大型语言模型的背景下，令牌KD级别通常可以最大程度地减少学生成果分布和教师成果分布之间的差异，并显示出强烈的经验表现。但是，先前的工作假设学生的产出分布和教师产出分布共享相同的最佳表示空间，这一前提在许多情况下可能不存在。为了解决这个问题，我们提出了Delta知识蒸馏（Delta-KD），这是令牌KD的新型扩展，鼓励学生通过明确保留在教师受监督的Finetunting（SFT）期间明确保留分配分配的三角洲（SFT）来近似最佳代表空间。关于胭脂指标的经验结果表明，Delta KD可以大大提高学生的绩效，同时保留更多教师的知识。</li>
</ul>

<h3>Title: Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors</h3>
<ul>
<li><strong>Authors: </strong>Zhengxiang Wang, Nafis Irtiza Tripto, Solha Park, Zhenzhen Li, Jiawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14543">https://arxiv.org/abs/2509.14543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14543">https://arxiv.org/pdf/2509.14543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14543]] Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors(https://arxiv.org/abs/2509.14543)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into personal writing tools, a critical question arises: can LLMs faithfully imitate an individual's writing style from just a few examples? Personal style is often subtle and implicit, making it difficult to specify through prompts yet essential for user-aligned generation. This work presents a comprehensive evaluation of state-of-the-art LLMs' ability to mimic personal writing styles via in-context learning from a small number of user-authored samples. We introduce an ensemble of complementary metrics-including authorship attribution, authorship verification, style matching, and AI detection-to robustly assess style imitation. Our evaluation spans over 40000 generations per model across domains such as news, email, forums, and blogs, covering writing samples from more than 400 real-world authors. Results show that while LLMs can approximate user styles in structured formats like news and email, they struggle with nuanced, informal writing in blogs and forums. Further analysis on various prompting strategies such as number of demonstrations reveal key limitations in effective personalization. Our findings highlight a fundamental gap in personalized LLM adaptation and the need for improved techniques to support implicit, style-consistent generation. To aid future research and for reproducibility, we open-source our data and code.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）越来越多地集成到个人写作工具中，因此出现了一个关键的问题：LLM可以忠实地从几个示例中模仿个人的写作风格吗？个人风格通常是微妙而隐性的，因此很难通过提示来指定，但对于用户一致的生成必不可少。这项工作介绍了最先进的LLMS通过在少数用户创作的样本中学习模仿个人写作样式的能力的全面评估。我们介绍了包括作者归因，作者身份验证，样式匹配和AI检测的互补指标的合奏，以评估风格模仿。我们的评估跨越了新闻，电子邮件，论坛和博客等领域的每个模型超过40000代，涵盖了400多个现实世界作者的撰写样本。结果表明，尽管LLM可以以新闻和电子邮件等结构化格式近似用户样式，但他们在博客和论坛上的细微差别，非正式的写作挣扎。对各种提示策略（例如示范数量）的进一步分析揭示了有效个性化的关键局限性。我们的发现突出了个性化LLM改编的根本差距，以及需要改进的技术来支持隐式，风格一致的一代。为了帮助未来的研究和可重复性，我们开源数据和代码。</li>
</ul>

<h3>Title: Controlling Language Difficulty in Dialogues with Linguistic Features</h3>
<ul>
<li><strong>Authors: </strong>Shuyao Xu, Wenguang Wang, Handong Gao, Wei Kang, Long Qin, Weizhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14545">https://arxiv.org/abs/2509.14545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14545">https://arxiv.org/pdf/2509.14545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14545]] Controlling Language Difficulty in Dialogues with Linguistic Features(https://arxiv.org/abs/2509.14545)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as powerful tools for supporting second language acquisition, particularly in simulating interactive dialogues for speaking practice. However, adapting the language difficulty of LLM-generated responses to match learners' proficiency levels remains a challenge. This work addresses this issue by proposing a framework for controlling language proficiency in educational dialogue systems. Our approach leverages three categories of linguistic features, readability features (e.g., Flesch-Kincaid Grade Level), syntactic features (e.g., syntactic tree depth), and lexical features (e.g., simple word ratio), to quantify and regulate text complexity. We demonstrate that training LLMs on linguistically annotated dialogue data enables precise modulation of language proficiency, outperforming prompt-based methods in both flexibility and stability. To evaluate this, we introduce Dilaprix, a novel metric integrating the aforementioned features, which shows strong correlation with expert judgments of language difficulty. Empirical results reveal that our approach achieves superior controllability of language proficiency while maintaining high dialogue quality.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已成为支持第二语言获取的强大工具，尤其是在模拟语言实践的交互式对话中。但是，适应LLM生成的响应的语言难度以匹配学习者的能力水平仍然是一个挑战。这项工作通过提出一个控制教育对话系统语言能力的框架来解决此问题。我们的方法利用了三类语言特征，可读性特征（例如Flesch-Kincaid等级），句法特征（例如，句法树的深度）和词汇特征（例如，简单的单词比率），以量化和调节文本复杂性。我们证明，对语言注释的对话数据进行培训LLM可以精确调制语言能力，在灵活性和稳定性方面都优于基于及时的方法。为了评估这一点，我们介绍了DiLaprix，这是一种整合上述特征的新型指标，该指标与语言难度的专家判断显示了很强的相关性。经验结果表明，我们的方法在保持较高的对话质量的同时，实现了语言能力的卓越可控性。</li>
</ul>

<h3>Title: Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seungjun Yi, Joakim Nguyen, Terence Lim, Andrew Well, Joseph Skrovan, Mehak Beri, YongGeon Lee, Kavita Radhakrishnan, Liu Leqi, Mia Markey, Ying Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14597">https://arxiv.org/abs/2509.14597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14597">https://arxiv.org/pdf/2509.14597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14597]] Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models(https://arxiv.org/abs/2509.14597)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This position paper examines how large language models (LLMs) can support thematic analysis of unstructured clinical transcripts, a widely used but resource-intensive method for uncovering patterns in patient and provider narratives. We conducted a systematic review of recent studies applying LLMs to thematic analysis, complemented by an interview with a practicing clinician. Our findings reveal that current approaches remain fragmented across multiple dimensions including types of thematic analysis, datasets, prompting strategies and models used, most notably in evaluation. Existing evaluation methods vary widely (from qualitative expert review to automatic similarity metrics), hindering progress and preventing meaningful benchmarking across studies. We argue that establishing standardized evaluation practices is critical for advancing the field. To this end, we propose an evaluation framework centered on three dimensions: validity, reliability, and interpretability.</li>
<li><strong>摘要：</strong>该立场论文探讨了大型语言模型（LLM）如何支持非结构化临床成绩单的主题分析，这是一种广泛使用但资源密集型的方法，用于发现患者和提供者叙事中的模式。我们对将LLMS应用于主题分析的最新研究进行了系统的综述，并接受了与执业临床医生的访谈。我们的发现表明，当前的方法在多个维度上保持分散，包括主题分析的类型，数据集，提示所使用的策略和模型，最著名的是评估。现有的评估方法差异很大（从定性专家审查到自动相似性指标），阻碍进步并防止在整个研究中进行有意义的基准测试。我们认为，建立标准化的评估实践对于推进该领域至关重要。为此，我们提出了一个以三个维度为中心的评估框架：有效性，可靠性和解释性。</li>
</ul>

<h3>Title: Leveraging IndoBERT and DistilBERT for Indonesian Emotion Classification in E-Commerce Reviews</h3>
<ul>
<li><strong>Authors: </strong>William Christian, Daniel Adamlu, Adrian Yu, Derwin Suhartono</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14611">https://arxiv.org/abs/2509.14611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14611">https://arxiv.org/pdf/2509.14611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14611]] Leveraging IndoBERT and DistilBERT for Indonesian Emotion Classification in E-Commerce Reviews(https://arxiv.org/abs/2509.14611)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Understanding emotions in the Indonesian language is essential for improving customer experiences in e-commerce. This study focuses on enhancing the accuracy of emotion classification in Indonesian by leveraging advanced language models, IndoBERT and DistilBERT. A key component of our approach was data processing, specifically data augmentation, which included techniques such as back-translation and synonym replacement. These methods played a significant role in boosting the model's performance. After hyperparameter tuning, IndoBERT achieved an accuracy of 80\%, demonstrating the impact of careful data processing. While combining multiple IndoBERT models led to a slight improvement, it did not significantly enhance performance. Our findings indicate that IndoBERT was the most effective model for emotion classification in Indonesian, with data augmentation proving to be a vital factor in achieving high accuracy. Future research should focus on exploring alternative architectures and strategies to improve generalization for Indonesian NLP tasks.</li>
<li><strong>摘要：</strong>了解印尼语言的情绪对于改善电子商务的客户体验至关重要。这项研究的重点是利用先进的语言模型，印度和德文伯特，提高印尼语中情感分类的准确性。我们方法的关键组成部分是数据处理，特别是数据增强，其中包括反向翻译和同义词替代的技术。这些方法在提高模型的性能方面发挥了重要作用。在高参数调整后，Indobert的精度为80 \％，证明了仔细的数据处理的影响。在结合多个印度近代模型的同时，它略有改进，但并未显着提高性能。我们的发现表明，印尼是印尼人情绪分类的最有效模型，数据增强被证明是实现高准确性的重要因素。未来的研究应着重于探索替代架构和策略，以改善印尼NLP任务的概括。</li>
</ul>

<h3>Title: Reveal and Release: Iterative LLM Unlearning with Self-generated Data</h3>
<ul>
<li><strong>Authors: </strong>Linxi Xie, Xin Teng, Shichang Ke, Hongyi Wen, Shengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14624">https://arxiv.org/abs/2509.14624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14624">https://arxiv.org/pdf/2509.14624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14624]] Reveal and Release: Iterative LLM Unlearning with Self-generated Data(https://arxiv.org/abs/2509.14624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) unlearning has demonstrated effectiveness in removing the influence of undesirable data (also known as forget data). Existing approaches typically assume full access to the forget dataset, overlooking two key challenges: (1) Forget data is often privacy-sensitive, rare, or legally regulated, making it expensive or impractical to obtain (2) The distribution of available forget data may not align with how that information is represented within the model. To address these limitations, we propose a ``Reveal-and-Release'' method to unlearn with self-generated data, where we prompt the model to reveal what it knows using optimized instructions. To fully utilize the self-generated forget data, we propose an iterative unlearning framework, where we make incremental adjustments to the model's weight space with parameter-efficient modules trained on the forget data. Experimental results demonstrate that our method balances the tradeoff between forget quality and utility preservation.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）未学习在消除不良数据的影响（也称为忘记数据）方面表现出了有效性。现有方法通常会完全访问《忘记数据集》，俯瞰两个关键挑战：（1）忘记数据通常对隐私敏感，稀有或法律调节，因此获得（2）可用忘记数据的分布可能与模型中的该信息表示不符。为了解决这些限制，我们提出了一种``揭示和释放''方法，以使用自我生成的数据进行学习，我们提示该模型使用优化的说明来揭示其知道的知识。为了充分利用自我生成的忘记数据，我们提出了一个迭代的学习框架，在其中使用对忘记数据训练的参数效率高效模块对模型的重量空间进行增量调整。实验结果表明，我们的方法平衡了忘记质量和效用保存之间的权衡。</li>
</ul>

<h3>Title: SWE-QA: Can Language Models Answer Repository-level Code Questions?</h3>
<ul>
<li><strong>Authors: </strong>Weihan Peng, Yuling Shi, Yuhang Wang, Xinyun Zhang, Beijun Shen, Xiaodong Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14635">https://arxiv.org/abs/2509.14635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14635">https://arxiv.org/pdf/2509.14635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14635]] SWE-QA: Can Language Models Answer Repository-level Code Questions?(https://arxiv.org/abs/2509.14635)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.</li>
<li><strong>摘要：</strong>了解和推理整个软件存储库是智能软件工程工具的重要功能。尽管CosQA和CodeQA等现有基准测试已经提高了该领域，但它们主要集中在小型的独立代码片段上。这些设置未能捕获现实世界存储库的复杂性，在这种情况下，有效的理解和推理通常需要导航多个文件，了解软件体系结构以及在远程代码依赖项中的接地答案。在本文中，我们介绍了SWE-QA，这是一个存储库级代码求解（QA）基准测试，旨在促进对现实代码环境中自动化质量检查系统的研究。 SWE-QA涉及576个高质量的提问对，包括意图理解，跨文件推理和多跳依赖性分析。为了构建SWE-QA，我们首先从11个流行存储库中爬了77,100个GitHub问题。基于对从这些问题提取的自然发生的开发人员问题的分析，我们开发了对存储库级问题的两级分类法，并为每个类别构建了一组种子问题。对于每个类别，我们手动策划和验证了问题，并收集了相应的答案。作为原型应用程序，我们进一步开发了SWE-QA-Agent，这是一个代理框架，其中LLM代理并采取行动自动找到答案。我们在各种环境增强策略下评估了SWE-QA的六个高级LLM。实验结果凸显了LLM，尤其是我们的SWE-QA代理框架的承诺，在解决存储库级别的质量质量质量质量方面，同时还揭示了公开的挑战并指出了未来的研究方向。</li>
</ul>

<h3>Title: MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siyu Yan, Long Zeng, Xuecheng Wu, Chengcheng Han, Kongcheng Zhang, Chong Peng, Xuezhi Cao, Xunliang Cai, Chenjuan Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14651">https://arxiv.org/abs/2509.14651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14651">https://arxiv.org/pdf/2509.14651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14651]] MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models(https://arxiv.org/abs/2509.14651)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models~(LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>随着大型语言模型〜（LLM）被广泛采用，确保其与人类价值观的一致性对于防止对手操纵模型以产生有害内容的越狱至关重要。尽管大多数防御措施针对单转攻击，但现实世界的使用通常涉及多转对话，将模型暴露于攻击中，以利用对话上下文绕过安全措施的攻击。我们介绍Muse，这是一个全面的框架，可解决攻击和防御角度的多转弯越狱。对于攻击，我们提出了Muse-A，一种使用框架语义和启发式树搜索来探索各种语义轨迹的方法。为了进行防御，我们提出了Muse-D，这是一种细粒度的安全对准方法，在对话初期介入以减少脆弱性。各种模型的广泛实验表明，Muse有效地识别并减轻了多转弯漏洞。代码可在\ href {此https url} {this https url}中获得。</li>
</ul>

<h3>Title: TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xiaobo Xing, Wei Yuan, Tong Chen, Quoc Viet Hung Nguyen, Xiangliang Zhang, Hongzhi Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14671">https://arxiv.org/abs/2509.14671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14671">https://arxiv.org/pdf/2509.14671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14671]] TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding(https://arxiv.org/abs/2509.14671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: this https URL</li>
<li><strong>摘要：</strong>从表格数据中对语义和结构信息进行建模仍然是有效桌子理解的核心挑战。现有的表格文本方法可以使大语模型（LLMS）的表面平坦，但失去了关键的结构提示，而表图像的方法可以保留结构，但在精细的语义上挣扎。最近的桌面策略试图结合文本和视觉视图，但是（1）它们（1）在大型多模式LLMS（MLLMS）中的每个查询表中静态处理两种方式，不可避免地引入了冗余甚至冲突，并且（2）取决于MLLMS的成本成本。鉴于此，我们提出了TableDart，这是一个训练有效的框架，通过重复使用预验证的单模式模型来整合多模式视图。 TableDart引入了一个轻巧的259万参数MLP门控网络，该网络动态选择每个表格对的最佳路径（仅文本，仅图像，仅图像或融合），从而有效地降低了两种方式的冗余和冲突。此外，我们提出了一种新颖的代理，通过分析基于文本和图像的模型的输出，选择最佳结果或通过推理综合新答案来介导跨模式知识的整合。该设计避免了全MLLM微调的高昂成本。对七个基准测试的广泛实验表明，TableDart在开源型号中建立了新的最先进的性能，使最大的基线平均超过4.02％。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: From Ground Trust to Truth: Disparities in Offensive Language Judgments on Contemporary Korean Political Discourse</h3>
<ul>
<li><strong>Authors: </strong>Seunguk Yu, Jungmin Yun, Jinhee Jang, Youngbin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14712">https://arxiv.org/abs/2509.14712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14712">https://arxiv.org/pdf/2509.14712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14712]] From Ground Trust to Truth: Disparities in Offensive Language Judgments on Contemporary Korean Political Discourse(https://arxiv.org/abs/2509.14712)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Although offensive language continually evolves over time, even recent studies using LLMs have predominantly relied on outdated datasets and rarely evaluated the generalization ability on unseen texts. In this study, we constructed a large-scale dataset of contemporary political discourse and employed three refined judgments in the absence of ground truth. Each judgment reflects a representative offensive language detection method and is carefully designed for optimal conditions. We identified distinct patterns for each judgment and demonstrated tendencies of label agreement using a leave-one-out strategy. By establishing pseudo-labels as ground trust for quantitative performance assessment, we observed that a strategically designed single prompting achieves comparable performance to more resource-intensive methods. This suggests a feasible approach applicable in real-world settings with inherent constraints.</li>
<li><strong>摘要：</strong>尽管进攻性语言会随着时间的流逝不断发展，但即使是使用LLM的最新研究也主要依赖于过时的数据集，并且很少评估看不见的文本的概括能力。在这项研究中，我们构建了当代政治话语的大规模数据集，并在没有地面真理的情况下采取了三项精致的判断。每个判断都反映了一种代表性的进攻性语言检测方法，并经过精心设计，以适合最佳条件。我们为每个判断确定了不同的模式，并使用遗留策略证明了标签协议的趋势。通过建立伪标签作为定量绩效评估的基础信任，我们观察到，战略性设计的单一提示可以实现与更含有资源密集型方法的可比性。这表明一种适用于具有固有约束的现实世界中的可行方法。</li>
</ul>

<h3>Title: Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM</h3>
<ul>
<li><strong>Authors: </strong>Chenkun Tan, Pengyu Wang, Shaojun Zhou, Botian Jiang, Zhaowei Li, Dong Zhang, Xinghao Wang, Yaqian Zhou, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14735">https://arxiv.org/abs/2509.14735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14735">https://arxiv.org/pdf/2509.14735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14735]] Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM(https://arxiv.org/abs/2509.14735)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have gained significant attention due to their impressive ability to integrate vision and language modalities. Recent advancements in MLLMs have primarily focused on improving performance through high-quality datasets, novel architectures, and optimized training strategies. However, in this paper, we identify a previously overlooked issue, language prior conflict, a mismatch between the inherent language priors of large language models (LLMs) and the language priors in training datasets. This conflict leads to suboptimal vision-language alignment, as MLLMs are prone to adapting to the language style of training samples. To address this issue, we propose a novel training method called Decoupled Proxy Alignment (DPA). DPA introduces two key innovations: (1) the use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates the language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. Our method not only improves the effectiveness of MLLM training but also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>多模式大型语言模型（MLLM）由于令人印象深刻的视力和语言方式而受到了极大的关注。 MLLM的最新进展主要集中在通过高质量的数据集，新型体系结构和优化的培训策略来提高性能。但是，在本文中，我们确定了先前被忽视的问题，语言先前的冲突，大语模型（LLMS）固有的语言先验与培训数据集中的语言先验之间的不匹配。这种冲突导致了次优的语言对齐，因为MLLM容易适应训练样本的语言风格。为了解决这个问题，我们提出了一种新型的培训方法，称为解耦代理对齐（DPA）。 DPA介绍了两个关键的创新：（1）在预处理过程中使用代理LLM，以将视觉语言对准过程与语言的先验干扰解脱，以及（2）基于视觉相关性的动态损失调整以增强视觉上相关令牌的优化信号。广泛的实验表明，DPA大大减轻了语言的冲突，从而在各种数据集，模型家族和量表上实现了卓越的一致性性能。我们的方法不仅提高了MLLM训练的有效性，而且还显示出出色的概括能力，使其成为视觉对齐的强大方法。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Wang, Shaojun Zhou, Chenkun Tan, Xinghao Wang, Wei Huang, Zhen Ye, Zhaowei Li, Botian Jiang, Dong Zhang, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14738">https://arxiv.org/abs/2509.14738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14738">https://arxiv.org/pdf/2509.14738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14738]] UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets(https://arxiv.org/abs/2509.14738)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Unified vision large language models (VLLMs) have recently achieved impressive advancements in both multimodal understanding and generation, powering applications such as visual question answering and text-guided image synthesis. However, progress in unified VLLMs remains constrained by the lack of datasets that fully exploit the synergistic potential between these two core abilities. Existing datasets typically address understanding and generation in isolation, thereby limiting the performance of unified VLLMs. To bridge this critical gap, we introduce a novel dataset construction framework, UnifiedVisual, and present UnifiedVisual-240K, a high-quality dataset meticulously designed to facilitate mutual enhancement between multimodal understanding and generation. UnifiedVisual-240K seamlessly integrates diverse visual and textual inputs and outputs, enabling comprehensive cross-modal reasoning and precise text-to-image alignment. Our dataset encompasses a wide spectrum of tasks and data sources, ensuring rich diversity and addressing key shortcomings of prior resources. Extensive experiments demonstrate that models trained on UnifiedVisual-240K consistently achieve strong performance across a wide range of tasks. Notably, these models exhibit significant mutual reinforcement between multimodal understanding and generation, further validating the effectiveness of our framework and dataset. We believe UnifiedVisual represents a new growth point for advancing unified VLLMs and unlocking their full potential. Our code and datasets is available at this https URL.</li>
<li><strong>摘要：</strong>统一的视力大语言模型（VLLM）最近在多模式理解和生成中取得了令人印象深刻的进步，诸如视觉问题答案和文本指导的图像合成之类的功能。但是，统一VLLM的进展仍然受到缺乏完全利用这两个核心能力之间协同潜力的数据集的限制。现有数据集通常孤立地解决理解和生成，从而限制了统一VLLM的性能。为了弥合这个关键的差距，我们引入了一个新颖的数据集构造框架，统一，并现在统一Visual-240k，这是一种精心设计的高质量数据集，旨在促进多模式理解和产生之间的相互增强。 Unified Visual-240k无缝地集成了各种视觉和文本输入和输出，从而实现了全面的跨模式推理和精确的文本对象对齐。我们的数据集涵盖了各种各样的任务和数据源，确保了丰富的多样性并解决了先前资源的主要缺点。广泛的实验表明，在UnifiedVisual-240K上训练的模型始终在各种任务中实现强劲的性能。值得注意的是，这些模型在多模式理解和生成之间表现出显着的相互加强，进一步验证了我们的框架和数据集的有效性。我们认为，统一是为了推进统一VLLM并释放其全部潜力的新增长点。我们的代码和数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Evaluating Large Language Models for Cross-Lingual Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Longfei Zuo, Pingjun Hong, Oliver Kraus, Barbara Plank, Robert Litschko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14749">https://arxiv.org/abs/2509.14749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14749">https://arxiv.org/pdf/2509.14749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14749]] Evaluating Large Language Models for Cross-Lingual Retrieval(https://arxiv.org/abs/2509.14749)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multi-stage information retrieval (IR) has become a widely-adopted paradigm in search. While Large Language Models (LLMs) have been extensively evaluated as second-stage reranking models for monolingual IR, a systematic large-scale comparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior work shows that LLM-based rerankers improve CLIR performance, their evaluation setup relies on lexical retrieval with machine translation (MT) for the first stage. This is not only prohibitively expensive but also prone to error propagation across stages. Our evaluation on passage-level and document-level CLIR reveals that further gains can be achieved with multilingual bi-encoders as first-stage retrievers and that the benefits of translation diminishes with stronger reranking models. We further show that pairwise rerankers based on instruction-tuned LLMs perform competitively with listwise rerankers. To the best of our knowledge, we are the first to study the interaction between retrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that, without MT, current state-of-the-art rerankers fall severely short when directly applied in CLIR.</li>
<li><strong>摘要：</strong>多阶段信息检索（IR）已成为搜索中广泛的范式。尽管大型语言模型（LLMS）已被广泛评估为单语IR的第二阶段重新疗法模型，但对于跨语义IR（CLIR）仍缺乏系统的大规模比较。此外，虽然先前的工作表明基于LLM的Rerankers提高了CLIR性能，但其评估设置依赖于第一阶段的机器翻译（MT）的词汇检索。这不仅非常昂贵，而且很容易在各个阶段进行错误传播。我们对通道级别和文档级别CLIR的评估表明，通过作为第一阶段检索器的多语言双重编码器可以取得进一步的收益，而翻译的好处则通过更强的重新骑行模型减少。我们进一步表明，基于指令调整的LLM的成对重读者与ListWise Rerankers竞争性地表现。据我们所知，我们是第一个在两阶段CLIR与LLMS中研究猎犬和rerankers之间的相互作用的人。我们的发现表明，没有MT，直接在Clir中直接应用时，当前的最新读者会严重短缺。</li>
</ul>

<h3>Title: KAIO: A Collection of More Challenging Korean Questions</h3>
<ul>
<li><strong>Authors: </strong>Nahyun Lee, Guijin Son, Hyunwoo Ko, Kyubeen Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14752">https://arxiv.org/abs/2509.14752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14752">https://arxiv.org/pdf/2509.14752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14752]] KAIO: A Collection of More Challenging Korean Questions(https://arxiv.org/abs/2509.14752)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>With the advancement of mid/post-training techniques, LLMs are pushing their boundaries at an accelerated pace. Legacy benchmarks saturate quickly (e.g., broad suites like MMLU over the years, newer ones like GPQA-D even faster), which makes frontier progress hard to track. The problem is especially acute in Korean: widely used benchmarks are fewer, often translated or narrow in scope, and updated more slowly, so saturation and contamination arrive sooner. Accordingly, at this moment, there is no Korean benchmark capable of evaluating and ranking frontier models. To bridge this gap, we introduce KAIO, a Korean, math-centric benchmark that stresses long-chain reasoning. Unlike recent Korean suites that are at or near saturation, KAIO remains far from saturated: the best-performing model, GPT-5, attains 62.8, followed by Gemini-2.5-Pro (52.3). Open models such as Qwen3-235B and DeepSeek-R1 cluster falls below 30, demonstrating substantial headroom, enabling robust tracking of frontier progress in Korean. To reduce contamination, KAIO will remain private and be served via a held-out evaluator until the best publicly known model reaches at least 80% accuracy, after which we will release the set and iterate to a harder version.</li>
<li><strong>摘要：</strong>随着中/后训练技术的发展，LLMS以加速的速度推动其界限。传统的基准迅速饱和（例如，多年来，像MMLU这样的宽敞套房，像GPQA-D这样的新套件甚至更快），这使得边境更加难以跟踪。在韩语中，问题尤其急切：广泛使用的基准较少，经常翻译或范围范围较窄，并且更新更慢，因此饱和和污染较早到来。因此，此刻，尚无韩国基准能够评估和排名前沿模型。为了弥合这一差距，我们介绍了韩国以数学为中心的基准Kaio强调长链推理。与最近饱和或接近饱和的韩国套房不同，Kaio远离饱和：表现最佳的型号GPT-5的速度为62.8，其次是Gemini-2.5-Pro（52.3）。 QWEN3-235B和DeepSeek-R1群集等开放型型号低于30，展示了大量的净空，从而可以对韩国的边境进步进行稳健的跟踪。为了减少污染，Kaio将保持私密，并通过持有的评估者提供服务，直到最公开的模型达到至少80％的准确性，然后我们将发布该集合并迭代到更难的版本。</li>
</ul>

<h3>Title: Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration</h3>
<ul>
<li><strong>Authors: </strong>Haoran Zhang, Yafu Li, Xuyang Hu, Dongrui Liu, Zhilin Wang, Bo Li, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14760">https://arxiv.org/abs/2509.14760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14760">https://arxiv.org/pdf/2509.14760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14760]] Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration(https://arxiv.org/abs/2509.14760)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）越来越多地应用于不同的现实世界中，每种情况都由用户或组织量身定制的定制行为和安全规范（SPEC）管辖。这些规格分为安全规格和行为规格，在各种情况下都不同，并且随着偏好和要求的变化而发展。我们将这一挑战正式为规范对齐，重点是LLMS从行为和安全角度遵循动态，特定方案规格的能力。为了应对这一挑战，我们提出了Align3，这是一种轻巧的方法，该方法采用测试时间审议（TTD），并进行层次反射和修订以在规范边界上进行推理。我们进一步介绍了Specbench，这是用于测量规范对齐的统一基准，涵盖了5个方案，103个规格和1,500个提示。对15种推理和18种具有多种TTD方法的指导模型进行的实验，包括自我refine，TPO和Morethink，得出三个关键的发现：（i）测试时间审议增强了规格对齐； （ii）Align3以最小的开销来提高安全性权衡的边界； （iii）规格有效地揭示了对齐差距。这些结果突出了测试时间审议的潜力，这是对现实世界规范界限推理的有效策略。</li>
</ul>

<h3>Title: ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance</h3>
<ul>
<li><strong>Authors: </strong>Hannah Sterz, Fabian David Schmidt, Goran Glavaš, Ivan Vulić</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14814">https://arxiv.org/abs/2509.14814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14814">https://arxiv.org/pdf/2509.14814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14814]] ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance(https://arxiv.org/abs/2509.14814)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As they become increasingly multilingual, Large Language Models (LLMs) exhibit more language confusion, i.e., they tend to generate answers in a language different from the language of the prompt or the answer language explicitly requested by the user. In this work, we propose ReCoVeR (REducing language COnfusion in VEctor Representations), a novel lightweight approach for reducing language confusion based on language-specific steering vectors. We first isolate language vectors with the help of multi-parallel corpus and then effectively leverage those vectors for effective LLM steering via fixed (i.e., unsupervised) as well as trainable steering functions. Our extensive evaluation, encompassing three benchmarks and 18 languages, shows that ReCoVeR effectively mitigates language confusion in both monolingual and cross-lingual setups while at the same time -- and in contrast to prior language steering methods -- retaining task performance. Our data code is available at this https URL.</li>
<li><strong>摘要：</strong>随着它们变得越来越多语言，大型语言模型（LLMS）表现出更多的语言混乱，即，它们倾向于以一种与提示语言或答案语言不同的语言产生答案。在这项工作中，我们建议恢复（减少向量表示中的语言混乱），这是一种基于语言特定的转向向量减少语言混乱的新型轻量级方法。我们首先在多平行语料库的帮助下隔离语言向量，然后有效利用这些向量通过固定（即无监督）以及可训练的转向功能来有效地转向LLM。我们广泛的评估包括三种基准和18种语言，表明恢复有效地减轻了单语言和跨语言设置中的语言混乱，同时 - 与先前的语言转向方法相反 - 保留任务性能。我们的数据代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring</h3>
<ul>
<li><strong>Authors: </strong>Jinhee Jang, Ayoung Moon, Minkyoung Jung, YoungBin Kim. Seung Jin Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14834">https://arxiv.org/abs/2509.14834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14834">https://arxiv.org/pdf/2509.14834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14834]] LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring(https://arxiv.org/abs/2509.14834)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods.</li>
<li><strong>摘要：</strong>大语言模型（LLM）的出现为自动化论文评分（AES）带来了新的范式，这是自然语言处理在教育中的长期且实用的应用。但是，实现人级的多观点理解和判断仍然是一个挑战。在这项工作中，我们提出了圆桌会议评分（RES），这是一个多代理评估框架，旨在在零照片的设置下执行精确且与人类一致的评分。 RES构造基于LLM的评估器代理，每个代理都针对特定的提示和主题上下文量身定制。每个代理都独立生成基于性状的标题，并进行多观点评估。然后，通过模拟圆桌会议的讨论，可以通过辩证的推理过程巩固单个评估，以产生最终的整体得分，与人类评估更加一致。通过在具有多种评估观点的代理商之间启用协作和共识，可以优于先前零射击AES的方法。使用Chatgpt和Claude在ASAP数据集上进行的实验表明，与直接提示（香草）方法相比，平均QWK的平均QWK提高了34.86％。</li>
</ul>

<h3>Title: V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qidong Wang, Junjie Hu, Ming Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14837">https://arxiv.org/abs/2509.14837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14837">https://arxiv.org/pdf/2509.14837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14837]] V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models(https://arxiv.org/abs/2509.14837)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advances in causal interpretability have extended from language models to vision-language models (VLMs), seeking to reveal their internal mechanisms through input interventions. While textual interventions often target semantics, visual interventions typically rely on coarse pixel-level perturbations, limiting semantic insights on multimodal integration. In this study, we introduce V-SEAM, a novel framework that combines Visual Semantic Editing and Attention Modulating for causal interpretation of VLMs. V-SEAM enables concept-level visual manipulations and identifies attention heads with positive or negative contributions to predictions across three semantic levels: objects, attributes, and relationships. We observe that positive heads are often shared within the same semantic level but vary across levels, while negative heads tend to generalize broadly. Finally, we introduce an automatic method to modulate key head embeddings, demonstrating enhanced performance for both LLaVA and InstructBLIP across three diverse VQA benchmarks. Our data and code are released at: this https URL.</li>
<li><strong>摘要：</strong>因果解释性的最新进展已从语言模型扩展到视觉语言模型（VLM），试图通过输入干预措施揭示其内部机制。虽然文本干预措施通常针对语义，但视觉干预措施通常依赖于粗像素级扰动，从而限制了对多模式集成的语义见解。在这项研究中，我们介绍了V-Seam，这是一个新型框架，结合了视觉语义编辑和注意力调节VLM的因果解释。 v-seam启用概念级的视觉操作，并确定对三个语义级别预测的正面或负面贡献的注意力头：对象，属性和关系。我们观察到，正面通常在相同的语义层面上共享，但在层次之间有所不同，而负面的头部往往会广泛概括。最后，我们引入了一种自动方法来调节钥匙头的嵌入，从而在三种不同的VQA基准测试中证明了LLAVA和指令的性能提高。我们的数据和代码发布在以下位置：此HTTPS URL。</li>
</ul>

<h3>Title: Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support</h3>
<ul>
<li><strong>Authors: </strong>Xianrong Yao, Dong She, Chenxu Zhang, Yimeng Zhang, Yueru Sun, Noman Ahmed, Yang Gao, Zhanpeng Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14851">https://arxiv.org/abs/2509.14851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14851">https://arxiv.org/pdf/2509.14851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14851]] Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support(https://arxiv.org/abs/2509.14851)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker's emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE's reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.</li>
<li><strong>摘要：</strong>同理心对于有效的心理健康支持至关重要，尤其是在解决长期咨询文本（LCT）时。但是，现有的大语言模型（LLM）通常会产生语义上流利的答复，但缺乏真正的心理支持所需的结构性推理，尤其是在中国背景下。为了弥合这一差距，我们引入了Ensathy-R1，这是一个新颖的框架，将同情链（COE）推理过程与增强学习（RL）集成在一起，以增强LCT的响应质量。受认知行为疗法的启发，我们的COE范式指导该模型，以依次推荐寻求帮助者的情绪，原因和意图，从而使其思维过程既透明又可以解释。我们的框架由新的大规模中国数据集，同理心QA和两个阶段的培训过程授权。首先，监督的微调灌输了COE的推理结构。随后，在专门的奖励模型的指导下，RL完善了最终响应的治疗相关性和上下文适当性。实验表明，Ensathy-R1在关键自动指标上实现了强劲的性能。更重要的是，人类评估证实了其优势，表明对强质基线的偏好明显偏好，并在我们的新基准中获得了44.30％的胜利。通过启用可解释的和上下文细微的响应，同理心R1代表了发展负责任和真正有益的心理健康支持AI方面的重大进步。</li>
</ul>

<h3>Title: Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens</h3>
<ul>
<li><strong>Authors: </strong>Issa Sugiura, Shuhei Kurita, Yusuke Oda, Ryuichiro Higashinaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14882">https://arxiv.org/abs/2509.14882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14882">https://arxiv.org/pdf/2509.14882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14882]] Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens(https://arxiv.org/abs/2509.14882)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We propose Llama-Mimi, a speech language model that uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi achieves state-of-the-art performance in acoustic consistency and possesses the ability to preserve speaker identity. Our analysis further demonstrates that increasing the number of quantizers improves acoustic fidelity but degrades linguistic performance, highlighting the inherent challenge of maintaining long-term coherence. We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs. Our models, code, and speech samples are publicly available.</li>
<li><strong>摘要：</strong>我们提出了Llama-Mimi，这是一种语音语言模型，该模型使用统一的令牌和单个变压器解码器来共同模拟交错的语义和声音令牌的序列。全面的评估表明，Llama-Mimi在声学一致性方面取得了最先进的表现，并具有保持说话者身份的能力。我们的分析进一步表明，增加量化器的数量可以提高声学保真度，但会降低语言性能，从而强调了保持长期连贯性的固有挑战。 We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs.我们的模型，代码和语音样本公开可用。</li>
</ul>

<h3>Title: A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ye Shen, Junying Wang, Farong Wen, Yijin Guo, Qi Jia, Zicheng Zhang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14886">https://arxiv.org/abs/2509.14886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14886">https://arxiv.org/pdf/2509.14886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14886]] A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation(https://arxiv.org/abs/2509.14886)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.</li>
<li><strong>摘要：</strong>多模式大语言模型（MLLM）的快速进步促使创建了许多基准。但是，常规的全面覆盖问题提问评估具有很高的冗余性和低效率。受到人类访谈过程的启发，我们提出了一个多到一的访谈范式，以进行有效的MLLM评估。我们的框架包括（i）两阶段的访谈策略，其中包含前视图和正式访谈阶段，（ii）对访调员的体重进行动态调整以确保公平性，以及（iii）问题难度级别选择的自适应机制。在不同基准上进行的实验表明，所提出的范式与全面覆盖结果的相关性明显高于随机抽样的相关性，而PLCC的提高最高为17.6％，而SRCC的相关性高达17.6％，同时减少了所需问题的数量。这些发现表明，所提出的范式为大规模MLLM基准测试提供了可靠，有效的替代方案。</li>
</ul>

<h3>Title: A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts</h3>
<ul>
<li><strong>Authors: </strong>Kian Tohidi, Kia Dashtipour, Simone Rebora, Sevda Pourfaramarz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14922">https://arxiv.org/abs/2509.14922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14922">https://arxiv.org/pdf/2509.14922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14922]] A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts(https://arxiv.org/abs/2509.14922)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study presents a comprehensive comparative evaluation of four state-of-the-art Large Language Models (LLMs)--Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o--for sentiment analysis and emotion detection in Persian social media texts. Comparative analysis among LLMs has witnessed a significant rise in recent years, however, most of these analyses have been conducted on English language tasks, creating gaps in understanding cross-linguistic performance patterns. This research addresses these gaps through rigorous experimental design using balanced Persian datasets containing 900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts for emotion detection (anger, fear, happiness, hate, sadness, surprise). The main focus was to allow for a direct and fair comparison among different models, by using consistent prompts, uniform processing parameters, and by analyzing the performance metrics such as precision, recall, F1-scores, along with misclassification patterns. The results show that all models reach an acceptable level of performance, and a statistical comparison of the best three models indicates no significant differences among them. However, GPT-4o demonstrated a marginally higher raw accuracy value for both tasks, while Gemini 2.0 Flash proved to be the most cost-efficient. The findings indicate that the emotion detection task is more challenging for all models compared to the sentiment analysis task, and the misclassification patterns can represent some challenges in Persian language texts. These findings establish performance benchmarks for Persian NLP applications and offer practical guidance for model selection based on accuracy, efficiency, and cost considerations, while revealing cultural and linguistic challenges that require consideration in multilingual AI system deployment.</li>
<li><strong>摘要：</strong>这项研究对四种最先进的大语模型（LLMS）进行了全面的比较评估-Claude 3.7十四行诗，DeepSeek-V3，Gemini 2.0 Flash和GPT-4O-在波斯语社交媒体文本中的情感分析和情感分析和情感检测。 LLMS之间的比较分析目睹了近年来的显着增长，但是，这些分析中的大多数是在英语任务上进行的，从而在理解跨语言绩效模式方面存在差距。这项研究通过使用平衡的波斯数据集来解决这些差距，其中包含900个文本用于情感分析（正面，负面，中性）和1,800条文本，以进行情感检测（愤怒，恐惧，恐惧，幸福，仇恨，悲伤，惊喜）。主要重点是通过使用一致的提示，统一的处理参数以及分析诸如精度，召回，F1得分以及错误分类模式的性能指标，允许不同模型之间进行直接和公平的比较。结果表明，所有模型都达到可接受的性能水平，最佳三个模型的统计比较表明它们之间没有显着差异。但是，GPT-4O在这两个任务中都显示出较高的原始精度值，而Gemini 2.0 Flash被证明是最具成本效益的。研究结果表明，与情感分析任务相比，情感检测任务对于所有模型都更具挑战性，并且错误分类模式可以代表波斯语文本中的一些挑战。这些发现为波斯NLP应用程序建立了绩效基准，并根据准确性，效率和成本注意事项为模型选择提供了实用的指导，同时揭示了在多语言AI系统部署中需要考虑的文化和语言挑战。</li>
</ul>

<h3>Title: Patent Language Model Pretraining with ModernBERT</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Yousefiramandi, Ciaran Cooney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14926">https://arxiv.org/abs/2509.14926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14926">https://arxiv.org/pdf/2509.14926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14926]] Patent Language Model Pretraining with ModernBERT(https://arxiv.org/abs/2509.14926)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.</li>
<li><strong>摘要：</strong>基于变压器的语言模型（例如BERT）已成为NLP的基础，但其性能在专利领域（如专利）中降低，其中包含长，技术和法律结构化的文本。事先使用专利NLP的方法主要依赖于微调通用模型或针对域适应的变体，这些变体曾经仔细研究了有限的数据。在这项工作中，我们使用Modernbert Architecture和精选的6000万次专利记录为专利的3种特定于领域的掩盖语言模型预算了专利。我们的方法结合了架构优化，包括闪光灯，旋转嵌入和GLU进料层。我们在四个下游专利分类任务上评估了模型。我们的模型Modernbert-Base-PT始终优于四个数据集中三个的通用现代基线，并通过基线专利专利公司实现了竞争性能。对Modernbert-Base-VX和Mosaic-Bert-Large进行的其他实验表明，扩展模型大小并自定义令牌仪进一步提高了所选任务的性能。值得注意的是，所有现代伯特变体的保留速度比PateNtbert的3倍更快地保留了推断，强调了它们对时间敏感应用的适用性。这些结果强调了针对专利的NLP任务的特定领域预训练和建筑改进的好处。</li>
</ul>

<h3>Title: Cross-Modal Knowledge Distillation for Speech Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Enzhi Wang, Qicheng Li, Zhiyuan Tang, Yuhang Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14930">https://arxiv.org/abs/2509.14930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14930">https://arxiv.org/pdf/2509.14930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14930]] Cross-Modal Knowledge Distillation for Speech Large Language Models(https://arxiv.org/abs/2509.14930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.</li>
<li><strong>摘要：</strong>在这项工作中，我们介绍了语音大语言模型中对灾难性遗忘和模式不等式的首次系统评估，表明即使输入保持文本，引入语音能力也会降低知识和推理，而绩效随着口头查询进一步降低。为了应对这些挑战，我们提出了一个跨模式知识蒸馏框架，该框架利用文本到文本和语音到文本渠道将知识从基于文本的教师模型转移到语音LLM。关于对话和音频理解任务的广泛实验验证了我们方法在维护文本知识，改善跨模式一致性以及增强基于语音互动中推理的有效性。</li>
</ul>

<h3>Title: Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts</h3>
<ul>
<li><strong>Authors: </strong>Alessandra Stramiglio, Andrea Schimmenti, Valentina Pasqual, Marieke van Erp, Francesco Sovrano, Fabio Vitali</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14943">https://arxiv.org/abs/2509.14943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14943">https://arxiv.org/pdf/2509.14943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14943]] Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts(https://arxiv.org/abs/2509.14943)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Text Implicitness has always been challenging in Natural Language Processing (NLP), with traditional methods relying on explicit statements to identify entities and their relationships. From the sentence "Zuhdi attends church every Sunday", the relationship between Zuhdi and Christianity is evident for a human reader, but it presents a challenge when it must be inferred automatically. Large language models (LLMs) have proven effective in NLP downstream tasks such as text comprehension and information extraction (IE). This study examines how textual implicitness affects IE tasks in pre-trained LLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of 10k implicit and explicit verbalization of biographic information to measure the impact on LLM performance and analyze whether fine-tuning implicit data improves their ability to generalize in implicit reasoning tasks. This research presents an experiment on the internal reasoning processes of LLMs in IE, particularly in dealing with implicit and explicit contexts. The results demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation) improves their performance in extracting information from implicit texts, contributing to better model interpretability and reliability.</li>
<li><strong>摘要：</strong>文本隐含性在自然语言处理（NLP）中一直具有挑战性，传统方法依赖于明确的陈述来识别实体及其关系。从“每个星期天的Zuhdi参加教堂”的判决中，Zuhdi和基督教之间的关系对于人类读者来说是显而易见的，但是当必须自动推断出它时，它提出了挑战。大型语言模型（LLMS）已被证明在NLP下游任务（例如文本理解和信息提取（IE））中有效。这项研究研究了文本隐含性如何影响IE在训练的LLMS中的任务：Llama 2.3，DeepSeekv1和Phi1.5。我们生成了两个合成数据集的10K隐式和显式的传记信息言语化，以衡量对LLM性能的影响，并分析微调隐式数据是否提高了其在隐式推理任务中的推广能力。这项研究介绍了IE中LLM的内部推理过程的实验，尤其是在处理隐式和明确的环境方面。结果表明，具有LORA（低排名适应性）的微调LLM模型在从隐性文本中提取信息方面提高了其性能，从而有助于更好的模型可解释性和可靠性。</li>
</ul>

<h3>Title: Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mario Sanz-Guerrero, Minh Duc Bui, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15020">https://arxiv.org/abs/2509.15020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15020">https://arxiv.org/pdf/2509.15020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15020]] Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs(https://arxiv.org/abs/2509.15020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string "Answer:" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model's confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results.</li>
<li><strong>摘要：</strong>在评估具有多项选择问题答案（MCQA）的大型语言模型（LLMS）时，通常会以字符串“答案：”结束提示，以促进通过下一步的概率促进自动化答案提取。但是，关于如何将结肠后的空间归为尚未达成共识，通常被视为一种微不足道的选择。在本文中，由于此（看似无关）的令牌变化以及改组模型排名，我们发现了高达11％的准确性差异，这引起了人们对LLM比较在先前工作中的可靠性的担忧。令人惊讶的是，我们能够推荐一种特定的策略 - 将空间与答案字母一起使用 - 我们观察到一致且统计学上显着的性能改善。此外，它改善了模型校准，增强了模型置信度估计的可靠性。我们的发现强调了仔细评估设计的重要性，并强调了对标准化的透明评估协议的需求，以确保可靠和可比较的结果。</li>
</ul>

<h3>Title: CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Thomas Huber, Christina Niklaus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15027">https://arxiv.org/abs/2509.15027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15027">https://arxiv.org/pdf/2509.15027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15027]] CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models(https://arxiv.org/abs/2509.15027)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.</li>
<li><strong>摘要：</strong>尽管LLM已被广泛研究了一般文本生成任务，但对文本重写，与一般文本生成有关的任务，尤其是在此任务上的模型行为相关的任务较少的研究。在本文中，我们分析了LLM在文本重写设置中的变化。我们专门关注论证文本及其改进，该任务名为“参数改进”（Argimp）。我们提出清晰的：一条评估管道，该管道由57个指标组成，映射到四个语言水平：词汇，句法，语义和务实。该管道用于在广泛的论证语料库中检查LLM划分论点的质量，并比较不同LLM在此任务上的行为，并在语言级别上分析了不同LLM在此任务上的行为。通过考虑所有四个语言级别，我们发现模型通过缩短文本而同时增加平均单词长度和合并句子来执行Argimp。总体而言，我们注意到说服力和连贯性维度有所增加。</li>
</ul>

<h3>Title: Value-Guided KV Compression for LLMs via Approximated CUR Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15038">https://arxiv.org/abs/2509.15038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15038">https://arxiv.org/pdf/2509.15038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15038]] Value-Guided KV Compression for LLMs via Approximated CUR Decomposition(https://arxiv.org/abs/2509.15038)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Key-value (KV) cache compression has emerged as a critical technique for reducing the memory and latency overhead of autoregressive language models during inference. Prior approaches predominantly rely on query-key attention scores to rank and evict cached tokens, assuming that attention intensity correlates with semantic importance. However, this heuristic overlooks the contribution of value vectors, which directly influence the attention output. In this paper, we propose CurDKV, a novel, value-centric KV compression method that selects keys and values based on leverage scores computed from CUR matrix decomposition. Our approach approximates the dominant subspace of the attention output $softmax(QK^T)V$, ensuring that the retained tokens best preserve the model's predictive behavior. Theoretically, we show that attention score approximation does not guarantee output preservation, and demonstrate that CUR-based selection minimizes end-to-end attention reconstruction loss. Empirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art methods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA and Mistral, while maintaining compatibility with FlashAttention and Grouped Query Attention. In addition to improved accuracy, CurDKV reduces generation latency by up to 40% at high compression, offering a practical speed-accuracy tradeoff.</li>
<li><strong>摘要：</strong>键值（KV）缓存压缩已成为一种关键技术，用于减少推理过程中自回归语言模型的内存和潜伏期开销。假设注意力强度与语义的重要性相关，则主要依靠查询键的注意力评分来排名和驱逐缓存的令牌。但是，这种启发式忽略了价值向量的贡献，该价值向量直接影响注意力输出。在本文中，我们提出了Curdkv，这是一种新颖的，价值为中心的KV压缩方法，该方法根据根据CUR矩阵分解计算出的杠杆分数选择键和值。我们的方法近似于注意力输出$ softmax（qk^t）v $的主要子空间，从而确保了保留的令牌最好保留模型的预测行为。从理论上讲，我们表明注意力评分近似不能保证产出，并证明基于CUR的选择可以最大程度地减少端到端注意力重建损失。从经验上讲，在对美洲驼和米斯特拉尔的激进压缩预算下，Curdkv的精度比Snapkv和Chunkkv（如SnapkV和Chunkkv）高出9.6％，同时保持了与闪光凸的兼容性和分组的查询关注。除了提高准确性外，CurdKV还可以在高压下将发电潜伏期降低多达40％，从而提供了实用的速度智能权衡折衷。</li>
</ul>

<h3>Title: Can maiBERT Speak for Maithili?</h3>
<ul>
<li><strong>Authors: </strong>Sumit Yadav, Raju Kumar Yadav, Utsav Maskey, Gautam Siddharth Kashyap Md Azizul Hoque, Ganesh Gautam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15048">https://arxiv.org/abs/2509.15048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15048">https://arxiv.org/pdf/2509.15048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15048]] Can maiBERT Speak for Maithili?(https://arxiv.org/abs/2509.15048)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Natural Language Understanding (NLU) for low-resource languages remains a major challenge in NLP due to the scarcity of high-quality data and language-specific models. Maithili, despite being spoken by millions, lacks adequate computational resources, limiting its inclusion in digital and AI-driven applications. To address this gap, we introducemaiBERT, a BERT-based language model pre-trained specifically for Maithili using the Masked Language Modeling (MLM) technique. Our model is trained on a newly constructed Maithili corpus and evaluated through a news classification task. In our experiments, maiBERT achieved an accuracy of 87.02%, outperforming existing regional models like NepBERTa and HindiBERT, with a 0.13% overall accuracy gain and 5-7% improvement across various classes. We have open-sourced maiBERT on Hugging Face enabling further fine-tuning for downstream tasks such as sentiment analysis and Named Entity Recognition (NER).</li>
<li><strong>摘要：</strong>由于缺乏高质量的数据和特定于语言的模型，因此低资源语言的自然语言理解（NLU）仍然是NLP的主要挑战。迈西莉（Maithili）尽管被数百万人说，但缺乏足够的计算资源，将其包含在数字和AI驱动应用中。为了解决这一差距，我们介绍了一种基于BERT的语言模型，该模型专门针对Maithili进行了使用蒙版语言建模（MLM）技术。我们的模型接受了新建的Maithili语料库的培训，并通过新闻分类任务进行了评估。在我们的实验中，Maibert的准确度达到了87.02％，表现优于尼泊尔特和印度底伯的现有区域模型，总体准确性增长了0.13％，各种类别的总体准确性增长率为5-7％。我们已经开源的Maibert拥抱面孔，为下游任务（例如情绪分析和指定实体识别（NER））提供了进一步的微调。</li>
</ul>

<h3>Title: LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyao Tu, Liang Zhang, Yujie Lin, Xin Lin, Haibo Zhang, Long Zhang, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15089">https://arxiv.org/abs/2509.15089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15089">https://arxiv.org/pdf/2509.15089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15089]] LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models(https://arxiv.org/abs/2509.15089)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The goal of open relation extraction (OpenRE) is to develop an RE model that can generalize to new relations not encountered during training. Existing studies primarily formulate OpenRE as a clustering task. They first cluster all test instances based on the similarity between the instances, and then manually assign a new relation to each cluster. However, their reliance on human annotation limits their practicality. In this paper, we propose an OpenRE framework based on large language models (LLMs), which directly predicts new relations for test instances by leveraging their strong language understanding and generation abilities, without human intervention. Specifically, our framework consists of two core components: (1) a relation discoverer (RD), designed to predict new relations for test instances based on \textit{demonstrations} formed by training instances with known relations; and (2) a relation predictor (RP), used to select the most likely relation for a test instance from $n$ candidate relations, guided by \textit{demonstrations} composed of their instances. To enhance the ability of our framework to predict new relations, we design a self-correcting inference strategy composed of three stages: relation discovery, relation denoising, and relation prediction. In the first stage, we use RD to preliminarily predict new relations for all test instances. Next, we apply RP to select some high-reliability test instances for each new relation from the prediction results of RD through a cross-validation method. During the third stage, we employ RP to re-predict the relations of all test instances based on the demonstrations constructed from these reliable test instances. Extensive experiments on three OpenRE datasets demonstrate the effectiveness of our framework. We release our code at this https URL.</li>
<li><strong>摘要：</strong>开放关系提取的目的（OpenRE）是开发一个可以推广到培训期间未遇到的新关系的RE模型。现有研究主要将OpenRE作为一项聚类任务。他们首先根据实例之间的相似性聚类所有测试实例，然后手动为每个集群分配新的关系。但是，他们对人类注释的依赖限制了他们的实用性。在本文中，我们提出了一个基于大语言模型（LLM）的OpenRE框架，该框架直接通过在没有人类干预的情况下利用其强大的语言理解和发电能力来预测测试实例的新关系。具体而言，我们的框架由两个核心组成部分组成：（1）关系发现者（RD），旨在预测基于\ textit {示例}的测试实例的新关系，该实例是由已知关系的培训实例形成的； （2）一个关系预测因子（RP），用于从\ textit {示例}指导下，从$ n $候选关系中选择最有可能的关系。为了增强我们的框架预测新关系的能力，我们设计了一个由三个阶段组成的自我校正推理策略：关系发现，关系deNoing和关系预测。在第一阶段，我们使用RD初步预测所有测试实例的新关系。接下来，我们将RP通过交叉验证方法从RD的预测结果中为每个新的关系选择一些高可靠性测试实例。在第三阶段，我们采用RP根据这些可靠的测试实例构建的演示来重新预测所有测试实例的关系。在三个OpenRE数据集上进行的大量实验证明了我们框架的有效性。我们在此HTTPS URL上发布代码。</li>
</ul>

<h3>Title: TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action</h3>
<ul>
<li><strong>Authors: </strong>Chenyue Zhou, Gürkan Solmaz, Flavio Cirillo, Kiril Gashteovski, Jonathan Fürst</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15098">https://arxiv.org/abs/2509.15098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15098">https://arxiv.org/pdf/2509.15098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15098]] TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action(https://arxiv.org/abs/2509.15098)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Humanitarian Mine Action has generated extensive best-practice knowledge, but much remains locked in unstructured reports. We introduce TextMine, an ontology-guided pipeline that uses Large Language Models to extract knowledge triples from HMA texts. TextMine integrates document chunking, domain-aware prompting, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. We also create the first HMA ontology and a curated dataset of real-world demining reports. Experiments show ontology-aligned prompts boost extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format conformance by 20.9% over baselines. While validated on Cambodian reports, TextMine can adapt to global demining efforts or other domains, transforming unstructured data into structured knowledge.</li>
<li><strong>摘要：</strong>人道主义的矿山行动已经产生了广泛的最佳实践知识，但仍在非结构化的报告中锁定了很多。我们介绍了Textmine，这是一种本体论引导的管道，它使用大型语言模型从HMA文本中提取知识三元。 TextMine集成了文档块，域感知的提示，三重提取以及基于参考的和LLM-AS-A-A-Gudge评估。我们还创建了第一个HMA本体论和现实世界中的Demining报告的策划数据集。实验表明，本体学一致的提示提示提取精度提高了44.2％，将幻觉降低了22.5％，并将格式的统一提高了20.9％。虽然在柬埔寨的报告中得到验证，但文本明显可以适应全球的demining努力或其他域，从而将非结构化数据转换为结构化知识。</li>
</ul>

<h3>Title: Large Language Model probabilities cannot distinguish between possible and impossible language</h3>
<ul>
<li><strong>Authors: </strong>Evelina Leivada, Raquel Montero, Paolo Morosi, Natalia Moskvina, Tamara Serrano, Marcel Aguilar, Fritz Guenther</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15114">https://arxiv.org/abs/2509.15114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15114">https://arxiv.org/pdf/2509.15114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15114]] Large Language Model probabilities cannot distinguish between possible and impossible language(https://arxiv.org/abs/2509.15114)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>A controversial test for Large Language Models concerns the ability to discern possible from impossible language. While some evidence attests to the models' sensitivity to what crosses the limits of grammatically impossible language, this evidence has been contested on the grounds of the soundness of the testing material. We use model-internal representations to tap directly into the way Large Language Models represent the 'grammatical-ungrammatical' distinction. In a novel benchmark, we elicit probabilities from 4 models and compute minimal-pair surprisal differences, juxtaposing probabilities assigned to grammatical sentences to probabilities assigned to (i) lower frequency grammatical sentences, (ii) ungrammatical sentences, (iii) semantically odd sentences, and (iv) pragmatically odd sentences. The prediction is that if string-probabilities can function as proxies for the limits of grammar, the ungrammatical condition will stand out among the conditions that involve linguistic violations, showing a spike in the surprisal rates. Our results do not reveal a unique surprisal signature for ungrammatical prompts, as the semantically and pragmatically odd conditions consistently show higher surprisal. We thus demonstrate that probabilities do not constitute reliable proxies for model-internal representations of syntactic knowledge. Consequently, claims about models being able to distinguish possible from impossible language need verification through a different methodology.</li>
<li><strong>摘要：</strong>对大语言模型的有争议的测试涉及从不可能的语言中辨别可能的能力。尽管某些证据证明了模型对超出语法不可能的语言限制的敏感性，但这些证据是基于测试材料的健全性而争论的。我们使用模型内部表示形式直接利用大型语言模型代表“语法无语法”区别的方式。在一个新颖的基准中，我们从4种模型中提出概率并计算最小对的惊人差异，将分配给语法句子分配给分配给（i）较低频率语法句子的概率的概率并置概率，（ii）不语法句子，（ii）（iii）奇怪的句子和（IV）奇怪的句子，以及（iv）（IV）具有奇怪的句子。预测是，如果字符串 - 探测能够充当语法限制的代理，则不语法的条件将在涉及语言违规的条件中脱颖而出，显示出惊人的速率激增。我们的结果并未显示出非语法提示的独特惊奇签名，因为语义和务实的奇数条件始终显示出更高的惊喜。因此，我们证明概率并不构成句法知识模型内部表示的可靠代理。因此，关于模型能够通过不同的方法来区分可能的语言的主张需要验证。</li>
</ul>

<h3>Title: A1: Asynchronous Test-Time Scaling via Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jing Xiong, Qiujiang Chen, Fanghua Ye, Zhongwei Wan, Chuanyang Zheng, Chenyang Zhao, Hui Shen, Alexander Hanbo Li, Chaofan Tao, Haochen Tan, Haoli Bai, Lifeng Shang, Lingpeng Kong, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15148">https://arxiv.org/abs/2509.15148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15148">https://arxiv.org/pdf/2509.15148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15148]] A1: Asynchronous Test-Time Scaling via Conformal Prediction(https://arxiv.org/abs/2509.15148)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) benefit from test-time scaling, but existing methods face significant challenges, including severe synchronization overhead, memory bottlenecks, and latency, especially during speculative decoding with long reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive inference framework that addresses these challenges. A1 refines arithmetic intensity to identify synchronization as the dominant bottleneck, proposes an online calibration strategy to enable asynchronous inference, and designs a three-stage rejection sampling pipeline that supports both sequential and parallel scaling. Through experiments on the MATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model families, we demonstrate that A1 achieves a remarkable 56.7x speedup in test-time scaling and a 4.14x improvement in throughput, all while maintaining accurate rejection-rate control, reducing latency and memory overhead, and no accuracy loss compared to using target model scaling alone. These results position A1 as an efficient and principled solution for scalable LLM inference. We have released the code at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）受益于测试时间缩放，但是现有的方法面临着重大挑战，包括高度同步开销，记忆瓶颈和潜伏期，尤其是在用长期推理链的投机解码过程中。我们介绍了A1（异步测试时间缩放），这是一个统计保证的适应性推理框架，可以解决这些挑战。 A1优化算术强度以识别同步为主要瓶颈，提出了一种在线校准策略，以实现异步推断，并设计了一个支持顺序和平行缩放的三阶段拒绝采样管道。通过数学的实验，AMC23，AIME24和AIME25数据集在各种靶标模型系列中，我们证明A1在测试时间缩放量表中实现了显着的56.7倍加速度，并且在逐步量表中提高了4.14倍的提高，同时又可以保持准确的抑制率控制效率和不准确的模型。这些结果将A1定位为可伸缩LLM推断的有效且原则的解决方案。我们已经在此HTTPS URL上发布了代码。</li>
</ul>

<h3>Title: SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huy Nghiem, Advik Sachdeva, Hal Daumé III</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15174">https://arxiv.org/abs/2509.15174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15174">https://arxiv.org/pdf/2509.15174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15174]] SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models(https://arxiv.org/abs/2509.15174)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.</li>
<li><strong>摘要：</strong>警告：本文包含进攻材料的例子。在社交媒体平台上，有毒内容已变得普遍。我们介绍了使用大型语言模型（LLMS）的可解释内容适中的数据有效的两阶段框架Smarter。在第1阶段，我们利用LLM自己的输出来生成正确和错误标签的合成解释，从而通过最小的人类监督来通过优先优化进行对齐。在第2阶段，我们通过跨模型训练来完善解释质量，从而使较弱的模型可以在风格和语义上与更强的模型保持一致。对三个基准任务的实验 -  Hatexplain，Littent Hate和Intient Hate-表明，Smarter使LLMS能够比标准的几杆基线相比，在仅使用一小部分完整培训数据的同时，可以实现13.5％的宏观F1改进。我们的框架通过利用LLMS的自我改善能力来分类和解释提供了可扩展的策略。</li>
</ul>

<h3>Title: Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yeongbin Seo, Dongha Lee, Jaehyung Kim, Jinyoung Yeo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15188">https://arxiv.org/abs/2509.15188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15188">https://arxiv.org/pdf/2509.15188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15188]] Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning(https://arxiv.org/abs/2509.15188)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.</li>
<li><strong>摘要：</strong>自回归（AR）语言模型一次生成文本一个令牌，这限制了其推理速度。基于扩散的语言模型提供了一种有希望的替代方案，因为它们可以并行解码多个令牌。但是，我们在当前扩散LMS中确定了一个关键的瓶颈：漫长的解码窗口问题，在该问题中，远离输入上下文的代币经常变得无关紧要或重复。以前的解决方案（例如半自动进取的解决方案）通过将窗口拆分为块来解决此问题，但这牺牲了速度和双向性，从而消除了扩散模型的主要优势。为了克服这一点，我们提出了卷积解码（CORV），这是一种基于标准化的方法，可缩小解码窗口而无需严格的分割，从而提高流利性和灵活性。此外，我们介绍了拒绝基于规则的微调（R2FT），这是一种事后培训计划，可以更好地在远非上下文的位置保持令牌。我们的方法在扩散的LM基线中获得开放式生成基准测试（例如Alpacaeval）的最新结果，其步骤大小明显低于以前的作品，这既显示速度和质量提高。</li>
</ul>

<h3>Title: Fair-GPTQ: Bias-Aware Quantization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Irina Proskurina, Guillaume Metzler, Julien Velcin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15206">https://arxiv.org/abs/2509.15206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15206">https://arxiv.org/pdf/2509.15206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15206]] Fair-GPTQ: Bias-Aware Quantization for Large Language Models(https://arxiv.org/abs/2509.15206)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>High memory demands of generative language models have drawn attention to quantization, which reduces computational cost, memory usage, and latency by mapping model weights to lower-precision integers. Approaches such as GPTQ effectively minimize input-weight product errors during quantization; however, recent empirical studies show that they can increase biased outputs and degrade performance on fairness benchmarks, and it remains unclear which specific weights cause this issue. In this work, we draw new links between quantization and model fairness by adding explicit group-fairness constraints to the quantization objective and introduce Fair-GPTQ, the first quantization method explicitly designed to reduce unfairness in large language models. The added constraints guide the learning of the rounding operation toward less-biased text generation for protected groups. Specifically, we focus on stereotype generation involving occupational bias and discriminatory language spanning gender, race, and religion. Fair-GPTQ has minimal impact on performance, preserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces unfairness relative to a half-precision model, and retains the memory and speed benefits of 4-bit quantization. We also compare the performance of Fair-GPTQ with existing debiasing methods and find that it achieves performance on par with the iterative null-space projection debiasing approach on racial-stereotype benchmarks. Overall, the results validate our theoretical solution to the quantization problem with a group-bias term, highlight its applicability for reducing group bias at quantization time in generative models, and demonstrate that our approach can further be used to analyze channel- and weight-level contributions to fairness during quantization.</li>
<li><strong>摘要：</strong>生成语言模型的高内存需求引起了人们对量化的关注，从而通过将模型权重映射到较低精确的整数来降低计算成本，内存使用和延迟。诸如GPTQ之类的方法可有效地最大程度地减少量化过程中的输入权重量误差；但是，最近的实证研究表明，它们可以增加偏见的产出并在公平基准上降低性能，并且尚不清楚哪些特定权重引起此问题。在这项工作中，我们通过在量化目标中添加明确的群体 - 财产约束并引入Fair-GPTQ，这是第一个量化方法，这是第一个明确设计的，旨在减少大语言模型中的不公平性。附加的约束指导将舍入操作的学习用于对受保护组的偏低文本生成。具体来说，我们专注于涉及职业偏见和歧视性语言的刻板印象产生，跨越了性别，种族和宗教。 Fair-GPTQ对性能的影响很小，在零射门基准上保留了至少90％的基线准确性，相对于半精度模型降低了不公平性，并保留了4位量化的内存和速度益处。我们还将Fair-GPTQ的性能与现有的偏见方法进行了比较，并发现它与种族型基准的迭代空间空间投影偏差方法相同。总体而言，结果验证了我们针对量子偏差项的理论解决方案，强调了其在生成模型中量化时间降低群体偏置的适用性，并证明我们的方法可以进一步用于分析量化期间通道和重量级别的贡献。</li>
</ul>

<h3>Title: What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques</h3>
<ul>
<li><strong>Authors: </strong>Petros Stylianos Giouroukis, Dimitris Dimitriadis, Dimitrios Papadopoulos, Zhenwen Shao, Grigorios Tsoumakas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15211">https://arxiv.org/abs/2509.15211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15211">https://arxiv.org/pdf/2509.15211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15211]] What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques(https://arxiv.org/abs/2509.15211)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Slide decks, serving as digital reports that bridge the gap between presentation slides and written documents, are a prevalent medium for conveying information in both academic and corporate settings. Their multimodal nature, combining text, images, and charts, presents challenges for retrieval-augmented generation systems, where the quality of retrieval directly impacts downstream performance. Traditional approaches to slide retrieval often involve separate indexing of modalities, which can increase complexity and lose contextual information. This paper investigates various methodologies for effective slide retrieval, including visual late-interaction embedding models like ColPali, the use of visual rerankers, and hybrid retrieval techniques that combine dense retrieval with BM25, further enhanced by textual rerankers and fusion methods like Reciprocal Rank Fusion. A novel Vision-Language Models-based captioning pipeline is also evaluated, demonstrating significantly reduced embedding storage requirements compared to visual late-interaction techniques, alongside comparable retrieval performance. Our analysis extends to the practical aspects of these methods, evaluating their runtime performance and storage demands alongside retrieval efficacy, thus offering practical guidance for the selection and development of efficient and robust slide retrieval systems for real-world applications.</li>
<li><strong>摘要：</strong>幻灯片甲板是数字报告，弥合了演示幻灯片和书面文件之间的差距，是传达在学术和公司环境中信息的普遍媒介。它们的多模式性质结合了文本，图像和图表，对检索型生成系统提出了挑战，其中检索的质量直接影响下游性能。传统的滑动检索方法通常涉及单独的模式索引，这可能会增加复杂性并失去上下文信息。本文研究了各种有效幻灯片检索的方法，包括Colpali等视觉延迟嵌入模型，视觉重读者的使用以及将密集检索与BM25结合的混合检索技术，通过文本rerankers和Fextual Rerankers和互惠互相融合融合的融合方法进一步增强。还评估了一种基于视觉模型的新型字幕字幕管道，与视觉后期交流技术相比，嵌入储存需求显着减少，以及可比的检索性能。我们的分析扩展到了这些方法的实际方面，评估了它们的运行时性能和存储需求以及检索功效，从而为选择和开发有效且强大的幻灯片检索系统为现实世界应用提供了实用的指导。</li>
</ul>

<h3>Title: Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15216">https://arxiv.org/abs/2509.15216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15216">https://arxiv.org/pdf/2509.15216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15216]] Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models(https://arxiv.org/abs/2509.15216)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (this https URL).</li>
<li><strong>摘要：</strong>由于在每个国家 /地区的独特，特定的排除，殖民化和社会地位的历史，衡量历史结构压迫与跨国有效性斗争的传统努力，并且经常依靠结构化指数，这些指标在忽略了居住的，基于身份的排斥的同时特权了物质资源。我们介绍了一个新颖的压迫度量框架，该框架利用大型语言模型（LLMS）在不同的地缘政治环境中产生了生活劣势的上下文敏感分数。使用多语言Covid-19的全球研究中的非结构化自我认同的种族话语，我们设计了规则引导的提示策略，鼓励模型产生可解释的，理论上基于压迫的估计。我们在多个最先进的LLM中系统地评估了这些策略。我们的结果表明，在以明确的规则为指导下，LLM可以捕获基于身份的历史压迫的细微差别形式。这种方法提供了一种互补的测量工具，该工具突出了系统排除的维​​度，提供了可扩展的跨文化镜头，以了解压迫如何在数据驱动的研究和公共卫生环境中表现出来。为了支持可重复的评估，我们发布了一个开源基准数据集，用于评估压迫测量的LLM（此HTTPS URL）。</li>
</ul>

<h3>Title: LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15218">https://arxiv.org/abs/2509.15218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15218">https://arxiv.org/pdf/2509.15218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15218]] LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models(https://arxiv.org/abs/2509.15218)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the model's greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at this https URL to facilitate research.</li>
<li><strong>摘要：</strong>现在，在开发大语言模型（LLMS）期间，数据污染的问题几乎是不可避免的，培训数据通常无意间甚至无意间都集成了这些评估基准。此问题随后使得很难公平基准LLMS。我们提出了一个新颖的框架\ textbf {lne-blocking}，而不是构造无污染的数据集（相当困难），以恢复模型性能，然后再污染潜在泄漏的数据集。我们的框架由两个组成部分组成：污染检测和破坏操作。对于提示，框架首先使用污染检测方法\ textbf {lne}来评估模型中污染的程度。基于此，它调整了中断操作的强度\ textbf {blocking}，以从模型中引起非移动响应。我们的框架是第一个有效恢复模型的贪婪解码性能的框架。在多个数据集上具有强大的性能，具有潜在的泄漏风险，并且始终在不同模型和不同级别的数据污染水平上实现稳定的恢复结果。我们在此HTTPS URL上发布代码以促进研究。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
