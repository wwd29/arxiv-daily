<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-23</h1>
<h3>Title: SouLLMate: An Application Enhancing Diverse Mental Health Support with Adaptive LLMs, Prompt Engineering, and RAG Techniques</h3>
<ul>
<li><strong>Authors: </strong>Qiming Guo, Jinwen Tang, Wenbo Sun, Haoteng Tang, Yi Shang, Wenlu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16322">https://arxiv.org/abs/2410.16322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16322">https://arxiv.org/pdf/2410.16322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16322]] SouLLMate: An Application Enhancing Diverse Mental Health Support with Adaptive LLMs, Prompt Engineering, and RAG Techniques(https://arxiv.org/abs/2410.16322)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Mental health issues significantly impact individuals' daily lives, yet many do not receive the help they need even with available online resources. This study aims to provide diverse, accessible, stigma-free, personalized, and real-time mental health support through cutting-edge AI technologies. It makes the following contributions: (1) Conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering, and domain knowledge. This system offers advanced features such as Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized profile uploads and Conversational Information Extraction. (3) Developing novel evaluation approaches for preliminary assessments and risk detection via professionally annotated interview data and real-life suicide tendency data. (4) Proposing the Key Indicator Summarization (KIS), Proactive Questioning Strategy (PQS), and Stacked Multi-Model Reasoning (SMMR) methods to enhance model performance and usability through context-sensitive response adjustments, semantic coherence evaluations, and enhanced accuracy of long-context reasoning in language models. This study contributes to advancing mental health support technologies, potentially improving the accessibility and effectiveness of mental health care globally.</li>
<li><strong>摘要：</strong>心理健康问题严重影响着人们的日常生活，但即使有网上资源，许多人也得不到他们需要的帮助。本研究旨在通过尖端人工智能技术提供多样化、可及性、无歧视、个性化和实时的心理健康支持。它做出了以下贡献：（1）对近期的心理健康支持方法进行广泛调查，以确定普遍存在的功能和未满足的需求。（2）介绍SouLLMate，一个自适应的LLM驱动系统，它集成了LLM技术、Chain、检索增强生成（RAG）、提示工程和领域知识。该系统提供风险检测和主动指导对话等高级功能，并利用RAG进行个性化个人资料上传和对话信息提取。（3）通过专业注释的访谈数据和现实生活中的自杀倾向数据，开发用于初步评估和风险检测的新型评估方法。 （4）提出关键指标总结（KIS）、主动提问策略（PQS）和堆叠式多模型推理（SMMR）方法，通过上下文敏感的响应调整、语义连贯性评估和语言模型中长上下文推理的准确性提高模型性能和可用性。这项研究有助于推进心理健康支持技术，有可能提高全球心理健康护理的可及性和有效性。</li>
</ul>

<h3>Title: This Candidate is [MASK]. Letters of Reference and Job Market Outcomes using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Fabian Slonimczyk</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16325">https://arxiv.org/abs/2410.16325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16325">https://arxiv.org/pdf/2410.16325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16325]] This Candidate is [MASK]. Letters of Reference and Job Market Outcomes using LLMs(https://arxiv.org/abs/2410.16325)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>I implement a prompt-based learning strategy to extract measures of sentiment and other features from confidential reference letters. I show that the contents of reference letters is clearly reflected in the performance of job market candidates in the Economics academic job market. In contrast, applying traditional ``bag-of-words'' approaches produces measures of sentiment that, while positively correlated to my LLM-based measure, are not predictive of job market outcomes. Using a random forest, I show that both letter quality and length are predictive of success in the job market. Letters authored by advisers appear to be as important as those written by other referees.</li>
<li><strong>摘要：</strong>我实施了一种基于提示的学习策略，从机密推荐信中提取情绪指标和其他特征。我表明，推荐信的内容清楚地反映在经济学学术就业市场中求职者的表现中。相比之下，应用传统的“词袋”方法产生的情绪指标虽然与我基于法学硕士的指标呈正相关，但不能预测就业市场的结果。使用随机森林，我表明信件的质量和长度都可以预测就业市场的成功。顾问撰写的信件似乎与其他推荐人撰写的信件一样重要。</li>
</ul>

<h3>Title: KatzBot: Revolutionizing Academic Chatbot for Enhanced Communication</h3>
<ul>
<li><strong>Authors: </strong>Sahil Kumar, Deepa Paikar, Kiran Sai Vutukuri, Haider Ali, Shashidhar Reddy Ainala, Aditya Murli Krishnan, Youshan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16385">https://arxiv.org/abs/2410.16385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16385">https://arxiv.org/pdf/2410.16385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16385]] KatzBot: Revolutionizing Academic Chatbot for Enhanced Communication(https://arxiv.org/abs/2410.16385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Effective communication within universities is crucial for addressing the diverse information needs of students, alumni, and external stakeholders. However, existing chatbot systems often fail to deliver accurate, context-specific responses, resulting in poor user experiences. In this paper, we present KatzBot, an innovative chatbot powered by KatzGPT, a custom Large Language Model (LLM) fine-tuned on domain-specific academic data. KatzGPT is trained on two university-specific datasets: 6,280 sentence-completion pairs and 7,330 question-answer pairs. KatzBot outperforms established existing open source LLMs, achieving higher accuracy and domain relevance. KatzBot offers a user-friendly interface, significantly enhancing user satisfaction in real-world applications. The source code is publicly available at \url{this https URL}.</li>
<li><strong>摘要：</strong>大学内部的有效沟通对于满足学生、校友和外部利益相关者的多样化信息需求至关重要。然而，现有的聊天机器人系统往往无法提供准确的、特定于上下文的响应，导致用户体验不佳。在本文中，我们介绍了 KatzBot，这是一款创新的聊天机器人，由 KatzGPT 提供支持，KatzGPT 是一种定制的大型语言模型 (LLM)，针对特定领域的学术数据进行了微调。KatzGPT 在两个特定于大学的数据集上进行训练：6,280 个句子完成对和 7,330 个问答对。KatzBot 的表现优于现有的开源 LLM，实现了更高的准确性和领域相关性。KatzBot 提供了用户友好的界面，显著提高了实际应用中的用户满意度。源代码可在 \url{this https URL} 上公开获取。</li>
</ul>

<h3>Title: LLM-based Optimization of Compound AI Systems: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Yiran Wu, Huan Liu, Jun Liu, Gao Huang, Yong-Jin Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16392">https://arxiv.org/abs/2410.16392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16392">https://arxiv.org/pdf/2410.16392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16392]] LLM-based Optimization of Compound AI Systems: A Survey(https://arxiv.org/abs/2410.16392)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>In a compound AI system, components such as an LLM call, a retriever, a code interpreter, or tools are interconnected. The system's behavior is primarily driven by parameters such as instructions or tool definitions. Recent advancements enable end-to-end optimization of these parameters using an LLM. Notably, leveraging an LLM as an optimizer is particularly efficient because it avoids gradient computation and can generate complex code and instructions. This paper presents a survey of the principles and emerging trends in LLM-based optimization of compound AI systems. It covers archetypes of compound AI systems, approaches to LLM-based end-to-end optimization, and insights into future directions and broader impacts. Importantly, this survey uses concepts from program analysis to provide a unified view of how an LLM optimizer is prompted to optimize a compound AI system. The exhaustive list of paper is provided at this https URL.</li>
<li><strong>摘要：</strong>在复合 AI 系统中，LLM 调用、检索器、代码解释器或工具等组件是相互连接的。系统的行为主要由指令或工具定义等参数驱动。最近的进展使得使用 LLM 可以对这些参数进行端到端优化成为可能。值得注意的是，利用 LLM 作为优化器特别有效，因为它可以避免梯度计算并可以生成复杂的代码和指令。本文概述了基于 LLM 的复合 AI 系统优化的原理和新兴趋势。它涵盖了复合 AI 系统的原型、基于 LLM 的端到端优化方法以及对未来方向和更广泛影响的见解。重要的是，本综述使用程序分析中的概念来提供 LLM 优化器如何优化复合 AI 系统的统一视图。详尽的论文列表在此 https URL 中提供。</li>
</ul>

<h3>Title: VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use</h3>
<ul>
<li><strong>Authors: </strong>Zhehao Zhang, Ryan Rossi, Tong Yu, Franck Dernoncourt, Ruiyi Zhang, Jiuxiang Gu, Sungchul Kim, Xiang Chen, Zichao Wang, Nedim Lipka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16400">https://arxiv.org/abs/2410.16400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16400">https://arxiv.org/pdf/2410.16400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16400]] VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use(https://arxiv.org/abs/2410.16400)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>While vision-language models (VLMs) have demonstrated remarkable performance across various tasks combining textual and visual information, they continue to struggle with fine-grained visual perception tasks that require detailed pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs on such intricate visual elements remains an open challenge. In this paper, we present VipAct, an agent framework that enhances VLMs by integrating multi-agent collaboration and vision expert models, enabling more precise visual understanding and comprehensive reasoning. VipAct consists of an orchestrator agent, which manages task requirement analysis, planning, and coordination, along with specialized agents that handle specific tasks such as image captioning and vision expert models that provide high-precision perceptual information. This multi-agent approach allows VLMs to better perform fine-grained visual perception tasks by synergizing planning, reasoning, and tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual perception tasks, with experimental results demonstrating significant performance improvements over state-of-the-art baselines across all tasks. Furthermore, comprehensive ablation studies reveal the critical role of multi-agent collaboration in eliciting more detailed System-2 reasoning and highlight the importance of image input for task planning. Additionally, our error analysis identifies patterns of VLMs' inherent limitations in visual perception, providing insights into potential future improvements. VipAct offers a flexible and extensible framework, paving the way for more advanced visual perception systems across various real-world applications.</li>
<li><strong>摘要：</strong>虽然视觉语言模型 (VLM) 在结合文本和视觉信息的各种任务中表现出色，但它们在需要详细像素级分析的细粒度视觉感知任务中仍然举步维艰。有效地从 VLM 中引出对如此复杂的视觉元素的全面推理仍然是一个悬而未决的挑战。在本文中，我们介绍了 VipAct，这是一个代理框架，它通过集成多代理协作和视觉专家模型来增强 VLM，从而实现更精确的视觉理解和全面推理。VipAct 由一个协调器代理组成，它管理任务需求分析、规划和协调，以及处理特定任务（例如图像字幕和提供高精度感知信息的视觉专家模型）的专用代理。这种多代理方法允许 VLM 通过协同规划、推理和工具使用来更好地执行细粒度的视觉感知任务。我们在具有多种视觉感知任务的基准上对 VipAct 进行了评估，实验结果表明，在所有任务中，其性能均比最先进的基线有显着提高。此外，全面的消融研究揭示了多智能体协作在引出更详细的 System-2 推理方面的关键作用，并强调了图像输入对任务规划的重要性。此外，我们的错误分析确定了 VLM 在视觉感知方面的固有局限性模式，为未来的潜在改进提供了见解。VipAct 提供了一个灵活且可扩展的框架，为各种实际应用中更先进的视觉感知系统铺平了道路。</li>
</ul>

<h3>Title: Improving Neuron-level Interpretability with White-box Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Bai, Yi Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16443">https://arxiv.org/abs/2410.16443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16443">https://arxiv.org/pdf/2410.16443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16443]] Improving Neuron-level Interpretability with White-box Language Models(https://arxiv.org/abs/2410.16443)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Neurons in auto-regressive language models like GPT-2 can be interpreted by analyzing their activation patterns. Recent studies have shown that techniques such as dictionary learning, a form of post-hoc sparse coding, enhance this neuron-level interpretability. In our research, we are driven by the goal to fundamentally improve neural network interpretability by embedding sparse coding directly within the model architecture, rather than applying it as an afterthought. In our study, we introduce a white-box transformer-like architecture named Coding RAte TransformEr (CRATE), explicitly engineered to capture sparse, low-dimensional structures within data distributions. Our comprehensive experiments showcase significant improvements (up to 103% relative improvement) in neuron-level interpretability across a variety of evaluation metrics. Detailed investigations confirm that this enhanced interpretability is steady across different layers irrespective of the model size, underlining CRATE's robust performance in enhancing neural network interpretability. Further analysis shows that CRATE's increased interpretability comes from its enhanced ability to consistently and distinctively activate on relevant tokens. These findings point towards a promising direction for creating white-box foundation models that excel in neuron-level interpretation.</li>
<li><strong>摘要：</strong>通过分析 GPT-2 等自回归语言模型中的神经元的激活模式，可以对其进行解释。最近的研究表明，诸如字典学习（一种事后稀疏编码）之类的技术可以增强这种神经元级的可解释性。在我们的研究中，我们的目标是通过将稀疏编码直接嵌入模型架构中，而不是事后再应用，从根本上提高神经网络的可解释性。在我们的研究中，我们引入了一种名为 Coding RAte TransformEr (CRATE) 的白盒变压器式架构，该架构专门设计用于捕获数据分布中的稀疏、低维结构。我们的综合实验展示了各种评估指标中神经元级可解释性的显着改进（相对改进高达 103%）。详细的调查证实，无论模型大小如何，这种增强的可解释性在不同层之间都是稳定的，这突显了 CRATE 在增强神经网络可解释性方面的强大性能。进一步的分析表明，CRATE 的可解释性增强来自于其增强的一致且独特地激活相关标记的能力。这些发现为创建在神经元级解释方面表现出色的白盒基础模型指明了有希望的方向。</li>
</ul>

<h3>Title: Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S</h3>
<ul>
<li><strong>Authors: </strong>Christabel Acquaye, Haozhe An, Rachel Rudinger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16451">https://arxiv.org/abs/2410.16451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16451">https://arxiv.org/pdf/2410.16451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16451]] Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S(https://arxiv.org/abs/2410.16451)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recent work has highlighted the culturally-contingent nature of commonsense knowledge. We introduce AMAMMER${\epsilon}$, a test set of 525 multiple-choice questions designed to evaluate the commonsense knowledge of English LLMs, relative to the cultural contexts of Ghana and the United States. To create AMAMMER${\epsilon}$, we select a set of multiple-choice questions (MCQs) from existing commonsense datasets and rewrite them in a multi-stage process involving surveys of Ghanaian and U.S. participants. In three rounds of surveys, participants from both pools are solicited to (1) write correct and incorrect answer choices, (2) rate individual answer choices on a 5-point Likert scale, and (3) select the best answer choice from the newly-constructed MCQ items, in a final validation step. By engaging participants at multiple stages, our procedure ensures that participant perspectives are incorporated both in the creation and validation of test items, resulting in high levels of agreement within each pool. We evaluate several off-the-shelf English LLMs on AMAMMER${\epsilon}$. Uniformly, models prefer answers choices that align with the preferences of U.S. annotators over Ghanaian annotators. Additionally, when test items specify a cultural context (Ghana or the U.S.), models exhibit some ability to adapt, but performance is consistently better in U.S. contexts than Ghanaian. As large resources are devoted to the advancement of English LLMs, our findings underscore the need for culturally adaptable models and evaluations to meet the needs of diverse English-speaking populations around the world.</li>
<li><strong>摘要：</strong>最近的研究强调了常识知识的文化偶然性。我们引入了 AMAMMER${\epsilon}$，这是一组由 525 个多项选择题组成的测试集，旨在评估英语法学硕士相对于加纳和美国文化背景的常识知识。为了创建 AMAMMER${\epsilon}$，我们从现有的常识数据集中选择了一组多项选择题 (MCQ)，并通过多阶段流程重写它们，其中涉及对加纳和美国参与者的调查。在三轮调查中，来自两个池的参与者被要求 (1) 写出正确和错误的答案选项，(2) 按照 5 点李克特量表对各个答案选项进行评分，以及 (3) 在最后的验证步骤中从新构建的 MCQ 项目中选择最佳答案选项。通过在多个阶段让参与者参与，我们的程序确保在测试项目的创建和验证中都纳入了参与者的观点，从而在每个池中实现高度一致。我们对 AMAMMER${\epsilon}$ 上的几种现成的英语法学硕士进行了评估。一致地，模型更倾向于符合美国注释者而非加纳注释者偏好的答案选择。此外，当测试项目指定文化背景（加纳或美国）时，模型表现出一定的适应能力，但在美国背景下的表现始终优于加纳。由于大量资源投入到英语法学硕士的推进中，我们的研究结果强调需要具有文化适应性的模型和评估来满足世界各地不同英语人群的需求。</li>
</ul>

<h3>Title: Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, Suhang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16454">https://arxiv.org/abs/2410.16454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16454">https://arxiv.org/pdf/2410.16454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16454]] Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge(https://arxiv.org/abs/2410.16454)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable proficiency in generating text, benefiting from extensive training on vast textual corpora. However, LLMs may also acquire unwanted behaviors from the diverse and sensitive nature of their training data, which can include copyrighted and private content. Machine unlearning has been introduced as a viable solution to remove the influence of such problematic content without the need for costly and time-consuming retraining. This process aims to erase specific knowledge from LLMs while preserving as much model utility as possible. Despite the effectiveness of current unlearning methods, little attention has been given to whether existing unlearning methods for LLMs truly achieve forgetting or merely hide the knowledge, which current unlearning benchmarks fail to detect. This paper reveals that applying quantization to models that have undergone unlearning can restore the "forgotten" information. To thoroughly evaluate this phenomenon, we conduct comprehensive experiments using various quantization techniques across multiple precision levels. We find that for unlearning methods with utility constraints, the unlearned model retains an average of 21\% of the intended forgotten knowledge in full precision, which significantly increases to 83\% after 4-bit quantization. Based on our empirical findings, we provide a theoretical explanation for the observed phenomenon and propose a quantization-robust unlearning strategy to mitigate this intricate issue...</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在文本生成方面表现出了非凡的能力，得益于对大量文本语料库的广泛训练。然而，由于训练数据的多样性和敏感性，LLM 也可能获得不良行为，这些训练数据可能包括受版权保护的内容和私人内容。机器反学习已被引入作为一种可行的解决方案，可以消除此类问题内容的影响，而无需昂贵且耗时的重新训练。此过程旨在从 LLM 中删除特定知识，同时尽可能多地保留模型效用。尽管当前的反学习方法很有效，但很少有人关注现有的 LLM 反学习方法是否真正实现了遗忘，还是仅仅隐藏了当前反学习基准无法检测到的知识。本文表明，对经过反学习的模型应用量化可以恢复“被遗忘”的信息。为了彻底评估这一现象，我们使用多种精度级别的各种量化技术进行了全面的实验。我们发现，对于具有效用约束的反学习方法，反学习模型平均可以完全精确地保留 21% 的预期遗忘知识，在 4 位量化后，该数字显著增加到 83%。根据我们的实证研究结果，我们对观察到的现象提供了理论解释，并提出了一种量化稳健的反学习策略来缓解这一复杂问题……</li>
</ul>

<h3>Title: To the Globe (TTG): Towards Language-Driven Guaranteed Travel Planning</h3>
<ul>
<li><strong>Authors: </strong>Da JU, Song Jiang, Andrew Cohen, Aaron Foss, Sasha Mitts, Arman Zharmagambetov, Brandon Amos, Xian Li, Justine T Kao, Maryam Fazel-Zarandi, Yuandong Tian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16456">https://arxiv.org/abs/2410.16456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16456">https://arxiv.org/pdf/2410.16456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16456]] To the Globe (TTG): Towards Language-Driven Guaranteed Travel Planning(https://arxiv.org/abs/2410.16456)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Travel planning is a challenging and time-consuming task that aims to find an itinerary which satisfies multiple, interdependent constraints regarding flights, accommodations, attractions, and other travel arrangements. In this paper, we propose To the Globe (TTG), a real-time demo system that takes natural language requests from users, translates it to symbolic form via a fine-tuned Large Language Model, and produces optimal travel itineraries with Mixed Integer Linear Programming solvers. The overall system takes ~5 seconds to reply to the user request with guaranteed itineraries. To train TTG, we develop a synthetic data pipeline that generates user requests, flight and hotel information in symbolic form without human annotations, based on the statistics of real-world datasets, and fine-tune an LLM to translate NL user requests to their symbolic form, which is sent to the symbolic solver to compute optimal itineraries. Our NL-symbolic translation achieves ~91% exact match in a backtranslation metric (i.e., whether the estimated symbolic form of generated natural language matches the groundtruth), and its returned itineraries have a ratio of 0.979 compared to the optimal cost of the ground truth user request. When evaluated by users, TTG achieves consistently high Net Promoter Scores (NPS) of 35-40% on generated itinerary.</li>
<li><strong>摘要：</strong>旅行规划是一项具有挑战性且耗时的任务，旨在找到一个满足有关航班、住宿、景点和其他旅行安排的多个相互依赖的约束条件的行程。在本文中，我们提出了 To the Globe (TTG)，这是一个实时演示系统，它接收用户的自然语言请求，通过经过微调的大型语言模型将其转换为符号形式，并使用混合整数线性规划求解器生成最佳旅行行程。整个系统需要大约 5 秒钟才能回复用户请求并保证行程。为了训练 TTG，我们开发了一个合成数据管道，该管道基于真实世界数据集的统计数据，以符号形式生成用户请求、航班和酒店信息（无需人工注释），并微调 LLM 以将 NL 用户请求转换为符号形式，然后将其发送给符号求解器以计算最佳行程。我们的 NL-符号翻译在反向翻译指标中实现了约 91% 的精确匹配（即，生成的自然语言的估计符号形式是否与事实相符），其返回的行程与事实用户请求的最佳成本相比具有 0.979 的比率。在用户评估中，TTG 在生成的行程上始终获得 35-40% 的高净推荐值 (NPS)。</li>
</ul>

<h3>Title: Comparative Study of Multilingual Idioms and Similes in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Paria Khoshtab, Danial Namazifard, Mostafa Masoudi, Ali Akhgary, Samin Mahdizadeh Sani, Yadollah Yaghoobzadeh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16461">https://arxiv.org/abs/2410.16461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16461">https://arxiv.org/pdf/2410.16461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16461]] Comparative Study of Multilingual Idioms and Similes in Large Language Models(https://arxiv.org/abs/2410.16461)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This study addresses the gap in the literature concerning the comparative performance of LLMs in interpreting different types of figurative language across multiple languages. By evaluating LLMs using two multilingual datasets on simile and idiom interpretation, we explore the effectiveness of various prompt engineering strategies, including chain-of-thought, few-shot, and English translation prompts. We extend the language of these datasets to Persian as well by building two new evaluation sets. Our comprehensive assessment involves both closed-source (GPT-3.5, GPT-4o mini, Gemini 1.5), and open-source models (Llama 3.1, Qwen2), highlighting significant differences in performance across languages and figurative types. Our findings reveal that while prompt engineering methods are generally effective, their success varies by figurative type, language, and model. We also observe that open-source models struggle particularly with low-resource languages in similes. Additionally, idiom interpretation is nearing saturation for many languages, necessitating more challenging evaluations.</li>
<li><strong>摘要：</strong>本研究填补了文献中关于 LLM 在多种语言中解释不同类型的比喻性语言的比较性能的空白。通过使用两个多语言数据集对明喻和习语解释的 LLM 进行评估，我们探索了各种提示工程策略的有效性，包括思路链、小样本和英语翻译提示。我们还通过构建两个新的评估集将这些数据集的语言扩展到波斯语。我们的全面评估涉及闭源模型（GPT-3.5、GPT-4o mini、Gemini 1.5）和开源模型（Llama 3.1、Qwen2），突出了不同语言和比喻类型的性能存在显著差异。我们的研究结果表明，虽然提示工程方法通常是有效的，但它们的成功因比喻类型、语言和模型而异。我们还观察到，开源模型在明喻中尤其难以处理资源匮乏的语言。此外，许多语言的习语解释已接近饱和，因此需要进行更具挑战性的评估。</li>
</ul>

<h3>Title: Beyond Browsing: API-Based Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Yueqi Song, Frank Xu, Shuyan Zhou, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16464">https://arxiv.org/abs/2410.16464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16464">https://arxiv.org/pdf/2410.16464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16464]] Beyond Browsing: API-Based Web Agents(https://arxiv.org/abs/2410.16464)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Web browsers are a portal to the internet, where much of human activity is undertaken. Thus, there has been significant research work in AI agents that interact with the internet through web browsing. However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs). In this paper we ask -- what if we were to take tasks traditionally tackled by browsing agents, and give AI agents access to APIs? To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a Hybrid Agent that can interact with online data through both web browsing and APIs. In experiments on WebArena, a widely-used and realistic benchmark for web navigation tasks, we find that API-based agents outperform web browsing agents. Hybrid Agents out-perform both others nearly uniformly across tasks, resulting in a more than 20.0% absolute improvement over web browsing alone, achieving a success rate of 35.8%, achiving the SOTA performance among task-agnostic agents. These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.</li>
<li><strong>摘要：</strong>Web 浏览器是互联网的门户，人类的大部分活动都在这里进行。因此，在通过 Web 浏览与互联网交互的 AI 代理方面，已经开展了大量研究工作。但是，还有另一个专门为机器与在线内容交互而设计的接口：应用程序编程接口 (API)。在本文中，我们提出一个问题——如果我们将传统上由浏览代理处理的任务交给 AI 代理，并让其访问 API，结果会怎样？为此，我们提出了两种代理：(1) API 调用代理，它尝试仅通过 API 执行在线任务，类似于传统的编码代理；(2) 混合代理，它可以通过 Web 浏览和 API 与在线数据交互。在 WebArena（一种广泛使用且现实的 Web 导航任务基准）上的实验中，我们发现基于 API 的代理优于 Web 浏览代理。混合代理在各个任务上的表现几乎一致地优于其他两种代理，与单独的 Web 浏览相比，其绝对改进超过 20.0%，成功率达到 35.8%，在与任务无关的代理中实现了 SOTA 性能。这些结果强烈表明，当 API 可用时，它们提供了一种比单纯依赖网页浏览更有吸引力的替代方案。</li>
</ul>

<h3>Title: DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding</h3>
<ul>
<li><strong>Authors: </strong>Manan Suri, Puneet Mathur, Franck Dernoncourt, Rajiv Jain, Vlad I Morariu, Ramit Sawhney, Preslav Nakov, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16472">https://arxiv.org/abs/2410.16472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16472">https://arxiv.org/pdf/2410.16472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16472]] DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding(https://arxiv.org/abs/2410.16472)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Document structure editing involves manipulating localized textual, visual, and layout components in document images based on the user's requests. Past works have shown that multimodal grounding of user requests in the document image and identifying the accurate structural components and their associated attributes remain key challenges for this task. To address these, we introduce the DocEdit-v2, a novel framework that performs end-to-end document editing by leveraging Large Multimodal Models (LMMs). It consists of three novel components: (1) Doc2Command, which simultaneously localizes edit regions of interest (RoI) and disambiguates user edit requests into edit commands; (2) LLM-based Command Reformulation prompting to tailor edit commands originally intended for specialized software into edit instructions suitable for generalist LMMs. (3) Moreover, DocEdit-v2 processes these outputs via Large Multimodal Models like GPT-4V and Gemini, to parse the document layout, execute edits on grounded Region of Interest (RoI), and generate the edited document image. Extensive experiments on the DocEdit dataset show that DocEdit-v2 significantly outperforms strong baselines on edit command generation (2-33%), RoI bounding box detection (12-31%), and overall document editing (1-12\%) tasks.</li>
<li><strong>摘要：</strong>文档结构编辑涉及根据用户的请求操作文档图像中的本地文本、视觉和布局组件。过去的研究表明，在文档图像中多模态地确定用户请求并识别准确的结构组件及其相关属性仍然是这项任务的关键挑战。为了解决这些问题，我们引入了 DocEdit-v2，这是一个利用大型多模态模型 (LMM) 执行端到端文档编辑的新框架。它由三个新组件组成：(1) Doc2Command，它同时定位编辑感兴趣区域 (RoI) 并将用户编辑请求消除歧义为编辑命令；(2) 基于 LLM 的命令重构提示将最初用于专用软件的编辑命令定制为适合通用 LMM 的编辑指令。(3) 此外，DocEdit-v2 通过 GPT-4V 和 Gemini 等大型多模态模型处理这些输出，以解析文档布局、在接地的感兴趣区域 (RoI) 上执行编辑并生成编辑后的文档图像。在 DocEdit 数据集上进行的大量实验表明，DocEdit-v2 在编辑命令生成（2-33%）、RoI 边界框检测（12-31%）和整体文档编辑（1-12\%）任务上的表现明显优于强基线。</li>
</ul>

<h3>Title: BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, Maarten Sap</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16491">https://arxiv.org/abs/2410.16491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16491">https://arxiv.org/pdf/2410.16491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16491]] BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data(https://arxiv.org/abs/2410.16491)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>In this work, we tackle the challenge of embedding realistic human personality traits into LLMs. Previous approaches have primarily focused on prompt-based methods that describe the behavior associated with the desired personality traits, suffering from realism and validity issues. To address these limitations, we introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues designed to ground models in how humans express their personality in text. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training-based methods to align LLMs more naturally with human personality patterns. Our methods outperform prompting on personality assessments such as BFI and IPIP-NEO, with trait correlations more closely matching human data. Furthermore, our experiments reveal that models trained to exhibit higher conscientiousness, higher agreeableness, lower extraversion, and lower neuroticism display better performance on reasoning tasks, aligning with psychological findings on how these traits impact human cognitive performance. To our knowledge, this work is the first comprehensive study to demonstrate how training-based methods can shape LLM personalities through learning from real human behaviors.</li>
<li><strong>摘要：</strong>在这项工作中，我们解决了将现实的人类性格特征嵌入 LLM 的挑战。以前的方法主要侧重于基于提示的方法，这些方法描述与所需性格特征相关的行为，存在现实性和有效性问题。为了解决这些限制，我们引入了 BIG5-CHAT，这是一个包含 100,000 个对话的大型数据集，旨在为人类如何在文本中表达自己的个性建立模型。利用这个数据集，我们探索监督微调和直接偏好优化作为基于训练的方法，以使 LLM 更自然地与人类性格模式保持一致。我们的方法在 BFI 和 IPIP-NEO 等人格评估中的表现优于提示，特征相关性与人类数据更接近。此外，我们的实验表明，经过训练表现出更高的尽责性、更高的亲和性、更低的外向性和更低的神经质的模型在推理任务中表现出更好的表现，这与这些特征如何影响人类认知表现的心理学发现一致。据我们所知，这项研究是第一项全面的研究，展示了基于培训的方法如何通过学习真实的人类行为来塑造法学硕士的个性。</li>
</ul>

<h3>Title: Rulebreakers Challenge: Revealing a Blind Spot in Large Language Models' Reasoning with Formal Logic</h3>
<ul>
<li><strong>Authors: </strong>Jason Chan, Robert Gaizauskas, Zhixue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16502">https://arxiv.org/abs/2410.16502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16502">https://arxiv.org/pdf/2410.16502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16502]] Rulebreakers Challenge: Revealing a Blind Spot in Large Language Models' Reasoning with Formal Logic(https://arxiv.org/abs/2410.16502)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Formal logic has long been applied to natural language reasoning, but this approach can sometimes lead to conclusions that, while logically entailed, are factually inconsistent with the premises or are not typically inferred by humans. This study introduces the concept of "rulebreakers", which refers to instances where logical entailment diverges from factually acceptable inference. We present RULEBREAKERS, a novel dataset for evaluating Large Language Models' (LLMs) ability to distinguish between rulebreakers and non-rulebreakers. Focusing on modus tollens and disjunctive syllogism, we assess six state-of-the-art LLMs using RULEBREAKERS, measuring their performance in terms of token-level exact accuracy and model confidence. Our findings reveal that while most models perform poorly to moderately in recognizing rulebreakers, they demonstrate a latent ability to distinguish rulebreakers when assessed by their confidence levels. Further analysis suggests that the failure to recognize rulebreakers is potentially associated with the models' world knowledge and their attention distribution patterns. This research highlights the limitation of LLMs' reasoning capabilities, and contributes to the ongoing discussion on reasoning in LLMs.</li>
<li><strong>摘要：</strong>形式逻辑早已应用于自然语言推理，但这种方法有时会得出一些结论，虽然这些结论在逻辑上是必然的，但在事实上与前提不一致，或者通常不是人类推断出来的。这项研究引入了“规则破坏者”的概念，它指的是逻辑蕴涵与事实上可接受的推理不一致的情况。我们提出了 RULEBREAKERS，这是一个用于评估大型语言模型 (LLM) 区分规则破坏者和非规则破坏者能力的新数据集。我们专注于否定后件和析取三段论，使用 RULEBREAKERS 评估了六种最先进的 LLM，衡量了它们在标记级精确度和模型置信度方面的性能。我们的研究结果表明，虽然大多数模型在识别规则破坏者方面表现不佳到中等，但当通过置信度水平进行评估时，它们表现出区分规则破坏者的潜在能力。进一步的分析表明，无法识别规则破坏者可能与模型的世界知识及其注意力分布模式有关。这项研究强调了法学硕士 (LLM) 推理能力的局限性，并有助于对法学硕士 (LLM) 推理能力的持续讨论。</li>
</ul>

<h3>Title: Learning from others' mistakes: Finetuning machine translation models with span-level error annotations</h3>
<ul>
<li><strong>Authors: </strong>Lily H. Zhang, Hamid Dadkhahi, Mara Finkelstein, Firas Trabelsi, Jiaming Luo, Markus Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16509">https://arxiv.org/abs/2410.16509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16509">https://arxiv.org/pdf/2410.16509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16509]] Learning from others' mistakes: Finetuning machine translation models with span-level error annotations(https://arxiv.org/abs/2410.16509)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite growing interest in incorporating feedback to improve language models, most efforts focus only on sequence-level annotations. In this work, we explore the potential of utilizing fine-grained span-level annotations from offline datasets to improve model quality. We develop a simple finetuning algorithm, called Training with Annotations (TWA), to directly train machine translation models on such annotated data. TWA utilizes targeted span-level error information while also flexibly learning what to penalize within a span. Moreover, TWA considers the overall trajectory of a sequence when deciding which non-error spans to utilize as positive signals. Experiments on English-German and Chinese-English machine translation show that TWA outperforms baselines such as Supervised FineTuning on sequences filtered for quality and Direct Preference Optimization on pairs constructed from the same data.</li>
<li><strong>摘要：</strong>尽管人们对结合反馈来改进语言模型的兴趣日益浓厚，但大多数努力仅集中在序列级注释上。在这项工作中，我们探索了利用离线数据集中的细粒度跨度级注释来提高模型质量的潜力。我们开发了一种简单的微调算法，称为带注释训练 (TWA)，以直接在此类带注释的数据上训练机器翻译模型。TWA 利用有针对性的跨度级错误信息，同时灵活地学习在跨度内要惩罚什么。此外，在决定将哪些非错误跨度用作正信号时，TWA 会考虑序列的整体轨迹。对英语-德语和中文-英语机器翻译的实验表明，TWA 的表现优于诸如对经过质量过滤的序列进行监督微调和对由相同数据构建的对进行直接偏好优化等基线。</li>
</ul>

<h3>Title: AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context</h3>
<ul>
<li><strong>Authors: </strong>Naba Rizvi, Harper Strickland, Daniel Gitelman, Tristan Cooper, Alexis Morales-Flores, Michael Golden, Aekta Kallepalli, Akshat Alurkar, Haaset Owens, Saleha Ahmedi, Isha Khirwadkar, Imani Munyaka, Nedjma Ousidhoum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16520">https://arxiv.org/abs/2410.16520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16520">https://arxiv.org/pdf/2410.16520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16520]] AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context(https://arxiv.org/abs/2410.16520)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As our understanding of autism and ableism continues to increase, so does our understanding of ableist language towards autistic people. Such language poses a significant challenge in NLP research due to its subtle and context-dependent nature. Yet, detecting anti-autistic ableist language remains underexplored, with existing NLP tools often failing to capture its nuanced expressions. We present AUTALIC, the first benchmark dataset dedicated to the detection of anti-autistic ableist language in context, addressing a significant gap in the field. The dataset comprises 2,400 autism-related sentences collected from Reddit, accompanied by surrounding context, and is annotated by trained experts with backgrounds in neurodiversity. Our comprehensive evaluation reveals that current language models, including state-of-the-art LLMs, struggle to reliably identify anti-autistic ableism and align with human judgments, underscoring their limitations in this domain. We publicly release AUTALIC along with the individual annotations which serve as a valuable resource to researchers working on ableism, neurodiversity, and also studying disagreements in annotation tasks. This dataset serves as a crucial step towards developing more inclusive and context-aware NLP systems that better reflect diverse perspectives.</li>
<li><strong>摘要：</strong>随着我们对自闭症和残疾歧视的理解不断加深，我们对针对自闭症患者的残疾歧视语言的理解也在不断加深。由于这种语言的微妙性和依赖上下文的特性，它对 NLP 研究提出了重大挑战。然而，检测反自闭症残疾歧视语言仍未得到充分探索，现有的 NLP 工具往往无法捕捉到其细微的表达。我们提出了 AUTALIC，这是第一个专门用于在上下文中检测反自闭症残疾歧视语言的基准数据集，填补了该领域的一个重大空白。该数据集包含从 Reddit 收集的 2,400 个与自闭症相关的句子，并附有周围环境，并由具有神经多样性背景的训练有素的专家进行注释。我们的全面评估表明，当前的语言模型（包括最先进的 LLM）难以可靠地识别反自闭症残疾歧视并与人类判断保持一致，这凸显了它们在这一领域的局限性。我们公开发布了 AUTALIC 以及个人注释，这对从事残疾歧视、神经多样性以及研究注释任务分歧的研究人员来说是一项宝贵的资源。该数据集是开发更具包容性和情境感知的 NLP 系统的关键一步，该系统可以更好地反映多样化的观点。</li>
</ul>

<h3>Title: Bayesian scaling laws for in-context learning</h3>
<ul>
<li><strong>Authors: </strong>Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.FL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16531">https://arxiv.org/abs/2410.16531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16531">https://arxiv.org/pdf/2410.16531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16531]] Bayesian scaling laws for in-context learning(https://arxiv.org/abs/2410.16531)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a family of novel Bayesian scaling laws for ICL. In experiments with \mbox{GPT-2} models of different sizes, our scaling laws exceed or match existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then experiment on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause the suppressed behavior to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 是一种强大的技术，可让语言模型在没有训练更新的情况下执行复杂任务。先前的研究已经建立了提供的上下文示例数量与模型预测准确性之间的强相关性。在本文中，我们试图通过展示 ICL 近似于贝叶斯学习者来解释这种相关性。这种观点产生了一系列用于 ICL 的新型贝叶斯缩放定律。在使用不同大小的 \mbox{GPT-2} 模型的实验中，我们的缩放定律在准确性上超过或匹配现有的缩放定律，同时还为任务先验、学习效率和每个示例的概率提供了可解释的术语。为了说明这种可解释的缩放定律提供的分析能力，我们报告了受控的合成数据集实验，旨在为现实世界的安全对齐研究提供信息。在我们的实验方案中，我们使用 SFT 来抑制不需要的现有模型功能，然后使用 ICL 尝试恢复该功能（多次越狱）。然后，我们使用能力基准以及新的多样本越狱数据集对现实世界的指令调整后的 LLM 进行实验。在所有情况下，贝叶斯缩放定律都能准确预测 ICL 会导致被抑制的行为重新出现的条件，这揭示了后训练在提高 LLM 安全性方面的无效性。</li>
</ul>

<h3>Title: A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration</h3>
<ul>
<li><strong>Authors: </strong>Yingqian Cui, Pengfei He, Xianfeng Tang, Qi He, Chen Luo, Jiliang Tang, Yue Xing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16540">https://arxiv.org/abs/2410.16540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16540">https://arxiv.org/pdf/2410.16540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16540]] A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration(https://arxiv.org/abs/2410.16540)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.</li>
<li><strong>摘要：</strong>少量的思维链 (CoT) 提示在提高大型语言模型 (LLM) 的推理能力方面表现出色。虽然已经进行了理论研究来理解 CoT，但这些研究中使用的底层转换器将 CoT 推理过程分离为独立的上下文学习步骤 (逐步 ICL)。在这项工作中，我们从理论上表明，与逐步 ICL 相比，如果整合了早期步骤 (连贯 CoT) 的推理，转换器将获得更好的纠错能力和更准确的预测。鉴于这种连贯推理会改变转换器的行为，我们进一步研究了当演示示例在推理阶段被破坏时，具有连贯 CoT 的转换器的敏感性。我们的理论结果表明，转换器对中间推理步骤中的错误比对最终结果更敏感。基于这一观察，我们提出了一种改进 CoT 的方法，即在演示中结合正确和不正确的推理路径。我们的实验验证了所提方法的有效性。</li>
</ul>

<h3>Title: Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongcheng Ding, Fuzhen Hu, Xuanze Zhao, Zixiao Jiang, Shamsul Nahar Abdullah, Deshinta Arrova Dewi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16589">https://arxiv.org/abs/2410.16589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16589">https://arxiv.org/pdf/2410.16589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16589]] Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis with Large Language Models(https://arxiv.org/abs/2410.16589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Sentiment analysis has become increasingly important for assessing public opinion and informing decision-making. Large language models (LLMs) have revolutionized this field by capturing nuanced language patterns. However, adapting LLMs to domain-specific sentiment analysis tasks remains challenging due to computational constraints and the need for optimal fine-tuning. To address these challenges, we propose a novel Dynamic Adaptive Rank Space Exploration (DARSE) framework for efficient and effective sentiment analysis using LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the optimal rank range, a fine-grained exploration algorithm to refine rank selection, and a dynamic rank allocation method to determine the optimal rank combination for each LLM layer. Extensive experiments demonstrate that DARSE significantly improves sentiment analysis accuracy, achieving a 15.1% improvement in MSE and a 4.3% improvement in accuracy compared to previous work. Our framework strikes a balance between computational efficiency and model performance, making it a promising approach for sentiment analysis with LLMs.</li>
<li><strong>摘要：</strong>情绪分析对于评估公众舆论和为决策提供信息变得越来越重要。大型语言模型 (LLM) 通过捕捉细微的语言模式彻底改变了这一领域。然而，由于计算限制和最佳微调的需求，将 LLM 适应特定领域的情绪分析任务仍然具有挑战性。为了应对这些挑战，我们提出了一种新颖的动态自适应秩空间探索 (DARSE) 框架，以使用 LLM 进行高效且有效的情绪分析。DARSE 由粗粒度贪婪算法组成，用于确定最佳秩范围，细粒度探索算法用于优化秩选择，以及动态秩分配方法用于确定每个 LLM 层的最佳秩组合。大量实验表明，与之前的工作相比，DARSE 显著提高了情绪分析的准确性，MSE 提高了 15.1%，准确性提高了 4.3%。我们的框架在计算效率和模型性能之间取得了平衡，使其成为使用 LLM 进行情绪分析的有前途的方法。</li>
</ul>

<h3>Title: Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for Improved Coverage and Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Prafulla Kumar Choubey, Xin Su, Man Luo, Xiangyu Peng, Caiming Xiong, Tiep Le, Shachar Rosenman, Vasudev Lal, Phil Mui, Ricky Ho, Phillip Howard, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16597">https://arxiv.org/abs/2410.16597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16597">https://arxiv.org/pdf/2410.16597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16597]] Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for Improved Coverage and Efficiency(https://arxiv.org/abs/2410.16597)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) generated by large language models (LLMs) are becoming increasingly valuable for Retrieval-Augmented Generation (RAG) applications that require knowledge-intensive reasoning. However, existing KG extraction methods predominantly rely on prompt-based approaches, which are inefficient for processing large-scale corpora. These approaches often suffer from information loss, particularly with long documents, due to the lack of specialized design for KG construction. Additionally, there is a gap in evaluation datasets and methodologies for ontology-free KG construction. To overcome these limitations, we propose SynthKG, a multi-step, document-level ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM on the synthesized document-KG pairs, we streamline the multi-step process into a single-step KG generation approach called Distill-SynthKG, substantially reducing the number of LLM inference calls. Furthermore, we re-purpose existing question-answering datasets to establish KG evaluation datasets and introduce new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a novel graph-based retrieval framework for RAG. Experimental results demonstrate that Distill-SynthKG not only surpasses all baseline models in KG quality -- including models up to eight times larger -- but also consistently excels in retrieval and question-answering tasks. Our proposed graph retrieval framework also outperforms all KG-retrieval methods across multiple benchmark datasets. We release the SynthKG dataset and Distill-SynthKG model publicly to support further research and development.</li>
<li><strong>摘要：</strong>由大型语言模型 (LLM) 生成的知识图谱 (KG) 对于需要知识密集型推理的检索增强生成 (RAG) 应用越来越有价值。然而，现有的 KG 提取方法主要依赖于基于提示的方法，这对于处理大规模语料库来说效率低下。由于缺乏专门的 KG 构建设计，这些方法通常会遭受信息丢失，尤其是在处理长文档时。此外，无本体 KG 构建的评估数据集和方法也存在差距。为了克服这些限制，我们提出了 SynthKG，这是一种基于 LLM 的多步骤、文档级无本体 KG 合成工作流程。通过在合成的文档-KG 对上微调较小的 LLM，我们将多步骤过程简化为单步 KG 生成方法 Distill-SynthKG，从而大大减少了 LLM 推理调用的次数。此外，我们重新利用现有的问答数据集来建立 KG 评估数据集并引入新的评估指标。我们还利用 Distill-SynthKG 生成的 KG，为 RAG 设计了一个基于图的新型检索框架。实验结果表明，Distill-SynthKG 不仅在 KG 质量上超越了所有基线模型（包括高达八倍大的模型），而且在检索和问答任务中也始终表现出色。我们提出的图检索框架在多个基准数据集上的表现也优于所有 KG 检索方法。我们公开发布了 SynthKG 数据集和 Distill-SynthKG 模型，以支持进一步的研究和开发。</li>
</ul>

<h3>Title: A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs</h3>
<ul>
<li><strong>Authors: </strong>Ryosuke Sonoda, Ramya Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16640">https://arxiv.org/abs/2410.16640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16640">https://arxiv.org/pdf/2410.16640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16640]] A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs(https://arxiv.org/abs/2410.16640)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) such as ChatGPT, GPT-4, Claude-3, and Llama are being integrated across a variety of industries. Despite this rapid proliferation, experts are calling for caution in the interpretation and adoption of LLMs, owing to numerous associated ethical concerns. Research has also uncovered shortcomings in LLMs' reasoning and logical abilities, raising questions on the potential of LLMs as evaluation tools. In this paper, we investigate LLMs' self-evaluation capabilities on a novel proverb reasoning task. We introduce a novel proverb database consisting of 300 proverb pairs that are similar in intent but different in wordings, across topics spanning gender, wisdom, and society. We propose tests to evaluate textual consistencies as well as numerical consistencies across similar proverbs, and demonstrate the effectiveness of our method and dataset in identifying failures in LLMs' self-evaluation which in turn can highlight issues related to gender stereotypes and lack of cultural understanding in LLMs.</li>
<li><strong>摘要：</strong>ChatGPT、GPT-4、Claude-3 和 Llama 等大型语言模型 (LLM) 正在各个行业中集成。尽管 LLM 迅速普及，但专家们仍呼吁谨慎解释和采用 LLM，因为存在许多相关的道德问题。研究还发现了 LLM 的推理和逻辑能力存在缺陷，这对 LLM 作为评估工具的潜力提出了质疑。在本文中，我们研究了 LLM 在一项新型谚语推理任务中的自我评估能力。我们引入了一个新型谚语数据库，该数据库由 300 对谚语组成，这些谚语在意图上相似，但在措辞上不同，涉及性别、智慧和社会等主题。我们提出了测试来评估类似谚语的文本一致性和数字一致性，并证明了我们的方法和数据集在识别 LLM 自我评估失败方面的有效性，这反过来可以凸显与 LLM 中的性别刻板印象和缺乏文化理解相关的问题。</li>
</ul>

<h3>Title: Chatting with Bots: AI, Speech Acts, and the Edge of Assertion</h3>
<ul>
<li><strong>Authors: </strong>Iwan Williams, Tim Bayne</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16645">https://arxiv.org/abs/2410.16645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16645">https://arxiv.org/pdf/2410.16645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16645]] Chatting with Bots: AI, Speech Acts, and the Edge of Assertion(https://arxiv.org/abs/2410.16645)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>This paper addresses the question of whether large language model-powered chatbots are capable of assertion. According to what we call the Thesis of Chatbot Assertion (TCA), chatbots are the kinds of things that can assert, and at least some of the output produced by current-generation chatbots qualifies as assertion. We provide some motivation for TCA, arguing that it ought to be taken seriously and not simply dismissed. We also review recent objections to TCA, arguing that these objections are weighty. We thus confront the following dilemma: how can we do justice to both the considerations for and against TCA? We consider two influential responses to this dilemma - the first appeals to the notion of proxy-assertion; the second appeals to fictionalism - and argue that neither is satisfactory. Instead, reflecting on the ontogenesis of assertion, we argue that we need to make space for a category of proto-assertion. We then apply the category of proto-assertion to chatbots, arguing that treating chatbots as proto-assertors provides a satisfactory resolution to the dilemma of chatbot assertion.</li>
<li><strong>摘要：</strong>本文探讨了大型语言模型驱动的聊天机器人是否能够断言的问题。根据我们所谓的聊天机器人断言论 (TCA)，聊天机器人是可以断言的事物，并且当前一代聊天机器人产生的部分输出至少符合断言的条件。我们为 TCA 提供了一些动机，认为应该认真对待它，而不是简单地将其驳回。我们还回顾了最近对 TCA 的反对意见，认为这些反对意见很有分量。因此，我们面临以下困境：我们如何才能公正地处理支持和反对 TCA 的考虑？我们考虑了对这一困境的两种有影响力的回应 - 第一种诉诸代理断言的概念；第二种诉诸虚构主义 - 并且认为两者都不令人满意。相反，反思断言的个体发生，我们认为我们需要为原始断言类别腾出空间。然后，我们将原始断言类别应用于聊天机器人，认为将聊天机器人视为原始断言者可以为聊天机器人断言的困境提供令人满意的解决方案。</li>
</ul>

<h3>Title: Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent</h3>
<ul>
<li><strong>Authors: </strong>Janghoon Ock, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16658">https://arxiv.org/abs/2410.16658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16658">https://arxiv.org/pdf/2410.16658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16658]] Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent(https://arxiv.org/abs/2410.16658)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Adsorption energy is a key reactivity descriptor in catalysis, enabling the efficient screening of potential catalysts. However, determining adsorption energy involves comparing the energies of multiple adsorbate-catalyst configurations, which is computationally demanding due to a large number of possible configurations. Current algorithmic approaches typically enumerate adsorption sites and configurations without leveraging theoretical insights to guide the initial setup. In this work, we present Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently derive system-specific stable adsorption configurations with minimal human intervention. Adsorb-Agent leverages built-in knowledge and emergent reasoning capabilities, significantly reducing the number of initial configurations required while improving accuracy in predicting the minimum adsorption energy. We demonstrate its performance using two example systems, NNH-CuPd3 (111) and NNH-Mo3Pd (111), for the Nitrogen Reduction Reaction (NRR), a sustainable alternative to the Haber-Bosch process. Adsorb-Agent outperforms conventional "heuristic" and "random" algorithms by identifying lower-energy configurations with fewer initial setups, reducing computational costs while enhancing accuracy. This highlights its potential to accelerate catalyst discovery.</li>
<li><strong>摘要：</strong>吸附能是催化反应性的关键描述符，可以有效筛选潜在的催化剂。然而，确定吸附能需要比较多种吸附物-催化剂配置的能量，由于可能的配置数量众多，因此计算量很大。当前的算法方法通常枚举吸附位点和配置，而不利用理论见解来指导初始设置。在这项工作中，我们提出了 Adsorb-Agent，这是一种大型语言模型 (LLM) 代理，旨在以最少的人为干预有效地得出系统特定的稳定吸附配置。Adsorb-Agent 利用内置知识和新兴推理能力，显着减少所需的初始配置数量，同时提高预测最小吸附能的准确性。我们使用两个示例系统 NNH-CuPd3 (111) 和 NNH-Mo3Pd (111) 来展示其在氮还原反应 (NRR) 中的性能，这是哈伯-博施工艺的可持续替代方案。 Adsorb-Agent 优于传统的“启发式”和“随机”算法，因为它可以用更少的初始设置识别低能量配置，从而降低计算成本并提高准确性。这凸显了其加速催化剂发现的潜力。</li>
</ul>

<h3>Title: RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary Detection in Partially Machine Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>Ram Mohan Rao Kadiyala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16659">https://arxiv.org/abs/2410.16659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16659">https://arxiv.org/pdf/2410.16659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16659]] RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary Detection in Partially Machine Generated Texts(https://arxiv.org/abs/2410.16659)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>With increasing usage of generative models for text generation and widespread use of machine generated texts in various domains, being able to distinguish between human written and machine generated texts is a significant challenge. While existing models and proprietary systems focus on identifying whether given text is entirely human written or entirely machine generated, only a few systems provide insights at sentence or paragraph level at likelihood of being machine generated at a non reliable accuracy level, working well only for a set of domains and generators. This paper introduces few reliable approaches for the novel task of identifying which part of a given text is machine generated at a word level while comparing results from different approaches and methods. We present a comparison with proprietary systems , performance of our model on unseen domains' and generators' texts. The findings reveal significant improvements in detection accuracy along with comparison on other aspects of detection capabilities. Finally we discuss potential avenues for improvement and implications of our work. The proposed model is also well suited for detecting which parts of a text are machine generated in outputs of Instruct variants of many LLMs.</li>
<li><strong>摘要：</strong>随着生成模型在文本生成的使用越来越多，以及机器生成文本在各个领域的广泛使用，区分人类书写的文本和机器生成的文本是一项重大挑战。虽然现有模型和专有系统专注于识别给定文本是完全由人类书写还是完全由机器生成的，但只有少数系统能够在句子或段落级别提供机器生成可能性的洞察，准确度不可靠，仅适用于一组域和生成器。本文介绍了几种可靠的方法，用于在单词级别识别给定文本的哪一部分是机器生成的新任务，同时比较了不同方法和方法的结果。我们将我们的模型在看不见的域和生成器的文本上的性能与专有系统进行了比较。研究结果显示检测准确度有显著提高，并在检测能力的其他方面进行了比较。最后，我们讨论了改进的潜在途径和我们工作的意义。所提出的模型还非常适合检测许多 LLM 的 Instruct 变体输出中文本的哪些部分是机器生成的。</li>
</ul>

<h3>Title: SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation</h3>
<ul>
<li><strong>Authors: </strong>Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16665">https://arxiv.org/abs/2410.16665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16665">https://arxiv.org/pdf/2410.16665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16665]] SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation(https://arxiv.org/abs/2410.16665)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>The ideal LLM content moderation system would be both structurally interpretable (so its decisions can be explained to users) and steerable (to reflect a community's values or align to safety standards). However, current systems fall short on both of these dimensions. To address this gap, we present SafetyAnalyst, a novel LLM safety moderation framework. Given a prompt, SafetyAnalyst creates a structured "harm-benefit tree," which identifies 1) the actions that could be taken if a compliant response were provided, 2) the harmful and beneficial effects of those actions (along with their likelihood, severity, and immediacy), and 3) the stakeholders that would be impacted by those effects. It then aggregates this structured representation into a harmfulness score based on a parameterized set of safety preferences, which can be transparently aligned to particular values. Using extensive harm-benefit features generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM to specialize in generating harm-benefit trees through symbolic knowledge distillation. On a comprehensive set of prompt safety benchmarks, we show that our system (average F1=0.75) outperforms existing LLM safety moderation systems (average F1$<$0.72) on prompt harmfulness classification, while offering the additional advantages of interpretability and steerability.</li>
<li><strong>摘要：</strong>理想的 LLM 内容审核系统应具有结构可解释性（因此其决策可以向用户解释）和可操控性（以反映社区的价值观或符合安全标准）。然而，目前的系统在这两个方面都存在不足。为了解决这一差距，我们提出了 SafetyAnalyst，这是一种新颖的 LLM 安全审核框架。给出提示后，SafetyAnalyst 会创建一个结构化的“危害-效益树”，该树可识别 1) 如果提供合规响应，可以采取的行动，2) 这些行动的有害和有益影响（以及它们的可能性、严重性和即时性），以及 3) 会受到这些影响的利益相关者。然后，它根据一组参数化的安全偏好将这种结构化表示聚合为危害性分数，这些分数可以透明地与特定值对齐。使用 SOTA LLM 在 19k 个提示上生成的大量危害-效益特征，我们对开放权重 LM 进行了微调，使其专门通过符号知识提炼生成危害-效益树。在一系列全面的即时安全基准上，我们表明我们的系统（平均 F1=0.75）在即时危害性分类方面优于现有的 LLM 安全调节系统（平均 F1$<$0.72），同时还提供了可解释性和可操纵性的额外优势。</li>
</ul>

<h3>Title: Methods of improving LLM training stability</h3>
<ul>
<li><strong>Authors: </strong>Oleg Rybakov, Mike Chrzanowski, Peter Dykas, Jinze Xue, Ben Lanir</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16682">https://arxiv.org/abs/2410.16682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16682">https://arxiv.org/pdf/2410.16682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16682]] Methods of improving LLM training stability(https://arxiv.org/abs/2410.16682)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Training stability of large language models(LLMs) is an important research topic. Reproducing training instabilities can be costly, so we use a small language model with 830M parameters and experiment with higher learning rates to force models to diverge. One of the sources of training instability is the growth of logits in attention layers. We extend the focus of the previous work and look not only at the magnitude of the logits but at all outputs of linear layers in the Transformer block. We observe that with a high learning rate the L2 norm of all linear layer outputs can grow with each training step and the model diverges. Specifically we observe that QKV, Proj and FC2 layers have the largest growth of the output magnitude. This prompts us to explore several options: 1) apply layer normalization not only after QK layers but also after Proj and FC2 layers too; 2) apply layer normalization after the QKV layer (and remove pre normalization). 3) apply QK layer normalization together with softmax capping. We show that with the last two methods we can increase learning rate by 1.5x (without model divergence) in comparison to an approach based on QK layer normalization only. Also we observe significant perplexity improvements for all three methods in comparison to the baseline model.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的训练稳定性是一个重要的研究课题。重现训练不稳定性的成本可能很高，因此我们使用具有 830M 个参数的小型语言模型，并尝试使用更高的学习率来强制模型发散。训练不稳定性的来源之一是注意力层中对数的增长。我们扩展了之前工作的重点，不仅关注对数的大小，还关注 Transformer 块中线性层的所有输出。我们观察到，在高学习率下，所有线性层输出的 L2 范数可以随着每个训练步骤而增长，并且模型发散。具体来说，我们观察到 QKV、Proj 和 FC2 层的输出幅度增长最大。这促使我们探索几种选择：1) 不仅在 QK 层之后应用层规范化，而且在 Proj 和 FC2 层之后也应用层规范化；2) 在 QKV 层之后应用层规范化（并删除预规范化）。3) 将 QK 层规范化与 softmax 上限一起应用。我们表明，与仅基于 QK 层规范化的方法相比，使用后两种方法我们可以将学习率提高 1.5 倍（不产生模型发散）。此外，与基线模型相比，我们观察到这三种方法的困惑度都有显著改善。</li>
</ul>

<h3>Title: PLDR-LLM: Large Language Model from Power Law Decoder Representations</h3>
<ul>
<li><strong>Authors: </strong>Burc Gokden</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16703">https://arxiv.org/abs/2410.16703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16703">https://arxiv.org/pdf/2410.16703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16703]] PLDR-LLM: Large Language Model from Power Law Decoder Representations(https://arxiv.org/abs/2410.16703)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present the Large Language Model from Power Law Decoder Representations (PLDR-LLM), a language model that leverages non-linear and linear transformations through Power Law Graph Attention mechanism to generate well-defined deductive and inductive outputs. We pretrain the PLDR-LLMs of varying layer sizes with a small batch size of 32 and $\sim$8B tokens from the RefinedWeb dataset, and show that they achieve competitive performance in zero-shot and few-shot settings compared to scaled dot-product LLMs of similar model size reported in the literature. We show that deductive outputs of PLDR-LLMs can be used to compare model characteristics or improve the performance by introducing the Directed Acyclic Graph (DAG) loss as a metric and regularizer. Our results indicate that the initial maximum learning rate and warm-up steps have a lasting impact on deductive outputs throughout the pretraining. We provide a detailed description of PLDR-LLM architecture, its implementation and the pretraining procedure.</li>
<li><strong>摘要：</strong>我们提出了基于幂律解码器表示的大型语言模型 (PLDR-LLM)，该语言模型通过幂律图注意机制利用非线性和线性变换来生成明确定义的演绎和归纳输出。我们使用 RefinedWeb 数据集中的 32 个小批量和 $\sim$8B 个标记对不同层大小的 PLDR-LLM 进行预训练，并表明与文献中报道的类似模型大小的缩放点积 LLM 相比，它们在零样本和少样本设置中实现了具有竞争力的性能。我们表明，PLDR-LLM 的演绎输出可用于比较模型特征或通过引入有向无环图 (DAG) 损失作为度量和正则化器来提高性能。我们的结果表明，初始最大学习率和预热步骤对整个预训练过程中的演绎输出具有持久影响。我们提供了 PLDR-LLM 架构、其实现和预训练过程的详细描述。</li>
</ul>

<h3>Title: Atomic Fact Decomposition Helps Attributed Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Xiaoli Li, Ru Li, Jeff Z.Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16708">https://arxiv.org/abs/2410.16708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16708">https://arxiv.org/pdf/2410.16708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16708]] Atomic Fact Decomposition Helps Attributed Question Answering(https://arxiv.org/abs/2410.16708)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Attributed Question Answering (AQA) aims to provide both a trustworthy answer and a reliable attribution report for a given question. Retrieval is a widely adopted approach, including two general paradigms: Retrieval-Then-Read (RTR) and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown remarkable proficiency, prompting growing interest in AQA among researchers. However, RTR-based AQA often suffers from irrelevant knowledge and rapidly changing information, even when LLMs are adopted, while post-hoc retrieval-based AQA struggles with comprehending long-form answers with complex logic, and precisely identifying the content needing revision and preserving the original intent. To tackle these problems, this paper proposes an Atomic fact decomposition-based Retrieval and Editing (ARE) framework, which decomposes the generated long-form answers into molecular clauses and atomic facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are fine-tuned using a well-constructed dataset, generated from large scale Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from a given set of entities and transforming the result into coherent long-form text. Subsequently, ARE leverages a search engine to retrieve evidences related to atomic facts, inputting these evidences into an LLM-based verifier to determine whether the facts require expansion for re-retrieval or editing. Furthermore, the edited facts are backtracked into the original answer, with evidence aggregated based on the relationship between molecular clauses and atomic facts. Extensive evaluations demonstrate the superior performance of our proposed method over the state-of-the-arts on several datasets, with an additionally proposed new metric $Attr_{p}$ for evaluating the precision of evidence attribution.</li>
<li><strong>摘要：</strong>归因问答系统 (AQA) 旨在为给定问题提供可信的答案和可靠的归因报告。检索是一种广泛采用的方法，包括两种通用范式：检索后阅读 (RTR) 和事后检索。近年来，大型语言模型 (LLM) 表现出色，引起了研究人员对 AQA 的兴趣。然而，即使采用 LLM，基于 RTR 的 AQA 也经常受到不相关知识和快速变化的信息的影响，而基于事后检索的 AQA 难以理解具有复杂逻辑的长格式答案，也无法准确识别需要修改的内容并保留原始意图。为了解决这些问题，本文提出了一种基于原子事实分解的检索和编辑 (ARE) 框架，该框架通过指令调优的 LLM 将生成的长格式答案分解为分子子句和原子事实。值得注意的是，指令调整的 LLM 是使用精心构建的数据集进行微调的，该数据集由大规模知识图谱 (KG) 生成。此过程涉及从给定的一组实体中提取一跳邻居，并将结果转换为连贯的长格式文本。随后，ARE 利用搜索引擎检索与原子事实相关的证据，将这些证据输入基于 LLM 的验证器，以确定事实是否需要扩展以进行重新检索或编辑。此外，编辑后的事实会回溯到原始答案，并根据分子子句和原子事实之间的关系汇总证据。广泛的评估表明，我们提出的方法在多个数据集上的性能优于最先进的方法，并另外提出了一个新指标 $Attr_{p}$ 来评估证据归因的准确性。</li>
</ul>

<h3>Title: Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment</h3>
<ul>
<li><strong>Authors: </strong>Mingzhi Wang, Chengdong Ma, Qizhi Chen, Linjian Meng, Yang Han, Jiancong Xiao, Zhaowei Zhang, Jing Huo, Weijie J. Su, Yaodong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16714">https://arxiv.org/abs/2410.16714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16714">https://arxiv.org/pdf/2410.16714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16714]] Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment(https://arxiv.org/abs/2410.16714)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) performance but also overcomes the limitations of traditional Bradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a preference-based, two-player constant-sum game. However, existing methods either guarantee only average-iterate convergence, incurring high storage and inference costs, or converge to the NE of a regularized game, failing to accurately reflect true human preferences. In this paper, we introduce Magnetic Preference Optimization (MPO), a novel approach capable of achieving last-iterate convergence to the NE of the original game, effectively overcoming the limitations of existing methods. Building upon Magnetic Mirror Descent (MMD), MPO attains a linear convergence rate, making it particularly suitable for fine-tuning LLMs. To ensure our algorithm is both theoretically sound and practically viable, we present a simple yet effective implementation that adapts the theoretical insights to the RLHF setting. Empirical results demonstrate that MPO can significantly enhance the performance of LLMs, highlighting the potential of self-play methods in alignment.</li>
<li><strong>摘要：</strong>自对弈方法在增强各个领域的模型能力方面取得了显著成功。在人类反馈强化学习 (RLHF) 的背景下，自对弈不仅可以提高大型语言模型 (LLM) 的性能，还可以通过找到基于偏好的双人常数和博弈的纳什均衡 (NE) 来克服传统 Bradley-Terry (BT) 模型假设的局限性。然而，现有方法要么只保证平均迭代收敛，从而产生高存储和推理成本，要么收敛到正则化游戏的 NE，无法准确反映真实的人类偏好。在本文中，我们引入了磁性偏好优化 (MPO)，这是一种能够实现最后一次迭代收敛到原始游戏 NE 的新方法，有效地克服了现有方法的局限性。在磁​​性镜像下降 (MMD) 的基础上，MPO 实现了线性收敛速度，使其特别适合微调 LLM。为了确保我们的算法在理论上合理且在实践中可行，我们提出了一种简单而有效的实现方法，将理论见解应用于 RLHF 设置。实证结果表明，MPO 可以显著提高 LLM 的性能，凸显了自对弈方法在对齐方面的潜力。</li>
</ul>

<h3>Title: Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through Failure-Inducing Exploration</h3>
<ul>
<li><strong>Authors: </strong>Qintong Li, Jiahui Gao, Sheng Wang, Renjie Pi, Xueliang Zhao, Chuan Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16736">https://arxiv.org/abs/2410.16736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16736">https://arxiv.org/pdf/2410.16736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16736]] Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through Failure-Inducing Exploration(https://arxiv.org/abs/2410.16736)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly benefited from training on diverse, high-quality task-specific data, leading to impressive performance across a range of downstream applications. Current methods often rely on human-annotated data or predefined task templates to direct powerful LLMs in synthesizing task-relevant data for effective model training. However, this dependence on manually designed components may constrain the scope of generated data, potentially overlooking critical edge cases or novel scenarios that could challenge the model. In this paper, we present a novel approach, ReverseGen, designed to automatically generate effective training samples that expose the weaknesses of LLMs. Specifically, we introduce a dedicated proposer trained to produce queries that lead target models to generate unsatisfactory responses. These failure-inducing queries are then used to construct training data, helping to address the models' shortcomings and improve overall performance. Our approach is flexible and can be applied to models of various scales (3B, 7B, and 8B). We evaluate ReverseGen on three key applications (safety, honesty, and math), demonstrating that our generated data is both highly effective and diverse. Models fine-tuned with ReverseGen-generated data consistently outperform those trained on human-annotated or general model-generated data, offering a new perspective on data synthesis for task-specific LLM enhancement.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已从对多样化、高质量任务特定数据的训练中受益匪浅，从而在一系列下游应用中取得了令人印象深刻的性能。当前的方法通常依赖于人工注释的数据或预定义的任务模板来指导强大的 LLM 合成与任务相关的数据以进行有效的模型训练。然而，这种对手动设计组件的依赖可能会限制生成数据的范围，可能会忽略可能挑战模型的关键边缘情况或新场景。在本文中，我们提出了一种新方法 ReverseGen，旨在自动生成有效的训练样本，以暴露 LLM 的弱点。具体来说，我们引入了一个专门的提议器，该提议器经过训练可以生成查询，导致目标模型生成不令人满意的响应。然后使用这些导致失败的查询来构建训练数据，帮助解决模型的缺点并提高整体性能。我们的方法很灵活，可以应用于各种规模的模型（3B、7B 和 8B）。我们从三个关键应用（安全性、诚实性和数学）对 ReverseGen 进行了评估，结果表明我们生成的数据既高效又多样化。使用 ReverseGen 生成的数据进行微调的模型始终优于使用人工注释或通用模型生成的数据进行训练的模型，为数据合成提供了新的视角，从而增强特定任务的 LLM。</li>
</ul>

<h3>Title: Context-Aware LLM Translation System Using Conversation Summarization and Dialogue History</h3>
<ul>
<li><strong>Authors: </strong>Mingi Sung, Seungmin Lee, Jiwon Kim, Sejoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16775">https://arxiv.org/abs/2410.16775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16775">https://arxiv.org/pdf/2410.16775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16775]] Context-Aware LLM Translation System Using Conversation Summarization and Dialogue History(https://arxiv.org/abs/2410.16775)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Translating conversational text, particularly in customer support contexts, presents unique challenges due to its informal and unstructured nature. We propose a context-aware LLM translation system that leverages conversation summarization and dialogue history to enhance translation quality for the English-Korean language pair. Our approach incorporates the two most recent dialogues as raw data and a summary of earlier conversations to manage context length effectively. We demonstrate that this method significantly improves translation accuracy, maintaining coherence and consistency across conversations. This system offers a practical solution for customer support translation tasks, addressing the complexities of conversational text.</li>
<li><strong>摘要：</strong>翻译对话文本（尤其是在客户支持环境中）因其非正式和非结构化性质而面临独特的挑战。我们提出了一种上下文感知的 LLM 翻译系统，该系统利用对话摘要和对话历史来提高英语-韩语对的翻译质量。我们的方法将最近的两个对话作为原始数据，并将早期对话的摘要结合起来，以有效管理上下文长度。我们证明这种方法显著提高了翻译准确性，保持了对话之间的连贯性和一致性。该系统为客户支持翻译任务提供了一个实用的解决方案，解决了对话文本的复杂性。</li>
</ul>

<h3>Title: Beyond Retrieval: Generating Narratives in Conversational Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16780">https://arxiv.org/abs/2410.16780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16780">https://arxiv.org/pdf/2410.16780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16780]] Beyond Retrieval: Generating Narratives in Conversational Recommender Systems(https://arxiv.org/abs/2410.16780)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions. First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.</li>
<li><strong>摘要：</strong>大型语言模型生成和推理能力的最新进展为开发真正的对话式推荐系统提供了机会。然而，有效地将推荐系统知识集成到 LLM 中，以生成针对推荐任务的自然语言，这仍然是一个挑战。本文通过两个关键贡献解决了这一挑战。首先，我们为对话式推荐中的自然语言生成任务引入了一个新数据集 (REGEN)。REGEN（通过生成性叙述增强的评论）通过丰富的用户叙述扩展了亚马逊产品评论数据集，包括个性化的产品偏好解释、推荐商品的产品代言以及用户购买历史摘要。REGEN 已公开，以促进进一步的研究。此外，我们使用众所周知的生成指标建立基准，并使用评估者 LLM 对新数据集进行自动评估。其次，本文介绍了一种融合架构（带有 LLM 的 CF 模型），作为 REGEN 的基线。据我们所知，这是首次尝试分析 LLM 在理解推荐信号和生成丰富叙述方面的能力。我们证明，LLM 可以利用基于交互的 CF 嵌入有效地从简单的融合架构中学习，并且可以使用与项目相关的元数据和个性化数据进一步增强这一点。我们的实验表明，与单独使用任一类型的嵌入相比，结合使用 CF 和内容嵌入可以使关键语言指标提高 4-12%。我们还提供了分析来解释 CF 和内容嵌入如何有助于这项新的生成任务。</li>
</ul>

<h3>Title: Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Lu, Bingshuo Qian, Caixia Yuan, Huixing Jiang, Xiaojie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16801">https://arxiv.org/abs/2410.16801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16801">https://arxiv.org/pdf/2410.16801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16801]] Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models(https://arxiv.org/abs/2410.16801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable capabilities in natural language processing but face catastrophic forgetting when learning new tasks, where adaptation to a new domain leads to a substantial decline in performance on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a subspace regularization method on LoRA structure. Aiming to reduce the scale of output change while introduce minimal constraint on model capacity, CLoRA imposes constraint on the direction of updating matrix null space. Experimental results on commonly used LLM finetuning tasks reveal that CLoRA significantly outperforms existing LoRA subsequent methods on both in-domain and outdomain evaluations, highlighting the superority of CLoRA as a effective parameter-efficient finetuning method with catastrophic forgetting mitigating. Further investigation for model parameters indicates that CLoRA effectively balances the trade-off between model capacity and degree of forgetting.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在自然语言处理中表现出卓越的能力，但在学习新任务时面临灾难性遗忘的问题，即对新领域的适应会导致先前任务的性能大幅下降。在本文中，我们提出了受控的LoRA（CLoRA），一种基于LoRA结构的子空间正则化方法。CLoRA对更新矩阵零空间的方向施加了约束，旨在减小输出变化的规模，同时对模型容量引入最小的约束。在常用的LLM微调任务上的实验结果表明，CLoRA在域内和域外评估中都明显优于现有的LoRA后续方法，凸显了CLoRA作为一种有效的参数高效微调方法的优势，并能减轻灾难性遗忘。对模型参数的进一步研究表明，CLoRA有效地平衡了模型容量和遗忘程度之间的权衡。</li>
</ul>

<h3>Title: Optimizing Chain-of-Thought Reasoning: Tackling Arranging Bottleneck via Plan Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuli Qiu, Jiashu Yao, Heyan Huang, Yuhang Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16812">https://arxiv.org/abs/2410.16812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16812">https://arxiv.org/pdf/2410.16812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16812]] Optimizing Chain-of-Thought Reasoning: Tackling Arranging Bottleneck via Plan Augmentation(https://arxiv.org/abs/2410.16812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Multi-step reasoning ability of large language models is crucial in tasks such as math and tool utilization. Current researches predominantly focus on enhancing model performance in these multi-step reasoning tasks through fine-tuning with Chain-of-Thought (CoT) steps, yet these methods tend to be heuristic, without exploring nor resolving the bottleneck. In this study, we subdivide CoT reasoning into two parts: arranging and executing, and identify that the bottleneck of models mainly lies in arranging rather than executing. Based on this finding, we propose a plan-based training and reasoning method that guides models to generate arranging steps through abstract plans. We experiment on both math (GSM8k) and tool utilization (ToolBench) benchmarks. Results show that compared to fine-tuning directly with CoT data, our approach achieves a better performance on alleviating arranging bottleneck, particularly excelling in long-distance reasoning generalization.</li>
<li><strong>摘要：</strong>大型语言模型的多步骤推理能力在数学、工具利用等任务中至关重要。当前的研究主要集中于通过使用思维链 (CoT) 步骤进行微调来提升模型在这些多步骤推理任务中的表现，但这些方法往往是启发式的，没有探索或解决瓶颈问题。在本研究中，我们将 CoT 推理细分为排列和执行两个部分，并发现模型的瓶颈主要在于排列而非执行。基于这一发现，我们提出了一种基于计划的训练和推理方法，通过抽象计划引导模型生成排列步骤。我们在数学 (GSM8k) 和工具利用 (ToolBench) 基准上进行了实验。结果表明，与直接使用 CoT 数据进行微调相比，我们的方法在缓解排列瓶颈方面取得了更好的效果，尤其在长距离推理泛化方面表现出色。</li>
</ul>

<h3>Title: Assessment of Transformer-Based Encoder-Decoder Model for Human-Like Summarization</h3>
<ul>
<li><strong>Authors: </strong>Sindhu Nair, Y.S. Rao, Radha Shankarmani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16842">https://arxiv.org/abs/2410.16842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16842">https://arxiv.org/pdf/2410.16842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16842]] Assessment of Transformer-Based Encoder-Decoder Model for Human-Like Summarization(https://arxiv.org/abs/2410.16842)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In recent times, extracting valuable information from large text is making significant progress. Especially in the current era of social media, people expect quick bites of information. Automatic text summarization seeks to tackle this by slimming large texts down into more manageable summaries. This important research area can aid in decision-making by digging out salient content from large text. With the progress in deep learning models, significant work in language models has emerged. The encoder-decoder framework in deep learning has become the central approach for automatic text summarization. This work leverages transformer-based BART model for human-like summarization which is an open-ended problem with many challenges. On training and fine-tuning the encoder-decoder model, it is tested with diverse sample articles and the quality of summaries of diverse samples is assessed based on human evaluation parameters. Further, the finetuned model performance is compared with the baseline pretrained model based on evaluation metrics like ROUGE score and BERTScore. Additionally, domain adaptation of the model is required for improved performance of abstractive summarization of dialogues between interlocutors. On investigating, the above popular evaluation metrics are found to be insensitive to factual errors. Further investigation of the summaries generated by finetuned model is done using the contemporary evaluation metrics of factual consistency like WeCheck and SummaC. Empirical results on BBC News articles highlight that the gold standard summaries written by humans are more factually consistent by 17% than the abstractive summaries generated by finetuned model.</li>
<li><strong>摘要：</strong>近年来，从大文本中提取有价值的信息正在取得重大进展。尤其是在当今的社交媒体时代，人们期望快速获取信息。自动文本摘要试图通过将大文本精简为更易于管理的摘要来解决此问题。这个重要的研究领域可以通过从大文本中挖掘出突出的内容来帮助决策。随着深度学习模型的进步，语言模型方面的重大研究已经出现。深度学习中的编码器-解码器框架已成为自动文本摘要的核心方法。这项工作利用基于转换器的 BART 模型进行类似人类的摘要，这是一个具有许多挑战的开放式问题。在训练和微调编码器-解码器模型时，使用不同的样本文章对其进行测试，并根据人工评估参数评估不同样本的摘要质量。此外，根据 ROUGE 分数和 BERTScore 等评估指标，将微调后的模型性能与基线预训练模型进行比较。此外，需要对模型进行领域自适应，以提高对话者之间对话的抽象摘要性能。调查发现，上述流行的评估指标对事实错误不敏感。使用 WeCheck 和 SummaC 等当代事实一致性评估指标对微调模型生成的摘要进行了进一步调查。BBC 新闻文章的实证结果表明，人类撰写的黄金标准摘要比微调模型生成的抽象摘要在事实一致性上高出 17%。</li>
</ul>

<h3>Title: Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zongmeng Zhang, Yufeng Shi, Jinhua Zhu, Wengang Zhou, Xiang Qi, Peng Zhang, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16843">https://arxiv.org/abs/2410.16843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16843">https://arxiv.org/pdf/2410.16843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16843]] Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning(https://arxiv.org/abs/2410.16843)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external evidence, retrieval-augmented generation still suffers from hallucinations, one primary cause of which is the conflict between contextual and parametric knowledge. We deem that retrieval-augmented language models have the inherent capabilities of supplying response according to both contextual and parametric knowledge. Inspired by aligning language models with human preference, we take the first step towards aligning retrieval-augmented language models to a status where it responds relying merely on the external evidence and disregards the interference of parametric knowledge. Specifically, we propose a reinforcement learning based algorithm Trustworthy-Alignment, theoretically and experimentally demonstrating large language models' capability of reaching a trustworthy status without explicit supervision on how to respond. Our work highlights the potential of large language models on exploring its intrinsic abilities by its own and expands the application scenarios of alignment from fulfilling human preference to creating trustworthy agents.</li>
<li><strong>摘要：</strong>可信度是大型语言模型在实际应用中的必要前提。本文主要研究语言模型在检索增强方面的可信度。尽管有外部证据的支持，检索增强生成仍然受到幻觉的影响，其中一个主要原因是上下文知识和参数知识之间的冲突。我们认为检索增强语言模型具有根据上下文知识和参数知识提供响应的内在能力。受到将语言模型与人类偏好对齐的启发，我们迈出了将检索增强语言模型对齐到仅依赖外部证据做出响应并忽略参数知识干扰的状态的第一步。具体而言，我们提出了一种基于强化学习的算法 Trustworthy-Alignment，从理论和实验上证明了大型语言模型能够在没有明确监督如何响应的情况下达到可信状态的能力。我们的工作凸显了大型语言模型自行探索其内在能力的潜力，并扩展了对齐的应用场景，从满足人类偏好到创建值得信赖的代理。</li>
</ul>

<h3>Title: ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage</h3>
<ul>
<li><strong>Authors: </strong>Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16848">https://arxiv.org/abs/2410.16848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16848">https://arxiv.org/pdf/2410.16848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16848]] ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage(https://arxiv.org/abs/2410.16848)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLM) capable of processing extremely long texts highlight the need for a dedicated evaluation benchmark to assess their long-context capabilities. However, existing methods, like the needle-in-a-haystack test, do not effectively assess whether these models fully utilize contextual information, raising concerns about the reliability of current evaluation techniques. To thoroughly examine the effectiveness of existing benchmarks, we introduce a new metric called information coverage (IC), which quantifies the proportion of the input context necessary for answering queries. Our findings indicate that current benchmarks exhibit low IC; although the input context may be extensive, the actual usable context is often limited. To address this, we present ETHIC, a novel benchmark designed to assess LLMs' ability to leverage the entire context. Our benchmark comprises 2,648 test instances spanning four long-context tasks with high IC scores in the domains of books, debates, medicine, and law. Our evaluations reveal significant performance drops in contemporary LLMs, highlighting a critical challenge in managing long contexts. Our benchmark is available at this https URL.</li>
<li><strong>摘要：</strong>能够处理极长文本的大型语言模型 (LLM) 的最新进展凸显了对专门的评估基准的需要，以评估它们的长上下文能力。然而，现有方法（如大海捞针测试）无法有效评估这些模型是否充分利用了上下文信息，这引发了人们对当前评估技术可靠性的担忧。为了彻底检查现有基准的有效性，我们引入了一个新的指标，称为信息覆盖率 (IC)，它量化了回答查询所需的输入上下文的比例。我们的研究结果表明，当前的基准表现出较低的 IC；尽管输入上下文可能很广泛，但实际可用的上下文通常有限。为了解决这个问题，我们提出了 ETHIC，这是一种旨在评估 LLM 利用整个上下文能力的新基准。我们的基准包括 2,648 个测试实例，涵盖四个长上下文任务，在书籍、辩论、医学和法律领域具有较高的 IC 分数。我们的评估显示，当代 LLM 的性能显著下降，凸显了管理长上下文的关键挑战。我们的基准可在此 https URL 上找到。</li>
</ul>

<h3>Title: Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</h3>
<ul>
<li><strong>Authors: </strong>Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16930">https://arxiv.org/abs/2410.16930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16930">https://arxiv.org/pdf/2410.16930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16930]] Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes(https://arxiv.org/abs/2410.16930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.</li>
<li><strong>摘要：</strong>数学推理是大型语言模型 (LLM) 研究中一个非常活跃的领域，因为它是人工智能的标志。然而，很少有研究探讨数学推理是如何在 LLM 参数中编码的，以及它是否是一种可以在模型中分离出来的技能。这样做可以进行有针对性的干预，以提高数学成绩，而不会改变非数学行为，并促进对模型如何编码数学推理的理解。我们引入了数学神经外科 (MathNeuro)，这是一种仅使用前向传递来隔离 LLM 中数学特定参数的方法。MathNeuro 以现有工作为基础，使用权重和激活来计算参数重要性，但通过删除对一般语言任务很重要的参数来隔离数学特定参数。修剪 MathNeuro 识别的参数会删除 LLM 的数学推理能力，而不会破坏其一般语言能力。将这些参数缩放一个小常数可以将预训练或指令调整的 LLM 在 GSM8K 上的性能提高 4-17%，同时保持非数学行为不变。 MathNeuro 的数据效率也很高：它的大部分有效性在使用单个样本识别数学特定参数时都有效。MathNeuro 强调了未来研究干预数学特定参数的潜力。</li>
</ul>

<h3>Title: Learning Mathematical Rules with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Antoine Gorceix, Bastien Le Chenadec, Ahmad Rammal, Nelson Vadori, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16973">https://arxiv.org/abs/2410.16973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16973">https://arxiv.org/pdf/2410.16973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16973]] Learning Mathematical Rules with Large Language Models(https://arxiv.org/abs/2410.16973)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we study the ability of large language models to learn specific mathematical rules such as distributivity or simplifying equations. We present an empirical analysis of their ability to generalize these rules, as well as to reuse them in the context of word problems. For this purpose, we provide a rigorous methodology to build synthetic data incorporating such rules, and perform fine-tuning of large language models on such data. Our experiments show that our model can learn and generalize these rules to some extent, as well as suitably reuse them in the context of word problems.</li>
<li><strong>摘要：</strong>在本文中，我们研究了大型语言模型学习特定数学规则（例如分配律或简化方程）的能力。我们对它们概括这些规则以及在应用题中重用这些规则的能力进行了实证分析。为此，我们提供了一种严格的方法来构建包含此类规则的合成数据，并在此类数据上对大型语言模型进行微调。我们的实验表明，我们的模型可以在一定程度上学习和概括这些规则，并适当地在应用题中重用它们。</li>
</ul>

<h3>Title: IPL: Leveraging Multimodal Large Language Models for Intelligent Product Listing</h3>
<ul>
<li><strong>Authors: </strong>Kang Chen, Qingheng Zhang, Chengbao Lian, Yixin Ji, Xuwei Liu, Shuguang Han, Guoqiang Wu, Fei Huang, Jufeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16977">https://arxiv.org/abs/2410.16977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16977">https://arxiv.org/pdf/2410.16977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16977]] IPL: Leveraging Multimodal Large Language Models for Intelligent Product Listing(https://arxiv.org/abs/2410.16977)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g., Amazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are mainly targeting individual sellers who usually lack sufficient experience in e-commerce. Individual sellers often struggle to compose proper descriptions for selling products. With the recent advancement of Multimodal Large Language Models (MLLMs), we attempt to integrate such state-of-the-art generative AI technologies into the product listing process. To this end, we develop IPL, an Intelligent Product Listing tool tailored to generate descriptions using various product attributes such as category, brand, color, condition, etc. IPL enables users to compose product descriptions by merely uploading photos of the selling product. More importantly, it can imitate the content style of our C2C platform Xianyu. This is achieved by employing domain-specific instruction tuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation (RAG) process. A comprehensive empirical evaluation demonstrates that the underlying model of IPL significantly outperforms the base model in domain-specific tasks while producing less hallucination. IPL has been successfully deployed in our production system, where 72% of users have their published product listings based on the generated content, and those product listings are shown to have a quality score 5.6% higher than those without AI assistance.</li>
<li><strong>摘要：</strong>与专业的企业对消费者 (B2C) 电子商务平台（例如亚马逊）不同，消费者对消费者 (C2C) 平台（例如 Facebook 市场）主要针对通常缺乏足够电子商务经验的个人卖家。个人卖家通常很难为所售产品撰写适当的描述。随着多模态大型语言模型 (MLLM) 的最新进展，我们尝试将这种最先进的生成式 AI 技术集成到产品列表流程中。为此，我们开发了 IPL，这是一种智能产品列表工具，专门用于使用各种产品属性（例如类别、品牌、颜色、条件等）生成描述。IPL 使用户只需上传所售产品的照片即可撰写产品描述。更重要的是，它可以模仿我们 C2C 平台闲鱼的内容风格。这是通过在 MLLM 上采用领域特定指令调整并采用多模态检索增强生成 (RAG) 过程实现的。全面的实证评估表明，IPL 的底层模型在特定领域任务中的表现明显优于基础模型，同时产生的幻觉更少。IPL 已成功部署在我们的生产系统中，其中 72% 的用户根据生成的内容发布了产品列表，这些产品列表的质量得分比没有 AI 帮助的用户高出 5.6%。</li>
</ul>

<h3>Title: Exploring Forgetting in Large Language Model Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Chonghua Liao, Ruobing Xie, Xingwu Sun, Haowen Sun, Zhanhui Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17018">https://arxiv.org/abs/2410.17018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17018">https://arxiv.org/pdf/2410.17018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17018]] Exploring Forgetting in Large Language Model Pre-Training(https://arxiv.org/abs/2410.17018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting remains a formidable obstacle to building an omniscient model in large language models (LLMs). Despite the pioneering research on task-level forgetting in LLM fine-tuning, there is scant focus on forgetting during pre-training. We systematically explored the existence and measurement of forgetting in pre-training, questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention. Based on our revised assessment of forgetting metrics, we explored low-cost, straightforward methods to mitigate forgetting during the pre-training phase. Further, we carefully analyzed the learning curves, offering insights into the dynamics of forgetting. Extensive evaluations and analyses on forgetting of pre-training could facilitate future research on LLMs.</li>
<li><strong>摘要：</strong>灾难性遗忘仍然是在大型语言模型 (LLM) 中构建全知模型的巨大障碍。尽管在 LLM 微调中对任务级遗忘进行了开创性的研究，但对预训练期间的遗忘的关注却很少。我们系统地探索了预训练中遗忘的存在和测量，质疑了困惑度 (PPL) 等传统指标，并引入了新指标以更好地检测实体记忆保留。基于我们对遗忘指标的修订评估，我们探索了在预训练阶段减轻遗忘的低成本、直接的方法。此外，我们仔细分析了学习曲线，深入了解了遗忘的动态。对预训练遗忘的广泛评估和分析可以促进未来对 LLM 的研究。</li>
</ul>

<h3>Title: SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine</h3>
<ul>
<li><strong>Authors: </strong>Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17021">https://arxiv.org/abs/2410.17021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17021">https://arxiv.org/pdf/2410.17021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17021]] SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine(https://arxiv.org/abs/2410.17021)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.</li>
<li><strong>摘要：</strong>具有思路链提示的大型语言模型（例如 OpenAI-o1）在自然语言推理任务中表现出了令人印象深刻的能力。然而，由于幻觉、错误传播和上下文长度有限等问题，多跳问答 (MHQA) 对于许多现有模型来说仍然具有挑战性。为了应对这些挑战并提高 LLM 在 MHQA 上的性能，我们提出了自引导提示有限状态机 (SG-FSM)，旨在增强多跳推理能力。与传统的思路链方法不同，SG-FSM 通过迭代地将复杂问题分解为子问题来解决 MHQA，并自我纠正以提高准确性。它一次处理一个子问题，根据当前上下文和结果动态决定下一步，功能类似于自动机。跨各种基准的实验证明了我们方法的有效性，在 Musique 等具有挑战性的数据集上的表现优于强大的基线。SG-FSM 减少了幻觉，尽管中间有错误，但仍能恢复正确的最终答案。它还提高了对指定输出格式的遵守，大大简化了评估。</li>
</ul>

<h3>Title: DIRI: Adversarial Patient Reidentification with Large Language Models for Evaluating Clinical Text Anonymization</h3>
<ul>
<li><strong>Authors: </strong>John X. Morris, Thomas R. Campion, Sri Laasya Nutheti, Yifan Peng, Akhil Raj, Ramin Zabih, Curtis L. Cole</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17035">https://arxiv.org/abs/2410.17035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17035">https://arxiv.org/pdf/2410.17035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17035]] DIRI: Adversarial Patient Reidentification with Large Language Models for Evaluating Clinical Text Anonymization(https://arxiv.org/abs/2410.17035)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Sharing protected health information (PHI) is critical for furthering biomedical research. Before data can be distributed, practitioners often perform deidentification to remove any PHI contained in the text. Contemporary deidentification methods are evaluated on highly saturated datasets (tools achieve near-perfect accuracy) which may not reflect the full variability or complexity of real-world clinical text and annotating them is resource intensive, which is a barrier to real-world applications. To address this gap, we developed an adversarial approach using a large language model (LLM) to re-identify the patient corresponding to a redacted clinical note and evaluated the performance with a novel De-Identification/Re-Identification (DIRI) method. Our method uses a large language model to reidentify the patient corresponding to a redacted clinical note. We demonstrate our method on medical data from Weill Cornell Medicine anonymized with three deidentification tools: rule-based Philter and two deep-learning-based models, BiLSTM-CRF and ClinicalBERT. Although ClinicalBERT was the most effective, masking all identified PII, our tool still reidentified 9% of clinical notes Our study highlights significant weaknesses in current deidentification technologies while providing a tool for iterative development and improvement.</li>
<li><strong>摘要：</strong>共享受保护的健康信息 (PHI) 对于促进生物医学研究至关重要。在分发数据之前，从业者通常会执行去识别化以删除文本中包含的任何 PHI。当代去识别化方法是在高度饱和的数据集上进行评估的（工具可实现近乎完美的准确性），这些数据集可能无法反映现实世界临床文本的全部变化或复杂性，并且注释它们需要大量资源，这对现实世界的应用是一个障碍。为了解决这一差距，我们开发了一种对抗性方法，使用大型语言模型 (LLM) 重新识别与删节临床记录相对应的患者，并使用新颖的去识别/重新识别 (DIRI) 方法评估了性能。我们的方法使用大型语言模型重新识别与删节临床记录相对应的患者。我们在来自威尔康奈尔医学院的医疗数据上展示了我们的方法，这些数据使用三种去识别化工具匿名化：基于规则的 Philter 和两个基于深度学习的模型，BiLSTM-CRF 和 ClinicalBERT。尽管 ClinicalBERT 是最有效的，可以掩盖所有已识别的 PII，但我们的工具仍然重新识别了 9% 的临床记录。我们的研究强调了当前去识别技术的重大弱点，同时提供了一种迭代开发和改进的工具。</li>
</ul>

<h3>Title: Arabic Dataset for LLM Safeguard Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yasser Ashraf, Yuxia Wang, Bin Gu, Preslav Nakov, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17040">https://arxiv.org/abs/2410.17040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17040">https://arxiv.org/pdf/2410.17040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17040]] Arabic Dataset for LLM Safeguard Evaluation(https://arxiv.org/abs/2410.17040)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The growing use of large language models (LLMs) has raised concerns regarding their safety. While many studies have focused on English, the safety of LLMs in Arabic, with its linguistic and cultural complexities, remains under-explored. Here, we aim to bridge this gap. In particular, we present an Arab-region-specific safety evaluation dataset consisting of 5,799 questions, including direct attacks, indirect attacks, and harmless requests with sensitive words, adapted to reflect the socio-cultural context of the Arab world. To uncover the impact of different stances in handling sensitive and controversial topics, we propose a dual-perspective evaluation framework. It assesses the LLM responses from both governmental and opposition viewpoints. Experiments over five leading Arabic-centric and multilingual LLMs reveal substantial disparities in their safety performance. This reinforces the need for culturally specific datasets to ensure the responsible deployment of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的使用日益广泛，引发了人们对其安全性的担忧。虽然许多研究都集中在英语上，但阿拉伯语 LLM 的安全性仍未得到充分探索，因为阿拉伯语具有语言和文化复杂性。在这里，我们旨在弥合这一差距。具体来说，我们提供了一个针对阿拉伯地区的安全评估数据集，其中包含 5,799 个问题，包括直接攻击、间接攻击和带有敏感词的无害请求，这些问题经过调整以反映阿拉伯世界的社会文化背景。为了揭示不同立场在处理敏感和有争议的话题方面的影响，我们提出了一个双视角评估框架。它从政府和反对派的角度评估 LLM 的回应。对五个领先的以阿拉伯语为中心和多语言的 LLM 进行的实验表明，它们的安全性能存在很大差异。这进一步表明需要特定于文化的数据集来确保负责任地部署 LLM。</li>
</ul>

<h3>Title: Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Haining Wang, Jason Clark, Hannah McKelvey, Leila Sterman, Zheng Gao, Zuoyu Tian, Sandra Kübler, Xiaozhong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17088">https://arxiv.org/abs/2410.17088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17088">https://arxiv.org/pdf/2410.17088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17088]] Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning(https://arxiv.org/abs/2410.17088)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A vast amount of scholarly work is published daily, yet much of it remains inaccessible to the general public due to dense jargon and complex language. To address this challenge in science communication, we introduce a reinforcement learning framework that fine-tunes a language model to rewrite scholarly abstracts into more comprehensible versions. Guided by a carefully balanced combination of word- and sentence-level accessibility rewards, our language model effectively substitutes technical terms with more accessible alternatives, a task which models supervised fine-tuned or guided by conventional readability measures struggle to accomplish. Our best model adjusts the readability level of scholarly abstracts by approximately six U.S. grade levels -- in other words, from a postgraduate to a high school level. This translates to roughly a 90% relative boost over the supervised fine-tuning baseline, all while maintaining factual accuracy and high-quality language. An in-depth analysis of our approach shows that balanced rewards lead to systematic modifications in the base model, likely contributing to smoother optimization and superior performance. We envision this work as a step toward bridging the gap between scholarly research and the general public, particularly younger readers and those without a college degree.</li>
<li><strong>摘要：</strong>每天都有大量学术著作出版，但由于术语密集且语言复杂，其中大部分仍无法为公众所理解。为了解决科学传播中的这一挑战，我们引入了一个强化学习框架，该框架可以微调语言模型，将学术摘要重写为更易理解的版本。在精心平衡的单词和句子级可访问性奖励组合的指导下，我们的语言模型有效地用更易于理解的替代词替代了技术术语，这是监督微调或由传统可读性指标指导的模型难以完成的任务。我们最好的模型将学术摘要的可读性水平调整了大约六个美国年级水平——换句话说，从研究生水平调整到高中水平。这意味着在监督微调基线上相对提升了大约 90%，同时保持了事实准确性和高质量的语言。对我们的方法的深入分析表明，平衡的奖励会导致基础模型的系统修改，可能有助于更顺畅的优化和卓越的性能。我们希望这项工作能够缩小学术研究与普通公众，特别是年轻读者和没有大学学位的读者之间的差距。</li>
</ul>

<h3>Title: Team Ryu's Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Zilong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17094">https://arxiv.org/abs/2410.17094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17094">https://arxiv.org/pdf/2410.17094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17094]] Team Ryu's Submission to SIGMORPHON 2024 Shared Task on Subword Tokenization(https://arxiv.org/abs/2410.17094)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This papers presents the submission of team Ryu to the canceled SIGMORPHON 2024 shared task on subword tokenization. My submission explores whether morphological segmentation methods can be used as a part of subword tokenizers. I adopt two approaches: the statistical segmentation method Morfessor and a transformer based sequence-to-sequence (seq2seq) segmentation model in tokenizers. The prediction results show that morphological segmentation could be as effective as commonly used subword tokenizers. Additionally, I investigate how a tokenizer's vocabulary influences the performance of language models. A tokenizer with a balanced token frequency distribution tends to work better. A balanced token vocabulary can be achieved by keeping frequent words as unique tokens.</li>
<li><strong>摘要：</strong>本论文介绍了 Ryu 团队对已取消的 SIGMORPHON 2024 子词标记化共享任务的提交。我的提交探讨了形态分割方法是否可以用作子词标记器的一部分。我采用了两种方法：统计分割方法 Morfessor 和基于转换器的标记器中的序列到序列 (seq2seq) 分割模型。预测结果表明，形态分割可能与常用的子词标记器一样有效。此外，我研究了标记器的词汇如何影响语言模型的性能。具有平衡标记频率分布的标记器往往效果更好。可以通过将常用词保留为唯一标记来实现平衡的标记词汇表。</li>
</ul>

<h3>Title: Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations</h3>
<ul>
<li><strong>Authors: </strong>Jiyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17099">https://arxiv.org/abs/2410.17099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17099">https://arxiv.org/pdf/2410.17099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17099]] Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations(https://arxiv.org/abs/2410.17099)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The quality is a crucial issue for crowd annotations. Answer aggregation is an important type of solution. The aggregated answers estimated from multiple crowd answers to the same instance are the eventually collected annotations, rather than the individual crowd answers themselves. Recently, the capability of Large Language Models (LLMs) on data annotation tasks has attracted interest from researchers. Most of the existing studies mainly focus on the average performance of individual crowd workers; several recent works studied the scenarios of aggregation on categorical labels and LLMs used as label creators. However, the scenario of aggregation on text answers and the role of LLMs as aggregators are not yet well-studied. In this paper, we investigate the capability of LLMs as aggregators in the scenario of close-ended crowd text answer aggregation. We propose a human-LLM hybrid text answer aggregation method with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We make the experiments based on public crowdsourcing datasets. The results show the effectiveness of our approach based on the collaboration of crowd workers and LLMs.</li>
<li><strong>摘要：</strong>质量是众包标注的关键问题。答案聚合是一种重要的解决方案。最终收集到的标注是由针对同一实例的多个众包答案估计得出的聚合答案，而不是单个众包答案本身。最近，大型语言模型 (LLM) 在数据标注任务上的能力引起了研究人员的兴趣。现有的大多数研究主要关注单个众包工作者的平均表现；最近的几篇论文研究了分类标签上的聚合场景和用作标签创建者的 LLM。然而，文本答案上的聚合场景和 LLM 作为聚合器的作用尚未得到很好的研究。在本文中，我们研究了 LLM 作为聚合器在封闭式众包文本答案聚合场景中的能力。我们提出了一种具有创建者-聚合器多阶段 (CAMS) 众包框架的人-LLM 混合文本答案聚合方法。我们基于公共众包数据集进行实验。结果表明，我们基于众包工作者和 LLM 协作的方法是有效的。</li>
</ul>

<h3>Title: Enhancing Answer Attribution for Faithful Text Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Juraj Vladika, Luca Mülln, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17112">https://arxiv.org/abs/2410.17112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17112">https://arxiv.org/pdf/2410.17112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17112]] Enhancing Answer Attribution for Faithful Text Generation with Large Language Models(https://arxiv.org/abs/2410.17112)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The increasing popularity of Large Language Models (LLMs) in recent years has changed the way users interact with and pose questions to AI-based conversational systems. An essential aspect for increasing the trustworthiness of generated LLM answers is the ability to trace the individual claims from responses back to relevant sources that support them, the process known as answer attribution. While recent work has started exploring the task of answer attribution in LLMs, some challenges still remain. In this work, we first perform a case study analyzing the effectiveness of existing answer attribution methods, with a focus on subtasks of answer segmentation and evidence retrieval. Based on the observed shortcomings, we propose new methods for producing more independent and contextualized claims for better retrieval and attribution. The new methods are evaluated and shown to improve the performance of answer attribution components. We end with a discussion and outline of future directions for the task.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 的日益普及改变了用户与基于 AI 的对话系统交互和提出问题的方式。提高生成的 LLM 答案可信度的一个重要方面是能够将响应中的单个声明追溯到支持它们的相关来源，这一过程称为答案归因。虽然最近的研究已经开始探索 LLM 中的答案归因任务，但仍然存在一些挑战。在这项工作中，我们首先进行案例研究，分析现有答案归因方法的有效性，重点关注答案细分和证据检索的子任务。根据观察到的缺点，我们提出了新方法来生成更独立和更情境化的声明，以便更好地检索和归因。对新方法进行了评估，并表明它们可以提高答案归因组件的性能。最后，我们讨论并概述了该任务的未来方向。</li>
</ul>

<h3>Title: Exploring RL-based LLM Training for Formal Language Tasks with Programmed Rewards</h3>
<ul>
<li><strong>Authors: </strong>Alexander G. Padula, Dennis J.N.J. Soemers</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17126">https://arxiv.org/abs/2410.17126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17126">https://arxiv.org/pdf/2410.17126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17126]] Exploring RL-based LLM Training for Formal Language Tasks with Programmed Rewards(https://arxiv.org/abs/2410.17126)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning from Human Feedback to align large language models (LLMs) with downstream tasks. This paper investigates the feasibility of using PPO for direct reinforcement learning (RL) from explicitly programmed reward signals, as opposed to indirect learning from human feedback via an intermediary reward model. We focus on tasks expressed through formal languages, such as mathematics and programming, where explicit reward functions can be programmed to automatically assess the quality of generated outputs. We apply this approach to a sentiment alignment task, a simple arithmetic task, and a more complex game synthesis task. The sentiment alignment task replicates prior research and serves to validate our experimental setup. Our results show that pure RL-based training for the two formal language tasks is challenging, with success being limited even for the simple arithmetic task. We propose a novel batch-entropy regularization term to aid exploration, although training is not yet entirely stable. Our findings suggest that direct RL training of LLMs may be more suitable for relatively minor changes, such as alignment, than for learning new tasks altogether, even if an informative reward signal can be expressed programmatically.</li>
<li><strong>摘要：</strong>近端策略优化 (PPO) 通常用于从人类反馈进行强化学习，以使大型语言模型 (LLM) 与下游任务对齐。本文研究了使用 PPO 从明确编程的奖励信号进行直接强化学习 (RL) 的可行性，而不是通过中间奖励模型从人类反馈进行间接学习。我们专注于通过形式语言表达的任务，例如数学和编程，其中可以对显式奖励函数进行编程以自动评估生成的输出的质量。我们将这种方法应用于情绪对齐任务、简单的算术任务和更复杂的游戏合成任务。情绪对齐任务复制了先前的研究并用于验证我们的实验设置。我们的结果表明，对这两个形式语言任务进行纯基于 RL 的训练具有挑战性，即使是简单的算术任务的成功率也很有限。我们提出了一种新颖的批量熵正则化项来帮助探索，尽管训练尚未完全稳定。我们的研究结果表明，即使可以通过编程表达信息奖励信号，直接对 LLM 进行 RL 训练可能更适合相对较小的变化（例如对齐），而不是学习全新的任务。</li>
</ul>

<h3>Title: Aligning Large Language Models via Self-Steering Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun, Jingren Zhou, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17131">https://arxiv.org/abs/2410.17131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17131">https://arxiv.org/pdf/2410.17131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17131]] Aligning Large Language Models via Self-Steering Optimization(https://arxiv.org/abs/2410.17131)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. $SSO$ maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. $SSO$ can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of $SSO$ with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, $SSO$ leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by $SSO$ significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.</li>
<li><strong>摘要：</strong>自动对齐以最少的人为干预开发对齐系统。自动对齐的关键在于提供可学习且准确的偏好信号，用于偏好学习而无需人工注释。在本文中，我们引入了自引导优化 ($SSO$)，这是一种在迭代训练期间基于预定义原则自主生成高质量偏好信号的算法，无需手动注释。$SSO$ 通过确保选择和拒绝的响应之间的一致差距来保持信号的准确性，同时保持它们都符合策略以适应当前策略模型的学习能力。$SSO$ 可以有益于策略模型的在线和离线训练，并增强奖励模型的训练。我们用两个基础模型 Qwen2 和 Llama3.1 验证了 $SSO$ 的有效性，表明它在整个迭代训练过程中提供了准确的、符合策略的偏好信号。在没有任何手动注释或外部模型的情况下，$SSO$ 在六个主观或客观基准上都显著提高了性能。此外，$SSO$ 生成的偏好数据显著提高了奖励模型在 Rewardbench 上的性能。我们的工作提出了一种可扩展的偏好优化方法，为更高效、更有效的自动对齐铺平了道路。</li>
</ul>

<h3>Title: Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?</h3>
<ul>
<li><strong>Authors: </strong>Jirat Chiaranaipanich, Naiyarat Hanmatheekuna, Jitkapat Sawatphol, Krittamate Tiankanon, Jiramet Kinchagawat, Amrest Chinkamol, Parinthapat Pengpun, Piyalitt Ittichaiwong, Peerat Limkonchotiwat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17145">https://arxiv.org/abs/2410.17145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17145">https://arxiv.org/pdf/2410.17145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17145]] Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?(https://arxiv.org/abs/2410.17145)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) perform well on common tasks but struggle with generalization in low-resource and low-computation settings. We examine this limitation by testing various LLMs and specialized translation models on English-Thai machine translation and code-switching datasets. Our findings reveal that under more strict computational constraints, such as 4-bit quantization, LLMs fail to translate effectively. In contrast, specialized models, with comparable or lower computational requirements, consistently outperform LLMs. This underscores the importance of specialized models for maintaining performance under resource constraints.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在常见任务上表现良好，但在资源和计算能力较低的情况下难以实现泛化。我们通过在英语-泰语机器翻译和代码转换数据集上测试各种 LLM 和专门的翻译模型来检验这一限制。我们的研究结果表明，在更严格的计算约束（例如 4 位量化）下，LLM 无法有效翻译。相比之下，具有相当或更低计算要求的专门模型始终优于 LLM。这强调了专门模型对于在资源受限的情况下保持性能的重要性。</li>
</ul>

<h3>Title: Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence</h3>
<ul>
<li><strong>Authors: </strong>İlker Işık, Ramazan Gokberk Cinbis, Ebru Aydin Gol</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17161">https://arxiv.org/abs/2410.17161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17161">https://arxiv.org/pdf/2410.17161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17161]] Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence(https://arxiv.org/abs/2410.17161)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We propose a novel approach for learning interchangeable tokens in language models to obtain an extendable vocabulary that can generalize to new tokens. Our method is designed to address alpha-equivalence, the principle that renaming bound variables in a syntactic expression preserves semantics. This property arises in many formal languages such as temporal logics, in which all proposition symbols represent the same concept but are distinguishable from each other. To handle such tokens, we develop a dual-part embedding approach. The first part is shared across all interchangeable tokens, thereby enforcing that they represent the same core concept. The second part is randomly generated for each token, which enables distinguishability. We evaluate our method in a Transformer encoder-decoder model on two tasks: solving linear temporal logic formulae and copying with extendable vocabulary. Our method demonstrates promising generalization capabilities in addition to introducing a favorable inductive bias for alpha-equivalence.</li>
<li><strong>摘要：</strong>我们提出了一种新方法来学习语言模型中的可互换标记，以获得可以推广到新标记的可扩展词汇表。我们的方法旨在解决 alpha-equivalence，即在句法表达式中重命名绑定变量可以保留语义的原则。此属性出现在许多形式语言中，例如时间逻辑，其中所有命题符号都代表相同的概念，但彼此可以区分。为了处理这样的标记，我们开发了一种双部分嵌入方法。第一部分在所有可互换标记之间共享，从而强制它们代表相同的核心概念。第二部分是为每个标记随机生成的，从而实现可区分性。我们在 Transformer 编码器-解码器模型中通过两个任务评估我们的方法：解决线性时间逻辑公式和使用可扩展词汇进行复制。除了为 alpha-equivalence 引入有利的归纳偏差外，我们的方法还展示了有希望的泛化能力。</li>
</ul>

<h3>Title: Self-calibration for Language Model Quantization and Pruning</h3>
<ul>
<li><strong>Authors: </strong>Miles Williams, George Chrysostomou, Nikolaos Aletras</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17170">https://arxiv.org/abs/2410.17170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17170">https://arxiv.org/pdf/2410.17170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17170]] Self-calibration for Language Model Quantization and Pruning(https://arxiv.org/abs/2410.17170)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Quantization and pruning are fundamental approaches for model compression, enabling efficient inference for language models. In a post-training setting, state-of-the-art quantization and pruning methods require calibration data, a small set of unlabeled examples. Conventionally, randomly sampled web text is used, aiming to reflect the model training data. However, this poses two key problems: (1) unrepresentative calibration examples can harm model performance, and (2) organizations increasingly avoid releasing model training data. In this paper, we propose self-calibration as a solution. Our approach requires no external data, instead leveraging the model itself to generate synthetic calibration data as a better approximation of the pre-training data distribution. We extensively compare the performance of self-calibration with several baselines, across a variety of models, compression methods, and tasks. Our approach proves consistently competitive in maximizing downstream task performance, frequently outperforming even using real data.</li>
<li><strong>摘要：</strong>量化和剪枝是模型压缩的基本方法，可实现语言模型的有效推理。在训练后环境中，最先进的量化和剪枝方法需要校准数据，即一小组未标记的示例。传统上，使用随机采样的网络文本，旨在反映模型训练数据。然而，这带来了两个关键问题：（1）不具代表性的校准示例会损害模型性能，（2）组织越来越避免发布模型训练数据。在本文中，我们提出了自我校准作为解决方案。我们的方法不需要外部数据，而是利用模型本身来生成合成校准数据，以更好地近似预训练数据分布。我们在各种模型、压缩方法和任务中广泛比较了自校准与几个基线的性能。事实证明，我们的方法在最大化下游任务性能方面具有持续的竞争力，甚至经常优于使用真实数据。</li>
</ul>

<h3>Title: From Attention to Activation: Unravelling the Enigmas of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Prannay Kaul, Chengcheng Ma, Ismail Elezi, Jiankang Deng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17174">https://arxiv.org/abs/2410.17174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17174">https://arxiv.org/pdf/2410.17174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17174]] From Attention to Activation: Unravelling the Enigmas of Large Language Models(https://arxiv.org/abs/2410.17174)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We study two strange phenomena in auto-regressive Transformers: (1) the dominance of the first token in attention heads; (2) the occurrence of large outlier activations in the hidden states. We find that popular large language models, such as Llama attend maximally to the first token in 98% of attention heads, a behaviour we attribute to the softmax function. To mitigate this issue, we propose a reformulation of softmax to softmax-1. Furthermore, we identify adaptive optimisers, e.g. Adam, as the primary contributor to the large outlier activations and introduce OrthoAdam, a novel optimiser that utilises orthogonal matrices to transform gradients, to address this issue. Finally, not only do our methods prevent these phenomena from occurring, but additionally, they enable Transformers to sustain their performance when quantised using basic algorithms, something that standard methods are unable to do. In summary, our methods reduce the attention proportion on the first token from 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to 3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3.</li>
<li><strong>摘要：</strong>我们研究了自回归 Transformer 中的两种奇怪现象：（1）注意力头中第一个标记的主导地位；（2）隐藏状态中出现较大的异常激活值。我们发现，流行的大型语言模型（如 Llama）在 98% 的注意力头中最大限度地关注第一个标记，我们将这种行为归因于 softmax 函数。为了缓解这个问题，我们建议将 softmax 重新表述为 softmax-1。此外，我们将自适应优化器（例如 Adam）确定为导致较大异常激活值的主要因素，并引入 OrthoAdam（一种利用正交矩阵来变换梯度的新型优化器）来解决此问题。最后，我们的方法不仅可以防止这些现象的发生，而且它们还可以使 Transformer 在使用基本算法量化时保持其性能，这是标准方法无法做到的。综上所述，我们的方法将第一个 token 上的注意力比例从 65% 降低到 3.3%，隐藏状态下的激活峰度从 1657 降低到 3.1，4 位权重量化下的困惑度惩罚从 3565 降低到 0.3。</li>
</ul>

<h3>Title: VoiceBench: Benchmarking LLM-Based Voice Assistants</h3>
<ul>
<li><strong>Authors: </strong>Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17196">https://arxiv.org/abs/2410.17196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17196">https://arxiv.org/pdf/2410.17196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17196]] VoiceBench: Benchmarking LLM-Based Voice Assistants(https://arxiv.org/abs/2410.17196)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 成功的基础上，GPT-4o 等最新进展已使基于 LLM 的语音助手能够进行实时语音交互，与传统的基于文本的交互相比，用户体验显著改善。然而，缺乏旨在评估这些语音交互能力的基准，阻碍了基于 LLM 的语音助手开发的进展。当前的评估主要侧重于自动语音识别 (ASR) 或对清晰语音的一般知识评估，而忽略了涉及各种说话者特征、环境和内容因素的更复杂的真实场景。为了解决这个问题，我们推出了 VoiceBench，这是第一个旨在对基于 LLM 的语音助手进行多方面评估的基准。VoiceBench 还包括真实和合成的语音指令，这些指令结合了上述三个关键的真实世界变化。大量实验揭示了当前基于 LLM 的语音助手模型的局限性，并为该领域未来的研究和开发提供了宝贵的见解。</li>
</ul>

<h3>Title: Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh through Large Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam, Mahathir Mohammad Bappy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17210">https://arxiv.org/abs/2410.17210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17210">https://arxiv.org/pdf/2410.17210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17210]] Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh through Large Language Modeling(https://arxiv.org/abs/2410.17210)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Purpose: Bangladesh's legal system struggles with major challenges like delays, complexity, high costs, and millions of unresolved cases, which deter many from pursuing legal action due to lack of knowledge or financial constraints. This research seeks to develop a specialized Large Language Model (LLM) to assist in the Bangladeshi legal system. Methods: We created UKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and scraping data on various legal acts. We fine-tuned the GPT-2 model on this dataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance in English. Results: The model was rigorously evaluated using semantic assessments, including case studies supported by expert opinions. The evaluation provided promising results, demonstrating the potential for the model to assist in legal matters within Bangladesh. Conclusion: Our work represents the first structured effort toward building an AI-based legal assistant for Bangladesh. While the results are encouraging, further refinements are necessary to improve the model's accuracy, credibility, and safety. This is a significant step toward creating a legal AI capable of serving the needs of a population of 180 million.</li>
<li><strong>摘要：</strong>目的：孟加拉国的法律体系面临着重大挑战，例如拖延、复杂、成本高以及数百万未解决的案件，由于缺乏知识或资金限制，许多人因此无法采取法律行动。本研究旨在开发一个专门的大型语言模型 (LLM) 来协助孟加拉国的法律体系。方法：我们通过收集和抓取各种法律行为的数据，创建了 UKIL-DB-EN，这是一个孟加拉国法律文件的英语语料库。我们在这个数据集上对 GPT-2 模型进行了微调，以开发 GPT2-UKIL-EN，这是一个专注于用英语提供法律援助的 LLM。结果：使用语义评估对该模型进行了严格评估，包括专家意见支持的案例研究。评估结果令人鼓舞，表明该模型有潜力协助孟加拉国的法律事务。结论：我们的工作是为孟加拉国建立基于 AI 的法律助理的首次结构化努力。虽然结果令人鼓舞，但仍需要进一步改进以提高模型的准确性、可信度和安全性。这是朝着创建能够满足 1.8 亿人口需求的法律人工智能迈出的重要一步。</li>
</ul>

<h3>Title: MiniPLM: Knowledge Distillation for Pre-Training Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17215">https://arxiv.org/abs/2410.17215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17215">https://arxiv.org/pdf/2410.17215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17215]] MiniPLM: Knowledge Distillation for Pre-Training Language Models(https://arxiv.org/abs/2410.17215)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at this https URL.</li>
<li><strong>摘要：</strong>知识蒸馏 (KD) 被广泛用于使用大型教师语言模型 (LM) 来训练小型、高性能的学生语言模型 (LM)。虽然在微调方面很有效，但在预训练期间进行知识蒸馏 (KD) 时面临着效率、灵活性和有效性方面的挑战。现有方法要么由于在线教师推理而产生高计算成本，要么需要在教师和学生语言模型之间进行标记匹配，要么冒着失去教师生成的训练数据的难度和多样性的风险。为了解决这些问题，我们提出了 MiniPLM，这是一个 KD 框架，用于通过使用教师的知识来细化训练数据分布来预训练语言模型。为了提高效率，MiniPLM 执行离线教师语言模型推理，允许对多个学生语言模型进行知识蒸馏，而不会增加训练时间成本。为了提高灵活性，MiniPLM 仅在训练语料库上运行，从而支持跨模型系列进行知识蒸馏。为了提高有效性，MiniPLM 利用大型和小型语言模型之间的差异来增强训练数据的难度和多样性，帮助学生语言模型获得多样化和复杂的知识。大量实验表明，MiniPLM 提升了学生 LM 在 9 个广泛使用的下游任务上的表现，提高了语言建模能力，并减少了预训练计算量。MiniPLM 的优势扩展到了大规模预训练，这可以通过扩展曲线的外推得到证明。进一步的分析表明，MiniPLM 支持跨模型系列的 KD，并提高了预训练数据的利用率。我们的模型、代码和数据可在此 https URL 上找到。</li>
</ul>

<h3>Title: Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods</h3>
<ul>
<li><strong>Authors: </strong>Tsachi Blau, Moshe Kimhi, Yonatan Belinkov, Alexander Bronstein, Chaim Baskin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17222">https://arxiv.org/abs/2410.17222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17222">https://arxiv.org/pdf/2410.17222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17222]] Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods(https://arxiv.org/abs/2410.17222)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) typically involves updating at least a few billions of parameters. A more parameter-efficient approach is Prompt Tuning (PT), which updates only a few learnable tokens, and differently, In-Context Learning (ICL) adapts the model to a new task by simply including examples in the input without any training. When applying optimization-based methods, such as fine-tuning and PT for few-shot learning, the model is specifically adapted to the small set of training examples, whereas ICL leaves the model unchanged. This distinction makes traditional learning methods more prone to overfitting; in contrast, ICL is less sensitive to the few-shot scenario. While ICL is not prone to overfitting, it does not fully extract the information that exists in the training examples. This work introduces Context-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and adversarial attacks. We build on the ICL strategy of concatenating examples before the input, but we extend this by PT-like learning, refining the context embedding through iterative optimization to extract deeper insights from the training examples. We carefully modify specific context tokens, considering the unique structure of input and output formats. Inspired by adversarial attacks, we adjust the input based on the labels present in the context, focusing on minimizing, rather than maximizing, the loss. Moreover, we apply a projected gradient descent algorithm to keep token embeddings close to their original values, under the assumption that the user-provided data is inherently valuable. Our method has been shown to achieve superior accuracy across multiple classification tasks using various LLM models.</li>
<li><strong>摘要：</strong>微调大型语言模型 (LLM) 通常涉及更新至少数十亿个参数。一种参数效率更高的方法是即时调整 (PT)，它只更新几个可学习的标记，而不同的是，上下文学习 (ICL) 只需在输入中包含示例而无需任何训练即可使模型适应新任务。当应用基于优化的方法（例如微调和 PT 用于小样本学习）时，模型会专门适应少量训练示例，而 ICL 则保持模型不变。这种区别使传统学习方法更容易过度拟合；相比之下，ICL 对小样本场景不太敏感。虽然 ICL 不容易过度拟合，但它不能完全提取训练示例中存在的信息。这项工作引入了上下文感知即时调整 (CPT)，这是一种受 ICL、PT 和对抗性攻击启发的方法。我们以 ICL 策略为基础，在输入之前连接示例，但我们通过类似 PT 的学习对其进行了扩展，通过迭代优化细化上下文嵌入，以从训练示例中提取更深入的见解。我们仔细修改特定的上下文标记，考虑到输入和输出格式的独特结构。受到对抗性攻击的启发，我们根据上下文中存在的标签调整输入，重点是最小化而不是最大化损失。此外，我们应用投影梯度下降算法使标记嵌入接近其原始值，假设用户提供的数据本身具有价值。我们的方法已被证明可以在使用各种 LLM 模型的多个分类任务中实现卓越的准确性。</li>
</ul>

<h3>Title: Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy</h3>
<ul>
<li><strong>Authors: </strong>Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, Kunal Handa, Yarin Gal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17234">https://arxiv.org/abs/2410.17234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17234">https://arxiv.org/pdf/2410.17234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17234]] Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy(https://arxiv.org/abs/2410.17234)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation strategies. While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses. To address these limitations, we propose fine-tuning using semantic entropy, an uncertainty measure derived from introspection into the model which does not require external labels. We demonstrate that our approach matches or outperforms models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 会产生幻觉，即生成看似合理但不准确的文本。这种现象对医学或法律等关键应用构成重大风险，因此需要制定强有力的幻觉缓解策略。虽然最近的研究提出了微调方法来教导 LLM 避免回答超出其知识或能力的问题，但这些方法依赖于真实标签的存在或仅限于简短形式的响应。为了解决这些限制，我们建议使用语义熵进行微调，语义熵是一种从模型自省中得出的不确定性度量，不需要外部标签。我们证明我们的方法与使用先前工作进行微调的模型相当或优于它们，并且在一系列数据集上实现了短格式和长格式生成的强大性能。</li>
</ul>

<h3>Title: Large Language Models Empowered Personalized Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17236">https://arxiv.org/abs/2410.17236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17236">https://arxiv.org/pdf/2410.17236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17236]] Large Language Models Empowered Personalized Web Agents(https://arxiv.org/abs/2410.17236)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Web agents have emerged as a promising direction to automate Web task completion based on user instructions, significantly enhancing user experience. Recently, Web agents have evolved from traditional agents to Large Language Models (LLMs)-based Web agents. Despite their success, existing LLM-based Web agents overlook the importance of personalized data (e.g., user profiles and historical Web behaviors) in assisting the understanding of users' personalized instructions and executing customized actions. To overcome the limitation, we first formulate the task of LLM-empowered personalized Web agents, which integrate personalized data and user instructions to personalize instruction comprehension and action execution. To address the absence of a comprehensive evaluation benchmark, we construct a Personalized Web Agent Benchmark (PersonalWAB), featuring user instructions, personalized user data, Web functions, and two evaluation paradigms across three personalized Web tasks. Moreover, we propose a Personalized User Memory-enhanced Alignment (PUMA) framework to adapt LLMs to the personalized Web agent task. PUMA utilizes a memory bank with a task-specific retrieval strategy to filter relevant historical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for personalized action execution through fine-tuning and direct preference optimization. Extensive experiments validate the superiority of PUMA over existing Web agents on PersonalWAB.</li>
<li><strong>摘要：</strong>Web 代理已成为一个有前途的方向，可根据用户指令自动完成 Web 任务，从而显著提升用户体验。最近，Web 代理已从传统代理发展为基于大型语言模型 (LLM) 的 Web 代理。尽管现有的基于 LLM 的 Web 代理取得了成功，但它们忽视了个性化数据（例如，用户配置文件和历史 Web 行为）在帮助理解用户的个性化指令和执行定制操作方面的重要性。为了克服这一限制，我们首先制定了 LLM 赋能的个性化 Web 代理的任务，该代理集成了个性化数据和用户指令，以个性化指令理解和操作执行。为了解决缺乏全面评估基准的问题，我们构建了一个个性化 Web 代理基准 (PersonalWAB)，其中包含用户指令、个性化用户数据、Web 功能和三个个性化 Web 任务中的两个评估范例。此外，我们提出了一个个性化用户记忆增强对齐 (PUMA) 框架，以使 LLM 适应个性化 Web 代理任务。PUMA 利用具有任务特定检索策略的记忆库来过滤相关的历史 Web 行为。然后，PUMA 根据行为通过微调和直接偏好优化来调整 LLM 以执行个性化操作。大量实验验证了 PUMA 优于 PersonalWAB 上现有的 Web 代理。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
