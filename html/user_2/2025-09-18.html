<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-18</h1>
<h3>Title: Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs</h3>
<ul>
<li><strong>Authors: </strong>Andrea Piergentili, Beatrice Savoldi, Matteo Negri, Luisa Bentivogli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13480">https://arxiv.org/abs/2509.13480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13480">https://arxiv.org/pdf/2509.13480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13480]] Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs(https://arxiv.org/abs/2509.13480)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Gender-neutral rewriting (GNR) aims to reformulate text to eliminate unnecessary gender specifications while preserving meaning, a particularly challenging task in grammatical-gender languages like Italian. In this work, we conduct the first systematic evaluation of state-of-the-art large language models (LLMs) for Italian GNR, introducing a two-dimensional framework that measures both neutrality and semantic fidelity to the input. We compare few-shot prompting across multiple LLMs, fine-tune selected models, and apply targeted cleaning to boost task relevance. Our findings show that open-weight LLMs outperform the only existing model dedicated to GNR in Italian, whereas our fine-tuned models match or exceed the best open-weight LLM's performance at a fraction of its size. Finally, we discuss the trade-off between optimizing the training data for neutrality and meaning preservation.</li>
<li><strong>摘要：</strong>性别中立的重写（GNR）旨在重新制定文本，以消除不必要的性别规格，同时保持意义，这是意大利语等语法性别语言中特别具有挑战性的任务。在这项工作中，我们对意大利GNR进行了最先进的大语言模型（LLM）的首次系统评估，并引入了一个二维框架，该框架对输入进行了衡量中立性和语义保真度。我们比较了多个llms，微调选定模型的几个弹药提示，并应用有针对性的清洁以提高任务相关性。我们的发现表明，开放式LLM的表现优于意大利语中GNR的唯一现有型号，而我们的微型型号匹配或超过最佳的开放式LLM的性能，其大小的一小部分。最后，我们讨论了优化培训数据的中立性和含义保存之间的权衡。</li>
</ul>

<h3>Title: Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Alisa Kanganis, Katherine A. Keith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13539">https://arxiv.org/abs/2509.13539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13539">https://arxiv.org/pdf/2509.13539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13539]] Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning(https://arxiv.org/abs/2509.13539)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets monetary policy, affecting the borrowing and spending decisions of millions of people. In this work, we release Op-Fed, a dataset of 1044 human-annotated sentences and their contexts from FOMC transcripts. We faced two major technical challenges in dataset creation: imbalanced classes -- we estimate fewer than 8% of sentences express a non-neutral stance towards monetary policy -- and inter-sentence dependence -- 65% of instances require context beyond the sentence-level. To address these challenges, we developed a five-stage hierarchical schema to isolate aspects of opinion, monetary policy, and stance towards monetary policy as well as the level of context needed. Second, we selected instances to annotate using active learning, roughly doubling the number of positive instances across all schema aspects. Using Op-Fed, we found a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion classification but only 0.61 zero-shot accuracy classifying stance towards monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be useful for future model training, confidence calibration, and as a seed dataset for future annotation efforts.</li>
<li><strong>摘要：</strong>美国联邦公开市场委员会（FOMC）定期讨论并制定货币政策，影响数百万人的借贷和支出决定。在这项工作中，我们发布了Op-fed，这是一个由FOMC成绩单中的1044个人类注销句子及其上下文的数据集。我们在数据集创建中面临着两个主要的技术挑战：不平衡的类 - 我们估计，句子中不到8％的句子表达了对货币政策的非中性立场，而句子间的依赖性 -  65％的实例需要句子级别以外的上下文。为了应对这些挑战，我们开发了一个五阶段的层次模式，以隔离意见，货币政策和对货币政策以及所需的背景水平的方面。其次，我们选择了使用主动学习的实例来注释，将所有模式方面的积极实例数量大致增加了一倍。我们发现，在意见分类中，我们发现了最佳表现，闭合的LLM在0.80零射击准确性方面，但只有0.61零射击的精度对货币政策进行分类，低于我们的基线为0.89。我们预计，运输方式将对未来的模型培训，信心校准以及作为未来注释工作的种子数据集有用。</li>
</ul>

<h3>Title: Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12</h3>
<ul>
<li><strong>Authors: </strong>John Mendonça, Lining Zhang, Rahul Mallidi, Alon Lavie, Isabel Trancoso, Luis Fernando D'Haro, João Sedoc</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13569">https://arxiv.org/abs/2509.13569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13569">https://arxiv.org/pdf/2509.13569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13569]] Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12(https://arxiv.org/abs/2509.13569)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has intensified the need for robust dialogue system evaluation, yet comprehensive assessment remains challenging. Traditional metrics often prove insufficient, and safety considerations are frequently narrowly defined or culturally biased. The DSTC12 Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and Safety," is part of the ongoing effort to address these critical gaps. The track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection. For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved the highest average Spearman's correlation (0.1681), indicating substantial room for improvement. In Task 2, while participating teams significantly outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126 ROC-AUC), highlighting critical needs in culturally-aware safety. This paper describes the datasets and baselines provided to participants, as well as submission evaluation results for each of the two proposed subtasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展增强了对对话系统评估的需求，但全面的评估仍然具有挑战性。传统指标通常被证明不足，并且安全考虑通常是狭义的定义或文化偏见。 DSTC12轨道1，“对话系统评估：维度，语言，文化和安全性”，是解决这些关键差距的持续努力的一部分。该轨道包括两个子任务：（1）对话级别，多维自动评估指标，以及（2）多语言和多元文化安全检测。对于任务1，专注于10个对话维度，Llama-3-8B基线达到了平均Spearman的相关性最高的（0.1681），这表明改进的空间很大。在任务2中，虽然参与的团队在多语言安全子集（顶级ROC-AUC 0.9648）上的表现显着优于Llama-Guard-3-1b基线，但基线在文化子集（0.5126 ROC-AUC）上表现出色，在文化中的关键需求凸显了文化的关键需求。本文介绍了提供给参与者的数据集和基准，以及两个提议的子任务中的每个子任务结果。</li>
</ul>

<h3>Title: Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Shambhavi Krishna, Atharva Naik, Chaitali Agarwal, Sudharshan Govindan, Taesung Lee, Haw-Shiuan Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13624">https://arxiv.org/abs/2509.13624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13624">https://arxiv.org/pdf/2509.13624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13624]] Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning(https://arxiv.org/abs/2509.13624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly deployed across diverse applications. This often includes tasks LLMs have not encountered during training. This implies that enumerating and obtaining the high-quality training data for all tasks is infeasible. Thus, we often need to rely on transfer learning using datasets with different characteristics, and anticipate out-of-distribution requests. Motivated by this practical need, we propose an analysis framework, building a transfer learning matrix and dimensionality reduction, to dissect these cross-task interactions. We train and analyze 10 models to identify latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic) and discover the side effects of the transfer learning. Our findings reveal that performance improvements often defy explanations based on surface-level dataset similarity or source data quality. Instead, hidden statistical factors of the source dataset, such as class distribution and generation length proclivities, alongside specific linguistic features, are actually more influential. This work offers insights into the complex dynamics of transfer learning, paving the way for more predictable and effective LLM adaptation.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地在不同的应用程序中部署。这通常包括LLM在培训期间没有遇到的任务。这意味着为所有任务列举和获得高质量的培训数据是不可行的。因此，我们通常需要使用具有不同特征的数据集依靠转移学习，并预测分发请求。在这种实际需求的推动下，我们提出了一个分析框架，建立转移学习矩阵和降低维度，以剖析这些交叉任务相互作用。我们训练和分析10个模型，以识别潜在能力（例如推理，情感分类，NLU，算术），并发现转移学习的副作用。我们的发现表明，基于表面级数据集的相似性或源数据质量的绩效改进通常不违反解释。取而代之的是，源数据集的隐藏统计因素，例如类别分布和发电长度倾向以及特定的语言特征，实际上更具影响力。这项工作为转移学习的复杂动态提供了见解，为更可预测和有效的LLM适应铺平了道路。</li>
</ul>

<h3>Title: Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhuoxuan Zhang, Jinhao Duan, Edward Kim, Kaidi Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13664">https://arxiv.org/abs/2509.13664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13664">https://arxiv.org/pdf/2509.13664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13664]] Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs(https://arxiv.org/abs/2509.13664)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Ambiguity is pervasive in real-world questions, yet large language models (LLMs) often respond with confident answers rather than seeking clarification. In this work, we show that question ambiguity is linearly encoded in the internal representations of LLMs and can be both detected and controlled at the neuron level. During the model's pre-filling stage, we identify that a small number of neurons, as few as one, encode question ambiguity information. Probes trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance on ambiguity detection and generalize across datasets, outperforming prompting-based and representation-based baselines. Layerwise analysis reveals that AENs emerge from shallow layers, suggesting early encoding of ambiguity signals in the model's processing pipeline. Finally, we show that through manipulating AENs, we can control LLM's behavior from direct answering to abstention. Our findings reveal that LLMs form compact internal representations of question ambiguity, enabling interpretable and controllable behavior.</li>
<li><strong>摘要：</strong>在现实世界中，模棱两可在现实问题中普遍存在，但是大型语言模型（LLMS）通常以自信的答案而不是寻求澄清。在这项工作中，我们表明歧义性的问题是在LLM的内部表示中线性编码的，并且可以在神经元级别检测和控制。在模型的预填充阶段，我们确定少数神经元（最少）编码问题歧义信息。对这些模棱两可的编码神经元（AEN）训练的探针在歧义检测上实现了强大的性能，并在跨数据集中概括，表现优于基于提示和基于表示的基线。 Layswise分析表明，AEN从浅层层中出现，表明该模型处理管道中歧义信号的早期编码。最后，我们表明，通过操纵AEN，我们可以控制LLM的行为从直接回答到弃权。我们的发现表明，LLM会形成问题歧义的紧凑内部表示形式，从而实现了可解释和可控制的行为。</li>
</ul>

<h3>Title: CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Shang Qin, Jingheng Ye, Yinghui Li, Hai-Tao Zheng, Qi Li, Jinxiao Shan, Zhixing Li, Hong-Gee Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13672">https://arxiv.org/abs/2509.13672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13672">https://arxiv.org/pdf/2509.13672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13672]] CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction(https://arxiv.org/abs/2509.13672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The growing demand for automated writing assistance in diverse academic domains highlights the need for robust Chinese Grammatical Error Correction (CGEC) systems that can adapt across disciplines. However, existing CGEC research largely lacks dedicated benchmarks for multi-disciplinary academic writing, overlooking continual learning (CL) as a promising solution to handle domain-specific linguistic variation and prevent catastrophic forgetting. To fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning benchmark for Chinese Literature Grammatical Error Correction, designed to evaluate adaptive CGEC across multiple academic fields. Our benchmark includes 10,000 human-annotated sentences spanning 10 disciplines, each exhibiting distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating grammatical error correction in a continual learning setting, simulating sequential exposure to diverse academic disciplines to reflect real-world editorial dynamics. We evaluate large language models under sequential tuning, parameter-efficient adaptation, and four representative CL algorithms, using both standard GEC metrics and continual learning metrics adapted to task-level variation. Experimental results reveal that regularization-based methods mitigate forgetting more effectively than replay-based or naive sequential approaches. Our benchmark provides a rigorous foundation for future research in adaptive grammatical error correction across diverse academic domains.</li>
<li><strong>摘要：</strong>对不同学术领域的自动写作援助的需求不断增长，这凸显了可以适应跨学科的强大语法校正（CGEC）系统的需求。但是，现有的CGEC研究在很大程度上缺乏用于多学科学术写作的专门基准，忽略了持续学习（CL），作为处理特定领域的语言变化并防止灾难性遗忘的有希望的解决方案。为了填补这一关键差距，我们介绍了Cl $^2 $ GEC，这是中国文学语法错误校正的第一个持续学习基准，旨在评估跨多个学术领域的自适应CG​​EC。我们的基准包括10,000个跨越10个学科的人类注销句子，每个句子都表现出不同的语言风格和错误模式。 Cl $^2 $ GEC专注于评估持续学习环境中的语法错误校正，从而模拟对不同学科的顺序暴露，以反映现实世界中的社论动力学。我们使用标准的GEC指标和适合任务级别变化的持续学习指标，评估顺序调整，参数有效适应和四种代表性CL算法的大型语言模型。实验结果表明，基于正则化的方法比基于重播或天真的顺序方法更有效地减轻忘记。我们的基准为未来的研究跨不同学术领域的自适应语法纠正校正提供了严格的基础。</li>
</ul>

<h3>Title: AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinxu Zhou, Jiaqi Bai, Zhenqi Sun, Fanxiang Zeng, Yue Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13677">https://arxiv.org/abs/2509.13677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13677">https://arxiv.org/pdf/2509.13677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13677]] AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation(https://arxiv.org/abs/2509.13677)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, agent</a></li>
<li><strong>Abstract: </strong>Although significant progress has been made in many tasks within the field of Natural Language Processing (NLP), Controlled Text Generation (CTG) continues to face numerous challenges, particularly in achieving fine-grained conditional control over generation. Additionally, in real scenario and online applications, cost considerations, scalability, domain knowledge learning and more precise control are required, presenting more challenge for CTG. This paper introduces a novel and scalable framework, AgentCTG, which aims to enhance precise and complex control over the text generation by simulating the control and regulation mechanisms in multi-agent workflows. We explore various collaboration methods among different agents and introduce an auto-prompt module to further enhance the generation effectiveness. AgentCTG achieves state-of-the-art results on multiple public datasets. To validate its effectiveness in practical applications, we propose a new challenging Character-Driven Rewriting task, which aims to convert the original text into new text that conform to specific character profiles and simultaneously preserve the domain knowledge. When applied to online navigation with role-playing, our approach significantly enhances the driving experience through improved content delivery. By optimizing the generation of contextually relevant text, we enable a more immersive interaction within online communities, fostering greater personalization and user engagement.</li>
<li><strong>摘要：</strong>尽管在自然语言处理（NLP）领域的许多任务中已经取得了重大进展，但受控文本生成（CTG）仍在面临许多挑战，尤其是在实现对发电的细粒度有条件控制方面。此外，在实际情况和在线应用程序中，需要考虑成本注意事项，可伸缩性，域知识学习和更精确的控制，对CTG提出了更多挑战。本文介绍了一个新颖且可扩展的框架AgentCTG，该框架旨在通过模拟多代理工作流中的控制和调节机制来增强对文本生成的精确和复杂的控制。我们探讨了不同代理之间的各种协作方法，并引入了一个自动推出模块，以进一步提高生成效率。 AgentCTG在多个公共数据集上实现了最新的结果。为了验证其在实际应用中的有效性，我们提出了一项新的具有挑战性的角色驱动的重写任务，该任务旨在将原始文本转换为符合特定角色配置文件并同时保留域知识的新文本。当使用角色扮演的在线导航应用于在线导航时，我们的方法可通过改进的内容交付来大大增强驾驶体验。通过优化上下文相关文本的生成，我们可以在在线社区内进行更身临其境的互动，从而促进更大的个性化和用户参与度。</li>
</ul>

<h3>Title: Improving Context Fidelity via Native Retrieval-Augmented Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Suyuchen Wang, Jinlin Wang, Xinyu Wang, Shiqi Li, Xiangru Tang, Sirui Hong, Xiao-Wen Chang, Chenglin Wu, Bang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13683">https://arxiv.org/abs/2509.13683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13683">https://arxiv.org/pdf/2509.13683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13683]] Improving Context Fidelity via Native Retrieval-Augmented Reasoning(https://arxiv.org/abs/2509.13683)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model's own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通常会在上下文忠诚度上挣扎，在根据提供的信息回答问题时产生不一致的答案。现有的方法要么依靠昂贵的监督微调来生成证据，要么在训练后进行培训模型来执行网络搜索，而不必改善给定上下文的利用。我们提出了Care，这是一种新颖的本地检索效果框架，它教LLMS将其推理过程中的秘密证据与模型自身的检索能力明确整合。我们的方法需要有限的标记证据数据，同时通过在推理链中策略性地检索策略性检索的策略性检索，从而显着提高了检索准确性和回答生成性能。对多个现实世界和反事实质量检查基准的广泛实验表明，我们的方法基本上优于监督微调，传统检索型生成方法和外部检索解决方案。这项工作代表了使LLMS更准确，可靠且有效地完成知识密集的任务的基本进步。</li>
</ul>

<h3>Title: Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?</h3>
<ul>
<li><strong>Authors: </strong>Yosuke Mikami, Daiki Matsuoka, Hitomi Yanaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13695">https://arxiv.org/abs/2509.13695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13695">https://arxiv.org/pdf/2509.13695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13695]] Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?(https://arxiv.org/abs/2509.13695)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) perform remarkably well in Natural Language Inference (NLI). However, NLI involving numerical and logical expressions remains challenging. Comparatives are a key linguistic phenomenon related to such inference, but the robustness of LLMs in handling them, especially in languages that are not dominant in the models' training data, such as Japanese, has not been sufficiently explored. To address this gap, we construct a Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in zero-shot and few-shot settings. Our results show that the performance of the models is sensitive to the prompt formats in the zero-shot setting and influenced by the gold labels in the few-shot examples. The LLMs also struggle to handle linguistic phenomena unique to Japanese. Furthermore, we observe that prompts containing logical semantic representations help the models predict the correct labels for inference problems that they struggle to solve even with few-shot examples.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在自然语言推理（NLI）方面表现出色。但是，涉及数值和逻辑表达的NLI仍然具有挑战性。比较是与这种推理有关的关键语言现象，但是LLM在处理它们时的鲁棒性，尤其是在模型的培训数据（例如日语）中并非主导的语言中，尚未得到充分的探索。为了解决这一差距，我们构建了一个日本NLI数据集，该数据集专注于比较并评估零射击和少量设置的各种LLM。我们的结果表明，模型的性能对零拍设置中的及时格式敏感，并在几个示例中受金标签的影响。 LLM还难以处理日语独有的语言现象。此外，我们观察到，包含逻辑语义表示的提示有助于模型预测正确的标签，即使在很少的示例中，他们也难以解决的推理问题。</li>
</ul>

<h3>Title: Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes</h3>
<ul>
<li><strong>Authors: </strong>Iyadh Ben Cheikh Larbi, Ajay Madhavan Ravichandran, Aljoscha Burchardt, Roland Roller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13696">https://arxiv.org/abs/2509.13696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13696">https://arxiv.org/pdf/2509.13696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13696]] Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes(https://arxiv.org/abs/2509.13696)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at text generation, but their ability to handle clinical classification tasks involving structured data, such as time series, remains underexplored. In this work, we adapt instruction-tuned LLMs using DSPy-based prompt optimization to process clinical notes and structured EHR inputs jointly. Our results show that this approach achieves performance on par with specialized multimodal systems while requiring less complexity and offering greater adaptability across tasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在文本生成方面表现出色，但是它们处理涉及结构化数据的临床分类任务（例如时间序列）的能力仍然没有得到充实的态度。在这项工作中，我们使用基于DSPY的迅速优化对指导调整的LLM进行调整，以共同处理临床注释和结构化的EHR输入。我们的结果表明，这种方法可以与专门的多模式系统达到相同的性能，同时需要较小的复杂性并在任务之间提供更大的适应性。</li>
</ul>

<h3>Title: DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13702">https://arxiv.org/abs/2509.13702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13702">https://arxiv.org/pdf/2509.13702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13702]] DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models(https://arxiv.org/abs/2509.13702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) hallucination is a significant barrier to their reliable deployment. Current methods like Retrieval-Augmented Generation (RAG) are often reactive. We introduce **Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that intervenes during autoregressive decoding. Inspired by dual-process cognitive theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During inference, these proxies dynamically steer a large target model by injecting a real-time steering vector, which is the difference between FAP and HDP logits, at each decoding step. This plug-and-play approach requires no modification to the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2% Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained the highest FActScore of 46.50. These results validate DSCC-HS as a principled and efficient solution for enhancing LLM factuality.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）幻觉是其可靠部署的重要障碍。当前的方法（例如检索增强发电（RAG））通常是反应性的。我们介绍了**动态自我增强校准以进行幻觉抑制（DSCC-HS）**，这是一种新颖的，积极主动的框架，在自回旋解码过程中介入。受双过程认知理论的启发，DSCC-HS使用紧凑的代理模型，以对抗性角色作为事实对准代理（FAP）和幻觉检测代理（HDP）训练。在推断期间，这些代理通过注入实时转向向量（这是FAP和HDP逻辑之间的差异）通过在每个解码步骤中动态引导大型目标模型。这种插件方法不需要对目标模型进行任何修改。我们对真实性和生物基因的实验表明DSCC-HS达到了最新的性能。关于真实性，它达到了99.2％的事实一致性率（FCR）。在长形式的生物基基准上，它达到了46.50的最高事实分数。这些结果验证了DSCC-HS作为提高LLM事实的原则有效解决方案。</li>
</ul>

<h3>Title: Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models</h3>
<ul>
<li><strong>Authors: </strong>Peter Beidler, Mark Nguyen, Kevin Lybarger, Ola Holmberg, Eric Ford, John Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13706">https://arxiv.org/abs/2509.13706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13706">https://arxiv.org/pdf/2509.13706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13706]] Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models(https://arxiv.org/abs/2509.13706)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>PURPOSE: Incident reports are an important tool for safety and quality improvement in healthcare, but manual review is time-consuming and requires subject matter expertise. Here we present a natural language processing (NLP) screening tool to detect high-severity incident reports in radiation oncology across two institutions. METHODS AND MATERIALS: We used two text datasets to train and evaluate our NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA SAFRON (SF), all of which had severity scores labeled by clinical content experts. We trained and evaluated two types of models: baseline support vector machines (SVM) and BlueBERT which is a large language model pretrained on PubMed abstracts and hospitalized patient data. We assessed for generalizability of our model in two ways. First, we evaluated models trained using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that was first fine-tuned on Inst.-train then on SF-train before testing on SF-test set. To further analyze model performance, we also examined a subset of 59 reports from our Inst. dataset, which were manually edited for clarity. RESULTS Classification performance on the Inst. test achieved AUROC 0.82 using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning, performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56 using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets, improved the performance on SF test to AUROC 0.78. Performance of SVM, and BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and 0.74) was similar to human performance (AUROC 0.81). CONCLUSION: In summary, we successfully developed cross-institution NLP models on incident report text from radiation oncology centers. These models were able to detect high-severity reports similarly to humans on a curated dataset.</li>
<li><strong>摘要：</strong>目的：事件报告是医疗保健安全和质量改进的重要工具，但是手动审查是耗时的，需要主题专业知识。在这里，我们提出了一种自然语言处理（NLP）筛选工具，以检测两个机构的辐射肿瘤学中的高度事件报告。方法和材料：我们使用两个文本数据集来训练和评估我们的NLP模型：来自机构（Inst。）的7,094个报告，以及来自IAEA Safron（SF）的571个报告，所有这些报告都具有临床内容专家标记的严重性评分。我们培训和评估了两种类型的模型：基线支持矢量机（SVM）和Bluebert，这是在PubMed摘要和住院的患者数据上鉴定的大型语言模型。我们通过两种方式评估了模型的普遍性。首先，我们评估了使用Inst.-Train在SF检验中训练的模型。其次，我们训练了Bluebert_transfer型号，该模型首先在Inst上进行了微调。然后在SF-Train上进行了训练，然后在SF-Test Set上进行测试。为了进一步分析模型性能，我们还研究了我们Inst的59个报告的子集。数据集，为清晰而手动编辑。结果分类的性能。使用Bluebert使用SVM和0.81实现了AUROC 0.82。在没有跨机构转移学习的情况下，使用Bluebert使用SVM进行SF测试的性能仅限于0.42的AUROC。 Bluebert_transfer在两个数据集上进行了微调，将SF测试的性能提高到AUROC 0.78。在手动策划的Inst上的SVM和Bluebert_transfer模型的性能。报告（AUROC 0.85和0.74）与人类表现相似（AUROC 0.81）。结论：总而言之，我们成功地开发了跨机构NLP模型，这些模型是从辐射肿瘤学中心的事件报告文本中。这些模型能够在策划数据集上类似地检测到与人类类似的高度报告。</li>
</ul>

<h3>Title: DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13723">https://arxiv.org/abs/2509.13723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13723">https://arxiv.org/pdf/2509.13723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13723]] DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning(https://arxiv.org/abs/2509.13723)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To address this prompt inflation problem, prompt compression has been proposed. However, most existing methods require training a small auxiliary model for compression, incurring a significant amount of additional computation. To avoid this, we propose a two-stage, training-free approach, called Dual-Stage Progressive Compression (DSPC). In the coarse-grained stage, semantic-related sentence filtering removes sentences with low semantic value based on TF-IDF. In the fine-grained stage, token importance is assessed using attention contribution, cross-model loss difference, and positional importance, enabling the pruning of low-utility tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget and observe consistent improvements. For instance, in the FewShot task of the Longbench dataset, DSPC achieves a performance of 49.17 by using only 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在许多自然语言处理（NLP）任务中取得了巨大的成功。为了实现更准确的输出，用于驱动LLM的提示已经越来越长，这会带来更高的计算成本。为了解决此及时通货膨胀问题，已经提出了及时的压缩。但是，大多数现有方法都需要训练一个小的辅助模型以进行压缩，从而产生大量额外的计算。为了避免这种情况，我们提出了一种两阶段的无训练方法，称为双阶段进行性压缩（DSPC）。在粗粒阶段，与语义相关的句子过滤消除了基于TF-IDF的语义值低的句子。在细粒阶段，使用注意力贡献，跨模型损失差和位置重要性评估令牌的重要性，从而在保留语义的同时可以修剪低耐用令牌。我们验证了DSPC在约束的标记预算下对Llama-3.1-8B-Instruct和GPT-3.5-Turbo验证，并观察到一致的改进。例如，在Longbench数据集的几个Shot任务中，DSPC仅使用少3倍代币来实现49.17的性能，以优于最好的最新基线LongllMlingua在7.76中的表现。</li>
</ul>

<h3>Title: Implementing a Logical Inference System for Japanese Comparatives</h3>
<ul>
<li><strong>Authors: </strong>Yosuke Mikami, Daiki Matsuoka, Hitomi Yanaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13734">https://arxiv.org/abs/2509.13734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13734">https://arxiv.org/pdf/2509.13734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13734]] Implementing a Logical Inference System for Japanese Comparatives(https://arxiv.org/abs/2509.13734)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Natural Language Inference (NLI) involving comparatives is challenging because it requires understanding quantities and comparative relations expressed by sentences. While some approaches leverage Large Language Models (LLMs), we focus on logic-based approaches grounded in compositional semantics, which are promising for robust handling of numerical and logical expressions. Previous studies along these lines have proposed logical inference systems for English comparatives. However, it has been pointed out that there are several morphological and semantic differences between Japanese and English comparatives. These differences make it difficult to apply such systems directly to Japanese comparatives. To address this gap, this study proposes ccg-jcomp, a logical inference system for Japanese comparatives based on compositional semantics. We evaluate the proposed system on a Japanese NLI dataset containing comparative expressions. We demonstrate the effectiveness of our system by comparing its accuracy with that of existing LLMs.</li>
<li><strong>摘要：</strong>涉及比较的自然语言推论（NLI）具有挑战性，因为它需要理解句子表达的数量和比较关系。尽管某些方法利用了大型语言模型（LLM），但我们专注于基于逻辑语义的方法，这些方法基于构图语义，这对于强大的数值和逻辑表达式进行了强大的处理。这些线的先前研究提出了用于英语比较的逻辑推断系统。但是，已经指出，日本和英语比较之间存在几种形态和语义差异。这些差异使得很难将这种系统直接应用于日本的比较。为了解决这一差距，本研究提出了CCG-JCOMP，这是一种基于组成语义的日本比较的逻辑推断系统。我们在包含比较表达式的日本NLI数据集上评估了所提出的系统。我们通过将其准确性与现有LLM的准确性进行比较来证明我们的系统的有效性。</li>
</ul>

<h3>Title: Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications</h3>
<ul>
<li><strong>Authors: </strong>Vani Kanjirangat, Ljiljana Dolamic, Fabio Rinaldi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13775">https://arxiv.org/abs/2509.13775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13775">https://arxiv.org/pdf/2509.13775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13775]] Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications(https://arxiv.org/abs/2509.13775)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper discusses our exploration of different data-efficient and parameter-efficient approaches to Arabic Dialect Identification (ADI). In particular, we investigate various soft-prompting strategies, including prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA reparameterizations. For the data-efficient strategy, we analyze hard prompting with zero-shot and few-shot inferences to analyze the dialect identification capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT approaches, we conducted our experiments using Arabic-specific encoder models on several major datasets. We also analyzed the n-shot inferences on open-source decoder-only models, a general multilingual model (Phi-3.5), and an Arabic-specific one(SILMA). We observed that the LLMs generally struggle to differentiate the dialectal nuances in the few-shot or zero-shot setups. The soft-prompted encoder variants perform better, while the LoRA-based fine-tuned models perform best, even surpassing full fine-tuning.</li>
<li><strong>摘要：</strong>本文讨论了我们对阿拉伯方言识别（ADI）的不同数据效率和参数有效方法的探索。特别是，我们研究了各种软宣传策略，包括前缀调整，及时调整，p-tuning和p-tuning v2以及洛拉（Lora）重新级分。对于数据有效的策略，我们分析了零射击和几乎没有推断的硬提示，以分析大语言模型（LLMS）的方言识别能力。对于参数有效的PEFT方法，我们在几个主要数据集上使用阿拉伯特异性编码器模型进行了实验。我们还分析了仅开源解码器模型，一般多语言模型（PHI-3.5）和阿拉伯特异性模型（SILMA）上的N-shot推断。我们观察到，LLM通常难以区分几次或零拍设置中的方言细微差别。软宣传的编码器变体的性能更好，而基于洛拉的微型模型的性能最佳，甚至超过完整的微调。</li>
</ul>

<h3>Title: Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning</h3>
<ul>
<li><strong>Authors: </strong>Yangning Li, Tingwei Lu, Yinghui Li, Yankai Chen, Wei-Chieh Huang, Wenhao Jiang, Hui Wang, Hai-Tao Zheng, Philip S.Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13790">https://arxiv.org/abs/2509.13790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13790">https://arxiv.org/pdf/2509.13790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13790]] Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning(https://arxiv.org/abs/2509.13790)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Efficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning.</li>
<li><strong>摘要：</strong>有效的指令调整旨在增强在给定指令数据集培训的大型语言模型（LLMS）的最终性能。课程学习作为典型的数据组织策略已显示在教学调整中有效性。但是，当前的课程调整方法遭受了课程僵化的影响，因为它们仅依赖于静态启发式难度指标。这些方法无法适应训练过程中模型不断发展的能力，从而导致了固定且潜在的亚地区学习轨迹。为了解决这个问题，提出了称为校园的能力意识到的多人课程教学教学框架。校园提供了几个优点：（1）亚型的动态选择。 （2）对课程时间表的能力感知能力调整。 （3）多个基于难度的调度。与其他最先进的基线相比，广泛的实验证明了校园的出色表现，以进行有效的指导调整。</li>
</ul>

<h3>Title: Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Edward Phillips, Sean Wu, Soheila Molaei, Danielle Belgrave, Anshul Thakur, David Clifton</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13813">https://arxiv.org/abs/2509.13813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13813">https://arxiv.org/pdf/2509.13813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13813]] Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs(https://arxiv.org/abs/2509.13813)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate impressive results across diverse tasks but are still known to hallucinate, generating linguistically plausible but incorrect answers to questions. Uncertainty quantification has been proposed as a strategy for hallucination detection, but no existing black-box approach provides estimates for both global and local uncertainty. The former attributes uncertainty to a batch of responses, while the latter attributes uncertainty to individual responses. Current local methods typically rely on white-box access to internal model states, whilst black-box methods only provide global uncertainty estimates. We introduce a geometric framework to address this, based on archetypal analysis of batches of responses sampled with only black-box model access. At the global level, we propose Geometric Volume, which measures the convex hull volume of archetypes derived from response embeddings. At the local level, we propose Geometric Suspicion, which ranks responses by reliability and enables hallucination reduction through preferential response selection. Unlike prior dispersion methods which yield only a single global score, our approach provides semantic boundary points which have utility for attributing reliability to individual responses. Experiments show that our framework performs comparably to or better than prior methods on short form question-answering datasets, and achieves superior results on medical datasets where hallucinations carry particularly critical risks. We also provide theoretical justification by proving a link between convex hull volume and entropy.</li>
<li><strong>摘要：</strong>大型语言模型在各种任务中表现出令人印象深刻的结果，但仍然众所周知，它会幻觉，在语言上具有合理的问题，但对问题的答案不正确。已经提出了不确定性量化作为幻觉检测的策略，但是现有的黑框方法均可估算全球和局部不确定性。前者将不确定性归因于一批响应，而后者则将不确定性归因于单个响应。当前的本地方法通常依赖于对内部模型状态的白色框访问，而黑框方法仅提供全局不确定性估计。我们引入了一个几何框架来解决此问题，基于对仅使用黑框模型访问采样的响应批次的原型分析。在全球层面，我们提出了几何体积，该几何体积测量了响应嵌入的原型凸壳体积。在地方一级，我们提出了几何怀疑，该怀疑是通过可靠性对响应进行排名，并通过优先响应选择可以减少幻觉。与仅产生单个全球分数的先前分散方法不同，我们的方法提供了语义边界点，这些点具有将可靠性归因于单个响应的实用性。实验表明，我们的框架的性能比简短的提问数据集的先前方法相当或更好，并且在幻觉带有特别关键风险的医疗数据集上取得了卓越的结果。我们还通过证明凸壳体积和熵之间的联系来提供理论上的理由。</li>
</ul>

<h3>Title: Findings of the Third Automatic Minuting (AutoMin) Challenge</h3>
<ul>
<li><strong>Authors: </strong>Kartik Shinde, Laurent Besacier, Ondrej Bojar, Thibaut Thonet, Tirthankar Ghosal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13814">https://arxiv.org/abs/2509.13814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13814">https://arxiv.org/pdf/2509.13814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13814]] Findings of the Third Automatic Minuting (AutoMin) Challenge(https://arxiv.org/abs/2509.13814)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper presents the third edition of AutoMin, a shared task on automatic meeting summarization into minutes. In 2025, AutoMin featured the main task of minuting, the creation of structured meeting minutes, as well as a new task: question answering (QA) based on meeting transcripts. The minuting task covered two languages, English and Czech, and two domains: project meetings and European Parliament sessions. The QA task focused solely on project meetings and was available in two settings: monolingual QA in English, and cross-lingual QA, where questions were asked and answered in Czech based on English meetings. Participation in 2025 was more limited compared to previous years, with only one team joining the minuting task and two teams participating in QA. However, as organizers, we included multiple baseline systems to enable a comprehensive evaluation of current (2025) large language models (LLMs) on both tasks.</li>
<li><strong>摘要：</strong>本文介绍了第三版汽车，这是自动会议摘要到几分钟的共同任务。在2025年，Automin的主要任务是挖掘的主要任务，结构化会议记录的创建以及一项新任务：基于成绩单的问题答案（QA）。精通的任务涵盖了两种语言：英语和捷克语，以及两个领域：项目会议和欧洲议会会议。质量检查任务仅着眼于项目会议，并在两种环境中提供：单语质量质量为英语和跨语言QA，在那里根据英语会议在捷克语中提出问题并回答。与往年相比，2025年的参与更加有限，只有一支团队加入了精简任务，两支球队参加了QA。但是，作为组织者，我们包括了多个基线系统，以对这两个任务的当前（2025）大语言模型（LLM）进行全面评估。</li>
</ul>

<h3>Title: Large Language Models Discriminate Against Speakers of German Dialects</h3>
<ul>
<li><strong>Authors: </strong>Minh Duc Bui, Carolin Holtermann, Valentin Hofmann, Anne Lauscher, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13835">https://arxiv.org/abs/2509.13835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13835">https://arxiv.org/pdf/2509.13835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13835]] Large Language Models Discriminate Against Speakers of German Dialects(https://arxiv.org/abs/2509.13835)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage.</li>
<li><strong>摘要：</strong>方言代表了人类文化的重要组成部分，并且在世界所有地区都可以找到。在德国，超过40％的人口说区域方言（Adler and Hansen，2022年）。然而，尽管文化重要性，但讲话方言的人通常面临负面的社会刻板印象。我们检查了这种刻板印象是否通过大型语言模型（LLM）反映。我们借鉴了方言感知的社会语言文献，以分析与方言扬声器通常相关的特征。基于这些特征，我们评估了LLMS在两个任务中表达的方言命名命名偏差和方言用法偏差：关联任务和决策任务。为了评估模型的方言用法偏见，我们构建了一个新颖的评估语料库，该语料库与七个区域性德国方言（例如Alemannic和Bavarian）的句子与他们的标准德国同行配对。我们发现：（1）在协会任务中，所有评估的LLM均表现出明显的方言命名和方言用法偏向于德国方言演讲者，以负面的形容词关联反映； （2）所有模型在决策中重现这些方言命名和方言用法偏见； （3）与先前的工作相反，与明确的人口统计学提及相反，我们发现明确标记语言人口统计学的标签（泥法方言扬声器）比诸如方言用法之类的隐含提示更大了偏见。</li>
</ul>

<h3>Title: Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Chenhui Chu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13869">https://arxiv.org/abs/2509.13869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13869">https://arxiv.org/pdf/2509.13869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13869]] Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs(https://arxiv.org/abs/2509.13869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can lead to undesired consequences when misaligned with human values, especially in scenarios involving complex and sensitive social biases. Previous studies have revealed the misalignment of LLMs with human values using expert-designed or agent-based emulated bias scenarios. However, it remains unclear whether the alignment of LLMs with human values differs across different types of scenarios (e.g., scenarios containing negative vs. non-negative questions). In this study, we investigate the alignment of LLMs with human values regarding social biases (HVSB) in different types of bias scenarios. Through extensive analysis of 12 LLMs from four model families and four datasets, we demonstrate that LLMs with large model parameter scales do not necessarily have lower misalignment rate and attack success rate. Moreover, LLMs show a certain degree of alignment preference for specific types of scenarios and the LLMs from the same model family tend to have higher judgment consistency. In addition, we study the understanding capacity of LLMs with their explanations of HVSB. We find no significant differences in the understanding of HVSB across LLMs. We also find LLMs prefer their own generated explanations. Additionally, we endow smaller language models (LMs) with the ability to explain HVSB. The generation results show that the explanations generated by the fine-tuned smaller LMs are more readable, but have a relatively lower model agreeability.</li>
<li><strong>摘要：</strong>当与人类价值观未对准时，大型语言模型（LLM）可能会导致不希望的后果，尤其是在涉及复杂和敏感的社会偏见的情况下。先前的研究揭示了使用专家设计或基于代理的模拟偏差方案的LLM与人类价值的未对准。但是，尚不清楚LLM与人类价值观的一致性在不同类型的场景之间是否有所不同（例如，包含负面与非负问题的情况）。在这项研究中，我们研究了LLM与不同类型的偏见场景中有关社会偏见（HVSB）的人类价值观的一致性。通过对来自四个模型家族和四个数据集的12个LLM的广泛分析，我们证明了具有较大模型参数量表的LLM不一定具有较低的未对准率和攻击成功率。此外，LLM对特定类型的场景显示一定程度的一致性偏好，而来自同一模型家族的LLM往往具有更高的判断一致性。此外，我们研究了LLM的理解能力以及他们对HVSB的解释。我们发现在LLM中对HVSB的理解没有显着差异。我们还发现LLM更喜欢自己生成的解释。此外，我们将较小的语言模型（LMS）赋予解释HVSB的能力。一代结果表明，微调较小的LMS产生的解释更可读性，但模型相对较低。</li>
</ul>

<h3>Title: Combining Evidence and Reasoning for Biomedical Fact-Checking</h3>
<ul>
<li><strong>Authors: </strong>Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13879">https://arxiv.org/abs/2509.13879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13879">https://arxiv.org/pdf/2509.13879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13879]] Combining Evidence and Reasoning for Biomedical Fact-Checking(https://arxiv.org/abs/2509.13879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical sys- tems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminol- ogy, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combin- ing Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language mod- els with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of halluci- nations, ensuring that generated outputs are grounded in veri- fiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the- art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https: //github.com/PRAISELab-PicusLab/CER.</li>
<li><strong>摘要：</strong>从疫苗犹豫到未经证实的治疗，医疗保健中的错误信息对公共卫生和对医疗系统的信任构成了风险。尽管机器学习和自然语言处理已提高自动化事实检查，但由于复杂的终端，对领域专业知识的需求以及在科学证据中基础的至关重要性，对生物医学主张的验证仍然具有巨大的挑战。我们介绍了CER（结合证据和推理），这是一个新型的生物医学事实检查框架，该框架整合了科学证据检索，通过大语言模型进行推理以及监督的真实性预​​测。通过将大型语言模型的文本生成能力与高质量生物医学科学证据的先进检索技术相结合，有效地减轻了幻觉的风险，以确保生成的产出以可验证的循证，基于证据的基于证据的来源基础。对专家注销数据集的评估（HealthFC，Bioasq-7B，Scifact）表明了最先进的性能和有希望的跨数据集概括。代码和数据发布以透明和可重复性：https：//github.com/praiselab-picuslab/cer。</li>
</ul>

<h3>Title: Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification</h3>
<ul>
<li><strong>Authors: </strong>Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13888">https://arxiv.org/abs/2509.13888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13888">https://arxiv.org/pdf/2509.13888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13888]] Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification(https://arxiv.org/abs/2509.13888)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: this https URL</li>
<li><strong>摘要：</strong>从疫苗犹豫到未经证实的治疗，医疗保健中的错误信息对公共卫生和对医疗系统的信任构成了风险。尽管机器学习和自然语言处理已经提高了自动化事实检查，但由于复杂的术语，对领域专业知识的需求以及基于科学证据的基础，验证生物医学主张仍然具有巨大的挑战。我们介绍了CER（结合证据和推理），这是一个新型的生物医学事实检查框架，该框架整合了科学证据检索，通过大语言模型进行推理以及监督的真实性预​​测。通过将大语言模型的文本生成能力与用于高质量生物医学科学证据的先进检索技术相结合，有效地降低了幻觉的风险，从而确保了生成的产出以可验证的循证来源为基础。对专家注销数据集的评估（HealthFC，Bioasq-7B，Scifact）表明了最先进的性能和有希望的跨数据集概括。释放代码和数据以供透明度和可重复性：此HTTPS URL</li>
</ul>

<h3>Title: Do Large Language Models Understand Word Senses?</h3>
<ul>
<li><strong>Authors: </strong>Domenico Meconi, Simone Stirpe, Federico Martelli, Leonardo Lavalle, Roberto Navigli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13905">https://arxiv.org/abs/2509.13905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13905">https://arxiv.org/pdf/2509.13905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13905]] Do Large Language Models Understand Word Senses?(https://arxiv.org/abs/2509.13905)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Understanding the meaning of words in context is a fundamental capability for Large Language Models (LLMs). Despite extensive evaluation efforts, the extent to which LLMs show evidence that they truly grasp word senses remains underexplored. In this paper, we address this gap by evaluating both i) the Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs, comparing their performance to state-of-the-art systems specifically designed for the task, and ii) the ability of two top-performing open- and closed-source LLMs to understand word senses in three generative settings: definition generation, free-form explanation, and example generation. Notably, we find that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve performance on par with specialized WSD systems, while also demonstrating greater robustness across domains and levels of difficulty. In the generation tasks, results reveal that LLMs can explain the meaning of words in context up to 98\% accuracy, with the highest performance observed in the free-form explanation task, which best aligns with their generative capabilities.</li>
<li><strong>摘要：</strong>在上下文中了解单词的含义是大语言模型（LLM）的基本能力。尽管进行了广泛的评估工作，但LLM显示出他们真正掌握单词感官的证据的程度仍然没有得到充实。在本文中，我们通过评估i）指导型llms的单词感觉歧义（WSD）功能，将它们的性能与专门为任务设计的最新系统进行了比较，以及ii）ii）两个表现最好的开放和封闭式LLM的能力，可以在三个生成的环境中识别出三生的自发性设置：定义生成的启示：定义的启动，定义，并示例定义。值得注意的是，我们发现，在WSD任务中，诸如GPT-4O和DeepSeek-V3之类的领先模型与专门的WSD系统相同，同时还表现出跨领域和难度水平的更大鲁棒性。在一代任务中，结果表明，LLM可以在上下文中解释单词的含义最高98 \％的准确性，并且在自由形式的解释任务中观察到了最高的性能，这最能与其生成能力保持一致。</li>
</ul>

<h3>Title: Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG</h3>
<ul>
<li><strong>Authors: </strong>Dayeon Ki, Marine Carpuat, Paul McNamee, Daniel Khashabi, Eugene Yang, Dawn Lawrie, Kevin Duh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13930">https://arxiv.org/abs/2509.13930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13930">https://arxiv.org/pdf/2509.13930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13930]] Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG(https://arxiv.org/abs/2509.13930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Multilingual Retrieval-Augmented Generation (mRAG) systems enable language models to answer knowledge-intensive queries with citation-supported responses across languages. While such systems have been proposed, an open questions is whether the mixture of different document languages impacts generation and citation in unintended ways. To investigate, we introduce a controlled methodology using model internals to measure language preference while holding other factors such as document relevance constant. Across eight languages and six open-weight models, we find that models preferentially cite English sources when queries are in English, with this bias amplified for lower-resource languages and for documents positioned mid-context. Crucially, we find that models sometimes trade-off document relevance for language preference, indicating that citation choices are not always driven by informativeness alone. Our findings shed light on how language models leverage multilingual context and influence citation behavior.</li>
<li><strong>摘要：</strong>多语言检索功能生成（MRAG）系统使语言模型能够通过语言跨语言的响应来回答知识密集的查询。尽管已经提出了这样的系统，但一个空旷的问题是，不同文档语言的混合是否会以意想不到的方式影响发电和引用。为了进行调查，我们使用模型内部介绍了一种受控的方法来衡量语言偏好，同时持有其他因素，例如文档相关性常数。在八种语言和六种开放式型号中，我们发现，当查询用英语查询时，模型优先引用英语来源，这种偏见会放大了低资源的语言和位于中文中的文档。至关重要的是，我们发现模型有时与语言偏好相关的权衡文件，表明引用选择并不总是由信息性驱动。我们的发现阐明了语言模型如何利用多语言上下文并影响引文行为。</li>
</ul>

<h3>Title: Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency</h3>
<ul>
<li><strong>Authors: </strong>Colin Hong, Xu Guo, Anand Chaanan Singh, Esha Choukse, Dmitrii Ustiugov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.13990">https://arxiv.org/abs/2509.13990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.13990">https://arxiv.org/pdf/2509.13990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.13990]] Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency(https://arxiv.org/abs/2509.13990)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recently, Test-Time Scaling (TTS) has gained increasing attention for improving LLM reasoning performance at test time without retraining the model. A notable TTS technique is Self-Consistency (SC), which generates multiple reasoning chains in parallel and selects the final answer via majority voting. While effective, the order-of-magnitude computational overhead limits its broad deployment. Prior attempts to accelerate SC mainly rely on model-based confidence scores or heuristics with limited empirical support. For the first time, we theoretically and empirically analyze the inefficiencies of SC and reveal actionable opportunities for improvement. Building on these insights, we propose Slim-SC, a step-wise pruning strategy that identifies and removes redundant chains using inter-chain similarity at the thought level. Experiments on three STEM reasoning datasets and two recent LLM architectures show that Slim-SC reduces inference latency and KVC usage by up to 45% and 26%, respectively, with R1-Distill, while maintaining or improving accuracy, thus offering a simple yet efficient TTS alternative for SC.</li>
<li><strong>摘要：</strong>最近，测试时间缩放（TTS）在不重新培训模型的情况下提高测试时间的LLM推理性能方面已越来越关注。著名的TTS技术是自洽（SC），它并行生成多个推理链，并通过多数投票选择最终答案。虽然有效，但设施级计算间接费用限制了其广泛的部署。先前尝试加速SC的尝试主要依赖于基于模型的置信度分数或启发式方法，并具有有限的经验支持。我们第一次从理论上和经验上分析了SC的效率低下，并揭示了可行的改进机会。在这些见解的基础上，我们提出了Slim-SC，这是一种逐步的修剪策略，使用链间相似性在思想水平上识别和去除冗余链。在三个STEM推理数据集和两个LLM架构上进行的实验表明，SLIM-SC的推理潜伏期和KVC使用分别降低了45％和26％，同时维持或提高了准确性，从而提供了简单而有效的SC替代品。</li>
</ul>

<h3>Title: Early Stopping Chain-of-thoughts in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minjia Mao, Bowen Yin, Yu Zhu, Xiao Fang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14004">https://arxiv.org/abs/2509.14004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14004">https://arxiv.org/pdf/2509.14004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14004]] Early Stopping Chain-of-thoughts in Large Language Models(https://arxiv.org/abs/2509.14004)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Reasoning large language models (LLMs) have demonstrated superior capacities in solving complicated problems by generating long chain-of-thoughts (CoT), but such a lengthy CoT incurs high inference costs. In this study, we introduce ES-CoT, an inference-time method that shortens CoT generation by detecting answer convergence and stopping early with minimal performance loss. At the end of each reasoning step, we prompt the LLM to output its current final answer, denoted as a step answer. We then track the run length of consecutive identical step answers as a measure of answer convergence. Once the run length exhibits a sharp increase and exceeds a minimum threshold, the generation is terminated. We provide both empirical and theoretical support for this heuristic: step answers steadily converge to the final answer, and large run-length jumps reliably mark this convergence. Experiments on five reasoning datasets across three LLMs show that ES-CoT reduces the number of inference tokens by about 41\% on average while maintaining accuracy comparable to standard CoT. Further, ES-CoT integrates seamlessly with self-consistency prompting and remains robust across hyperparameter choices, highlighting it as a practical and effective approach for efficient reasoning.</li>
<li><strong>摘要：</strong>推理大语言模型（LLM）通过产生长长的思想链（COT）在解决复杂问题方面表现出了较高的能力，但是如此长的COT会带来很高的推理成本。在这项研究中，我们介绍了ES-COT，ES-COT是一种推理时间方法，该方法通过检测答案收敛并尽早停止绩效损失来缩短COT的产生。在每个推理步骤结束时，我们提示LLM输出其当前的最终答案，称为步骤答案。然后，我们跟踪连续相同步骤答案的运行长度，以衡量答案收敛。一旦跑步长度显示出急剧的增加并超过最小阈值，就会终止生成。我们为此启发式提供了经验和理论支持：步骤答案稳步融合到最终答案，并且大型运行长度可靠地标志着这种融合。在三个LLM的五个推理数据集上进行的实验表明，ES-COT平均将推理令牌的数量减少了约41 \％，同时保持与标准COT相当的精度。此外，ES-COT无缝集成与自稳态的提示，并在超级参数选择中保持健壮，从而将其作为一种实用有效的有效推理方法。</li>
</ul>

<h3>Title: Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale</h3>
<ul>
<li><strong>Authors: </strong>Hasan Abed Al Kader Hammoud, Mohammad Zbeeb, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14008">https://arxiv.org/abs/2509.14008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14008">https://arxiv.org/pdf/2509.14008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14008]] Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale(https://arxiv.org/abs/2509.14008)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the "nano" ($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP.</li>
<li><strong>摘要：</strong>我们介绍了Hala，这是一个以阿拉伯语为中心的教学和翻译模型的家族，并通过我们的翻译管道建立。我们首先将强大的AR $ \ leftrightArrow $ en老师压缩到FP8（产生$ \ sim $ 2 $ \ times $较高的吞吐量，没有质量损失），并使用它来创建高保真双语的监督。然后对此数据进行了微调，轻巧的语言模型LFM2-1.2B进行了微调，并用于将高质量的英语教学集转化为阿拉伯语，并产生了一百万级的语料库。我们以350m，700m，1.2b和9b参数训练HALA型号，并应用SLERP合并以平衡阿拉伯专业和基本模型强度。在以阿拉伯语为中心的基准上，Hala在“ Nano”（$ \ leq $ 2B）和“ Small”（7-9b）类别中都取得了最先进的结果，表现优于其基础。我们发布模型，数据，评估和食谱，以加速阿拉伯语NLP的研究。</li>
</ul>

<h3>Title: Enhancing Multi-Agent Debate System Performance via Confidence Expression</h3>
<ul>
<li><strong>Authors: </strong>Zijie Lin, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14034">https://arxiv.org/abs/2509.14034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14034">https://arxiv.org/pdf/2509.14034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14034]] Enhancing Multi-Agent Debate System Performance via Confidence Expression(https://arxiv.org/abs/2509.14034)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Generative Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Recent research has introduced Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate human debate and thereby improve task performance. However, while some LLMs may possess superior knowledge or reasoning capabilities for specific tasks, they often struggle to clearly communicate this advantage during debates, in part due to a lack of confidence expression. Moreover, inappropriate confidence expression can cause agents in MAD systems to either stubbornly maintain incorrect beliefs or converge prematurely on suboptimal answers, ultimately reducing debate effectiveness and overall system performance. To address these challenges, we propose incorporating confidence expression into MAD systems to allow LLMs to explicitly communicate their confidence levels. To validate this approach, we develop ConfMAD, a MAD framework that integrates confidence expression throughout the debate process. Experimental results demonstrate the effectiveness of our method, and we further analyze how confidence influences debate dynamics, offering insights into the design of confidence-aware MAD systems.</li>
<li><strong>摘要：</strong>生成的大语言模型（LLMS）在各种任务中表现出了出色的表现。最近的研究引入了多代理辩论（MAD）系统，该系统利用多个LLM来模拟人类辩论并改善任务绩效。但是，尽管某些LLM可能具有特定任务的优越的知识或推理能力，但他们经常在辩论中清楚地传达这一优势，部分原因是缺乏信心表达。此外，不当的信心表达可能会导致疯狂系统中的代理人顽固地保持不正确的信念或过早地融合了次优的答案，最终降低了辩论有效性和整体系统绩效。为了应对这些挑战，我们建议将置信度表达到疯狂的系统中，以允许LLM明确传达其信心水平。为了验证这种方法，我们开发了Confmad，这是一个疯狂的框架，在整个辩论过程中整合了置信度表达。实验结果证明了我们方法的有效性，我们进一步分析了置信度如何影响争论动态，从而洞悉了置信度吸引的疯狂系统的设计。</li>
</ul>

<h3>Title: Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST</h3>
<ul>
<li><strong>Authors: </strong>Monica Sekoyan, Nithin Rao Koluguri, Nune Tadevosyan, Piotr Zelasko, Travis Bartley, Nick Karpov, Jagadeesh Balam, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14128">https://arxiv.org/abs/2509.14128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14128">https://arxiv.org/pdf/2509.14128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14128]] Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST(https://arxiv.org/abs/2509.14128)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>This report introduces Canary-1B-v2, a fast, robust multilingual model for Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built with a FastConformer encoder and Transformer decoder, it supports 25 languages primarily European. The model was trained on 1.7M hours of total data samples, including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce hallucinations for ASR and AST. We describe its two-stage pre-training and fine-tuning process with dynamic data balancing, as well as experiments with an nGPT encoder. Results show nGPT scales well with massive data, while FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2 outperforms Whisper-large-v3 on English ASR while being 10x faster, and delivers competitive multilingual ASR and AST performance against larger models like Seamless-M4T-v2-large and LLM-based systems. We also release Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the same 25 languages with just 600M parameters.</li>
<li><strong>摘要：</strong>该报告介绍了Canary-1B-V2，这是一种快速，可靠的多语言模型，用于自动语音识别（ASR）和语音到文本翻译（AST）。它是由快速构造器编码器和变压器解码器构建的，主要支持25种主要欧洲语言。该模型接受了170万小时的总数据样本的培训，包括Granary和Nemo ASR Set 3.0，并增加了非语音音频，以减少ASR和AST的幻觉。我们用动态数据平衡描述了其两阶段的预训练和微调过程，以及NGPT编码器的实验。结果表明，NGPT与大量数据相当良好，而快速构造器在微调后表现出色。对于时间戳，Canary-1B-V2使用Nemo强制对准器（NFA）与辅助CTC模型，为ASR和AST提供可靠的段级时级。评估表明，Canary-1b-V2在英语ASR上的低语v3优于10倍，而对较大的模型（如无缝M4T-V2-LARGE和LLM基系统）提供竞争性的多语言ASR和AST性能。我们还发布了V2的继任者Parakeet-TDT-0.6B-V3，提供了只有600m参数的相同25种语言的多语言ASR。</li>
</ul>

<h3>Title: AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity</h3>
<ul>
<li><strong>Authors: </strong>Yifan Liu, Wenkuan Zhao, Shanshan Zhong, Jinghui Qin, Mingfu Liang, Zhongzhan Huang, Wushao Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14171">https://arxiv.org/abs/2509.14171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14171">https://arxiv.org/pdf/2509.14171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14171]] AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity(https://arxiv.org/abs/2509.14171)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal large language models (MLLMs) have garnered significant attention, offering a promising pathway toward artificial general intelligence (AGI). Among the essential capabilities required for AGI, creativity has emerged as a critical trait for MLLMs, with association serving as its foundation. Association reflects a model' s ability to think creatively, making it vital to evaluate and understand. While several frameworks have been proposed to assess associative ability, they often overlook the inherent ambiguity in association tasks, which arises from the divergent nature of associations and undermines the reliability of evaluations. To address this issue, we decompose ambiguity into two types-internal ambiguity and external ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative ability while circumventing the ambiguity through a hybrid computational method. We then conduct extensive experiments on MLLMs, revealing a strong positive correlation between cognition and association. Additionally, we observe that the presence of ambiguity in the evaluation process causes MLLMs' behavior to become more random-like. Finally, we validate the effectiveness of our method in ensuring more accurate and reliable evaluations. See Project Page for the data and codes.</li>
<li><strong>摘要：</strong>多模式大语言模型（MLLM）的最新进步引起了人们的重大关注，为人工通用智能（AGI）提供了有希望的途径。在AGI所需的基本能力中，创造力已成为MLLM的关键特征，并成为其基础。协会反映了模型的创造性思考能力，使评估和理解至关重要。尽管已经提出了几个框架来评估关联能力，但它们经常忽略关联任务中固有的歧义，这是由于关联的不同性质引起的，并破坏了评估的可靠性。为了解决这个问题，我们将歧义分解为两种类型的内部歧义和外部歧义，并引入了Associam，这是一种旨在评估关联能力的基准，同时通过混合计算方法来规避歧义。然后，我们在MLLM上进行了广泛的实验，揭示了认知与关联之间存在强大的正相关。此外，我们观察到评估过程中歧义的存在会导致MLLM的行为变得更加随机。最后，我们验证方法在确保更准确和可靠的评估方面的有效性。有关数据和代码，请参见项目页面。</li>
</ul>

<h3>Title: Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs</h3>
<ul>
<li><strong>Authors: </strong>Akhil Theerthala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14180">https://arxiv.org/abs/2509.14180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14180">https://arxiv.org/pdf/2509.14180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14180]] Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs(https://arxiv.org/abs/2509.14180)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Personalized financial advice requires consideration of user goals, constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on support systems for investors and financial planners. Simultaneously, numerous recent studies examine broader personal finance tasks, including budgeting, debt management, retirement, and estate planning, through agentic pipelines that incur high maintenance costs, yielding less than 25% of their expected financial returns. In this study, we introduce a novel and reproducible framework that integrates relevant financial context with behavioral finance studies to construct supervision data for end-to-end advisors. Using this framework, we create a 19k sample reasoning dataset and conduct a comprehensive fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test split and a blind LLM-jury study, we demonstrate that through careful data curation and behavioral integration, our 8B model achieves performance comparable to significantly larger baselines (14-32B parameters) across factual accuracy, fluency, and personalization metrics while incurring 80% lower costs than the larger counterparts.</li>
<li><strong>摘要：</strong>个性化的财务建议需要考虑用户目标，约束，风险承受能力和管辖权。先前的LLM工作专注于投资者和财务计划者的支持系统。同时，许多最近的研究研究了更广泛的个人理财任务，包括预算，债务管理，退休和房地产规划，通过产生高维护成本的代理管道，占预期财务收益的不到25％。在这项研究中，我们介绍了一个新颖且可重复的框架，将相关的财务背景与行为财务研究相结合，以构建端到端顾问的监督数据。使用此框架，我们创建了一个19k样本推理数据集，并对数据集上的QWEN-3-8B模型进行了全面的微调。通过持有的测试拆分和盲目的LLM陪审团研究，我们证明，通过仔细的数据策展和行为整合，我们的8B模型可以在跨个性的准确性，流利度和个性化指标的效果相当，同时比更大的80％成本相当。</li>
</ul>

<h3>Title: Framing Migration: A Computational Analysis of UK Parliamentary Discourse</h3>
<ul>
<li><strong>Authors: </strong>Vahid Ghafouri, Robert McNeil, Teodor Yankov, Madeleine Sumption, Luc Rocher, Scott A. Hale, Adam Mahdi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14197">https://arxiv.org/abs/2509.14197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14197">https://arxiv.org/pdf/2509.14197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14197]] Framing Migration: A Computational Analysis of UK Parliamentary Discourse(https://arxiv.org/abs/2509.14197)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We present a large-scale computational analysis of migration-related discourse in UK parliamentary debates spanning over 75 years and compare it with US congressional discourse. Using open-weight LLMs, we annotate each statement with high-level stances toward migrants and track the net tone toward migrants across time and political parties. For the UK, we extend this with a semi-automated framework for extracting fine-grained narrative frames to capture nuances of migration discourse. Our findings show that, while US discourse has grown increasingly polarised, UK parliamentary attitudes remain relatively aligned across parties, with a persistent ideological gap between Labour and the Conservatives, reaching its most negative level in 2025. The analysis of narrative frames in the UK parliamentary statements reveals a shift toward securitised narratives such as border control and illegal immigration, while longer-term integration-oriented frames such as social integration have declined. Moreover, discussions of national law about immigration have been replaced over time by international law and human rights, revealing nuances in discourse trends. Taken together broadly, our findings demonstrate how LLMs can support scalable, fine-grained discourse analysis in political and historical contexts.</li>
<li><strong>摘要：</strong>我们对英国议会辩论中与移民相关的话语进行了大规模计算分析，跨越了75年，并将其与美国国会话语进行了比较。我们使用开放的LLM，以高级立场对移民的高级立场注释每个声明，并在时间和政党跨越对移民的网络语调。对于英国，我们将其扩展到一个半自动化的框架，用于提取细粒度的叙事框架以捕捉迁移话语的细微差别。我们的发现表明，尽管我们的话语越来越两极化，但英国议会的态度仍然相对保持在各方的相对阶段，并且在劳动力和保守派之间存在持久的意识形态差距，在2025年达到其最负面的水平，在2025年达到其最负面的水平。对英国校园的叙事框架的分析在英国的叙事范围内呈现出诸如证券范围内的不断变化 - 揭示了诸如证券界面的转移 - 诸如证券界面的转移 - 范围内的叙事范围 - 范围内的叙事范围 - 随着社会融合的下降。此外，随着时间的推移，关于移民的国家法律的讨论已被国际法和人权所取代，揭示了话语趋势的细微差别。从广义上讲，我们的发现表明了LLM如何在政治和历史背景下支持可扩展的，精细的话语分析。</li>
</ul>

<h3>Title: Apertus: Democratizing Open and Compliant LLMs for Global Language Environments</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Hernández-Cano, Alexander Hägele, Allen Hao Huang, Angelika Romanou, Antoni-Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Ďurech, Ido Hakimi, Juan García Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko Sabolčec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas, Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros, Nicholas Browning, Fabian Bösch, Maximilian Böther, Niklas Canova, Camille Challier, Clement Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, María Grandury, Diba Hashemi, Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Frederike Lübeck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon Matrenok, Henrique Mendoncça, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel, Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Panferov, Léo Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Marian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vamvas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre, David Rosenthal, Elliott Ash, Florian Tramèr, Joost VandeVondele, Livio Veraldi, Martin Rajman, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, Imanol Schlag</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14233">https://arxiv.org/abs/2509.14233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14233">https://arxiv.org/pdf/2509.14233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14233]] Apertus: Democratizing Open and Compliant LLMs for Global Language Environments(https://arxiv.org/abs/2509.14233)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting this http URL exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.</li>
<li><strong>摘要：</strong>我们提出了Apertus，这是一个完全开放的大型语言模型（LLMS）的套件，旨在解决当今开放模型生态系统中的两个系统缺陷：数据合规性和多语言表示。与许多没有可重现数据管道的权重或对内容所有者权利的释放权重的模型不同，APERTUS模型仅在公开可用的数据上仔细预测，以追溯尊重此HTTP URL排除和过滤对非腐败，有毒和个人身份的内容的过滤。为了减轻记忆的风险，我们在预处理过程中采用了金鱼目标，在保留下游任务绩效的同时，强烈抑制了逐字记忆的数据。 Apertus模型还扩大了多种语言的覆盖范围，从1800多种语言上对15T代币进行了培训，分配给非英语内容的预处理数据的约40％。 Apertus以8B和70B量表发布，在多语言基准测试，竞争或超过开放权重的同类产品上，完全开放的模型中的最新模型中的最先进的结果。除了模型权重之外，我们还通过允许的许可来释放我们开发周期中的所有科学工件，包括数据准备脚本，检查点，评估套件和培训代码，从而实现透明的审核和扩展。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
