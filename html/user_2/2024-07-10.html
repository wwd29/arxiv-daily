<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-10</h1>
<h3>Title: Title:
          CodeUpdateArena: Benchmarking Knowledge Editing on API Updates</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CodeUpdateArena: Benchmarking Knowledge Editing on API Updates(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly being used to synthesize and reason about source code. However, the static nature of these models' knowledge does not reflect the fact that libraries and API functions they invoke are continuously evolving, with functionality being added or changing. While numerous benchmarks evaluate how LLMs can generate code, no prior work has studied how an LLMs' knowledge about code API functions can be updated. To fill this gap, we present CodeUpdateArena, a benchmark for knowledge editing in the code domain. An instance in our benchmark consists of a synthetic API function update paired with a program synthesis example that uses the updated functionality; our goal is to update an LLM to be able to solve this program synthesis example without providing documentation of the update at inference time. Compared to knowledge editing for facts encoded in text, success here is more challenging: a code LLM must correctly reason about the semantics of the modified function rather than just reproduce its syntax. Our dataset is constructed by first prompting GPT-4 to generate atomic and executable function updates. Then, for each update, we generate program synthesis examples whose code solutions are prone to use the update. Our benchmark covers updates of various types to 54 functions from seven diverse Python packages, with a total of 670 program synthesis examples. Our experiments show that prepending documentation of the update to open-source code LLMs (i.e., DeepSeek, CodeLlama) does not allow them to incorporate changes for problem solving, and existing knowledge editing techniques also have substantial room for improvement. We hope our benchmark will inspire new methods for knowledge updating in code LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于合成和推理源代码。然而，这些模型知识的静态性质并不能反映它们调用的库和 API 函数不断发展的事实，功能不断增加或变化。虽然许多基准测试评估了 LLM 如何生成代码，但之前没有研究过如何更新 LLM 关于代码 API 函数的知识。为了填补这一空白，我们提出了 CodeUpdateArena，这是一个用于代码领域知识编辑的基准测试。我们的基准测试中的一个实例由合成 API 函数更新与使用更新功能的程序合成示例配对组成；我们的目标是更新 LLM 以能够解决这个程序合成示例，而无需在推理时提供更新的文档。与针对文本中编码的事实的知识编辑相比，这里的成功更具挑战性：代码 LLM 必须正确推理修改后的函数的语义，而不仅仅是重现其语法。我们的数据集是通过首先提示 GPT-4 生成原子和可执行函数更新来构建的。然后，对于每次更新，我们都会生成程序合成示例，其代码解决方案容易使用更新。我们的基准测试涵盖了来自七个不同 Python 包的 54 个函数的各种类型的更新，总共有 670 个程序综合示例。我们的实验表明，在开源代码 LLM（即 DeepSeek、CodeLlama）的更新前添加文档无法让它们纳入用于解决问题的更改，现有的知识编辑技术也有很大改进空间。我们希望我们的基准测试能够激发代码 LLM 中知识更新的新方法。</li>
</ul>

<h3>Title: Title:
          When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails</h3>
<ul>
<li><strong>Authors: </strong>Manish Nagireddy, Inkit Padhi, Soumya Ghosh, Prasanna Sattigeri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have convincing performance in a variety of downstream tasks. However, these systems are prone to generating undesirable outputs such as harmful and biased text. In order to remedy such generations, the development of guardrail (or detector) models has gained traction. Motivated by findings from developing a detector for social bias, we adopt the notion of a use-mention distinction - which we identified as the primary source of under-performance in the preliminary versions of our social bias detector. Armed with this information, we describe a fully extensible and reproducible synthetic data generation pipeline which leverages taxonomy-driven instructions to create targeted and labeled data. Using this pipeline, we generate over 300K unique contrastive samples and provide extensive experiments to systematically evaluate performance on a suite of open source datasets. We show that our method achieves competitive performance with a fraction of the cost in compute and offers insight into iteratively developing efficient and capable guardrail models. Warning: This paper contains examples of text which are toxic, biased, and potentially harmful.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种下游任务中具有令人信服的性能。然而，这些系统容易产生不良输出，例如有害和有偏见的文本。为了纠正此类生成，护栏（或检测器）模型的开发已获得关注。受开发社交偏见检测器的发现的启发，我们采用了使用-提及区分的概念 - 我们将其确定为社交偏见检测器初步版本中性能不佳的主要原因。利用这些信息，我们描述了一个完全可扩展和可重复的合成数据生成管道，该管道利用分类驱动的指令来创建有针对性和标记的数据。使用此管道，我们生成了超过 300K 个独特的对比样本，并进行了广泛的实验以系统地评估一套开源数据集的性能。我们表明，我们的方法以极低的计算成本实现了具有竞争力的性能，并提供了迭代开发高效且功能强大的护栏模型的见解。警告：本文包含有毒、有偏见和潜在有害的文本示例。</li>
</ul>

<h3>Title: Title:
          Large Language Model Recall Uncertainty is Modulated by the Fan Effect</h3>
<ul>
<li><strong>Authors: </strong>Jesse Roberts, Kyle Moore, Thao Pham, Oseremhen Ewaleifoh, Doug Fisher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Model Recall Uncertainty is Modulated by the Fan Effect(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper evaluates whether large language models (LLMs) exhibit cognitive fan effects, similar to those discovered by Anderson in humans, after being pre-trained on human textual data. We conduct two sets of in-context recall experiments designed to elicit fan effects. Consistent with human results, we find that LLM recall uncertainty, measured via token probability, is influenced by the fan effect. Our results show that removing uncertainty disrupts the observed effect. The experiments suggest the fan effect is consistent whether the fan value is induced in-context or in the pre-training data. Finally, these findings provide in-silico evidence that fan effects and typicality are expressions of the same phenomena.</li>
<li><strong>摘要：</strong>本文评估大型语言模型 (LLM) 在对人类文本数据进行预训练后，是否表现出类似于 Anderson 在人类身上发现的认知粉丝效应。我们进行了两组旨在引出粉丝效应的上下文回忆实验。与人类结果一致，我们发现 LLM 回忆不确定性（通过标记概率测量）受粉丝效应的影响。我们的结果表明，消除不确定性会破坏观察到的效应。实验表明，无论粉丝值是在上下文中还是在预训练数据中诱导的，粉丝效应都是一致的。最后，这些发现提供了计算机证据，表明粉丝效应和典型性是同一现象的表现。</li>
</ul>

<h3>Title: Title:
          Data, Data Everywhere: A Guide for Pretraining Dataset Construction</h3>
<ul>
<li><strong>Authors: </strong>Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Bo Liu, Aastha Jhunjhunwala, Zhilin Wang, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Data, Data Everywhere: A Guide for Pretraining Dataset Construction(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The impressive capabilities of recent language models can be largely attributed to the multi-trillion token pretraining datasets that they are trained on. However, model developers fail to disclose their construction methodology which has lead to a lack of open information on how to develop effective pretraining sets. To address this issue, we perform the first systematic study across the entire pipeline of pretraining set construction. First, we run ablations on existing techniques for pretraining set development to identify which methods translate to the largest gains in model accuracy on downstream evaluations. Then, we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain. Finally, we show how such attribute information can be used to further refine and improve the quality of a pretraining set. These findings constitute an actionable set of steps that practitioners can use to develop high quality pretraining sets.</li>
<li><strong>摘要：</strong>近期语言模型的出色能力很大程度上归功于它们所训练的数万亿个 token 预训练数据集。然而，模型开发人员未能披露其构建方法，这导致缺乏有关如何开发有效预训练集的公开信息。为了解决这个问题，我们对整个预训练集构建流程进行了首次系统性研究。首先，我们对现有的预训练集开发技术进行消融，以确定哪些方法可以在下游评估中最大程度地提高模型准确率。然后，我们根据毒性、质量、语音类型和领域等属性对最广泛使用的数据源——网络爬虫快照进行分类。最后，我们展示了如何使用此类属性信息进一步完善和提高预训练集的质量。这些发现构成了一套可行的步骤，从业者可以使用它们来开发高质量的预训练集。</li>
</ul>

<h3>Title: Title:
          DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations</h3>
<ul>
<li><strong>Authors: </strong>Luke Yoffe, Alfonso Amayuelas, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>To enhance Large Language Model (LLM) capabilities, multi-agent debates have been introduced, where multiple LLMs discuss solutions to a problem over several rounds of debate. However, LLMs often produce incorrect responses that appear deceptively confident, which can mislead other agents. This is partly because agents do not express their confidence levels during standard debates. To address this, we introduce DebUnc, a multi-agent debate framework that uses uncertainty metrics to assess agent confidence levels. We adapted the LLM attention mechanism to adjust token weights based on confidence levels and also explored using textual prompts to convey confidence. Our evaluations across various benchmarks show that attention-based methods are particularly effective, and that as uncertainty metrics evolve, performance will continue to increase. The code is available at this https URL</li>
<li><strong>摘要：</strong>为了增强大型语言模型 (LLM) 的功能，引入了多智能体辩论，其中多个 LLM 在几轮辩论中讨论问题的解决方案。然而，LLM 通常会产生看似自信的错误答案，这可能会误导其他智能体。部分原因是智能体在标准辩论期间不会表达他们的信心水平。为了解决这个问题，我们引入了 DebUnc，这是一个多智能体辩论框架，它使用不确定性指标来评估智能体的信心水平。我们调整了 LLM 注意力机制，以根据信心水平调整标记权重，并探索使用文本提示来传达信心。我们在各种基准上的评估表明，基于注意力的方法特别有效，并且随着不确定性指标的发展，性能将继续提高。代码可在此 https URL 上找到</li>
</ul>

<h3>Title: Title:
          An Empirical Study of Gendered Stereotypes in Emotional Attributes for Bangla in Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jayanta Sadhu, Maneesha Rani Saha, Rifat Shahriyar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          An Empirical Study of Gendered Stereotypes in Emotional Attributes for Bangla in Multilingual Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The influence of Large Language Models (LLMs) is rapidly growing, automating more jobs over time. Assessing the fairness of LLMs is crucial due to their expanding impact. Studies reveal the reflection of societal norms and biases in LLMs, which creates a risk of propagating societal stereotypes in downstream tasks. Many studies on bias in LLMs focus on gender bias in various NLP applications. However, there's a gap in research on bias in emotional attributes, despite the close societal link between emotion and gender. This gap is even larger for low-resource languages like Bangla. Historically, women are associated with emotions like empathy, fear, and guilt, while men are linked to anger, bravado, and authority. This pattern reflects societal norms in Bangla-speaking regions. We offer the first thorough investigation of gendered emotion attribution in Bangla for both closed and open source LLMs in this work. Our aim is to elucidate the intricate societal relationship between gender and emotion specifically within the context of Bangla. We have been successful in showing the existence of gender bias in the context of emotions in Bangla through analytical methods and also show how emotion attribution changes on the basis of gendered role selection in LLMs. All of our resources including code and data are made publicly available to support future research on Bangla NLP. Warning: This paper contains explicit stereotypical statements that many may find offensive.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的影响力正在迅速增长，随着时间的推移，越来越多的工作实现自动化。由于 LLM 的影响力不断扩大，评估其公平性至关重要。研究表明，LLM 反映了社会规范和偏见，这有可能在下游任务中传播社会刻板印象。许多关于 LLM 偏见的研究都集中在各种 NLP 应用中的性别偏见。然而，尽管情感与性别之间存在密切的社会联系，但在情感属性偏见研究方面仍存在差距。对于孟加拉语等资源匮乏的语言来说，这种差距甚至更大。从历史上看，女性与同情、恐惧和内疚等情绪有关，而男性则与愤怒、虚张声势和权威有关。这种模式反映了孟加拉语地区的社会规范。在这项研究中，我们首次对孟加拉语中闭源和开源 LLM 的性别情感归因进行了彻底调查。我们的目标是阐明孟加拉语背景下的性别与情感之间错综复杂的社会关系。我们已成功通过分析方法证明了孟加拉语中情感背景下的性别偏见的存在，并展示了 LLM 中情感归因如何根据性别角色选择而发生变化。我们所有的资源（包括代码和数据）都是公开的，以支持未来对孟加拉语 NLP 的研究。警告：本文包含许多人可能会觉得冒犯的明确陈词滥调。</li>
</ul>

<h3>Title: Title:
          MUSE: Machine Unlearning Six-Way Evaluation for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah A. Smith, Chiyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MUSE: Machine Unlearning Six-Way Evaluation for Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: this http URL</li>
<li><strong>摘要：</strong>语言模型 (LM) 是在大量文本数据上进行训练的，其中可能包括私人和受版权保护的内容。出于隐私或版权方面的考虑，数据所有者可能会要求从经过训练的模型中删除他们的数据。然而，在现代模型中，准确地取消学习这些数据点（即使用已删除的数据进行重新训练）是难以实现的。这导致了许多近似取消学习算法的开发。对这些算法的有效性的评估传统上范围很窄，无法从模型部署者和数据所有者的角度准确量化算法的成功和实用性。我们通过提出 MUSE 来解决这个问题，MUSE 是一个全面的机器取消学习评估基准，它列举了取消学习模型的六个不同的理想属性：(1) 不逐字记忆，(2) 不记忆知识，(3) 无隐私泄露，(4) 保留不打算删除的数据的效用，(5) 相对于删除请求大小的可扩展性，以及 (6) 对连续取消学习请求的可持续性。使用这些标准，我们对 7B 参数 LM 上的八种流行反学习算法反学习《哈利波特》书籍和新闻文章的有效性进行了基准测试。我们的结果表明，大多数算法可以在不同程度上防止逐字记忆和知识记忆，但只有一种算法不会导致严重的隐私泄露。此外，现有算法无法满足部署者的期望，因为它们通常会降低通用模型效用，也无法持续适应连续的反学习请求或大规模内容删除。我们的研究结果确定了语言模型上现有反学习算法实用性的关键问题，我们发布了基准以促进进一步评估：此 http URL</li>
</ul>

<h3>Title: Title:
          Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons</h3>
<ul>
<li><strong>Authors: </strong>Yongqi Leng, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have demonstrated superior multi-task capabilities, understanding the learning mechanisms behind this is still a challenging problem. In this paper, we attempt to understand such mechanisms from the perspective of neurons. Specifically, we detect task-sensitive neurons in LLMs via gradient attribution on task-specific data. Through extensive deactivation and fine-tuning experiments, we demonstrate that the detected neurons are highly correlated with the given task, which we term as task-specific neurons. With these identified task-specific neurons, we delve into two common problems in multi-task learning and continuous learning: Generalization and Catastrophic Forgetting. We find that the overlap of task-specific neurons is strongly associated with generalization and specialization across tasks. Interestingly, at certain layers of LLMs, there is a high similarity in the parameters of different task-specific neurons, and such similarity is highly correlated with the generalization performance. Inspired by these findings, we propose a neuron-level continuous fine-tuning method that only fine-tunes the current task-specific neurons during continuous learning, and extensive experiments demonstrate the effectiveness of the proposed method. Our study provides insights into the interpretability of LLMs in multi-task learning.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 已经展现出卓越的多任务能力，但理解其背后的学习机制仍然是一个具有挑战性的问题。在本文中，我们尝试从神经元的角度来理解这种机制。具体而言，我们通过对任务特定数据的梯度归因来检测 LLM 中的任务敏感神经元。通过大量的失活和微调实验，我们证明检测到的神经元与给定任务高度相关，我们将其称为任务特定神经元。利用这些已识别的任务特定神经元，我们深入研究多任务学习和持续学习中的两个常见问题：泛化和灾难性遗忘。我们发现任务特定神经元的重叠与跨任务的泛化和特化密切相关。有趣的是，在 LLM 的某些层，不同任务特定神经元的参数具有高度相似性，并且这种相似性与泛化性能高度相关。受这些发现的启发，我们提出了一种神经元级连续微调方法，该方法在连续学习过程中仅对当前任务特定的神经元进行微调，大量实验证明了该方法的有效性。我们的研究为 LLM 在多任务学习中的可解释性提供了见解。</li>
</ul>

<h3>Title: Title:
          Efficient and Accurate Memorable Conversation Model using DPO based on sLLM</h3>
<ul>
<li><strong>Authors: </strong>Youngkyung Seo, Yoonseok Heo, Jun-Seok Koh, Du-Seoung Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Efficient and Accurate Memorable Conversation Model using DPO based on sLLM(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In multi-session dialog system, it is essential to continuously update the memory as the session progresses. Simply accumulating memory can make it difficult to focus on the content of the conversation for inference due to the limited input sentence size. Therefore, efficient and accurate conversation model that is capable of managing memory to reflect the conversation history continuously is necessary. This paper presents a conversation model that efficiently manages memory as sessions progress and incorporates this into the model to reflect the conversation history accurately with 3 methodologies: SFT, DPO and DPO with SFT model. Our model using DPO algorithm shows an improvement about 0.0591 of BERTScore in memory accuracy, and the rate of responses reflecting the memory increased as well. Also, response generation performance enhanced about 4.292 in fluency, 3.935 in coherence, and 2.896 in consistency. This paper describes a training method that yields better performance than models with more than twice the parameter size, even when the model size is smaller. Thus, our model demonstrates efficiency not only in terms of accuracy but also in resource utilization.</li>
<li><strong>摘要：</strong>在多会话对话系统中，随着会话的进行不断更新记忆至关重要。由于输入句子的大小有限，单​​纯地积累记忆会使人们难以专注于对话内容进行推理。因此，需要一种能够管理记忆以持续反映对话历史的高效准确的对话模型。本文提出了一种对话模型，该模型可以在会话进行时有效地管理记忆，并将其纳入模型中，以使用 3 种方法准确反映对话历史：SFT、DPO 和 DPO 与 SFT 模型。我们使用 DPO 算法的模型显示记忆准确度提高了约 0.0591 的 BERTScore，并且反映记忆的响应率也增加了。此外，响应生成性能在流畅度上提高了约 4.292，在连贯性上提高了 3.935，在一致性上提高了 2.896。本文描述了一种训练方法，即使模型大小较小，它也能比参数大小两倍以上的模型产生更好的性能。因此，我们的模型不仅在准确性方面而且在资源利用率方面也表现出了效率。</li>
</ul>

<h3>Title: Title:
          Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation: A Case Study</h3>
<ul>
<li><strong>Authors: </strong>Aniruddha Roy, Pretam Ray, Ayush Maheshwari, Sudeshna Sarkar, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation: A Case Study(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Neural Machine Translation (NMT) remains a formidable challenge, especially when dealing with low-resource languages. Pre-trained sequence-to-sequence (seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive performance in various low-resource NMT tasks. However, their pre-training has been confined to 50 languages, leaving out support for numerous low-resource languages, particularly those spoken in the Indian subcontinent. Expanding mBART-50's language support requires complex pre-training, risking performance decline due to catastrophic forgetting. Considering these expanding challenges, this paper explores a framework that leverages the benefits of a pre-trained language model along with knowledge distillation in a seq2seq architecture to facilitate translation for low-resource languages, including those not covered by mBART-50. The proposed framework employs a multilingual encoder-based seq2seq model as the foundational architecture and subsequently uses complementary knowledge distillation techniques to mitigate the impact of imbalanced training. Our framework is evaluated on three low-resource Indic languages in four Indic-to-Indic directions, yielding significant BLEU-4 and chrF improvements over baselines. Further, we conduct human evaluation to confirm effectiveness of our approach. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>神经机器翻译 (NMT) 仍然是一项艰巨的挑战，尤其是在处理资源匮乏的语言时。预训练的序列到序列 (seq2seq) 多语言模型（例如 mBART-50）在各种资源匮乏的 NMT 任务中表现出色。然而，它们的预训练仅限于 50 种语言，无法支持许多资源匮乏的语言，尤其是印度次大陆使用的语言。扩展 mBART-50 的语言支持需要复杂的预训练，可能会因灾难性遗忘而导致性能下降。考虑到这些日益严峻的挑战，本文探讨了一个框架，该框架利用预训练语言模型的优势以及 seq2seq 架构中的知识提炼来促进资源匮乏的语言（包括 mBART-50 未涵盖的语言）的翻译。所提出的框架采用基于多语言编码器的 seq2seq 模型作为基础架构，随后使用互补的知识提炼技术来减轻不平衡训练的影响。我们的框架在三种资源匮乏的印度语上进行了评估，评估方向为四种印度语到印度语，与基线相比，BLEU-4 和 chrF 得分显著提高。此外，我们还进行了人工评估，以确认我们方法的有效性。我们的代码在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Title:
          LIONs: An Empirically Optimized Approach to Align Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yu, Qingyang Wu, Yu Li, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LIONs: An Empirically Optimized Approach to Align Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring the impact of various design choices throughout the whole training process. We first conduct a rigorous analysis over a three-stage training pipeline consisting of supervised fine-tuning, offline preference learning, and online preference learning. We have found that using techniques like sequence packing, loss masking in SFT, increasing the preference dataset size in DPO, and online DPO training can significantly improve the performance of language models. We then train from Gemma-2b-base and LLama-3-8b-base, and find that our best models exceed the performance of the official instruct models tuned with closed-source data and algorithms. Our code and models can be found at this https URL.</li>
<li><strong>摘要：</strong>对齐是增强语言模型的指令遵循和对话能力的关键步骤。尽管最近有许多研究提出了新的算法、数据集和训练流程，但缺乏全面的研究来衡量整个训练过程中各种设计选择的影响。我们首先对由监督微调、离线偏好学习和在线偏好学习组成的三阶段训练流程进行严格分析。我们发现，使用序列打包、SFT 中的损失掩蔽、增加 DPO 中的偏好数据集大小和在线 DPO 训练等技术可以显著提高语言模型的性能。然后，我们从 Gemma-2b-base 和 LLama-3-8b-base 进行训练，发现我们的最佳模型超过了使用闭源数据和算法调整的官方指令模型的性能。我们的代码和模型可以在这个 https URL 中找到。</li>
</ul>

<h3>Title: Title:
          OffsetBias: Leveraging Debiased Data for Tuning Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, Sanghyuk Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OffsetBias: Leveraging Debiased Data for Tuning Evaluators(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Employing Large Language Models (LLMs) to assess the quality of generated responses, such as prompting instruct-tuned models or fine-tuning judge models, has become a widely adopted evaluation method. It is also known that such evaluators are vulnerable to biases, such as favoring longer responses. While it is important to overcome this problem, the specifics of these biases remain under-explored. In this work, we qualitatively identify six types of biases inherent in various judge models. We propose EvalBiasBench as a meta-evaluation collection of hand-crafted test cases for each bias type. Additionally, we present de-biasing dataset construction methods and the associated preference dataset OffsetBias. Experimental results demonstrate that fine-tuning on our dataset significantly enhances the robustness of judge models against biases and improves performance across most evaluation scenarios. We release our datasets and the fine-tuned judge model to public.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 来评估生成的响应的质量，例如提示指令调整模型或微调判断模型，已成为一种广泛采用的评估方法。众所周知，此类评估者容易受到偏见的影响，例如偏爱较长的响应。虽然克服这个问题很重要，但这些偏见的具体细节仍未得到充分探索。在这项工作中，我们定性地识别了各种判断模型中固有的六种偏见。我们提出 EvalBiasBench 作为针对每种偏见类型的手工制作测试用例的元评估集合。此外，我们提出了去偏见数据集构建方法和相关的偏好数据集 OffsetBias。实验结果表明，对我们的数据集进行微调可显着增强判断模型对偏见的稳健性，并提高大多数评估场景的性能。我们向公众发布了我们的数据集和微调后的判断模型。</li>
</ul>

<h3>Title: Title:
          Combining Knowledge Graphs and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amanda Kau, Xuzeng He, Aishwarya Nambissan, Aland Astudillo, Hui Yin, Amir Aryani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Combining Knowledge Graphs and Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>In recent years, Natural Language Processing (NLP) has played a significant role in various Artificial Intelligence (AI) applications such as chatbots, text generation, and language translation. The emergence of large language models (LLMs) has greatly improved the performance of these applications, showing astonishing results in language understanding and generation. However, they still show some disadvantages, such as hallucinations and lack of domain-specific knowledge, that affect their performance in real-world tasks. These issues can be effectively mitigated by incorporating knowledge graphs (KGs), which organise information in structured formats that capture relationships between entities in a versatile and interpretable fashion. Likewise, the construction and validation of KGs present challenges that LLMs can help resolve. The complementary relationship between LLMs and KGs has led to a trend that combines these technologies to achieve trustworthy results. This work collected 28 papers outlining methods for KG-powered LLMs, LLM-based KGs, and LLM-KG hybrid approaches. We systematically analysed and compared these approaches to provide a comprehensive overview highlighting key trends, innovative techniques, and common challenges. This synthesis will benefit researchers new to the field and those seeking to deepen their understanding of how KGs and LLMs can be effectively combined to enhance AI applications capabilities.</li>
<li><strong>摘要：</strong>近年来，自然语言处理 (NLP) 在聊天机器人、文本生成和语言翻译等各种人工智能 (AI) 应用中发挥了重要作用。大型语言模型 (LLM) 的出现极大地提高了这些应用程序的性能，在语言理解和生成方面取得了惊人的成果。然而，它们仍然表现出一些缺点，例如幻觉和缺乏领域特定知识，这些缺点影响了它们在现实世界任务中的表现。这些问题可以通过结合知识图谱 (KG) 来有效缓解，知识图谱以结构化格式组织信息，以通用和可解释的方式捕获实体之间的关系。同样，KG 的构建和验证也带来了一些挑战，LLM 可以帮助解决这些挑战。LLM 和 KG 之间的互补关系导致了一种趋势，即将这些技术结合起来以实现值得信赖的结果。这项工作收集了 28 篇论文，概述了基于 KG 的 LLM、基于 LLM 的 KG 和 LLM-KG 混合方法的方法。我们系统地分析和比较了这些方法，以提供全面的概述，重点介绍主要趋势、创新技术和常见挑战。这种综合将使该领域的新研究人员和那些寻求加深理解如何有效结合 KG 和 LLM 以增强 AI 应用能力的研究人员受益。</li>
</ul>

<h3>Title: Title:
          FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W. Suchow, Rong Liu, Zhenyu Cui, Denghui Zhang, Zhaozhuo Xu, Koduvayur Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, Qianqian Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce the FinCon, an LLM-based multi-agent framework with CONceptual verbal reinforcement tailored for diverse FINancial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent's behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including single stock trading and portfolio management.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在执行复杂任务方面表现出显著的潜力，并越来越多地用于各种金融应用。然而，高质量的连续金融投资决策仍然具有挑战性。这些任务需要与每个决策的动荡环境进行多次交互，需要足够的智能来最大化回报和管理风险。尽管 LLM 已用于开发超越人类团队并产生令人印象深刻的投资回报的代理系统，但通过及时的经验改进来增强多源信息合成和优化决策结果的机会仍未得到探索。在这里，我们介绍了 FinCon，这是一个基于 LLM 的多代理框架，具有针对各种金融任务量身定制的概念性口头强化。受现实世界中有效的投资公司组织结构的启发，FinCon 采用了经理-分析师沟通层次结构。这种结构允许通过自然语言交互实现同步的跨职能代理协作以实现统一目标，并为每个代理配备比人类更大的记忆容量。此外，FinCon 中的风险控制组件通过偶尔启动自我批评机制来更新系统投资信念，从而提高决策质量。概念化的信念可作为未来代理行为的口头强化，并可选择性地传播到需要知识更新的适当节点。此功能可显著提高性能，同时减少不必要的点对点通信成本。此外，FinCon 在各种金融任务中表现出强大的泛化能力，包括单一股票交易和投资组合管理。</li>
</ul>

<h3>Title: Title:
          Virtual Personas for Language Models via an Anthology of Backstories</h3>
<ul>
<li><strong>Authors: </strong>Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, David M. Chan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Virtual Personas for Language Models via an Anthology of Backstories(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce "Anthology", a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as "backstories." We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center's American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics. Our code and generated backstories are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是从数百万不同作者撰写的大量文本库中训练出来的，反映了人类特征的巨大多样性。虽然这些模型有可能在行为研究中用作人类受试者的近似值，但之前的努力仅限于引导模型响应以匹配单个人类用户。在这项工作中，我们引入了“Anthology”，这是一种通过利用开放式生活叙述（我们称之为“背景故事”）将 LLM 调节到特定虚拟角色的方法。我们表明，我们的方法提高了实验结果的一致性和可靠性，同时确保更好地代表不同的亚群。在皮尤研究中心美国趋势小组 (ATP) 进行的三项具有全国代表性的人类调查中，我们表明 Anthology 在匹配人类受访者的响应分布方面实现了高达 18% 的改进，在一致性指标方面实现了 27% 的改进。我们的代码和生成的背景故事可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Hongfei Huang, Tingting Liang, Xixi Sun, Zikang Jin, Yuyu Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Existing research on learning with noisy labels predominantly focuses on synthetic label noise. Although synthetic noise possesses well-defined structural properties, it often fails to accurately replicate real-world noise patterns. In recent years, there has been a concerted effort to construct generalizable and controllable instance-dependent noise datasets for image classification, significantly advancing the development of noise-robust learning in this area. However, studies on noisy label learning for text classification remain scarce. To better understand label noise in real-world text classification settings, we constructed the benchmark dataset NoisyAG-News through manual annotation. Initially, we analyzed the annotated data to gather observations about real-world noise. We qualitatively and quantitatively demonstrated that real-world noisy labels adhere to instance-dependent patterns. Subsequently, we conducted comprehensive learning experiments on NoisyAG-News and its corresponding synthetic noise datasets using pre-trained language models and noise-handling techniques. Our findings reveal that while pre-trained models are resilient to synthetic noise, they struggle against instance-dependent noise, with samples of varying confusion levels showing inconsistent performance during training and testing. These real-world noise patterns pose new, significant challenges, prompting a reevaluation of noisy label handling methods. We hope that NoisyAG-News will facilitate the development and evaluation of future solutions for learning with noisy labels.</li>
<li><strong>摘要：</strong>现有的关于使用噪声标签进行学习的研究主要集中在合成标签噪声上。尽管合成噪声具有明确的结构特性，但它往往无法准确复制现实世界的噪声模式。近年来，人们齐心协力为图像分类构建了可推广和可控的实例相关噪声数据集，大大推动了该领域抗噪学习的发展。然而，关于文本分类的噪声标签学习的研究仍然很少。为了更好地理解现实世界文本分类环境中的标签噪声，我们通过人工注释构建了基准数据集 NoisyAG-News。最初，我们分析了注释数据以收集有关现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例相关模式。随后，我们使用预训练的语言模型和噪声处理技术对 NoisyAG-News 及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型能够抵御合成噪声，但它们难以抵御与实例相关的噪声，不同混淆程度的样本在训练和测试期间表现出不一致的性能。这些现实世界的噪声模式带来了新的重大挑战，促使人们重新评估噪声标签处理方法。我们希望 NoisyAG-News 能够促进未来噪声标签学习解决方案的开发和评估。</li>
</ul>

<h3>Title: Title:
          A Word Order Synchronization Metric for Evaluating Simultaneous Interpretation and Translation</h3>
<ul>
<li><strong>Authors: </strong>Mana Makinae, Katsuhito Sudoh, Mararu Yamada, Satoshi Nakamura</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Word Order Synchronization Metric for Evaluating Simultaneous Interpretation and Translation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Simultaneous interpretation (SI), the translation of one language to another in real time, starts translation before the original speech has finished. Its evaluation needs to consider both latency and quality. This trade-off is challenging especially for distant word order language pairs such as English and Japanese. To handle this word order gap, interpreters maintain the word order of the source language as much as possible to keep up with original language to minimize its latency while maintaining its quality, whereas in translation reordering happens to keep fluency in the target language. This means outputs synchronized with the source language are desirable based on the real SI situation, and it's a key for further progress in computational SI and simultaneous machine translation (SiMT). In this work, we propose an automatic evaluation metric for SI and SiMT focusing on word order synchronization. Our evaluation metric is based on rank correlation coefficients, leveraging cross-lingual pre-trained language models. Our experimental results on NAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word order synchronization between source and target language.</li>
<li><strong>摘要：</strong>同声传译 (SI) 是指将一种语言实时翻译成另一种语言，在原始语音结束之前就开始翻译。其评估需要考虑延迟和质量。这种权衡具有挑战性，尤其是对于英语和日语等词序相差较大的语言对。为了处理这种词序差距，口译员尽可能保持源语言的词序以跟上原始语言，从而在保持其质量的同时最大限度地减少其延迟，而在翻译中，重新排序恰好可以保持目标语言的流畅性。这意味着根据真实的 SI 情况，与源语言同步的输出是可取的，这是计算 SI 和同步机器翻译 (SiMT) 取得进一步进展的关键。在这项工作中，我们提出了一种针对 SI 和 SiMT 的自动评估指标，重点关注词序同步。我们的评估指标基于等级相关系数，利用跨语言预训练语言模型。我们在 NAIST-SIC-Aligned 和 JNPC 上的实验结果表明，我们的指标在衡量源语言和目标语言之间的词序同步方面是有效的。</li>
</ul>

<h3>Title: Title:
          SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Nan He, Weichen Xiong, Hanwen Liu, Yi Liao, Lei Ding, Kai Zhang, Guohua Tang, Xiao Han, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and neglects the varying degrees of duplication. To address this, we propose a soft deduplication method that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of "data commonness", a metric we introduce to quantify the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的有效性通常会受到其大量预训练数据集中重复数据的阻碍。当前的方法主要侧重于检测和删除重复数据，这有丢失有价值信息的风险，并且忽略了不同程度的重复。为了解决这个问题，我们提出了一种软重复数据删除方法，该方法在保持数据集完整性的同时，有选择地减少具有高共性的数据的采样权重。我们方法的核心是“数据共性”的概念，我们引入该指标来量化重复程度，通过使用 n-gram 模型测量样本的出现概率。实证分析表明，该方法显著提高了训练效率，在所需的训练步骤至少减少 26% 的情况下实现了相当的困惑度分数。此外，在训练时间相同的情况下，它将平均小样本下游准确率提高了 1.77%。重要的是，即使在严格去重的数据集上，这种方法也能持续提高性能，这表明它有可能补充现有方法并成为 LLM 的标准预训练过程。</li>
</ul>

<h3>Title: Title:
          Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules</h3>
<ul>
<li><strong>Authors: </strong>Zhuocheng Gong, Ang Lv, Jian Guan, Junxi Yan, Wei Wu, Huishuai Zhang, Minlie Huang, Dongyan Zhao, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Is it always necessary to compute tokens from shallow to deep layers in Transformers? The continued success of vanilla Transformers and their variants suggests an undoubted "yes". In this work, however, we attempt to break the depth-ordered convention by proposing a novel architecture dubbed mixture-of-modules (MoM), which is motivated by an intuition that any layer, regardless of its position, can be used to compute a token as long as it possesses the needed processing capabilities. The construction of MoM starts from a finite set of modules defined by multi-head attention and feed-forward networks, each distinguished by its unique parameterization. Two routers then iteratively select attention modules and feed-forward modules from the set to process a token. The selection dynamically expands the computation graph in the forward pass of the token, culminating in an assembly of modules. We show that MoM provides not only a unified framework for Transformers and their numerous variants but also a flexible and learnable approach for reducing redundancy in Transformer parameterization. We pre-train various MoMs using OpenWebText. Empirical results demonstrate that MoMs, of different parameter counts, consistently outperform vanilla transformers on both GLUE and XSUM benchmarks. More interestingly, with a fixed parameter budget, MoM-large enables an over 38% increase in depth for computation graphs compared to GPT-2-large, resulting in absolute gains of 1.4 on GLUE and 1 on XSUM. On the other hand, MoM-large also enables an over 60% reduction in depth while involving more modules per layer, yielding a 16% reduction in TFLOPs and a 43% decrease in memory usage compared to GPT-2-large, while maintaining comparable performance.</li>
<li><strong>摘要：</strong>在 Transformers 中，从浅层到深层计算 token 是否总是必要的？普通 Transformers 及其变体的持续成功表明，答案无疑是“是”。然而，在这项工作中，我们试图打破深度排序的惯例，提出一种称为混合模块 (MoM) 的新架构，其动机是直觉，即任何层，无论其位置如何，只要它具有所需的处理能力，都可以用于计算 token。MoM 的构建从由多头注意和前馈网络定义的一组有限模块开始，每个模块都以其独特的参数化为特征。然后，两个路由器从集合中迭代选择注意模块和前馈模块来处理 token。选择会动态扩展 token 前向传递中的计算图，最终形成模块组合。我们表明，MoM 不仅为 Transformers 及其众多变体提供了统一的框架，而且还提供了一种灵活且可学习的方法来减少 Transformer 参数化中的冗余。我们使用 OpenWebText 预训练各种 MoM。实证结果表明，不同参数数量的 MoM 在 GLUE 和 XSUM 基准测试中的表现始终优于 vanilla transformers。更有趣的是，在参数预算固定的情况下，与 GPT-2-large 相比，MoM-large 可将计算图的深度提高 38% 以上，从而在 GLUE 上获得 1.4 的绝对增益，在 XSUM 上获得 1 的绝对增益。另一方面，与 GPT-2-large 相比，MoM-large 还可将深度减少 60% 以上，同时每层涉及更多模块，TFLOP 减少 16%，内存使用量减少 43%，同时保持相当的性能。</li>
</ul>

<h3>Title: Title:
          Using Pretrained Large Language Model with Prompt Engineering to Answer Biomedical Questions</h3>
<ul>
<li><strong>Authors: </strong>Wenxin Zhou, Thuy Hang Ngo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Using Pretrained Large Language Model with Prompt Engineering to Answer Biomedical Questions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Our team participated in the BioASQ 2024 Task12b and Synergy tasks to build a system that can answer biomedical questions by retrieving relevant articles and snippets from the PubMed database and generating exact and ideal answers. We propose a two-level information retrieval and question-answering system based on pre-trained large language models (LLM), focused on LLM prompt engineering and response post-processing. We construct prompts with in-context few-shot examples and utilize post-processing techniques like resampling and malformed response detection. We compare the performance of various pre-trained LLM models on this challenge, including Mixtral, OpenAI GPT and Llama2. Our best-performing system achieved 0.14 MAP score on document retrieval, 0.05 MAP score on snippet retrieval, 0.96 F1 score for yes/no questions, 0.38 MRR score for factoid questions and 0.50 F1 score for list questions in Task 12b.</li>
<li><strong>摘要：</strong>我们的团队参加了 BioASQ 2024 Task12b 和 Synergy 任务，旨在构建一个能够回答生物医学问题的系统，通过从 PubMed 数据库中检索相关文章和片段并生成准确而理想的答案。我们提出了一个基于预训练大型语言模型 (LLM) 的两级信息检索和问答系统，专注于 LLM 提示工程和响应后处理。我们使用上下文中的少量样本示例构建提示，并利用重采样和畸形响应检测等后处理技术。我们比较了各种预训练的 LLM 模型在此挑战中的表现，包括 Mixtral、OpenAI GPT 和 Llama2。我们表现​​最佳的系统在文档检索中获得了 0.14 的 MAP 分数，在片段检索中获得了 0.05 的 MAP 分数，在是/否问题中获得了 0.96 的 F1 分数，在事实问题中获得了 0.38 的 MRR 分数，在任务 12b 中获得了 0.50 的 F1 分数。</li>
</ul>

<h3>Title: Title:
          Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders</h3>
<ul>
<li><strong>Authors: </strong>Jinseok Kim, Jaewon Jung, Sangyeop Kim, Sohyung Park, Sungzoon Cho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite the impressive capabilities of Large Language Models (LLMs) in various tasks, their vulnerability to unsafe prompts remains a critical issue. These prompts can lead LLMs to generate responses on illegal or sensitive topics, posing a significant threat to their safe and ethical use. Existing approaches attempt to address this issue using classification models, but they have several drawbacks. With the increasing complexity of unsafe prompts, similarity search-based techniques that identify specific features of unsafe prompts provide a more robust and effective solution to this evolving problem. This paper investigates the potential of sentence encoders to distinguish safe from unsafe prompts, and the ability to classify various unsafe prompts according to a safety taxonomy. We introduce new pairwise datasets and the Categorical Purity (CP) metric to measure this capability. Our findings reveal both the effectiveness and limitations of existing sentence encoders, proposing directions to improve sentence encoders to operate as more robust safety detectors. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 在各种任务中都具有令人印象深刻的能力，但它们对不安全提示的脆弱性仍然是一个关键问题。这些提示可能导致 LLM 生成有关非法或敏感主题的响应，对其安全和道德使用构成重大威胁。现有方法试图使用分类模型解决此问题，但它们有几个缺点。随着不安全提示的复杂性不断增加，基于相似性搜索的技术可以识别不安全提示的特定特征，为这一不断发展的问题提供更强大、更有效的解决方案。本文探讨了句子编码器区分安全和不安全提示的潜力，以及根据安全分类法对各种不安全提示进行分类的能力。我们引入了新的成对数据集和分类纯度 (CP) 指标来衡量这种能力。我们的研究结果揭示了现有句子编码器的有效性和局限性，并提出了改进句子编码器以作为更强大的安全检测器的方向。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context</h3>
<ul>
<li><strong>Authors: </strong>Victoria R. Li, Yida Chen, Naomi Saphra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.</li>
<li><strong>摘要：</strong>虽然生产中的语言模型的偏见已被广泛记录，但其护栏的偏见却被忽视了。本文研究了用户的上下文信息如何影响 LLM 拒绝执行请求的可能性。通过生成提供意识形态和人口统计信息的用户传记，我们发现 GPT-3.5 的护栏敏感性存在许多偏见。年轻人、女性和亚裔美国人在请求审查或非法信息时更有可能触发拒绝护栏。护栏也是谄媚的，拒绝遵守用户可能不同意政治立场的请求。我们发现某些身份群体和看似无害的信息（例如体育迷）会引起护栏敏感性的变化，类似于直接陈述政治意识形态。对于每个人口统计类别，甚至对于美式足球队粉丝，我们发现 ChatGPT 似乎可以推断出可能的政治意识形态并相应地修改护栏行为。</li>
</ul>

<h3>Title: Title:
          Measuring Sustainability Intention of ESG Fund Disclosure using Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Mayank Singh, Nazia Nafis, Abhijeet Kumar, Mridul Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Measuring Sustainability Intention of ESG Fund Disclosure using Few-Shot Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Global sustainable fund universe encompasses open-end funds and exchange-traded funds (ETF) that, by prospectus or other regulatory filings, claim to focus on Environment, Social and Governance (ESG). Challengingly, the claims can only be confirmed by examining the textual disclosures to check if there is presence of intentionality and ESG focus on its investment strategy. Currently, there is no regulation to enforce sustainability in ESG products space. This paper proposes a unique method and system to classify and score the fund prospectuses in the sustainable universe regarding specificity and transparency of language. We aim to employ few-shot learners to identify specific, ambiguous, and generic sustainable investment-related language. Additionally, we construct a ratio metric to determine language score and rating to rank products and quantify sustainability claims for US sustainable universe. As a by-product, we publish manually annotated quality training dataset on Hugging Face (ESG-Prospectus-Clarity-Category under cc-by-nc-sa-4.0) of more than 1K ESG textual statements. The performance of the few-shot finetuning approach is compared with zero-shot models e.g., Llama-13B, GPT 3.5 Turbo etc. We found that prompting large language models are not accurate for domain specific tasks due to misalignment issues. The few-shot finetuning techniques outperform zero-shot models by large margins of more than absolute ~30% in precision, recall and F1 metrics on completely unseen ESG languages (test set). Overall, the paper attempts to establish a systematic and scalable approach to measure and rate sustainability intention quantitatively for sustainable funds using texts in prospectus. Regulatory bodies, investors, and advisors may utilize the findings of this research to reduce cognitive load in investigating or screening of ESG funds which accurately reflects the ESG intention.</li>
<li><strong>摘要：</strong>全球可持续基金领域包括开放式基金和交易所交易基金 (ETF)，这些基金通过招股说明书或其他监管文件声称专注于环境、社会和治理 (ESG)。具有挑战性的是，只有通过检查文本披露来确认这些说法，以检查是否存在意图以及其投资策略是否关注 ESG。目前，没有法规来强制 ESG 产品领域的可持续性。本文提出了一种独特的方法和系统，对可持续领域的基金招股说明书进行分类和评分，以评估语言的特殊性和透明度。我们旨在使用少样本学习器来识别特定的、模糊的和通用的可持续投资相关语言。此外，我们构建了一个比率指标来确定语言分数和评级，以对产品进行排名并量化美国可持续领域的可持续性声明。作为副产品，我们在 Hugging Face 上发布了手动注释的质量训练数据集 (cc-by-nc-sa-4.0 下的 ESG-Prospectus-Clarity-Category)，其中包含超过 1K 条 ESG 文本声明。将少样本微调方法的性能与零样本模型（例如 Llama-13B、GPT 3.5 Turbo 等）进行了比较。我们发现，由于错位问题，提示大型语言模型对于特定领域的任务并不准确。在完全看不见的 ESG 语言（测试集）上的精确度、召回率和 F1 指标上，少样本微调技术的表现远胜于零样本模型，绝对优势超过约 30%。总体而言，本文试图建立一种系统的、可扩展的方法，使用招股说明书中的文本对可持续基金的可持续发展意图进行定量测量和评估。监管机构、投资者和顾问可以利用本研究的结果来减少调查或筛选准确反映 ESG 意图的 ESG 基金的认知负荷。</li>
</ul>

<h3>Title: Title:
          Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Susanna Paoli, Alba Curry, Dirk Hovy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Emotions play important epistemological and cognitive roles in our lives, revealing our values and guiding our actions. Previous work has shown that LLMs display biases in emotion attribution along gender lines. However, unlike gender, which says little about our values, religion, as a socio-cultural system, prescribes a set of beliefs and values for its followers. Religions, therefore, cultivate certain emotions. Moreover, these rules are explicitly laid out and interpreted by religious leaders. Using emotion attribution, we explore how different religions are represented in LLMs. We find that: Major religions in the US and European countries are represented with more nuance, displaying a more shaded model of their beliefs. Eastern religions like Hinduism and Buddhism are strongly stereotyped. Judaism and Islam are stigmatized -- the models' refusal skyrocket. We ascribe these to cultural bias in LLMs and the scarcity of NLP literature on religion. In the rare instances where religion is discussed, it is often in the context of toxic language, perpetuating the perception of these religions as inherently toxic. This finding underscores the urgent need to address and rectify these biases. Our research underscores the crucial role emotions play in our lives and how our values influence them.</li>
<li><strong>摘要：</strong>情绪在我们的生活中扮演着重要的认识论和认知角色，揭示我们的价值观并指导我们的行为。先前的研究表明，法学硕士在情绪归因方面存在性别偏见。然而，与性别不同，性别对我们的价值观影响不大，而宗教作为一种社会文化体系，为其追随者规定了一套信仰和价值观。因此，宗教会培养某些情绪。此外，这些规则由宗教领袖明确制定和解释。通过情绪归因，我们探索了不同宗教在法学硕士中的体现。我们发现：美国和欧洲国家的主要宗教表现得更加微妙，展现出一种更为隐晦的信仰模式。印度教和佛教等东方宗教被强烈刻板化。犹太教和伊斯兰教被污名化——拒绝接受这些模式的人数急剧上升。我们将其归因于法学硕士中的文化偏见和宗教 NLP 文献的稀缺。在极少数情况下，人们讨论宗教时，往往使用带有恶意的语言，使人们认为这些宗教本质上是有害的。这一发现强调了解决和纠正这些偏见的迫切需要。我们的研究强调了情绪在我们生活中发挥的关键作用，以及我们的价值观如何影响情绪。</li>
</ul>

<h3>Title: Title:
          Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zara Siddique, Liam D. Turner, Luis Espinosa-Anke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to propagate and amplify harmful stereotypes, particularly those that disproportionately affect marginalised communities. To understand the effect of these stereotypes more comprehensively, we introduce GlobalBias, a dataset of 876k sentences incorporating 40 distinct gender-by-ethnicity groups alongside descriptors typically used in bias literature, which enables us to study a broad set of stereotypes from around the world. We use GlobalBias to directly probe a suite of LMs via perplexity, which we use as a proxy to determine how certain stereotypes are represented in the model's internal representations. Following this, we generate character profiles based on given names and evaluate the prevalence of stereotypes in model outputs. We find that the demographic groups associated with various stereotypes remain consistent across model likelihoods and model outputs. Furthermore, larger models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已被证明会传播和放大有害的刻板印象，尤其是那些对边缘化社区影响尤为严重的刻板印象。为了更全面地了解这些刻板印象的影响，我们引入了 GlobalBias，这是一个包含 876k 个句子的数据集，包含 40 个不同的性别和种族群体以及偏见文献中通常使用的描述符，这使我们能够研究来自世界各地的一系列刻板印象。我们使用 GlobalBias 通过困惑度直接探测一组 LM，我们将其用作代理来确定某些刻板印象在模型的内部表示中是如何表现的。随后，我们根据名字生成角色资料，并评估刻板印象在模型输出中的普遍性。我们发现，与各种刻板印象相关的人口统计群体在模型可能性和模型输出中保持一致。此外，更大的模型始终显示更高水平的刻板印象输出，即使明确指示不要这样做。</li>
</ul>

<h3>Title: Title:
          Raply: A profanity-mitigated rap generator</h3>
<ul>
<li><strong>Authors: </strong>Omar Manil Bendali, Samir Ferroum, Ekaterina Kozachenko, Youssef Parviz, Hanna Shcharbakova, Anna Tokareva, Shemair Williams</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Raply: A profanity-mitigated rap generator(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>The task of writing rap is challenging and involves producing complex rhyming schemes, yet meaningful lyrics. In this work, we propose Raply, a fine-tuned GPT-2 model capable of producing meaningful rhyming text in the style of rap. In addition to its rhyming capabilities, the model is able to generate less offensive content. It was achieved through the fine-tuning the model on a new dataset Mitislurs, a profanity-mitigated corpus. We evaluate the output of the model on two criteria: 1) rhyming based on the rhyme density metric; 2) profanity content, using the list of profanities for the English language. To our knowledge, this is the first attempt at profanity mitigation for rap lyrics generation.</li>
<li><strong>摘要：</strong>创作说唱歌曲是一项艰巨的任务，需要创作出押韵复杂但又有意义的歌词。在这项工作中，我们提出了 Raply，这是一个经过微调的 GPT-2 模型，能够以说唱风格创作有意义的押韵文本。除了押韵能力之外，该模型还能够生成较少的冒犯性内容。这是通过在新的数据集 Mitislurs（一个减轻脏话的语料库）上对模型进行微调而实现的。我们根据两个标准评估模型的输出：1) 基于押韵密度度量的押韵；2) 脏话内容，使用英语的脏话列表。据我们所知，这是首次尝试减轻说唱歌词的脏话。</li>
</ul>

<h3>Title: Title:
          Self-Recognition in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tim R. Davidson, Viacheslav Surkov, Veniamin Veselovsky, Giuseppe Russo, Robert West, Caglar Gulcehre</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Self-Recognition in Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A rapidly growing number of applications rely on a small set of closed-source language models (LMs). This dependency might introduce novel security risks if LMs develop self-recognition capabilities. Inspired by human identity verification methods, we propose a novel approach for assessing self-recognition in LMs using model-generated "security questions". Our test can be externally administered to keep track of frontier models as it does not require access to internal model parameters or output probabilities. We use our test to examine self-recognition in ten of the most capable open- and closed-source LMs currently publicly available. Our extensive experiments found no empirical evidence of general or consistent self-recognition in any examined LM. Instead, our results suggest that given a set of alternatives, LMs seek to pick the "best" answer, regardless of its origin. Moreover, we find indications that preferences about which models produce the best answers are consistent across LMs. We additionally uncover novel insights on position bias considerations for LMs in multiple-choice settings.</li>
<li><strong>摘要：</strong>越来越多的应用程序依赖于一小部分闭源语言模型 (LM)。如果 LM 开发出自我识别功能，这种依赖性可能会带来新的安全风险。受人类身份验证方法的启发，我们提出了一种使用模型生成的“安全问题”来评估 LM 中自我识别的新方法。我们的测试可以从外部进行管理，以跟踪前沿模型，因为它不需要访问内部模型参数或输出概率。我们使用我们的测试来检查目前公开可用的十个最强大的开源和闭源 LM 中的自我识别。我们进行了广泛的实验，没有发现任何被检查的 LM 中存在普遍或一致的自我识别的经验证据。相反，我们的结果表明，给定一组替代方案，LM 会寻求选择“最佳”答案，而不管其来源如何。此外，我们发现迹象表明，对哪些模型产生最佳答案的偏好在 LM 之间是一致的。我们还发现了关于多项选择设置中 LM 的位置偏差考虑的新见解。</li>
</ul>

<h3>Title: Title:
          Segment-Based Interactive Machine Translation for Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Angel Navarro, Francisco Casacuberta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Segment-Based Interactive Machine Translation for Pre-trained Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Pre-trained large language models (LLM) are starting to be widely used in many applications. In this work, we explore the use of these models in interactive machine translation (IMT) environments. In particular, we have chosen mBART (multilingual Bidirectional and Auto-Regressive Transformer) and mT5 (multilingual Text-to-Text Transfer Transformer) as the LLMs to perform our experiments. The system generates perfect translations interactively using the feedback provided by the user at each iteration. The Neural Machine Translation (NMT) model generates a preliminary hypothesis with the feedback, and the user validates new correct segments and performs a word correction--repeating the process until the sentence is correctly translated. We compared the performance of mBART, mT5, and a state-of-the-art (SoTA) machine translation model on a benchmark dataset regarding user effort, Word Stroke Ratio (WSR), Key Stroke Ratio (KSR), and Mouse Action Ratio (MAR). The experimental results indicate that mBART performed comparably with SoTA models, suggesting that it is a viable option for this field of IMT. The implications of this finding extend to the development of new machine translation models for interactive environments, as it indicates that some novel pre-trained models exhibit SoTA performance in this domain, highlighting the potential benefits of adapting these models to specific needs.</li>
<li><strong>摘要：</strong>预训练大型语言模型 (LLM) 开始广泛应用于许多应用。在这项工作中，我们探索了这些模型在交互式机器翻译 (IMT) 环境中的使用。具体来说，我们选择了 mBART（多语言双向自回归变换器）和 mT5（多语言文本到文本传输变换器）作为 LLM 来进行实验。系统使用用户在每次迭代中提供的反馈以交互方式生成完美的翻译。神经机器翻译 (NMT) 模型根据反馈生成初步假设，用户验证新的正确句段并执行单词校正——重复该过程，直到句子被正确翻译。我们在基准数据集上比较了 mBART、mT5 和最先进的 (SoTA) 机器翻译模型在用户努力、单词笔划率 (WSR)、按键率 (KSR) 和鼠标动作率 (MAR) 方面的表现。实验结果表明，mBART 的表现与 SoTA 模型相当，表明它是 IMT 这一领域的可行选择。这一发现的意义延伸到开发用于交互式环境的新型机器翻译模型，因为它表明一些新型预训练模型在此领域表现出 SoTA 性能，凸显了根据特定需求调整这些模型的潜在好处。</li>
</ul>

<h3>Title: Title:
          Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>J. Crosbie, E. Shutova</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a few-shot ICL setting. We analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the model's ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present fine-grained evidence for the role that the induction mechanism plays in ICL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已表现出通过情境学习 (ICL) 学习和执行复杂任务的卓越能力。然而，对其内部机制的全面了解仍然缺乏。本文探讨了诱导头在少样本 ICL 设置中的作用。我们分析了两个最先进的模型 Llama-3-8B 和 InternLM2-20B 在抽象模式识别和 NLP 任务上的表现。我们的结果表明，即使是最小程度地切除诱导头，也会导致抽象模式识别任务的 ICL 性能下降高达约 32%，使性能接近随机。对于 NLP 任务，这种切除大大降低了模型从示例中受益的能力，使少样本 ICL 性能接近零样本提示的性能。我们进一步使用注意力敲除来禁用特定的诱导模式，并为诱导机制在 ICL 中的作用提供细粒度证据。</li>
</ul>

<h3>Title: Title:
          Using Large Language Models for Generating Smart Contracts for Health Insurance from Textual Policies</h3>
<ul>
<li><strong>Authors: </strong>Inwon Kang, William Van Woensel, Oshani Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Using Large Language Models for Generating Smart Contracts for Health Insurance from Textual Policies(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We explore using Large Language Models (LLMs) to generate application code that automates health insurance processes from text-based policies. We target blockchain-based smart contracts as they offer immutability, verifiability, scalability, and a trustless setting: any number of parties can use the smart contracts, and they need not have previously established trust relationships with each other. Our methodology generates outputs at increasing levels of technical detail: (1) textual summaries, (2) declarative decision logic, and (3) smart contract code with unit tests. We ascertain LLMs are good at the task (1), and the structured output is useful to validate tasks (2) and (3). Declarative languages (task 2) are often used to formalize healthcare policies, but their execution on blockchain is non-trivial. Hence, task (3) attempts to directly automate the process using smart contracts. To assess the LLM output, we propose completeness, soundness, clarity, syntax, and functioning code as metrics. Our evaluation employs three health insurance policies (scenarios) with increasing difficulty from Medicare's official booklet. Our evaluation uses GPT-3.5 Turbo, GPT-3.5 Turbo 16K, GPT-4, GPT-4 Turbo and CodeLLaMA. Our findings confirm that LLMs perform quite well in generating textual summaries. Although outputs from tasks (2)-(3) are useful starting points, they require human oversight: in multiple cases, even "runnable" code will not yield sound results; the popularity of the target language affects the output quality; and more complex scenarios still seem a bridge too far. Nevertheless, our experiments demonstrate the promise of LLMs for translating textual process descriptions into smart contracts.</li>
<li><strong>摘要：</strong>我们探索使用大型语言模型 (LLM) 生成应用程序代码，以自动化基于文本的政策的健康保险流程。我们的目标是基于区块链的智能合约，因为它们提供不变性、可验证性、可扩展性和无信任设置：任何数量的参与方都可以使用智能合约，并且他们不必事先建立彼此之间的信任关系。我们的方法以不断增加的技术细节级别生成输出：(1) 文本摘要、(2) 声明性决策逻辑和 (3) 带有单元测试的智能合约代码。我们确定 LLM 擅长任务 (1)，结构化输出可用于验证任务 (2) 和 (3)。声明性语言（任务 2）通常用于形式化医疗保健政策，但它们在区块链上的执行并非易事。因此，任务 (3) 尝试使用智能合约直接自动化该过程。为了评估 LLM 输出，我们提出以完整性、健全性、清晰度、语法和功能代码作为指标。我们的评估采用了 Medicare 官方手册中难度逐渐增加的三种健康保险政策（场景）。我们的评估使用了 GPT-3.5 Turbo、GPT-3.5 Turbo 16K、GPT-4、GPT-4 Turbo 和 CodeLLaMA。我们的研究结果证实，LLM 在生成文本摘要方面表现相当出色。尽管任务 (2)-(3) 的输出是有用的起点，但它们需要人工监督：在很多情况下，即使是“可运行”的代码也不会产生合理的结果；目标语言的流行度会影响输出质量；更复杂的场景似乎仍然遥不可及。尽管如此，我们的实验证明了 LLM 在将文本流程描述转化为智能合约方面的前景。</li>
</ul>

<h3>Title: Title:
          Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. Our codebase has been released at \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展为开发高性能自主代理铺平了道路。然而，现有的多代理框架往往难以集成各种功能强大的第三方代理，因为它们依赖于其自身生态系统中定义的代理。它们在模拟分布式环境方面也面临挑战，因为大多数框架仅限于单设备设置。此外，这些框架通常依赖于硬编码的通信管道，限制了它们对动态任务要求的适应性。受互联网概念的启发，我们提出了代理互联网 (IoA)，这是一种新颖的框架，它通过为基于 LLM 的多代理协作提供灵活且可扩展的平台来解决这些限制。IoA 引入了代理集成协议、类似即时消息的架构设计以及用于代理协作和对话流控制的动态机制。通过对一般助理任务、具身 AI 任务和检索增强生成基准进行大量实验，我们证明了 IoA 始终优于最先进的基准，展示了其促进异构代理之间有效协作的能力。IoA 代表着在类似互联网的环境中连接不同代理的一步，代理可以无缝协作以实现更高的智能和能力。我们的代码库已在 \url{此 https URL} 发布。</li>
</ul>

<h3>Title: Title:
          Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</h3>
<ul>
<li><strong>Authors: </strong>Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.</li>
<li><strong>摘要：</strong>当被要求总结文章或回答给定段落的问题时，大型语言模型 (LLM) 可能会对细节产生幻觉，并给出与输入上下文不准确的未经证实的答案。本文介绍了一种检测此类上下文幻觉的简单方法。我们假设上下文幻觉与 LLM 关注所提供上下文中的信息与其自身生成中信息的程度有关。基于这种直觉，我们提出了一个简单的幻觉检测模型，其输入特征由上下文与新生成的标记（对于每个注意力头）上的注意力权重之比给出。我们发现基于这些回溯比率特征的线性分类器与利用 LLM 或基于文本的蕴涵模型的整个隐藏状态的更丰富的检测器一样有效。基于回溯比率的检测器——Lookback Lens——被发现可以跨任务甚至模型进行迁移，允许在 7B 模型上训练的检测器（无需重新训练）应用于更大的 13B 模型。我们进一步应用该检测器来减轻情境幻觉，并发现一种简单的分类器引导解码方法能够减少幻觉的数量，例如在 XSum 摘要任务中减少 9.6%。</li>
</ul>

<h3>Title: Title:
          Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Shaltiel Shmidman, Avi Shmidman, Amir DN Cohen, Moshe Koppel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) in low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and DictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a substantial corpus of approximately 200 billion tokens in both Hebrew and English. Adapting a pre-trained model to a new language involves specialized techniques that differ significantly from training a model from scratch or further training existing models on well-resourced languages such as English. We outline these novel training methodologies, which facilitate effective learning and adaptation to the linguistic properties of Hebrew. Additionally, we fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to enhance its performance on task-specific instructions. To rigorously evaluate our models, we introduce a new benchmark suite for Hebrew LLM evaluation, covering a diverse set of tasks including Question Answering, Sentiment Analysis, Winograd Schema Challenge, Translation, and Summarization. Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.</li>
<li><strong>摘要：</strong>用希伯来语等资源匮乏的语言训练大型语言模型 (LLM) 面临着独特的挑战。在本文中，我们介绍了 DictaLM2.0 和 DictaLM2.0-Instruct，这两个 LLM 源自 Mistral 模型，在希伯来语和英语的约 2000 亿个标记的大量语料库上进行训练。将预训练模型适应新语言需要专门的技术，这些技术与从头开始训练模型或在英语等资源丰富的语言上进一步训练现有模型有很大不同。我们概述了这些新颖的训练方法，它们有助于有效学习和适应希伯来语的语言特性。此外，我们在综合指令数据集上对 DictaLM2.0-Instruct 进行了微调，以提高其在特定于任务的指令上的性能。为了严格评估我们的模型，我们引入了一套新的希伯来语 LLM 评估基准套件，涵盖了一系列不同的任务，包括问答、情感分析、Winograd 模式挑战、翻译和总结。我们的工作不仅解决了在低资源语言中训练 LLM 的复杂性，而且还提出了一个框架，可用于将其他 LLM 适应各种非英语语言，为多语言 NLP 的更广泛领域做出贡献。</li>
</ul>

<h3>Title: Title:
          CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation</h3>
<ul>
<li><strong>Authors: </strong>Tong Chen, Akari Asai, Niloofar Mireshghallah, Sewon Min, James Grimmelmann, Yejin Choi, Hannaneh Hajishirzi, Luke Zettlemoyer, Pang Wei Koh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Evaluating the degree of reproduction of copyright-protected content by language models (LMs) is of significant interest to the AI and legal communities. Although both literal and non-literal similarities are considered by courts when assessing the degree of reproduction, prior research has focused only on literal similarities. To bridge this gap, we introduce CopyBench, a benchmark designed to measure both literal and non-literal copying in LM generations. Using copyrighted fiction books as text sources, we provide automatic evaluation protocols to assess literal and non-literal copying, balanced against the model utility in terms of the ability to recall facts from the copyrighted works and generate fluent completions. We find that, although literal copying is relatively rare, two types of non-literal copying -- event copying and character copying -- occur even in models as small as 7B parameters. Larger models demonstrate significantly more copying, with literal copying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3% to 6.9% when comparing Llama3-8B and 70B models, respectively. We further evaluate the effectiveness of current strategies for mitigating copying and show that (1) training-time alignment can reduce literal copying but may increase non-literal copying, and (2) current inference-time mitigation methods primarily reduce literal but not non-literal copying.</li>
<li><strong>摘要：</strong>评估语言模型 (LM) 对受版权保护内容的复制程度是人工智能和法律界的重大关注点。尽管法院在评估复制程度时会考虑文字和非文字的相似性，但之前的研究仅关注文字的相似性。为了弥补这一差距，我们引入了 CopyBench，这是一个旨在衡量 LM 生成中的文字和非文字复制的基准。使用受版权保护的小说书籍作为文本来源，我们提供自动评估协议来评估文字和非文字复制，并与模型效用在回忆受版权保护作品的事实和生成流畅完成的能力方面进行平衡。我们发现，虽然文字复制相对罕见，但即使在只有 7B 个参数的模型中也会出现两种非文字复制——事件复制和字符复制。较大的模型表现出更多的复制行为，与 Llama3-8B 和 70B 模型相比，文字复制率分别从 0.2% 增加到 10.5%，非文字复制率从 2.3% 增加到 6.9%。我们进一步评估了当前缓解复制策略的有效性，并表明 (1) 训练时间对齐可以减少文字复制，但可能会增加非文字复制，以及 (2) 当前的推理时间缓解方法主要减少文字复制，而不是非文字复制。</li>
</ul>

<h3>Title: Title:
          FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation</h3>
<ul>
<li><strong>Authors: </strong>Liqun Ma, Mingjie Sun, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work presents a Fully BInarized Large Language Model (FBI-LLM), demonstrating for the first time how to train a large-scale binary language model from scratch (not the partial binary or ternary LLM like BitNet b1.58) to match the performance of its full-precision counterparts (e.g., FP16 or BF16) in transformer-based LLMs. It achieves this by employing an autoregressive distillation (AD) loss with maintaining equivalent model dimensions (130M, 1.3B, 7B) and training data volume as regular LLM pretraining, while delivering competitive results in terms of perplexity and task-specific effectiveness. Intriguingly, by analyzing the training trajectory, we find that the pretrained weight is not necessary for training binarized LLMs from scratch. This research encourages a new computational framework and may facilitate the future design of specialized hardware tailored for fully 1-bit LLMs. We make all models, code, and training dataset fully accessible and transparent to support further research (Code: this https URL. Model: this https URL).</li>
<li><strong>摘要：</strong>这项研究提出了一种完全二元化的大型语言模型 (FBI-LLM)，首次展示了如何从头开始训练大规模二元语言模型（而不是像 BitNet b1.58 这样的部分二元或三元 LLM），以匹配基于 Transformer 的 LLM 中全精度对应模型（例如 FP16 或 BF16）的性能。它通过采用自回归蒸馏 (AD) 损失来实现这一点，同时保持与常规 LLM 预训练相同的模型维度（130M、1.3B、7B）和训练数据量，同时在困惑度和任务特定有效性方面提供具有竞争力的结果。有趣的是，通过分析训练轨迹，我们发现预训练权重对于从头开始训练二元化 LLM 不是必需的。这项研究鼓励了一种新的计算框架，并可能促进未来为完全 1 位 LLM 量身定制的专用硬件的设计。我们使所有模型、代码和训练数据集完全可访问和透明，以支持进一步的研究（代码：此 https URL。模型：此 https URL）。</li>
</ul>

<h3>Title: Title:
          AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Cui, Wentao Zhang, Jing Tang, Xudong Tong, Zhenwei Zhang, Amie, Jing Wen, Rongsheng Wang, Pengfei Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The pervasive deployment of Large Language Models-LLMs in various sectors often neglects the nuanced requirements of individuals and small organizations, who benefit more from models precisely tailored to their specific business contexts rather than those with broadly superior general capabilities. This work introduces \textbf{AnyTaskTune}, a novel fine-tuning methodology coined as \textbf{Task-Fine-Tune}, specifically developed to elevate model performance on a diverse array of domain-specific tasks. This method involves a meticulous process to identify and define targeted sub-tasks within a domain, followed by the creation of specialized enhancement datasets for fine-tuning, thereby optimizing task-specific model performance. We conducted comprehensive fine-tuning experiments not only in the legal domain for tasks such as keyword extraction and sentence prediction but across over twenty different sub-tasks derived from the domains of finance, healthcare, law, psychology, consumer services, and human resources. To substantiate our approach and facilitate community engagement, we will open-source these bilingual task datasets. Our findings demonstrate that models fine-tuned using the \textbf{Task-Fine-Tune} methodology not only achieve superior performance on these specific tasks but also significantly outperform models with higher general capabilities in their respective domains. Our work is publicly available at \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各个领域的广泛部署往往忽视了个人和小型组织的细微需求，他们更需要精确定制特定业务环境的模型，而不是具有广泛优越通用能力的模型。这项工作引入了 \textbf{AnyTaskTune}，这是一种新颖的微调方法，被称为 \textbf{Task-Fine-Tune}，专门用于提升模型在各种特定领域任务上的性能。此方法涉及一个细致的过程来识别和定义域内的目标子任务，然后创建专门的增强数据集进行微调，从而优化特定任务的模型性能。我们不仅在法律领域针对关键字提取和句子预测等任务进行了全面的微调实验，还针对来自金融、医疗保健、法律、心理学、消费者服务和人力资源领域的二十多个不同子任务进行了微调实验。为了证实我们的方法并促进社区参与，我们将开源这些双语任务数据集。我们的研究结果表明，使用 \textbf{Task-Fine-Tune} 方法微调的模型不仅在这些特定任务上取得了优异的表现，而且在各自的领域中也明显优于具有更高通用能力的模型。我们的工作成果已在 \url{此 https URL} 上公开发布。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
