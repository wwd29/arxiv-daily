<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-05</h1>
<h3>Title: Security and Privacy Challenges of Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00888">https://arxiv.org/abs/2402.00888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00888">https://arxiv.org/pdf/2402.00888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00888]] Security and Privacy Challenges of Large Language Models: A Survey(https://arxiv.org/abs/2402.00888)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potential defense mechanisms. Additionally, the survey outlines existing research gaps in this domain and highlights future research directions.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）展示了非凡的能力，并对多个领域做出了贡献，例如生成和总结文本、语言翻译和问答。如今，法学硕士正在成为计算机语言处理任务中非常流行的工具，能够分析复杂的语言模式并根据上下文提供相关且适当的响应。这些模型在提供显着优势的同时，也容易受到安全和隐私攻击，例如越狱攻击、数据中毒攻击和个人身份信息（PII）泄漏攻击。这项调查全面审查了法学硕士对培训数据和用户的安全和隐私挑战，以及交通、教育和医疗保健等各个领域基于应用程序的风险。我们评估法学硕士漏洞的程度，调查法学硕士新兴的安全和隐私攻击，并审查潜在的防御机制。此外，该调查概述了该领域现有的研究空白，并强调了未来的研究方向。</li>
</ul>

<h3>Title: Real Sparks of Artificial Intelligence and the Importance of Inner  Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Alex Grzankowski</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00901">https://arxiv.org/abs/2402.00901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00901">https://arxiv.org/pdf/2402.00901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00901]] Real Sparks of Artificial Intelligence and the Importance of Inner  Interpretability(https://arxiv.org/abs/2402.00901)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>The present paper looks at one of the most thorough articles on the intelligence of GPT, research conducted by engineers at Microsoft. Although there is a great deal of value in their work, I will argue that, for familiar philosophical reasons, their methodology, !Blackbox Interpretability"#is wrongheaded. But there is a better way. There is an exciting and emerging discipline of !Inner Interpretability"#(and specifically Mechanistic Interpretability) that aims to uncover the internal activations and weights of models in order to understand what they represent and the algorithms they implement. In my view, a crucial mistake in Black-box Interpretability is the failure to appreciate that how processes are carried out matters when it comes to intelligence and understanding. I can#t pretend to have a full story that provides both necessary and sufficient conditions for being intelligent, but I do think that Inner Interpretability dovetails nicely with plausible philosophical views of what intelligence requires. So the conclusion is modest, but the important point in my view is seeing how to get the research on the right track. Towards the end of the paper, I will show how some of the philosophical concepts can be used to further refine how Inner Interpretability is approached, so the paper helps draw out a profitable, future two-way exchange between Philosophers and Computer Scientists.</li>
<li><strong>摘要：</strong>本文着眼于关于 GPT 智能的最全面的文章之一，即 Microsoft 工程师进行的研究。尽管他们的工作有很大的价值，但我认为，出于熟悉的哲学原因，他们的方法论“黑盒可解释性”#是错误的。但是有更好的方法。有一个令人兴奋的新兴学科！内部可解释性”#（特别是机械可解释性）旨在揭示模型的内部激活和权重，以便理解它们代表什么以及它们实现的算法。在我看来，黑盒可解释性的一个关键错误是未能认识到流程的执行方式对于情报和理解至关重要。我不能假装有一个完整的故事来为智能提供必要和充分的条件，但我确实认为内在可解释性与智能所需的合理的哲学观点非常吻合。所以结论是温和的，但我认为重要的一点是如何让研究走上正轨。在本文的最后，我将展示如何使用一些哲学概念来进一步完善如何接近​​内在可解释性，因此本文有助于在哲学家和计算机科学家之间建立有利可图的、未来的双向交流。</li>
</ul>

<h3>Title: Exploring Spatial Schema Intuitions in Large Language and Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Philipp Wicke, Lennart Wachowiak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00956">https://arxiv.org/abs/2402.00956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00956">https://arxiv.org/pdf/2402.00956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00956]] Exploring Spatial Schema Intuitions in Large Language and Vision Models(https://arxiv.org/abs/2402.00956)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action. Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spatial experiences, and the computations made by large language models. More at https://cisnlp.github.io/Spatial_Schemas/</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）在人工智能研究中无处不在，但 LLM 中的体现问题仍未得到充分探索，这将它们与机器人中的体现系统区分开来，在机器人中，感官知觉直接通知身体动作。我们的调查探索了一个有趣的领域：法学硕士尽管具有非具身性，但能否有效地捕捉人类对语言的基本空间构建块的隐含直觉。我们利用通过早期感觉运动经验发展​​起来的空间认知基础的见解，通过三个心理语言学实验的再现来指导我们的探索。令人惊讶的是，模型输出和人类反应之间出现了相关性，揭示了与具体经验没有实际联系的适应性。显着的区别包括极化的语言模型响应和视觉语言模型的相关性降低。这项研究有助于深入理解语言、空间体验和大型语言模型计算之间的相互作用。更多信息请访问 https://cisnlp.github.io/Spatial_Schemas/</li>
</ul>

<h3>Title: SPARQL Generation with Entity Pre-trained GPT for KG Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Diego Bustamante, Hideaki Takeda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00969">https://arxiv.org/abs/2402.00969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00969">https://arxiv.org/pdf/2402.00969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00969]] SPARQL Generation with Entity Pre-trained GPT for KG Question Answering(https://arxiv.org/abs/2402.00969)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, rag</a></li>
<li><strong>Abstract: </strong>Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.</li>
<li><strong>摘要：</strong>知识图谱的受欢迎程度在过去几年中迅速增长。所有这些知识都可供人们通过互联网上的许多在线数据库查询。不过，如果非程序员用户能够访问他们想知道的任何信息，那将是一个巨大的成就。为了使用自然语言处理工具和通过许多挑战鼓励创造力来解决这项任务，人们付出了很多努力。我们的方法侧重于假设链接到自然语言问题的正确实体，并训练 GPT 模型以从中创建 SPARQL 查询。我们成功地隔离了任务的哪些属性在少量或零样本情况下最难解决，并且我们建议对所有实体（在 CWA 下）进行预训练以提高性能。我们在 3 次测试中获得了 62.703% 的 SPARQL 精确匹配准确率，实体链接挑战的 F1 为 0.809，问答挑战的 F1 为 0.009。</li>
</ul>

<h3>Title: Closure Discovery for Coarse-Grained Partial Differential Equations  using Multi-Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jan-Philipp von Bassewitz, Sebastian Kaltenbach, Petros Koumoutsakos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00972">https://arxiv.org/abs/2402.00972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00972">https://arxiv.org/pdf/2402.00972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00972]] Closure Discovery for Coarse-Grained Partial Differential Equations  using Multi-Agent Reinforcement Learning(https://arxiv.org/abs/2402.00972)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Reliable predictions of critical phenomena, such as weather, wildfires and epidemics are often founded on models described by Partial Differential Equations (PDEs). However, simulations that capture the full range of spatio-temporal scales in such PDEs are often prohibitively expensive. Consequently, coarse-grained simulations that employ heuristics and empirical closure terms are frequently utilized as an alternative. We propose a novel and systematic approach for identifying closures in under-resolved PDEs using Multi-Agent Reinforcement Learning (MARL). The MARL formulation incorporates inductive bias and exploits locality by deploying a central policy represented efficiently by Convolutional Neural Networks (CNN). We demonstrate the capabilities and limitations of MARL through numerical solutions of the advection equation and the Burgers' equation. Our results show accurate predictions for in- and out-of-distribution test cases as well as a significant speedup compared to resolving all scales.</li>
<li><strong>摘要：</strong>对天气、野火和流行病等关键现象的可靠预测通常基于偏微分方程 (PDE) 描述的模型。然而，在此类偏微分方程中捕获全方位时空尺度的模拟通常成本高昂。因此，采用启发式和经验闭包项的粗粒度模拟经常被用作替代方案。我们提出了一种新颖且系统的方法，使用多智能体强化学习（MARL）来识别未解决的偏微分方程中的闭包。 MARL 公式结合了归纳偏差，并通过部署由卷积神经网络 (CNN) 有效表示的中央策略来利用局部性。我们通过平流方程和 Burgers 方程的数值解展示了 MARL 的功能和局限性。我们的结果显示了对分布内和分布外测试用例的准确预测，以及与解析所有尺度相比的显着加速。</li>
</ul>

<h3>Title: Recurrent Transformers with Dynamic Halt</h3>
<ul>
<li><strong>Authors: </strong>Jishnu Ray Chowdhury, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00976">https://arxiv.org/abs/2402.00976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00976">https://arxiv.org/pdf/2402.00976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00976]] Recurrent Transformers with Dynamic Halt(https://arxiv.org/abs/2402.00976)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.</li>
<li><strong>摘要：</strong>在本文中，我们研究了通过循环机制增强 Transformer 的两种主要方法的归纳偏差 - (1) 结合类似于 Universal Transformers 的深度循环的方法； (2) 合并按块时间循环的方法，如时间潜在瓶颈。此外，我们提出并研究了扩展和组合上述方法的新方法 - 例如，我们为 Universal Transformer 提出了一种基于全局均值的动态停止机制，并使用 Universal Transformer 的元素增强了时间潜在瓶颈。我们比较了这些模型，并探讨了它们在远程竞技场 (LRA)、触发器语言建模、ListOps 和逻辑推理等多个诊断任务中的归纳偏差。</li>
</ul>

<h3>Title: Self-Supervised Contrastive Pre-Training for Multivariate Point  Processes</h3>
<ul>
<li><strong>Authors: </strong>Xiao Shou, Dharmashankar Subramanian, Debarun Bhattacharjya, Tian Gao, Kristin P. Bennet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00987">https://arxiv.org/abs/2402.00987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00987">https://arxiv.org/pdf/2402.00987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00987]] Self-Supervised Contrastive Pre-Training for Multivariate Point  Processes(https://arxiv.org/abs/2402.00987)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>Self-supervision is one of the hallmarks of representation learning in the increasingly popular suite of foundation models including large language models such as BERT and GPT-3, but it has not been pursued in the context of multivariate event streams, to the best of our knowledge. We introduce a new paradigm for self-supervised learning for multivariate point processes using a transformer encoder. Specifically, we design a novel pre-training strategy for the encoder where we not only mask random event epochs but also insert randomly sampled "void" epochs where an event does not occur; this differs from the typical discrete-time pretext tasks such as word-masking in BERT but expands the effectiveness of masking to better capture continuous-time dynamics. To improve downstream tasks, we introduce a contrasting module that compares real events to simulated void instances. The pre-trained model can subsequently be fine-tuned on a potentially much smaller event dataset, similar conceptually to the typical transfer of popular pre-trained language models. We demonstrate the effectiveness of our proposed paradigm on the next-event prediction task using synthetic datasets and 3 real applications, observing a relative performance boost of as high as up to 20% compared to state-of-the-art models.</li>
<li><strong>摘要：</strong>在日益流行的基础模型套件（包括 BERT 和 GPT-3 等大型语言模型）中，自监督是表示学习的标志之一，但据我们所知，自监督尚未在多变量事件流的背景下得到实现。知识。我们引入了一种使用变压器编码器进行多变量点过程自监督学习的新范例。具体来说，我们为编码器设计了一种新颖的预训练策略，其中我们不仅屏蔽随机事件时期，而且还插入随机采样的“空”时期，其中事件没有发生；这与典型的离散时间借口任务（例如 BERT 中的字屏蔽）不同，但扩展了屏蔽的有效性，以更好地捕获连续时间动态。为了改进下游任务，我们引入了一个对比模块，用于将真实事件与模拟的空实例进行比较。随后可以在可能小得多的事件数据集上对预训练模型进行微调，这在概念上类似于流行的预训练语言模型的典型迁移。我们使用合成数据集和 3 个真实应用程序证明了我们提出的范例在下一个事件预测任务上的有效性，观察到与最先进的模型相比，相对性能提升高达 20%。</li>
</ul>

<h3>Title: HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent</h3>
<ul>
<li><strong>Authors: </strong>Weijie Xu, Zicheng Huang, Wenxiang Hu, Xi Fang, Rajesh Kumar Cherukuri, Naumaan Nayyar, Lorenzo Malandri, Srinivasan H. Sengamedu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01018">https://arxiv.org/abs/2402.01018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01018">https://arxiv.org/pdf/2402.01018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01018]] HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent(https://arxiv.org/abs/2402.01018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains to evaluate LLM Agent. Our work has the following contributions: (1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展正在重塑多个领域的自然语言处理 (NLP) 任务。它们在人力资源 (HR) 领域的应用仍有扩展空间，并且可能有益于完成一些耗时的任务。休假申请、医疗索赔申请和访问请求等例子值得注意，但它们绝不是唯一的例子。然而，上述发展必须应对构建高质量训练数据集的关键挑战。一方面，大多数对话数据集是为客户而不是员工解决问题。另一方面，收集与人力资源部门的对话可能会引起隐私问题。为了解决这个问题，我们引入了 HR-Multiwoz，这是一个包含 10 个 HR 领域的 550 个对话的完全标记数据集，用于评估 LLM Agent。我们的工作有以下贡献：（1）它是人力资源领域第一个用于 NLP 研究的标记开源对话数据集。 (2) 它提供了数据生成过程以及数据分析和人工评估的详细方法。数据生成管道是可转移的，并且可以轻松地适应其他领域中的标记对话数据生成。 (3) 所提出的数据收集管道主要基于法学硕士，注释方面的人工参与最少，既省时又经济。</li>
</ul>

<h3>Title: Graph-based Clustering for Detecting Semantic Change Across Time and  Languages</h3>
<ul>
<li><strong>Authors: </strong>Xianghe Ma, Michael Strube, Wei Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01025">https://arxiv.org/abs/2402.01025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01025">https://arxiv.org/pdf/2402.01025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01025]] Graph-based Clustering for Detecting Semantic Change Across Time and  Languages(https://arxiv.org/abs/2402.01025)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Despite the predominance of contextualized embeddings in NLP, approaches to detect semantic change relying on these embeddings and clustering methods underperform simpler counterparts based on static word embeddings. This stems from the poor quality of the clustering methods to produce sense clusters -- which struggle to capture word senses, especially those with low frequency. This issue hinders the next step in examining how changes in word senses in one language influence another. To address this issue, we propose a graph-based clustering approach to capture nuanced changes in both high- and low-frequency word senses across time and languages, including the acquisition and loss of these senses over time. Our experimental results show that our approach substantially surpasses previous approaches in the SemEval2020 binary classification task across four languages. Moreover, we showcase the ability of our approach as a versatile visualization tool to detect semantic changes in both intra-language and inter-language setups. We make our code and data publicly available.</li>
<li><strong>摘要：</strong>尽管上下文嵌入在 NLP 中占据主导地位，但依赖这些嵌入和聚类方法来检测语义变化的方法的性能不如基于静态词嵌入的更简单的对应方法。这是由于产生词义簇的聚类方法质量较差，难以捕捉词义，尤其是那些频率较低的词义。这个问题阻碍了下一步研究一种语言的词义变化如何影响另一种语言。为了解决这个问题，我们提出了一种基于图的聚类方法来捕获高频和低频词义在时间和语言上的细微变化，包括随着时间的推移这些含义的获得和丢失。我们的实验结果表明，我们的方法在 SemEval2020 跨四种语言的二元分类任务中大大超越了之前的方法。此外，我们展示了我们的方法作为多功能可视化工具的能力，可以检测语言内和语言间设置中的语义变化。我们公开我们的代码和数据。</li>
</ul>

<h3>Title: Executable Code Actions Elicit Better LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01030">https://arxiv.org/abs/2402.01030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01030">https://arxiv.org/pdf/2402.01030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01030]] Executable Code Actions Elicit Better LLM Agents(https://arxiv.org/abs/2402.01030)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, rag, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）代理能够执行广泛的操作，例如调用工具和控制机器人，在应对现实世界的挑战方面显示出巨大的潜力。通常会提示 LLM 代理通过生成预定义格式的 JSON 或文本来生成操作，这通常受到操作空间（例如预定义工具的范围）和灵活性的限制（例如无法组合多个工具）的限制）。这项工作建议使用可执行的Python代码将LLM代理的操作整合到统一的操作空间（CodeAct）中。 CodeAct 与 Python 解释器集成，可以执行代码操作并动态修改先前的操作或通过多轮交互根据新观察发出新操作。我们对 API-Bank 上 17 个法学硕士的广泛分析和新制定的基准表明，CodeAct 的性能优于广泛使用的替代方案（成功率高出 20%）。 CodeAct 令人鼓舞的性能激励我们构建一个开源 LLM 代理，它通过执行可解释的代码与环境交互，并使用自然语言与用户协作。为此，我们收集了一个指令调优数据集 CodeActInstruct，其中包含使用 CodeAct 的 7k 多轮交互。我们证明，它可以与现有数据一起使用，以改进面向代理的任务中的模型，而不会影响其一般能力。 CodeActAgent 经过 Llama2 和 Mistral 的微调，与 Python 解释器集成，经过专门定制，可使用现有库和自主自调试来执行复杂的任务（例如模型训练）。</li>
</ul>

<h3>Title: Repeat After Me: Transformers are Better than State Space Models at  Copying</h3>
<ul>
<li><strong>Authors: </strong>Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01032">https://arxiv.org/abs/2402.01032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01032">https://arxiv.org/pdf/2402.01032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01032]] Repeat After Me: Transformers are Better than State Space Models at  Copying(https://arxiv.org/abs/2402.01032)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.</li>
<li><strong>摘要：</strong>Transformer 是序列建模的主要架构，但人们对使用不依赖于序列长度的固定大小潜在状态的模型越来越感兴趣，我们将其称为“广义状态空间模型”（GSSM）。在本文中，我们表明，虽然 GSSM 在推理时间效率方面很有前景，但与需要从输入上下文复制的任务上的 Transformer 模型相比，它们是有限的。我们首先对字符串复制这个简单任务进行理论分析，并证明两层 Transformer 可以复制指数长度的字符串，而 GSSM 从根本上受到其固定大小潜在状态的限制。根据经验，我们发现 Transformer 在需要复制上下文的合成任务的效率和泛化方面优于 GSSM。最后，我们评估了预训练的大型语言模型，发现 Transformer 模型在从上下文复制和检索信息方面显着优于状态空间模型。总而言之，这些结果表明 Transformer 和 GSSM 在实际感兴趣的任务上存在根本差距。</li>
</ul>

<h3>Title: Getting the most out of your tokenizer for pre-training and domain  adaptation</h3>
<ul>
<li><strong>Authors: </strong>Gautier Dagan, Gabriele Synnaeve, Baptiste Rozière</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01035">https://arxiv.org/abs/2402.01035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01035">https://arxiv.org/pdf/2402.01035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01035]] Getting the most out of your tokenizer for pre-training and domain  adaptation(https://arxiv.org/abs/2402.01035)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, code</a></li>
<li><strong>Abstract: </strong>Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size.</li>
<li><strong>摘要：</strong>代币化是现代法学硕士中一个未被充分研究且经常被忽视的组成部分。大多数已发表的作品都使用单个标记器进行所有实验，通常是从另一个模型借用的，而不执行消融或分析来优化标记化。此外，在微调基本模型时，分词器通常保持不变。在本文中，我们表明分词器的大小、预分词正则表达式和训练数据可以显着影响模型的生成速度、有效上下文大小、内存使用和下游性能。我们训练专门的字节对编码代码分词器，并对分词器设计对用于代码生成任务（例如 HumanEval 和 MBPP）的 LLM 性能的影响进行广泛的消融，并为分词器超参数选择和在不同的环境中切换分词器提供建议。预训练的法学硕士。我们对从头开始训练的模型和预先训练的模型进行实验，验证它们对广泛用例的适用性。我们发现，当对超过 500 亿个 token 进行微调时，我们可以专门化预训练的 LLM 的 tokenizer，以获得生成速度和有效上下文大小的巨大增益。</li>
</ul>

<h3>Title: Generation, Distillation and Evaluation of Motivational  Interviewing-Style Reflections with a Foundational Language Model</h3>
<ul>
<li><strong>Authors: </strong>Andrew Brown, Jiading Zhu, Mohamed Abdelwahab, Alec Dong, Cindy Wang, Jonathan Rose</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01051">https://arxiv.org/abs/2402.01051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01051">https://arxiv.org/pdf/2402.01051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01051]] Generation, Distillation and Evaluation of Motivational  Interviewing-Style Reflections with a Foundational Language Model(https://arxiv.org/abs/2402.01051)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat, rag</a></li>
<li><strong>Abstract: </strong>Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.</li>
<li><strong>摘要：</strong>大型基础语言模型能够执行高级别的许多任务，但由于其规模和专有所有权而难以在许多应用程序中部署。许多人将有动力将基础模型的特定功能提炼成可以拥有和控制的较小模型。在开发治疗性聊天机器人的过程中，我们希望提炼出一种称为反思性倾听的能力，其中治疗师可以对客户的言语进行反思。这些反思要么重述客户所说的内容，要么将所说的内容与相关的观察、想法或猜测联系起来，鼓励和引导客户继续思考。在本文中，我们提出了一种将基础语言模型（GPT-4）的反射生成提取为更小的模型的方法。我们首先证明，使用零样本提示的 GPT-4 可以以接近 100% 的成功率生成反射，优于之前的所有方法。使用 GPT-4 生成的反射，我们微调 GPT-2 系列的不同大小。 GPT-2-small 模型在保留测试集上取得了 83% 的成功，GPT-2 XL 取得了 90% 的成功。我们还表明，GPT-4 可以帮助完成评估蒸馏模型质量的劳动密集型任务，将其用作零样本分类器。使用三人评审作为指导，分类器实现了 0.66 的 Cohen-Kappa，这是一个重要的评估者间可靠性数字。</li>
</ul>

<h3>Title: Plan-Grounded Large Language Models for Dual Goal Conversational  Settings</h3>
<ul>
<li><strong>Authors: </strong>Diogo Glória-Silva, Rafael Ferreira, Diogo Tavares, David Semedo, João Magalhães</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01053">https://arxiv.org/abs/2402.01053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01053">https://arxiv.org/pdf/2402.01053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01053]] Plan-Grounded Large Language Models for Dual Goal Conversational  Settings(https://arxiv.org/abs/2402.01053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Training Large Language Models (LLMs) to follow user instructions has been shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the LLM's responses to unexpected user behavior. Experiments in controlled settings and with real users show that the best-performing model, which we call PlanLLM, achieves a 2.1x improvement over a strong baseline. Moreover, experiments also show good generalization to unseen domains.</li>
<li><strong>摘要：</strong>事实证明，训练大型语言模型 (LLM) 遵循用户指令可以为 LLM 提供足够的能力，使其在与人类保持一致的同时能够流利地交谈。然而，尚不完全清楚法学硕士如何在混合主动设置中引导基于计划的对话，其中指令在对话的两个方向流动，即法学硕士和用户都向彼此提供指令。在本文中，我们解决了双重目标混合主动对话设置，其中法学硕士不仅将对话建立在任意计划上，而且还寻求满足程序计划和用户指令。然后，法学硕士负责指导用户完成计划，同时适应新情况、回答问题并在需要时激活安全护栏。我们提出了一种新颖的法学硕士，它将对话建立在程序计划的基础上，可以采取对话主动性，并对系统行为实施护栏，同时还改进了法学硕士对意外用户行为的响应。在受控设置和真实用户中进行的实验表明，性能最佳的模型（我们称之为 PlanLLM）比强大的基线提高了 2.1 倍。此外，实验还显示出对未见领域的良好泛化能力。</li>
</ul>

<h3>Title: Expert Proximity as Surrogate Rewards for Single Demonstration Imitation  Learning</h3>
<ul>
<li><strong>Authors: </strong>Chia-Cheng Chiang, Li-Cheng Lan, Wei-Fang Sun, Chien Feng, Cho-Jui Hsieh, Chun-Yi Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01057">https://arxiv.org/abs/2402.01057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01057">https://arxiv.org/pdf/2402.01057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01057]] Expert Proximity as Surrogate Rewards for Single Demonstration Imitation  Learning(https://arxiv.org/abs/2402.01057)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existing IL approaches and achieves expert-level performance in the single-demonstration IL setting across five widely adopted MuJoCo benchmarks as well as the "Adroit Door" environment.</li>
<li><strong>摘要：</strong>在本文中，我们重点关注单演示模仿学习（IL），这是一种适用于现实世界应用的实用方法，在这种情况下，获得大量专家演示的成本很高或不可行。与具有多个演示的典型 IL 设置相比，单演示 IL 涉及代理只能访问一个专家轨迹。我们强调了这种情况下奖励信号稀疏的问题，并建议通过我们提出的基于转移判别器的 IL (TDIL) 方法来缓解这个问题。 TDIL 是一种 IRL 方法，旨在通过引入考虑环境动态的更密集的替代奖励函数来解决奖励稀疏性问题。这种代理奖励函数鼓励代理导航到最接近专家状态的状态。在实践中，TDIL 训练一个转换鉴别器来区分给定环境中的有效和无效转换，以计算替代奖励。实验表明，TDIL 优于现有 IL 方法，并在五个广泛采用的 MuJoCo 基准以及“Adroit Door”环境的单演示 IL 设置中实现了专家级性能。</li>
</ul>

<h3>Title: Evaluation Methodology for Large Language Models for Multilingual  Document Question and Answer</h3>
<ul>
<li><strong>Authors: </strong>Adar Kahana, Jaya Susan Mathew, Said Bleik, Jeremy Reynolds, Oren Elisha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01065">https://arxiv.org/abs/2402.01065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01065">https://arxiv.org/pdf/2402.01065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01065]] Evaluation Methodology for Large Language Models for Multilingual  Document Question and Answer(https://arxiv.org/abs/2402.01065)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models. Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的广泛采用，在本文中我们研究了这些模型的多语言能力。我们的初步结果表明，将母语上下文、问题和答案翻译成高资源语言产生了最佳结果。</li>
</ul>

<h3>Title: FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via  Weight Shift Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Jungwon Seo, Chunming Rong, Minhoe Kim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01070">https://arxiv.org/abs/2402.01070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01070">https://arxiv.org/pdf/2402.01070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01070]] FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via  Weight Shift Aggregation(https://arxiv.org/abs/2402.01070)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) offers a compelling method for training machine learning models with a focus on preserving data privacy. The presence of system heterogeneity and statistical heterogeneity, recognized challenges in FL, arises from the diversity of client hardware, network, and dataset distribution. This diversity can critically affect the training pace and the performance of models. While many studies address either system or statistical heterogeneity by introducing communication-efficient or stable convergence algorithms, addressing these challenges in isolation often leads to compromises due to unaddressed heterogeneity. In response, this paper introduces FedShift, a novel algorithm designed to enhance both the training speed and the models' accuracy in a dual heterogeneity scenario. Our solution can improve client engagement through quantization and mitigate the adverse effects on performance typically associated with quantization by employing a shifting technique. This technique has proven to enhance accuracy by an average of 3.9% in diverse heterogeneity environments.</li>
<li><strong>摘要：</strong>联邦学习 (FL) 提供了一种引人注目的方法来训练机器学习模型，重点是保护数据隐私。系统异构性和统计异构性的存在是 FL 中公认的挑战，它源于客户端硬件、网络和数据集分布的多样性。这种多样性会严重影响模型的训练速度和性能。虽然许多研究通过引入有效通信或稳定的收敛算法来解决系统或统计异质性，但孤立地解决这些挑战往往会因未解决的异质性而导致妥协。为此，本文介绍了 FedShift，这是一种新颖的算法，旨在提高双异构场景中的训练速度和模型的准确性。我们的解决方案可以通过量化提高客户参与度，并通过采用移位技术减轻通常与量化相关的性能不利影响。事实证明，该技术可以在不同的异构环境中将准确率平均提高 3.9%。</li>
</ul>

<h3>Title: Chameleon: Foundation Models for Fairness-aware Multi-modal Data  Augmentation to Enhance Coverage of Minorities</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Erfanian, H. V. Jagadish, Abolfazl Asudeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01071">https://arxiv.org/abs/2402.01071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01071">https://arxiv.org/pdf/2402.01071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01071]] Chameleon: Foundation Models for Fairness-aware Multi-modal Data  Augmentation to Enhance Coverage of Minorities(https://arxiv.org/abs/2402.01071)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>The potential harms of the under-representation of minorities in training data, particularly in multi-modal settings, is a well-recognized concern. While there has been extensive effort in detecting such under-representation, resolution has remained a challenge. With recent advancements in generative AI, large language models and foundation models have emerged as versatile tools across various domains. In this paper, we propose Chameleon, a system that efficiently utilizes these tools to augment a data set with a minimal addition of synthetically generated tuples, in order to enhance the coverage of the under-represented groups. Our system follows a rejection sampling approach to ensure the generated tuples have a high quality and follow the underlying distribution. In order to minimize the rejection chance of the generated tuples, we propose multiple strategies for providing a guide for the foundation model. Our experiment results, in addition to confirming the efficiency of our proposed algorithms, illustrate the effectiveness of our approach, as the unfairness of the model in a downstream task significantly dropped after data repair using Chameleon.</li>
<li><strong>摘要：</strong>培训数据中少数群体代表性不足的潜在危害，特别是在多模式环境中，是一个众所周知的问题。尽管为发现这种代表性不足问题付出了巨大的努力，但解决问题仍然是一个挑战。随着生成式人工智能的最新进展，大型语言模型和基础模型已成为跨各个领域的多功能工具。在本文中，我们提出了 Chameleon，这是一个有效利用这些工具来通过最少添加合成生成的元组来增强数据集的系统，以增强对代表性不足群体的覆盖范围。我们的系统遵循拒绝采样方法，以确保生成的元组具有高质量并遵循基础分布。为了最大限度地减少生成元组的拒绝机会，我们提出了多种策略来为基础模型提供指导。我们的实验结果除了证实了我们提出的算法的效率之外，还说明了我们方法的有效性，因为在使用 Chameleon 进行数据修复后，下游任务中模型的不公平性显着下降。</li>
</ul>

<h3>Title: Reading Between the Tweets: Deciphering Ideological Stances of  Interconnected Mixed-Ideology Communities</h3>
<ul>
<li><strong>Authors: </strong>Zihao He, Ashwin Rao, Siyi Guo, Negar Mokhberian, Kristina Lerman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01091">https://arxiv.org/abs/2402.01091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01091">https://arxiv.org/pdf/2402.01091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01091]] Reading Between the Tweets: Deciphering Ideological Stances of  Interconnected Mixed-Ideology Communities(https://arxiv.org/abs/2402.01091)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advances in NLP have improved our ability to understand the nuanced worldviews of online communities. Existing research focused on probing ideological stances treats liberals and conservatives as separate groups. However, this fails to account for the nuanced views of the organically formed online communities and the connections between them. In this paper, we study discussions of the 2020 U.S. election on Twitter to identify complex interacting communities. Capitalizing on this interconnectedness, we introduce a novel approach that harnesses message passing when finetuning language models (LMs) to probe the nuanced ideologies of these communities. By comparing the responses generated by LMs and real-world survey results, our method shows higher alignment than existing baselines, highlighting the potential of using LMs in revealing complex ideologies within and across interconnected mixed-ideology communities.</li>
<li><strong>摘要：</strong>NLP 的最新进展提高了我们理解在线社区微妙世界观的能力。现有的专注于探究意识形态立场的研究将自由派和保守派视为不同的群体。然而，这未能解释有机形成的在线社区的微妙观点以及它们之间的联系。在本文中，我们研究了 Twitter 上关于 2020 年美国大选的讨论，以确定复杂的互动社区。利用这种相互关联性，我们引入了一种新颖的方法，该方法在微调语言模型（LM）时利用消息传递来探究这些社区的微妙意识形态。通过比较语言模型产生的响应和现实世界的调查结果，我们的方法显示出比现有基线更高的一致性，突显了使用语言模型在揭示相互关联的混合意识形态社区内部和之间的复杂意识形态方面的潜力。</li>
</ul>

<h3>Title: Specialized Language Models with Cheap Inference from Limited Domain  Data</h3>
<ul>
<li><strong>Authors: </strong>David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01093">https://arxiv.org/abs/2402.01093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01093">https://arxiv.org/pdf/2402.01093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01093]] Specialized Language Models with Cheap Inference from Limited Domain  Data(https://arxiv.org/abs/2402.01093)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.</li>
<li><strong>摘要：</strong>大型语言模型已成为一种多功能工具，但应用于缺乏大量推理预算和大型域内训练集的任务具有挑战性。这项工作形式化了这些约束并区分了四个重要变量：预训练预算（用于在目标域已知之前进行训练）、专业化预算（用于在目标域已知之后进行训练）、推理预算和域内训练集尺寸。在这些设置中，我们比较了机器学习文献中的不同方法。受推理成本的限制，我们找到了训练非常大的普通变压器模型的标准实践的更好替代方案。特别是，我们表明超网络和专家混合对于大量预训练预算具有更好的困惑度，而在重要采样数据集上训练的小型模型对于大量专业化预算具有吸引力。</li>
</ul>

<h3>Title: Let's Negotiate! A Survey of Negotiation Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma, Zhuang Li, Lizhen Qu, Zhaleh Semnani Azad, Ingrid Zukerman, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01097">https://arxiv.org/abs/2402.01097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01097">https://arxiv.org/pdf/2402.01097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01097]] Let's Negotiate! A Survey of Negotiation Dialogue Systems(https://arxiv.org/abs/2402.01097)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Negotiation is a crucial ability in human communication. Recently, there has been a resurgent research interest in negotiation dialogue systems, whose goal is to create intelligent agents that can assist people in resolving conflicts or reaching agreements. Although there have been many explorations into negotiation dialogue systems, a systematic review of this task has not been performed to date. We aim to fill this gap by investigating recent studies in the field of negotiation dialogue systems, and covering benchmarks, evaluations and methodologies within the literature. We also discuss potential future directions, including multi-modal, multi-party and cross-cultural negotiation scenarios. Our goal is to provide the community with a systematic overview of negotiation dialogue systems and to inspire future research.</li>
<li><strong>摘要：</strong>谈判是人类沟通中至关重要的能力。最近，人们对谈判对话系统的研究兴趣重新兴起，其目标是创建可以帮助人们解决冲突或达成协议的智能代理。尽管人们对谈判对话系统进行了许多探索，但迄今为止尚未对该任务进行系统回顾。我们的目标是通过调查谈判对话系统领域的最新研究并涵盖文献中的基准、评估和方法来填补这一空白。我们还讨论了未来潜在的方向，包括多模式、多方和跨文化谈判场景。我们的目标是为社区提供谈判对话系统的系统概述并激发未来的研究。</li>
</ul>

<h3>Title: Bayesian Deep Learning for Remaining Useful Life Estimation via Stein  Variational Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Luca Della Libera, Jacopo Andreoli, Davide Dalle Pezze, Mirco Ravanelli, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01098">https://arxiv.org/abs/2402.01098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01098">https://arxiv.org/pdf/2402.01098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01098]] Bayesian Deep Learning for Remaining Useful Life Estimation via Stein  Variational Gradient Descent(https://arxiv.org/abs/2402.01098)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>A crucial task in predictive maintenance is estimating the remaining useful life of physical systems. In the last decade, deep learning has improved considerably upon traditional model-based and statistical approaches in terms of predictive performance. However, in order to optimally plan maintenance operations, it is also important to quantify the uncertainty inherent to the predictions. This issue can be addressed by turning standard frequentist neural networks into Bayesian neural networks, which are naturally capable of providing confidence intervals around the estimates. Several methods exist for training those models. Researchers have focused mostly on parametric variational inference and sampling-based techniques, which notoriously suffer from limited approximation power and large computational burden, respectively. In this work, we use Stein variational gradient descent, a recently proposed algorithm for approximating intractable distributions that overcomes the drawbacks of the aforementioned techniques. In particular, we show through experimental studies on simulated run-to-failure turbofan engine degradation data that Bayesian deep learning models trained via Stein variational gradient descent consistently outperform with respect to convergence speed and predictive performance both the same models trained via parametric variational inference and their frequentist counterparts trained via backpropagation. Furthermore, we propose a method to enhance performance based on the uncertainty information provided by the Bayesian models. We release the source code at https://github.com/lucadellalib/bdl-rul-svgd.</li>
<li><strong>摘要：</strong>预测性维护的一项关键任务是估计物理系统的剩余使用寿命。在过去的十年中，深度学习在预测性能方面比传统的基于模型和统计方法有了很大的改进。然而，为了最佳地规划维护操作，量化预测固有的不确定性也很重要。这个问题可以通过将标准频率论神经网络转变为贝叶斯神经网络来解决，贝叶斯神经网络自然能够提供估计值的置信区间。存在多种训练这些模型的方法。研究人员主要关注参数变分推理和基于采样的技术，众所周知，这些技术分别面临着有限的近似能力和巨大的计算负担。在这项工作中，我们使用 Stein 变分梯度下降，这是一种最近提出的算法，用于逼近难以处理的分布，克服了上述技术的缺点。特别是，我们通过对模拟运行至故障涡轮风扇发动机退化数据的实验研究表明，通过斯坦因变分梯度下降训练的贝叶斯深度学习模型在收敛速度和预测性能方面始终优于通过参数变分推理训练的相同模型和他们的常客同行通过反向传播进行训练。此外，我们提出了一种基于贝叶斯模型提供的不确定性信息来增强性能的方法。我们在 https://github.com/lucadellalib/bdl-rul-svgd 发布源代码。</li>
</ul>

<h3>Title: A Survey for Foundation Models in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, Yiqing Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01105">https://arxiv.org/abs/2402.01105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01105">https://arxiv.org/pdf/2402.01105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01105]] A Survey for Foundation Models in Autonomous Driving(https://arxiv.org/abs/2402.01105)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.</li>
<li><strong>摘要：</strong>基础模型的出现彻底改变了自然语言处理和计算机视觉领域，为其在自动驾驶（AD）中的应用铺平了道路。这项调查对 40 多篇研究论文进行了全面回顾，展示了基础模型在增强 AD 方面的作用。大型语言模型有助于 AD 中的规划和模拟，特别是通过它们在推理、代码生成和翻译方面的熟练程度。与此同时，视觉基础模型越来越适合关键任务，例如 3D 对象检测和跟踪，以及创建用于模拟和测试的真实驾驶场景。多模态基础模型集成了不同的输入，表现出卓越的视觉理解和空间推理，这对于端到端 AD 至关重要。这项调查不仅提供了结构化的分类法，根据 AD 领域内的模式和功能对基础模型进行分类，而且还深入研究了当前研究中采用的方法。它确定了现有基础模型和尖端 AD 方法之间的差距，从而规划了未来的研究方向并提出了弥合这些差距的路线图。</li>
</ul>

<h3>Title: Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and  Human-Centered Solutions</h3>
<ul>
<li><strong>Authors: </strong>Pouya Pezeshkpour, Eser Kandogan, Nikita Bhutani, Sajjadur Rahman, Tom Mitchell, Estevam Hruschka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01108">https://arxiv.org/abs/2402.01108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01108">https://arxiv.org/pdf/2402.01108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01108]] Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and  Human-Centered Solutions(https://arxiv.org/abs/2402.01108)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration of constraints during optimization and establish connections among different components within the system, which also enable a more holistic and comprehensive approach to evaluation. We present a formal definition of reasoning capacity and illustrate its utility in identifying limitations within each component of the system. We then argue how these limitations can be addressed with a self-reflective process wherein human-feedback is used to alleviate shortcomings in reasoning and enhance overall consistency of the system.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中的出色表现为在生产环境中使用它们带来了许多机遇和挑战。为了实际采用法学硕士，多代理系统有望在更大的企业平台环境中增强、集成和编排法学硕士，这些平台使用现有的专有数据和模型来处理复杂的现实世界任务。尽管这些系统取得了巨大成功，但当前的方法依赖于狭窄的、单一焦点的目标来进行优化和评估，往往忽视了现实场景中的潜在限制，包括有限的预算、资源和时间。此外，解释、分析和调试这些系统需要对不同的组件进行相互关联的评估。目前，这种需求对于现有方法来说是不可行的。在本文中，我们引入推理能力的概念作为统一标准，以便在优化过程中整合约束并在系统内不同组件之间建立联系，这也使得能够采用更全面、更全面的评估方法。我们提出了推理能力的正式定义，并说明了其在识别系统每个组件的局限性方面的效用。然后，我们讨论如何通过自我反思过程来解决这些限制，其中使用人类反馈来减轻推理中的缺陷并增强系统的整体一致性。</li>
</ul>

<h3>Title: Vaccine: Perturbation-aware Alignment for Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Tiansheng Huang, Sihao Hu, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01109">https://arxiv.org/abs/2402.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01109">https://arxiv.org/pdf/2402.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01109]] Vaccine: Perturbation-aware Alignment for Large Language Model(https://arxiv.org/abs/2402.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at \url{https://github.com/git-disl/Vaccine}.</li>
<li><strong>摘要：</strong>微调即服务的新范例为大型语言模型 (LLM) 引入了新的攻击面：用户上传的一些有害数据可以轻松欺骗微调以生成对齐损坏的模型。我们进行了实证分析，发现了 \textit{有害嵌入漂移} 现象，显示了对齐破坏效应的可能原因。受我们研究结果的启发，我们提出了 Vaccine，这是一种扰动感知对齐技术，可以减轻用户微​​调的安全风险。 Vaccine 的核心思想是通过在对齐阶段逐步添加精心设计的扰动来产生不变的隐藏嵌入。这使得嵌入能够在微调阶段承受来自未经净化的用户数据的有害扰动。我们对开源主流 LLM（例如 Llama2、Opt、Vicuna）的结果表明，Vaccine 可以提高针对有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。我们的代码可在 \url{https://github.com/git-disl/Vaccine} 获取。</li>
</ul>

<h3>Title: Near-Optimal Reinforcement Learning with Self-Play under Adaptivity  Constraints</h3>
<ul>
<li><strong>Authors: </strong>Dan Qiao, Yu-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01111">https://arxiv.org/abs/2402.01111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01111">https://arxiv.org/pdf/2402.01111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01111]] Near-Optimal Reinforcement Learning with Self-Play under Adaptivity  Constraints(https://arxiv.org/abs/2402.01111)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these are the first line of results towards understanding MARL with low adaptivity.</li>
<li><strong>摘要：</strong>我们研究具有适应性约束的多智能体强化学习（MARL）问题——这是一个由现实应用程序引发的新问题，其中新策略的部署成本高昂，并且必须最大限度地减少策略更新的数量。对于两人零和马尔可夫游戏，我们设计了一种基于（策略）消除的算法，该算法实现了 $\widetilde{O}(\sqrt{H^3 S^2 ABK})$ 的遗憾，而批量复杂度为只有$O(H+\log\log K)$。其中，$S$ 表示状态数，$A,B$ 分别是两个玩家的动作数，$H$ 是地平线，$K$ 是剧集数。此外，我们证明了所有具有 $\widetilde{O}(\sqrt{K} 的算法的批量复杂度下界 $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ )$ 后悔界限，它与我们的对数因子上限相匹配。作为副产品，我们的技术自然延伸到在接近最佳批量复杂性的情况下学习强盗游戏和无奖励 MARL。据我们所知，这些是理解低适应性 MARL 的第一批结果。</li>
</ul>

<h3>Title: Interpretation of Intracardiac Electrograms Through Textual  Representations</h3>
<ul>
<li><strong>Authors: </strong>William Jongwon Han, Diana Gomez, Avi Alok, Chaojing Duan, Michael A. Rosenberg, Douglas Weber, Emerson Liu, Ding Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01115">https://arxiv.org/abs/2402.01115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01115">https://arxiv.org/pdf/2402.01115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01115]] Interpretation of Intracardiac Electrograms Through Textual  Representations(https://arxiv.org/abs/2402.01115)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretability study to provide a multi-perspective intuition of the model's behavior, which could greatly benefit the clinical use.</li>
<li><strong>摘要：</strong>了解心房颤动 (AFib) 的不规则电活动一直是心电图检查中的一个关键挑战。对于严重的 AFib 病例，需要进行导管消融来收集心内电图 (EGM)。 EGM 提供复杂详细的局部心脏电活动，是可解释心脏研究的理想方式。人工智能 (AI) 的最新进展使得一些研究能够利用深度学习框架来解释 AFib 期间的 EGM。此外，语言模型 (LM) 在能够泛化到未知领域（尤其是在医疗保健领域）方面表现出了卓越的性能。在这项研究中，我们首次利用预训练的 LM 通过掩码语言模型来微调 EGM 插值和 AFib 分类。我们将 EGM 制定为文本序列，并与其他表示形式相比，在 AFib 分类上呈现有竞争力的表现。最后，我们提供了全面的可解释性研究，以提供模型行为的多视角直观，这将极大有利于临床使用。</li>
</ul>

<h3>Title: DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Pourreza, Davood Rafiei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01117">https://arxiv.org/abs/2402.01117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01117">https://arxiv.org/pdf/2402.01117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01117]] DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models(https://arxiv.org/abs/2402.01117)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy. Closing the performance gap between small open-source models and large proprietary models is crucial to mitigate this reliance. To this end, we introduce a novel two-stage fine-tuning approach that decomposes the task into two simpler tasks. Through comprehensive evaluation on two large cross-domain datasets and two small LLMs, we show that this approach improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.</li>
<li><strong>摘要：</strong>文本到 SQL 任务的领先模型严重依赖专有的大型语言模型 (LLM)，这引发了对数据隐私的担忧。缩小小型开源模型和大型专有模型之间的性能差距对于减轻这种依赖至关重要。为此，我们引入了一种新颖的两阶段微调方法，将任务分解为两个更简单的任务。通过对两个大型跨域数据集和两个小型法学硕士的综合评估，我们表明这种方法将执行准确性提高了 3% 到 7%，有效地将开源模型的性能与其专有模型的性能保持一致。</li>
</ul>

<h3>Title: PokéLLMon: A Human-Parity Agent for Pokémon Battles with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sihao Hu, Tiansheng Huang, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01118">https://arxiv.org/abs/2402.01118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01118">https://arxiv.org/pdf/2402.01118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01118]] PokéLLMon: A Human-Parity Agent for Pokémon Battles with Large  Language Models(https://arxiv.org/abs/2402.01118)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://github.com/git-disl/PokeLLMon}.</li>
<li><strong>摘要：</strong>我们引入了 \textsc{Pok\'eLLMon}，这是第一个体现 LLM 的代理，它在战术战斗游戏中实现了与人类同等的性能，正如 Pok'emon 战斗中所展示的那样。 \textsc{Pok\'eLLMon} 的设计包含三个关键策略：（i）上下文强化学习，立即消耗从战斗中获得的基于文本的反馈，以迭代地完善策略； (ii) 知识增强生成，检索外部知识以对抗幻觉，并使智能体能够及时、正确地采取行动； (iii) 一致的动作生成，以减轻当智能体面对强大的对手并想要逃避战斗时的\textit{恐慌切换}现象。我们通过与人类的在线对战展示了 \textsc{Pok\'eLLMon} 的类人战斗策略和及时决策，在天梯赛中实现了 49\% 的胜率和 56\% 的胜率在受邀的战斗中。我们的实现和可玩战斗日志可在以下位置找到：\url{https://github.com/git-disl/PokeLLMon}。</li>
</ul>

<h3>Title: Root Cause Analysis In Microservice Using Neural Granger Causal  Discovery</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Ming Lin, Ching Chang, Wei-Yao Wang, Kuang-Da Wang, Wen-Chih Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01140">https://arxiv.org/abs/2402.01140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01140">https://arxiv.org/pdf/2402.01140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01140]] Root Cause Analysis In Microservice Using Neural Granger Causal  Discovery(https://arxiv.org/abs/2402.01140)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>In recent years, microservices have gained widespread adoption in IT operations due to their scalability, maintenance, and flexibility. However, it becomes challenging for site reliability engineers (SREs) to pinpoint the root cause due to the complex relationships in microservices when facing system malfunctions. Previous research employed structured learning methods (e.g., PC-algorithm) to establish causal relationships and derive root causes from causal graphs. Nevertheless, they ignored the temporal order of time series data and failed to leverage the rich information inherent in the temporal relationships. For instance, in cases where there is a sudden spike in CPU utilization, it can lead to an increase in latency for other microservices. However, in this scenario, the anomaly in CPU utilization occurs before the latency increase, rather than simultaneously. As a result, the PC-algorithm fails to capture such characteristics. To address these challenges, we propose RUN, a novel approach for root cause analysis using neural Granger causal discovery with contrastive learning. RUN enhances the backbone encoder by integrating contextual information from time series, and leverages a time series forecasting model to conduct neural Granger causal discovery. In addition, RUN incorporates Pagerank with a personalization vector to efficiently recommend the top-k root causes. Extensive experiments conducted on the synthetic and real-world microservice-based datasets demonstrate that RUN noticeably outperforms the state-of-the-art root cause analysis methods. Moreover, we provide an analysis scenario for the sock-shop case to showcase the practicality and efficacy of RUN in microservice-based applications. Our code is publicly available at https://github.com/zmlin1998/RUN.</li>
<li><strong>摘要：</strong>近年来，微服务凭借其可扩展性、可维护性和灵活性，在 IT 运营中得到了广泛采用。然而，由于微服务中复杂的关系，当面临系统故障时，站点可靠性工程师（SRE）很难查明根本原因。先前的研究采用结构化学习方法（例如 PC 算法）来建立因果关系并从因果图中得出根本原因。然而，他们忽略了时间序列数据的时间顺序，未能利用时间关系中固有的丰富信息。例如，如果 CPU 利用率突然激增，可能会导致其他微服务的延迟增加。但是，在这种情况下，CPU 利用率异常发生在延迟增加之前，而不是同时发生。结果，PC 算法无法捕获此类特征。为了应对这些挑战，我们提出了 RUN，这是一种使用神经格兰杰因果发现和对比学习进行根本原因分析的新方法。 RUN 通过集成时间序列的上下文信息来增强主干编码器，并利用时间序列预测模型进行神经格兰杰因果发现。此外，RUN 将 Pagerank 与个性化向量相结合，以有效地推荐前 k 个根本原因。对基于微服务的合成数据集和现实世界数据集进行的大量实验表明，RUN 的性能明显优于最先进的根本原因分析方法。此外，我们还提供了袜子店案例的分析场景，以展示 RUN 在基于微服务的应用程序中的实用性和有效性。我们的代码可在 https://github.com/zmlin1998/RUN 上公开获取。</li>
</ul>

<h3>Title: Learning Network Representations with Disentangled Graph Auto-Encoder</h3>
<ul>
<li><strong>Authors: </strong>Di Fan, Chuanhou Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01143">https://arxiv.org/abs/2402.01143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01143">https://arxiv.org/pdf/2402.01143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01143]] Learning Network Representations with Disentangled Graph Auto-Encoder(https://arxiv.org/abs/2402.01143)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to each disentangled latent factor. Subsequently, a component-wise flow is applied to each channel to enhance the expressive capabilities of disentangled variational graph auto-encoder. Additionally, we design a factor-wise decoder, considering the characteristics of disentangled representations. In order to further enhance the independence among representations, we introduce independence constraints on mapping channels for different latent factors. Empirical experiments on both synthetic and real-world datasets show the superiority of our proposed method compared to several state-of-the-art baselines.</li>
<li><strong>摘要：</strong>（变分）图自动编码器广泛用于学习图结构数据的表示。然而，现实世界图的形成是一个受潜在因素影响的复杂且异构的过程。现有的编码器从根本上来说是整体的，忽略了潜在因素的纠缠。这不仅使图分析任务的效率降低，而且使理解和解释表示变得更加困难。使用（变分）图自动编码器学习解纠缠的图表示提出了重大挑战，并且在现有文献中很大程度上仍未得到探索。在本文中，我们介绍了解缠结图自动编码器（DGA）和解缠结变分图自动编码器（DVGA），这些方法利用生成模型来学习解缠结表示。具体来说，我们首先设计一个具有多通道消息传递层的解纠缠图卷积网络，作为编码器聚合与每个解纠缠潜在因子相关的信息。随后，将组件级流应用于每个通道，以增强解纠缠变分图自动编码器的表达能力。此外，考虑到解缠结表示的特征，我们设计了一个逐因子解码器。为了进一步增强表示之间的独立性，我们对不同潜在因素的映射通道引入独立性约束。对合成数据集和真实数据集的实证实验表明，与几种最先进的基线相比，我们提出的方法具有优越性。</li>
</ul>

<h3>Title: Limited Memory Online Gradient Descent for Kernelized Pairwise Learning  with Dynamic Averaging</h3>
<ul>
<li><strong>Authors: </strong>Hilal AlQuabeh, William de Vazelhes, Bin Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01146">https://arxiv.org/abs/2402.01146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01146">https://arxiv.org/pdf/2402.01146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01146]] Limited Memory Online Gradient Descent for Kernelized Pairwise Learning  with Dynamic Averaging(https://arxiv.org/abs/2402.01146)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Pairwise learning, an important domain within machine learning, addresses loss functions defined on pairs of training examples, including those in metric learning and AUC maximization. Acknowledging the quadratic growth in computation complexity accompanying pairwise loss as the sample size grows, researchers have turned to online gradient descent (OGD) methods for enhanced scalability. Recently, an OGD algorithm emerged, employing gradient computation involving prior and most recent examples, a step that effectively reduces algorithmic complexity to $O(T)$, with $T$ being the number of received examples. This approach, however, confines itself to linear models while assuming the independence of example arrivals. We introduce a lightweight OGD algorithm that does not require the independence of examples and generalizes to kernel pairwise learning. Our algorithm builds the gradient based on a random example and a moving average representing the past data, which results in a sub-linear regret bound with a complexity of $O(T)$. Furthermore, through the integration of $O(\sqrt{T}{\log{T}})$ random Fourier features, the complexity of kernel calculations is effectively minimized. Several experiments with real-world datasets show that the proposed technique outperforms kernel and linear algorithms in offline and online scenarios.</li>
<li><strong>摘要：</strong>成对学习是机器学习中的一个重要领域，它解决了在训练示例对上定义的损失函数，包括度量学习和 AUC 最大化中的损失函数。研究人员认识到，随着样本量的增加，计算复杂性会呈二次方增长，并伴随着成对损失，因此研究人员转向在线梯度下降（OGD）方法来增强可扩展性。最近，出现了一种 OGD 算法，采用涉及先前和最近示例的梯度计算，这一步骤有效地将算法复杂度降低到 $O(T)$，其中 $T$ 是接收到的示例的数量。然而，这种方法将其自身局限于线性模型，同时假设示例到达的独立性。我们引入了一种轻量级 OGD 算法，该算法不需要示例的独立性，并且可以推广到核成对学习。我们的算法基于随机示例和代表过去数据的移动平均值构建梯度，这会产生复杂度为 $O(T)$ 的亚线性后悔界限。此外，通过整合$O(\sqrt{T}{\log{T}})$随机傅里叶特征，有效地最小化了核计算的复杂度。对现实世界数据集的多次实验表明，所提出的技术在离线和在线场景中优于内核算法和线性算法。</li>
</ul>

<h3>Title: Efficient Reinforcement Learning for Routing Jobs in Heterogeneous  Queueing Systems</h3>
<ul>
<li><strong>Authors: </strong>Neharika Jali, Guannan Qu, Weina Wang, Gauri Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01147">https://arxiv.org/abs/2402.01147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01147">https://arxiv.org/pdf/2402.01147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01147]] Efficient Reinforcement Learning for Routing Jobs in Heterogeneous  Queueing Systems(https://arxiv.org/abs/2402.01147)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We consider the problem of efficiently routing jobs that arrive into a central queue to a system of heterogeneous servers. Unlike homogeneous systems, a threshold policy, that routes jobs to the slow server(s) when the queue length exceeds a certain threshold, is known to be optimal for the one-fast-one-slow two-server system. But an optimal policy for the multi-server system is unknown and non-trivial to find. While Reinforcement Learning (RL) has been recognized to have great potential for learning policies in such cases, our problem has an exponentially large state space size, rendering standard RL inefficient. In this work, we propose ACHQ, an efficient policy gradient based algorithm with a low dimensional soft threshold policy parameterization that leverages the underlying queueing structure. We provide stationary-point convergence guarantees for the general case and despite the low-dimensional parameterization prove that ACHQ converges to an approximate global optimum for the special case of two servers. Simulations demonstrate an improvement in expected response time of up to ~30% over the greedy policy that routes to the fastest available server.</li>
<li><strong>摘要：</strong>我们考虑将到达中央队列的作业有效路由到异构服务器系统的问题。与同类系统不同，当队列长度超过特定阈值时将作业路由到慢速服务器的阈值策略对于一快一慢的双服务器系统来说是最佳的。但多服务器系统的最佳策略是未知的，并且很难找到。虽然强化学习 (RL) 已被认为在此类情况下具有学习策略的巨大潜力，但我们的问题具有指数级大的状态空间大小，导致标准 RL 效率低下。在这项工作中，我们提出了 ACHQ，一种基于策略梯度的高效算法，具有低维软阈值策略参数化，利用底层排队结构。我们为一般情况提供驻点收敛保证，尽管低维参数化证明 ACHQ 对于两个服务器的特殊情况收敛到近似全局最优。模拟表明，与路由到最快可用服务器的贪婪策略相比，预期响应时间提高了约 30%。</li>
</ul>

<h3>Title: AccentFold: A Journey through African Accents for Zero-Shot ASR  Adaptation to Target Accents</h3>
<ul>
<li><strong>Authors: </strong>Abraham Toluwase Owodunni, Aditya Yadavalli, Chris Chinenye Emezue, Tobi Olatunji, Clinton C Mbataku</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01152">https://arxiv.org/abs/2402.01152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01152">https://arxiv.org/pdf/2402.01152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01152]] AccentFold: A Journey through African Accents for Zero-Shot ASR  Adaptation to Target Accents(https://arxiv.org/abs/2402.01152)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose \textit{AccentFold}, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness of AccentFold by showing that, for out-of-distribution (OOD) accents, sampling accent subsets for training based on AccentFold information outperforms strong baselines a relative WER improvement of 4.6%. AccentFold presents a promising approach for improving ASR performance on accented speech, particularly in the context of African accents, where data scarcity and budget constraints pose significant challenges. Our findings emphasize the potential of leveraging linguistic relationships to improve zero-shot ASR adaptation to target accents.</li>
<li><strong>摘要：</strong>尽管语音识别取得了进步，但带口音的语音仍然具有挑战性。虽然以前的方法侧重于建模技术或创建带口音的语音数据集，但由于其纯粹的多样性和相关的预算限制，为多种口音收集足够的数据，特别是在非洲背景下，仍然不切实际。为了解决这些挑战，我们提出了 \textit{AccentFold}，一种利用学习的口音嵌入之间的空间关系来改进下游自动语音识别（ASR）的方法。我们对代表 100 多种非洲口音的语音嵌入进行探索性分析，揭示了有趣的空间口音关系，突出了地理和谱系的相似性，捕获了一致的语音和形态规律，所有这些都是从语音中凭经验学习的。此外，我们还发现了民族语言者以前未曾描述过的口音关系。通过实证评估，我们证明了 AccentFold 的有效性，表明对于分布外 (OOD) 口音，基于 AccentFold 信息采样口音子集进行训练优于强基线，相对 WER 提高了 4.6%。 AccentFold 提出了一种很有前途的方法，可以提高口音语音的 ASR 性能，特别是在非洲口音的背景下，数据稀缺和预算限制构成了重大挑战。我们的研究结果强调了利用语言关系来改善零样本 ASR 对目标口音的适应的潜力。</li>
</ul>

<h3>Title: CABINET: Content Relevance based Noise Reduction for Table Question  Answering</h3>
<ul>
<li><strong>Authors: </strong>Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumita Bhatia, Yaman Kumar, Balaji Krishnamurthy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01155">https://arxiv.org/abs/2402.01155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01155">https://arxiv.org/pdf/2402.01155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01155]] CABINET: Content Relevance based Noise Reduction for Table Question  Answering(https://arxiv.org/abs/2402.01155)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的表格理解能力已经通过表格问答（QA）任务得到了广泛的研究。通常，整个表中只有一小部分与得出给定问题的答案相关。不相关的部分充当噪音并分散注意力，由于法学硕士容易受到噪音的影响，导致表现不佳。为了缓解这一问题，我们提出了 CABINET（基于内容相关性的表格问答降噪）——一个框架，使法学硕士能够通过抑制无关信息来专注于相关的表格数据。 CABINET 包含一个无监督相关性评分器 (URS)，经过 QA LLM 的差异化训练，在将表内容输入到问答 LLM (QA LLM) 之前，根据其与输入问题的相关性对表格内容进行权衡。为了进一步帮助相关性评分器，CABINET 采用了一个弱监督模块，该模块生成一个解析语句，描述与问题相关的行和列的标准，并突出显示相应表格单元格的内容。 CABINET 显着优于各种表格 LLM 基线以及基于 GPT3 的上下文学习方法，对噪声更加鲁棒，在不同大小的表上保持优异的性能，并在 WikiTQ、FeTaQA 和 WikiSQL 数据集上建立新的 SoTA 性能。我们在 https://github.com/Sohanpatnaik106/CABINET_QA 发布了我们的代码和数据集。</li>
</ul>

<h3>Title: LLM-Detector: Improving AI-Generated Chinese Text Detection with  Open-Source LLM Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Rongsheng Wang, Haoming Chen, Ruizhe Zhou, Han Ma, Yaofei Duan, Yanlan Kang, Songhua Yang, Baoyu Fan, Tao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01158">https://arxiv.org/abs/2402.01158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01158">https://arxiv.org/pdf/2402.01158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01158]] LLM-Detector: Improving AI-Generated Chinese Text Detection with  Open-Source LLM Instruction Tuning(https://arxiv.org/abs/2402.01158)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>ChatGPT and other general large language models (LLMs) have achieved remarkable success, but they have also raised concerns about the misuse of AI-generated texts. Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance. In this paper, we first collected Chinese text responses generated by human experts and 9 types of LLMs, for which to multiple domains questions, and further created a dataset that mixed human-written sentences and sentences polished by LLMs. We then proposed LLM-Detector, a novel method for both document-level and sentence-level text detection through Instruction Tuning of LLMs. Our method leverages the wealth of knowledge LLMs acquire during pre-training, enabling them to detect the text they generate. Instruction tuning aligns the model's responses with the user's expected text detection tasks. Experimental results show that previous methods struggle with sentence-level AI-generated text detection and OOD detection. In contrast, our proposed method not only significantly outperforms baseline methods in both sentence-level and document-level text detection but also demonstrates strong generalization capabilities. Furthermore, since LLM-Detector is trained based on open-source LLMs, it is easy to customize for deployment.</li>
<li><strong>摘要：</strong>ChatGPT 和其他通用大语言模型（LLM）取得了显着的成功，但它们也引起了人们对人工智能生成文本的滥用的担忧。现有的人工智能生成的文本检测模型，例如基于 BERT 和 RoBERTa 的模型，很容易出现域内过度拟合，导致域外（OOD）检测性能较差。在本文中，我们首先收集了人类专家和 9 种类型的法学硕士生成的中文文本回答，针对多个领域的问题，并进一步创建了一个混合人类编写的句子和法学硕士修饰的句子的数据集。然后，我们提出了 LLM-Detector，这是一种通过 LLM 指令调整进行文档级和句子级文本检测的新颖方法。我们的方法利用了法学硕士在预训练期间获得的丰富知识，使他们能够检测他们生成的文本。指令调整使模型的响应与用户预期的文本检测任务保持一致。实验结果表明，以前的方法在句子级人工智能生成的文本检测和 OOD 检测方面遇到了困难。相比之下，我们提出的方法不仅在句子级和文档级文本检测方面显着优于基线方法，而且还表现出强大的泛化能力。此外，由于LLM-Detector是基于开源LLM进行训练的，因此很容易定制部署。</li>
</ul>

<h3>Title: Efficient Prompt Caching via Embedding Similarity</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Zhu, Banghua Zhu, Jiantao Jiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01173">https://arxiv.org/abs/2402.01173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01173">https://arxiv.org/pdf/2402.01173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01173]] Efficient Prompt Caching via Embedding Similarity(https://arxiv.org/abs/2402.01173)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empirically, we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51. We then fine-tune the above embedding model, which significantly improves the AUC of caching prediction from 0.51 to 0.81. We also conduct simulations demonstrating that our trained models achieve better caching efficiency than the previous embedding model.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在众多自然语言处理 (NLP) 任务中取得了巨大成功。然而，它在推理过程中面临着大量资源消耗的挑战。在本文中，我们的目标是通过提示缓存来提高LLM的推理效率，即，如果当前提示可以通过前一个提示的相同响应来回答，则可以直接利用前一个响应而无需调用LLM。具体来说，我们通过嵌入相似度关注单轮问答任务提示缓存的预测准确性。现有的提示嵌入大多关注两个提示在语义上是否相似，这并不一定等同于相同的响应是否可以回答它们。因此，我们提出了一种基于蒸馏的方法来微调现有的嵌入，以实现更好的缓存预测。理论上，我们为我们的方法在不同类型的损失函数下的收敛提供有限样本保证。根据经验，我们根据 Kwiatkowski 等人仔细构建了一个硬数据集。 (2019)，现有嵌入模型 (Wang et al., 2022) 的 AUC 仅为 0.51。然后我们对上述嵌入模型进行微调，这将缓存预测的 AUC 从 0.51 显着提高到 0.81。我们还进行了模拟，证明我们训练的模型比以前的嵌入模型实现了更好的缓存效率。</li>
</ul>

<h3>Title: Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing  External Corpus</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01176">https://arxiv.org/abs/2402.01176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01176">https://arxiv.org/pdf/2402.01176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01176]] Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing  External Corpus(https://arxiv.org/abs/2402.01176)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) has showcased their efficacy across various domains, yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources. To improve factual accuracy of language models, retrieval-augmented generation (RAG) has emerged as a popular solution. However, traditional retrieval modules often rely on large-scale document indexes, which can be disconnected from generative tasks. Through generative retrieval (GR) approach, language models can achieve superior retrieval performance by directly generating relevant document identifiers (DocIDs). However, the relationship between GR and downstream tasks, as well as the potential of LLMs in GR, remains unexplored. In this paper, we present a unified language model that utilizes external corpus to handle various knowledge-intensive tasks by seamlessly integrating generative retrieval, closed-book generation, and RAG. In order to achieve effective retrieval and generation through a unified continuous decoding process, we introduce the following mechanisms: (1) a ranking-oriented DocID decoding strategy, which improves ranking ability by directly learning from a DocID ranking list; (2) a continuous generation strategy to facilitate effective and efficient RAG; (3) well-designed auxiliary DocID understanding tasks to enhance the model's comprehension of DocIDs and their relevance to downstream tasks. Our approach is evaluated on the widely used KILT benchmark using two variants of backbone models: an encoder-decoder T5 model and a decoder-only LLM, Llama2. Experimental results showcase the superior performance of our models in both retrieval and downstream knowledge-intensive tasks.</li>
<li><strong>摘要：</strong>大语言模型（LLM）的出现展示了它们在各个领域的功效，但它们经常产生幻觉，特别是在需要外部知识源的知识密集型任务中。为了提高语言模型的事实准确性，检索增强生成（RAG）已成为一种流行的解决方案。然而，传统的检索模块通常依赖于大规模文档索引，这可能与生成任务脱节。通过生成检索（GR）方法，语言模型可以通过直接生成相关文档标识符（DocID）来实现卓越的检索性能。然而，GR 与下游任务之间的关系以及 LLM 在 GR 中的潜力仍有待探索。在本文中，我们提出了一种统一的语言模型，通过无缝集成生成检索、闭卷生成和 RAG，利用外部语料库来处理各种知识密集型任务。为了通过统一的连续解码过程实现有效的检索和生成，我们引入了以下机制：（1）面向排名的DocID解码策略，通过直接从DocID排名列表中学习来提高排名能力； (2) 持续生成策略以促进有效且高效的 RAG； (3)精心设计的辅助DocID理解任务，以增强模型对DocID的理解及其与下游任务的相关性。我们的方法在广泛使用的 KILT 基准上使用骨干模型的两种变体进行评估：编码器-解码器 T5 模型和仅解码器的 LLM，Llama2。实验结果展示了我们的模型在检索和下游知识密集型任务中的卓越性能。</li>
</ul>

<h3>Title: In-Context Learning for Few-Shot Nested Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Meishan Zhang, Bin Wang, Hao Fei, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01182">https://arxiv.org/abs/2402.01182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01182">https://arxiv.org/pdf/2402.01182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01182]] In-Context Learning for Few-Shot Nested Named Entity Recognition(https://arxiv.org/abs/2402.01182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In nested Named entity recognition (NER), entities are nested with each other, and thus requiring more data annotations to address. This leads to the development of few-shot nested NER, where the prevalence of pretrained language models with in-context learning (ICL) offers promising solutions. In this work, we introduce an effective and innovative ICL framework for the setting of few-shot nested NER. We improve the ICL prompt by devising a novel example demonstration selection mechanism, EnDe retriever. In EnDe retriever, we employ contrastive learning to perform three types of representation learning, in terms of semantic similarity, boundary similarity, and label similarity, to generate high-quality demonstration examples. Extensive experiments over three nested NER and four flat NER datasets demonstrate the efficacy of our system.</li>
<li><strong>摘要：</strong>在嵌套命名实体识别（NER）中，实体彼此嵌套，因此需要更多的数据注释来解决。这导致了少样本嵌套 NER 的发展，其中带有上下文学习 (ICL) 的预训练语言模型的流行提供了有前途的解决方案。在这项工作中，我们引入了一种有效且创新的 ICL 框架，用于设置少样本嵌套 NER。我们通过设计一种新颖的示例演示选择机制 EnDe 检索器来改进 ICL 提示。在EnDe检索器中，我们采用对比学习来执行三种类型的表示学习，即语义相似性、边界相似性和标签相似性，以生成高质量的演示示例。对三个嵌套 NER 和四个平面 NER 数据集的广泛实验证明了我们系统的有效性。</li>
</ul>

<h3>Title: Few-Shot Class-Incremental Learning with Prior Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Jiang, Duo Li, Menghan Hu, Guangtao Zhai, Xiaokang Yang, Xiao-Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01201">https://arxiv.org/abs/2402.01201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01201">https://arxiv.org/pdf/2402.01201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01201]] Few-Shot Class-Incremental Learning with Prior Knowledge(https://arxiv.org/abs/2402.01201)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>To tackle the issues of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL), previous work has primarily concentrated on preserving the memory of old knowledge during the incremental phase. The role of pre-trained model in shaping the effectiveness of incremental learning is frequently underestimated in these studies. Therefore, to enhance the generalization ability of the pre-trained model, we propose Learning with Prior Knowledge (LwPK) by introducing nearly free prior knowledge from a few unlabeled data of subsequent incremental classes. We cluster unlabeled incremental class samples to produce pseudo-labels, then jointly train these with labeled base class samples, effectively allocating embedding space for both old and new class data. Experimental results indicate that LwPK effectively enhances the model resilience against catastrophic forgetting, with theoretical analysis based on empirical risk minimization and class distance measurement corroborating its operational principles. The source code of LwPK is publicly available at: \url{https://github.com/StevenJ308/LwPK}.</li>
<li><strong>摘要：</strong>为了解决小样本类增量学习（FSCIL）中的灾难性遗忘和过度拟合问题，之前的工作主要集中在增量阶段保留旧知识的记忆。在这些研究中，预训练模型在塑造增量学习有效性方面的作用经常被低估。因此，为了增强预训练模型的泛化能力，我们提出了先验知识学习（LwPK），通过从后续增量类的一些未标记数据中引入几乎免费的先验知识。我们对未标记的增量类样本进行聚类以产生伪标签，然后与标记的基类样本联合训练它们，从而有效地为新旧类数据分配嵌入空间。实验结果表明，LwPK有效增强了模型对抗灾难性遗忘的弹性，基于经验风险最小化和类距离测量的理论分析证实了其运行原理。 LwPK 的源代码公开在：\url{https://github.com/StevenJ308/LwPK}。</li>
</ul>

<h3>Title: Structured World Modeling via Semantic Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Yi-Fu Wu, Minseung Lee, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01203">https://arxiv.org/abs/2402.01203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01203">https://arxiv.org/pdf/2402.01203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01203]] Structured World Modeling via Semantic Vector Quantization(https://arxiv.org/abs/2402.01203)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Neural discrete representations are crucial components of modern neural networks. However, their main limitation is that the primary strategies such as VQ-VAE can only provide representations at the patch level. Therefore, one of the main goals of representation learning, acquiring structured, semantic, and compositional abstractions such as the color and shape of an object, remains elusive. In this paper, we present the first approach to semantic neural discrete representation learning. The proposed model, called Semantic Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in unsupervised object-centric learning to address this limitation. Specifically, we observe that a simple approach quantizing at the object level poses a significant challenge and propose constructing scene representations hierarchically, from low-level discrete concept schemas to object representations. Additionally, we suggest a novel method for structured semantic world modeling by training a prior over these representations, enabling the ability to generate images by sampling the semantic properties of the objects in the scene. In experiments on various 2D and 3D object-centric datasets, we find that our model achieves superior generation performance compared to non-semantic vector quantization methods such as VQ-VAE and previous object-centric generative models. Furthermore, we find that the semantic discrete representations can solve downstream scene understanding tasks that require reasoning about the properties of different objects in the scene.</li>
<li><strong>摘要：</strong>神经离散表示是现代神经网络的重要组成部分。然而，它们的主要限制是 VQ-VAE 等主要策略只能提供补丁级别的表示。因此，表示学习的主要目标之一，即获取结构化、语义和组合抽象，例如物体的颜色和形状，仍然难以实现。在本文中，我们提出了第一种语义神经离散表示学习方法。所提出的模型称为语义向量量化变分自动编码器（SVQ），利用无监督的以对象为中心的学习的最新进展来解决这一限制。具体来说，我们观察到在对象级别量化的简单方法提出了重大挑战，并提出从低级离散概念模式到对象表示分层构建场景表示。此外，我们提出了一种通过训练这些表示的先验来进行结构化语义世界建模的新方法，从而能够通过对场景中对象的语义属性进行采样来生成图像。在各种 2D 和 3D 以对象为中心的数据集上的实验中，我们发现与 VQ-VAE 等非语义矢量量化方法和以前的以对象为中心的生成模型相比，我们的模型实现了卓越的生成性能。此外，我们发现语义离散表示可以解决需要推理场景中不同对象属性的下游场景理解任务。</li>
</ul>

<h3>Title: A Survey on Self-Supervised Learning for Non-Sequential Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Wei-Yao Wang, Wei-Wei Du, Derek Xu, Wei Wang, Wen-Chih Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01204">https://arxiv.org/abs/2402.01204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01204">https://arxiv.org/pdf/2402.01204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01204]] A Survey on Self-Supervised Learning for Non-Sequential Tabular Data(https://arxiv.org/abs/2402.01204)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to discuss the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain and improving the foundations for implicit tabular data.</li>
<li><strong>摘要：</strong>自监督学习 (SSL) 已被纳入各个领域的许多最先进的模型中，其中 SSL 基于未标记的数据集定义了借口任务，以学习上下文化和稳健的表示。最近，SSL 成为探索表格数据领域表示学习能力的新趋势，由于没有学习描述性表示的明确关系，这更具挑战性。本次调查旨在系统回顾和总结SSL用于非序列表格数据（SSL4NS-TD）的最新进展和挑战。我们首先提出 NS-TD 的正式定义并阐明其与相关研究的相关性。然后，这些方法被分为三组——预测学习、对比学习和混合学习，以及每个方向内代表性方法的动机和优势。在此基础上，提出了SSL4NS-TD的应用问题，包括自动数据工程、跨表可迁移性和领域知识集成。此外，我们还详细阐述了 NS-TD 应用程序的现有基准和数据集，以讨论现有表格模型的性能。最后，我们讨论了 SSL4NS-TD 的挑战并为未来的研究提供了潜在的方向。我们希望我们的工作有助于鼓励更多研究降低表格域的 SSL 进入门槛并改善隐式表格数据的基础。</li>
</ul>

<h3>Title: Comparative Evaluation of Weather Forecasting using Machine Learning  Models</h3>
<ul>
<li><strong>Authors: </strong>Md Saydur Rahman, Farhana Akter Tumpa, Md Shazid Islam, Abul Al Arabi, Md Sanzid Bin Hossain, Md Saad Ul Haque</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01206">https://arxiv.org/abs/2402.01206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01206">https://arxiv.org/pdf/2402.01206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01206]] Comparative Evaluation of Weather Forecasting using Machine Learning  Models(https://arxiv.org/abs/2402.01206)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Gaining a deeper understanding of weather and being able to predict its future conduct have always been considered important endeavors for the growth of our society. This research paper explores the advancements in understanding and predicting nature's behavior, particularly in the context of weather forecasting, through the application of machine learning algorithms. By leveraging the power of machine learning, data mining, and data analysis techniques, significant progress has been made in this field. This study focuses on analyzing the contributions of various machine learning algorithms in predicting precipitation and temperature patterns using a 20-year dataset from a single weather station in Dhaka city. Algorithms such as Gradient Boosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest, Stacking Neural Network, and Stacking KNN are evaluated and compared based on their performance metrics, including Confusion matrix measurements. The findings highlight remarkable achievements and provide valuable insights into their performances and features correlation.</li>
<li><strong>摘要：</strong>更深入地了解天气并能够预测其未来的行为一直被认为是我们社会发展的重要努力。本研究论文探讨了通过应用机器学习算法在理解和预测自然行为方面取得的进展，特别是在天气预报方面。通过利用机器学习、数据挖掘和数据分析技术的力量，该领域取得了重大进展。本研究的重点是使用达卡市单个气象站的 20 年数据集来分析各种机器学习算法在预测降水和温度模式方面的贡献。梯度提升、AdaBoosting、人工神经网络、堆叠随机森林、堆叠神经网络和堆叠 KNN 等算法根据其性能指标（包括混淆矩阵测量）进行评估和比较。研究结果凸显了非凡的成就，并为他们的表现和特征相关性提供了宝贵的见解。</li>
</ul>

<h3>Title: Efficient Causal Graph Discovery Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, Yoshua Bengio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01207">https://arxiv.org/abs/2402.01207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01207">https://arxiv.org/pdf/2402.01207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01207]] Efficient Causal Graph Discovery Using Large Language Models(https://arxiv.org/abs/2402.01207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.</li>
<li><strong>摘要：</strong>我们提出了一种利用法学硕士进行完整因果图发现的新颖框架。虽然以前基于 LLM 的方法使用了成对查询方法，但这需要二次数量的查询，这对于较大的因果图很快变得不切实际。相比之下，所提出的框架使用广度优先搜索（BFS）方法，该方法允许它仅使用线性数量的查询。我们还表明，所提出的方法可以轻松地合并可用的观测数据，以提高性能。除了时间和数据效率更高之外，所提出的框架还在不同大小的现实世界因果图上实现了最先进的结果。结果证明了所提出的方法在发现因果关系方面的有效性和效率，展示了其在跨不同领域的因果图发现任务中广泛适用的潜力。</li>
</ul>

<h3>Title: HW-SW Optimization of DNNs for Privacy-preserving People Counting on  Low-resolution Infrared Arrays</h3>
<ul>
<li><strong>Authors: </strong>Matteo Risso, Chen Xie, Francesco Daghero, Alessio Burrello, Seyedmorteza Mollaei, Marco Castellano, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01226">https://arxiv.org/abs/2402.01226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01226">https://arxiv.org/pdf/2402.01226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01226]] HW-SW Optimization of DNNs for Privacy-preserving People Counting on  Low-resolution Infrared Arrays(https://arxiv.org/abs/2402.01226)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>Low-resolution infrared (IR) array sensors enable people counting applications such as monitoring the occupancy of spaces and people flows while preserving privacy and minimizing energy consumption. Deep Neural Networks (DNNs) have been shown to be well-suited to process these sensor data in an accurate and efficient manner. Nevertheless, the space of DNNs' architectures is huge and its manual exploration is burdensome and often leads to sub-optimal solutions. To overcome this problem, in this work, we propose a highly automated full-stack optimization flow for DNNs that goes from neural architecture search, mixed-precision quantization, and post-processing, down to the realization of a new smart sensor prototype, including a Microcontroller with a customized instruction set. Integrating these cross-layer optimizations, we obtain a large set of Pareto-optimal solutions in the 3D-space of energy, memory, and accuracy. Deploying such solutions on our hardware platform, we improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x code size reduction, and 15.38x energy reduction at iso-accuracy.</li>
<li><strong>摘要：</strong>低分辨率红外 (IR) 阵列传感器支持人数统计应用，例如监控空间占用情况和人流量，同时保护隐私并最大限度地减少能源消耗。深度神经网络 (DNN) 已被证明非常适合以准确且高效的方式处理这些传感器数据。然而，DNN 的架构空间巨大，手动探索非常繁重，并且常常会导致次优解决方案。为了克服这个问题，在这项工作中，我们提出了一种高度自动化的 DNN 全栈优化流程，从神经架构搜索、混合精度量化和后处理，一直到新智能传感器原型的实现，包括具有定制指令集的微控制器。整合这些跨层优化，我们在能量、内存和准确性的 3D 空间中获得了大量帕累托最优解。在我们的硬件平台上部署此类解决方案，我们改进了最先进的技术，在 iso 精度下实现了高达 4.2 倍的模型尺寸减小、23.8 倍的代码尺寸减小和 15.38 倍的能量减小。</li>
</ul>

<h3>Title: Flexible Variational Information Bottleneck: Achieving Diverse  Compression with a Single Training</h3>
<ul>
<li><strong>Authors: </strong>Sota Kudo, Naoaki Ono, Shigehiko Kanaya, Ming Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01238">https://arxiv.org/abs/2402.01238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01238">https://arxiv.org/pdf/2402.01238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01238]] Flexible Variational Information Bottleneck: Achieving Diverse  Compression with a Single Training(https://arxiv.org/abs/2402.01238)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VIB. Furthermore, in terms of calibration performance, FVIB outperforms other IB and calibration methods by enabling continuous optimization of $\beta$. Our codes are available at https://github.com/sotakudo/fvib.</li>
<li><strong>摘要：</strong>信息瓶颈 (IB) 是一种广泛使用的框架，可以从源随机变量中提取与目标随机变量相关的信息。在目标函数中，IB通过拉格朗日乘子$\beta$控制数据压缩和预测性之间的权衡。传统上，为了找到要学习的权衡，IB 需要通过多个训练周期搜索 $\beta$，这在计算上是昂贵的。在这项研究中，我们引入了灵活变分信息瓶颈（FVIB），这是一种创新的分类任务框架，可以通过单一、计算高效的训练获得所有 $\beta$ 值的最佳模型。我们从理论上证明，在所有合理的 $\beta$ 值上，FVIB 可以同时最大化变分信息瓶颈（VIB）（传统 IB 方法）的目标函数的近似值。然后我们凭经验证明 FVIB 可以像 VIB 一样有效地学习 VIB 目标。此外，在校准性能方面，FVIB 通过实现 $\beta$ 的持续优化，优于其他 IB 和校准方法。我们的代码可在 https://github.com/sotakudo/fvib 获取。</li>
</ul>

<h3>Title: TEDDY: Trimming Edges with Degree-based Discrimination strategY</h3>
<ul>
<li><strong>Authors: </strong>Hyunjin Seo, Jihun Yun, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01261">https://arxiv.org/abs/2402.01261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01261">https://arxiv.org/pdf/2402.01261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01261]] TEDDY: Trimming Edges with Degree-based Discrimination strategY(https://arxiv.org/abs/2402.01261)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient descent on the $\ell_0$ ball. Given the target sparsity levels for both the graph structure and the model parameters, our TEDDY facilitates efficient and rapid realization of GLT within a single training. Remarkably, our experimental results demonstrate that TEDDY significantly surpasses conventional iterative approaches in generalization, even when conducting one-shot sparsification that solely utilizes graph structures, without taking node features into account.</li>
<li><strong>摘要：</strong>自从 Chen 等人提出图神经网络（GNN）彩票假设的开创性工作以来。 （2021）中，寻找图彩票（GLT）的研究已成为 GNN 社区的关键焦点之一，激励研究人员发现更稀疏的 GLT，同时实现与原始密集网络相当的性能。与此同时，图结构作为 GNN 训练动态的关键因素而受到广泛关注，最近的几项研究也阐明了这一点。尽管如此，当代关于 GLT 的研究总体上还没有充分利用图结构中的内在路径，并以迭代的方式识别票证，这是耗时且低效的。为了解决这些限制，我们引入了 TEDDY，这是一种一次性边缘稀疏化框架，它通过合并边缘度信息来利用结构信息。在边缘稀疏化之后，我们通过 $\ell_0$ 球上的简单投影梯度下降来鼓励训练期间的参数稀疏性。给定图结构和模型参数的目标稀疏度水平，我们的 TEDDY 有助于在单次训练中高效、快速地实现 GLT。值得注意的是，我们的实验结果表明，即使在不考虑节点特征的情况下进行仅利用图结构的一次性稀疏化时，TEDDY 在泛化方面也显着超越了传统的迭代方法。</li>
</ul>

<h3>Title: The Human and the Mechanical: logos, truthfulness, and ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Anastasia Giannakidou, Alda Mari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01267">https://arxiv.org/abs/2402.01267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01267">https://arxiv.org/pdf/2402.01267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01267]] The Human and the Mechanical: logos, truthfulness, and ChatGPT(https://arxiv.org/abs/2402.01267)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>The paper addresses the question of whether it is appropriate to talk about `mechanical minds' at all, and whether ChatGPT models can indeed be thought of as realizations of that. Our paper adds a semantic argument to the current debate. The act of human assertion requires the formation of a veridicality judgment. Modification of assertions with modals (John must be at home) and the use of subjective elements (John is obviously at home) indicate that the speaker is manipulating her judgments and, in a cooperative context, intends her epistemic state to be transparent to the addressee. Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs. `Mechanical minds' lack these two components: (i) they do not relate to reality and (ii) do not have endogenous evidence. Therefore they lack the ability to form a belief about the world and a veridicality judgments altogether. They can only mimic that judgment, but the output is not ground in the very foundations for it.</li>
<li><strong>摘要：</strong>本文解决了谈论“机械思维”是否合适以及 ChatGPT 模型是否确实可以被视为其实现的问题。我们的论文为当前的争论添加了语义论证。人类断言的行为需要形成真实性判断。用情态动词修改断言（约翰必须在家）和使用主观元素（约翰显然在家）表明说话者正在操纵她的判断，并且在合作的背景下，希望她的认知状态对收件人是透明的。真实性判断是基于两个组成部分形成的：（i）与现实相关的证据（外生证据）和（ii）内生证据，例如偏好和私人信仰。 “机械思维”缺乏这两个组成部分：（i）它们与现实无关，（ii）没有内生证据。因此，他们缺乏形成对世界的信念和真实性判断的能力。他们只能模仿这种判断，但输出并没有奠定其基础。</li>
</ul>

<h3>Title: Federated Unlearning: a Perspective of Stability and Fairness</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Shao, Tao Lin, Xuanyu Cao, Bing Luo</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01276">https://arxiv.org/abs/2402.01276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01276">https://arxiv.org/pdf/2402.01276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01276]] Federated Unlearning: a Perspective of Stability and Fairness(https://arxiv.org/abs/2402.01276)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.</li>
<li><strong>摘要：</strong>本文探讨了具有数据异构性的联邦遗忘（FU）的多方面后果。我们引入了 FU 评估的关键指标，重点关注验证、全球稳定性和局部公平性，并调查了内在的权衡。此外，我们通过优化框架制定了具有数据异构性的遗忘过程。我们的主要贡献在于对 FU 权衡进行了全面的理论分析，并提供了数据异构性对 FU 影响的见解。利用这些见解，我们提出 FU 机制来管理权衡，指导 FU 机制的进一步发展。我们凭经验验证了我们的 FU 机制有效地平衡了权衡，证实了我们从理论分析中得出的见解。</li>
</ul>

<h3>Title: Can MLLMs Perform Text-to-Image In-Context Learning?</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, Kangwook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01293">https://arxiv.org/abs/2402.01293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01293">https://arxiv.org/pdf/2402.01293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01293]] Can MLLMs Perform Text-to-Image In-Context Learning?(https://arxiv.org/abs/2402.01293)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.</li>
<li><strong>摘要：</strong>从大型语言模型 (LLM) 到多模态大型语言模型 (MLLM) 的演变激发了将情境学习 (ICL) 扩展到其多模态对应物的研究。现有的此类研究主要集中在图像到文本 ICL。然而，文本到图像 ICL (T2I-ICL) 以其独特的特性和潜在的应用，仍未得到充分开发。为了解决这一差距，我们正式定义了 T2I-ICL 的任务，并提出了 CoBSAT，这是第一个 T2I-ICL 基准数据集，包含十项任务。利用我们的数据集对六种最先进的 MLLM 进行基准测试，我们发现了 MLLM 在解决 T2I-ICL 时遇到的相当大的困难。我们认为主要挑战是多模态和图像生成固有的复杂性。为了克服这些挑战，我们探索了微调和思想链提示等策略，并展示了显着的改进。我们的代码和数据集可在 \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT} 获取。</li>
</ul>

<h3>Title: Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted  Inference</h3>
<ul>
<li><strong>Authors: </strong>Man-Jie Yuan, Zheng Zou, Wei Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01296">https://arxiv.org/abs/2402.01296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01296">https://arxiv.org/pdf/2402.01296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01296]] Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted  Inference(https://arxiv.org/abs/2402.01296)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Privacy-preserving neural networks have attracted increasing attention in recent years, and various algorithms have been developed to keep the balance between accuracy, computational complexity and information security from the cryptographic view. This work takes a different view from the input data and structure of neural networks. We decompose the input data (e.g., some images) into sensitive and insensitive segments according to importance and privacy. The sensitive segment includes some important and private information such as human faces and we take strong homomorphic encryption to keep security, whereas the insensitive one contains some background and we add perturbations. We propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal with two segments, respectively, and ciphertext branch could utilize the information from plaintext branch by unidirectional connections. We adopt knowledge distillation for our bi-CryptoNets by transferring representations from a well-trained teacher neural network. Empirical studies show the effectiveness and decrease of inference latency for our bi-CryptoNets.</li>
<li><strong>摘要：</strong>近年来，隐私保护神经网络受到越来越多的关注，并且已经开发出各种算法来从密码学的角度保持准确性、计算复杂性和信息安全之间的平衡。这项工作对神经网络的输入数据和结构采取了不同的观点。我们根据重要性和隐私将输入数据（例如，一些图像）分解为敏感和不敏感的部分。敏感部分包含一些重要的隐私信息，例如人脸，我们采用强同态加密来保证安全，而不敏感部分包含一些背景信息，我们添加扰动。我们提出双加密网络，即明文和密文分支，分别处理两个片段，密文分支可以通过单向连接利用来自明文分支的信息。我们通过从训练有素的教师神经网络转移表示来为我们的双加密网络采用知识蒸馏。实证研究表明我们的双加密网络推理延迟的有效性和减少。</li>
</ul>

<h3>Title: KTO: Model Alignment as Prospect Theoretic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01306">https://arxiv.org/abs/2402.01306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01306">https://arxiv.org/pdf/2402.01306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01306]] KTO: Model Alignment as Prospect Theoretic Optimization(https://arxiv.org/abs/2402.01306)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether an output is desirable or undesirable for a given input. This makes it far easier to use in the real world, where preference data is scarce and expensive.</li>
<li><strong>摘要：</strong>卡尼曼和特沃斯基的$\textit{前景理论}$告诉我们，人类以一种有偏见但定义明确的方式感知随机变量；例如，人类是出了名的厌恶损失。我们表明，使法学硕士与人类反馈保持一致的目标隐含地包含了许多这些偏差——这些目标（例如，DPO）在交叉熵最小化上的成功可以部分归因于它们是$\textit{人类感知的损失函数} $（光环）。然而，这些方法赋予人类的效用函数仍然与前景理论文献中的不同。使用人类效用的 Kahneman-Tversky 模型，我们提出了一个 HALO，它直接最大化各代人的效用，而不是像当前方法那样最大化偏好的对数似然。我们将这种方法称为 Kahneman-Tversky Optimization (KTO)，它在 1B 到 30B 的规模上匹配或超过了基于偏好的方法的性能。至关重要的是，KTO 不需要偏好——只需要一个关于给定输入的输出是否理想的二进制信号。这使得它在偏好数据稀缺且昂贵的现实世界中更容易使用。</li>
</ul>

<h3>Title: Fundamental Properties of Causal Entropy and Information Gain</h3>
<ul>
<li><strong>Authors: </strong>Francisco N. F. Q. Simoes, Mehdi Dastani, Thijs van Ommen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01341">https://arxiv.org/abs/2402.01341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01341">https://arxiv.org/pdf/2402.01341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01341]] Fundamental Properties of Causal Entropy and Information Gain(https://arxiv.org/abs/2402.01341)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks through the study of recently-proposed information theoretic quantities grounded in considerations about causality.</li>
<li><strong>摘要：</strong>最近的发展使得在给定结构因果模型（SCM）的情况下量化因果控制成为可能。这是通过引入对一个变量干预另一个变量时熵的变化进行编码的量来实现的。这些措施被称为因果熵和因果信息增益，旨在解决现有信息论方法在机器学习任务中的局限性，其中因果关系起着至关重要的作用。它们尚未经过适当的数学研究。我们的研究通过建立和分析因果熵和因果信息增益概念的基本属性（包括界限和链规则），有助于对这些概念的正式理解。此外，我们阐明了因果熵和随机干预之间的关系。我们还提出了因果条件熵和因果条件信息增益的定义。总体而言，这一探索通过研究最近提出的基于因果关系的信息论量，为增强因果机器学习任务铺平了道路。</li>
</ul>

<h3>Title: Training-time Neuron Alignment through Permutation Subspace for  Improving Linear Mode Connectivity and Model Fusion</h3>
<ul>
<li><strong>Authors: </strong>Zexi Li, Zhiqi Li, Jie Lin, Tao Shen, Tao Lin, Chao Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01342">https://arxiv.org/abs/2402.01342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01342">https://arxiv.org/pdf/2402.01342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01342]] Training-time Neuron Alignment through Permutation Subspace for  Improving Linear Mode Connectivity and Model Fusion(https://arxiv.org/abs/2402.01342)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and empirically validated for reducing LMC barriers. It excels in wide model fusion applications, especially in federated learning, two algorithms based on TNA-FPN that are proposed to show its prospects even under heterogeneous datasets. Moreover, TNA-PFN can enhance the generalization of model soup for vision transformers and ColD fusion for pretrained language models.</li>
<li><strong>摘要：</strong>在深度学习中，即使在相同的初始化下，随机梯度下降通常也会在权重空间中产生功能相似但广泛分散的解决方案，从而在线性模式连接（LMC）领域造成障碍。克服这些障碍对于理解深度学习动态和增强模型融合算法至关重要。先前的研究强调了排列对称性在通过网络排列减少训练后障碍方面的作用。然而，由于存在大量的置换矩阵，这些事后方法需要额外的计算，对于较大、复杂的模型（例如 ViT、LLM）效果较差。因此，在本文中，我们研究训练时神经元对齐。我们的假设表明，训练时间排列子空间可以免费减少 LMC 障碍。我们发现初始化时的修剪支持这一点。除了剪枝之外，我们还引入了 TNA-PFN，这是一种在训练期间使用部分梯度掩模的简单但无损的算法。 TNA-PFN 在减少 LMC 障碍方面经过了理论和经验验证。它在广泛的模型融合应用中表现出色，特别是在联邦学习中，这两种基于 TNA-FPN 的算法即使在异构数据集下也显示出其前景。此外，TNA-PFN 可以增强视觉变换器模型汤的泛化和预训练语言模型的 ColD 融合。</li>
</ul>

<h3>Title: Shapelet-based Model-agnostic Counterfactual Local Explanations for Time  Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Qi Huang, Wei Chen, Thomas Bäck, Niki van Stein</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01343">https://arxiv.org/abs/2402.01343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01343">https://arxiv.org/pdf/2402.01343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01343]] Shapelet-based Model-agnostic Counterfactual Local Explanations for Time  Series Classification(https://arxiv.org/abs/2402.01343)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this work, we propose a model-agnostic instance-based post-hoc explainability method for time series classification. The proposed algorithm, namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual explanations for arbitrary time series classifiers. We validate the proposed method on several real-world univariate time series classification tasks from the UCR Time Series Archive. The results indicate that the counterfactual instances generated by Time-CF when compared to state-of-the-art methods, demonstrate better performance in terms of four explainability metrics: closeness, sensibility, plausibility, and sparsity.</li>
<li><strong>摘要：</strong>在这项工作中，我们提出了一种与模型无关的基于实例的事后可解释性方法，用于时间序列分类。所提出的算法，即 Time-CF，利用 shapelet 和 TimeGAN 为任意时间序列分类器提供反事实解释。我们在 UCR 时间序列档案中的几个现实世界单变量时间序列分类任务上验证了所提出的方法。结果表明，与最先进的方法相比，Time-CF 生成的反事实实例在四个可解释性指标方面表现出更好的性能：接近性、敏感性、合理性和稀疏性。</li>
</ul>

<h3>Title: CORE: Mitigating Catastrophic Forgetting in Continual Learning through  Cognitive Replay</h3>
<ul>
<li><strong>Authors: </strong>Jianshu Zhang, Yankai Fu, Ziheng Peng, Dongyu Yao, Kun He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01348">https://arxiv.org/abs/2402.01348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01348">https://arxiv.org/pdf/2402.01348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01348]] CORE: Mitigating Catastrophic Forgetting in Continual Learning through  Cognitive Replay(https://arxiv.org/abs/2402.01348)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information. Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer. In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes. CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer. Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, it significantly enhances the accuracy of the poorest-performing task by 6.30% compared to the top baseline.</li>
<li><strong>摘要：</strong>本文介绍了一种显着减轻持续学习（CL）中灾难性遗忘的新视角，强调模型保留现有知识和吸收新信息的能力。当前基于重放的方法平等地对待每个任务和数据样本，因此无法充分利用重放缓冲区的潜力。作为回应，我们提出了认知重放（CORE），它从人类认知回顾过程中汲取灵感。 CORE 包括两个关键策略：自适应数量分配和注重质量的数据选择。前者根据每个任务的遗忘率自适应地调整重播缓冲区分配，而后者则保证包含最好地封装缓冲区内每个任务的特征的代表性数据。我们的方法在 split-CIFAR10 上的平均准确率达到 37.95%，比最佳基线方法高出 6.52%。此外，与最高基线相比，它还将表现最差的任务的准确性显着提高了 6.30%。</li>
</ul>

<h3>Title: Beyond the Answers: Reviewing the Rationality of Multiple Choice  Question Answering for the Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haochun Wang, Sendong Zhao, Zewen Qiang, Bing Qin, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01349">https://arxiv.org/abs/2402.01349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01349">https://arxiv.org/pdf/2402.01349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01349]] Beyond the Answers: Reviewing the Rationality of Multiple Choice  Question Answering for the Evaluation of Large Language Models(https://arxiv.org/abs/2402.01349)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need for more robust evaluation mechanisms in assessing the performance of LLMs.</li>
<li><strong>摘要：</strong>在自然语言处理（NLP）领域，大型语言模型（LLM）引发了范式转变，显着提高了自然语言生成任务的性能。尽管取得了这些进步，法学硕士的综合评估仍然是社区不可避免的挑战。最近，利用多项选择题回答（MCQA）作为法学硕士的基准已经获得了相当大的关注。本研究探讨了 MCQA 作为法学硕士评估方法的合理性。如果法学硕士真正理解问题的语义，他们的表现应该在从相同问题派生的不同配置中表现出一致性。与这一预期相反，我们的实证结果表明，LLM 响应的一致性存在显着差异，我们将其定义为 LLM 的响应变异性综合症 (REVAS)，这表明当前基于 MCQA 的基准可能无法充分捕捉 LLM 的真实能力，这强调了在评估法学硕士的表现时需要更强有力的评估机制。</li>
</ul>

<h3>Title: FedMoE: Data-Level Personalization with Mixture of Experts for  Model-Heterogeneous Personalized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Liping Yi, Han Yu, Chao Ren, Heng Zhang, Gang Wang, Xiaoguang Liu, Xiaoxiao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01350">https://arxiv.org/abs/2402.01350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01350">https://arxiv.org/pdf/2402.01350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01350]] FedMoE: Data-Level Personalization with Mixture of Experts for  Model-Heterogeneous Personalized Federated Learning(https://arxiv.org/abs/2402.01350)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. The local gating network produces personalized weights for extracted representations from both experts on each data sample. The three models form a local heterogeneous MoE. The weighted mixed representation fuses global generalized and local personalized features and is processed by the local heterogeneous large model's header with personalized prediction information for output. The MoE and prediction header are updated synchronously. (2) The trained local homogeneous small feature extractors are sent to the server for cross-client information fusion via aggregation. Briefly, FedMoE first enhances local model personalization at a fine-grained data level while supporting model heterogeneity.</li>
<li><strong>摘要：</strong>联邦学习（FL）广泛应用于去中心化数据的协作训练，但面临数据、系统和模型异构性等挑战。这促使模型异构个性化联邦学习（MHPFL）的出现。然而，当前 MHPFL 方法中对数据和模型隐私、模型性能、通信和计算成本的担忧仍然存在。为了解决这些问题，我们提出了一种新颖的模型——异构个性化联邦学习算法（FedMoE）和混合专家（MoE），该算法以增强大型语言模型（LLM）而闻名。它为每个客户端的本地异构大模型分配一个共享的同质小特征提取器和一个本地门控网络。 （1）在局部训练过程中，局部异构模型的特征提取器充当局部专家，进行个性化特征（表示）提取，而共享同质小特征提取器则充当全局专家，进行广义特征提取。本地门控网络为每个数据样本的两位专家提取的表示生成个性化权重。这三个模型形成了本地异构MoE。加权混合表示融合了全局广义和局部个性化特征，并由局部异构大模型的标头处理，并输出个性化预测信息。 MoE和预测头同步更新。 (2)将训练好的局部同质小特征提取器发送到服务器，通过聚合进行跨客户端信息融合。简而言之，FedMoE 首先在细粒度数据级别增强本地模型个性化，同时支持模型异构性。</li>
</ul>

<h3>Title: Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting  the Variation in Human Signals during Visuo-Linguistic Processes</h3>
<ul>
<li><strong>Authors: </strong>Ece Takmaz, Sandro Pezzelle, Raquel Fernández</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01352">https://arxiv.org/abs/2402.01352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01352">https://arxiv.org/pdf/2402.01352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01352]] Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting  the Variation in Human Signals during Visuo-Linguistic Processes(https://arxiv.org/abs/2402.01352)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for humans and what leads to variations in human outputs.</li>
<li><strong>摘要：</strong>图像的属性与人类在描述图像时的行为方式之间存在着复杂的关系。这种行为表现出充足的变化，如眼球运动等人类信号以及人类开始描述图像时所体现的那样。尽管这些视觉语言变异信号很有价值，但在当前预训练模型的训练中它们实际上被忽视了，这激发了进一步的研究。使用荷兰语图像描述语料库和同时收集的眼动追踪数据，我们探索了视觉语言信号变化的本质，并发现它们彼此相关。鉴于这个结果，我们假设变化部分源于图像的属性，并探索由预训练的视觉编码器编码的图像表示是否可以捕获这种变化。我们的结果表明，预训练模型在弱到中等程度上做到了这一点，这表明这些模型对于人类刺激的复杂性以及导致人类输出变化的原因缺乏偏见。</li>
</ul>

<h3>Title: What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation  Properties for Fact Verification</h3>
<ul>
<li><strong>Authors: </strong>Amelie Wührl, Yarik Menchaca Resendiz, Lara Grimminger, Roman Klinger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01360">https://arxiv.org/abs/2402.01360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01360">https://arxiv.org/pdf/2402.01360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01360]] What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation  Properties for Fact Verification(https://arxiv.org/abs/2402.01360)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Biomedical claim verification fails if no evidence can be discovered. In these cases, the fact-checking verdict remains unknown and the claim is unverifiable. To improve upon this, we have to understand if there are any claim properties that impact its verifiability. In this work we assume that entities and relations define the core variables in a biomedical claim's anatomy and analyze if their properties help us to differentiate verifiable from unverifiable claims. In a study with trained annotation experts we prompt them to find evidence for biomedical claims, and observe how they refine search queries for their evidence search. This leads to the first corpus for scientific fact verification annotated with subject-relation-object triplets, evidence documents, and fact-checking verdicts (the BEAR-Fact corpus). We find (1) that discovering evidence for negated claims (e.g., X-does-not-cause-Y) is particularly challenging. Further, we see that annotators process queries mostly by adding constraints to the search and by normalizing entities to canonical names. (2) We compare our in-house annotations with a small crowdsourcing setting where we employ medical experts and laypeople. We find that domain expertise does not have a substantial effect on the reliability of annotations. Finally, (3), we demonstrate that it is possible to reliably estimate the success of evidence retrieval purely from the claim text~(.82\F), whereas identifying unverifiable claims proves more challenging (.27\F). The dataset is available at this http URL</li>
<li><strong>摘要：</strong>如果找不到证据，生物医学声明验证就会失败。在这些情况下，事实核查的判决仍然未知，而且索赔也无法核实。为了改进这一点，我们必须了解是否有任何声明属性会影响其可验证性。在这项工作中，我们假设实体和关系定义了生物医学声明的解剖学中的核心变量，并分析它们的属性是否有助于我们区分可验证的声明和不可验证的声明。在与训练有素的注释专家进行的一项研究中，我们提示他们寻找生物医学主张的证据，并观察他们如何完善证据搜索的搜索查询。这导致了第一个用主语-关系-客体三元组、证据文件和事实检查判决注释的科学事实验证语料库（BEAR-Fact语料库）。我们发现（1）发现否定主张的证据（例如，X-不导致-Y）特别具有挑战性。此外，我们看到注释器主要通过向搜索添加约束以及将实体规范化为规范名称来处理查询。 (2) 我们将内部注释与雇用医学专家和非专业人士的小型众包环境进行比较。我们发现领域专业知识对注释的可靠性没有实质性影响。最后，（3），我们证明可以纯粹从声明文本可靠地估计证据检索的成功~（.82\F），而识别不可验证的声明则更具挑战性（.27\F）。该数据集可在此 http URL 中获取</li>
</ul>

<h3>Title: To the Max: Reinventing Reward in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Grigorii Veviurko, Wendelin Böhmer, Mathijs de Weerdt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01361">https://arxiv.org/abs/2402.01361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01361">https://arxiv.org/pdf/2402.01361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01361]] To the Max: Reinventing Reward in Reinforcement Learning(https://arxiv.org/abs/2402.01361)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach to using rewards for learning. We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is publicly available.</li>
<li><strong>摘要：</strong>在强化学习（RL）中，不同的奖励可以定义相同的最优策略，但会导致截然不同的学习表现。对于某些人来说，代理会陷入次优的行为，而对于另一些人来说，它可以有效地解决任务。因此，选择一个好的奖励函数是一个极其重要但具有挑战性的问题。在本文中，我们探索了一种使用奖励进行学习的替代方法。我们引入了最大奖励强化学习，其中代理优化最大而不是累积奖励。与早期的工作不同，我们的方法适用于确定性和随机环境，并且可以轻松与最先进的 RL 算法相结合。在实验中，我们研究了 Gymnasium-Robotics 的两个目标达成环境中最大奖励 RL 算法的性能，并证明了其相对于标准 RL 的优势。该代码是公开的。</li>
</ul>

<h3>Title: Continual Learning for Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01364">https://arxiv.org/abs/2402.01364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01364">https://arxiv.org/pdf/2402.01364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01364]] Continual Learning for Large Language Models: A Survey(https://arxiv.org/abs/2402.01364)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）不适合频繁的重新训练，因为其规模庞大，训练成本很高。然而，更新是必要的，以赋予法学硕士新的技能，并让他们跟上快速发展的人类知识。本文调查了有关法学硕士持续学习的最新研究成果。由于法学硕士的独特性质，我们将继续学习技术编入一种新颖的多阶段分类方案中，包括持续预训练、指令调整和对齐。我们将法学硕士的持续学习与较小模型中使用的更简单的适应方法以及其他增强策略（例如检索增强生成和模型编辑）进行了对比。此外，通过对基准和评估的讨论，我们确定了这项关键任务的一些挑战和未来的工作方向。</li>
</ul>

<h3>Title: Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with  Multi-Modal Priors</h3>
<ul>
<li><strong>Authors: </strong>Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01369">https://arxiv.org/abs/2402.01369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01369">https://arxiv.org/pdf/2402.01369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01369]] Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with  Multi-Modal Priors(https://arxiv.org/abs/2402.01369)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, code</a></li>
<li><strong>Abstract: </strong>Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks the first successful attempt of transfer-based attack to commercial T2I models. Our code is publicly available at \url{https://github.com/ydc123/MMP-Attack}.</li>
<li><strong>摘要：</strong>扩散模型已广泛应用于各种图像生成任务中，展示了图像和文本模式之间的非凡联系。然而，它们面临着被恶意利用的挑战，即通过在原始提示中附加特定后缀来生成有害或敏感图像。现有的工作主要集中于利用单模态信息进行攻击，未能利用多模态特征，导致性能不太理想。结合多模态先验（MMP），即文本和图像特征，我们在这项工作中提出了一种名为 MMP-Attack 的有针对性的攻击方法。具体来说，MMP-Attack 的目标是将目标对象添加到图像内容中，同时删除原始对象。 MMP-Attack较现有作品具有显着优势，具有卓越的通用性和可移植性，可以有效攻击DALL-E 3等商业文本到图像（T2I）模型。据我们所知，这标志着首次成功攻击尝试对商业 T2I 模型进行基于转移的攻击。我们的代码可在 \url{https://github.com/ydc123/MMP-Attack} 上公开获取。</li>
</ul>

<h3>Title: Critic-Actor for Average Reward MDPs with Function Approximation: A  Finite-Time Analysis</h3>
<ul>
<li><strong>Authors: </strong>Prashansa Panda, Shalabh Bhatnagar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01371">https://arxiv.org/abs/2402.01371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01371">https://arxiv.org/pdf/2402.01371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01371]] Critic-Actor for Average Reward MDPs with Function Approximation: A  Finite-Time Analysis(https://arxiv.org/abs/2402.01371)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\epsilon$ which is better than the one obtained for actor-critic in a similar setting. We also show the results of numerical experiments on three benchmark settings and observe that the critic-actor algorithm competes well with the actor-critic algorithm.</li>
<li><strong>摘要：</strong>近年来，有大量的研究工作活动集中在对两个时间尺度的行动者批评者算法进行渐近和非渐近收敛分析，其中行动者更新的时间尺度比批评者的更新速度慢。在最近的一项工作中，针对查找表情况下的无限水平折扣成本设置，提出了批评者-行动者算法，其中行动者和批评者的时间尺度相反，并提出了渐近收敛分析。在我们的工作中，我们提出了第一个具有函数逼近和长期平均奖励设置的批评者-演员算法，并提出了此类方案的第一个有限时间（非渐近）分析。我们获得了最佳学习率，并证明我们的算法实现了 $\mathcal{\tilde{O}}(\epsilon^{-2.08})$ 的样本复杂度，批评者的均方误差上限为 $\ epsilon$ 比类似环境中演员评论家获得的更好。我们还展示了三个基准设置上的数值实验结果，并观察到批评者-演员算法与演员-批评者算法具有良好的竞争性。</li>
</ul>

<h3>Title: Dive into the Chasm: Probing the Gap between In- and Cross-Topic  Generalization</h3>
<ul>
<li><strong>Authors: </strong>Andreas Waldis, Yufang Hou, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01375">https://arxiv.org/abs/2402.01375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01375">https://arxiv.org/pdf/2402.01375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01375]] Dive into the Chasm: Probing the Gap between In- and Cross-Topic  Generalization(https://arxiv.org/abs/2402.01375)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (LMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics -- such as Gun Control. This study analyzes various LMs with three probing-based experiments to shed light on the reasons behind the In- vs. Cross-Topic generalization gap. Thereby, we demonstrate, for the first time, that generalization gaps and the robustness of the embedding space vary significantly across LMs. Additionally, we assess larger LMs and underscore the relevance of our analysis for recent models. Overall, diverse pre-training objectives, architectural regularization, or data deduplication contribute to more robust LMs and diminish generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.</li>
<li><strong>摘要：</strong>预训练语言模型 (LM) 在主题内设置中表现良好，其中训练和测试数据来自同一主题。然而，他们在跨主题场景中面临挑战，其中测试数据来自不同的主题，例如枪支管制。本研究通过三个基于探测的实验分析了各种 LM，以揭示 In-与 Cross-Topic 泛化差距背后的原因。因此，我们首次证明了不同语言模型的泛化差距和嵌入空间的稳健性存在显着差异。此外，我们评估了更大的 LM，并强调了我们的分析与最新模型的相关性。总体而言，多样化的预训练目标、架构正则化或重复数据删除有助于增强 LM 并缩小泛化差距。我们的研究有助于更深入地理解和比较不同泛化场景的语言模型。</li>
</ul>

<h3>Title: LoTR: Low Tensor Rank Weight Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01376">https://arxiv.org/abs/2402.01376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01376">https://arxiv.org/pdf/2402.01376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01376]] LoTR: Low Tensor Rank Weight Adaptation(https://arxiv.org/abs/2402.01376)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.</li>
<li><strong>摘要：</strong>在本文中，我们概括并扩展了基于 Transformer 架构的大型语言模型 (LLM) 的低秩自适应 (LoRA) 思想。广泛使用的类似 LoRA 的 LLM 微调方法基于梯度更新的矩阵分解。我们引入了 LoTR，一种用于 LLM 参数高效微调的新方法，它以张量分解的形式表示参数的梯度更新。每层的低阶适配器被构造为三个矩阵的乘积，并且张量结构是通过在各层之间共享该乘积的左右乘数而产生的。同时压缩具有低秩张量表示的层序列使得 LoTR 能够实现比 LoRA 更好的参数效率，特别是对于深度模型。此外，核心张量不依赖于原始权重维度，并且可以任意小，这允许极其便宜且快速的下游微调。</li>
</ul>

<h3>Title: Regularized boosting with an increasing coefficient magnitude stop  criterion as meta-learner in hyperparameter optimization stacking ensemble</h3>
<ul>
<li><strong>Authors: </strong>Laura Fdez-Díaz, José Ramón Quevedo, Elena Montañés</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01379">https://arxiv.org/abs/2402.01379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01379">https://arxiv.org/pdf/2402.01379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01379]] Regularized boosting with an increasing coefficient magnitude stop  criterion as meta-learner in hyperparameter optimization stacking ensemble(https://arxiv.org/abs/2402.01379)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In Hyperparameter Optimization (HPO), only the hyperparameter configuration with the best performance is chosen after performing several trials, then, discarding the effort of training all the models with every hyperparameter configuration trial and performing an ensemble of all them. This ensemble consists of simply averaging the model predictions or weighting the models by a certain probability. Recently, other more sophisticated ensemble strategies, such as the Caruana method or the stacking strategy has been proposed. On the one hand, the Caruana method performs well in HPO ensemble, since it is not affected by the effects of multicollinearity, which is prevalent in HPO. It just computes the average over a subset of predictions with replacement. But it does not benefit from the generalization power of a learning process. On the other hand, stacking methods include a learning procedure since a meta-learner is required to perform the ensemble. Yet, one hardly finds advice about which meta-learner is adequate. Besides, some meta-learners may suffer from the effects of multicollinearity or need to be tuned to reduce them. This paper explores meta-learners for stacking ensemble in HPO, free of hyperparameter tuning, able to reduce the effects of multicollinearity and considering the ensemble learning process generalization power. At this respect, the boosting strategy seems promising as a stacking meta-learner. In fact, it completely removes the effects of multicollinearity. This paper also proposes an implicit regularization in the classical boosting method and a novel non-parametric stop criterion suitable only for boosting and specifically designed for HPO. The synergy between these two improvements over boosting exhibits competitive and promising predictive power performance compared to other existing meta-learners and ensemble approaches for HPO other than the stacking ensemble.</li>
<li><strong>摘要：</strong>在超参数优化（HPO）中，在执行多次试验后仅选择性能最佳的超参数配置，然后放弃在每次超参数配置试验中训练所有模型并执行所有模型的集成的工作。该集成包括简单地对模型预测进行平均或按一定概率对模型进行加权。最近，提出了其他更复杂的集成策略，例如 Caruana 方法或堆叠策略。一方面，Caruana 方法在 HPO 系综中表现良好，因为它不受 HPO 中普遍存在的多重共线性的影响。它只是计算带有替换的预测子集的平均值。但它并没有受益于学习过程的泛化能力。另一方面，堆叠方法包括学习过程，因为需要元学习器来执行集成。然而，人们很难找到关于哪种元学习器足够的建议。此外，一些元学习器可能会受到多重共线性的影响，或者需要进行调整以减少多重共线性的影响。本文探索了 HPO 中堆叠集成的元学习器，无需超参数调整，能够减少多重共线性的影响，并考虑集成学习过程的泛化能力。在这方面，提升策略作为堆叠元学习器似乎很有前途。事实上，它完全消除了多重共线性的影响。本文还提出了经典 boosting 方法中的隐式正则化以及一种仅适用于 boosting 且专门为 HPO 设计的新型非参数停止准则。与除堆叠集成之外的其他现有元学习器和 HPO 集成方法相比，这两种增强改进之间的协同作用表现出具有竞争力和有前途的预测能力性能。</li>
</ul>

<h3>Title: LLM-based NLG Evaluation: Current Status and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01383">https://arxiv.org/abs/2402.01383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01383">https://arxiv.org/pdf/2402.01383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01383]] LLM-based NLG Evaluation: Current Status and Challenges(https://arxiv.org/abs/2402.01383)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.</li>
<li><strong>摘要：</strong>评估自然语言生成（NLG）是人工智能中一个至关重要但具有挑战性的问题。传统的评估指标主要捕获系统输出和参考之间的内容（例如n-gram）重叠，这远远不能令人满意，而诸如ChatGPT之类的大型语言模型（LLM）近年来在NLG评估中表现出了巨大的潜力。人们提出了各种基于LLM的自动评估方法，包括从LLM导出的指标、提示LLM以及使用标记评估数据微调LLM。在本次调查中，我们首先对基于LLM的NLG评估方法进行了分类，并分别讨论了它们的优缺点。我们还讨论了 NLG 评估的人与法学硕士合作。最后，我们讨论了该领域的几个悬而未决的问题并指出了未来的研究方向。</li>
</ul>

<h3>Title: A Probabilistic Model to explain Self-Supervised Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Alice Bizeul, Bernhard Schölkopf, Carl Allen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01399">https://arxiv.org/abs/2402.01399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01399">https://arxiv.org/pdf/2402.01399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01399]] A Probabilistic Model to explain Self-Supervised Representation Learning(https://arxiv.org/abs/2402.01399)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows the gap to discriminative methods on _content_ classification and, as our analysis predicts, outperforms them where _style_ information is required, taking a step toward task-agnostic representations.</li>
<li><strong>摘要：</strong>自监督学习（SSL）通过利用辅助无监督任务来学习表示，例如对语义相关的样本进行分类，例如不同的数据增强或模式。在 SSL 的众多方法中，对比方法，例如SimCLR、CLIP 和 VicREG 因学习表示能够实现接近监督学习的下游性能而受到关注。然而，对这些方法背后的机制的理论理解却很困难。我们提出了一种数据的生成潜在变量模型，并表明包括对比方法在内的几个判别性自监督算法系列近似地诱导了其表征上的潜在结构，提供了一个统一的理论框架。我们还证明了相互信息的链接和投影头的使用的合理性。 SimVE 生成式拟合我们的模型，在常见基准（例如 FashionMNIST、CIFAR10、CelebA）上比以前的 VAE 方法提高了性能，缩小了与 _content_ 分类上的判别方法的差距，并且正如我们的分析预测的那样，在需要 _style_ 信息的情况下优于它们，朝着与任务无关的表示迈出了一步。</li>
</ul>

<h3>Title: Climbing the Ladder of Interpretability with Counterfactual Concept  Bottleneck Models</h3>
<ul>
<li><strong>Authors: </strong>Gabriele Dominici, Pietro Barbiero, Francesco Giannini, Martin Gjoreski, Giuseppe Marra, Marc Langheinrich</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01408">https://arxiv.org/abs/2402.01408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01408">https://arxiv.org/pdf/2402.01408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01408]] Climbing the Ladder of Interpretability with Counterfactual Concept  Bottleneck Models(https://arxiv.org/abs/2402.01408)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the "What?"), explain task predictions (the "Why?"), and imagine alternative scenarios that could result in different predictions (the "What if?"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the "What?"), simple explanations for task predictions (the "Why?"), and interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) show users how to get a desired class label, and (iii) propose concept interventions via "task-driven" interventions.</li>
<li><strong>摘要：</strong>当前的深度学习模型并非旨在同时解决三个基本问题：预测类标签以解决给定的分类任务（“什么？”）、解释任务预测（“为什么？”）以及想象可能导致以下结果的替代场景：不同的预测（“如果？”）。无法回答这些问题意味着在部署可靠的人工智能代理、校准人类信任和深化人机交互方面存在重大差距。为了弥补这一差距，我们引入了 CounterFactual 概念瓶颈模型 (CF-CBM)，这是一类模型，旨在一次性有效地解决上述查询，而无需运行事后搜索。我们的结果表明，CF-CBM 产生：准确的预测（“什么？”）、对任务预测的简单解释（“为什么？”）和可解释的反事实（“如果怎样？”）。 CF-CBM 还可以采样或估计最可能的反事实：（i）解释概念干预对任务的影响，（ii）向用户展示如何获得所需的类别标签，以及（iii）通过“任务-”提出概念干预驱动”的干预措施。</li>
</ul>

<h3>Title: SMLP: Symbolic Machine Learning Prover</h3>
<ul>
<li><strong>Authors: </strong>Franz Brauße, Zurab Khasidashvili, Konstantin Korovin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO, cs.SC, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01415">https://arxiv.org/abs/2402.01415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01415">https://arxiv.org/pdf/2402.01415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01415]] SMLP: Symbolic Machine Learning Prover(https://arxiv.org/abs/2402.01415)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors. SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods. SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level. SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models.</li>
<li><strong>摘要：</strong>符号机器学习证明器 (SMLP) 是一种基于通过在多个输入向量上模拟或执行系统而获得的数据样本进行系统探索的工具和库。 SMLP旨在通过采用灰盒方法来探索基于这些数据的系统：SMLP将数据探索的统计方法与在系统响应的闭反馈环中构建和探索机器学习模型相结合，并通过结合概率和形式来探索这些模型方法。 SMLP 已应用于英特尔的工业环境中，用于在模拟级别分析和优化硬件设计。 SMLP 是一种通用工具，可应用于可通过机器学习模型进行采样和建模的系统。</li>
</ul>

<h3>Title: Sequence Shortening for Context-Aware Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Paweł Mąka, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01416">https://arxiv.org/abs/2402.01416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01416">https://arxiv.org/pdf/2402.01416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01416]] Sequence Shortening for Context-Aware Machine Translation(https://arxiv.org/abs/2402.01416)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that the two methods achieve competitive BLEU and COMET scores and accuracies on the contrastive datasets to the other tested methods while potentially allowing for higher interpretability and reducing the growth of memory requirements with increased context size.</li>
<li><strong>摘要：</strong>上下文感知机器翻译旨在通过将周围的句子合并为上下文来改进句子的翻译。对于此任务，应用了两种主要架构，即单编码器（基于串联）和多编码器模型。在这项研究中，我们展示了多编码器架构的一个特殊情况，其中源句子的潜在表示被缓存并重用为下一步的上下文，在对比数据集上实现了更高的准确性（模型必须对模型进行排序）所提供句子中的正确翻译）以及与单编码器和多编码器方法相当的 BLEU 和 COMET 分数。此外，我们研究了序列缩短在缓存表示中的应用。我们测试了三种基于池化的缩短技术，并引入了两种新颖的方法 - 潜在分组和潜在选择，其中网络学习对令牌进行分组或选择要缓存为上下文的令牌。我们的实验表明，与其他测试方法相比，这两种方法在数据集上实现了具有竞争力的 BLEU 和 COMET 分数和准确性，同时可能允许更高的可解释性，并随着上下文大小的增加而减少内存需求的增长。</li>
</ul>

<h3>Title: The effect of diversity on group decision-making</h3>
<ul>
<li><strong>Authors: </strong>Georgi Karadzhov, Andreas Vlachos, Tom Stafford</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01427">https://arxiv.org/abs/2402.01427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01427">https://arxiv.org/pdf/2402.01427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01427]] The effect of diversity on group decision-making(https://arxiv.org/abs/2402.01427)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>We explore different aspects of cognitive diversity and its effect on the success of group deliberation. To evaluate this, we use 500 dialogues from small, online groups discussing the Wason Card Selection task - the DeliData corpus. Leveraging the corpus, we perform quantitative analysis evaluating three different measures of cognitive diversity. First, we analyse the effect of group size as a proxy measure for diversity. Second, we evaluate the effect of the size of the initial idea pool. Finally, we look into the content of the discussion by analysing discussed solutions, discussion patterns, and how conversational probing can improve those characteristics. Despite the reputation of groups for compounding bias, we show that small groups can, through dialogue, overcome intuitive biases and improve individual decision-making. Across a large sample and different operationalisations, we consistently find that greater cognitive diversity is associated with more successful group deliberation. Code and data used for the analysis are available in the anonymised repository: https://anonymous.4open.science/ r/cogsci24-FD6D</li>
<li><strong>摘要：</strong>我们探讨认知多样性的不同方面及其对小组审议成功的影响。为了评估这一点，我们使用了讨论 Wason 卡选择任务的小型在线小组的 500 个对话 - DeliData 语料库。利用该语料库，我们进行定量分析，评估认知多样性的三种不同衡量标准。首先，我们分析群体规模作为多样性代理衡量标准的影响。其次，我们评估初始创意池规模的影响。最后，我们通过分析讨论的解决方案、讨论模式以及对话式探究如何改进这些特征来研究讨论的内容。尽管群体因加剧偏见而闻名，但我们表明，小群体可以通过对话克服直觉偏见并改善个人决策。在大样本和不同的操作中，我们一致发现更大的认知多样性与更成功的群体审议相关。用于分析的代码和数据可在匿名存储库中找到：https://anonymous.4open.science/r/cogsci24-FD6D</li>
</ul>

<h3>Title: From Words to Molecules: A Survey of Large Language Models in Chemistry</h3>
<ul>
<li><strong>Authors: </strong>Chang Liao, Yemin Yu, Yu Mei, Ying Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01439">https://arxiv.org/abs/2402.01439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01439">https://arxiv.org/pdf/2402.01439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01439]] From Words to Molecules: A Survey of Large Language Models in Chemistry(https://arxiv.org/abs/2402.01439)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their application in chemistry tasks. Finally, we identify promising research directions, including further integration with chemical knowledge, advancements in continual learning, and improvements in model interpretability, paving the way for groundbreaking developments in the field.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLM）在自然语言处理（NLP）和各个跨学科领域取得了巨大的成功。然而，将法学硕士应用于化学是一项复杂的任务，需要专门的领域知识。本文对将法学硕士融入化学领域所采用的细致入微的方法进行了彻底的探索，深入探讨了这个跨学科交叉点的复杂性和创新。具体来说，我们的分析首先检查分子信息如何通过各种表示和标记化方法输入法学硕士。然后，我们根据化学法学硕士输入数据的领域和模式将化学法学硕士分为三个不同的组，并讨论为法学硕士整合这些输入的方法。此外，本文还深入研究了针对化学法学硕士的预培训目标。之后，我们探索法学硕士在化学中的多样化应用，包括其在化学任务中应用的新颖范式。最后，我们确定了有前途的研究方向，包括与化学知识的进一步整合、持续学习的进步以及模型可解释性的改进，为该领域的突破性发展铺平了道路。</li>
</ul>

<h3>Title: Few-Shot Learning on Graphs: from Meta-learning to Pre-training and  Prompting</h3>
<ul>
<li><strong>Authors: </strong>Xingtong Yu, Yuan Fang, Zemin Liu, Yuxia Wu, Zhihao Wen, Jianyuan Bo, Xinming Zhang, Steven C.H. Hoi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01440">https://arxiv.org/abs/2402.01440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01440">https://arxiv.org/pdf/2402.01440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01440]] Few-Shot Learning on Graphs: from Meta-learning to Pre-training and  Prompting(https://arxiv.org/abs/2402.01440)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to catalyze continued innovation in this field.</li>
<li><strong>摘要：</strong>图表示学习是以图为中心的任务中的关键一步，已经取得了显着的进步。早期的技术通常在端到端环境中运行，其中性能很大程度上依赖于充足标记数据的可用性。这种限制刺激了图上小样本学习的出现，其中每个任务只有几个特定于任务的标签可用。鉴于该领域的大量文献，本调查致力于综合最新进展，提供比较见解并确定未来方向。我们系统地将现有研究分为三个主要系列：元学习方法、预训练方法和混合方法，每个系列都有更细粒度的分类，以帮助读者选择方法。在每个类别中，我们分析这些方法之间的关系并比较它们的优点和局限性。最后，我们概述了图上的小样本学习的未来方向，以促进该领域的持续创新。</li>
</ul>

<h3>Title: The Queen of England is not England's Queen: On the Lack of Factual  Coherency in PLMs</h3>
<ul>
<li><strong>Authors: </strong>Paul Youssef, Jörg Schlötterer, Christin Seifert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01453">https://arxiv.org/abs/2402.01453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01453">https://arxiv.org/pdf/2402.01453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01453]] The Queen of England is not England's Queen: On the Lack of Factual  Coherency in PLMs(https://arxiv.org/abs/2402.01453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 中编码的事实知识丰富了它们的表示形式，并证明它们作为知识库的使用是合理的。以前的工作重点是通过测量 PLM 在给定主题和关系的情况下正确预测对象实体的频率来探索 PLM 的事实知识，并通过优化用于查询 PLM 的提示来改进事实检索。在这项工作中，我们考虑一个补充方面，即 PLM 中事实知识的一致性，即，在给定对象实体的初始预测的情况下，PLM 可以多久预测一次主题实体。这超出了评估 PLM 知道多少的范围，而是关注其内部知识的状态。我们的结果表明，使用手动编写、优化和释义的提示时，PLM 的一致性较低，但包含证据段落可以带来实质性的改进。这表明 PLM 无法对逆关系进行建模，需要进一步增强才能以连贯的方式处理从参数中检索事实，并被视为知识库。</li>
</ul>

<h3>Title: Integrating Large Language Models in Causal Discovery: A Statistical  Causal Approach</h3>
<ul>
<li><strong>Authors: </strong>Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01454">https://arxiv.org/abs/2402.01454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01454">https://arxiv.org/pdf/2402.01454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01454]] Integrating Large Language Models in Causal Discovery: A Statistical  Causal Approach(https://arxiv.org/abs/2402.01454)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.</li>
<li><strong>摘要：</strong>在实际统计因果发现（SCD）中，将领域专家知识作为约束嵌入到算法中被广泛认为对于创建一致的有意义的因果模型具有重要意义，尽管在系统获取背景知识方面存在公认的挑战。为了克服这些挑战，本文提出了一种新颖的因果推理方法，其中通过 LLM 和先验知识的“统计因果提示（SCP）”综合了 SCD 方法和具有大型语言模型（LLM）的基于知识的因果推理（KBCI）。 SCD 的知识增强。实验表明，GPT-4可以使LLM-KBCI的输出和带有LLM-KBCI先验知识的SCD结果接近ground true，并且如果GPT-4经过SCP，SCD结果可以进一步改善。此外，已经澄清，即使 LLM 不包含数据集信息，LLM 也可以利用其背景知识来提高 SCD。因此，所提出的方法可以解决数据集偏差和限制等挑战，说明法学硕士在改善不同科学领域的数据驱动因果推理方面的潜力。</li>
</ul>

<h3>Title: AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through  Process Feedback</h3>
<ul>
<li><strong>Authors: </strong>Jian Guan, Wei Wu, Zujie Wen, Peng Xu, Hongning Wang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01469">https://arxiv.org/abs/2402.01469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01469">https://arxiv.org/pdf/2402.01469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01469]] AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through  Process Feedback(https://arxiv.org/abs/2402.01469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets and enables AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的显着成功引发了构建语言代理来完成各种复杂任务的热潮。我们提出了 AMOR，一个基于开源法学硕士的代理框架，它利用外部知识库进行推理，并通过人类监督推理过程来适应特定领域。 AMOR 在有限状态机 (FSM) 上构建推理逻辑，通过解耦模块的自主执行和转换来解决问题。这使得人类可以向各个模块提供直接反馈，从而自然地形成过程监督。基于这个推理和反馈框架，我们通过两个阶段的微调来开发AMOR：预热和适应。前者使用从各种公共数据集自动构建的示例对 LLM 进行微调，并使 AMOR 能够在不同的知识环境中进行泛化，而后者则使用过程反馈将 AMOR 定制到特定领域。跨多个领域的广泛实验证明了 AMOR 相对于强大基线的优势，这要归功于其基于 FSM 的推理和过程反馈机制。</li>
</ul>

<h3>Title: Multi-level protein pre-training with Vabs-Net</h3>
<ul>
<li><strong>Authors: </strong>Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, Shuqi Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01481">https://arxiv.org/abs/2402.01481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01481">https://arxiv.org/pdf/2402.01481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01481]] Multi-level protein pre-training with Vabs-Net(https://arxiv.org/abs/2402.01481)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein chains to learn meaningful representations of both residues and atoms. This leads to a simple yet effective approach to learning protein representation suitable for diverse downstream tasks. Extensive experimental results on binding site prediction and function prediction tasks demonstrate our proposed pre-training approach significantly outperforms other methods. Our code will be made public.</li>
<li><strong>摘要：</strong>近年来，基于 3D 结构的预训练蛋白质模型的发展激增，代表着各种下游任务中预训练蛋白质语言模型的显着进步。然而，大多数现有的基于结构的预训练模型主要关注残基水平，即α碳原子，而忽略了侧链原子等其他原子。我们认为，在残基和原子水平上对蛋白质进行建模很重要，因为侧链原子对于许多下游任务（例如分子对接）也至关重要。然而，我们发现在预训练期间天真地组合残基和原子信息通常会失败。我们发现一个关键原因是输入中包含原子结构导致的信息泄漏，这使得残基级预训练任务变得微不足道，并导致残基表示表达能力不足。为了解决这个问题，我们在 3D 蛋白质链上引入了跨度掩模预训练策略，以学习残基和原子的有意义的表示。这导致了一种简单而有效的方法来学习适合不同下游任务的蛋白质表示。关于结合位点预测和功能预测任务的大量实验结果表明，我们提出的预训练方法明显优于其他方法。我们的代码将被公开。</li>
</ul>

<h3>Title: A Comparative Analysis of Conversational Large Language Models in  Knowledge-Based Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Phillip Schneider, Manuel Klettner, Elena Simperl, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01495">https://arxiv.org/abs/2402.01495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01495">https://arxiv.org/pdf/2402.01495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01495]] A Comparative Analysis of Conversational Large Language Models in  Knowledge-Based Text Generation(https://arxiv.org/abs/2402.01495)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, agent</a></li>
<li><strong>Abstract: </strong>Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.</li>
<li><strong>摘要：</strong>从图结构数据生成自然语言文本对于对话式信息搜索至关重要。从知识图派生的语义三元组可以为对话代理所传达的信息提供事实基础，从而成为对话代理响应的宝贵来源。这在大型语言模型的背景下尤其重要，大型语言模型为对话交互提供了巨大的潜力，但很容易产生幻觉、遗漏或产生冲突的信息。在本研究中，我们对从语义三元组生成自然语言文本的会话大语言模型进行了实证分析。我们比较了四种不同大小的大型语言模型和不同的提示技术。通过对 WebNLG 数据集进行一系列基准实验，我们分析了模型的性能并确定了生成的预测中最常见的问题。我们的研究结果表明，通过几次提示、后处理和高效的微调技术，可以显着提高大型语言模型在三重语言化方面的能力，特别是对于表现出较低零样本性能的较小模型。</li>
</ul>

<h3>Title: Code-Switched Language Identification is Harder Than You Think</h3>
<ul>
<li><strong>Authors: </strong>Laurie Burchell, Alexandra Birch, Robert P. Thompson, Kenneth Heafield</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01505">https://arxiv.org/abs/2402.01505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01505">https://arxiv.org/pdf/2402.01505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01505]] Code-Switched Language Identification is Harder Than You Think(https://arxiv.org/abs/2402.01505)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Code switching (CS) is a very common phenomenon in written and spoken communication but one that is handled poorly by many natural language processing applications. Looking to the application of building CS corpora, we explore CS language identification (LID) for corpus building. We make the task more realistic by scaling it to more languages and considering models with simpler architectures for faster inference. We also reformulate the task as a sentence-level multi-label tagging problem to make it more tractable. Having defined the task, we investigate three reasonable models for this task and define metrics which better reflect desired performance. We present empirical evidence that no current approach is adequate and finally provide recommendations for future work in this area.</li>
<li><strong>摘要：</strong>代码转换 (CS) 是书面和口头交流中一种非常常见的现象，但许多自然语言处理应用程序处理得不好。着眼于构建CS语料库的应用，我们探索了用于语料库构建的CS语言识别（LID）。我们通过将其扩展到更多语言并考虑具有更简单架构的模型来加快推理速度，从而使任务更加现实。我们还将该任务重新表述为句子级多标签标记问题，以使其更容易处理。定义任务后，我们研究了该任务的三个合理模型，并定义了更好地反映所需性能的指标。我们提供的经验证据表明，目前的方法都不够充分，并最终为该领域的未来工作提供建议。</li>
</ul>

<h3>Title: A Hybrid Strategy for Chat Transcript Summarization</h3>
<ul>
<li><strong>Authors: </strong>Pratik K. Biswas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01510">https://arxiv.org/abs/2402.01510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01510">https://arxiv.org/pdf/2402.01510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01510]] A Hybrid Strategy for Chat Transcript Summarization(https://arxiv.org/abs/2402.01510)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, agent</a></li>
<li><strong>Abstract: </strong>Text summarization is the process of condensing a piece of text to fewer sentences, while still preserving its content. Chat transcript, in this context, is a textual copy of a digital or online conversation between a customer (caller) and agent(s). This paper presents an indigenously (locally) developed hybrid method that first combines extractive and abstractive summarization techniques in compressing ill-punctuated or un-punctuated chat transcripts to produce more readable punctuated summaries and then optimizes the overall quality of summarization through reinforcement learning. Extensive testing, evaluations, comparisons, and validation have demonstrated the efficacy of this approach for large-scale deployment of chat transcript summarization, in the absence of manually generated reference (annotated) summaries.</li>
<li><strong>摘要：</strong>文本摘要是将一段文本压缩为更少的句子，同时仍保留其内容的过程。在这种情况下，聊天记录是客户（呼叫者）和代理之间的数字或在线对话的文本副本。本文提出了一种本地开发的混合方法，该方法首先结合提取和抽象摘要技术来压缩不标点或不标点的聊天记录，以生成更具可读性的标点摘要，然后通过强化学习优化摘要的整体质量。广泛的测试、评估、比较和验证已经证明了这种方法在缺乏手动生成的参考（带注释）摘要的情况下大规模部署聊天记录摘要的有效性。</li>
</ul>

<h3>Title: Mapping the Multiverse of Latent Representations</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Wayland, Corinna Coupette, Bastian Rieck</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01514">https://arxiv.org/abs/2402.01514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01514">https://arxiv.org/pdf/2402.01514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01514]] Mapping the Multiverse of Latent Representations(https://arxiv.org/abs/2402.01514)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperparameter search spaces.</li>
<li><strong>摘要：</strong>为了呼应最近通过多元宇宙分析来应对机器学习中的可靠性和鲁棒性问题的呼吁，我们提出了 PRESTO，这是一个原则框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。尽管此类模型得到了广泛采用，但对其嵌入的可变性仍然知之甚少，从而导致不必要的复杂性和不可信的表示。我们的框架使用持久同源性来表征不同机器学习方法、（超）参数配置和数据集的不同组合所产生的潜在空间，使我们能够测量它们的成对（不）相似性并对其分布进行统计推理。正如我们在理论和经验上所证明的那样，我们的管道保留了潜在表示集合的理想属性，并且可以利用它来执行敏感性分析、检测异常嵌入或高效且有效地导航超参数搜索空间。</li>
</ul>

<h3>Title: K-Level Reasoning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01521">https://arxiv.org/abs/2402.01521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01521">https://arxiv.org/pdf/2402.01521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01521]] K-Level Reasoning with Large Language Models(https://arxiv.org/abs/2402.01521)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on available historical information, which significantly improves the prediction accuracy of rivals' subsequent moves and informs more strategic decision-making. This research not only sets a robust quantitative benchmark for the assessment of dynamic reasoning but also markedly enhances the proficiency of LLMs in dynamic contexts.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 已经证明了它们在复杂推理任务中的熟练程度，但它们在动态、交互式和竞争性场景（例如业务战略和股票市场分析）中的性能仍然未被充分开发。为了弥补这一差距，我们正式探索了法学硕士在快速发展的环境中进行决策的动态推理能力。我们引入了两个基于博弈论的试点挑战，反映了现实世界动态决策的复杂性。这些挑战是明确的，可以对法学硕士的动态推理能力进行清晰、可控和精确的评估。通过大量的实验，我们发现现有的推理方法在需要 k 级思维的动态环境中往往会出现问题——这是以前的工作没有解决的一个关键概念。为了解决这个问题，我们为法学硕士提出了一种新颖的推理方法，称为“K 级推理”。该方法采用对手的视角，基于可用的历史信息递归地采用k级思维，这显着提高了对手后续行动的预测准确性，并为更多的战略决策提供信息。这项研究不仅为动态推理的评估设定了稳健的定量基准，而且显着提高了法学硕士在动态环境中的熟练程度。</li>
</ul>

<h3>Title: Decoding Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01528">https://arxiv.org/abs/2402.01528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01528">https://arxiv.org/pdf/2402.01528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01528]] Decoding Speculative Decoding(https://arxiv.org/abs/2402.01528)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be used to decide the right draft model for a given workload. Further, using our insights we design a new draft model for LLaMA-65B which can provide 30% higher throughput than existing draft models.</li>
<li><strong>摘要：</strong>推测解码是一种广泛使用的技术，可在不修改其结果的情况下加速大型语言模型 (LLM) 的推理。在 LLM 上执行推理时，推测性解码使用较小的草稿模型来生成推测令牌，然后使用目标 LLM 来验证这些草稿令牌。推测解码提供的加速很大程度上取决于草稿模型的选择。人们广泛建议选择一个草稿模型，该模型提供生成的令牌被 LLM 接受的高概率，以实现最高吞吐量。然而，我们的实验表明相反，随着生成的令牌被目标模型接受的概率增加，吞吐量会减少。为了理解这种现象，我们进行了广泛的实验来描述影响推测解码的不同因素以及这些因素如何相互作用并影响加速。根据我们的实验，我们描述了一个分析模型，可用于为给定的工作负载确定正确的草稿模型。此外，利用我们的见解，我们为 LLaMA-65B 设计了新的草案模型，其吞吐量比现有草案模型高出 30%。</li>
</ul>

<h3>Title: An Empirical Analysis of Diversity in Argument Summarization</h3>
<ul>
<li><strong>Authors: </strong>Michiel van der Meer, Piek Vossen, Catholijn M. Jonker, Pradeep K. Murukannaiah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01535">https://arxiv.org/abs/2402.01535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01535">https://arxiv.org/pdf/2402.01535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01535]] An Empirical Analysis of Diversity in Argument Summarization(https://arxiv.org/abs/2402.01535)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.</li>
<li><strong>摘要：</strong>提出高层次的论点是促进在线社会讨论参与的一项关键任务。当前的论点总结方法忽略了这项任务的一个重要方面——捕捉多样性——这对于容纳多种观点很重要。我们介绍多样性的三个方面：观点、注释者和来源。我们评估了一项名为“关键点分析”的流行论点总结任务的方法，该任务显示了这些方法如何努力（1）代表少数人共享的论点，（2）处理来自不同来源的数据，以及（3）与人类的主观性保持一致- 提供注释。我们发现通用 LLM 和专用 KPA 模型都表现出这种行为，但具有互补的优势。此外，我们观察到训练数据的多样化可能会改善泛化。解决论点总结中的多样性需要采取多种策略来处理主观性。</li>
</ul>

<h3>Title: Adaptive Optimization for Prediction with Missing Data</h3>
<ul>
<li><strong>Authors: </strong>Dimitris Bertsimas, Arthur Delarue, Jean Pauphilet</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01543">https://arxiv.org/abs/2402.01543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01543">https://arxiv.org/pdf/2402.01543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01543]] Adaptive Optimization for Prediction with Missing Data(https://arxiv.org/abs/2402.01543)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.</li>
<li><strong>摘要：</strong>在针对缺失条目的数据训练预测模型时，最广泛使用和通用的方法是管道技术，我们首先估算缺失条目，然后计算预测。在本文中，我们将缺失数据的预测视为两阶段自适应优化问题，并提出了一类新的模型，即自适应线性回归模型，其中回归系数适应观察到的特征集。我们表明，一些自适应线性回归模型相当于同时而不是顺序学习插补规则和下游线性回归模型。我们利用这种联合估算然后回归的解释将我们的框架推广到非线性模型。在数据绝对不会随机丢失的情况下，我们的方法将样本外准确率提高了 2-10%。</li>
</ul>

<h3>Title: Privacy-Preserving Distributed Learning for Residential Short-Term Load  Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yi Dong, Yingjie Wang, Mariana Gama, Mustafa A. Mustafa, Geert Deconinck, Xiaowei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC, cs.MA, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01546">https://arxiv.org/abs/2402.01546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01546">https://arxiv.org/pdf/2402.01546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01546]] Privacy-Preserving Distributed Learning for Residential Short-Term Load  Forecasting(https://arxiv.org/abs/2402.01546)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy. Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security. While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks. To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage. However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenarios where one or more sub-centers are unavailable. To address these challenges, we introduce a Markovian Switching-based distributed training framework, the convergence of which is substantiated through rigorous theoretical analysis. The Distributed Markovian Switching (DMS) topology shows strong robustness towards the poisoning attacks as well. Case studies employing real-world power system load data validate the efficacy of our proposed algorithm. It not only significantly minimizes communication complexity but also maintains accuracy levels comparable to traditional FL methods, thereby enhancing the scalability of our load forecasting algorithm.</li>
<li><strong>摘要：</strong>在电力系统领域，住宅用户越来越多地参与负荷预测应用，加剧了对数据隐私的担忧。具体来说，负载数据可能会无意中泄露住宅用户的日常生活，从而对其财产安全构成风险。虽然联邦学习（FL）已被用来通过在不交换原始数据的情况下进行模型训练来保护用户隐私，但这些 FL 模型在新兴攻击技术方面表现出了漏洞，例如梯度深度泄漏和中毒攻击。为了解决这些问题，我们首先采用安全聚合（SecAgg）算法，该算法利用多方计算加密技术来减轻梯度泄漏的风险。然而，SecAgg的引入需要部署额外的分中心服务器来执行多方计算协议，从而增加了计算复杂度并降低了系统的鲁棒性，尤其是在一个或多个分中心不可用的情况下。为了应对这些挑战，我们引入了基于马尔可夫切换的分布式训练框架，其收敛性通过严格的理论分析得到证实。分布式马尔可夫交换（DMS）拓扑对于中毒攻击也表现出很强的鲁棒性。使用现实世界电力系统负载数据的案例研究验证了我们提出的算法的有效性。它不仅显着降低了通信复杂性，而且保持了与传统 FL 方法相当的精度水平，从而增强了负载预测算法的可扩展性。</li>
</ul>

<h3>Title: Automating Sound Change Prediction for Phylogenetic Inference: A  Tukanoan Case Study</h3>
<ul>
<li><strong>Authors: </strong>Kalvin Chang, Nathaniel R. Robinson, Anna Cai, Ting Chen, Annie Zhang, David R. Mortensen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01582">https://arxiv.org/abs/2402.01582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01582">https://arxiv.org/pdf/2402.01582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01582]] Automating Sound Change Prediction for Phylogenetic Inference: A  Tukanoan Case Study(https://arxiv.org/abs/2402.01582)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes. We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it comparably effective to sound laws from expert annotation. Our code is publicly available at https://github.com/cmu-llab/aiscp.</li>
<li><strong>摘要：</strong>我们描述了一组新方法来部分自动化语言系统发育推理，给定（1）同源集及其各自的原型和声音规律，（2）从音素到其发音特征的映射以及（3）声音变化的类型学数据库。我们根据这些声音变化数据训练神经网络，以加权音素之间的发音距离，并预测历史原型与其现代后代之间的中间声音变化步骤，在基于简约的系统发育推理算法中取代语言专家。在我们对图卡诺语言的最佳实验中，该方法生成的树与使用专家注释的树的广义四重距离为 0.12，这比其他半自动基线有显着改进。我们讨论了我们的神经方法和基于简约的树预测的潜在优点和缺点。我们还尝试使用最小泛化学习器来自动归纳声音规律，发现它与专家注释的声音规律相当有效。我们的代码可在 https://github.com/cmu-llab/aiscp 上公开获取。</li>
</ul>

<h3>Title: TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent  Constitution</h3>
<ul>
<li><strong>Authors: </strong>Wenyue Hua, Xianjun Yang, Zelong Li, Cheng Wei, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01586">https://arxiv.org/abs/2402.01586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01586">https://arxiv.org/pdf/2402.01586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01586]] TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent  Constitution(https://arxiv.org/abs/2402.01586)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, code, agent</a></li>
<li><strong>Abstract: </strong>The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficacy as a safe agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.</li>
<li><strong>摘要：</strong>法学硕士代理人的出现引起了相当大的关注，但他们的可信度仍然是一个尚未得到充分探索的领域。由于代理可以直接与物理环境交互，因此其可靠性和安全性至关重要。本文提出了一种基于 Agent-Constitution 的代理框架 TrustAgent，这是对提高基于 LLM 的代理的可信度安全维度的初步研究。该框架由三重策略组成：预先规划策略，在计划生成之前将安全知识注入模型；规划中策略，在计划生成过程中增强安全性；规划后策略，通过规划后检查确保安全。通过实验分析，我们展示了这些方法如何通过识别和预防潜在危险来有效提高法学硕士代理人的安全性。此外，我们还探讨了安全性与有用性之间以及模型的推理能力与其作为安全代理的功效之间的复杂关系。本文强调了将安全意识和可信度融入到基于 LLM 的代理的设计和部署中的必要性，不仅是为了提高它们的性能，而且是为了确保它们负责任地集成到以人为中心的环境中。数据和代码可在 https://github.com/agiresearch/TrustAgent 获取。</li>
</ul>

<h3>Title: Towards Sustainable Workplace Mental Health: A Novel Approach to Early  Intervention and Support</h3>
<ul>
<li><strong>Authors: </strong>David W. Vinson, Mihael Arcan, David-Paul Niland, Fionn Delahunty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01592">https://arxiv.org/abs/2402.01592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01592">https://arxiv.org/pdf/2402.01592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01592]] Towards Sustainable Workplace Mental Health: A Novel Approach to Early  Intervention and Support(https://arxiv.org/abs/2402.01592)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, rag</a></li>
<li><strong>Abstract: </strong>Employee well-being is a critical concern in the contemporary workplace, as highlighted by the American Psychological Association's 2021 report, indicating that 71% of employees experience stress or tension. This stress contributes significantly to workplace attrition and absenteeism, with 61% of attrition and 16% of sick days attributed to poor mental health. A major challenge for employers is that employees often remain unaware of their mental health issues until they reach a crisis point, resulting in limited utilization of corporate well-being benefits. This research addresses this challenge by presenting a groundbreaking stress detection algorithm that provides real-time support preemptively. Leveraging automated chatbot technology, the algorithm objectively measures mental health levels by analyzing chat conversations, offering personalized treatment suggestions in real-time based on linguistic biomarkers. The study explores the feasibility of integrating these innovations into practical learning applications within real-world contexts and introduces a chatbot-style system integrated into the broader employee experience platform. This platform, encompassing various features, aims to enhance overall employee well-being, detect stress in real time, and proactively engage with individuals to improve support effectiveness, demonstrating a 22% increase when assistance is provided early. Overall, the study emphasizes the importance of fostering a supportive workplace environment for employees' mental health.</li>
<li><strong>摘要：</strong>正如美国心理协会 2021 年报告所强调的那样，员工福祉是当代工作场所的一个关键问题，表明 71% 的员工感到压力或紧张。这种压力在很大程度上导致了工作场所的人员流失和缺勤，其中 61% 的人员流失和 16% 的病假归因于心理健康状况不佳。雇主面临的一个主要挑战是，员工往往直到达到危机点才意识到自己的心理健康问题，导致企业福利福利的利用有限。这项研究通过提出一种突破性的压力检测算法来解决这一挑战，该算法可以预先提供实时支持。该算法利用自动化聊天机器人技术，通过分析聊天对话来客观地测量心理健康水平，并根据语言生物标记实时提供个性化的治疗建议。该研究探讨了将这些创新整合到现实环境中的实际学习应用中的可行性，并引入了集成到更广泛的员工体验平台中的聊天机器人式系统。该平台包含各种功能，旨在提高员工的整体福祉，实时检测压力，并主动与个人互动以提高支持效率，早期提供帮助时可提高 22%。总体而言，该研究强调了为员工心理健康营造支持性工作环境的重要性。</li>
</ul>

<h3>Title: Foundation Model Sherpas: Guiding Foundation Models through Knowledge  and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Debarun Bhattacharjya, Junkyu Lee, Don Joven Agravante, Balaji Ganesan, Radu Marinescu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01602">https://arxiv.org/abs/2402.01602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01602">https://arxiv.org/pdf/2402.01602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01602]] Foundation Model Sherpas: Guiding Foundation Models through Knowledge  and Reasoning(https://arxiv.org/abs/2402.01602)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, agent</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction protocols, highlighting the nature and extent of involvement of the various agent roles. The proposed framework provides guidance for future directions to further realize the power of FMs in practical AI systems.</li>
<li><strong>摘要：</strong>大型语言模型等基础模型（FM）在各种任务中表现出卓越的性能，彻底改变了人工智能领域。然而，它们表现出许多限制，阻碍了它们在许多现实世界系统中的广泛采用，而这些系统通常需要更高的可信度和可用性标准。由于 FM 使用损失函数进行训练，旨在以自我监督的方式重建训练语料库，因此无法保证模型的输出符合用户对当前特定任务的偏好。在这篇调查论文中，我们提出了一个概念框架，它封装了智能体与 FM 交互的不同模式，并指导它们适当地完成一组任务，特别是通过知识增强和推理。我们的框架阐明了代理角色类别，例如更新底层 FM、协助提示 FM 以及评估 FM 输出。我们还将几种最先进的方法分类为代理交互协议，突出了各种代理角色参与的性质和程度。所提出的框架为未来的方向提供了指导，以进一步发挥 FM 在实际 AI 系统中的威力。</li>
</ul>

<h3>Title: Contingency Analysis of a Grid of Connected EVs for Primary Frequency  Control of an Industrial Microgrid Using Efficient Control Scheme</h3>
<ul>
<li><strong>Authors: </strong>J.N. Sabhahit, S.S. Solanke, V.K. Jadoun, H. Malik, F.P. García Márquez, J.M. Pinar-Pérez</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01608">https://arxiv.org/abs/2402.01608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01608">https://arxiv.org/pdf/2402.01608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01608]] Contingency Analysis of a Grid of Connected EVs for Primary Frequency  Control of an Industrial Microgrid Using Efficient Control Scheme(https://arxiv.org/abs/2402.01608)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>After over a century of internal combustion engines ruling the transport sector, electric vehicles appear to be on the verge of gaining traction due to a slew of advantages, including lower operating costs and lower CO2 emissions. By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are utilized as load) approach, EVs can operate as both a load and a source. Primary frequency regulation and congestion management are two essential characteristics of this technology that are added to an industrial microgrid. Industrial Microgrids are made up of different energy sources such as wind farms and PV farms, storage systems, and loads. EVs have gained a lot of interest as a technique for frequency management because of their ability to regulate quickly. Grid reliability depends on this quick reaction. Different contingency, state of charge of the electric vehicles, and a varying number of EVs in an EV fleet are considered in this work, and a proposed control scheme for frequency management is presented. This control scheme enables bidirectional power flow, allowing for primary frequency regulation during the various scenarios that an industrial microgrid may encounter over the course of a 24-h period. The presented controller will provide dependable frequency regulation support to the industrial microgrid during contingencies, as will be demonstrated by simulation results, achieving a more reliable system. However, simulation results will show that by increasing a number of the EVs in a fleet for the Vehicle-to-Grid approach, an industrial microgrid\'s frequency can be enhanced even further.</li>
<li><strong>摘要：</strong>在内燃机统治交通行业一个多世纪之后，电动汽车由于具有一系列优势，包括较低的运营成本和较低的二氧化碳排放量，似乎即将获得牵引力。通过使用车辆到电网（如果使用电动汽车 (EV) 作为负载，则为电网到车辆）方法，电动汽车可以同时作为负载和电源运行。一次频率调节和拥塞管理是该技术添加到工业微电网的两个基本特征。工业微电网由风电场、光伏发电场等不同能源、存储系统和负载组成。由于电动汽车具有快速调节的能力，因此作为频率管理技术而引起了广泛关注。电网的可靠性取决于这种快速反应。这项工作考虑了不同的意外情况、电动汽车的充电状态以及电动汽车车队中不同数量的电动汽车，并提出了一种建议的频率管理控制方案。该控制方案可实现双向功率流，允许在工业微电网在 24 小时内可能遇到的各种情况下进行一次频率调节。正如仿真结果所证明的那样，所提出的控制器将在突发事件期间为工业微电网提供可靠的频率调节支持，从而实现更可靠的系统。然而，模拟结果将表明，通过增加车辆到电网方法车队中电动汽车的数量，工业微电网的频率可以进一步提高。</li>
</ul>

<h3>Title: Nomic Embed: Training a Reproducible Long Context Text Embedder</h3>
<ul>
<li><strong>Authors: </strong>Zach Nussbaum, John X. Morris, Brandon Duderstadt, Andriy Mulyar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01613">https://arxiv.org/abs/2402.01613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01613">https://arxiv.org/pdf/2402.01613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01613]] Nomic Embed: Training a Reproducible Long Context Text Embedder(https://arxiv.org/abs/2402.01613)</code><input type="text"></li>
<li><strong>Keywords: </strong>long context, code</a></li>
<li><strong>Abstract: </strong>This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors</li>
<li><strong>摘要：</strong>该技术报告描述了 nomic-embed-text-v1 的训练，这是第一个完全可复制、开源、开放权重、开放数据、8192 上下文长度的英文文本嵌入模型，其性能优于 OpenAI Ada-002 和 OpenAI text- embedding-3-small 适用于短上下文和长上下文任务。我们根据 Apache 2 许可证发布训练代码和模型权重。与其他开源模型相比，我们发布了一个包含 2.35 亿个精选文本对的训练数据加载器，可以完全复制 nomic-embed-text-v1。您可以在 https://github.com/nomic-ai/contrastors 找到复制模型的代码和数据</li>
</ul>

<h3>Title: L2G2G: a Scalable Local-to-Global Network Embedding with Graph  Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Ruikang Ouyang, Andrew Elliott, Stratis Limnios, Mihai Cucuringu, Gesine Reinert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01614">https://arxiv.org/abs/2402.01614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01614">https://arxiv.org/pdf/2402.01614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01614]] L2G2G: a Scalable Local-to-Global Network Embedding with Graph  Autoencoders(https://arxiv.org/abs/2402.01614)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>For analysing real-world networks, graph representation learning is a popular tool. These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. While GAEs tend to be fairly accurate, they suffer from scalability issues. For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability. This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs. It also benefits from the decoder computing an only local patch loss. Hence, aligning the local embeddings in each epoch utilises more information from the graph than a single post-training alignment does, while maintaining scalability. We illustrate on synthetic benchmarks, as well as real-world examples, that L2G2G achieves higher accuracy than the standard Local2Global approach and scales efficiently on the larger data sets. We find that for large and dense networks, it even outperforms the slow, but assumed more accurate, GAEs.</li>
<li><strong>摘要：</strong>对于分析现实世界的网络，图表示学习是一种流行的工具。这些方法，例如图自动编码器（GAE），通常依赖于低维表示，也称为嵌入，它是通过最小化损失函数获得的；这些嵌入与解码器一起用于下游任务，例如节点分类和边缘预测。虽然 GAE 往往相当准确，但它们存在可扩展性问题。为了提高速度，结合了基于特征向量同步的图块嵌入的 Local2Global 方法被证明速度快且精度高。在这里，我们提出了 L2G2G，一种 Local2Global 方法，可以在不牺牲可扩展性的情况下提高 GAE 的准确性。这一改进是通过在训练 GAE 时动态同步潜在节点表示来实现的。它还受益于解码器仅计算局部补丁丢失。因此，在每个时期中对齐局部嵌入比单个训练后对齐利用了更多的图中信息，同时保持了可扩展性。我们通过综合基准以及现实世界的示例来说明，L2G2G 比标准 Local2Global 方法实现了更高的准确性，并且可以在更大的数据集上有效地扩展。我们发现，对于大型且密集的网络，它甚至优于缓慢但假设更准确的 GAE。</li>
</ul>

<h3>Title: Style Vectors for Steering Generative Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Kai Konen, Sophie Jentzsch, Diaoulé Diallo, Peer Schütt, Oliver Bensch, Roxanne El Baff, Dominik Opitz, Tobias Hecking</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01618">https://arxiv.org/abs/2402.01618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01618">https://arxiv.org/pdf/2402.01618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01618]] Style Vectors for Steering Generative Large Language Model(https://arxiv.org/abs/2402.01618)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.</li>
<li><strong>摘要：</strong>这项研究探索了通过在文本生成过程中将风格向量添加到隐藏层的激活中来将大型语言模型（LLM）的输出转向特定风格（例如情绪、情绪或写作风格）的策略。我们表明，与更复杂的基于训练的方法相比，可以根据特定样式的输入文本的记录层激活来简单地计算样式向量。通过一系列实验，我们证明了激活工程使用此类样式向量以细致入微且可参数化的方式影响生成文本的样式的有效性，这与提示工程不同。所提出的研究是开发更具适应性和更有效的人工智能交互系统的重要一步。</li>
</ul>

<h3>Title: KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce  Programs over Low-resourced Knowledge Bases</h3>
<ul>
<li><strong>Authors: </strong>Jiajie Zhang, Shulin Cao, Linmei Hu, Ling Feng, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01619">https://arxiv.org/abs/2402.01619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01619">https://arxiv.org/pdf/2402.01619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01619]] KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce  Programs over Low-resourced Knowledge Bases(https://arxiv.org/abs/2402.01619)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB. Experiments on five heterogeneous KBQA datasets show that KB-Plugin achieves better or comparable performance with 25$\times$ smaller backbone LLM compared to SoTA PI methods for low-resourced KBs, and even approaches the performance of supervised methods. Our code and data are available at https://github.com/THU-KEG/KB-Plugin.</li>
<li><strong>摘要：</strong>程序归纳 (PI) 已成为使用知识库 (KB) 帮助大型语言模型 (LLM) 回答复杂的知识密集型问题的有前途的范例。尽管如此，PI 通常依赖于大量并行的问题程序对来使法学硕士了解给定知识库的模式，因此对于许多缺乏注释数据的资源匮乏的知识库来说是一个挑战。为此，我们提出了 KB-Plugin，这是一个即插即用的框架，使 LLM 能够在任何资源匮乏的 KB 上诱导程序。首先，KB-Plugin采用自监督学习将给定知识库的详细模式信息编码为可插拔模块，即模式插件。其次，KB-Plugin利用来自资源丰富的知识库的丰富注释数据来训练另一个可插入模块，即PI插件，它可以帮助LLM从任何知识库的模式插件中提取与问题相关的模式信息，并利用这些信息来诱导程序超过这个知识库。在五个异构 KBQA 数据集上进行的实验表明，与低资源 KB 的 SoTA PI 方法相比，KB-Plugin 通过缩小 25 倍的骨干 LLM 实现了更好或相当的性能，甚至接近监督方法的性能。我们的代码和数据可在 https://github.com/THU-KEG/KB-Plugin 获取。</li>
</ul>

<h3>Title: MAGDi: Structured Distillation of Multi-Agent Interaction Graphs  Improves Reasoning in Smaller Language Models</h3>
<ul>
<li><strong>Authors: </strong>Justin Chih-Yao Chen, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01620">https://arxiv.org/abs/2402.01620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01620">https://arxiv.org/pdf/2402.01620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01620]] MAGDi: Structured Distillation of Multi-Agent Interaction Graphs  Improves Reasoning in Smaller Language Models(https://arxiv.org/abs/2402.01620)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, agent</a></li>
<li><strong>Abstract: </strong>Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency - an inference technique that relies on model diversity.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 代理之间的多代理交互在各种推理任务上显示出重大改进。然而，这些涉及多轮多个模型的长时间生成，因此成本高昂。此外，这些多智能体方法无法提供最终的单一模型来进行有效的推理。为了解决这个问题，我们引入了 MAGDi，这是一种将多个 LLM 之间的推理交互结构化蒸馏为更小的 LM 的新方法。 MAGDi 通过将多智能体交互表示为图形、使用图形编码器增强基础学生模型以及使用三个目标函数提炼知识来教授较小的模型：下一个标记预测、正确和不正确推理之间的对比损失以及基于图形的模型目标是对交互结构进行建模。对七个广泛使用的常识和数学推理基准的实验表明，MAGDi 提高了较小模型的推理能力，优于从单个教师和多个教师中提取的几种方法。此外，MAGDi 的效率也比其教师高出一个数量级。我们进行了广泛的分析，表明 MAGDi (1) 增强了对域外任务的通用性，(2) 与基础学生模型的规模和强度呈正相关，(3) 获得了更大的改进（通过我们的多教师培训）在应用自我一致性时 - 一种依赖于模型多样性的推理技术。</li>
</ul>

<h3>Title: Stochastic Two Points Method for Deep Model Zeroth-order Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yijiang Pang, Jiayu Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01621">https://arxiv.org/abs/2402.01621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01621">https://arxiv.org/pdf/2402.01621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01621]] Stochastic Two Points Method for Deep Model Zeroth-order Optimization(https://arxiv.org/abs/2402.01621)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms standard methods across various model types and scales, with 2 $\times$ speed-up in training over most conducted tasks.</li>
<li><strong>摘要：</strong>大型基础模型，例如大型语言模型，在各种应用场景中都表现得异常出色。由于硬件预算或缺乏反向传播的访问权限，构建或完全微调如此大的模型通常是令人望而却步的。零阶方法为解决这一挑战提供了一个有希望的方向，其中只需要前向传递来更新模型。本文介绍了无梯度范围内的有效随机两点（S2P）方法。我们提出了在一般和宽松平滑假设下 S2P 的理论收敛特性。通过利用我们新的收敛特性，该理论特性还揭示了更快、更稳定的 S2P 变体，即加速 S2P (AS2P)，该收敛特性可以更好地代表训练中深度模型的动态。我们全面的实证结果表明，AS2P 在优化大型深度模型（包括语言模型）的目标方面非常有效，并且在各种模型类型和规模上都优于标准方法，在大多数执行的任务中训练速度提高了 2 美元\倍$。</li>
</ul>

<h3>Title: TravelPlanner: A Benchmark for Real-World Planning with Language Agents</h3>
<ul>
<li><strong>Authors: </strong>Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01622">https://arxiv.org/abs/2402.01622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01622">https://arxiv.org/pdf/2402.01622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01622]] TravelPlanner: A Benchmark for Real-World Planning with Language Agents(https://arxiv.org/abs/2402.01622)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.</li>
<li><strong>摘要：</strong>自人工智能概念诞生以来，规划一直是人工智能核心追求的一部分，但早期的人工智能代理主要关注受限环境，因为缺乏人类水平规划所需的许多认知基础。最近，由大型语言模型（LLM）支持的语言代理表现出了有趣的功能，例如工具使用和推理。这些语言代理是否能够在先前人工智能代理无法企及的更复杂的环境中进行规划？为了推进这项调查，我们提出了 TravelPlanner，这是一个新的规划基准，专注于旅行规划（一种常见的现实世界规划场景）。它提供了丰富的沙箱环境、用于访问近 400 万条数据记录的各种工具以及 1,225 个精心策划的规划意图和参考计划。综合评估表明，当前的语言智能体还没有能力处理如此复杂的规划任务——即使是 GPT-4 也只能取得 0.6% 的成功率。语言代理努力完成任务、使用正确的工具收集信息或跟踪多种约束。然而，我们注意到，语言智能体解决如此复杂问题的可能性本身就是一个不平凡的进步。 TravelPlanner 为未来的语言代理提供了一个具有挑战性但有意义的测试平台。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
