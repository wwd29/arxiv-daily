<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-22</h1>
<h3>Title: Question-Based Retrieval using Atomic Units for Enterprise RAG</h3>
<ul>
<li><strong>Authors: </strong>Vatsal Raina, Mark Gales</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12363">https://arxiv.org/abs/2405.12363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12363">https://arxiv.org/pdf/2405.12363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12363]] Question-Based Retrieval using Atomic Units for Enterprise RAG(https://arxiv.org/abs/2405.12363)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Enterprise retrieval augmented generation (RAG) offers a highly flexible framework for combining powerful large language models (LLMs) with internal, possibly temporally changing, documents. In RAG, documents are first chunked. Relevant chunks are then retrieved for a specific user query, which are passed as context to a synthesizer LLM to generate the query response. However, the retrieval step can limit performance, as incorrect chunks can lead the synthesizer LLM to generate a false response. This work proposes a zero-shot adaptation of standard dense retrieval steps for more accurate chunk recall. Specifically, a chunk is first decomposed into atomic statements. A set of synthetic questions are then generated on these atoms (with the chunk as the context). Dense retrieval involves finding the closest set of synthetic questions, and associated chunks, to the user query. It is found that retrieval with the atoms leads to higher recall than retrieval with chunks. Further performance gain is observed with retrieval using the synthetic questions generated over the atoms. Higher recall at the retrieval step enables higher performance of the enterprise LLM using the RAG pipeline.</li>
<li><strong>摘要：</strong>企业检索增强生成 (RAG) 提供了一个高度灵活的框架，用于将强大的大型语言模型 (LLM) 与内部（可能会随时间变化）文档相结合。在 RAG 中，文档首先被分块。然后针对特定用户查询检索相关块，并将其作为上下文传递给合成器 LLM 以生成查询响应。然而，检索步骤可能会限制性能，因为不正确的块可能会导致合成器 LLM 生成错误响应。这项工作提出了标准密集检索步骤的零样本适应，以实现更准确的块召回。具体来说，块首先被分解为原子语句。然后在这些原子上生成一组综合问题（以块作为上下文）。密集检索涉及查找与用户查询最接近的一组综合问题和相关块。研究发现，使用原子检索比使用块检索具有更高的召回率。通过使用在原子上生成的综合问题进行检索，可以观察到进一步的性能提升。检索步骤中更高的召回率可以提高使用 RAG 管道的企业 LLM 的性能。</li>
</ul>

<h3>Title: Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Charles O'Neill, Thang Bui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12522">https://arxiv.org/abs/2405.12522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12522">https://arxiv.org/pdf/2405.12522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12522]] Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models(https://arxiv.org/abs/2405.12522)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters. We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations. By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds. Notably, we require only 5-10 text examples for each task to learn robust representations. Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models.</li>
<li><strong>摘要：</strong>本文介绍了一种使用离散稀疏自动编码器在大型语言模型中发现可解释电路的高效且稳健的方法。我们的方法解决了现有技术的关键限制，即计算复杂性和对超参数的敏感性。我们建议在精心设计的正例和负例上训练稀疏自动编码器，其中模型只能正确预测正例的下一个标记。我们假设，学习到的注意力头输出表示会在头进行特定计算时发出信号。通过将学习到的表示离散化为整数代码，并测量每个头的正例所特有的代码之间的重叠，我们可以直接识别电路中涉及的注意力头，而不需要昂贵的消融或架构修改。在三个经过充分研究的任务上——间接对象识别、大于比较和文档字符串完成——与最先进的基线相比，所提出的方法在恢复地面实况电路方面实现了更高的精度和召回率，同时将运行时间缩短了数小时至秒。值得注意的是，每项任务我们只需要 5-10 个文本示例即可学习稳健的表示。我们的研究结果强调了离散稀疏自动编码器在可扩展和高效的机械可解释性方面的前景，为分析大型语言模型的内部工作原理提供了新的方向。</li>
</ul>

<h3>Title: SirLLM: Streaming Infinite Retentive LLM</h3>
<ul>
<li><strong>Authors: </strong>Yao Yao, Zuchao Li, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12528">https://arxiv.org/abs/2405.12528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12528">https://arxiv.org/pdf/2405.12528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12528]] SirLLM: Streaming Infinite Retentive LLM(https://arxiv.org/abs/2405.12528)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs' pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model's long-term memory capabilities. Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, "A sir could forget himself," but SirLLM never does! Our code is publicly available at this https URL</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在各个领域中变得越来越普遍，它们处理任意长度的输入并保持一定程度的记忆的能力变得至关重要。然而，过长文本的一次性输入是有限的，研究表明，当输入长度超过LLM预先训练的文本长度时，文本生成能力会急剧下降。此外，简单地延长预训练文本的长度是不切实际的，因为获取长文本数据很困难，而且这会给法学硕士带来大量的内存消耗成本。最近的努力采用了流式输入来减轻过长文本输入的压力，但这种方法会严重损害模型的长期记忆能力。受这一挑战的推动，我们引入了 Streaming Infinite Retentive LLM (SirLLM)，它允许 LLM 在无限长度的对话期间保持更长的内存，而无需进行微调。 SirLLM利用Token Entropy指标和记忆衰减机制来过滤关键短语，赋予LLM持久且灵活的记忆。我们设计了三个不同的任务并构建了三个数据集来从不同角度衡量 SirLLM 的有效性：（1）DailyDialog； (2) 杂货购物； （3）剪刀石头布。我们的实验结果有力地证明了 SirLLM 可以在不同的 LLM 和任务中实现稳定且显着的改进，有力地证明了其有效性。谈话时，“先生可能会忘记自己”，但 SirLLM 却绝不会这样做！我们的代码可通过此 https URL 公开获取</li>
</ul>

<h3>Title: PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Dongjie Yang, XiaoDong Han, Yan Gao, Yao Hu, Shilin Zhang, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12532">https://arxiv.org/abs/2405.12532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12532">https://arxiv.org/pdf/2405.12532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12532]] PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference(https://arxiv.org/abs/2405.12532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出了卓越的理解能力，但在推理过程中面临 GPU 内存使用的挑战，阻碍了它们在聊天机器人等实时应用程序中的可扩展性。为了加速推理，我们将计算出的键和值（KV 缓存）存储在 GPU 内存中。现有方法研究 KV 缓存压缩，通过修剪预先计算的 KV 缓存来减少内存。然而，他们忽略了层与层之间的层间依赖以及预计算中巨大的内存消耗。为了探索这些缺陷，我们发现影响后代的关键键和值的数量逐层减少，我们可以通过注意力权重的一致性来提取它们。基于这些发现，我们提出了 PyramidInfer，一种通过分层保留关键上下文来压缩 KV 缓存的方法。 PyramidInfer 通过计算更少的键和值来节省大量内存，而不会牺牲性能。实验结果表明，与 Accelerate 相比，PyramidInfer 的吞吐量提高了 2.2 倍，KV 缓存中的 GPU 内存减少了 54% 以上。</li>
</ul>

<h3>Title: Mining the Explainability and Generalization: Fact Verification Based on Self-Instruction</h3>
<ul>
<li><strong>Authors: </strong>Guangyao Lu, Yulin Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12579">https://arxiv.org/abs/2405.12579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12579">https://arxiv.org/pdf/2405.12579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12579]] Mining the Explainability and Generalization: Fact Verification Based on Self-Instruction(https://arxiv.org/abs/2405.12579)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Fact-checking based on commercial LLMs has become mainstream. Although these methods offer high explainability, it falls short in accuracy compared to traditional fine-tuning approaches, and data security is also a significant concern. In this paper, we propose a self-instruction based fine-tuning approach for fact-checking that balances accuracy and explainability. Our method consists of Data Augmentation and Improved DPO fine-tuning. The former starts by instructing the model to generate both positive and negative explanations based on claim-evidence pairs and labels, then sampling the dataset according to our customized difficulty standards. The latter employs our proposed improved DPO to fine-tune the model using the generated samples. We fine-tune the smallest-scale LLaMA-7B model and evaluate it on the challenging fact-checking datasets FEVEROUS and HOVER, utilizing four fine-tuning methods and three few-shot learning methods for comparison. The experiments demonstrate that our approach not only retains accuracy comparable to, or even surpassing, traditional fine-tuning methods, but also generates fluent explanation text. Moreover, it also exhibit high generalization performance. Our method is the first to leverage self-supervised learning for fact-checking and innovatively combines contrastive learning and improved DPO in fine-tuning LLMs, as shown in the experiments.</li>
<li><strong>摘要：</strong>基于商业法学硕士的事实核查已成为主流。尽管这些方法具有很高的可解释性，但与传统的微调方法相比，其准确性较差，而且数据安全也是一个重要问题。在本文中，我们提出了一种基于自我指令的微调方法，用于平衡准确性和可解释性的事实检查。我们的方法包括数据增强和改进的 DPO 微调。前者首先指示模型根据主张证据对和标签生成正面和负面解释，然后根据我们定制的难度标准对数据集进行采样。后者采用我们提出的改进的 DPO，使用生成的样本对模型进行微调。我们对最小规模的 LLaMA-7B 模型进行微调，并在具有挑战性的事实检查数据集 FEVEROUS 和 HOVER 上对其进行评估，利用四种微调方法和三种小样本学习方法进行比较。实验表明，我们的方法不仅保持了与传统微调方法相当甚至超越的准确性，而且还生成了流畅的解释文本。此外，它还表现出很高的泛化性能。正如实验所示，我们的方法是第一个利用自我监督学习进行事实检查的方法，并创新性地结合对比学习和改进的 DPO 来微调法学硕士。</li>
</ul>

<h3>Title: Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression</h3>
<ul>
<li><strong>Authors: </strong>Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Yipeng Ma, Tao Wang, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12591">https://arxiv.org/abs/2405.12591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12591">https://arxiv.org/pdf/2405.12591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12591]] Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression(https://arxiv.org/abs/2405.12591)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Key-value~(KV) caching is an important technique to accelerate the inference of large language models~(LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce \textbf{DecoQuant}, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a $\sim$75\% reduction in memory footprint while maintaining comparable generation quality.</li>
<li><strong>摘要：</strong>键值（KV）缓存是加速大型语言模型（LLM）推理的重要技术，但会产生大量内存开销。为了压缩 KV 缓存的大小，现有方法通常会牺牲精度或需要额外的数据进行校准，从而限制了它们在 LLM 部署中的实用性。在本文中，我们引入了 \textbf{DecoQuant}，一种基于张量分解方法的新型无数据低位量化技术，以有效压缩 KV 缓存。我们的核心思想是通过进行张量分解来调整原始矩阵的离群分布，从而将量化困难从矩阵迁移到分解的局部张量。特别地，我们发现异常值主要集中在小的局部张量上，而大的张量往往具有较窄的值范围。基于这一发现，我们建议对大张量应用低位量化，同时保持小张量的高精度表示。此外，我们利用所提出的量化方法来压缩 LLM 的 KV 缓存，以加速推理并开发专为 DecoQuant 定制的高效反量化内核。通过大量的实验，DecoQuant 展示了显着的效率提升，内存占用量减少了 $\sim$75\%，同时保持了相当的生成质量。</li>
</ul>

<h3>Title: Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming</h3>
<ul>
<li><strong>Authors: </strong>Jiaxu Liu, Xiangyu Yin, Sihao Wu, Jianhong Wang, Meng Fang, Xinping Yi, Xiaowei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12604">https://arxiv.org/abs/2405.12604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12604">https://arxiv.org/pdf/2405.12604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12604]] Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming(https://arxiv.org/abs/2405.12604)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>With the proliferation of red-teaming strategies for Large Language Models (LLMs), the deficiency in the literature about improving the safety and robustness of LLM defense strategies is becoming increasingly pronounced. This paper introduces the LLM-based \textbf{sentinel} model as a plug-and-play prefix module designed to reconstruct the input prompt with just a few ($<30$) additional tokens, effectively reducing toxicity in responses from target LLMs. The sentinel model naturally overcomes the \textit{parameter inefficiency} and \textit{limited model accessibility} for fine-tuning large target models. We employ an interleaved training regimen using Proximal Policy Optimization (PPO) to optimize both red team and sentinel models dynamically, incorporating a value head-sharing mechanism inspired by the multi-agent centralized critic to manage the complex interplay between agents. Our extensive experiments across text-to-text and text-to-image demonstrate the effectiveness of our approach in mitigating toxic outputs, even when dealing with larger models like \texttt{Llama-2}, \texttt{GPT-3.5} and \texttt{Stable-Diffusion}, highlighting the potential of our framework in enhancing safety and robustness in various applications.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）红队策略的激增，有关提高 LLM 防御策略的安全性和稳健性的文献中的缺陷变得越来越明显。本文介绍了基于 LLM 的 \textbf{sentinel} 模型作为即插即用的前缀模块，旨在仅使用少量（$<30$）额外令牌即可重建输入提示，从而有效减少目标 LLM 响应中的毒性。哨兵模型自然地克服了用于微调大型目标模型的 \textit{参数效率低下} 和 \textit{有限的模型可访问性}。我们采用近端策略优化（PPO）的交错训练方案来动态优化红队和哨兵模型，并结合受多智能体集中批评家启发的价值头共享机制来管理智能体之间复杂的相互作用。我们在文本到文本和文本到图像方面的广泛实验证明了我们的方法在减轻有毒输出方面的有效性，即使在处理 \texttt{Llama-2}、\texttt{GPT-3.5} 和 \ texttt{Stable-Diffusion}，强调了我们的框架在增强各种应用程序的安全性和鲁棒性方面的潜力。</li>
</ul>

<h3>Title: Tagengo: A Multilingual Chat Dataset</h3>
<ul>
<li><strong>Authors: </strong>Peter Devine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12612">https://arxiv.org/abs/2405.12612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12612">https://arxiv.org/pdf/2405.12612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12612]] Tagengo: A Multilingual Chat Dataset(https://arxiv.org/abs/2405.12612)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Open source large language models (LLMs) have shown great improvements in recent times. However, many of these models are focused solely on popular spoken languages. We present a high quality dataset of more than 70k prompt-response pairs in 74 languages which consist of human generated prompts and synthetic responses. We use this dataset to train a state-of-the-art open source English LLM to chat multilingually. We evaluate our model on MT-Bench chat benchmarks in 6 languages, finding that our multilingual model outperforms previous state-of-the-art open source LLMs across each language. We further find that training on more multilingual data is beneficial to the performance in a chosen target language (Japanese) compared to simply training on only data in that language. These results indicate the necessity of training on large amounts of high quality multilingual data to make a more accessible LLM.</li>
<li><strong>摘要：</strong>开源大语言模型（LLM）最近已经显示出巨大的进步。然而，其中许多模型仅关注流行的口语。我们提供了包含 74 种语言的超过 70k 个提示-响应对的高质量数据集，其中包括人类生成的提示和合成响应。我们使用这个数据集来训练最先进的开源英语法学硕士进行多语言聊天。我们在 6 种语言的 MT-Bench 聊天基准上评估了我们的模型，发现我们的多语言模型在每种语言上都优于以前最先进的开源 LLM。我们进一步发现，与仅对特定语言的数据进行训练相比，对更多多语言数据进行训练有利于所选目标语言（日语）的性能。这些结果表明有必要对大量高质量的多语言数据进行培训，以使法学硕士更容易获得。</li>
</ul>

<h3>Title: Quantifying Emergence in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hang Chen, Xinyu Yang, Jiaying Zhu, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12617">https://arxiv.org/abs/2405.12617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12617">https://arxiv.org/pdf/2405.12617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12617]] Quantifying Emergence in Large Language Models(https://arxiv.org/abs/2405.12617)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Emergence, broadly conceptualized as the ``intelligent'' behaviors of LLMs, has recently been studied and proved challenging to quantify due to the lack of a measurable definition. Most commonly, it has been estimated statistically through model performances across extensive datasets and tasks, which consumes significant resources. In addition, such estimation is difficult to interpret and may not accurately reflect the models' intrinsic emergence. In this work, we propose a quantifiable solution for estimating emergence. Inspired by emergentism in dynamics, we quantify the strength of emergence by comparing the entropy reduction of the macroscopic (semantic) level with that of the microscopic (token) level, both of which are derived from the representations within the transformer block. Using a low-cost estimator, our quantification method demonstrates consistent behaviors across a suite of LMs (GPT-2, GEMMA, etc.) under both in-context learning (ICL) and natural sentences. Empirical results show that (1) our method gives consistent measurements which align with existing observations based on performance metrics, validating the effectiveness of our emergence quantification; (2) our proposed metric uncovers novel emergence patterns such as the correlations between the variance of our metric and the number of ``shots'' in ICL, which further suggests a new way of interpreting hallucinations in LLMs; (3) we offer a potential solution towards estimating the emergence of larger and closed-resource LMs via smaller LMs like GPT-2. Our codes are available at: this https URL.</li>
<li><strong>摘要：</strong>涌现，广泛概念化为法学硕士的“智能”行为，最近经过研究并证明由于缺乏可测量的定义而难以量化。最常见的是，它是通过跨大量数据集和任务的模型性能进行统计估计的，这会消耗大量资源。此外，这种估计很难解释，并且可能无法准确反映模型的内在出现。在这项工作中，我们提出了一种用于估计出现率的量化解决方案。受动力学中涌现论的启发，我们通过比较宏观（语义）级别与微观（令牌）级别的熵减少来量化涌现的强度，这两者都源自 Transformer 块内的表示。使用低成本估计器，我们的量化方法在上下文学习 (ICL) 和自然句子下展示了一系列 LM（GPT-2、GEMMA 等）的一致行为。实证结果表明（1）我们的方法给出了一致的测量结果，与基于性能指标的现有观察结果相一致，验证了我们的涌现量化的有效性； (2) 我们提出的指标揭示了新的出现模式，例如我们的指标方差与 ICL 中“镜头”数量之间的相关性，这进一步提出了一种解释法学硕士中幻觉的新方法； (3) 我们提供了一个潜在的解决方案，通过 GPT-2 等较小的 LM 来估计更大且封闭资源的 LM 的出现。我们的代码位于：此 https URL。</li>
</ul>

<h3>Title: Exploration of Masked and Causal Language Modelling for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Nicolo Micheletti, Samuel Belkadi, Lifeng Han, Goran Nenadic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12630">https://arxiv.org/abs/2405.12630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12630">https://arxiv.org/pdf/2405.12630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12630]] Exploration of Masked and Causal Language Modelling for Text Generation(https://arxiv.org/abs/2405.12630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionised the field of Natural Language Processing (NLP) and have achieved state-of-the-art performance in practically every task in this field. However, the prevalent approach used in text generation, Causal Language Modelling (CLM), which generates text sequentially from left to right, inherently limits the freedom of the model, which does not decide when and where each token is generated. In contrast, Masked Language Modelling (MLM), primarily used for language understanding tasks, can generate tokens anywhere in the text and any order. This paper conducts an extensive comparison of MLM and CLM approaches for text generation tasks. To do so, we pre-train several language models of comparable sizes on three different datasets, namely 1) medical discharge summaries, 2) movie plot synopses, and 3) authorship verification datasets. To assess the quality of the generations, we first employ quantitative metrics and then perform a qualitative human evaluation to analyse coherence and grammatical correctness. In addition, we evaluate the usefulness of the generated texts by using them in three different downstream tasks: 1) Entity Recognition, 2) Text Classification, and 3) Authorship Verification. The results show that MLM consistently outperforms CLM in text generation across all datasets, with higher quantitative scores and better coherence in the generated text. The study also finds \textit{no strong correlation} between the quality of the generated text and the performance of the models in the downstream tasks. With this study, we show that MLM for text generation has great potential for future research and provides direction for future studies in this area.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了自然语言处理 (NLP) 领域，并在该领域的几乎每项任务中都取得了最先进的性能。然而，文本生成中使用的流行方法因果语言模型（CLM）从左到右顺序生成文本，本质上限制了模型的自由度，模型不能决定每个标记的生成时间和位置。相比之下，主要用于语言理解任务的掩码语言建模（MLM）可以在文本中的任何位置以任何顺序生成标记。本文对用于文本生成任务的 MLM 和 CLM 方法进行了广泛的比较。为此，我们在三个不同的数据集上预训练了几种大小相当的语言模型，即 1) 出院摘要、2) 电影情节概要和 3) 作者身份验证数据集。为了评估各代的质量，我们首先采用定量指标，然后进行定性人类评估以分析连贯性和语法正确性。此外，我们通过在三个不同的下游任务中使用生成的文本来评估它们的有用性：1）实体识别，2）文本分类和3）作者验证。结果表明，MLM 在所有数据集的文本生成方面始终优于 CLM，生成的文本具有更高的定量分数和更好的连贯性。研究还发现，生成文本的质量与下游任务中模型的性能之间没有很强的相关性。通过这项研究，我们表明用于文本生成的 MLM 在未来的研究中具有巨大的潜力，并为该领域的未来研究提供了方向。</li>
</ul>

<h3>Title: Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsiang Lin, Huang-Ting Shieh, Chih-Yu Liu, Kuang-Ting Lee, Hsiao-Cheng Chang, Jing-Lun Yang, Yu-Sheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12656">https://arxiv.org/abs/2405.12656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12656">https://arxiv.org/pdf/2405.12656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12656]] Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction(https://arxiv.org/abs/2405.12656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Extrapolation in Large language models (LLMs) for open-ended inquiry encounters two pivotal issues: (1) hallucination and (2) expensive training costs. These issues present challenges for LLMs in specialized domains and personalized data, requiring truthful responses and low fine-tuning costs. Existing works attempt to tackle the problem by augmenting the input of a smaller language model with information from a knowledge graph (KG). However, they have two limitations: (1) failing to extract relevant information from a large one-hop neighborhood in KG and (2) applying the same augmentation strategy for KGs with different characteristics that may result in low performance. Moreover, open-ended inquiry typically yields multiple responses, further complicating extrapolation. We propose a new task, the extreme multi-label KG link prediction task, to enable a model to perform extrapolation with multiple responses using structured real-world knowledge. Our retriever identifies relevant one-hop neighbors by considering entity, relation, and textual data together. Our experiments demonstrate that (1) KGs with different characteristics require different augmenting strategies, and (2) augmenting the language model's input with textual data improves task performance significantly. By incorporating the retrieval-augmented framework with KG, our framework, with a small parameter size, is able to extrapolate based on a given KG. The code can be obtained on GitHub: this https URL</li>
<li><strong>摘要：</strong>用于开放式探究的大型语言模型（LLM）外推遇到两个关键问题：（1）幻觉和（2）昂贵的培训成本。这些问题给专业领域和个性化数据的法学硕士带来了挑战，需要真实的回应和较低的微调成本。现有的工作试图通过使用知识图（KG）中的信息来增强较小语言模型的输入来解决该问题。然而，它们有两个局限性：（1）无法从知识图谱中的大型一跳邻域中提取相关信息；（2）对具有不同特征的知识图谱应用相同的增强策略，这可能会导致性能低下。此外，开放式询问通常会产生多种答复，使推断进一步复杂化。我们提出了一项新任务，即极限多标签 KG 链接预测任务，使模型能够使用结构化的现实世界知识对多个响应进行外推。我们的检索器通过一起考虑实体、关系和文本数据来识别相关的一跳邻居。我们的实验表明，（1）具有不同特征的知识图谱需要不同的增强策略，（2）用文本数据增强语言模型的输入可以显着提高任务性能。通过将检索增强框架与知识图谱相结合，我们的框架能够以较小的参数大小根据给定的知识图谱进行推断。代码可以在GitHub上获取：这个https URL</li>
</ul>

<h3>Title: Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text</h3>
<ul>
<li><strong>Authors: </strong>Yafu Li, Zhilin Wang, Leyang Cui, Wei Bi, Shuming Shi, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12689">https://arxiv.org/abs/2405.12689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12689">https://arxiv.org/pdf/2405.12689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12689]] Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text(https://arxiv.org/abs/2405.12689)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>AI-generated text detection has attracted increasing attention as powerful language models approach human-level generation. Limited work is devoted to detecting (partially) AI-paraphrased texts. However, AI paraphrasing is commonly employed in various application scenarios for text refinement and diversity. To this end, we propose a novel detection framework, paraphrased text span detection (PTD), aiming to identify paraphrased text spans within a text. Different from text-level detection, PTD takes in the full text and assigns each of the sentences with a score indicating the paraphrasing degree. We construct a dedicated dataset, PASTED, for paraphrased text span detection. Both in-distribution and out-of-distribution results demonstrate the effectiveness of PTD models in identifying AI-paraphrased text spans. Statistical and model analysis explains the crucial role of the surrounding context of the paraphrased text spans. Extensive experiments show that PTD models can generalize to versatile paraphrasing prompts and multiple paraphrased text spans. We release our resources at this https URL.</li>
<li><strong>摘要：</strong>随着强大的语言模型接近人类水平的生成，人工智能生成的文本检测引起了越来越多的关注。致力于检测（部分）人工智能释义文本的工作有限。然而，人工智能释义普遍应用于各种应用场景，以实现文本的细化和多样性。为此，我们提出了一种新颖的检测框架，即释义文本跨度检测（PTD），旨在识别文本中的释义文本跨度。与文本级检测不同，PTD 获取全文并为每个句子分配一个表示释义程度的分数。我们构建了一个专用数据集 PASTED，用于释义文本跨度检测。分布内和分布外结果都证明了 PTD 模型在识别 AI 释义文本跨度方面的有效性。统计和模型分析解释了释义文本范围周围上下文的关键作用。大量实验表明，PTD 模型可以推广到多功能释义提示和多个释义文本范围。我们在此 https URL 发布我们的资源。</li>
</ul>

<h3>Title: OLAPH: Improving Factuality in Biomedical Long-form Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Minbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12701">https://arxiv.org/abs/2405.12701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12701">https://arxiv.org/pdf/2405.12701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12701]] OLAPH: Improving Factuality in Biomedical Long-form Question Answering(https://arxiv.org/abs/2405.12701)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims, highlighting the need for an automated method to evaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset reconstructed using long-form question-answering datasets related to the biomedical domain. We use MedLFQA to facilitate the automatic evaluations of factuality. We also propose OLAPH, a simple and novel framework that enables the improvement of factuality through automatic evaluations. The OLAPH framework iteratively trains LLMs to mitigate hallucinations using sampling predictions and preference optimization. In other words, we iteratively set the highest-scoring response as a preferred response derived from sampling predictions and train LLMs to align with the preferred response that improves factuality. We highlight that, even on evaluation metrics not used during training, LLMs trained with our OLAPH framework demonstrate significant performance improvement in factuality. Our findings reveal that a 7B LLM trained with our OLAPH framework can provide long answers comparable to the medical experts' answers in terms of factuality. We believe that our work could shed light on gauging the long-text generation ability of LLMs in the medical domain. Our code and datasets are available at this https URL}{this https URL.</li>
<li><strong>摘要：</strong>在医学领域，许多场景都需要大型语言模型 (LLM) 具备长格式生成能力。具体来说，在回答患者的问题时，模型的回答必须传达事实主张，这凸显了需要一种自动化方法来评估这些主张。因此，我们引入了 MedLFQA，这是一个使用与生物医学领域相关的长格式问答数据集重建的基准数据集。我们使用 MedLFQA 来促进事实性的自动评估。我们还提出了 OLAPH，这是一个简单而新颖的框架，可通过自动评估来提高事实性。OLAPH 框架使用采样预测和偏好优化迭代训练 LLM 以减轻幻觉。换句话说，我们迭代地将得分最高的回答设置为从采样预测中得出的首选回答，并训练 LLM 以与提高事实性的首选回答保持一致。我们强调，即使在训练期间未使用的评估指标上，使用我们的 OLAPH 框架训练的 LLM 在事实性方面也表现出显着的性能提升。我们的研究结果表明，使用我们的 OLAPH 框架训练的 7B LLM 可以提供与医学专家答案在事实性方面相当的长篇答案。我们相信我们的工作可以为衡量医学领域 LLM 的长文本生成能力提供启示。我们的代码和数据集可在此 https URL}{此 https URL 上找到。</li>
</ul>

<h3>Title: Large Language Models Meet NLP: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12819">https://arxiv.org/abs/2405.12819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12819">https://arxiv.org/pdf/2405.12819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12819]] Large Language Models Meet NLP: A Survey(https://arxiv.org/abs/2405.12819)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored. This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature? (2) Have traditional NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for NLP? To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP. Specifically, we first introduce a unified taxonomy including (1) parameter-frozen application and (2) parameter-tuning application to offer a unified perspective for understanding the current progress of LLMs in NLP. Furthermore, we summarize the new frontiers and the associated challenges, aiming to inspire further groundbreaking advancements. We hope this work offers valuable insights into the {potential and limitations} of LLMs in NLP, while also serving as a practical guide for building effective LLMs in NLP.</li>
<li><strong>摘要：</strong>虽然像 ChatGPT 这样的大型语言模型 (LLM) 在自然语言处理 (NLP) 任务中表现出了令人印象深刻的能力，但对其在该领域潜力的系统研究在很大程度上仍未得到探索。本研究旨在通过探讨以下问题来弥补这一差距：（1）目前文献中法学硕士如何应用于 NLP 任务？ (2) 传统的 NLP 任务是否已经被法学硕士解决了？ (3) NLP 法学硕士的前景如何？为了回答这些问题，我们首先对 NLP 领域的法学硕士进行全面概述。具体来说，我们首先引入一个统一的分类法，包括（1）参数冻结应用程序和（2）参数调整应用程序，为理解法学硕士在 NLP 领域的当前进展提供统一的视角。此外，我们总结了新的领域和相关的挑战，旨在激发进一步的突破性进展。我们希望这项工作能够为 NLP 法学硕士的{潜力和局限性}提供有价值的见解，同时也为构建有效的 NLP 法学硕士提供实用指南。</li>
</ul>

<h3>Title: Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Abdurahmman Alzahrani, Eyad Babkier, Faisal Yanbaawi, Firas Yanbaawi, Hassan Alhuzali</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12884">https://arxiv.org/abs/2405.12884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12884">https://arxiv.org/pdf/2405.12884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12884]] Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models(https://arxiv.org/abs/2405.12884)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>In the current era of digital communication and widespread use of social media, it is crucial to develop an understanding of persuasive techniques employed in written text. This knowledge is essential for effectively discerning accurate information and making informed decisions. To address this need, this paper presents a comprehensive empirical study focused on identifying persuasive techniques in Arabic social media content. To achieve this objective, we utilize Pre-trained Language Models (PLMs) and leverage the ArAlEval dataset, which encompasses two tasks: binary classification to determine the presence or absence of persuasion techniques, and multi-label classification to identify the specific types of techniques employed in the text. Our study explores three different learning approaches by harnessing the power of PLMs: feature extraction, fine-tuning, and prompt engineering techniques. Through extensive experimentation, we find that the fine-tuning approach yields the highest results on the aforementioned dataset, achieving an f1-micro score of 0.865 and an f1-weighted score of 0.861. Furthermore, our analysis sheds light on an interesting finding. While the performance of the GPT model is relatively lower compared to the other approaches, we have observed that by employing few-shot learning techniques, we can enhance its results by up to 20\%. This offers promising directions for future research and exploration in this topic\footnote{Upon Acceptance, the source code will be released on GitHub.}.</li>
<li><strong>摘要：</strong>在当今数字通信和社交媒体广泛使用的时代，了解书面文本中使用的说服技巧至关重要。这些知识对于有效识别准确信息并做出明智决策至关重要。为了满足这一需求，本文提出了一项全面的实证研究，重点是确定阿拉伯社交媒体内容中的说服技巧。为了实现这一目标，我们利用预训练语言模型 (PLM) 并利用 ArAlEval 数据集，该数据集包含两项任务：二元分类以确定说服技术的存在或不存在，以及多标签分类以确定特定类型的技术文中采用。我们的研究利用 PLM 的力量探索了三种不同的学习方法：特征提取、微调和即时工程技术。通过大量的实验，我们发现微调方法在上述数据集上产生了最高的结果，达到了 0.865 的 f1-micro 分数和 0.861 的 f1 加权分数。此外，我们的分析揭示了一个有趣的发现。虽然与其他方法相比，GPT 模型的性能相对较低，但我们观察到，通过采用少样本学习技术，我们可以将其结果提高多达 20%。这为本主题的未来研究和探索提供了有前景的方向\footnote{一旦接受，源代码将在 GitHub 上发布。}。</li>
</ul>

<h3>Title: Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents</h3>
<ul>
<li><strong>Authors: </strong>San Kim, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12900">https://arxiv.org/abs/2405.12900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12900">https://arxiv.org/pdf/2405.12900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12900]] Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents(https://arxiv.org/abs/2405.12900)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model's resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.</li>
<li><strong>摘要：</strong>高质量大语言模型（LLM）和各种有效训练方法的出现推动了开放域对话系统的最新进展。然而，这些模型中存在的毒性提出了重大挑战，可能会降低用户体验。在这项研究中，我们引入了一种创新的训练算法，它是对直接偏好优化 (DPO) 的改进，称为对抗性 DPO (ADPO)。 ADPO 算法旨在训练模型，为首选响应分配较高的概率分布，为不安全响应分配较低的概率分布，这些分布是使用有毒控制令牌自行生成的。我们证明 ADPO 增强了模型针对有害对话的弹性，同时最大限度地减少了性能下降。此外，我们还表明，与传统的 DPO 相比，ADPO 提供了更稳定的训练过程。据我们所知，这是 DPO 算法的首次改编，直接将有害数据纳入生成模型，从而减少了人为创建安全对话数据的需要。</li>
</ul>

<h3>Title: Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment</h3>
<ul>
<li><strong>Authors: </strong>Holli Sargeant, Ahmed Izzidien, Felix Steffek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12910">https://arxiv.org/abs/2405.12910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12910">https://arxiv.org/pdf/2405.12910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12910]] Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment(https://arxiv.org/abs/2405.12910)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic modelling summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.10%. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.</li>
<li><strong>摘要：</strong>本文通过开发和应用一种新颖的分类法来对英国简易判决案件进行主题建模，从而解决了法律分析中的一个关键差距。通过总结判断案例的精选数据集，我们使用大型语言模型 Claude 3 Opus 来探索功能性主题和趋势。我们发现 Claude 3 Opus 正确分类了主题，准确率为 87.10%。分析揭示了简易判决在不同法律领域的应用的不同模式。由于英国判例法最初并未标有关键词或主题过滤选项，因此研究结果不仅加深了我们对即决判决主题基础的理解，而且还说明了在法律分类中将传统方法与人工智能驱动方法相结合的潜力。因此，本文为英国法律提供了一种新的通用分类法。这项工作的意义为司法行政和计算法律研究方法领域的进一步研究和政策讨论奠定了基础。</li>
</ul>

<h3>Title: G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Xingyuan Pan, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Shanbo Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable abilities in general scenarios. Instruction finetuning empowers them to align with humans in various tasks. Nevertheless, the Diversity and Quality of the instruction data remain two main challenges for instruction finetuning. With regard to this, in this paper, we propose a novel gradient-based method to automatically select high-quality and diverse instruction finetuning data for machine translation. Our key innovation centers around analyzing how individual training examples influence the model during training. Specifically, we select training examples that exert beneficial influences on the model as high-quality ones by means of Influence Function plus a small high-quality seed dataset. Moreover, to enhance the diversity of the training data we maximize the variety of influences they have on the model by clustering on their gradients and resampling. Extensive experiments on WMT22 and FLORES translation tasks demonstrate the superiority of our methods, and in-depth analysis further validates their effectiveness and generalization.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在一般场景中表现出了非凡的能力。指令微调使它们能够在各种任务中与人类保持一致。尽管如此，指令数据的多样性和质量仍然是指令微调的两个主要挑战。为此，在本文中，我们提出了一种新颖的基于梯度的方法来自动选择高质量和多样化的机器翻译指令微调数据。我们的关键创新集中于分析单个训练示例在训练过程中如何影响模型。具体来说，我们通过影响函数加上小型高质量种子数据集的方式，选择对模型产生有益影响的训练样例作为高质量训练样例。此外，为了增强训练数据的多样性，我们通过对梯度进行聚类和重采样来最大化它们对模型的影响。在 WMT22 和 FLORES 翻译任务上的大量实验证明了我们方法的优越性，深入分析进一步验证了其有效性和泛化性。</li>
</ul>

<h3>Title: Code-mixed Sentiment and Hate-speech Prediction</h3>
<ul>
<li><strong>Authors: </strong>Anjali Yadav, Tanya Garg, Matej Klemen, Matej Ulcar, Basant Agarwal, Marko Robnik Sikonja</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12929">https://arxiv.org/abs/2405.12929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12929">https://arxiv.org/pdf/2405.12929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12929]] Code-mixed Sentiment and Hate-speech Prediction(https://arxiv.org/abs/2405.12929)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Code-mixed discourse combines multiple languages in a single text. It is commonly used in informal discourse in countries with several official languages, but also in many other countries in combination with English or neighboring languages. As recently large language models have dominated most natural language processing tasks, we investigated their performance in code-mixed settings for relevant tasks. We first created four new bilingual pre-trained masked language models for English-Hindi and English-Slovene languages, specifically aimed to support informal language. Then we performed an evaluation of monolingual, bilingual, few-lingual, and massively multilingual models on several languages, using two tasks that frequently contain code-mixed text, in particular, sentiment analysis and offensive language detection in social media texts. The results show that the most successful classifiers are fine-tuned bilingual models and multilingual models, specialized for social media texts, followed by non-specialized massively multilingual and monolingual models, while huge generative models are not competitive. For our affective problems, the models mostly perform slightly better on code-mixed data compared to non-code-mixed data.</li>
<li><strong>摘要：</strong>代码混合话语将多种语言结合在一个文本中。它通常用于拥有多种官方语言的国家的非正式对话中，但也在许多其他国家与英语或邻近语言结合使用。由于最近大型语言模型主导了大多数自然语言处理任务，我们研究了它们在相关任务的代码混合设置中的性能。我们首先为英语-印地语和英语-斯洛文尼亚语创建了四种新的双语预训练掩码语言模型，专门用于支持非正式语言。然后，我们使用两个经常包含代码混合文本的任务，特别是社交媒体文本中的情感分析和攻击性语言检测，对多种语言的单语、双语、少数语言和大规模多语言模型进行了评估。结果表明，最成功的分类器是专门针对社交媒体文本的微调双语模型和多语言模型，其次是非专业的大规模多语言和单语言模型，而大型生成模型则没有竞争力。对于我们的情感问题，与非代码混合数据相比，模型在代码混合数据上的表现大多略好。</li>
</ul>

<h3>Title: Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bilgehan Sel, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, Ming Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12933">https://arxiv.org/abs/2405.12933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12933">https://arxiv.org/pdf/2405.12933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12933]] Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs(https://arxiv.org/abs/2405.12933)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities in tasks such as summarization, arithmetic reasoning, and question answering. However, they encounter significant challenges in the domain of moral reasoning and ethical decision-making, especially in complex scenarios with multiple stakeholders. This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing moral reasoning in LLMs by exploring decisions' consequences from multiple stakeholder perspectives. Central to SKIG's mechanism is simulating accountability for actions, which, alongside empathy exercises and risk assessment, is pivotal to its effectiveness. We validate SKIG's performance across various moral reasoning benchmarks with proprietary and opensource LLMs, and investigate its crucial components through extensive ablation analyses.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在摘要、算术推理和问答等任务中表现出了卓越的能力。然而，他们在道德推理和道德决策领域遇到了重大挑战，特别是在涉及多个利益相关者的复杂场景中。本文介绍了“游戏中的皮肤”（SKIG）框架，旨在通过从多个利益相关者的角度探索决策的后果来增强法学硕士的道德推理。 SKIG 机制的核心是模拟行动问责制，这与同理心练习和风险评估一起，对其有效性至关重要。我们通过专有和开源法学硕士验证 SKIG 在各种道德推理基准上的表现，并通过广泛的消融分析研究其关键组成部分。</li>
</ul>

<h3>Title: Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Tianxiang Sun, Cheng Chang, Qinyuan Cheng, Ding Wang, Xiaofeng Mou, Xipeng Qiu, XuanJing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.12939">https://arxiv.org/abs/2405.12939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.12939">https://arxiv.org/pdf/2405.12939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.12939]] Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models(https://arxiv.org/abs/2405.12939)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks. Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency. However, this approach fails in scenarios where the correct answers are in the minority. We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers. To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains. Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task. Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods. Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods.</li>
<li><strong>摘要：</strong>思想链提示的最新进展促进了大型语言模型（LLM）在复杂推理任务中的重大突破。目前的研究通过对多个推理链进行采样并根据答案频率进行集成来增强法学硕士的推理性能。然而，在正确答案为少数的情况下，这种方法会失败。我们认为这是限制法学硕士推理能力的主要因素，这一限制不能仅根据预测答案来解决。为了解决这个缺点，我们引入了分层推理聚合框架AoR（推理聚合），它根据推理链的评估来选择答案。此外，AoR 还结合了动态采样，根据任务的复杂程度调整推理链的数量。一系列复杂推理任务的实验结果表明，AoR 优于著名的集成方法。进一步的分析表明，AoR 不仅适用于各种 LLM，而且与现有方法相比，还实现了优越的性能上限。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
