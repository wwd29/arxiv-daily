<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-06-05</h1>
<h3>Title: Title:
          Rotation and Permutation for Advanced Outlier Management and Efficient Quantization of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Rotation and Permutation for Advanced Outlier Management and Efficient Quantization of LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Quantizing large language models (LLMs) presents significant challenges, primarily due to outlier activations that compromise the efficiency of low-bit representation. Traditional approaches mainly focus on solving Normal Outliers-activations with consistently high magnitudes across all tokens. However, these techniques falter when dealing with Massive Outliers, which are significantly higher in value and often cause substantial performance losses during low-bit quantization. In this study, we propose DuQuant, an innovative quantization strategy employing rotation and permutation transformations to more effectively eliminate both types of outliers. Initially, DuQuant constructs rotation matrices informed by specific outlier dimensions, redistributing these outliers across adjacent channels within different rotation blocks. Subsequently, a zigzag permutation is applied to ensure a balanced distribution of outliers among blocks, minimizing block-wise variance. An additional rotation further enhances the smoothness of the activation landscape, thereby improving model performance. DuQuant streamlines the quantization process and demonstrates superior outlier management, achieving top-tier results in multiple tasks with various LLM architectures even under 4-bit weight-activation quantization. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>量化大型语言模型 (LLM) 面临巨大挑战，主要是因为异常激活会影响低位表示的效率。传统方法主要侧重于解决正常异常激活，即所有标记的幅度始终很高的激活。然而，这些技术在处理大量异常值时会失效，因为异常值的值要高得多，并且经常在低位量化期间造成巨大的性能损失。在本研究中，我们提出了 DuQuant，这是一种创新的量化策略，它采用旋转和置换变换来更有效地消除这两种类型的异常值。首先，DuQuant 构建由特定异常值维度形成的旋转矩阵，将这些异常值重新分布在不同旋转块内的相邻通道上。随后，应用锯齿形置换以确保异常值在块之间均衡分布，从而最大限度地减少块方差。额外的旋转进一步增强了激活景观的平滑度，从而提高了模型性能。 DuQuant 简化了量化过程并展示了卓越的异常值管理，即使在 4 位权重激活量化下，也能在多个任务中使用各种 LLM 架构实现顶级结果。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          Towards Harnessing Large Language Models for Comprehension of Conversational Grounding</h3>
<ul>
<li><strong>Authors: </strong>Kristiina Jokinen, Phillip Schneider, Taiga Mori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Harnessing Large Language Models for Comprehension of Conversational Grounding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Conversational grounding is a collaborative mechanism for establishing mutual knowledge among participants engaged in a dialogue. This experimental study analyzes information-seeking conversations to investigate the capabilities of large language models in classifying dialogue turns related to explicit or implicit grounding and predicting grounded knowledge elements. Our experimental results reveal challenges encountered by large language models in the two tasks and discuss ongoing research efforts to enhance large language model-based conversational grounding comprehension through pipeline architectures and knowledge bases. These initiatives aim to develop more effective dialogue systems that are better equipped to handle the intricacies of grounded knowledge in conversations.</li>
<li><strong>摘要：</strong>对话基础是一种协作机制，用于在参与对话的参与者之间建立相互知识。这项实验研究分析了寻求信息的对话，以调查大型语言模型在对与显性或隐性基础相关的对话轮次进行分类以及预测基础知识元素方面的能力。我们的实验结果揭示了大型语言模型在这两个任务中遇到的挑战，并讨论了正在进行的研究工作，以通过管道架构和知识库来增强基于大型语言模型的对话基础理解。这些举措旨在开发更有效的对话系统，以更好地处理对话中基础知识的复杂性。</li>
</ul>

<h3>Title: Title:
          LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback</h3>
<ul>
<li><strong>Authors: </strong>Wen Lai, Mohsen Mesgar, Alexander Fraser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.</li>
<li><strong>摘要：</strong>为了将大型语言模型 (LLM) 普及到大多数自然语言，必须使这些模型能够理解和生成多种语言的文本，尤其是资源匮乏的语言。虽然最近的多语言 LLM 在这些功能方面表现出色，但由于缺乏资源匮乏语言的训练数据，这些 LLM 仍然支持有限数量的人类语言。此外，这些 LLM 尚未与人类对下游任务的偏好保持一致，而这一点对于英语 LLM 的成功至关重要。在本文中，我们介绍了 xLLaMA-100 和 xBLOOM-100（统称为 xLLMs-100），它们将 LLaMA 和 BLOOM 的多语言功能扩展到 100 种语言。为此，我们构建了两个数据集：一个包括 100 种语言的多语言教学数据集，它代表了迄今为止最大的语言覆盖范围，以及一个涵盖 30 种语言的跨语言人工反馈数据集。我们对构建的指令数据进行多语言指令调整，并使用跨语言人类反馈数据集上的 DPO 算法进一步将 LLM 与人类反馈对齐。我们在五个多语言基准上评估了 xLLMs-100 的多语言理解和生成能力。实验结果表明，xLLMs-100 在基准测试中始终以相当大的优势领先于同类产品，定义了一种支持 100 种语言的全新最先进的多语言 LLM。</li>
</ul>

<h3>Title: Title:
          OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kerim Büyükakyüz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) has revolutionized natural language processing, enabling unprecedented capabilities in understanding and generating human-like text. However, the computational cost and convergence times associated with fine-tuning these models remain significant challenges. Low-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these issues by introducing efficient fine-tuning techniques with a reduced number of trainable parameters. In this paper, we present OLoRA, an enhancement to the LoRA method that leverages orthonormal matrix initialization through QR decomposition. OLoRA significantly accelerates the convergence of LLM training while preserving the efficiency benefits of LoRA, such as the number of trainable parameters and GPU memory footprint. Our empirical evaluations demonstrate that OLoRA not only converges faster but also exhibits improved performance compared to standard LoRA across a variety of language modeling tasks. This advancement opens new avenues for more efficient and accessible fine-tuning of LLMs, potentially enabling broader adoption and innovation in natural language applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现彻底改变了自然语言处理，使理解和生成类似人类的文本的能力达到了前所未有的水平。然而，与微调这些模型相关的计算成本和收敛时间仍然是重大挑战。低秩自适应 (LoRA) 已成为一种有前途的方法，它通过引入有效的微调技术并减少可训练参数的数量来缓解这些问题。在本文中，我们介绍了 OLoRA，这是对 LoRA 方法的增强，它通过 QR 分解利用正交矩阵初始化。OLoRA 显著加快了 LLM 训练的收敛速度，同时保留了 LoRA 的效率优势，例如可训练参数的数量和 GPU 内存占用。我们的实证评估表明，与标准 LoRA 相比，OLoRA 不仅收敛速度更快，而且在各种语言建模任务中表现出更好的性能。这一进步为更高效、更易于访问的 LLM 微调开辟了新途径，有可能实现自然语言应用的更广泛采用和创新。</li>
</ul>

<h3>Title: Title:
          Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhen Lin, Shubhendu Trivedi, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks. For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence. Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components. For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction. Additionally, different tokens should be weighted differently depending on the context. In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM. By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure. We refer to this new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts. Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现极大地推动了众多自然语言生成任务的最新进展。要可靠地应用 LLM，必须准确衡量其置信度。目前，最常用的置信度得分函数是生成序列的似然度，然而，它混淆了语义和句法成分。例如，在问答 (QA) 任务中，正确答案的措辞不当可能会导致概率预测较低。此外，不同的标记应根据上下文赋予不同的权重。在这项工作中，我们建议通过使用从基础 LLM 中引出的注意力值为各种标记分配不同的权重来增强预测序列概率。通过使用验证集，我们可以识别相关的注意力头，从而显著提高原始序列概率置信度测量的可靠性。我们将这个新分数称为上下文序列似然度 (CSL)。CSL 易于实现，计算速度快，并且通过特定于任务的提示提供了巨大的进一步改进潜力。在多个 QA 数据集和多种 LLM 中，CSL 在预测生成质量方面表现出比最先进基线高得多的可靠性，如 AUROC 或 AUARC 所测量的那样。</li>
</ul>

<h3>Title: Title:
          TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability</h3>
<ul>
<li><strong>Authors: </strong>Aisha Khatun, Daniel G. Brown</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) evaluation is currently one of the most important areas of research, with existing benchmarks proving to be insufficient and not completely representative of LLMs' various capabilities. We present a curated collection of challenging statements on sensitive topics for LLM benchmarking called TruthEval. These statements were curated by hand and contain known truth values. The categories were chosen to distinguish LLMs' abilities from their stochastic nature. We perform some initial analyses using this dataset and find several instances of LLMs failing in simple tasks showing their inability to understand simple questions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 评估是目前最重要的研究领域之一，现有的基准测试已被证明不足，不能完全代表 LLM 的各种能力。我们为 LLM 基准测试提供了一个精选的敏感主题挑战性陈述集合，称为 TruthEval。这些陈述是手工精选的，包含已知的真值。选择这些类别是为了将 LLM 的能力与其随机性区分开来。我们使用此数据集进行了一些初步分析，发现几个 LLM 在简单任务中失败的实例表明它们无法理解简单的问题。</li>
</ul>

<h3>Title: Title:
          Eliciting the Priors of Large Language Models using Iterated In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jian-Qiao Zhu, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Eliciting the Priors of Large Language Models using Iterated In-Context Learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly deployed in real-world settings, understanding the knowledge they implicitly use when making decisions is critical. One way to capture this knowledge is in the form of Bayesian prior distributions. We develop a prompt-based workflow for eliciting prior distributions from LLMs. Our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution. We validated our method in settings where iterated learning has previously been used to estimate the priors of human participants -- causal learning, proportion estimation, and predicting everyday quantities. We found that priors elicited from GPT-4 qualitatively align with human priors in these settings. We then used the same method to elicit priors from GPT-4 for a variety of speculative events, such as the timing of the development of superhuman AI.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 越来越多地部署在现实世界中，了解它们在做决策时隐含使用的知识至关重要。获取这种知识的一种方法是采用贝叶斯先验分布的形式。我们开发了一个基于提示的工作流程，用于从 LLM 中引出先验分布。我们的方法基于迭代学习，这是一种马尔可夫链蒙特卡罗方法，其中连续推理以支持从先验分布中采样的方式链接起来。我们在迭代学习以前曾用于估计人类参与者的先验的环境中验证了我们的方法——因果学习、比例估计和预测日常数量。我们发现从 GPT-4 中引出的先验在这些环境中与人类先验在质量上一致。然后，我们使用相同的方法从 GPT-4 中引出各种推测事件的先验，例如超人类人工智能的发展时机。</li>
</ul>

<h3>Title: Title:
          Towards Effective Time-Aware Language Representation: Exploring Enhanced Temporal Understanding in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiexin Wang, Adam Jatowt, Yi Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Effective Time-Aware Language Representation: Exploring Enhanced Temporal Understanding in Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the evolving field of Natural Language Processing, understanding the temporal context of text is increasingly crucial. This study investigates methods to incorporate temporal information during pre-training, aiming to achieve effective time-aware language representation for improved performance on time-related tasks. In contrast to common pre-trained models like BERT, which rely on synchronic document collections such as BookCorpus and Wikipedia, our research introduces BiTimeBERT 2.0, a novel language model pre-trained on a temporal news article collection. BiTimeBERT 2.0 utilizes this temporal news collection, focusing on three innovative pre-training objectives: Time-Aware Masked Language Modeling (TAMLM), Document Dating (DD), and Time-Sensitive Entity Replacement (TSER). Each objective targets a unique aspect of temporal information. TAMLM is designed to enhance the understanding of temporal contexts and relations, DD integrates document timestamps as chronological markers, and TSER focuses on the temporal dynamics of "Person" entities, recognizing their inherent temporal significance. The experimental results consistently demonstrate that BiTimeBERT 2.0 outperforms models like BERT and other existing pre-trained models, achieving substantial gains across a variety of downstream NLP tasks and applications where time plays a pivotal role.</li>
<li><strong>摘要：</strong>在不断发展的自然语言处理领域，理解文本的时间背景越来越重要。本研究探讨了在预训练过程中整合时间信息的方法，旨在实现有效的时间感知语言表示，从而提高与时间相关的任务的性能。与依赖于同步文档集（如 BookCorpus 和 Wikipedia）的常见预训练模型（如 BERT）不同，我们的研究引入了 BiTimeBERT 2.0，这是一种在时间新闻文章集上进行预训练的新型语言模型。BiTimeBERT 2.0 利用这个时间新闻集，专注于三个创新的预训练目标：时间感知掩蔽语言建模 (TAMLM)、文档日期 (DD) 和时间敏感实体替换 (TSER)。每个目标都针对时间信息的一个独特方面。TAMLM 旨在增强对时间背景和关系的理解，DD 将文档时间戳集成为时间标记，TSER 专注于“人”实体的时间动态，认识到其固有的时间意义。实验结果一致表明，BiTimeBERT 2.0 的表现优于 BERT 等模型以及其他现有的预训练模型，在时间起着关键作用的各种下游 NLP 任务和应用中取得了显著的进步。</li>
</ul>

<h3>Title: Title:
          CR-UTP: Certified Robustness against Universal Text Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Qian Lou, Xin Liang, Jiaqi Xue, Yancheng Zhang, Rui Xie, Mengxin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CR-UTP: Certified Robustness against Universal Text Perturbations(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>It is imperative to ensure the stability of every prediction made by a language model; that is, a language's prediction should remain consistent despite minor input variations, like word substitutions. In this paper, we investigate the problem of certifying a language model's robustness against Universal Text Perturbations (UTPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing certified robustness based on random smoothing has shown considerable promise in certifying the input-specific text perturbations (ISTPs), operating under the assumption that any random alteration of a sample's clean or adversarial words would negate the impact of sample-wise perturbations. However, with UTPs, masking only the adversarial words can eliminate the attack. A naive method is to simply increase the masking ratio and the likelihood of masking attack tokens, but it leads to a significant reduction in both certified accuracy and the certified radius due to input corruption by extensive masking. To solve this challenge, we introduce a novel approach, the superior prompt search method, designed to identify a superior prompt that maintains higher certified accuracy under extensive masking. Additionally, we theoretically motivate why ensembles are a particularly suitable choice as base prompts for random smoothing. The method is denoted by superior prompt ensembling technique. We also empirically confirm this technique, obtaining state-of-the-art results in multiple settings. These methodologies, for the first time, enable high certified accuracy against both UTPs and ISTPs. The source code of CR-UTP is available at this https URL.</li>
<li><strong>摘要：</strong>必须确保语言模型所做的每个预测的稳定性；也就是说，语言的预测应该在输入发生微小变化（如单词替换）的情况下保持一致。在本文中，我们研究了认证语言模型对通用文本扰动 (UTP) 的鲁棒性的问题，UTP 已广泛用于通用对抗攻击和后门攻击。现有的基于随机平滑的认证鲁棒性在认证输入特定的文本扰动 (ISTP) 方面表现出了相当大的前景，其假设是样本的干净词或对抗词的任何随机改变都会抵消样本扰动的影响。然而，对于 UTP，只屏蔽对抗词就可以消除攻击。一种简单的方法是简单地增加屏蔽率和屏蔽攻击标记的可能性，但由于大量屏蔽会破坏输入，这会导致认证准确度和认证半径显著降低。为了解决这一挑战，我们引入了一种新方法，即优越提示搜索方法，旨在识别在广泛掩蔽下保持更高认证准确度的优越提示。此外，我们从理论上解释了为什么集合是作为随机平滑的基本提示的特别合适的选择。该方法由优越提示集合技术表示。我们还通过经验证实了这项技术，在多种设置中获得了最先进的结果。这些方法首次实现了针对 UTP 和 ISTP 的高认证准确度。CR-UTP 的源代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          OTTAWA: Optimal TransporT Adaptive Word Aligner for Hallucination and Omission Translation Errors Detection</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Huang, Abbas Ghaddar, Ivan Kobyzev, Mehdi Rezagholizadeh, Osmar R. Zaiane, Boxing Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OTTAWA: Optimal TransporT Adaptive Word Aligner for Hallucination and Omission Translation Errors Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>Recently, there has been considerable attention on detecting hallucinations and omissions in Machine Translation (MT) systems. The two dominant approaches to tackle this task involve analyzing the MT system's internal states or relying on the output of external tools, such as sentence similarity or MT quality estimators. In this work, we introduce OTTAWA, a novel Optimal Transport (OT)-based word aligner specifically designed to enhance the detection of hallucinations and omissions in MT systems. Our approach explicitly models the missing alignments by introducing a "null" vector, for which we propose a novel one-side constrained OT setting to allow an adaptive null alignment. Our approach yields competitive results compared to state-of-the-art methods across 18 language pairs on the HalOmi benchmark. In addition, it shows promising features, such as the ability to distinguish between both error types and perform word-level detection without accessing the MT system's internal states.</li>
<li><strong>摘要：</strong>最近，人们对机器翻译 (MT) 系统中的幻觉和遗漏检测给予了极大关注。解决此任务的两种主要方法是分析 MT 系统的内部状态或依赖外部工具的输出，例如句子相似性或 MT 质量估计器。在这项工作中，我们引入了 OTTAWA，这是一种基于最佳传输 (OT) 的新型词对齐器，专门用于增强 MT 系统中幻觉和遗漏的检测。我们的方法通过引入“空”向量明确地模拟缺失对齐，为此我们提出了一种新颖的单侧约束 OT 设置以允许自适应空对齐。与 HalOmi 基准上 18 种语言对的最新方法相比，我们的方法产生了具有竞争力的结果。此外，它还展示了有希望的功能，例如能够区分两种错误类型并执行单词级检测而无需访问 MT 系统的内部状态。</li>
</ul>

<h3>Title: Title:
          Dishonesty in Helpful and Harmless Alignment</h3>
<ul>
<li><strong>Authors: </strong>Youcheng Huang, Jingkun Tang, Duanyu Feng, Zheng Zhang, Wenqiang Lei, Jiancheng Lv, Anthony G. Cohn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Dishonesty in Helpful and Harmless Alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>People tell lies when seeking rewards. Large language models (LLMs) are aligned to human values with reinforcement learning where they get rewards if they satisfy human preference. We find that this also induces dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses. Using the latest interpreting tools, we detect dishonesty, show how LLMs can be harmful if their honesty is increased, and analyze such conflicts at the parameter-level. Given these preliminaries and the hypothesis that reward-seeking stimulates dishonesty, we theoretically show that the dishonesty can in-turn decrease the alignment performances and augment reward-seeking alignment with representation regularization. Extensive results, including GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we can train more honest, helpful, and harmless LLMs. We will make all our codes and results be open-sourced upon this paper's acceptance.</li>
<li><strong>摘要：</strong>人们在寻求奖励时会撒谎。大型语言模型 (LLM) 通过强化学习与人类价值观保持一致，如果它们满足人类偏好，它们就会获得奖励。我们发现，这也会在有益和无害的对齐中引发不诚实，而 LLM 在生成无害反应时会撒谎。使用最新的解释工具，我们可以检测不诚实行为，展示如果 LLM 的诚实度增加，它们会如何有害，并在参数级别分析此类冲突。鉴于这些准备工作以及寻求奖励会刺激不诚实的假设，我们从理论上表明，不诚实反过来会降低对齐性能，并通过表示正则化增强寻求奖励的对齐。包括 GPT-4 注释的胜率、困惑度和案例研究在内的大量结果表明，我们可以训练出更诚实、更有益、更无害的 LLM。我们将在本文被接受后将我们的所有代码和结果开源。</li>
</ul>

<h3>Title: Title:
          Process-Driven Autoformalization in Lean 4</h3>
<ul>
<li><strong>Authors: </strong>Jianqiao Lu, Zhengying Liu, Yingjia Wan, Yinya Huang, Haiming Wang, Zhicheng Yang, Jing Tang, Zhijiang Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Process-Driven Autoformalization in Lean 4(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Autoformalization, the conversion of natural language mathematics into formal languages, offers significant potential for advancing mathematical reasoning. However, existing efforts are limited to formal languages with substantial online corpora and struggle to keep pace with rapidly evolving languages like Lean 4. To bridge this gap, we propose a new benchmark \textbf{Form}alization for \textbf{L}ean~\textbf{4} (\textbf{\name}) designed to evaluate the autoformalization capabilities of large language models (LLMs). This benchmark encompasses a comprehensive assessment of questions, answers, formal statements, and proofs. Additionally, we introduce a \textbf{P}rocess-\textbf{S}upervised \textbf{V}erifier (\textbf{PSV}) model that leverages the precise feedback from Lean 4 compilers to enhance autoformalization. Our experiments demonstrate that the PSV method improves autoformalization, enabling higher accuracy using less filtered training data. Furthermore, when fine-tuned with data containing detailed process information, PSV can leverage the data more effectively, leading to more significant improvements in autoformalization for Lean 4. Our dataset and code are available at \url{this https URL}.</li>
<li><strong>摘要：</strong>自动形式化，即将自然语言数学转换为形式语言，为推进数学推理提供了巨大的潜力。然而，现有的努力仅限于具有大量在线语料库的形式语言，难以跟上 Lean 4 等快速发展的语言的步伐。为了弥补这一差距，我们提出了一个新的基准 \textbf{Formal}alization for \textbf{L}ean~\textbf{4} (\textbf{\name})，旨在评估大型语言模型 (LLM) 的自动形式化能力。该基准涵盖对问题、答案、形式陈述和证明的全面评估。此外，我们引入了一个 \textbf{P}rocess-\textbf{S}supervised \textbf{V}erifier (\textbf{PSV}) 模型，该模型利用 Lean 4 编译器的精确反馈来增强自动形式化。我们的实验表明，PSV 方法改进了自动形式化，使用更少的过滤训练数据实现更高的准确性。此外，当使用包含详细流程信息的数据进行微调时，PSV 可以更有效地利用数据，从而显著提高 Lean 4 的自动化形式化水平。我们的数据集和代码可在 \url{此 https URL} 处找到。</li>
</ul>

<h3>Title: Title:
          Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nik Bear Brown</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>This paper surveys evaluation techniques to enhance the trustworthiness and understanding of Large Language Models (LLMs). As reliance on LLMs grows, ensuring their reliability, fairness, and transparency is crucial. We explore algorithmic methods and metrics to assess LLM performance, identify weaknesses, and guide development towards more trustworthy applications. Key evaluation metrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR, BERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot Learning Performance, Transfer Learning Evaluation, Adversarial Testing, and Fairness and Bias Evaluation. We introduce innovative approaches like LLMMaps for stratified evaluation, Benchmarking and Leaderboards for competitive assessment, Stratified Analysis for in-depth understanding, Visualization of Blooms Taxonomy for cognitive level accuracy distribution, Hallucination Score for quantifying inaccuracies, Knowledge Stratification Strategy for hierarchical analysis, and Machine Learning Models for Hierarchy Generation. Human Evaluation is highlighted for capturing nuances that automated metrics may miss. These techniques form a framework for evaluating LLMs, aiming to enhance transparency, guide development, and establish user trust. Future papers will describe metric visualization and demonstrate each approach on practical examples.</li>
<li><strong>摘要：</strong>本文调查了用于增强大型语言模型 (LLM) 的可信度和理解度的评估技术。随着对 LLM 的依赖性不断增长，确保其可靠性、公平性和透明度至关重要。我们探索算法方法和指标来评估 LLM 性能、识别弱点并指导开发更值得信赖的应用程序。关键评估指标包括困惑度测量、NLP 指标（BLEU、ROUGE、METEOR、BERTScore、GLEU、单词错误率、字符错误率）、零样本和少量样本学习性能、迁移学习评估、对抗性测试以及公平性和偏见评估。我们引入了创新方法，例如用于分层评估的 LLMMaps、用于竞争性评估的基准测试和排行榜、用于深入理解的分层分析、用于认知水平准确度分布的布卢姆分类法可视化、用于量化不准确性的幻觉评分、用于分层分析的知识分层策略和用于层次结构的机器学习模型。人工评估可以捕捉到自动化指标可能遗漏的细微差别。这些技术构成了评估 LLM 的框架，旨在提高透明度、指导开发并建立用户信任。未来的论文将描述度量可视化并通过实际示例展示每种方法。</li>
</ul>

<h3>Title: Title:
          Conditional Language Learning with Context</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zhang, Miao Li, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Conditional Language Learning with Context(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models can learn sophisticated language understanding skills from fitting raw text. They also unselectively learn useless corpus statistics and biases, especially during finetuning on domain-specific corpora. In this paper, we propose a simple modification to causal language modeling called conditional finetuning, which performs language modeling conditioned on a context. We show that a context can "explain away" certain corpus statistics and make the model avoid learning them. In this fashion, conditional finetuning achieves selective learning from a corpus, learning knowledge useful for downstream tasks while avoiding learning useless corpus statistics like topic biases. This selective learning effect leads to less forgetting and better stability-plasticity tradeoff in domain finetuning, potentially benefitting lifelong learning with language models.</li>
<li><strong>摘要：</strong>语言模型可以通过拟合原始文本来学习复杂的语言理解技能。它们还会不加选择地学习无用的语料库统计数据和偏差，尤其是在对特定领域语料库进行微调时。在本文中，我们提出了一种对因果语言建模的简单修改，称为条件微调，它根据上下文执行语言建模。我们表明，上下文可以“解释”某些语料库统计数据，并使模型避免学习它们。通过这种方式，条件微调可以从语料库中实现选择性学习，学习对下游任务有用的知识，同时避免学习无用的语料库统计数据（如主题偏差）。这种选择性学习效果可以减少遗忘，并在领域微调中实现更好的稳定性-可塑性权衡，从而可能有益于语言模型的终身学习。</li>
</ul>

<h3>Title: Title:
          Zyda: A 1.3T Dataset for Open Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim, James Whittington, Quentin Anthony</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Zyda: A 1.3T Dataset for Open Language Modeling(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 的规模急剧扩大，其计算和数据需求也相应激增。最先进的语言模型，即使规模相对较小，通常也需要至少一万亿个标记进行训练。这种快速发展已经超过了可用于大规模 LLM 预训练的开源数据集的增长。在本文中，我们介绍了 Zyda (Zyphra 数据集)，这是一个在宽松许可下的数据集，包含 1.3 万亿个标记，通过将几个主要的受人尊敬的开源数据集整合成一个高质量的语料库而组装而成。我们在数据集内和数据集之间应用严格的过滤和重复数据删除过程，以维护和提高从原始数据集获得的质量。我们的评估表明，Zyda 不仅可以与 Dolma、FineWeb 和 RefinedWeb 等其他开放数据集竞争，而且还大大提高了 Pythia 套件中类似模型的性能。我们严格的数据处理方法显著提高了 Zyda 的有效性，单独使用时其表现甚至优于其组成数据集中最好的部分。</li>
</ul>

<h3>Title: Title:
          RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \textbf{R}everse \textbf{KL}-Divergence-based Knowledge \textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.</li>
<li><strong>摘要：</strong>随着被遗忘权 (RTBF) 法规的通过和语言模型训练数据集的扩大，对大型语言模型 (LLM) 中的模型遗忘的研究变得更加重要。在 LLM 时代之前，机器遗忘研究主要集中在具有小参数的模型中的分类任务上。在这些任务中，要遗忘或保留的内容清晰明了。然而，随着参数大小的增加和任务变得越来越复杂，平衡遗忘质量和模型效用变得更具挑战性，特别是在涉及个人数据而非分类结果的场景中。现有的基于梯度上升及其变体的方法经常难以实现这种平衡，导致意外的信息丢失或部分遗忘。为了应对这一挑战，我们提出了 RKLD，一种新颖的基于 \textbf{R} 逆 \textbf{KL} 发散的知识 \textbf{D} 蒸馏遗忘算法，用于针对个人信息遗忘的 LLM。通过 RKLD，我们在实验中实现了显着的遗忘质量并有效地保持了模型效用。</li>
</ul>

<h3>Title: Title:
          Personalized Topic Selection Model for Topic-Grounded Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Shixuan Fan, Wei Wei, Xiaofei Wen, Xianling Mao, Jixiong Chen, Dangyang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Personalized Topic Selection Model for Topic-Grounded Dialogue(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Recently, the topic-grounded dialogue (TGD) system has become increasingly popular as its powerful capability to actively guide users to accomplish specific tasks through topic-guided conversations. Most existing works utilize side information (\eg topics or personas) in isolation to enhance the topic selection ability. However, due to disregarding the noise within these auxiliary information sources and their mutual influence, current models tend to predict user-uninteresting and contextually irrelevant topics. To build user-engaging and coherent dialogue agent, we propose a \textbf{P}ersonalized topic s\textbf{E}lection model for \textbf{T}opic-grounded \textbf{D}ialogue, named \textbf{PETD}, which takes account of the interaction of side information to selectively aggregate such information for more accurately predicting subsequent topics. Specifically, we evaluate the correlation between global topics and personas and selectively incorporate the global topics aligned with user personas. Furthermore, we propose a contrastive learning based persona selector to filter out irrelevant personas under the constraint of lacking pertinent persona annotations. Throughout the selection and generation, diverse relevant side information is considered. Extensive experiments demonstrate that our proposed method can generate engaging and diverse responses, outperforming state-of-the-art baselines across various evaluation metrics.</li>
<li><strong>摘要：</strong>近年来，基于主题的对话 (TGD) 系统因其强大的功能而越来越受欢迎，它能够通过主题引导对话主动引导用户完成特定任务。大多数现有工作单独使用辅助信息（例如主题或角色）来增强主题选择能力。然而，由于忽略了这些辅助信息源中的噪声及其相互影响，当前的模型倾向于预测用户不感兴趣和与上下文无关的主题。为了构建用户参与且连贯的对话代理，我们提出了一种基于主题的 \textbf{D} 对话的 \textbf{P} 个性化主题 s\textbf{E} 选择模型，名为 \textbf{PETD}，它考虑了辅助信息的相互作用以选择性地聚合此类信息，从而更准确地预测后续主题。具体而言，我们评估全局主题和角色之间的相关性，并选择性地纳入与用户角色一致的全局主题。此外，我们提出了一种基于对比学习的角色选择器，在缺乏相关角色注释的约束下过滤掉不相关的角色。在整个选择和生成过程中，会考虑各种相关的辅助信息。大量实验表明，我们提出的方法可以生成引人入胜且多样化的响应，在各种评估指标中的表现均优于最先进的基线。</li>
</ul>

<h3>Title: Title:
          Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Shixuan Fan, Wei Wei, Wendi Li, Xian-Ling Mao, Wenfeng Xie, Dangyang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The core of the dialogue system is to generate relevant, informative, and human-like responses based on extensive dialogue history. Recently, dialogue generation domain has seen mainstream adoption of large language models (LLMs), due to its powerful capability in generating utterances. However, there is a natural deficiency for such models, that is, inherent position bias, which may lead them to pay more attention to the nearby utterances instead of causally relevant ones, resulting in generating irrelevant and generic responses in long-term dialogue. To alleviate such problem, in this paper, we propose a novel method, named Causal Perception long-term Dialogue framework (CPD), which employs perturbation-based causal variable discovery method to extract casually relevant utterances from the dialogue history and enhances model causal perception during fine-tuning. Specifically, a local-position awareness method is proposed in CPD for inter-sentence position correlation elimination, which helps models extract causally relevant utterances based on perturbations. Then, a casual-perception fine-tuning strategy is also proposed, to enhance the capability of discovering the causal invariant factors, by differently perturbing causally relevant and non-casually relevant ones for response generation. Experimental results on two datasets prove that our proposed method can effectively alleviate the position bias for multiple LLMs and achieve significant progress compared with existing baselines.</li>
<li><strong>摘要：</strong>对话系统的核心是根据广泛的对话历史生成相关、信息丰富且像人类一样的回应。最近，大型语言模型 (LLM) 因其强大的话语生成能力而在对话生成领域成为主流。然而，此类模型有一个天然的缺陷，即固有的位置偏差，这可能导致它们更加关注附近的话语而不是有因果关系的话语，从而在长期对话中生成不相关且通用的回应。为了缓解这个问题，在本文中，我们提出了一种新方法，即因果感知长期对话框架 (CPD)，它采用基于扰动的因果变量发现方法从对话历史中提取因果相关的话语，并在微调过程中增强模型因果感知。具体而言，在 CPD 中提出了一种局部位置感知方法来消除句子间位置相关性，这有助于模型根据扰动提取因果相关的话语。然后，还提出了一种因果感知微调策略，通过对因果相关和非因果相关的因素进行不同的扰动来增强发现因果不变因素的能力，以生成响应。在两个数据集上的实验结果证明，我们提出的方法可以有效缓解多个 LLM 的位置偏差，并且与现有基线相比取得了显着进步。</li>
</ul>

<h3>Title: Title:
          Why Would You Suggest That? Human Trust in Language Model Responses</h3>
<ul>
<li><strong>Authors: </strong>Manasi Sharma, Ho Chit Siu, Rohan Paleja, Jaime D. Peña</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Why Would You Suggest That? Human Trust in Language Model Responses(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现表明，人机协作的需求日益增长，尤其是在信任和依赖至关重要的创造性决策场景中。通过对 LaMP 基准的开放式新闻标题生成任务进行人工研究和模型评估，我们分析了解释的框架和存在如何影响用户信任和模型性能。总体而言，我们提供的证据表明，当用户有机会比较各种响应时，在模型响应中添加解释以证明其推理的合理性会显著提高用户对模型的自报信任度。这些解释的位置和忠实度也是重要因素。然而，当用户看到独立响应时，这些收益就会消失，这表明当单独显示响应时，人类会公平地信任所有模型响应，包括欺骗性响应。我们的研究结果敦促未来的研究更深入地研究人机协作系统中信任的细微评估。</li>
</ul>

<h3>Title: Title:
          Multimodal Reasoning with Multimodal Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Junlin Lee, Yequan Wang, Jing Li, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multimodal Reasoning with Multimodal Knowledge Graph(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM's parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 进行多模态推理经常会出现幻觉，以及 LLM 中存在知识不足或过时的情况。一些方法试图通过使用文本知识图谱来缓解这些问题，但它们单一的知识模态限制了全面的跨模态理解。在本文中，我们提出了基于多模态知识图谱的多模态推理 (MR-MKG) 方法，该方法利用多模态知识图谱 (MMKG) 来学习跨模态的丰富语义知识，显著增强了 LLM 的多模态推理能力。具体而言，关系图注意网络用于编码 MMKG，并设计跨模态对齐模块来优化图像-文本对齐。构建了一个基于 MMKG 的数据集，通过预训练使 LLM 具备多模态推理方面的初步专业知识。值得注意的是，MR-MKG 仅在一小部分参数上进行训练即可实现卓越的性能，大约占 LLM 参数大小的 2.25%。在多模态问答和多模态类比推理任务上的实验结果表明，我们的 MR-MKG 方法优于以前最先进的模型。</li>
</ul>

<h3>Title: Title:
          QROA: A Black-Box Query-Response Optimization Attack on LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hussein Jawad, Nicolas J.-B. BRUNEL (LaMME)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          QROA: A Black-Box Query-Response Optimization Attack on LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have surged in popularity in recent months, yet they possess concerning capabilities for generating harmful content when manipulated. This study introduces the Query-Response Optimization Attack (QROA), an optimization-based strategy designed to exploit LLMs through a black-box, query-only interaction. QROA adds an optimized trigger to a malicious instruction to compel the LLM to generate harmful content. Unlike previous approaches, QROA does not require access to the model's logit information or any other internal data and operates solely through the standard query-response interface of LLMs. Inspired by deep Q-learning and Greedy coordinate descent, the method iteratively updates tokens to maximize a designed reward function. We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\%. We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed. This study demonstrates the feasibility of generating jailbreak attacks against deployed LLMs in the public domain using black-box optimization methods, enabling more comprehensive safety testing of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 近几个月来人气飙升，但它们在被操纵时具有生成有害内容的能力，令人担忧。本研究介绍了查询响应优化攻击 (QROA)，这是一种基于优化的策略，旨在通过黑盒、仅查询交互利用 LLM。QROA 向恶意指令添加优化触发器，以迫使 LLM 生成有害内容。与以前的方法不同，QROA 不需要访问模型的逻辑信息或任何其他内部数据，并且仅通过 LLM 的标准查询响应接口运行。受深度 Q 学习和贪婪坐标下降的启发，该方法迭代更新令牌以最大化设计的奖励函数。我们在 Vicuna、Falcon 和 Mistral 等各种 LLM 上测试了我们的方法，实现了超过 80% 的攻击成功率 (ASR)。我们还针对 Llama2-chat 测试了该模型，Llama2 是经过微调的 Llama2 版本，旨在抵御越狱攻击，以次优的初始触发种子实现了良好的 ASR。本研究证明了使用黑盒优化方法对公共领域中部署的 LLM 生成越狱攻击的可行性，从而能够对 LLM 进行更全面的安全测试。</li>
</ul>

<h3>Title: Title:
          Analyzing Social Biases in Japanese Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hitomi Yanaka, Han Namgi, Ryoma Kumon, Jie Lu, Masashi Takeshita, Ryo Sekizawa, Taisei Kato, Hiromi Arai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Analyzing Social Biases in Japanese Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the development of Large Language Models (LLMs), social biases in the LLMs have become a crucial issue. While various benchmarks for social biases have been provided across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, and analyze social biases in Japanese LLMs. The results show that while current Japanese LLMs improve their accuracies on JBBQ by instruction-tuning, their bias scores become larger. In addition, augmenting their prompts with warning about social biases reduces the effect of biases in some models.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的发展，LLM 中的社会偏见已成为一个关键问题。虽然已经为不同语言提供了各种社会偏见基准，但日语 LLM 表现出社会偏见的程度尚未得到充分研究。在本研究中，我们基于英语偏见基准 BBQ 构建了日语问答偏见基准数据集 (JBBQ)，并分析了日语 LLM 中的社会偏见。结果表明，虽然当前的日语 LLM 通过指令调整提高了其在 JBBQ 上的准确率，但它们的偏见分数变得更大。此外，增加提示以警告社会偏见可以降低某些模型中偏见的影响。</li>
</ul>

<h3>Title: Title:
          I've got the "Answer"! Interpretation of LLMs Hidden States in Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Valeriya Goloviznina, Evgeny Kotelnikov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          I've got the "Answer"! Interpretation of LLMs Hidden States in Question Answering(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Interpretability and explainability of AI are becoming increasingly important in light of the rapid development of large language models (LLMs). This paper investigates the interpretation of LLMs in the context of the knowledge-based question answering. The main hypothesis of the study is that correct and incorrect model behavior can be distinguished at the level of hidden states. The quantized models LLaMA-2-7B-Chat, Mistral-7B, Vicuna-7B and the MuSeRC question-answering dataset are used to test this hypothesis. The results of the analysis support the proposed hypothesis. We also identify the layers which have a negative effect on the model's behavior. As a prospect of practical application of the hypothesis, we propose to train such "weak" layers additionally in order to improve the quality of the task solution.</li>
<li><strong>摘要：</strong>鉴于大型语言模型 (LLM) 的快速发展，人工智能的可解释性和可解释性变得越来越重要。本文研究了基于知识的问答背景下的 LLM 解释。这项研究的主要假设是，可以在隐藏状态级别区分正确和不正确的模型行为。量化模型 LLaMA-2-7B-Chat、Mistral-7B、Vicuna-7B 和 MuSeRC 问答数据集用于检验这一假设。分析结果支持了提出的假设。我们还确定了对模型行为产生负面影响的层。作为该假设实际应用的前景，我们建议额外训练这些“弱”层，以提高任务解决方案的质量。</li>
</ul>

<h3>Title: Title:
          PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling</h3>
<ul>
<li><strong>Authors: </strong>Zefan Cai., Yichi Zhang, Bofei Gao, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, Wen Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusin on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques achieving up to a 20.5 absolute accuracy improvement on TREC.</li>
<li><strong>摘要：</strong>在本研究中，我们调查了大型语言模型 (LLM) 内部基于注意力的信息流是否通过明显的模式进行聚合，以进行长上下文处理。我们的观察表明，LLM 通过金字塔信息漏斗聚合信息，其中注意力广泛分散在较低层，逐渐在特定上下文中巩固，并最终集中在较高层的关键标记（又称大规模激活或注意力汇聚）。受这些见解的启发，我们开发了 PyramidKV，一种新颖有效的 KV 缓存压缩方法。这种方法动态调整不同层之间的 KV 缓存大小，在较低层分配更多缓存，在较高层分配较少缓存，这与保持统一 KV 缓存大小的传统方法不同。我们利用 LongBench 基准进行的实验评估表明，PyramidKV 与具有完整 KV 缓存的模型的性能相匹配，同时仅保留 12% 的 KV 缓存，从而显着降低了内存使用量。在强调内存效率的场景中，仅维护 0.7% 的 KV 缓存，PyramidKV 超越了其他 KV 缓存压缩技术，在 TREC 上实现了高达 20.5 的绝对准确率提升。</li>
</ul>

<h3>Title: Title:
          Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yida Cai, Hao Sun, Hsiu-Yuan Huang, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Information Extraction (IE) plays a crucial role in Natural Language Processing (NLP) by extracting structured information from unstructured text, thereby facilitating seamless integration with various real-world applications that rely on structured data. Despite its significance, recent experiments focusing on English IE tasks have shed light on the challenges faced by Large Language Models (LLMs) in achieving optimal performance, particularly in sub-tasks like Named Entity Recognition (NER). In this paper, we delve into a comprehensive investigation of the performance of mainstream Chinese open-source LLMs in tackling IE tasks, specifically under zero-shot conditions where the models are not fine-tuned for specific tasks. Additionally, we present the outcomes of several few-shot experiments to further gauge the capability of these models. Moreover, our study includes a comparative analysis between these open-source LLMs and ChatGPT, a widely recognized language model, on IE performance. Through meticulous experimentation and analysis, we aim to provide insights into the strengths, limitations, and potential enhancements of existing Chinese open-source LLMs in the domain of Information Extraction within the context of NLP.</li>
<li><strong>摘要：</strong>信息抽取 (IE) 在自然语言处理 (NLP) 中起着至关重要的作用，它从非结构化文本中提取结构化信息，从而促进与依赖结构化数据的各种实际应用程序的无缝集成。尽管其意义重大，但最近针对英语 IE 任务的实验揭示了大型语言模型 (LLM) 在实现最佳性能方面面临的挑战，特别是在命名实体识别 (NER) 等子任务中。在本文中，我们深入研究了主流中文开源 LLM 在处理 IE 任务方面的表现，特别是在零样本条件下，模型未针对特定任务进行微调。此外，我们展示了几个少样本实验的结果，以进一步衡量这些模型的能力。此外，我们的研究还包括对这些开源 LLM 与广受认可的语言模型 ChatGPT 在 IE 性能方面的比较分析。通过细致的实验和分析，我们旨在深入了解现有中国开源 LLM 在 NLP 背景下的信息提取领域的优势、局限性和潜在的增强功能。</li>
</ul>

<h3>Title: Title:
          LongSSM: On the Length Extension of State-space Models in Language Modelling</h3>
<ul>
<li><strong>Authors: </strong>Shida Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LongSSM: On the Length Extension of State-space Models in Language Modelling(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the length-extension of state-space models (SSMs) in language modeling. Length extension involves training models on short sequences and testing them on longer ones. We show that state-space models trained with zero hidden states initialization have difficulty doing length extension. We explain this difficulty by pointing out the length extension is equivalent to polynomial extrapolation. Based on the theory, we propose a simple yet effective method - changing the hidden states initialization scheme - to improve the length extension. Moreover, our method shows that using long training sequence length is beneficial but not necessary to length extension. Changing the hidden state initialization enables the efficient training of long-memory model with a smaller training context length.</li>
<li><strong>摘要：</strong>在本文中，我们研究了语言建模中状态空间模型 (SSM) 的长度扩展。长度扩展涉及在短序列上训练模型并在较长的序列上测试它们。我们表明，使用零隐藏状态初始化训练的状态空间模型难以进行长度扩展。我们通过指出长度扩展等同于多项式外推来解释这一困难。基于该理论，我们提出了一种简单而有效的方法——改变隐藏状态初始化方案——来改进长度扩展。此外，我们的方法表明使用长训练序列长度对长度扩展有益但不是必要的。更改隐藏状态初始化使得能够以较小的训练上下文长度高效训练长记忆模型。</li>
</ul>

<h3>Title: Title:
          Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Haolong Li, Yu Ma, Yinqi Zhang, Chen Ye, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown excellent performance in language understanding, text generation, code synthesis, and many other tasks, while they still struggle in complex multi-step reasoning problems, such as mathematical reasoning. In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine-tuning on high-quality synthetic data. Experimental results with the open-llama-3B model on three different test datasets show that not only the model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also demonstrates certain generalization capabilities on the out-of-domain datasets. Specifically, this paper has designed two out-of-domain datasets in the form of extending the numerical range and the composing components of the arithmetical puzzle problem separately. The fine-tuned models have shown encouraging performance on these two far more difficult tasks with the zero-shot pass@1 at 0.33 and 0.35, respectively.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在语言理解、文本生成、代码合成等诸多任务中展现出了优异的性能，但在数学推理等复杂的多步骤推理问题上仍举步维艰。本文通过一个新提出的算术谜题问题，证明了模型通过在高质量合成数据上进行微调，可以在多步骤推理任务上取得良好表现。使用 open-llama-3B 模型在三个不同测试数据集上的实验结果表明，模型不仅能在域内数据集上达到 0.44 的 zero-shot pass@1，而且在域外数据集上也展现出一定的泛化能力。具体而言，本文分别以扩展算术谜题问题的数值范围和组成部分的形式设计了两个域外数据集。经过微调的模型在这两个难度大得多的任务上都表现出令人鼓舞的性能，zero-shot pass@1 分别达到 0.33 和 0.35。</li>
</ul>

<h3>Title: Title:
          MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Wang, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents. Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning. We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step. These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities. Our data and models are publicly accessible at this https URL.</li>
<li><strong>摘要：</strong>为了使大型语言模型 (LLM) 能够作为具有可泛化推理能力的有意识的代理发挥作用，它们必须具备推理能力，以理解由环境因素或其他代理的行为引发的分布情况变化（转换）。尽管这种能力具有根本意义，但由于对事件及其相关分布的无限可能变化进行建模的复杂性，以及缺乏具有情境转换的基准数据，这种能力仍未得到充分探索。为了解决这些差距，我们提出了一种新颖的推理公式，将分布变化作为一个三步判别过程，称为形而上学推理。然后，我们引入了第一个基准 MARS，它包含与每个步骤相对应的三个任务。这些任务系统地评估了 LLM 在推理 (i) 动作变化、(ii) 动作变化引起的状态和 (iii) 动作变化驱动的情境转换的合理性方面的能力。对 20 个不同规模和方法的 (L)LM 进行的广泛评估表明，此过程中的所有三个任务都带来了重大挑战，即使对于最先进的 LLM 和微调后的 LM 也是如此。进一步的分析揭示了 LLM 表现不佳的潜在原因，并表明在大规模概念化分类法上对它们进行预训练可以潜在地增强它们的形而上学推理能力。我们的数据和模型可通过此 https URL 公开访问。</li>
</ul>

<h3>Title: Title:
          UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu, Junzhao Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>OwnThink stands as the most extensive Chinese open-domain knowledge graph introduced in recent times. Despite prior attempts in question answering over OwnThink (OQA), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering. In this paper, we introduce UniOQA, a unified framework that integrates two complementary parallel workflows. Unlike conventional approaches, UniOQA harnesses large language models (LLMs) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement. Initially, to bolster representation capacity, we fine-tune an LLM to translate questions into the Cypher query language (CQL), tackling issues associated with restricted semantic understanding and hallucinations. Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated CQL. Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation (RAG) process to the knowledge graph. Ultimately, we optimize answer accuracy through a dynamic decision algorithm. Experimental findings illustrate that UniOQA notably advances SpCQL Logical Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new state-of-the-art results on this benchmark. Through ablation experiments, we delve into the superior representation capacity of UniOQA and quantify its performance breakthrough.</li>
<li><strong>摘要：</strong>OwnThink 是近年来推出的最为全面的中文开放领域知识图谱。尽管之前已有基于 OwnThink (OQA) 的问答系统尝试，但现有研究在模型表征能力方面仍存在局限性，对进一步提高问答系统的整体准确率提出了挑战。本文介绍了 UniOQA，这是一个集成了两个互补并行工作流程的统一框架。与传统方法不同，UniOQA 利用大型语言模型 (LLM) 进行精确问答，并结合直接答案预测过程作为经济高效的补充。首先，为了增强表征能力，我们对 LLM 进行了微调，将问题翻译成 Cypher 查询语言 (CQL)，解决了语义理解受限和幻觉相关的问题。随后，我们引入了实体和关系替换算法来确保生成的 CQL 的可执行性。同时，为了提高问答系统的整体准确率，我们进一步将检索增强生成 (RAG) 过程适应知识图谱。最后，我们通过动态决策算法优化答案准确率。实验结果表明，UniOQA 将 SpCQL 逻辑准确率提升至 21.2%，执行准确率提升至 54.9%，在该基准上取得了新的最佳成绩。通过消融实验，我们深入研究了 UniOQA 的卓越表示能力，并量化了其性能突破。</li>
</ul>

<h3>Title: Title:
          Diver: Large Language Model Decoding with Span-Level Mutual Information Verification</h3>
<ul>
<li><strong>Authors: </strong>Jinliang Lu, Chen Wang, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Diver: Large Language Model Decoding with Span-Level Mutual Information Verification(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown impressive capabilities in adapting to various tasks when provided with task-specific instructions. However, LLMs using standard decoding strategies often struggle with deviations from the inputs. Intuitively, compliant LLM outputs should reflect the information present in the input, which can be measured by point-wise mutual information (PMI) scores. Therefore, we propose Diver, a novel approach that enhances LLM Decoding through span-level PMI verification. During inference, Diver first identifies divergence steps that may lead to multiple candidate spans. Subsequently, it calculates the PMI scores by assessing the log-likelihood gains of the input if the candidate spans are generated. Finally, the optimal span is selected based on the PMI re-ranked output distributions. We evaluate our method across various downstream tasks, and empirical results demonstrate that Diver significantly outperforms existing decoding methods in both performance and versatility.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在提供特定于任务的指令时，表现出令人印象深刻的适应各种任务的能力。然而，使用标准解码策略的 LLM 通常会难以应对与输入的偏差。直观地说，合规的 LLM 输出应该反映输入中存在的信息，这可以通过逐点互信息 (PMI) 分数来衡量。因此，我们提出了 Diver，这是一种通过跨度级 PMI 验证增强 LLM 解码的新方法。在推理过程中，Diver 首先识别可能导致多个候选跨度的发散步骤。随后，如果生成了候选跨度，它会通过评估输入的对数似然增益来计算 PMI 分数。最后，根据 PMI 重新排序的输出分布选择最佳跨度。我们在各种下游任务中评估了我们的方法，实证结果表明，Diver 在性能和多功能性方面都明显优于现有的解码方法。</li>
</ul>

<h3>Title: Title:
          The current status of large language models in summarizing radiology report impressions</h3>
<ul>
<li><strong>Authors: </strong>Danqing Hu, Shanyuan Zhang, Qing Liu, Xiaofeng Zhu, Bing Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The current status of large language models in summarizing radiology report impressions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like ChatGPT show excellent capabilities in various natural language processing tasks, especially for text generation. The effectiveness of LLMs in summarizing radiology report impressions remains unclear. In this study, we explore the capability of eight LLMs on the radiology report impression summarization. Three types of radiology reports, i.e., CT, PET-CT, and Ultrasound reports, are collected from Peking University Cancer Hospital and Institute. We use the report findings to construct the zero-shot, one-shot, and three-shot prompts with complete example reports to generate the impressions. Besides the automatic quantitative evaluation metrics, we define five human evaluation metrics, i.e., completeness, correctness, conciseness, verisimilitude, and replaceability, to evaluate the semantics of the generated impressions. Two thoracic surgeons (ZSY and LB) and one radiologist (LQ) compare the generated impressions with the reference impressions and score each impression under the five human evaluation metrics. Experimental results show that there is a gap between the generated impressions and reference impressions. Although the LLMs achieve comparable performance in completeness and correctness, the conciseness and verisimilitude scores are not very high. Using few-shot prompts can improve the LLMs' performance in conciseness and verisimilitude, but the clinicians still think the LLMs can not replace the radiologists in summarizing the radiology impressions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) （例如 ChatGPT）在各种自然语言处理任务中表现出色，尤其是在文本生成方面。LLM 在总结放射学报告印象方面的有效性仍不清楚。在本研究中，我们探讨了八个 LLM 在放射学报告印象总结方面的能力。从北京大学肿瘤医院暨研究所收集了三种类型的放射学报告，即 CT、PET-CT 和超声波报告。我们使用报告结果构建了零次、一次和三次提示，并使用完整的示例报告来生成印象。除了自动定量评估指标之外，我们还定义了五个人工评估指标，即完整性、正确性、简洁性、逼真度和可替换性，以评估所生成印象的语义。两名胸外科医生（ZSY 和 LB）和一名放射科医生（LQ）将生成的印象与参考印象进行比较，并根据五个人工评估指标对每个印象进行评分。实验结果表明，生成的印象与参考印象之间存在差距。虽然LLM在完整性和正确性方面达到了相当的表现，但简洁性和逼真度得分不是很高。使用少样本提示可以提高LLM在简洁性和逼真度方面的表现，但临床医生仍然认为LLM无法取代放射科医生总结放射学印象。</li>
</ul>

<h3>Title: Title:
          Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Bo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Learning multi-task models for jointly detecting stance and verifying rumors poses challenges due to the need for training data of stance at post level and rumor veracity at claim level, which are difficult to obtain. To address this issue, we leverage large language models (LLMs) as the foundation annotators for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed as JSDRV. We introduce a novel reinforcement tuning framework to enhance the joint predictive capabilities of LLM-based SD and RV components. Specifically, we devise a policy for selecting LLM-annotated data at the two levels, employing a hybrid reward mechanism to choose high-quality labels for effective LLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the capabilities of LLMs in the joint tasks, not only outperforming state-of-the-art methods but also generalizing to non-LLMs accommodated as task models.</li>
<li><strong>摘要：</strong>学习用于联合检测立场和验证谣言的多任务模型具有挑战性，因为需要帖子级别的立场和声明级别的谣言真实性的训练数据，而这些数据很难获得。为了解决这个问题，我们利用大型语言模型 (LLM) 作为联合立场检测 (SD) 和谣言验证 (RV) 任务的基础注释器，称为 JSDRV。我们引入了一种新颖的强化调整框架来增强基于 LLM 的 SD 和 RV 组件的联合预测能力。具体来说，我们设计了一种在两个级别选择 LLM 注释数据的策略，采用混合奖励机制来选择高质量标签，以便对两个任务进行有效的 LLM 微调。结果表明，JSDRV 提高了 LLM 在联合任务中的能力，不仅优于最先进的方法，而且还推广到作为任务模型的非 LLM。</li>
</ul>

<h3>Title: Title:
          Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qingkai Min, Qipeng Guo, Xiangkun Hu, Songfang Huang, Zheng Zhang, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events. Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions. However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences. Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks. In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM. The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting. Then, the SLM refines its learning of event representations based on these insights during fine-tuning. Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage. Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios.</li>
<li><strong>摘要：</strong>跨文档事件共指解析 (CDECR) 涉及对涉及相同现实事件的多个文档中的事件提及进行聚类。现有方法利用 BERT 等小型语言模型 (SLM) 的微调来解决事件提及上下文之间的兼容性问题。然而，由于上下文的复杂性和多样性，这些模型容易学习简单的共现。最近，像 ChatGPT 这样的大型语言模型 (LLM) 已经展示了令人印象深刻的上下文理解能力，但它们在适应特定的信息提取 (IE) 任务时遇到了挑战。在本文中，我们提出了一种用于 CDECR 的协作方法，利用通用 LLM 和特定于任务的 SLM 的功能。协作策略始于 LLM 通过提示准确全面地总结事件。然后，SLM 在微调过程中根据这些见解改进其对事件表示的学习。实验结果表明，我们的方法分别超越了大型和小型语言模型的性能，形成了互补优势。在各种数据集中，我们的方法都实现了最先进的性能，凸显了其在不同场景中的有效性。</li>
</ul>

<h3>Title: Title:
          A multilingual dataset for offensive language and hate speech detection for hausa, yoruba and igbo languages</h3>
<ul>
<li><strong>Authors: </strong>Saminu Mohammad Aliyu, Gregory Maksha Wajiga, Muhammad Murtala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A multilingual dataset for offensive language and hate speech detection for hausa, yoruba and igbo languages(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The proliferation of online offensive language necessitates the development of effective detection mechanisms, especially in multilingual contexts. This study addresses the challenge by developing and introducing novel datasets for offensive language detection in three major Nigerian languages: Hausa, Yoruba, and Igbo. We collected data from Twitter and manually annotated it to create datasets for each of the three languages, using native speakers. We used pre-trained language models to evaluate their efficacy in detecting offensive language in our datasets. The best-performing model achieved an accuracy of 90\%. To further support research in offensive language detection, we plan to make the dataset and our models publicly available.</li>
<li><strong>摘要：</strong>网络攻击性语言的泛滥使得开发有效的检测机制成为必要，尤其是在多语言环境中。本研究通过开发和引入用于检测三种主要尼日利亚语言（豪萨语、约鲁巴语和伊博语）攻击性语言的新数据集来解决这一挑战。我们从 Twitter 收集数据并手动注释，使用母语人士为这三种语言中的每一种创建数据集。我们使用预先训练的语言模型来评估它们在我们的数据集中检测攻击性语言的有效性。表现最佳的模型实现了 90% 的准确率。为了进一步支持攻击性语言检测研究，我们计划公开数据集和我们的模型。</li>
</ul>

<h3>Title: Title:
          FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tao Fan, Guoqiang Ma, Yan Kang, Hanlin Gu, Lixin Fan, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, heterogeneous, homogeneous, and one-to-one, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLP text generation tasks. Empirical results demonstrate significant performance improvements in clients' SLMs with the aid of the LLM. Furthermore, the LLM optimized by FedMKT achieves a performance comparable to that achieved through direct fine-tuning based on clients' data, highlighting the effectiveness and adaptability of FedMKT.</li>
<li><strong>摘要：</strong>联邦大型语言模型 (LLM) 的最新研究主要集中在使客户端能够协作微调其本地部署的同构 LLM，或将知识从基于服务器的 LLM 转移到下游客户端的小型语言模型 (SLM)。然而，在同时相互增强服务器的 LLM 和客户端的 SLM 方面仍然存在很大的差距。为了弥补这一差距，我们提出了 FedMKT，这是一个用于大型和小型语言模型的参数高效的联邦相互知识转移框架。该框架旨在自适应地将知识从服务器的 LLM 转移到客户端的 SLM，同时使用客户端独特的领域见解丰富 LLM。我们使用最小编辑距离 (MinED) 促进标记对齐，然后在客户端 SLM 和服务器端 LLM 之间进行选择性相互知识转移，旨在共同提高它们的性能。通过在异构、同构和一对一三种不同场景中进行大量实验，我们评估了 FedMKT 在一系列 NLP 文本生成任务中使用各种公共 LLM 和 SLM 的有效性。实证结果表明，在 LLM 的帮助下，客户的 SLM 性能得到了显著提升。此外，通过 FedMKT 优化后的 LLM 达到了与直接基于客户数据进行微调相当的性能，凸显了 FedMKT 的有效性和适应性。</li>
</ul>

<h3>Title: Title:
          Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning Compressor</h3>
<ul>
<li><strong>Authors: </strong>Chuankai Xu, Dongming Zhao, Bo Wang, Hanwen Xing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning Compressor(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Despite the prevalence of retrieval-augmented language models (RALMs), the seamless integration of these models with retrieval mechanisms to enhance performance in document-based tasks remains challenging. While some post-retrieval processing Retrieval-Augmented Generation (RAG) methods have achieved success, most still lack the ability to distinguish pertinent from extraneous information, leading to potential inconsistencies and reduced precision in the generated output, which subsequently affects the truthfulness of the language model's responses. To address these limitations, this work proposes a novel two-stage consistency learning approach for retrieved information compression in retrieval-augmented language models to enhance performance. By incorporating consistency learning, the aim is to generate summaries that maintain coherence and alignment with the intended semantic representations of a teacher model while improving faithfulness to the original retrieved documents. The proposed method is empirically validated across multiple datasets, demonstrating notable enhancements in precision and efficiency for question-answering tasks. It outperforms existing baselines and showcases the synergistic effects of combining contrastive and consistency learning paradigms within the retrieval-augmented generation framework.</li>
<li><strong>摘要：</strong>尽管检索增强语言模型 (RALM) 非常流行，但将这些模型与检索机制无缝集成以提高基于文档的任务的性能仍然具有挑战性。虽然一些检索后处理的检索增强生成 (RAG) 方法取得了成功，但大多数方法仍然缺乏区分相关信息和无关信息的能力，导致生成的输出可能存在不一致和精度降低，进而影响语言模型响应的真实性。为了解决这些限制，这项工作提出了一种新颖的两阶段一致性学习方法，用于检索增强语言模型中的检索信息压缩以提高性能。通过结合一致性学习，目标是生成与教师模型的预期语义表示保持一致和一致的摘要，同时提高对原始检索文档的忠实度。所提出的方法在多个数据集中得到了实证验证，表明问答任务的精度和效率显着提高。它超越了现有的基线，并展示了在检索增强生成框架内结合对比和一致性学习范式的协同效应。</li>
</ul>

<h3>Title: Title:
          Prompting Large Language Models with Human Error Markings for Self-Correcting Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Nathaniel Berger, Stefan Riezler, Miriam Exel, Matthias Huck</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Prompting Large Language Models with Human Error Markings for Self-Correcting Machine Translation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) pre-trained on massive amounts of unpaired language data have reached the state-of-the-art in machine translation (MT) of general domain texts, post-editing (PE) is still required to correct errors and to enhance term translation quality in specialized domains. In this paper we present a pilot study of enhancing translation memories (TM) produced by PE (source segments, machine translations, and reference translations, henceforth called PE-TM) for the needs of correct and consistent term translation in technical domains. We investigate a light-weight two-step scenario where, at inference time, a human translator marks errors in the first translation step, and in a second step a few similar examples are extracted from the PE-TM to prompt an LLM. Our experiment shows that the additional effort of augmenting translations with human error markings guides the LLM to focus on a correction of the marked errors, yielding consistent improvements over automatic PE (APE) and MT from scratch.</li>
<li><strong>摘要：</strong>尽管在大量非配对语言数据上进行预训练的大型语言模型 (LLM) 已达到通用领域文本机器翻译 (MT) 的最新水平，但仍需要后期编辑 (PE) 来纠正错误并提高专业领域的术语翻译质量。在本文中，我们进行了一项初步研究，旨在增强由 PE（源句段、机器翻译和参考翻译，以下称为 PE-TM）生成的翻译记忆 (TM)，以满足技术领域正确且一致的术语翻译的需求。我们研究了一个轻量级的两步场景，其中，在推理时，人工翻译在第一步翻译中标记错误，在第二步中从 PE-TM 中提取一些类似的示例以提示 LLM。我们的实验表明，通过人工错误标记增强翻译的额外努力会引导 LLM 专注于纠正标记的错误，从而比从头开始的自动 PE (APE) 和 MT 产生持续的改进。</li>
</ul>

<h3>Title: Title:
          mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huiyuan Lai, Malvina Nissim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual context, the question of how reliable this reasoning capability is in different languages is still open. To address it directly, we study multilingual reasoning consistency across multiple languages, using popular open-source LLMs. First, we compile the first large-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven diverse languages. Then, we introduce multilingual CoT instruction tuning to boost reasoning capability across languages, thereby improving model consistency. While existing LLMs show substantial variation across the languages we consider, and especially low performance for lesser resourced languages, our 7B parameter model mCoT achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models even of much larger sizes.</li>
<li><strong>摘要：</strong>具有思维链 (CoT) 的大型语言模型 (LLM) 最近成为一种强大的技术，可用于引发推理以改进各种下游任务。由于大多数研究主要集中在英语上，在多语言环境中的探索很少，因此这种推理能力在不同语言中的可靠性问题仍未得到解决。为了直接解决这个问题，我们使用流行的开源 LLM 研究跨多种语言的多语言推理一致性。首先，我们编译了第一个大规模多语言数学推理数据集 mCoT-MAT​​H，涵盖了 11 种不同的语言。然后，我们引入了多语言 CoT 指令调整，以增强跨语言的推理能力，从而提高模型一致性。虽然现有的 LLM 在我们考虑的语言之间表现出很大的差异，尤其是资源较少的语言的性能较低，但我们的 7B 参数模型 mCoT 实现了跨语言的令人印象深刻的一致性，并且即使规模大得多，其性能也优于或可与闭源和开源模型相媲美。</li>
</ul>

<h3>Title: Title:
          Technical Language Processing for Telecommunications Specifications</h3>
<ul>
<li><strong>Authors: </strong>Felipe A. Rodriguez Y.</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Technical Language Processing for Telecommunications Specifications(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are continuously being applied in a more diverse set of contexts. At their current state, however, even state-of-the-art LLMs such as Generative Pre-Trained Transformer 4 (GTP-4) have challenges when extracting information from real-world technical documentation without a heavy preprocessing. One such area with real-world technical documentation is telecommunications engineering, which could greatly benefit from domain-specific LLMs. The unique format and overall structure of telecommunications internal specifications differs greatly from standard English and thus it is evident that the application of out-of-the-box Natural Language Processing (NLP) tools is not a viable option. In this article, we outline the limitations of out-of-the-box NLP tools for processing technical information generated by telecommunications experts, and expand the concept of Technical Language Processing (TLP) to the telecommunication domain. Additionally, we explore the effect of domain-specific LLMs in the work of Specification Engineers, emphasizing the potential benefits of adopting domain-specific LLMs to speed up the training of experts in different telecommunications fields.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 正在不断应用于更加多样化的环境中。然而，就目前而言，即使是最先进的 LLM（例如生成式预训练 Transformer 4 (GTP-4)）在从现实世界的技术文档中提取信息时也会面临挑战，因为无需进行大量预处理。电信工程就是一个拥有现实世界技术文档的领域，它可以从特定领域的 LLM 中受益匪浅。电信内部规范的独特格式和整体结构与标准英语大不相同，因此很明显，使用现成的自然语言处理 (NLP) 工具并不是一个可行的选择。在本文中，我们概述了现成的 NLP 工具在处理电信专家生成的技术信息方面的局限性，并将技术语言处理 (TLP) 的概念扩展到电信领域。此外，我们还探讨了特定领域的 LLM 对规范工程师工作的影响，强调了采用特定领域的 LLM 来加快不同电信领域专家培训的潜在好处。</li>
</ul>

<h3>Title: Title:
          Probing the Category of Verbal Aspect in Transformer Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anisia Katinskaia, Roman Yangarber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Probing the Category of Verbal Aspect in Transformer Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We investigate how pretrained language models (PLM) encode the grammatical category of verbal aspect in Russian. Encoding of aspect in transformer LMs has not been studied previously in any language. A particular challenge is posed by "alternative contexts": where either the perfective or the imperfective aspect is suitable grammatically and semantically. We perform probing using BERT and RoBERTa on alternative and non-alternative contexts. First, we assess the models' performance on aspect prediction, via behavioral probing. Next, we examine the models' performance when their contextual representations are substituted with counterfactual representations, via causal probing. These counterfactuals alter the value of the "boundedness" feature--a semantic feature, which characterizes the action in the context. Experiments show that BERT and RoBERTa do encode aspect--mostly in their final layers. The counterfactual interventions affect perfective and imperfective in opposite ways, which is consistent with grammar: perfective is positively affected by adding the meaning of boundedness, and vice versa. The practical implications of our probing results are that fine-tuning only the last layers of BERT on predicting aspect is faster and more effective than fine-tuning the whole model. The model has high predictive uncertainty about aspect in alternative contexts, which tend to lack explicit hints about the boundedness of the described action.</li>
<li><strong>摘要：</strong>我们研究了预训练语言模型 (PLM) 如何对俄语动词体貌的语法类别进行编码。之前尚未在任何语言中研究过转换语言模型中的体貌编码。“替代语境”带来了特殊挑战：在替代语境中，完成体貌或未完成体貌在语法和语义上都是合适的。我们使用 BERT 和 RoBERTa 对替代语境和非替代语境进行探测。首先，我们通过行为探测评估模型在体貌预测方面的表现。接下来，我们通过因果探测检查当模型的语境表示被反事实表示取代时，模型的表现。这些反事实会改变“有界性”特征（一种语义特征，用于描述语境中的动作）的值。实验表明，BERT 和 RoBERTa 确实对体貌进行了编码——主要是在它们的最后一层。反事实干预对完成体和未完成体的影响相反，这与语法一致：增加有界性的意义会对完成体产生积极影响，反之亦然。我们探索性结果的实际意义在于，仅对 BERT 的最后几层进行微调以预测体貌比微调整个模型更快、更有效。该模型对替代语境中的体貌具有很高的预测不确定性，而这些语境往往缺乏关于所描述动作有界性的明确提示。</li>
</ul>

<h3>Title: Title:
          LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing</h3>
<ul>
<li><strong>Authors: </strong>Maojun Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown amazing capabilities in knowledge memorization and present. However, when it comes to domain-specific knowledge and downstream tasks like medical, general LLMs are often unable to give precise answers. In addition, when people want LLMs to answer classification questions, they usually go through instruction tuning first, however, LLMs do not always give a direct index of the categorization after instruction tuning. In this paper, we proposed LlamaCare, a fine-tuned medical language model, and Extended Classification Integration(ECI), a module to handle classification problems of LLMs. Our contributions are : (i) We fine-tuned a large language model of medical knowledge with very low carbon emissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundant categorical answers and improved the performance of LLMs by proposing a new module called Extended Classification Integration. (iii) We released our processed data for one-shot and few-shot training for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method achieves a close effect with the state-of-the-art model in benchmarks while costing lower GPU resources compared to LLMs with the same quantity of parameters. Our models, codes, and datasets can be found in this https URL</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在知识记忆和呈现方面表现出了惊人的能力。然而，当涉及到领域特定知识和医疗等下游任务时，一般的LLM往往无法给出精确的答案。此外，当人们希望LLM回答分类问题时，他们通常会先进行指令调整，然而，指令调整后的LLM并不总是给出分类的直接索引。在本文中，我们提出了一个微调的医学语言模型LlamaCare，以及一个处理LLM分类问题的模块扩展分类集成（ECI）。我们的贡献是：（i）我们对一个碳排放非常低的大型医学知识语言模型进行了微调，并使用24G GPU实现了与ChatGPT类似的性能。（ii）我们通过提出一个名为扩展分类集成的新模块解决了冗余分类答案的问题并提高了LLM的性能。（iii）我们发布了一些基准测试（如PubMedQA和USMLE 1-3步）的一次性和少量训练的处理数据。我们的方法在基准测试中取得了与最先进模型相近的效果，同时与具有相同数量参数的 LLM 相比，消耗的 GPU 资源更少。我们的模型、代码和数据集可在此 https URL 中找到</li>
</ul>

<h3>Title: Title:
          Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang Huang, Shanbo Cheng, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can consistently perform well even at high compression ratios, which also offers significant benefits in terms of inference cost and throughput.</li>
<li><strong>摘要：</strong>大型语言模型的日益流行引发了人们对大型语言模型 (LLM) 上下文压缩的兴趣。然而，以前的方法的性能随着压缩率的提高而急剧下降，有时甚至会下降到闭卷水平。这种下降可以归因于压缩过程中关键信息的丢失。我们的初步研究支持这一假设，强调了保留关键信息对于在高压缩率下保持模型性能的重要性。因此，我们引入了查询引导压缩器 (QGC)，它利用查询来引导上下文压缩过程，有效地保留压缩上下文中的关键信息。此外，我们采用了动态压缩策略。我们在问答任务上验证了我们提出的 QGC 的有效性，包括 NaturalQuestions、TriviaQA 和 HotpotQA 数据集。实验结果表明，即使在高压缩率下，QGC 也能始终表现良好，这在推理成本和吞吐量方面也具有显着优势。</li>
</ul>

<h3>Title: Title:
          On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept</h3>
<ul>
<li><strong>Authors: </strong>Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Kristen Johnson, Jiliang Tang, Rongrong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can improve their responses when instructed to do so, a capability known as self-correction. When these instructions lack specific details about the issues in the response, this is referred to as leveraging the intrinsic self-correction capability. The empirical success of self-correction can be found in various applications, e.g., text detoxification and social bias mitigation. However, leveraging this self-correction capability may not always be effective, as it has the potential to revise an initially correct response into an incorrect one. In this paper, we endeavor to understand how and why leveraging the self-correction capability is effective. We identify that appropriate instructions can guide LLMs to a convergence state, wherein additional self-correction steps do not yield further performance improvements. We empirically demonstrate that model uncertainty and activated latent concepts jointly characterize the effectiveness of self-correction. Furthermore, we provide a mathematical formulation indicating that the activated latent concept drives the convergence of the model uncertainty and self-correction performance. Our analysis can also be generalized to the self-correction behaviors observed in Vision-Language Models (VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from our principle in terms of selecting effective fine-tuning samples. Such initial success demonstrates the potential extensibility for better instruction tuning and safety alignment.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可以根据指令改进其响应，这种能力称为自我纠正。当这些指令缺乏有关响应中问题的具体细节时，这被称为利用内在的自我纠正能力。自我纠正的经验成功可以在各种应用中找到，例如文本解毒和社会偏见缓解。然而，利用这种自我纠正能力可能并不总是有效的，因为它有可能将最初正确的响应修改为不正确的响应。在本文中，我们努力了解利用自我纠正能力的方式和原因。我们发现适当的指令可以引导 LLM 进入收敛状态，其中额外的自我纠正步骤不会带来进一步的性能改进。我们通过经验证明模型不确定性和激活的潜在概念共同表征了自我纠正的有效性。此外，我们提供了一个数学公式，表明激活的潜在概念推动了模型不确定性和自我纠正性能的收敛。我们的分析还可以推广到在视觉语言模型 (VLM) 中观察到的自我校正行为。此外，我们强调，在选择有效的微调样本方面，与任务无关的去偏可以从我们的原则中受益。这种初步成功证明了更好的指令调整和安全对齐的潜在可扩展性。</li>
</ul>

<h3>Title: Title:
          Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data</h3>
<ul>
<li><strong>Authors: </strong>Maxime Griot, Jean Vanderdonckt, Demet Yuksel, Coralie Hemptinne</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like ChatGPT demonstrate significant potential in the medical field, often evaluated using multiple-choice questions (MCQs) similar to those found on the USMLE. Despite their prevalence in medical education, MCQs have limitations that might be exacerbated when assessing LLMs. To evaluate the effectiveness of MCQs in assessing the performance of LLMs, we developed a fictional medical benchmark focused on a non-existent gland, the Glianorex. This approach allowed us to isolate the knowledge of the LLM from its test-taking abilities. We used GPT-4 to generate a comprehensive textbook on the Glianorex in both English and French and developed corresponding multiple-choice questions in both languages. We evaluated various open-source, proprietary, and domain-specific LLMs using these questions in a zero-shot setting. The models achieved average scores around 67%, with minor performance differences between larger and smaller models. Performance was slightly higher in English than in French. Fine-tuned medical models showed some improvement over their base versions in English but not in French. The uniformly high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs' clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills. This study underscores the need for more robust evaluation methods to better assess the true capabilities of LLMs in medical contexts.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM)（如 ChatGPT）在医学领域展现出巨大潜力，通常使用类似于 USMLE 中的多项选择题 (MCQ) 进行评估。尽管 MCQ 在医学教育中很普遍，但它的局限性在评估 LLM 时可能会加剧。为了评估 MCQ 在评估 LLM 表现方面的有效性，我们开发了一个虚构的医学基准，重点关注一个不存在的腺体 Glianorex。这种方法使我们能够将 LLM 的知识与其应试能力分离开来。我们使用 GPT-4 用英语和法语生成了一本关于 Glianorex 的综合教科书，并用这两种语言开发了相应的多项选择题。我们在零样本设置中使用这些问题评估了各种开源、专有和特定领域的 LLM。这些模型的平均得分约为 67%，较大和较小模型之间的性能差异很小。英语的表现略高于法语。经过微调的医学模型在英语方面比其基本版本有所改进，但在法语方面没有。各个模型的统一高绩效表明，传统的基于 MCQ 的基准可能无法准确衡量法学硕士的临床知识和推理能力，而是突出他们的模式识别技能。这项研究强调需要更强大的评估方法来更好地评估法学硕士在医学背景下的真正能力。</li>
</ul>

<h3>Title: Title:
          Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.</li>
<li><strong>摘要：</strong>随着在线新闻数量的不断增加，数字领域正在迅速发展，这凸显了对复杂事件进行快速而精确的分析的必要性。我们将由一段较长时间内的许多新闻文章组成的复杂事件称为时间复杂事件 (TCE)。本文提出了一种新方法，使用大型语言模型 (LLM) 系统地提取和分析 TCE 中的事件链，以其关键点和时间戳为特征。我们建立了一个名为 TCELongBench 的基准，以评估 LLM 处理时间动态和理解大量文本的能力。该基准涵盖三个不同的任务——阅读理解、时间排序和未来事件预测。在实验中，我们利用检索增强生成 (RAG) 方法和具有长上下文窗口的 LLM 来处理 TCE 的长篇新闻文章。我们的研究结果表明，具有合适检索器的模型与使用长上下文窗口的模型表现出相当的性能。</li>
</ul>

<h3>Title: Title:
          Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion</h3>
<ul>
<li><strong>Authors: </strong>Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the help of simple fine-tuning, one can artificially embed hidden text into large language models (LLMs). This text is revealed only when triggered by a specific query to the LLM. Two primary applications are LLM fingerprinting and steganography. In the context of LLM fingerprinting, a unique text identifier (fingerprint) is embedded within the model to verify licensing compliance. In the context of steganography, the LLM serves as a carrier for hidden messages that can be disclosed through a designated trigger. Our work demonstrates that embedding hidden text in the LLM via fine-tuning, though seemingly secure due to the vast number of potential triggers (any sequence of characters or tokens could serve as a trigger), is susceptible to extraction through analysis of the LLM's output decoding process. We propose a novel approach to extraction called Unconditional Token Forcing. It is premised on the hypothesis that iteratively feeding each token from the LLM's vocabulary into the model should reveal sequences with abnormally high token probabilities, indicating potential embedded text candidates. Additionally, our experiments show that when the first token of a hidden fingerprint is used as an input, the LLM not only produces an output sequence with high token probabilities, but also repetitively generates the fingerprint itself. We also present a method to hide text in such a way that it is resistant to Unconditional Token Forcing, which we named Unconditional Token Forcing Confusion.</li>
<li><strong>摘要：</strong>借助简单的微调，人们可以将隐藏文本人为地嵌入到大型语言模型 (LLM) 中。只有当对 LLM 的特定查询触发时，才会显示此文本。两个主要应用是 LLM 指纹识别和隐写术。在 LLM 指纹识别中，模型中嵌入一个唯一的文本标识符（指纹）来验证许可合规性。在隐写术中，LLM 充当隐藏消息的载体，可以通过指定的触发器披露这些消息。我们的工作表明，通过微调将隐藏文本嵌入 LLM 中虽然由于大量潜在触发器（任何字符或标记序列都可以作为触发器）而看似安全，但很容易通过分析 LLM 的输出解码过程提取。我们提出了一种称为无条件标记强制的新型提取方法。它基于这样的假设：将 LLM 词汇表中的每个标记迭代地输入到模型中应该会显示具有异常高标记概率的序列，从而表明潜在的嵌入文本候选。此外，我们的实验表明，当隐藏指纹的第一个标记用作输入时，LLM 不仅会产生具有高标记概率的输出序列，还会重复生成指纹本身。我们还提出了一种隐藏文本的方法，使其能够抵抗无条件标记强制，我们将其称为无条件标记强制混淆。</li>
</ul>

<h3>Title: Title:
          CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks</h3>
<ul>
<li><strong>Authors: </strong>Maciej Besta, Lorenzo Paleari, Ales Kubicek, Piotr Nyczyk, Robert Gerstenberger, Patrick Iff, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are revolutionizing various domains, yet verifying their answers remains a significant challenge, especially for intricate open-ended tasks such as consolidation, summarization, and extraction of knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and simple LLM verification approach. CheckEmbed is driven by a straightforward yet powerful idea: in order to compare LLM solutions to one another or to the ground-truth, compare their corresponding answer-level embeddings obtained with a model such as GPT Text Embedding Large. This reduces a complex textual answer to a single embedding, facilitating straightforward, fast, and meaningful verification. We develop a comprehensive verification pipeline implementing the CheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for assessing the truthfulness of the LLM answers, such as embedding heatmaps and their summaries. We show how to use these metrics for deploying practical engines that decide whether an LLM answer is satisfactory or not. We apply the pipeline to real-world document analysis tasks, including term extraction and document summarization, showcasing significant improvements in accuracy, cost-effectiveness, and runtime performance compared to existing token-, sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 正在彻底改变各个领域，但验证其答案仍然是一项重大挑战，尤其是对于复杂的开放式任务，例如整合、总结和知识提取。在这项工作中，我们提出了 CheckEmbed：一种准确、可扩展且简单的 LLM 验证方法。CheckEmbed 由一个简单而强大的想法驱动：为了将 LLM 解决方案相互比较或与基本事实进行比较，请比较使用 GPT Text Embedding Large 等模型获得的相应答案级嵌入。这将复杂的文本答案简化为单个嵌入，从而促进直接、快速和有意义的验证。我们开发了一个实施 CheckEmbed 方法的综合验证管道。CheckEmbed 管道还附带了用于评估 LLM 答案真实性的指标，例如嵌入热图及其摘要。我们展示了如何使用这些指标来部署实际引擎，以决定 LLM 答案是否令人满意。我们将该流程应用于现实世界的文档分析任务，包括术语提取和文档摘要，与现有的标记、句子和事实级方案（如 BERTScore 或 SelfCheckGPT）相比，准确性、成本效益和运行时性能有显着改善。</li>
</ul>

<h3>Title: Title:
          Scalable MatMul-free Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Scalable MatMul-free Language Modeling(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at \url{this https URL}.</li>
<li><strong>摘要：</strong>矩阵乘法 (MatMul) 通常占据大型语言模型 (LLM) 总体计算成本的主导地位。随着 LLM 扩展到更大的嵌入维度和上下文长度，此成本只会增加。在这项工作中，我们表明 MatMul 操作可以完全从 LLM 中消除，同时在十亿参数规模下保持强劲性能。我们的实验表明，我们提出的无 MatMul 模型实现了与最先进的 Transformers 相当的性能，后者在推理期间需要更多的内存，规模至少达到 2.7B 参数。我们研究了缩放规律，发现我们的无 MatMul 模型和全精度 Transformers 之间的性能差距随着模型尺寸的增加而缩小。我们还提供了此模型的 GPU 高效实现，与未优化的基线相比，在训练期间将内存使用量降低了 61%。通过在推理过程中使用优化的内核，与未优化的模型相比，我们的模型的内存消耗可以减少 10 倍以上。为了正确量化我们架构的效率，我们在 FPGA 上构建了一个自定义硬件解决方案，该解决方案利用了 GPU 无法处理的轻量级操作。我们以超出人类可读吞吐量 13W 的速度处理了十亿参数规模的模型，使 LLM 更接近类似大脑的效率。这项工作不仅展示了 LLM 可以在多大程度上被精简同时仍然保持高效运行，还指出了未来加速器在处理下一代轻量级 LLM 时应该优化的操作类型。我们的代码实现可在 \url{此 https URL} 上找到。</li>
</ul>

<h3>Title: Title:
          SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices</h3>
<ul>
<li><strong>Authors: </strong>Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a "cache" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.</li>
<li><strong>摘要：</strong>随着大型语言模型得到广泛采用，高效运行它们变得至关重要。最近关于 LLM 推理的研究使用推测解码来实现极速加速。然而，这些研究大多隐含地为高端数据中心硬件设计了算法。在这项工作中，我们提出了相反的问题：我们能在消费级机器上多快运行 LLM？消费级 GPU 不再适合最大的可用模型（50B+ 参数），必须将它们卸载到 RAM 或 SSD 上。当使用卸载的参数运行时，推理引擎可以同时处理数百或数千个标记的批次，就像只处理一个标记一样，这使其非常适合推测解码。我们提出了 SpecExec（推测执行），这是一种简单的并行解码方法，可以为流行的 LLM 系列在每次目标模型迭代中生成最多 20 个标记。它利用了现代 LLM 中标记概率分布的高尖峰性和模型输出概率之间的高度一致。 SpecExec 从草稿模型中获取最有可能的令牌延续，为目标模型构建“缓存”树，然后在一次传递中对其进行验证。使用 SpecExec，我们演示了在消费级 GPU 上以每秒 4-6 个令牌（4 位量化）或每秒 2-3 个令牌（16 位权重）进行 50B+ 参数 LLM 推理，并使用 RAM 卸载。</li>
</ul>

<h3>Title: Title:
          Mitigate Position Bias in Large Language Models via Scaling a Single Dimension</h3>
<ul>
<li><strong>Authors: </strong>Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Mitigate Position Bias in Large Language Models via Scaling a Single Dimension(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as "lost in the middle", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其出色的泛化能力和强大的生成能力而越来越多地应用于各种实际场景。然而，它们表现出位置偏差，也称为“迷失在中间”，这种现象在长上下文场景中尤为明显，这表明关键信息在提示的不同位置的放置会显著影响准确性。本文首先探讨了位置偏差的微观表现，得出结论，注意力权重是位置偏差的微观表现。它进一步发现，除了位置嵌入之外，因果注意力掩码也通过创建特定于位置的隐藏状态来导致位置偏差。基于这些见解，我们提出了一种通过扩展此位置隐藏状态来减轻位置偏差的方法。在 NaturalQuestions 多文档问答、KV 检索、LongBench 和时间线重新排序任务上使用各种模型（包括 RoPE 模型、上下文窗口扩展模型和 Alibi 模型）进行的实验证明了我们方法的有效性和可推广性。我们的方法只需修改隐藏状态的一个维度，即可将性能提高高达 15.2%。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Title:
          TopViewRS: Vision-Language Models as Top-View Spatial Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vulić</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          TopViewRS: Vision-Language Models as Top-View Spatial Reasoners(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored. In this work, we thus study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.</li>
<li><strong>摘要：</strong>俯视视角是人类阅读和推理不同类型地图的典型方式，它对于人类以及“非人类”代理（例如由大型视觉语言模型 (VLM) 支持的代理）的定位和导航至关重要。尽管如此，现代 VLM 的空间推理能力仍未得到证实和充分探索。因此，在这项工作中，我们研究了它们从俯视理解和推理空间关系的能力。对俯视的关注还使得能够以不同的空间推理粒度进行受控评估；我们清楚地区分了不同的能力（例如，识别特定对象与理解它们的相对位置）。我们引入了 TopViewRS（空间俯视推理）数据集，该数据集包含 11,384 个多项选择题，以现实或语义俯视地图作为视觉输入。然后，我们使用它来研究和评估 4 个不同复杂程度的感知和推理任务中的 VLM。对 10 个具有代表性的开源和闭源 VLM 的评估表明，与人类平均表现相比，VLM 的差距超过 50%，在某些情况下甚至低于随机基线。尽管其他实验表明，思维链推理平均可以将模型能力提高 5.82%，但 VLM 的整体性能仍然有限。我们的研究结果强调了在顶视图空间推理中增强模型能力的迫切需求，并为进一步研究 VLM 在现实世界多模态任务中达到人类水平的能力奠定了基础。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
