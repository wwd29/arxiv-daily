<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-01</h1>
<h3>Title: Fragile Mastery: Are Domain-Specific Trade-Offs Undermining On-Device Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Basab Jha, Firoj Paudel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22698">https://arxiv.org/abs/2503.22698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22698">https://arxiv.org/pdf/2503.22698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22698]] Fragile Mastery: Are Domain-Specific Trade-Offs Undermining On-Device Language Models?(https://arxiv.org/abs/2503.22698)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>The application of on-device language models (ODLMs) on resource-constrained edge devices is a multi-dimensional problem that strikes a fine balance between computational effectiveness, memory, power usage, and linguistic capacity across heterogeneous tasks. This holistic study conducts a thorough investigation of the trade-offs between domain-specific optimization and cross-domain robustness, culminating in the proposal of the Generalized Edge Model (GEM), a new architecture that aims to balance specialization and generalization in a harmonious manner. With a rigorous experimental approach testing 47 well-chosen benchmarks in eight domains--healthcare, law, finance, STEM, commonsense, conversational AI, multilingual, and domain-adaptive tasks--we show that conventional optimization techniques decrease target task perplexity by 18-25% but result in a precipitous decline in general-task performance with F1 scores decreasing by 12-29%, as reported by Liu et al. GEM employs a Sparse Cross-Attention Router (SCAR) to dynamically allocate computation to a variable number of computing resources with a cross-domain F1 accuracy of 0.89 on less than 100ms latency across Raspberry Pi 4, Pixel 6, iPhone 13, and bespoke custom neural processing units (NPUs). Compared to GPT-4 Lite, GEM enhances the general-task level by 7% with respect and parity in domain-specific performance. We propose three new measurement tools--Domain Specialization Index (DSI), Generalization Gap (GG), and Cross-Domain Transfer Ratio (CDTR)--which show strong correlation between model compression intensity and brittleness.</li>
<li><strong>摘要：</strong>在资源受限的边缘设备上应用设备语言模型（ODLMS）是一个多维问题，它在跨异构任务的计算有效性，内存，功率使用和语言能力之间取得了良好的平衡。这项整体研究对域特异性优化与跨域鲁棒性之间的权衡进行了彻底的研究，最终导致广义边缘模型（GEM）的建议，这是一种新的体系结构，旨在以和谐的方式平衡专业化和概括。通过严格的实验方法测试，在八个领域中进行了47个精心挑选的基准测试 - 健康，法律，财务，STEM，常识，对话，对话性AI，多语言和域名适应性任务 - 我们表明，常规优化技术将目标任务引起的目标降低了18-25％，但报告降低了一般的TASK率降低了F1的一般性降低。 GEM采用稀疏的跨注意路由器（SCAR）将计算分配给可变数量的计算资源，跨域F1精度在Raspberry PI 4，Pixel 6，iPhone 13，iPhone 13和定制的自定义神经处理单元（NPU）的跨域F1精度为0.89。与GPT-4 Lite相比，GEM在特定于域的性能方面的尊重和奇偶校验将一般任务水平提高了7％。我们提出了三个新的测量工具 - 域专业指数（DSI），概括差距（GG）和跨域转移比（CDTR），它们在模型压缩强度和勃tritsens之间表现出很强的相关性。</li>
</ul>

<h3>Title: A Large-Scale Vision-Language Dataset Derived from Open Scientific Literature to Advance Biomedical Generalist AI</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Lozano, Min Woo Sun, James Burgess, Jeffrey J. Nirschl, Christopher Polzak, Yuhui Zhang, Liangyu Chen, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Anita Rau, Austin Wolfgang Katzer, Collin Chiu, Orr Zohar, Xiaohan Wang, Alfred Seunghoon Song, Chiang Chia-Chun, Robert Tibshirani, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22727">https://arxiv.org/abs/2503.22727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22727">https://arxiv.org/pdf/2503.22727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22727]] A Large-Scale Vision-Language Dataset Derived from Open Scientific Literature to Advance Biomedical Generalist AI(https://arxiv.org/abs/2503.22727)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, agent</a></li>
<li><strong>Abstract: </strong>Despite the excitement behind biomedical artificial intelligence (AI), access to high-quality, diverse, and large-scale data - the foundation for modern AI systems - is still a bottleneck to unlocking its full potential. To address this gap, we introduce Biomedica, an open-source dataset derived from the PubMed Central Open Access subset, containing over 6 million scientific articles and 24 million image-text pairs, along with 27 metadata fields (including expert human annotations). To overcome the challenges of accessing our large-scale dataset, we provide scalable streaming and search APIs through a web server, facilitating seamless integration with AI systems. We demonstrate the utility of the Biomedica dataset by building embedding models, chat-style models, and retrieval-augmented chat agents. Notably, all our AI models surpass previous open systems in their respective categories, underscoring the critical role of diverse, high-quality, and large-scale biomedical data.</li>
<li><strong>摘要：</strong>尽管生物医学人工智能（AI）的兴奋，但获得高质量，多样化和大规模数据的访问 - 现代AI系统的基础 - 仍然是瓶颈，可以发挥其全部潜力。为了解决这一差距，我们介绍了BioMedica，这是一种开源数据集，该数据集衍生自PubMed Central Open Access子集，其中包含超过600万个科学文章和2400万个图像文本对，以及27个元数据领域（包括专家人类注释）。为了克服访问大规模数据集的挑战，我们通过Web服务器提供可扩展的流和搜索API，从而促进与AI系统的无缝集成。我们通过构建嵌入模型，聊天风格的模型和检索式聊天代理来证明BioMedica数据集的实用性。值得注意的是，我们所有的AI模型都超过了各自类别中以前的开放系统，强调了各种，高质量和大规模生物医学数据的关键作用。</li>
</ul>

<h3>Title: Susceptibility of Large Language Models to User-Driven Factors in Medical Queries</h3>
<ul>
<li><strong>Authors: </strong>Kyung Ho Lim, Ujin Kang, Xiang Li, Jin Sung Kim, Young-Chul Jung, Sangjoon Park, Byung-Hoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22746">https://arxiv.org/abs/2503.22746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22746">https://arxiv.org/pdf/2503.22746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22746]] Susceptibility of Large Language Models to User-Driven Factors in Medical Queries(https://arxiv.org/abs/2503.22746)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in healthcare, but their reliability is heavily influenced by user-driven factors such as question phrasing and the completeness of clinical information. In this study, we examined how misinformation framing, source authority, model persona, and omission of key clinical details affect the diagnostic accuracy and reliability of LLM outputs. We conducted two experiments: one introducing misleading external opinions with varying assertiveness (perturbation test), and another removing specific categories of patient information (ablation test). Using public datasets (MedQA and Medbullets), we evaluated proprietary models (GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash) and open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All models were vulnerable to user-driven misinformation, with proprietary models especially affected by definitive and authoritative language. Assertive tone had the greatest negative impact on accuracy. In the ablation test, omitting physical exam findings and lab results caused the most significant performance drop. Although proprietary models had higher baseline accuracy, their performance declined sharply under misinformation. These results highlight the need for well-structured prompts and complete clinical context. Users should avoid authoritative framing of misinformation and provide full clinical details, especially for complex cases.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用于医疗保健中，但是它们的可靠性受到用户驱动因素（例如问题措辞和临床信息的完整性）的严重影响。在这项研究中，我们研究了错误信息框架，来源权威，模型角色以及关键临床细节的遗漏如何影响LLM产出的诊断准确性和可靠性。我们进行了两个实验：一项引入具有不同自信（扰动测试）的误导性外部意见，另一个删除特定类别的患者信息（消融测试）。使用公共数据集（MEDQA和MEDBULLETS），我们评估了专有模型（GPT-4O，Claude 3.5十四行诗，Claude 3.5 Haiku，Gemini 1.5 Pro，Gemini 1.5 1.5 Flash）和开放源模型（Llama 3 8b，Llama 3 8b，Llama 3 Med42 8b，deepseek R1 R1 8B）。所有模型都容易受到用户驱动的错误信息的影响，专有模型尤其受确定性和权威性语言影响。自信的语调对准确性产生了最大的负面影响。在消融测试中，省略身体检查结果和实验室结果导致最显着的性能下降。尽管专有模型的基线准确性较高，但其性能在错误信息下急剧下降。这些结果突出了结构良好的提示和完整的临床环境的需求。用户应避免对错误信息的权威框架，并提供完整的临床细节，尤其是对于复杂情况。</li>
</ul>

<h3>Title: Boosting Large Language Models with Mask Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Zhang, Yue Bai, Huan Wang, Yizhou Wang, Qihua Dong, Yun Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22764">https://arxiv.org/abs/2503.22764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22764">https://arxiv.org/pdf/2503.22764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22764]] Boosting Large Language Models with Mask Fine-Tuning(https://arxiv.org/abs/2503.22764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The model is usually kept integral in the mainstream large language model (LLM) fine-tuning protocols. No works have questioned whether maintaining the integrity of the model is indispensable for performance. In this work, we introduce Mask Fine-Tuning (MFT), a brand-new LLM fine-tuning paradigm to show that properly breaking the integrity of the model can surprisingly lead to improved performance. Specifically, MFT learns a set of binary masks supervised by the typical LLM fine-tuning objective. Extensive experiments show that MFT gains a consistent performance boost across various domains and backbones (e.g., 1.95%/1.88% average gain in coding with LLaMA2-7B/3.1-8B). Detailed procedures are provided to study the proposed MFT from different hyperparameter perspectives for better insight. In particular, MFT naturally updates the current LLM training protocol by deploying it on a complete well-trained model. This study extends the functionality of mask learning from its conventional network pruning context for model compression to a more general scope.</li>
<li><strong>摘要：</strong>该模型通常在主流大语言模型（LLM）微调协议中保持不可或缺。没有疑问是否要维持模型的完整性对于性能是必不可少的。在这项工作中，我们介绍了面具微调（MFT），这是一种全新的LLM微调范式，以表明正确打破模型的完整性可以令人惊讶地导致性能提高。具体而言，MFT学习了一组由典型的LLM微调目标监督的二进制口罩。广泛的实验表明，MFT在各种域和骨架上都获得了一致的性能提升（例如，用Llama2-7b/3.1-8b的编码中的1.95％/1.88％的平均增益）。提供了详细的程序，以从不同的超参数角度研究拟议的MFT，以更好地洞悉。特别是，MFT自然会通过将其部署到完整训练的模型上来更新当前的LLM培训协议。这项研究将蒙版学习的功能从其常规的网络修剪环境中以模型压缩到更一般的范围。</li>
</ul>

<h3>Title: Learning to Reason for Long-Form Story Generation</h3>
<ul>
<li><strong>Authors: </strong>Alexander Gurung, Mirella Lapata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22828">https://arxiv.org/abs/2503.22828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22828">https://arxiv.org/pdf/2503.22828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22828]] Learning to Reason for Long-Form Story Generation(https://arxiv.org/abs/2503.22828)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Generating high-quality stories spanning thousands of tokens requires competency across a variety of skills, from tracking plot and character arcs to keeping a consistent and engaging style. Due to the difficulty of sourcing labeled datasets and precise quality measurements, most work using large language models (LLMs) for long-form story generation uses combinations of hand-designed prompting techniques to elicit author-like behavior. This is a manual process that is highly dependent on the specific story-generation task. Motivated by the recent success of applying RL with Verifiable Rewards to domains like math and coding, we propose a general story-generation task (Next-Chapter Prediction) and a reward formulation (Verified Rewards via Completion Likelihood Improvement) that allows us to use an unlabeled book dataset as a learning signal for reasoning. We learn to reason over a story's condensed information and generate a detailed plan for the next chapter. Our reasoning is evaluated via the chapters it helps a story-generator create, and compared against non-trained and supervised finetuning (SFT) baselines. Pairwise human judgments reveal the chapters our learned reasoning produces are preferred across almost all metrics, and the effect is more pronounced in Scifi and Fantasy genres.</li>
<li><strong>摘要：</strong>产生跨越数千个令牌的高质量故事需要各种技能的能力，从跟踪情节和角色弧到保持一致而引人入胜的风格。由于难以采购标记数据集和精确的质量测量结果，大多数使用大型语言模型（LLM）进行长篇故事生成的工作使用了手工设计的提示技术的组合来引起作者般的行为。这是一个高度依赖特定故事生成任务的手动过程。由于最近在数学和编码等领域应用RL将RL应用于诸如数学和编码之类的领域的成功，我们提出了一项一般的故事生成任务（下一章预测）和奖励表述（通过完成的可能性改善验证的奖励），使我们能够将未标记的书数据集用作推理的学习信号。我们学会推理故事的凝结信息，并为下一章制定详细的计划。我们的推理是通过章节评估的，它有助于故事产生者创建，并与未经训练和监督的Finetunting（SFT）基线进行比较。成对的人类判断揭示了我们学到的推理所产生的章节在几乎所有指标中都优先，并且在Scifi和Fantasy类型中的效果更为明显。</li>
</ul>

<h3>Title: Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets</h3>
<ul>
<li><strong>Authors: </strong>Shanshan Bai, Anna Kruspe, Xiaoxiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22856">https://arxiv.org/abs/2503.22856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22856">https://arxiv.org/pdf/2503.22856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22856]] Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets(https://arxiv.org/abs/2503.22856)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Tweets provides valuable semantic context for earth observation tasks and serves as a complementary modality to remote sensing imagery. In building function classification (BFC), tweets are often collected using geographic heuristics and labeled via external databases, an inherently weakly supervised process that introduces both label noise and sentence level feature noise (e.g., irrelevant or uninformative tweets). While label noise has been widely studied, the impact of sentence level feature noise remains underexplored, largely due to the lack of clean benchmark datasets for controlled analysis. In this work, we propose a method for generating a synthetic oracle dataset using LLM, designed to contain only tweets that are both correctly labeled and semantically relevant to their associated buildings. This oracle dataset enables systematic investigation of noise impacts that are otherwise difficult to isolate in real-world data. To assess its utility, we compare model performance using Naive Bayes and mBERT classifiers under three configurations: real vs. synthetic training data, and cross-domain generalization. Results show that noise in real tweets significantly degrades the contextual learning capacity of mBERT, reducing its performance to that of a simple keyword-based model. In contrast, the clean synthetic dataset allows mBERT to learn effectively, outperforming Naive Bayes Bayes by a large margin. These findings highlight that addressing feature noise is more critical than model complexity in this task. Our synthetic dataset offers a novel experimental environment for future noise injection studies and is publicly available on GitHub.</li>
<li><strong>摘要：</strong>推文为地球观察任务提供了宝贵的语义上下文，并作为遥感图像的互补方式。在建筑函数分类（BFC）中，通常使用地理启发式方法收集推文，并通过外部数据库进行标记，这是一个固有的弱监督过程，引入了标签噪声和句子级别的特征噪声（例如，无关或无信息推文）。尽管已广泛研究了标签噪声，但句子级别特征噪声的影响仍未被忽视，这主要是由于缺乏干净的基准数据集进行控制分析。在这项工作中，我们提出了一种使用LLM生成合成甲骨文数据集的方法，该方法仅包含与其相关建筑物相关的正确标记和语义相关的推文。该Oracle数据集可以系统地研究噪声影响，这些噪声影响很难在现实世界中隔离。为了评估其效用，我们在三种配置下使用天真的贝叶斯和Mbert分类器比较模型性能：实际与合成训练数据和跨域概括。结果表明，实际推文中的噪声会大大降低姆伯特的上下文学习能力，从而将其性能降低到基于简单的基于关键字的模型的性能。相比之下，干净的合成数据集使Mbert可以有效地学习，从而超过了幼稚的贝叶斯贝叶斯。这些发现强调，在此任务中，解决特征噪声比模型复杂性更为重要。我们的合成数据集为未来的噪声注射研究提供了一个新颖的实验环境，并在GitHub上公开可用。</li>
</ul>

<h3>Title: Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models</h3>
<ul>
<li><strong>Authors: </strong>Bruno Coelho, Shujaat Mirza, Yuyuan Cui, Christina Pöpper, Damon McCoy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22877">https://arxiv.org/abs/2503.22877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22877">https://arxiv.org/pdf/2503.22877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22877]] Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models(https://arxiv.org/abs/2503.22877)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Fact-checking is a potentially useful application of Large Language Models (LLMs) to combat the growing dissemination of disinformation. However, the performance of LLMs varies across geographic regions. In this paper, we evaluate the factual accuracy of open and private models across a diverse set of regions and scenarios. Using a dataset containing 600 fact-checked statements balanced across six global regions we examine three experimental setups of fact-checking a statement: (1) when just the statement is available, (2) when an LLM-based agent with Wikipedia access is utilized, and (3) as a best case scenario when a Retrieval-Augmented Generation (RAG) system provided with the official fact check is employed. Our findings reveal that regardless of the scenario and LLM used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global North perform substantially better than those from the Global South. Furthermore, this gap is broadened for the more realistic case of a Wikipedia agent-based system, highlighting that overly general knowledge bases have a limited ability to address region-specific nuances. These results underscore the urgent need for better dataset balancing and robust retrieval strategies to enhance LLM fact-checking capabilities, particularly in geographically diverse contexts.</li>
<li><strong>摘要：</strong>事实检查是大型语言模型（LLM）的潜在有用的应用，以打击日益增长的虚假信息传播。但是，LLM的性能在各个地理区域都不同。在本文中，我们评估了各种区域和场景中开放模型和私人模型的事实准确性。使用包含600个事实检查的陈述的数据集在六个全球区域之间保持平衡，我们检查了事实检查的三个实验设置：（1）仅使用该陈述时，（2）当利用具有Wikipedia访问的基于LLM的代理商时，（2）将使用Wikipedia访问的代理，（3）当提供了与官方（rag in ag ins system a process in aig systare in Section the systarie feque）的最佳情况。我们的发现表明，不管使用GPT-4，Claude Sonnet和Llama在内的场景和LLM如何，全球北方的陈述的表现都比全球南方的声明要好得多。此外，对于基于Wikipedia代理的系统的更现实的情况，这一差距扩大了，强调过度通用知识库的解决方案的能力有限。这些结果强调了迫切需要更好的数据集平衡和强大的检索策略，以增强LLM事实检查功能，尤其是在地理上不同的环境中。</li>
</ul>

<h3>Title: Resona: Improving Context Copying in Linear Recurrence Models with Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Linrui Ma, Jerry Huang, Peng Lu, Prasanna Parthasarathi, Xiao-Wen Chang, Boxing Chen, Yufei Cui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22913">https://arxiv.org/abs/2503.22913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22913">https://arxiv.org/pdf/2503.22913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22913]] Resona: Improving Context Copying in Linear Recurrence Models with Retrieval(https://arxiv.org/abs/2503.22913)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent shifts in the space of large language model (LLM) research have shown an increasing focus on novel architectures to compete with prototypical Transformer-based models that have long dominated this space. Linear recurrent models have proven to be a viable competitor due to their computational efficiency. However, such models still demonstrate a sizable gap compared to Transformers in terms of in-context learning among other tasks that require recalling information from a context. In this work, we introduce __Resona__, a simple and scalable framework for augmenting linear recurrent models with retrieval. __Resona__~augments models with the ability to integrate retrieved information from the provided input context, enabling tailored behavior to diverse task requirements. Experiments on a variety of linear recurrent models demonstrate that __Resona__-augmented models observe significant performance gains on a variety of synthetic as well as real-world natural language tasks, highlighting its ability to act as a general purpose method to improve the in-context learning and language modeling abilities of linear recurrent LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）研究空间的最新变化表明，人们对新型体系结构的关注越来越多，以与长期以来一直主导该空间的典型变压器模型竞争。由于其计算效率，线性复发模型已被证明是可行的竞争对手。但是，与变压器相比，在需要从上下文中召回信息的其他任务和其他任务方面，这种模型仍然表现出很大的差距。 In this work, we introduce __Resona__, a simple and scalable framework for augmenting linear recurrent models with retrieval. __RESONA__〜增强模型，能够从提供的输入上下文中整合检索到的信息，从而使量身定制的行为达到各种任务要求。在各种线性复发模型上进行的实验表明，__Resona __-增强模型可在各种合成以及现实世界的自然语言任务上观察到显着的性能提高，从而强调了其作为提高近距离学习和语言建模能力的通用方法的能力。</li>
</ul>

<h3>Title: SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Tianyang Xu, Xiaoze Liu, Feijie Wu, Xiaoqian Wang, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22948">https://arxiv.org/abs/2503.22948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22948">https://arxiv.org/pdf/2503.22948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22948]] SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning(https://arxiv.org/abs/2503.22948)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have transformed natural language processing by learning from massive datasets, yet this rapid progress has also drawn legal scrutiny, as the ability to unintentionally generate copyrighted content has already prompted several prominent lawsuits. In this work, we introduce SUV (Selective Unlearning for Verbatim data), a selective unlearning framework designed to prevent LLM from memorizing copyrighted content while preserving its overall utility. In detail, the proposed method constructs a dataset that captures instances of copyrighted infringement cases by the targeted LLM. With the dataset, we unlearn the content from the LLM by means of Direct Preference Optimization (DPO), which replaces the verbatim copyrighted content with plausible and coherent alternatives. Since DPO may hinder the LLM's performance in other unrelated tasks, we integrate gradient projection and Fisher information regularization to mitigate the degradation. We validate our approach using a large-scale dataset of 500 famous books (predominantly copyrighted works) and demonstrate that SUV significantly reduces verbatim memorization with negligible impact on the performance on unrelated tasks. Extensive experiments on both our dataset and public benchmarks confirm the scalability and efficacy of our approach, offering a promising solution for mitigating copyright risks in real-world LLM applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过从大规模数据集中学习来改变了自然语言处理，但是这种快速的进步也引起了法律审查，因为无意间生成版权内容的能力已经引发了几项突出的诉讼。在这项工作中，我们介绍了SUV（逐字数据的选择性学习性学习），这是一个选择性的刻板框架，旨在防止LLM在保留其整体实用程序的同时记住受版权保护的内容。详细介绍，提出的方法构建了一个数据集，该数据集捕获了目标LLM的受版权侵权案例的实例。使用数据集，我们通过直接偏好优化（DPO）从LLM中学习了内容，该内容将逐字感受的内容替换为合理且相干的替代方案。由于DPO可能会阻碍LLM在其他无关任务中的表现，因此我们集成了梯度投影和Fisher信息正则化以减轻降解。我们使用500本著名书籍（主要是受版权保护的作品）的大规模数据集验证了我们的方法，并证明SUV大大减少了逐字记忆的记忆，并且对无关任务的绩效的影响可忽略不计。对我们的数据集和公共基准测试的广泛实验证实了我们方法的可扩展性和功效，为减轻现实世界中LLM应用程序中的版权风险提供了有希望的解决方案。</li>
</ul>

<h3>Title: Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Yao, Aditya Sannabhadti, Holly Wiberg, Karmel S. Shehadeh, Rema Padman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22954">https://arxiv.org/abs/2503.22954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22954">https://arxiv.org/pdf/2503.22954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22954]] Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective(https://arxiv.org/abs/2503.22954)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Medical knowledge graphs (KGs) are essential for clinical decision support and biomedical research, yet they often exhibit incompleteness due to knowledge gaps and structural limitations in medical coding systems. This issue is particularly evident in treatment mapping, where coding systems such as ICD, Mondo, and ATC lack comprehensive coverage, resulting in missing or inconsistent associations between diseases and their potential treatments. To address this issue, we have explored the use of Large Language Models (LLMs) for imputing missing treatment relationships. Although LLMs offer promising capabilities in knowledge augmentation, their application in medical knowledge imputation presents significant risks, including factual inaccuracies, hallucinated associations, and instability between and within LLMs. In this study, we systematically evaluate LLM-driven treatment mapping, assessing its reliability through benchmark comparisons. Our findings highlight critical limitations, including inconsistencies with established clinical guidelines and potential risks to patient safety. This study serves as a cautionary guide for researchers and practitioners, underscoring the importance of critical evaluation and hybrid approaches when leveraging LLMs to enhance treatment mappings on medical knowledge graphs.</li>
<li><strong>摘要：</strong>医学知识图（KGS）对于临床决策支持和生物医学研究至关重要，但是由于知识差距和医学编码系统的结构限制，它们经常表现出不完整。这个问题在治疗映射中尤为明显，在治疗映射中，ICD，Mondo和ATC等编码系统缺乏全面的覆盖范围，从而导致疾病及其潜在治疗之间缺失或不一致的关联。为了解决这个问题，我们探索了大型语言模型（LLM）的使用来推出缺失的治疗关系。尽管LLM在知识增强方面具有有希望的能力，但它们在医学知识中的应用却带来了重大风险，包括事实上的不准确性，幻觉的关联以及LLMS内部和内部的不稳定。在这项研究中，我们系统地评估了LLM驱动的处理映射，并通过基准比较评估其可靠性。我们的发现突出了临界局限性，包括与已建立的临床准则和患者安全的潜在风险的矛盾。这项研究是研究人员和从业人员的警示指南，强调了当利用LLMS增强医学知识图上的治疗映射时，批判性评估和混合方法的重要性。</li>
</ul>

<h3>Title: XL-Instruct: Synthetic Data for Cross-Lingual Open-Ended Generation</h3>
<ul>
<li><strong>Authors: </strong>Vivek Iyer, Ricardo Rei, Pinzhen Chen, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22973">https://arxiv.org/abs/2503.22973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22973">https://arxiv.org/pdf/2503.22973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22973]] XL-Instruct: Synthetic Data for Cross-Lingual Open-Ended Generation(https://arxiv.org/abs/2503.22973)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Cross-lingual open-ended generation -- i.e. generating responses in a desired language different from that of the user's query -- is an important yet understudied problem. We introduce XL-AlpacaEval, a new benchmark for evaluating cross-lingual generation capabilities in Large Language Models (LLMs), and propose XL-Instruct, a high-quality synthetic data generation method. Fine-tuning with just 8K XL-Instruct-generated instructions significantly improves model performance, increasing the win rate against GPT-4o-Mini from 7.4% to 21.5%, and improving on several fine-grained quality metrics. Additionally, models fine-tuned on XL-Instruct exhibit strong zero-shot transfer to both English-only and multilingual generation tasks. Given its consistent gains across the board, we strongly recommend incorporating XL-Instruct in the post-training pipeline of future multilingual LLMs. To facilitate further research, we will publicly and freely release the XL-Instruct and XL-AlpacaEval datasets, which constitute two of the few cross-lingual resources currently available in the literature.</li>
<li><strong>摘要：</strong>跨语言开放式一代 - 即以所需语言与用户查询不同的响应生成响应 - 是一个重要但有研究的问题。我们介绍了XL-Alpacaeval，这是一种用于评估大语言模型（LLMS）跨语言生成能力的新基准，并提出了一种高质量的合成数据生成方法XL-Instruct。仅使用8K XL教学生成的指示进行微调可显着提高模型性能，从而将GPT-4O-Mini的获胜率从7.4％提高到21.5％，并改善了几种细粒度的质量指标。此外，在XL教学上进行了微调的模型表现出强烈的零射击转移到仅英语和多语言生成任务。鉴于其一致的收益，我们强烈建议将XL教学纳入未来多语言LLM的训练后管道中。为了促进进一步的研究，我们将公开，自由发布XL教学和XL-Alpacaeval数据集，这些数据集构成了文献中目前可用的少数几种跨语性资源中的两种。</li>
</ul>

<h3>Title: FReM: A Flexible Reasoning Mechanism for Balancing Quick and Slow Thinking in Long-Context Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Zhao, Shubo Zhang, Zezhong Wang, Bin Liang, Binyang Li, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22985">https://arxiv.org/abs/2503.22985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22985">https://arxiv.org/pdf/2503.22985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22985]] FReM: A Flexible Reasoning Mechanism for Balancing Quick and Slow Thinking in Long-Context Question Answering(https://arxiv.org/abs/2503.22985)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Long-context question-answering (LCQA) systems have greatly benefited from the powerful reasoning capabilities of large language models (LLMs), which can be categorized into slow and quick reasoning modes. However, both modes have their limitations. Slow thinking generally leans to explore every possible reasoning path, which leads to heavy overthinking and wastes time. Quick thinking usually relies on pattern matching rather than truly understanding the query logic, which misses proper understanding. To address these issues, we propose FReM: Flexible Reasoning Mechanism, a method that adjusts reasoning depth according to the complexity of each question. Specifically, FReM leverages synthetic reference QA examples to provide an explicit chain of thought, enabling efficient handling of simple queries while allowing deeper reasoning for more complex ones. By doing so, FReM helps quick-thinking models move beyond superficial pattern matching and narrows the reasoning space for slow-thinking models to avoid unnecessary exploration. Experiments on seven QA datasets show that FReM improves reasoning accuracy and scalability, particularly for complex multihop questions, indicating its potential to advance LCQA methodologies.</li>
<li><strong>摘要：</strong>长篇小说提问（LCQA）系统从大型语言模型（LLMS）的强大推理能力（LLMS）中受益匪浅，这些功能可以归类为缓慢而快速的推理模式。但是，这两种模式都有其局限性。缓慢的思维通常倾向于探索所有可能的推理路径，这会导致大量思考和浪费时间。快速思考通常依赖于模式匹配，而不是真正理解查询逻辑，这错过了适当的理解。为了解决这些问题，我们提出了FREM：灵活的推理机制，该方法可根据每个问题的复杂性来调整推理深度。具体而言，FREM利用合成参考QA示例提供了明确的思想链，从而有效地处理了简单的查询，同时允许更深入地推理更复杂的查询。通过这样做，弗莱姆（Frem）帮助快速思考的模型超越了表面模式匹配，并缩小了慢速思维模型的推理空间，以避免不必要的探索。七个质量检查数据集的实验表明，FREM提高了推理的准确性和可扩展性，尤其是对于复杂的多主题问题，表明其潜力提高了LCQA方法。</li>
</ul>

<h3>Title: Sparse Mixture of Experts as Unified Competitive Learning</h3>
<ul>
<li><strong>Authors: </strong>Giang Do, Hung Le, Truyen Tran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22996">https://arxiv.org/abs/2503.22996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22996">https://arxiv.org/pdf/2503.22996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22996]] Sparse Mixture of Experts as Unified Competitive Learning(https://arxiv.org/abs/2503.22996)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sparse Mixture of Experts (SMoE) improves the efficiency of large language model training by directing input tokens to a subset of experts. Despite its success in generation tasks, its generalization ability remains an open question. In this paper, we demonstrate that current SMoEs, which fall into two categories: (1) Token Choice ;and (2) Expert Choice, struggle with tasks such as the Massive Text Embedding Benchmark (MTEB). By analyzing their mechanism through the lens of competitive learning, our study finds that the Token Choice approach may overly focus on irrelevant experts, while the Expert Choice approach risks discarding important tokens, potentially affecting performance. Motivated by this analysis, we propose Unified Competitive Learning SMoE (USMoE), a novel and efficient framework designed to improve the performance of existing SMoEs in both scenarios: with and without training. Extensive experiments across various tasks show that USMoE achieves up to a 10% improvement over traditional approaches or reduces computational inference costs by 14% while maintaining strong performance.</li>
<li><strong>摘要：</strong>专家（SMOE）的稀疏混合物通过将输入令牌引导到一部分专家来提高大语模型培训的效率。尽管它在发电任务方面取得了成功，但其概括能力仍然是一个悬而未决的问题。在本文中，我们证明了当前的Smoes分为两类：（1）令牌选择；（2）专家选择，与诸如大规模文本嵌入基准（MTEB）之类的任务斗争。通过通过竞争性学习的角度分析其机制，我们的研究发现，令牌选择方法可能会过于关注无关的专家，而专家选择方法则有可能丢弃重要代币，可能影响性能。在这种分析的驱动下，我们提出了统一的竞争学习SMOE（USMOE），这是一个新颖而高效的框架，旨在在两种情况下：有和没有培训。跨各种任务的广泛实验表明，USMOE比传统方法提高了10％的提高，或者在保持强劲绩效的同时，将计算推断成本降低了14％。</li>
</ul>

<h3>Title: S2MoE: Robust Sparse Mixture of Experts via Stochastic Learning</h3>
<ul>
<li><strong>Authors: </strong>Giang Do, Hung Le, Truyen Tran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23007">https://arxiv.org/abs/2503.23007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23007">https://arxiv.org/pdf/2503.23007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23007]] S2MoE: Robust Sparse Mixture of Experts via Stochastic Learning(https://arxiv.org/abs/2503.23007)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sparse Mixture of Experts (SMoE) enables efficient training of large language models by routing input tokens to a select number of experts. However, training SMoE remains challenging due to the issue of representation collapse. Recent studies have focused on improving the router to mitigate this problem, but existing approaches face two key limitations: (1) expert embeddings are significantly smaller than the model's dimension, contributing to representation collapse, and (2) routing each input to the Top-K experts can cause them to learn overly similar features. In this work, we propose a novel approach called Robust Sparse Mixture of Experts via Stochastic Learning (S2MoE), which is a mixture of experts designed to learn from both deterministic and non-deterministic inputs via Learning under Uncertainty. Extensive experiments across various tasks demonstrate that S2MoE achieves performance comparable to other routing methods while reducing computational inference costs by 28%.</li>
<li><strong>摘要：</strong>专家（SMOE）的稀疏混合物通过将输入令牌路由到选定的专家来进行有效的大语模型培训。但是，由于代表崩溃的问题，训练SMOE仍然具有挑战性。最近的研究重点是改善路由器以减轻此问题，但是现有方法面临两个关键局限性：（1）专家嵌入明显小于模型的维度，导致表示代表崩溃，并且（2）将每个输入路由到Top-K专家可以导致他们学习过度相似的特征。在这项工作中，我们提出了一种新颖的方法，称为专家通过随机学习（S2MOE）的稀疏混合物，该方法是专家的混合物，旨在通过在不确定性下学习从确定性和非确定性输入中学习。各种任务的广泛实验表明，S2MOE的性能与其他路由方法相当，同时将计算推断成本降低28％。</li>
</ul>

<h3>Title: A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support</h3>
<ul>
<li><strong>Authors: </strong>Yichun Feng, Jiawei Wang, Ruikun He, Lu Zhou, Yixue Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23029">https://arxiv.org/abs/2503.23029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23029">https://arxiv.org/pdf/2503.23029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23029]] A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support(https://arxiv.org/abs/2503.23029)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge graphs and large language models (LLMs) are key tools for biomedical knowledge integration and reasoning, facilitating structured organization of scientific articles and discovery of complex semantic relationships. However, current methods face challenges: knowledge graph construction is limited by complex terminology, data heterogeneity, and rapid knowledge evolution, while LLMs show limitations in retrieval and reasoning, making it difficult to uncover cross-document associations and reasoning pathways. To address these issues, we propose a pipeline that uses LLMs to construct a biomedical knowledge graph (BioStrataKG) from large-scale articles and builds a cross-document question-answering dataset (BioCDQA) to evaluate latent knowledge retrieval and multi-hop reasoning. We then introduce Integrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to enhance retrieval accuracy and knowledge reasoning. IP-RAR maximizes information recall through Integrated Reasoning-based Retrieval and refines knowledge via Progressive Reasoning-based Generation, using self-reflection to achieve deep thinking and precise contextual understanding. Experiments show that IP-RAR improves document retrieval F1 score by 20\% and answer generation accuracy by 25\% over existing methods. This framework helps doctors efficiently integrate treatment evidence for personalized medication plans and enables researchers to analyze advancements and research gaps, accelerating scientific discovery and decision-making.</li>
<li><strong>摘要：</strong>知识图和大型语言模型（LLM）是生物医学知识整合和推理的关键工具，促进了科学文章的结构化组织以及发现复杂的语义关系。但是，当前方法面临挑战：知识图构造受复杂术语，数据异质性和快速知识演变的限制，而LLMS则显示出检索和推理的局限性，因此很难揭示跨文档关联和推理途径。为了解决这些问题，我们提出了一条使用LLM的管道来构建大型文章中的生物医学知识图（BioStratakg），并构建了跨文档提问数据集（BioCDQA）来评估潜在知识检索和多霍普推理。然后，我们介绍了综合且渐进的检索提示推理（IP-RAR），以增强检索准确性和知识推理。 IP-RAR通过基于集成推理的检索来最大化信息回忆，并通过基于渐进的推理的生成来完善知识，并使用自我反思来实现深入思考和精确的上下文理解。实验表明，IP-RAR将文件检索得分提高了20 \％，而与现有方法相比，IP-RAR将生成精度提高了25 \％。该框架有助于医生有效地整合个性化药物计划的治疗证据，并使研究人员能够分析进步和研究差距，从而加速科学发现和决策。</li>
</ul>

<h3>Title: A Training-free LLM Framework with Interaction between Contextually Related Subtasks in Solving Complex Tasks</h3>
<ul>
<li><strong>Authors: </strong>Hongjia Liu, Jinlong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23053">https://arxiv.org/abs/2503.23053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23053">https://arxiv.org/pdf/2503.23053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23053]] A Training-free LLM Framework with Interaction between Contextually Related Subtasks in Solving Complex Tasks(https://arxiv.org/abs/2503.23053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable capabilities in solving complex tasks. Recent work has explored decomposing such tasks into subtasks with independent contexts. However, some contextually related subtasks may encounter information loss during execution, leading to redundant operations or execution failures. To address this issue, we propose a training-free framework with an interaction mechanism, which enables a subtask to query specific information or trigger certain actions in completed subtasks by sending requests. To implement interaction, we introduce a subtask trajectory memory to enable resumption of completed subtasks upon receiving interaction requests. Additionally, we propose a new action during execution, which generates a concise and precise description of execution process and outcomes of a subtask, to assist subsequent subtasks in determining interaction targets and requests. We evaluate our framework on interactive decision-making task WebShop and multi-hop question answering HotpotQA, with GPT-3.5 and GPT-4, and comparison results show that our framework outperforms the state-of-the-art training-free baselines.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在解决复杂的任务方面表现出了显着的功能。最近的工作探索了将这些任务分解为具有独立上下文的子任务。但是，某些与上下文相关的子任务可能会在执行过程中遇到信息损失，从而导致多余的操作或执行失败。为了解决此问题，我们提出了一个没有互动机制的无培训框架，该框架使子任务可以通过发送请求来查询特定的信息或在完成的子任务中触发某些操作。要实现互动，我们引入了一个子任务轨迹内存，以在接收交互请求后恢复完整的子任务。此外，我们在执行过程中提出了一项新操作，该操作生成了对执行过程和子任务结果的简洁明了描述，以协助后续子任务来确定交互目标和请求。我们评估了有关互动决策任务网络商店和多跳问题回答HotPotQA的框架，GPT-3.5和GPT-4，比较结果表明，我们的框架的表现优于最先进的无培训基础。</li>
</ul>

<h3>Title: Efficient Inference for Large Reasoning Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23077">https://arxiv.org/abs/2503.23077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23077">https://arxiv.org/pdf/2503.23077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23077]] Efficient Inference for Large Reasoning Models: A Survey(https://arxiv.org/abs/2503.23077)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field\footnote{this https URL}.</li>
<li><strong>摘要：</strong>大型推理模型（LRMS）通过学习推理来显着提高大语言模型（LLM）的推理能力，在复杂的任务解决中表现出有希望的表现。但是，他们的审议推理过程导致代币使用，记忆消耗和推理时间效率低下。因此，这项调查提供了专门为LRMS设计的有效推理方法的综述，重点是减轻令牌效率低下，同时保留推理质量。首先，我们引入了分类法，将最近的方法分为两个主要类别：（a）明确的紧凑型经营链（COT），该方法在保持显式推理结构的同时降低了代币，以及（b）内隐性潜在的COT，该COT在隐藏表示内部编码的推理步骤在隐藏的表示内无明确的标记。同时，我们讨论他们的优势和缺点。然后，我们对绩效和效率方面的现有方法进行了经验分析。此外，我们提出了该领域的公开挑战，包括以人为中心的可控推理，解释性和推理效率之间的权衡，确保有效推理的安全性以及更广泛的有效推理的应用。此外，我们强调了通过模型合并，新体系结构和代理路由器等技术增强LRMS推理效率的关键见解。我们希望这项工作是有价值的指南，帮助研究人员克服了这个充满活力的领域\脚注{this HTTPS url}。</li>
</ul>

<h3>Title: EventWeave: A Dynamic Framework for Capturing Core and Supporting Events in Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Zhao, Shubo Zhang, Yiming Du, Bin Liang, Baojun Wang, Zhongyang Li, Binyang Li, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23078">https://arxiv.org/abs/2503.23078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23078">https://arxiv.org/pdf/2503.23078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23078]] EventWeave: A Dynamic Framework for Capturing Core and Supporting Events in Dialogue Systems(https://arxiv.org/abs/2503.23078)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Existing large language models (LLMs) have shown remarkable progress in dialogue systems. However, many approaches still overlook the fundamental role of events throughout multi-turn interactions, leading to \textbf{incomplete context tracking}. Without tracking these events, dialogue systems often lose coherence and miss subtle shifts in user intent, causing disjointed responses. To bridge this gap, we present \textbf{EventWeave}, an event-centric framework that identifies and updates both core and supporting events as the conversation unfolds. Specifically, we organize these events into a dynamic event graph, which represents the interplay between \textbf{core events} that shape the primary idea and \textbf{supporting events} that provide critical context during the whole dialogue. By leveraging this dynamic graph, EventWeave helps models focus on the most relevant events when generating responses, thus avoiding repeated visits of the entire dialogue history. Experimental results on two benchmark datasets show that EventWeave improves response quality and event relevance without fine-tuning.</li>
<li><strong>摘要：</strong>现有的大型语言模型（LLM）在对话系统中显示出了显着的进步。但是，许多方法仍然忽略了整个多转交互的事件的基本作用，从而导致\ textbf {不完整的上下文跟踪}。在没有跟踪这些事件的情况下，对话系统通常会失去连贯性，而错过了用户意图的细微转变，从而导致响应不连续。为了弥合这一差距，我们提出\ textbf {eventweave}，这是一个以事件为中心的框架，随着对话的展开，可以识别和更新核心和支持事件。具体来说，我们将这些事件组织到动态事件图中，该图表示\ textbf {core events}之间的相互作用，该事件在整个对话过程中塑造了主要思想和\ textbf {支持事件}。通过利用此动态图，EventWeave可以帮助模型在生成响应时专注于最相关的事件，从而避免反复访问整个对话历史记录。两个基准数据集的实验结果表明，事件编织可以提高响应质量和事件相关性而无需微调。</li>
</ul>

<h3>Title: The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction</h3>
<ul>
<li><strong>Authors: </strong>Yihuai Hong, Dian Zhou, Meng Cao, Lei Yu, Zhijing Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23084">https://arxiv.org/abs/2503.23084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23084">https://arxiv.org/pdf/2503.23084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23084]] The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction(https://arxiv.org/abs/2503.23084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在各种推理基准上都表现出色，但是以前的研究表明，由于过度依赖记忆的培训示例，他们有时会努力概括不见问题。但是，在文本生成过程中，LLMS在推理和记忆之间切换的确切条件尚不清楚。在这项工作中，我们通过识别模型的残差流中的一组线性特征来控制LLMS的推理误解动力学的机械理解，这些线性控制着真正的推理和记忆回忆之间的平衡。这些功能不仅将推理任务与记忆密集型任务区分开，而且还可以操纵以在原因上影响推理任务的模型绩效。此外，我们表明，介入这些推理功能有助于模型在答案生成过程中更准确地激活最相关的解决问题的功能。我们的发现为LLMS中推理和记忆的基本机制提供了新的见解，并为开发更健壮和可解释的生成AI系统铺平了道路。</li>
</ul>

<h3>Title: Memory-Aware and Uncertainty-Guided Retrieval for Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yuelyu Ji, Rui Meng, Zhuochun Li, Daqing He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23095">https://arxiv.org/abs/2503.23095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23095">https://arxiv.org/pdf/2503.23095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23095]] Memory-Aware and Uncertainty-Guided Retrieval for Multi-Hop Question Answering(https://arxiv.org/abs/2503.23095)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Multi-hop question answering (QA) requires models to retrieve and reason over multiple pieces of evidence. While Retrieval-Augmented Generation (RAG) has made progress in this area, existing methods often suffer from two key limitations: (1) fixed or overly frequent retrieval steps, and (2) ineffective use of previously retrieved knowledge. We propose MIND (Memory-Informed and INteractive Dynamic RAG), a framework that addresses these challenges through: (i) prompt-based entity extraction to identify reasoning-relevant elements, (ii) dynamic retrieval triggering based on token-level entropy and attention signals, and (iii) memory-aware filtering, which stores high-confidence facts across reasoning steps to enable consistent multi-hop generation.</li>
<li><strong>摘要：</strong>多跳问题回答（QA）需要模型来检索和推理多个证据。虽然检索增强的生成（RAG）在该领域取得了进展，但现有方法通常受到两个关键局限性：（1）固定或过度频繁的检索步骤，以及（2）无效使用先前检索的知识。我们提出了思维（内存信息和交互式动态抹布），该框架通过以下框架来解决这些挑战：（i）基于及时的实体提取以识别与推理相关的元素，（ii）基于令牌级熵和注意力信号的动态检索触发触发，以及（iii）记忆唤醒的过滤，（iii）跨度跨度的推理跨度跨度跨度跨度跨度跨度跨度跨度的步骤，以促进跨度的步骤，以促进跨度的跨度跨越跨度的步骤，以促进跨度的跨度跨度跨度的跨度用于跨度的跨度。</li>
</ul>

<h3>Title: The realization of tones in spontaneous spoken Taiwan Mandarin: a corpus-based survey and theory-driven computational modeling</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Lu, Yu-Ying Chuang, R.Harald Baayen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23163">https://arxiv.org/abs/2503.23163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23163">https://arxiv.org/pdf/2503.23163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23163]] The realization of tones in spontaneous spoken Taiwan Mandarin: a corpus-based survey and theory-driven computational modeling(https://arxiv.org/abs/2503.23163)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>A growing body of literature has demonstrated that semantics can co-determine fine phonetic detail. However, the complex interplay between phonetic realization and semantics remains understudied, particularly in pitch realization. The current study investigates the tonal realization of Mandarin disyllabic words with all 20 possible combinations of two tones, as found in a corpus of Taiwan Mandarin spontaneous speech. We made use of Generalized Additive Mixed Models (GAMs) to model f0 contours as a function of a series of predictors, including gender, tonal context, tone pattern, speech rate, word position, bigram probability, speaker and word. In the GAM analysis, word and sense emerged as crucial predictors of f0 contours, with effect sizes that exceed those of tone pattern. For each word token in our dataset, we then obtained a contextualized embedding by applying the GPT-2 large language model to the context of that token in the corpus. We show that the pitch contours of word tokens can be predicted to a considerable extent from these contextualized embeddings, which approximate token-specific meanings in contexts of use. The results of our corpus study show that meaning in context and phonetic realization are far more entangled than standard linguistic theory predicts.</li>
<li><strong>摘要：</strong>越来越多的文献表明，语义可以共同确定性精细的语音细节。但是，语音实现和语义之间的复杂相互作用仍在研究中，尤其是在音高实现中。当前的研究调查了普通话disyllabic单词的音调实现，并在台湾普通话的自发演讲中发现了所有20种可能的组合。我们将广义加性混合模型（GAM）用于模型F0轮廓，这是一系列预测因子的函数，包括性别，音调上下文，音调模式，语音率，语音率，单词位置，Bigram概率，扬声器和词。在GAM分析中，单词和感性成为F0轮廓的关键预测指标，其效果大小超过了音调模式。对于我们数据集中的每个单词令牌，然后我们通过将GPT-2大语言模型应用于语料库的上下文中获得了上下文化的嵌入。我们表明，可以从这些上下文化的嵌入中预测单词代币的音调轮廓，在使用的上下文化嵌入中，在使用的上下文中近似特定于令牌的含义。我们的语料库研究的结果表明，在上下文和语音实现中的意义比标准语言理论预测更为纠缠。</li>
</ul>

<h3>Title: The Challenge of Achieving Attributability in Multilingual Table-to-Text Generation with Question-Answer Blueprints</h3>
<ul>
<li><strong>Authors: </strong>Aden Haussmann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23204">https://arxiv.org/abs/2503.23204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23204">https://arxiv.org/pdf/2503.23204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23204]] The Challenge of Achieving Attributability in Multilingual Table-to-Text Generation with Question-Answer Blueprints(https://arxiv.org/abs/2503.23204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Multilingual Natural Language Generation (NLG) is challenging due to the lack of training data for low-resource languages. However, some low-resource languages have up to tens of millions of speakers globally, making it important to improve NLG tools for them. Table-to-Text NLG is an excellent measure of models' reasoning abilities but is very challenging in the multilingual setting. System outputs are often not attributable, or faithful, to the data in the source table. Intermediate planning techniques like Question-Answer (QA) blueprints have been shown to improve attributability on summarisation tasks. This work explores whether QA blueprints make multilingual Table-to-Text outputs more attributable to the input tables. This paper extends the challenging multilingual Table-to-Text dataset, TaTA, which includes African languages, with QA blueprints. Sequence-to-sequence language models are then finetuned on this dataset, with and without blueprints. Results show that QA blueprints improve performance for models finetuned and evaluated only on English examples, but do not demonstrate gains in the multilingual setting. This is due to inaccuracies in machine translating the blueprints from English into target languages when generating the training data, and models failing to rely closely on the blueprints they generate. An in-depth analysis is conducted on why this is challenging.</li>
<li><strong>摘要：</strong>由于缺乏低资源语言的培训数据，多语言自然语言生成（NLG）是具有挑战性的。但是，一些低资源的语言在全球范围内具有数千万扬声器，因此为其改善NLG工具非常重要。桌面到文本NLG是模型推理能力的绝佳度量，但在多语言环境中非常具有挑战性。系统输出通常不可归因于源表中的数据。已证明诸如问答（QA）蓝图之类的中间计划技术可改善对摘要任务的归因性。这项工作探讨了QA蓝图是否使多语言表对文本输出更归因于输入表。本文扩展了具有挑战性的多语言表到文本数据集TATA，其中包括非洲语言，具有QA蓝图。然后，在此数据集上，有和没有蓝图，将对序列到序列的语言模型进行填充。结果表明，QA蓝图可改善仅在英语示例上进行固定和评估的模型的性能，但在多语言环境中没有证明增长。这是由于机器中的不准确性在生成培训数据时将蓝图从英语转换为目标语言的原因，并且模型未能紧密依赖它们生成的蓝图。对为什么具有挑战性进行了深入分析。</li>
</ul>

<h3>Title: Enhancing Knowledge Graph Completion with Entity Neighborhood and Relation Context</h3>
<ul>
<li><strong>Authors: </strong>Jianfang Chen, Kai Zhang, Aoran Gan, Shiwei Tong, Shuanghong Shen, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23205">https://arxiv.org/abs/2503.23205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23205">https://arxiv.org/pdf/2503.23205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23205]] Enhancing Knowledge Graph Completion with Entity Neighborhood and Relation Context(https://arxiv.org/abs/2503.23205)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge Graph Completion (KGC) aims to infer missing information in Knowledge Graphs (KGs) to address their inherent incompleteness. Traditional structure-based KGC methods, while effective, face significant computational demands and scalability challenges due to the need for dense embedding learning and scoring all entities in the KG for each prediction. Recent text-based approaches using language models like T5 and BERT have mitigated these issues by converting KG triples into text for reasoning. However, they often fail to fully utilize contextual information, focusing mainly on the neighborhood of the entity and neglecting the context of the relation. To address this issue, we propose KGC-ERC, a framework that integrates both types of context to enrich the input of generative language models and enhance their reasoning capabilities. Additionally, we introduce a sampling strategy to effectively select relevant context within input token constraints, which optimizes the utilization of contextual information and potentially improves model performance. Experiments on the Wikidata5M, Wiki27K, and FB15K-237-N datasets show that KGC-ERC outperforms or matches state-of-the-art baselines in predictive performance and scalability.</li>
<li><strong>摘要：</strong>知识图完成（KGC）旨在推断知识图（kgs）中缺少信息以解决其固有的不完整。传统的基于结构的KGC方法虽然有效，但由于需要密集的嵌入学习和对每个预测中的所有实体进行评分，因此面临着巨大的计算需求和可伸缩性挑战。使用T5和BERT等语言模型的最新基于文本的方法通过将KG三元组转换为文本以进行推理来减轻这些问题。但是，他们通常无法完全利用上下文信息，主要集中在实体的邻里并忽略了关系的上下文。为了解决这个问题，我们提出了KGC-ERC，该框架集成了两种类型的上下文，以丰富生成语言模型的输入并增强其推理能力。此外，我们介绍了一种抽样策略，以在输入令牌约束中有效选择相关上下文，该策略优化了上下文信息的利用并可能改善模型性能。 Wikidata5M，Wiki27K和FB15K-237-N数据集的实验表明，KGC-CERC在预测性能和可扩展性方面的表现优于最先进的基准。</li>
</ul>

<h3>Title: RECALL-MM: A Multimodal Dataset of Consumer Product Recalls for Risk Analysis using Computational Methods and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Diana Bolanos, Mohammadmehdi Ataei, Daniele Grandi, Kosa Goucher-Lambert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23213">https://arxiv.org/abs/2503.23213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23213">https://arxiv.org/pdf/2503.23213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23213]] RECALL-MM: A Multimodal Dataset of Consumer Product Recalls for Risk Analysis using Computational Methods and Large Language Models(https://arxiv.org/abs/2503.23213)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Product recalls provide valuable insights into potential risks and hazards within the engineering design process, yet their full potential remains underutilized. In this study, we curate data from the United States Consumer Product Safety Commission (CPSC) recalls database to develop a multimodal dataset, RECALL-MM, that informs data-driven risk assessment using historical information, and augment it using generative methods. Patterns in the dataset highlight specific areas where improved safety measures could have significant impact. We extend our analysis by demonstrating interactive clustering maps that embed all recalls into a shared latent space based on recall descriptions and product names. Leveraging these data-driven tools, we explore three case studies to demonstrate the dataset's utility in identifying product risks and guiding safer design decisions. The first two case studies illustrate how designers can visualize patterns across recalled products and situate new product ideas within the broader recall landscape to proactively anticipate hazards. In the third case study, we extend our approach by employing a large language model (LLM) to predict potential hazards based solely on product images. This demonstrates the model's ability to leverage visual context to identify risk factors, revealing strong alignment with historical recall data across many hazard categories. However, the analysis also highlights areas where hazard prediction remains challenging, underscoring the importance of risk awareness throughout the design process. Collectively, this work aims to bridge the gap between historical recall data and future product safety, presenting a scalable, data-driven approach to safer engineering design.</li>
<li><strong>摘要：</strong>产品召回提供了对工程设计过程中潜在风险和危害的宝贵见解，但其全部潜力仍然不足。在这项研究中，我们策划了美国消费者产品安全委员会（CPSC）的数据，回顾了数据库，以开发一个多模式数据集（Recce Recce-MM），该数据库使用历史信息为数据驱动的风险评估提供了信息，并使用生成方法来增强IT。数据集中的模式突出显示了改进安全措施的特定领域可能会产生重大影响。我们通过演示基于召回描述和产品名称的共享潜在空间中嵌入所有召回的互动聚类图来扩展分析。利用这些数据驱动的工具，我们探索了三个案例研究，以证明数据集在识别产品风险和指导更安全的设计决策方面的实用性。前两个案例研究说明了设计人员如何在更广泛的召回景观中可视化召回产品的模式，并在更广泛的召回景观中摆放新产品的想法，以主动预测危害。在第三个案例研究中，我们通过采用大型语言模型（LLM）来扩展我们的方法，以仅根据产品图像预测潜在危害。这证明了该模型利用视觉上下文来识别风险因素的能力，揭示了与许多危险类别的历史召回数据的紧密结合。但是，该分析还强调了危险预测仍然具有挑战性的领域，强调了整个设计过程中风险意识的重要性。总的来说，这项工作旨在弥合历史召回数据与未来产品安全性之间的差距，并为更安全的工程设计提供可扩展的，数据驱动的方法。</li>
</ul>

<h3>Title: Beyond speculation: Measuring the growing presence of LLM-generated texts in multilingual disinformation</h3>
<ul>
<li><strong>Authors: </strong>Dominik Macko, Aashish Anantha Ramakrishnan, Jason Samuel Lucas, Robert Moro, Ivan Srba, Adaku Uchendu, Dongwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23242">https://arxiv.org/abs/2503.23242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23242">https://arxiv.org/pdf/2503.23242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23242]] Beyond speculation: Measuring the growing presence of LLM-generated texts in multilingual disinformation(https://arxiv.org/abs/2503.23242)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Increased sophistication of large language models (LLMs) and the consequent quality of generated multilingual text raises concerns about potential disinformation misuse. While humans struggle to distinguish LLM-generated content from human-written texts, the scholarly debate about their impact remains divided. Some argue that heightened fears are overblown due to natural ecosystem limitations, while others contend that specific "longtail" contexts face overlooked risks. Our study bridges this debate by providing the first empirical evidence of LLM presence in the latest real-world disinformation datasets, documenting the increase of machine-generated content following ChatGPT's release, and revealing crucial patterns across languages, platforms, and time periods.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的复杂性增加以及产生的多语​​言文本的导致质量引起了人们对潜在的虚假信息滥用的担忧。尽管人类努力将LLM生成的内容与人写的文本区分开来，但有关其影响的学术辩论仍然存在分歧。一些人认为，由于自然的生态系统限制，恐惧的加剧被夸大了，而另一些人则认为特定的“长尾”环境面临着被忽视的风险。我们的研究通过在最新的现实世界虚假信息数据集中提供LLM存在的第一个经验证据来弥合这一辩论，从而记录了Chatgpt发布后机器生成内容的增加，并揭示了跨语言，平台和时间段的关键模式。</li>
</ul>

<h3>Title: Evaluating how LLM annotations represent diverse views on contentious topics</h3>
<ul>
<li><strong>Authors: </strong>Megan A. Brown, Shubham Atreja, Libby Hemphill, Patrick Y. Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23243">https://arxiv.org/abs/2503.23243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23243">https://arxiv.org/pdf/2503.23243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23243]] Evaluating how LLM annotations represent diverse views on contentious topics(https://arxiv.org/abs/2503.23243)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Researchers have proposed the use of generative large language models (LLMs) to label data for both research and applied settings. This literature emphasizes the improved performance of LLMs relative to other natural language models, noting that LLMs typically outperform other models on standard metrics such as accuracy, precision, recall, and F1 score. However, previous literature has also highlighted the bias embedded in language models, particularly around contentious topics such as potentially toxic content. This bias could result in labels applied by LLMs that disproportionately align with majority groups over a more diverse set of viewpoints. In this paper, we evaluate how LLMs represent diverse viewpoints on these contentious tasks. Across four annotation tasks on four datasets, we show that LLMs do not show substantial disagreement with annotators on the basis of demographics. Instead, the model, prompt, and disagreement between human annotators on the labeling task are far more predictive of LLM agreement. Our findings suggest that when using LLMs to annotate data, under-representing the views of particular groups is not a substantial concern. We conclude with a discussion of the implications for researchers and practitioners.</li>
<li><strong>摘要：</strong>研究人员建议使用生成大语言模型（LLM）为研究和应用设置标记数据。该文献强调了LLM相对于其他自然语言模型的提高性能，并指出LLM通常在准确性，精度，回忆和F1分数等标准指标上优于其他模型。但是，以前的文献还强调了语言模型中嵌入的偏见，尤其是围绕潜在有毒内容等有争议的话题。这种偏见可能会导致LLMS应用的标签，该标签与多样化的观点集合不成比例地与多数群体保持一致。在本文中，我们评估了LLM在这些有争议的任务上如何代表各种观点。在四个数据集上的四个注释任务中，我们表明LLM在人口统计的基础上并没有与注释者表现出很大的分歧。取而代之的是，在标签任务上的人类注释者之间的模型，提示和分歧更大程度地预测了LLM一致性。我们的发现表明，在使用LLMS注释数据时，代表特定组的观点不足并不是一个重大问题。最后，我们讨论了对研究人员和从业者的影响。</li>
</ul>

<h3>Title: PromptDistill: Query-based Selective Token Retention in Intermediate Layers for Efficient Large Language Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Weisheng Jin, Maojia Song, Tej Deep Pala, Yew Ken Chia, Amir Zadeh, Chuan Li, Soujanya Poria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23274">https://arxiv.org/abs/2503.23274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23274">https://arxiv.org/pdf/2503.23274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23274]] PromptDistill: Query-based Selective Token Retention in Intermediate Layers for Efficient Large Language Model Inference(https://arxiv.org/abs/2503.23274)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) tackle increasingly complex tasks and longer documents, their computational and memory costs during inference become a major bottleneck. To address this, we propose PromptDistill, a novel, training-free method that improves inference efficiency while preserving generation quality. PromptDistill identifies and retains the most informative tokens by leveraging attention interactions in early layers, preserving their hidden states while reducing the computational burden in later layers. This allows the model to focus on essential contextual information without fully processing all tokens. Unlike previous methods such as H2O and SnapKV, which perform compression only after processing the entire input, or GemFilter, which selects a fixed portion of the initial prompt without considering contextual dependencies, PromptDistill dynamically allocates computational resources to the most relevant tokens while maintaining a global awareness of the input. Experiments using our method and baseline approaches with base models such as LLaMA 3.1 8B Instruct, Phi 3.5 Mini Instruct, and Qwen2 7B Instruct on benchmarks including LongBench, InfBench, and Needle in a Haystack demonstrate that PromptDistill significantly improves efficiency while having minimal impact on output quality compared to the original models. With a single-stage selection strategy, PromptDistill effectively balances performance and efficiency, outperforming prior methods like GemFilter, H2O, and SnapKV due to its superior ability to retain essential information. Specifically, compared to GemFilter, PromptDistill achieves an overall $1\%$ to $5\%$ performance improvement while also offering better time efficiency. Additionally, we explore multi-stage selection, which further improves efficiency while maintaining strong generation performance.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLMS）处理越来越复杂的任务和更长的文档，其推断期间的计算和记忆成本成为主要瓶颈。为了解决这个问题，我们提出了提示Distill，这是一种新颖，无训练的方法，可提高推理效率，同时保持发电质量。及时迪斯蒂尔通过利用早期层次的注意力相互作用来确定并保留最有用的令牌，从而保留其隐藏状态，同时减少后来的计算负担。这使模型可以专注于基本的上下文信息，而无需完全处理所有令牌。与以前的方法（例如H2O和SNAPKV），仅在处理整个输入或GEMFILTER之后执行压缩，该方法选择了初始提示的固定部分而不考虑上下文依赖关系，请迅速将计算资源动态分配给最相关的令牌，同时保持输入的全局输入。使用我们的方法和基线方法与基本模型（例如Llama 3.1 8B指示，PHI 3.5 MINI指示）和QWEN2 7B指令（包括Longbench，Infbench和Needle）在Haystack中的基准指示的实验表明，与原始模型相比，迅速效率对效率的影响最小。通过单阶段的选择策略，迅速的迪斯蒂尔有效地平衡了性能和效率，优于诸如Gemfilter，H2O和SnapkV的先前方法，因为它具有出色的保留基本信息的能力。具体而言，与Gemfilter相比，PrestDistill的总体$ 1 \％$至$ 5 \％$ $ $ $ $，同时还提供了更高的时间效率。此外，我们探索了多阶段的选择，这进一步提高了效率，同时保持强劲的发电性能。</li>
</ul>

<h3>Title: Extracting Patient History from Clinical Text: A Comparative Study of Clinical Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hieu Nghiem, Tuan-Dung Le, Suhao Chen, Thanh Thieu, Andrew Gin, Ellie Phuong Nguyen, Dursun Delen, Johnson Thomas, Jivan Lamichhane, Zhuqi Miao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23281">https://arxiv.org/abs/2503.23281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23281">https://arxiv.org/pdf/2503.23281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23281]] Extracting Patient History from Clinical Text: A Comparative Study of Clinical Large Language Models(https://arxiv.org/abs/2503.23281)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Extracting medical history entities (MHEs) related to a patient's chief complaint (CC), history of present illness (HPI), and past, family, and social history (PFSH) helps structure free-text clinical notes into standardized EHRs, streamlining downstream tasks like continuity of care, medical coding, and quality metrics. Fine-tuned clinical large language models (cLLMs) can assist in this process while ensuring the protection of sensitive data via on-premises deployment. This study evaluates the performance of cLLMs in recognizing CC/HPI/PFSH-related MHEs and examines how note characteristics impact model accuracy. We annotated 1,449 MHEs across 61 outpatient-related clinical notes from the MTSamples repository. To recognize these entities, we fine-tuned seven state-of-the-art cLLMs. Additionally, we assessed the models' performance when enhanced by integrating, problems, tests, treatments, and other basic medical entities (BMEs). We compared the performance of these models against GPT-4o in a zero-shot setting. To further understand the textual characteristics affecting model accuracy, we conducted an error analysis focused on note length, entity length, and segmentation. The cLLMs showed potential in reducing the time required for extracting MHEs by over 20%. However, detecting many types of MHEs remained challenging due to their polysemous nature and the frequent involvement of non-medical vocabulary. Fine-tuned GatorTron and GatorTronS, two of the most extensively trained cLLMs, demonstrated the highest performance. Integrating pre-identified BME information improved model performance for certain entities. Regarding the impact of textual characteristics on model performance, we found that longer entities were harder to identify, note length did not correlate with a higher error rate, and well-organized segments with headings are beneficial for the extraction.</li>
<li><strong>摘要：</strong>提取与患者的主要投诉（CC），当前疾病史（HPI）以及过去，家庭和社会历史（PFSH）有关的病史实体（MES）有助于将自由文本临床注释构造到标准化的EHR中，从而简化了下游任务，例如连续的护理，医疗编码，医疗编码和优质的指标。微调的临床大语言模型（CLLM）可以通过本地部署来确保敏感数据的保护。这项研究评估了CLLM在识别CC/HPI/PFSH相关的MES时的性能，并检查了注意特征如何影响模型的准确性。我们在61个与MTSAMPLES存储库中的61个门诊相关临床注释中注释了1,449次木乃伊。为了认识这些实体，我们微调了七个最先进的Cllms。此外，我们通过整合，问题，测试，治疗和其他基本医疗实体（BME）评估了模型的性能。我们将这些模型的性能与零射击设置中的GPT-4O进行了比较。为了进一步了解影响模型准确性的文本特征，我们进行了一个错误分析，该分析集中在注释长度，实体长度和分段。 CLLM显示出将粘土提取超过20％的时间减少所需的时间的潜力。但是，由于其多义性质以及非医学词汇的经常参与，检测许多类型的木乃伊仍然具有挑战性。训练最广泛的两个CLLM的微型Gatortron和Gatortron的表现最高。整合预识别的BME信息改善了某些实体的模型性能。关于文本特征对模型性能的影响，我们发现较长的实体难以识别，注意长度与更高的错误率无关，并且具有标题的组织良好的段对提取有益。</li>
</ul>

<h3>Title: Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Wei Tao, Bin Zhang, Xiaoyang Qu, Jiguang Wan, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23294">https://arxiv.org/abs/2503.23294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23294">https://arxiv.org/pdf/2503.23294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23294]] Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference(https://arxiv.org/abs/2503.23294)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have been able to handle longer and longer contexts. However, a context that is too long may cause intolerant inference latency and GPU memory usage. Existing methods propose mixed-precision quantization to the key-value (KV) cache in LLMs based on token granularity, which is time-consuming in the search process and hardware inefficient during computation. This paper introduces a novel approach called Cocktail, which employs chunk-adaptive mixed-precision quantization to optimize the KV cache. Cocktail consists of two modules: chunk-level quantization search and chunk-level KV cache computation. Chunk-level quantization search determines the optimal bitwidth configuration of the KV cache chunks quickly based on the similarity scores between the corresponding context chunks and the query, maintaining the model accuracy. Furthermore, chunk-level KV cache computation reorders the KV cache chunks before quantization, avoiding the hardware inefficiency caused by mixed-precision quantization in inference computation. Extensive experiments demonstrate that Cocktail outperforms state-of-the-art KV cache quantization methods on various models and datasets.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）能够处理越来越长的上下文。但是，时间太长可能会导致不宽容的推理潜伏期和GPU内存使用量。现有方法将基于令牌粒度的LLMS中的键值（KV）缓存提出了混合精确量化，该粒度在搜索过程中耗时，计算过程中的硬件效率低下。本文介绍了一种名为Cocktail的新方法，该方法采用了大块的自适应混合精液量化来优化KV缓存。鸡尾酒由两个模块组成：块级量化搜索和块级别的KV缓存计算。块级量化搜索根据相应上下文块和查询之间的相似性得分快速确定KV高速缓存块的最佳位宽配置，从而保持模型精度。此外，块级kV缓存计算在量化之前将KV缓存块重新定位，从而避免了推理计算中混合精液量化引起的硬件无效。广泛的实验表明，鸡尾酒在各种模型和数据集上都优于最先进的KV缓存量化方法。</li>
</ul>

<h3>Title: Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts</h3>
<ul>
<li><strong>Authors: </strong>Youxiang Zhu, Ruochen Li, Danqing Wang, Daniel Haehn, Xiaohui Liang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23306">https://arxiv.org/abs/2503.23306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23306">https://arxiv.org/pdf/2503.23306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23306]] Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts(https://arxiv.org/abs/2503.23306)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Long-context large language models (LLMs) are prone to be distracted by irrelevant contexts. The reason for distraction remains poorly understood. In this paper, we first identify the contextual heads, a special group of attention heads that control the overall attention of the LLM. Then, we demonstrate that distraction arises when contextual heads fail to allocate sufficient attention to relevant contexts and can be mitigated by increasing attention to these contexts. We further identify focus directions, located at the key and query activations of these heads, which enable them to allocate more attention to relevant contexts without explicitly specifying which context is relevant. We comprehensively evaluate the effect of focus direction on various long-context tasks and find out focus directions could help to mitigate the poor task alignment of the long-context LLMs. We believe our findings could promote further research on long-context LLM alignment.</li>
<li><strong>摘要：</strong>长篇文章大语模型（LLM）容易被无关紧要的环境分散注意力。分散注意力的原因仍然很众所周知。在本文中，我们首先确定了上下文头部，这是一个控制LLM总体关注的特殊注意力头。然后，我们证明当上下文头部无法分配足够的关注对相关环境时，会出现干扰，并且可以通过越来越关注这些上下文来缓解。我们进一步确定位于这些头部的密钥和查询激活的重点指示，这使他们能够将更多关注分配给相关上下文，而无需明确指定哪些背景是相关的。我们全面评估了重点方向对各种长篇文章任务的影响，并找出聚焦方向可以帮助减轻长篇文化LLM的不良任务对齐。我们认为，我们的发现可以促进对长篇小说LLM一致性的进一步研究。</li>
</ul>

<h3>Title: Not All LoRA Parameters Are Essential: Insights on Inference Necessity</h3>
<ul>
<li><strong>Authors: </strong>Guanhua Chen, Yutong Yao, Ci-Jun Gao, Lidia S. Chao, Feng Wan, Derek F. Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23360">https://arxiv.org/abs/2503.23360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23360">https://arxiv.org/pdf/2503.23360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23360]] Not All LoRA Parameters Are Essential: Insights on Inference Necessity(https://arxiv.org/abs/2503.23360)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Current research on LoRA primarily focuses on minimizing the number of fine-tuned parameters or optimizing its architecture. However, the necessity of all fine-tuned LoRA layers during inference remains underexplored. In this paper, we investigate the contribution of each LoRA layer to the model's ability to predict the ground truth and hypothesize that lower-layer LoRA modules play a more critical role in model reasoning and understanding. To address this, we propose a simple yet effective method to enhance the performance of large language models (LLMs) fine-tuned with LoRA. Specifically, we identify a ``boundary layer'' that distinguishes essential LoRA layers by analyzing a small set of validation samples. During inference, we drop all LoRA layers beyond this boundary. We evaluate our approach on three strong baselines across four widely-used text generation datasets. Our results demonstrate consistent and significant improvements, underscoring the effectiveness of selectively retaining critical LoRA layers during inference.</li>
<li><strong>摘要：</strong>当前对洛拉的研究主要集中在最大程度地减少微型参数的数量或优化其体系结构。但是，在推理期间所有微调的洛拉层的必要性仍然没有得到充实。在本文中，我们研究了每个洛拉层对模型预测地面真理的能力的贡献，并假设下层洛拉模块在模型推理和理解中起着更为关键的作用。为了解决这个问题，我们提出了一种简单而有效的方法，以增强用洛拉微调的大语言模型（LLMS）的性能。具体而言，我们确定了``边界层''，该``边界层''通过分析一小部分验证样本来区分必需的洛拉层。在推断期间，我们将所有洛拉层都放在了这个边界之外。我们在四个广泛使用的文本生成数据集中评估了三个强基线的方法。我们的结果表明，一致和显着的改进，强调了推断过程中选择性保留关键洛拉层的有效性。</li>
</ul>

<h3>Title: Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base</h3>
<ul>
<li><strong>Authors: </strong>Linxin Song, Xuwei Ding, Jieyu Zhang, Taiwei Shi, Ryotaro Shimizu, Rahul Gupta, Yang Liu, Jian Kang, Jieyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23361">https://arxiv.org/abs/2503.23361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23361">https://arxiv.org/pdf/2503.23361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23361]] Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base(https://arxiv.org/abs/2503.23361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）具有令人印象深刻的语言能力，但通常无法忠实地保留事实知识，从而导致幻觉和不可靠的产出。通过对全尺度知识库进行详尽的评估来了解LLMS的知识缺陷在计算上是令人难以置信的，尤其是对于封闭的模型。我们提出了随机误差上升（SEA），这是一个可扩展有效的框架，用于在严格的查询预算下发现封闭重量LLM的知识缺陷（错误）。 SEA并没有天真地探测所有知识候选者，而是将错误发现作为随机优化过程：它通过利用语义相似性与以前观察到的失败的方式来迭代地检索新的高误差候选者。为了进一步提高搜索效率和覆盖范围，SEA采用跨文档和段落级别进行层次结构检索，并构建一个有针对性的无环图以建模错误传播并识别系统的故障模式。从经验上讲，海洋的知识错误比自动化能力发现多40.7倍，比自动化的能力发现多26.7％，同时分别减少599倍和9倍的每月成本。人类评估证实了产生的问题的高质量，而消融和收敛分析验证了每个组成部分在海中的贡献。对发现错误的进一步分析表明，LLM家族之间的相关故障模式以及反复出现的赤字，突出了对未来LLM开发中有更好的数据覆盖范围和针对性调查的需求。</li>
</ul>

<h3>Title: Mixture of Routers</h3>
<ul>
<li><strong>Authors: </strong>Jia-Chen Zhang, Yu-Jie Xiong, Xi-He Qiu, Chun-Ming Xia, Fei Dai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23362">https://arxiv.org/abs/2503.23362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23362">https://arxiv.org/pdf/2503.23362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23362]] Mixture of Routers(https://arxiv.org/abs/2503.23362)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is a milestone in aligning large language models with human instructions and adapting them to downstream tasks. In particular, Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter efficiency. However, its impact on improving the performance of large models remains limited. Recent studies suggest that combining LoRA with Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE adapts to the diversity and complexity of datasets by dynamically selecting the most suitable experts, thereby improving task accuracy and efficiency. Despite impressive results, recent studies reveal issues in the MoE routing mechanism, such as incorrect assignments and imbalanced expert allocation. Inspired by the principles of Redundancy and Fault Tolerance Theory. We innovatively integrate the concept of Mixture of Experts into the routing mechanism and propose an efficient fine-tuning method called Mixture of Routers (MoR). It employs multiple sub-routers for joint selection and uses a learnable main router to determine the weights of the sub-routers. The results show that MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method suitable for a wide range of applications. Our code is available here: this https URL.</li>
<li><strong>摘要：</strong>监督的微调（SFT）是将大型语言模型与人类指示保持一致的里程碑，并将其调整为下游任务。特别是，由于其参数效率，低级适应性（LORA）引起了广泛的关注。但是，它对改善大型模型的影响仍然有限。最近的研究表明，将洛拉与杂质混合物（MOE）结合起来可以显着提高微调性能。 MOE通过动态选择最合适的专家来适应数据集的多样性和复杂性，从而提高了任务准确性和效率。尽管结果令人印象深刻，但最近的研究揭示了MOE路由机制中的问题，例如不正确的任务和专家分配不平衡。受冗余和容错理论的原则的启发。我们将专家的混合概念融合到路由机制中，并提出了一种称为路由器混合物（MOR）的有效微调方法。它使用多个子路由来进行关节选择，并使用可学习的主路由器来确定子路由器的权重。结果表明，MOR在大多数任务上都优于基线模型，从而达到1％的平均绩效提高。 MOR可以用作适用于广泛应用的插件，参数有效的微调方法。我们的代码可在此处提供：此HTTPS URL。</li>
</ul>

<h3>Title: FeRG-LLM : Feature Engineering by Reason Generation Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jeonghyun Ko, Gyeongyun Park, Donghoon Lee, Kyunam Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23371">https://arxiv.org/abs/2503.23371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23371">https://arxiv.org/pdf/2503.23371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23371]] FeRG-LLM : Feature Engineering by Reason Generation Large Language Models(https://arxiv.org/abs/2503.23371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>One of the key tasks in machine learning for tabular data is feature engineering. Although it is vital for improving the performance of models, it demands considerable human expertise and deep domain knowledge, making it labor-intensive endeavor. To address this issue, we propose a novel framework, \textbf{FeRG-LLM} (\textbf{Fe}ature engineering by \textbf{R}eason \textbf{G}eneration \textbf{L}arge \textbf{L}anguage \textbf{M}odels), a large language model designed to automatically perform feature engineering at an 8-billion-parameter scale. We have constructed two-stage conversational dialogues that enable language models to analyze machine learning tasks and discovering new features, exhibiting their Chain-of-Thought (CoT) capabilities. We use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct Preference Optimization (DPO) to receive feedback improving quality of new features and the model's performance. Our experiments show that FeRG-LLM performs comparably to or better than Llama 3.1 70B on most datasets, while using fewer resources and achieving reduced inference time. It outperforms other studies in classification tasks and performs well in regression tasks. Moreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API costs when generating features, it can be deployed locally, addressing security concerns.</li>
<li><strong>摘要：</strong>手表数据的机器学习中的关键任务之一是功能工程。尽管这对于改善模型的性能至关重要，但它需要大量的人类专业知识和深层领域的知识，从而使其成为劳动密集型的努力。为了解决这个问题，我们提出了一个新颖的框架，\ textbf {ferg-llm}（\ textbf {fextbf {fextbf {fextbf {r textbf {r}工程为80亿参数。我们已经构建了两个阶段的对话对话，使语言模型能够分析机器学习任务并发现新功能，并展示其思想链（COT）功能。我们使用这些对话来微调美洲驼3.1 8B型号，并集成直接偏好优化（DPO），以接收反馈，以提高新功能的质量和模型的性能。我们的实验表明，在大多数数据集中，FERG-LLM的性能与Llama 3.1 70B相当或更好，同时使用较少的资源并实现了减少的推理时间。它在分类任务方面的表现优于其他研究，并且在回归任务中表现良好。此外，由于生成功能时它不依赖云托管的LLM，例如GPT-4具有额外API成本，因此可以在本地部署它，从而解决安全问题。</li>
</ul>

<h3>Title: ToRL: Scaling Tool-Integrated RL</h3>
<ul>
<li><strong>Authors: </strong>Xuefeng Li, Haoyang Zou, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23383">https://arxiv.org/abs/2503.23383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23383">https://arxiv.org/pdf/2503.23383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23383]] ToRL: Scaling Tool-Integrated RL(https://arxiv.org/abs/2503.23383)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for training large language models (LLMs) to autonomously use computational tools via reinforcement learning. Unlike supervised fine-tuning, ToRL allows models to explore and discover optimal strategies for tool use. Experiments with Qwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\% accuracy on AIME~24, surpassing reinforcement learning without tool integration by 14\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\%. Further analysis reveals emergent behaviors such as strategic tool invocation, self-regulation of ineffective code, and dynamic adaptation between computational and analytical reasoning, all arising purely through reward-driven learning.</li>
<li><strong>摘要：</strong>我们介绍了Torl（工具集成的增强学习），这是一个培训大语言模型（LLMS）的框架，以通过增强学习自主使用计算工具。与受监督的微调不同，TORL允许模型探索和发现用于工具使用的最佳策略。 QWEN2.5-MATH模型的实验显示出重大改进：TORL-7B在AIME〜24上达到43.3 \％的准确性，超过14 \％的工具集成的强化学习，而现有的工具综合推理（TIR）模型（TIR）模型最多。进一步的分析揭示了新兴行为，例如战略工具调用，对无效的代码的自我调节以及计算和分析推理之间的动态适应性，所有这些都纯粹是通过奖励驱动的学习而产生的。</li>
</ul>

<h3>Title: An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Alexander Murphy, Mohd Sanad Zaki Rizvi, Aden Haussmann, Ping Nie, Guifu Liu, Aryo Pradipta Gema, Pasquale Minervini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23415">https://arxiv.org/abs/2503.23415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23415">https://arxiv.org/pdf/2503.23415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23415]] An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering(https://arxiv.org/abs/2503.23415)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently produce factually inaccurate outputs - a phenomenon known as hallucination - which limits their accuracy in knowledge-intensive NLP tasks. Retrieval-augmented generation and agentic frameworks such as Reasoning and Acting (ReAct) can address this issue by giving the model access to external knowledge. However, LLMs often fail to remain faithful to retrieved information. Mitigating this is critical, especially if LLMs are required to reason about the retrieved information. Recent research has explored training-free decoding strategies to improve the faithfulness of model generations. We present a systematic analysis of how the combination of the ReAct framework and decoding strategies (i.e., DeCoRe, DoLa, and CAD) can influence the faithfulness of LLM-generated answers. Our results show that combining an agentic framework for knowledge retrieval with decoding methods that enhance faithfulness can increase accuracy on the downstream Multi-Hop Question Answering tasks. For example, we observe an F1 increase from 19.5 to 32.6 on HotpotQA when using ReAct and DoLa.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）经常产生实际上不准确的产出 - 一种称为幻觉的现象 - 限制了其在知识密集的NLP任务中的准确性。检索授权的生成和代理框架（例如推理和代理（REACT））可以通过使模型访问外部知识来解决此问题。但是，LLM通常无法忠于检索信息。减轻这种情况至关重要，特别是如果需要LLM来推理检索到的信息。最近的研究探讨了无培训的解码策略，以提高模型世代的忠诚。我们对反应框架和解码策略的组合（即，Decore，Dola和CAD）的组合如何影响LLM生成的答案的忠诚。我们的结果表明，将知识检索的代理框架与解码方法相结合，以增强忠诚的方式可以提高下游多跳的问题回答任务的准确性。例如，我们在使用React和Dola时观察到HotPotQA上的F1从19.5增加到32.6。</li>
</ul>

<h3>Title: CoRanking: Collaborative Ranking with Small and Large Ranking Agents</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Liu, Xinyu Ma, Yutao Zhu, Lixin Su, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23427">https://arxiv.org/abs/2503.23427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23427">https://arxiv.org/pdf/2503.23427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23427]] CoRanking: Collaborative Ranking with Small and Large Ranking Agents(https://arxiv.org/abs/2503.23427)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated superior listwise ranking performance. However, their superior performance often relies on large-scale parameters (\eg, GPT-4) and a repetitive sliding window process, which introduces significant efficiency challenges. In this paper, we propose \textbf{CoRanking}, a novel collaborative ranking framework that combines small and large ranking models for efficient and effective ranking. CoRanking first employs a small-size reranker to pre-rank all the candidate passages, bringing relevant ones to the top part of the list (\eg, top-20). Then, the LLM listwise reranker is applied to only rerank these top-ranked passages instead of the whole list, substantially enhancing overall ranking efficiency. Although more efficient, previous studies have revealed that the LLM listwise reranker have significant positional biases on the order of input passages. Directly feed the top-ranked passages from small reranker may result in the sub-optimal performance of LLM listwise reranker. To alleviate this problem, we introduce a passage order adjuster trained via reinforcement learning, which reorders the top passages from the small reranker to align with the LLM's preferences of passage order. Extensive experiments on three IR benchmarks demonstrate that CoRanking significantly improves efficiency (reducing ranking latency by about 70\%) while achieving even better effectiveness compared to using only the LLM listwise reranker.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）已证明了列表排名较高的性能。但是，它们的出色性能通常依赖于大规模参数（\ EG，GPT-4）和重复的滑动窗口过程，这引入了重大效率挑战。在本文中，我们提出了\ textbf {Coranking}，这是一个新颖的协作排名框架，结合了大小排名模型，以实现高效和有效排名。 Coranking首先采用小型的Reranker来预先所有候选段落，将相关的段落带入列表的顶部（\ EG，TOP-20）。然后，将LLM listwise Reranker应用于这些排名最高的段落而不是整个列表，从而大大提高了整体排名效率。尽管效率更高，但先前的研究表明，LLM listwise Reranker在输入段落的顺序上具有明显的位置偏见。直接从小书籍中直接喂养排名最高的段落，可能会导致LLM Listwise Reranker的次优性能。为了减轻这个问题，我们引入了通过加固学习训练的通道调节器，该通讯是从小型阅读者中重新定位的，以与LLM对通过订单的偏好保持一致。对三个IR基准测试的广泛实验表明，与仅使用LLM ListWise Reranker相比，Coranking显着提高了效率（将排名延迟降低约70 \％），而实现了更好的有效性。</li>
</ul>

<h3>Title: Speculative End-Turn Detector for Efficient Speech Chatbot Assistant</h3>
<ul>
<li><strong>Authors: </strong>Hyunjong Ok, Suho Yoo, Jaeho Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23439">https://arxiv.org/abs/2503.23439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23439">https://arxiv.org/pdf/2503.23439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23439]] Speculative End-Turn Detector for Efficient Speech Chatbot Assistant(https://arxiv.org/abs/2503.23439)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>Spoken dialogue systems powered by large language models have demonstrated remarkable abilities in understanding human speech and generating appropriate spoken responses. However, these systems struggle with end-turn detection (ETD) -- the ability to distinguish between user turn completion and hesitation. This limitation often leads to premature or delayed responses, disrupting the flow of spoken conversations. In this paper, we introduce the ETD Dataset, the first public dataset for end-turn detection. The ETD dataset consists of both synthetic speech data generated with text-to-speech models and real-world speech data collected from web sources. We also propose SpeculativeETD, a novel collaborative inference framework that balances efficiency and accuracy to improve real-time ETD in resource-constrained environments. Our approach jointly employs a lightweight GRU-based model, which rapidly detects the non-speaking units in real-time on local devices, and a high-performance Wav2vec-based model running on the server to make a more challenging classification of distinguishing turn ends from mere pauses. Experiments demonstrate that the proposed SpeculativeETD significantly improves ETD accuracy while keeping the required computations low. Datasets and code will be available after the review.</li>
<li><strong>摘要：</strong>由大型语言模型提供支持的口语对话系统在理解人类言语和产生适当的口语反应方面表现出了非凡的能力。但是，这些系统在端转弯检测（ETD）方面努力 - 区分用户转向完成和犹豫的能力。这种限制通常会导致响应过早或延迟，从而破坏了口语对话的流动。在本文中，我们介绍了ETD数据集，这是第一个用于终端检测的公共数据集。 ETD数据集包括使用文本到语音模型生成的综合语音数据和从Web来源收集的现实语音数据。我们还提出了一种新颖的协作推理框架，可以平衡效率和准确性，以改善资源受限环境中的实时ETD。我们的方法共同采用了一个基于GRU的轻型模型，该模型迅速在本地设备上实时检测非说话单元，以及在服务器上运行的高性能WAV2VEC模型，以使区分转弯的更具挑战性分类与单纯的暂停结束。实验表明，所提出的投机者显着提高了ETD的准确性，同时保持所需的计算较低。审查后，数据集和代码将可用。</li>
</ul>

<h3>Title: Order Independence With Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Katrina Brown, Reid McIlroy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23483">https://arxiv.org/abs/2503.23483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23483">https://arxiv.org/pdf/2503.23483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23483]] Order Independence With Finetuning(https://arxiv.org/abs/2503.23483)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable performance on many NLP tasks, yet often exhibit order dependence: simply reordering semantically identical tokens (e.g., answer choices in multiple-choice questions) can lead to inconsistent predictions. Recent work proposes Set-Based Prompting (SBP) as a way to remove order information from designated token subsets, thereby mitigating positional biases. However, applying SBP on base models induces an out-of-distribution input format, which can degrade in-distribution performance. We introduce a fine-tuning strategy that integrates SBP into the training process, "pulling" these set-formatted prompts closer to the model's training manifold. We show that SBP can be incorporated into a model via fine-tuning. Our experiments on in-distribution (MMLU) and out-of-distribution (CSQA, ARC Challenge) multiple-choice tasks show that SBP fine-tuning significantly improves accuracy and robustness to answer-order permutations, all while preserving broader language modeling capabilities. We discuss the broader implications of order-invariant modeling and outline future directions for building fairer, more consistent LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在许多NLP任务上表现出了出色的性能，但经常表现出秩序依赖性：简单地重新排序语义相同的令牌（例如，在多项选择问题中的答案选择）可能会导致不一致的预测。最近的工作提出了基于集合的提示（SBP），作为从指定的令牌子集中删除订单信息的一种方法，从而减轻位置偏见。但是，在基本模型上应用SBP会诱导分布式输入格式，从而可以降低分布性能。我们引入了一种微调策略，将SBP集成到培训过程中，“拉”这些设定的形式提示，更接近模型的训练歧管。我们表明，可以通过微调将SBP纳入模型中。我们对分布（MMLU）和分布（CSQA，ARC挑战）多项选择任务的实验表明，SBP微调显着提高了对答案订单排列的准确性和稳健性，同时又保留了更广泛的语言建模能力。我们讨论订单不变建模的更广泛含义，并概述未来的方向，以构建更公平，更一致的LLM。</li>
</ul>

<h3>Title: Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sid Bharthulwar, John Rho, Katrina Brown</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23503">https://arxiv.org/abs/2503.23503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23503">https://arxiv.org/pdf/2503.23503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23503]] Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models(https://arxiv.org/abs/2503.23503)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>We present a framework for optimizing prompts in vision-language models to elicit multimodal reasoning without model retraining. Using an evolutionary algorithm to guide prompt updates downstream of visual tasks, our approach improves upon baseline prompt-updating algorithms, which lack evolution-style "survival of the fittest" iteration. Crucially, we find this approach enables the language model to independently discover progressive problem-solving techniques across several evolution generations. For example, the model reasons that to "break down" visually complex spatial tasks, making a tool call to a Python interpreter to perform tasks (such as cropping, image segmentation, or saturation changes) would improve performance significantly. Our experimentation shows that explicitly evoking this "tool calling" call, via system-level XML $...\texttt{<tool>} ... \texttt{</tool>}...$ tags, can effectively flag Python interpreter access for the same language model to generate relevant programs, generating advanced multimodal functionality. This functionality can be crystallized into a system-level prompt that induces improved performance at inference time, and our experimentation suggests up to $\approx 50\%$ relative improvement across select visual tasks. Downstream performance is trained and evaluated across subtasks from MathVista, M3CoT, and GeoBench-VLM datasets. Importantly, our approach shows that evolutionary prompt optimization guides language models towards self-reasoning discoveries, which result in improved zero-shot generalization across tasks.</li>
<li><strong>摘要：</strong>我们提出了一个框架，以优化视觉模型中的提示，以引起多模式推理，而无需模型再培训。使用进化算法来指导视觉任务下游的提示更新，我们的方法可以改善基线提示升级算法，后者缺乏进化风格的“适合最佳”迭代的“生存”。至关重要的是，我们发现这种方法使语言模型能够独立地发现几代人的渐进问题解决技术。例如，“分解”视觉上复杂的空间任务，对Python解释器进行工具来执行任务（例如裁剪，图像分割或饱和度更改）的模型原因将大大提高性能。我们的实验表明，通过系统级XML $ ... \ texttt {<tool>} ... \ texttt {</tool>} ... $标记可以有效地标记同一语言模型以生成相关的多型iMododAlodal功能的相同语言模型。该功能可以将其结晶成系统级的提示，该提示可以在推理时间诱导性能的提高，我们的实验表明，在某些视觉任务中，最多可达$ \％$ \％$相对改进。在Mathvista，M3Cot和GeoBench-VLM数据集的子任务中对下游性能进行了训练和评估。重要的是，我们的方法表明，进化及时的优化将语言模型引导到自我复杂的发现，从而改善了整个任务的零弹性概括。</li>
</ul>

<h3>Title: SCORE: Story Coherence and Retrieval Enhancement for AI Narratives</h3>
<ul>
<li><strong>Authors: </strong>Qiang Yi, Yangfan He, Jianhui Wang, Xinyuan Song, Shiyao Qian, Miao Zhang, Li Sun, Tianyu Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23512">https://arxiv.org/abs/2503.23512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23512">https://arxiv.org/pdf/2503.23512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23512]] SCORE: Story Coherence and Retrieval Enhancement for AI Narratives(https://arxiv.org/abs/2503.23512)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at generating creative narratives but struggle with long-term coherence and emotional consistency in complex stories. To address this, we propose SCORE (Story Coherence and Retrieval Enhancement), a framework integrating three components: 1) Dynamic State Tracking (monitoring objects/characters via symbolic logic), 2) Context-Aware Summarization (hierarchical episode summaries for temporal progression), and 3) Hybrid Retrieval (combining TF-IDF keyword relevance with cosine similarity-based semantic embeddings). The system employs a temporally-aligned Retrieval-Augmented Generation (RAG) pipeline to validate contextual consistency. Evaluations show SCORE achieves 23.6% higher coherence (NCI-2.0 benchmark), 89.7% emotional consistency (EASM metric), and 41.8% fewer hallucinations versus baseline GPT models. Its modular design supports incremental knowledge graph construction for persistent story memory and multi-LLM backend compatibility, offering an explainable solution for industrial-scale narrative systems requiring long-term consistency.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）擅长产生创造性的叙事，但在复杂的故事中以长期连贯性和情感一致性而挣扎。为了解决这个问题，我们提出了分数（故事的连贯性和检索增强），一个整合了三个组成部分的框架：1）动态状态跟踪（通过符号逻辑监视对象/字符/角色），2）上下文意识到的摘要（层次结构插入时间汇总）（用于时间进度的层次概述），以及3）基于hybrid reterival（将hybrid formity（3）与tf-id相关性相关性相关性。该系统采用了暂时一致的检索仪（RAG）管道来验证上下文一致性。评估表明，得分的相干性提高了23.6％（NCI-2.0基准），情绪一致性为89.7％（EASM度量），与基线GPT模型相比，幻觉减少了41.8％。它的模块化设计支持持续的故事记忆和多LLM后端兼容性的增量知识图构造，为需要长期一致性的工业规模叙事系统提供了可解释的解决方案。</li>
</ul>

<h3>Title: RARE: Retrieval-Augmented Reasoning Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Weinan E, Linpeng Tang, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23513">https://arxiv.org/abs/2503.23513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23513">https://arxiv.org/pdf/2503.23513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23513]] RARE: Retrieval-Augmented Reasoning Modeling(https://arxiv.org/abs/2503.23513)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Domain-specific intelligence demands specialized knowledge and sophisticated reasoning for problem-solving, posing significant challenges for large language models (LLMs) that struggle with knowledge hallucination and inadequate reasoning capabilities under constrained parameter budgets. Inspired by Bloom's Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning Modeling (RARE), a novel paradigm that decouples knowledge storage from reasoning optimization. RARE externalizes domain knowledge to retrievable sources and internalizes domain-specific reasoning patterns during training. Specifically, by injecting retrieved knowledge into training prompts, RARE transforms learning objectives from rote memorization to contextualized reasoning application. It enables models to bypass parameter-intensive memorization and prioritize the development of higher-order cognitive processes. Our experiments demonstrate that lightweight RARE-trained models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing retrieval-augmented GPT-4 and Deepseek-R1 distilled counterparts. RARE establishes a paradigm shift where maintainable external knowledge bases synergize with compact, reasoning-optimized models, collectively driving more scalable domain-specific intelligence. Repo: this https URL</li>
<li><strong>摘要：</strong>特定于领域的智能要求解决问题的专业知识和复杂的推理，对大型语言模型（LLM）提出了重大挑战，这些模型（LLMS）与知识幻觉和在约束参数预算下的推理能力不足。受Bloom在教育理论中的分类学的启发，我们提出了检索提示的推理建模（稀有），这是一种新颖的范式，它使知识存储无法储存推理优化。稀有将域知识的外部化为可检索的来源，并在训练过程中内化了特定于领域的推理模式。具体而言，通过将知识检索到培训提示中，稀有的学习目标从死记硬背的记忆到上下文化的推理应用程序。它使模型能够绕过参数密集型记忆并确定高阶认知过程的发展。我们的实验表明，轻巧的稀有训练模型（例如Llama-3.1-8b）可以实现最先进的性能，超过检索效果的GPT-4和DeepSeek-R1蒸馏器。 Rare建立了一个范式转变，可维护的外部知识基础与紧凑的，推理优化的模型协同，共同推动了更可扩展的域特异性智能。回购：此HTTPS URL</li>
</ul>

<h3>Title: If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Siqi Fan, Xiusheng Huang, Yiqun Yao, Xuezhi Fang, Kang Liu, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23514">https://arxiv.org/abs/2503.23514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23514">https://arxiv.org/pdf/2503.23514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23514]] If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs(https://arxiv.org/abs/2503.23514)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can carry out human-like dialogue, but unlike humans, they are stateless due to the superposition property. However, during multi-turn, multi-agent interactions, LLMs begin to exhibit consistent, character-like behaviors, hinting at a form of emergent lifelong learning. Despite this, existing benchmarks often fail to capture these dynamics, primarily focusing on static, open-ended evaluations. To address this gap, we introduce LIFESTATE-BENCH, a benchmark designed to assess lifelong learning in LLMs. It features two episodic datasets: Hamlet and a synthetic script collection, rich in narrative structure and character interactions. Our fact checking evaluation probes models' self-awareness, episodic memory retrieval, and relationship tracking, across both parametric and non-parametric approaches. Experiments on models like Llama3.1-8B, GPT-4-turbo, and DeepSeek R1, we demonstrate that nonparametric methods significantly outperform parametric ones in managing stateful learning. However, all models exhibit challenges with catastrophic forgetting as interactions extend, highlighting the need for further advancements in lifelong learning.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）可以进行类似人类的对话，但与人类不同，由于叠加属性，它们是无国籍的。但是，在多转变的多代理相互作用期间，LLM开始表现出一致的角色式行为，暗示了一种新兴的终身学习形式。尽管如此，现有的基准通常无法捕获这些动态，主要集中于静态的开放式评估。为了解决这一差距，我们介绍了Lifestate Bench，这是一种基准测试，旨在评估LLMS中的终身学习。它具有两个情节数据集：小村庄和一个合成脚本集合，富含叙事结构和角色相互作用。我们的事实检查评估探针模型的自我意识，情节记忆检索和关系跟踪，跨参数和非参数方法。在Llama3.1-8B，GPT-4-Turbo和DeepSeek R1等模型上进行的实验，我们证明了非参数方法在管理状态学习时明显优于参数。但是，随着互动的扩展，所有模型都会遇到灾难性遗忘，这表明需要进一步发展终身学习。</li>
</ul>

<h3>Title: Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haochen Liu, Song Wang, Chen Chen, Jundong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23523">https://arxiv.org/abs/2503.23523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23523">https://arxiv.org/pdf/2503.23523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23523]] Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models(https://arxiv.org/abs/2503.23523)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often struggle with tasks requiring external knowledge, such as knowledge-intensive Multiple Choice Question Answering (MCQA). Integrating Knowledge Graphs (KGs) can enhance reasoning; however, existing methods typically demand costly fine-tuning or retrieve noisy KG information. Recent approaches leverage Graph Neural Networks (GNNs) to generate KG-based input embedding prefixes as soft prompts for LLMs but fail to account for question relevance, resulting in noisy prompts. Moreover, in MCQA tasks, the absence of relevant KG knowledge for certain answer options remains a significant challenge. To address these issues, we propose Question-Aware Knowledge Graph Prompting (QAP), which incorporates question embeddings into GNN aggregation to dynamically assess KG relevance. QAP employs global attention to capture inter-option relationships, enriching soft prompts with inferred knowledge. Experimental results demonstrate that QAP outperforms state-of-the-art methods across multiple datasets, highlighting its effectiveness.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）经常在需要外部知识的任务上挣扎，例如知识密集的多项选择问答（MCQA）。集成知识图（kgs）可以增强推理；但是，现有方法通常需要昂贵的微调或检索嘈杂的kg信息。最近的方法利用图形神经网络（GNN）生成基于KG的输入嵌入前缀作为LLM的软提示，但无法解决问题相关性，从而导致嘈杂的提示。此外，在MCQA任务中，缺乏某些答案选项的相关知识仍然是一个重大挑战。为了解决这些问题，我们提出了问题感知知识图提示（QAP），该提示将问题嵌入到GNN聚合中以动态评估KG相关性。 QAP利用全球关注来捕获选项之间的关系，以推断知识丰富软提示。实验结果表明，QAP在多个数据集中胜过最先进的方法，突出了其有效性。</li>
</ul>

<h3>Title: Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Xabier de Zuazo, Eva Navas, Ibon Saratxaga, Inma Hernáez Rioja</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23542">https://arxiv.org/abs/2503.23542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23542">https://arxiv.org/pdf/2503.23542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23542]] Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages(https://arxiv.org/abs/2503.23542)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51\% for in-distribution datasets and up to 34\% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at this http URL.</li>
<li><strong>摘要：</strong>自动语音识别系统无疑是随着多语言和多任务模型（例如Whisper）的整合而提出的，这些模型（如Whisper）表现出了有希望的能力，可以理解和处理各种语言的语音。尽管它们具有稳健性，但这些模型通常在处理少数语言的语言区分方面通常缺乏。这项研究通过将传统和新颖的语言模型与微调的耳语模型相结合，以提高其较不常见的语言的表现来解决这一差距。通过跨多个数据集进行严格的微调和评估，我们证明了单词错误率的实质性提高，尤其是在低资源场景中。我们的方法不仅可以利用广泛的数据窃窃私语，而且还可以通过结合语言模型来补充其语言适应性。使用统计语言模型，我们获得了分布数据集的最高51 \％的改进，对于分发句子的最高为34 \％，而大语言模型则在各种语言环境中提供了适度但始终如一的改进。研究结果表明，尽管集成可靠地使所有模型大小都受益，但改进的程度有所不同，突出了优化语言模型参数的重要性。最后，我们强调使用基于变压器的ASR模型报告结果时选择适当的评估参数的重要性。总而言之，这项研究为更具包容性的ASR技术清除了道路，这些技术通过丰富了语言知识来更好地跨语言。有关本研究的进一步实施详细信息，可以在此HTTP URL上获得技术文档和源代码。</li>
</ul>

<h3>Title: When LLM Therapists Become Salespeople: Evaluating Large Language Models for Ethical Motivational Interviewing</h3>
<ul>
<li><strong>Authors: </strong>Haein Kong, Seonghyeon Moon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23566">https://arxiv.org/abs/2503.23566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23566">https://arxiv.org/pdf/2503.23566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23566]] When LLM Therapists Become Salespeople: Evaluating Large Language Models for Ethical Motivational Interviewing(https://arxiv.org/abs/2503.23566)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been actively applied in the mental health field. Recent research shows the promise of LLMs in applying psychotherapy, especially motivational interviewing (MI). However, there is a lack of studies investigating how language models understand MI ethics. Given the risks that malicious actors can use language models to apply MI for unethical purposes, it is important to evaluate their capability of differentiating ethical and unethical MI practices. Thus, this study investigates the ethical awareness of LLMs in MI with multiple experiments. Our findings show that LLMs have a moderate to strong level of knowledge in MI. However, their ethical standards are not aligned with the MI spirit, as they generated unethical responses and performed poorly in detecting unethical responses. We proposed a Chain-of-Ethic prompt to mitigate those risks and improve safety. Finally, our proposed strategy effectively improved ethical MI response generation and detection performance. These findings highlight the need for safety evaluations and guidelines for building ethical LLM-powered psychotherapy.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已在心理健康领域积极应用。最近的研究表明，LLM在应用心理治疗，尤其是动机访谈（MI）方面的希望。但是，缺乏研究语言模型如何理解MI伦理的研究。鉴于恶意参与者可以使用语言模型将MI应用于不道德目的的风险，因此评估其区分道德和不道德的MI实践的能力很重要。因此，这项研究通过多个实验研究了MI中LLM的伦理意识。我们的发现表明，LLM在MI中具有中等至强的知识水平。但是，他们的道德标准并不与MI精神保持一致，因为它们产生了不道德的反应，并且在检测不道德的反应方面表现不佳。我们提出了一个富裕的提示，以减轻这些风险并提高安全性。最后，我们提出的策略有效地改善了道德MI响应的产生和检测性能。这些发现凸显了对建立道德LLM驱动心理疗法的安全评估和准则的需求。</li>
</ul>

<h3>Title: CrossFormer: Cross-Segment Semantic Fusion for Document Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tongke Ni, Yang Fan, Junru Zhou, Xiangping Wu, Qingcai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23671">https://arxiv.org/abs/2503.23671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23671">https://arxiv.org/pdf/2503.23671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23671]] CrossFormer: Cross-Segment Semantic Fusion for Document Segmentation(https://arxiv.org/abs/2503.23671)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Text semantic segmentation involves partitioning a document into multiple paragraphs with continuous semantics based on the subject matter, contextual information, and document structure. Traditional approaches have typically relied on preprocessing documents into segments to address input length constraints, resulting in the loss of critical semantic information across segments. To address this, we present CrossFormer, a transformer-based model featuring a novel cross-segment fusion module that dynamically models latent semantic dependencies across document segments, substantially elevating segmentation accuracy. Additionally, CrossFormer can replace rule-based chunk methods within the Retrieval-Augmented Generation (RAG) system, producing more semantically coherent chunks that enhance its efficacy. Comprehensive evaluations confirm CrossFormer's state-of-the-art performance on public text semantic segmentation datasets, alongside considerable gains on RAG benchmarks.</li>
<li><strong>摘要：</strong>文本语义细分涉及根据主题，上下文信息和文档结构将文档分为多个段落。传统方法通常依靠将文档进行段落的段来解决输入长度限制，从而导致整个细分市场的关键语义信息丧失。为了解决这个问题，我们提出了CrossFormer，这是一个基于变压器的模型，具有一种新型的跨段融合模块，该模块在文档段中动态模拟潜在的语义依赖性，从而实质上提高了分段精度。此外，交叉形式可以替代检索型生成（RAG）系统中基于规则的块方法，从而产生更具语义上的连贯块以增强其功效。全面的评估证实了交叉形式在公共文本语义细分数据集上的最先进性能，以及抹布基准的可观收益。</li>
</ul>

<h3>Title: WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Zhao, Shubo Zhang, Bin Liang, Binyang Li, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23673">https://arxiv.org/abs/2503.23673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23673">https://arxiv.org/pdf/2503.23673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23673]] WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation(https://arxiv.org/abs/2503.23673)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>In Biomedical Natural Language Processing (BioNLP) tasks, such as Relation Extraction, Named Entity Recognition, and Text Classification, the scarcity of high-quality data remains a significant challenge. This limitation poisons large language models to correctly understand relationships between biological entities, such as molecules and diseases, or drug interactions, and further results in potential misinterpretation of biomedical documents. To address this issue, current approaches generally adopt the Synthetic Data Augmentation method which involves similarity computation followed by word replacement, but counterfactual data are usually generated. As a result, these methods disrupt meaningful word sets or produce sentences with meanings that deviate substantially from the original context, rendering them ineffective in improving model performance. To this end, this paper proposes a biomedical-dedicated rationale-based synthetic data augmentation method. Beyond the naive lexicon similarity, specific bio-relation similarity is measured to hold the augmented instance having a strong correlation with bio-relation instead of simply increasing the diversity of augmented data. Moreover, a multi-agents-involved reflection mechanism helps the model iteratively distinguish different usage of similar entities to escape falling into the mis-replace trap. We evaluate our method on the BLURB and BigBIO benchmark, which includes 9 common datasets spanning four major BioNLP tasks. Our experimental results demonstrate consistent performance improvements across all tasks, highlighting the effectiveness of our approach in addressing the challenges associated with data scarcity and enhancing the overall performance of biomedical NLP models.</li>
<li><strong>摘要：</strong>在生物医学自然语言处理（BIONLP）任务中，例如提取关系，命名实体识别和文本分类，高质量数据的稀缺仍然是一个重大挑战。这种限制毒害大型语言模型，以正确理解生物实体之间的关系，例如分子和疾病或药物相互作用，并进一步导致潜在的误解生物医学文献。为了解决此问题，当前方法通常采用综合数据增强方法，该方法涉及相似性计算，然后进行单词置换，但通常会生成反事实数据。结果，这些方法破坏了有意义的单词集或产生句子，其含义很大程度上偏离了原始上下文，从而使它们无效地改善了模型性能。为此，本文提出了一种基于生物医学的基于理性的合成数据增强方法。除了天真的词典相似性之外，还测量了特定的生物关系相似性，以保持与生物关系有很强相关性的增强实例，而不是简单地增加增强数据的多样性。此外，多代理的反射机制有助于迭代区分相似实体的不同用法，以逃避落入错误的替换陷阱中。我们在Blurb和BigBio基准测试上评估了我们的方法，其中包括9个跨越BionLP任务的常见数据集。我们的实验结果表明，在所有任务中的性能一致，强调了我们方法在解决与数据稀缺相关的挑战和增强生物医学NLP模型的整体性能方面的有效性。</li>
</ul>

<h3>Title: Large Language Models Pass the Turing Test</h3>
<ul>
<li><strong>Authors: </strong>Cameron R. Jones, Benjamin K. Bergen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23674">https://arxiv.org/abs/2503.23674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23674">https://arxiv.org/pdf/2503.23674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23674]] Large Language Models Pass the Turing Test(https://arxiv.org/abs/2503.23674)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>We evaluated 4 systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two randomised, controlled, and pre-registered Turing tests on independent populations. Participants had 5 minute conversations simultaneously with another human participant and one of these systems before judging which conversational partner they thought was human. When prompted to adopt a humanlike persona, GPT-4.5 was judged to be the human 73% of the time: significantly more often than interrogators selected the real human participant. LLaMa-3.1, with the same prompt, was judged to be the human 56% of the time -- not significantly more or less often than the humans they were being compared to -- while baseline models (ELIZA and GPT-4o) achieved win rates significantly below chance (23% and 21% respectively). The results constitute the first empirical evidence that any artificial system passes a standard three-party Turing test. The results have implications for debates about what kind of intelligence is exhibited by Large Language Models (LLMs), and the social and economic impacts these systems are likely to have.</li>
<li><strong>摘要：</strong>我们在两个随机，受控和预先注册的图灵测试中评估了4个系统（Eliza，GPT-4O，Llama-3.1-405b和GPT-4.5）。参与者在判断他们认为是人类的对话伙伴之前，与另一个人的参与者同时进行了5分钟的对话。当提示采用人类角色时，GPT-4.5被认为是73％的时间：比询问者选择了真正的人类参与者的频率要多得多。 Llama-3.1有相同的提示，被认为是人类的56％的时间 - 不比他们被比较的人类比人类的频率高得多，而基线模型（Eliza和GPT-4O）的获胜率显着低于机会（分别为23％和21％）。结果构成了第一个经验证据，即任何人造系统都通过标准的三方图灵测试。结果对大型语言模型（LLM）展示了哪种智能的辩论具有影响，这些系统可能会产生的社会和经济影响。</li>
</ul>

<h3>Title: MKA: Leveraging Cross-Lingual Consensus for Model Abstention</h3>
<ul>
<li><strong>Authors: </strong>Sharad Duwal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23687">https://arxiv.org/abs/2503.23687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23687">https://arxiv.org/pdf/2503.23687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23687]] MKA: Leveraging Cross-Lingual Consensus for Model Abstention(https://arxiv.org/abs/2503.23687)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Reliability of LLMs is questionable even as they get better at more tasks. A wider adoption of LLMs is contingent on whether they are usably factual. And if they are not, on whether they can properly calibrate their confidence in their responses. This work focuses on utilizing the multilingual knowledge of an LLM to inform its decision to abstain or answer when prompted. We develop a multilingual pipeline to calibrate the model's confidence and let it abstain when uncertain. We run several multilingual models through the pipeline to profile them across different languages. We find that the performance of the pipeline varies by model and language, but that in general they benefit from it. This is evidenced by the accuracy improvement of $71.2\%$ for Bengali over a baseline performance without the pipeline. Even a high-resource language like English sees a $15.5\%$ improvement. These results hint at possible further improvements.</li>
<li><strong>摘要：</strong>LLM的可靠性是值得怀疑的，即使它们在更多任务上会变得更好。 LLM的更广泛采用取决于它们是否是事实。如果不是，他们是否可以适当地校准他们对回应的信心。这项工作着重于利用LLM的多语言知识来告知其在提示时弃权或回答的决定。我们开发了一条多语言管道来校准模型的信心，并在不确定时弃权。我们通过管道运行多种多语言模型，以跨不同的语言对其进行介绍。我们发现管道的性能因模型和语言而有所不同，但通常它们从中受益。在没有管道的基线表现上，孟加拉语的准确性提高了$ 71.2 \％$的准确性提高。即使是像英语这样的高资源语言，也可以提高$ 15.5 \％$。这些结果暗示了可能的进一步改进。</li>
</ul>

<h3>Title: Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.-China Tensions</h3>
<ul>
<li><strong>Authors: </strong>William Guey, Pierrick Bougault, Vitor D. de Moura, Wei Zhang, Jose O. Gomes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23688">https://arxiv.org/abs/2503.23688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23688">https://arxiv.org/pdf/2503.23688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23688]] Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.-China Tensions(https://arxiv.org/abs/2503.23688)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study systematically analyzes geopolitical bias across 11 prominent Large Language Models (LLMs) by examining their responses to seven critical topics in U.S.-China relations. Utilizing a bilingual (English and Chinese) and dual-framing (affirmative and reverse) methodology, we generated 19,712 prompts designed to detect ideological leanings in model outputs. Responses were quantitatively assessed on a normalized scale from -2 (strongly Pro-China) to +2 (strongly Pro-U.S.) and categorized according to stance, neutrality, and refusal rates. The findings demonstrate significant and consistent ideological alignments correlated with the LLMs' geographic origins; U.S.-based models predominantly favored Pro-U.S. stances, while Chinese-origin models exhibited pronounced Pro-China biases. Notably, language and prompt framing substantially influenced model responses, with several LLMs exhibiting stance reversals based on prompt polarity or linguistic context. Additionally, we introduced comprehensive metrics to evaluate response consistency across languages and framing conditions, identifying variability and vulnerabilities in model behaviors. These results offer practical insights that can guide organizations and individuals in selecting LLMs best aligned with their operational priorities and geopolitical considerations, underscoring the importance of careful model evaluation in politically sensitive applications. Furthermore, the research highlights specific prompt structures and linguistic variations that can strategically trigger distinct responses from models, revealing methods for effectively navigating and influencing LLM outputs.</li>
<li><strong>摘要：</strong>这项研究系统地分析了11种突出的大语言模型（LLM）的地缘政治偏见，通过研究其对美国 - 中国关系中7个关键主题的反应。利用双语（英语和中文）以及双框架（肯定和反向）方法，我们产生了19,712个提示，旨在检测模型输出中的意识形态倾向。对响应进行定量评估，从-2（强烈亲切）到+2（强烈支持U.S。），并根据立场，中立性和拒绝率进行分类。研究结果表明，意识形态一致性与LLMS的地理起源相关。总部位于美国的模型主要赞成Pro-U.S。立场，而中文模型则表现出明显的亲中国偏见。值得注意的是，语言和迅速构架基本影响了模型响应，几个LLM基于迅速的极性或语言环境表现出立场逆转。此外，我们引入了全面的指标，以评估跨语言和框架条件的响应一致性，从而确定模型行为中的可变性和脆弱性。这些结果提供了实用的见解，可以指导组织和个人选择最适合其运营优先级和地缘政治考虑的LLM，从而强调了在政治上敏感的应用中仔细的模型评估的重要性。此外，该研究强调了特定的及时结构和语言变化，这些结构和语言变化可以从策略上触发模型的不同响应，从而揭示了有效导航和影响LLM输出的方法。</li>
</ul>

<h3>Title: Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23714">https://arxiv.org/abs/2503.23714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23714">https://arxiv.org/pdf/2503.23714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23714]] Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models(https://arxiv.org/abs/2503.23714)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases.</li>
<li><strong>摘要：</strong>指令调整对于启用大型语言模型（LLMS）来解决现实世界任务至关重要。先前的工作显示了仅从LLMS合成的指导调查数据的有效性，这提出了一个基本问题：我们是否仍需要以人为原始的信号进行教学调整？这项工作肯定地回答了这个问题：我们仅将它们与LLM生成的响应配对，构建从人写的指令中提出的最先进的说明数据集。 LLM在我们的数据集上进行了微调，始终超过对现有数据的细微调的LLM。我们的数据构建方法很容易适应其他语言。我们为日语构建数据集，并确认LLMS对我们的数据达到最新性能进行了调整。分析表明，用新语言进行教学调整允许LLM遵循说明，而调谐模型则表现出明显缺乏该语言的文化特定知识。数据集和微调模型将公开可用。我们的数据集（由开放量LLMS合成）公开分配在允许许可下，允许使用不同的用例。</li>
</ul>

<h3>Title: AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Du, Xiaochen Wang, Chi Chen, Jiabo Ye, Yiru Wang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Zhifang Sui, Maosong Sun, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23733">https://arxiv.org/abs/2503.23733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23733">https://arxiv.org/pdf/2503.23733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23733]] AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization(https://arxiv.org/abs/2503.23733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.</li>
<li><strong>摘要：</strong>最近，模型合并方法在结合多个大型语言模型（LLM）的各种任务的能力方面表现出了强大的优势。尽管以前的模型合并方法主要集中于将同质模型与相同的体系结构合并，但在处理具有固有异质属性的多模式大语言模型（MLLM）时，它们遇到了挑战，包括模型体系结构和参数空间中的不对称差异。在这项工作中，我们提出了Adamms，这是一种针对异构MLLM量身定制的新型模型合并方法。我们的方法在三个步骤中应对挑战：映射，合并和搜索。具体而言，我们首先在模型之间设计映射功能，以在具有不同体系结构的MLLM上应用模型合并。然后，我们将线性插值应用于模型权重，以主动调整异质MLLM中的不对称性。最后，在超参数搜索步骤中，我们提出了一种用于模型合并的无监督的高参数选择方法。作为能够合并异质MLLM的第一个模型合并方法，没有标记数据，对各种模型组合的广泛实验表明，Adamms的表现优于先前的模型合并方法，该方法在各种视觉基准上进行了合并。</li>
</ul>

<h3>Title: LANID: LLM-assisted New Intent Discovery</h3>
<ul>
<li><strong>Authors: </strong>Lu Fan, Jiashu Pu, Rongsheng Zhang, Xiao-Ming Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23740">https://arxiv.org/abs/2503.23740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23740">https://arxiv.org/pdf/2503.23740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23740]] LANID: LLM-assisted New Intent Discovery(https://arxiv.org/abs/2503.23740)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Task-oriented Dialogue Systems (TODS) often face the challenge of encountering new intents. New Intent Discovery (NID) is a crucial task that aims to identify these novel intents while maintaining the capability to recognize existing ones. Previous efforts to adapt TODS to new intents have struggled with inadequate semantic representation or have depended on external knowledge, which is often not scalable or flexible. Recently, Large Language Models (LLMs) have demonstrated strong zero-shot capabilities; however, their scale can be impractical for real-world applications that involve extensive queries. To address the limitations of existing NID methods by leveraging LLMs, we propose LANID, a framework that enhances the semantic representation of lightweight NID encoders with the guidance of LLMs. Specifically, LANID employs the $K$-nearest neighbors and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithms to sample selective utterance pairs from the training set. It then queries an LLM to ascertain the relationships between these pairs. The data produced from this process is utilized to design a contrastive fine-tuning task, which is then used to train a small encoder with a contrastive triplet loss. Our experimental results demonstrate the efficacy of the proposed method across three distinct NID datasets, surpassing strong baselines in both unsupervised and semi-supervised settings. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>面向任务的对话系统（TOD）通常面临遇到新意图的挑战。新意图发现（NID）是一项至关重要的任务，旨在确定这些新颖意图，同时保持识别现有的意图。以前的努力使TOD适应新的意图已经在语义表示不足或取决于外部知识方面苦苦挣扎，这通常是不可扩展或灵活的。最近，大型语言模型（LLMS）表现出强大的零击功能。但是，对于涉及广泛查询的现实应用程序，它们的规模可能是不切实际的。为了通过利用LLM来解决现有NID方法的局限性，我们提出了LANID，该框架可以通过LLMS的指导来增强轻质NID编码的语义表示。具体而言，LANID采用$ K $ neart的邻居和具有噪声（DBSCAN）算法的应用程序的基于密度的空间聚类来对训练集进行选择性话语对采样。然后，它查询LLM以确定这些对之间的关​​系。此过程产生的数据用于设计一项对比度微调任务，然后将其用于训练具有对比三重损失的小编码器。我们的实验结果证明了该方法在三个不同的NID数据集中的功效，在无监督和半监督的设置中超过了强基础。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Texture or Semantics? Vision-Language Models Get Lost in Font Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23768">https://arxiv.org/abs/2503.23768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23768">https://arxiv.org/pdf/2503.23768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23768]] Texture or Semantics? Vision-Language Models Get Lost in Font Recognition(https://arxiv.org/abs/2503.23768)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features.</li>
<li><strong>摘要：</strong>现代视觉模型（VLMS）具有出色的视觉和语言能力，在各种任务中取得了令人印象深刻的表现，例如图像识别和对象定位。但是，它们在细粒度任务中的有效性仍然是一个悬而未决的问题。在日常情况下，遇到设计材料的个人，例如杂志，排版教程，研究论文或品牌内容，可能希望确定文本中使用的美学上令人愉悦的字体。鉴于它们的多模式功能和免费的可访问性，许多VLM通常被认为是字体识别的潜在工具。这就提出了一个基本问题：VLM是否真的具有识别字体的能力？为了进行调查，我们介绍了字体识别基准（FRB），这是一个紧凑且结构良好的数据集，其中包括15个常用字体。 FRB包含两个版本：（i）一个简单的版本，其中10个句子以不同的字体呈现，以及（ii）硬版本，每个文本示例都由15个字体本身的名称组成，引入了质疑模型感知的Stroop效果。通过对字体识别任务的各种VLM的广泛评估，我们得出以下关键发现：（i）当前的VLMS具有有限的字体识别功能，许多最新的模型无法实现令人满意的性能。 （ii）几乎没有射击的学习和经营链（COT）提示在提高不同VLM的字体识别精度方面提供了最小的好处。 （iii）注意力分析阐明了VLM在捕获语义特征中的固有局限性。</li>
</ul>

<h3>Title: CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiangnan Li, Thuy-Trang Vu, Christian Herold, Amirhossein Tebbifakhr, Shahram Khadivi, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23777">https://arxiv.org/abs/2503.23777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23777">https://arxiv.org/pdf/2503.23777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23777]] CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment(https://arxiv.org/abs/2503.23777)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Naive joint training of large language models (LLMs) for multilingual preference alignment can suffer from negative interference. This is a known issue in multilingual training, where conflicting objectives degrade overall performance. However, the impact of this phenomenon in the context of multilingual preference alignment remains largely underexplored. To address this issue, we propose CONGRAD, a scalable and effective filtering method that selects high-quality preference samples with minimal gradient conflicts across languages. Our method leverages gradient surgery to retain samples aligned with an aggregated multilingual update direction. Additionally, we incorporate a sublinear gradient compression strategy that reduces memory overhead during gradient accumulation. We integrate CONGRAD into self-rewarding framework and evaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that CONGRAD consistently outperforms strong baselines in both seen and unseen languages, with minimal alignment tax.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的幼稚联合培训用于多语言偏好一致性可能会受到负面干扰。这是多语言培训中的一个已知问题，目的矛盾降低了整体绩效。但是，这种现象在多语言偏好对齐的背景下的影响仍然很大程度上没有被倍增。为了解决此问题，我们提出了一种可扩展有效的过滤方法，该方法选择了跨语言的梯度冲突最小的高质量偏好样本。我们的方法利用梯度手术来保留与汇总的多语言更新方向对齐的样品。此外，我们结合了均方根梯度压缩策略，该策略可在梯度积累期间降低内存开销。我们将征服范围纳入自我奖励框架，并在10种语言上对Llama3-8B和Gemma2-2b进行评估。结果表明，在可见语言和看不见的语言中，始终优于强大的基线，并以最少的一致性税。</li>
</ul>

<h3>Title: WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization</h3>
<ul>
<li><strong>Authors: </strong>Ine Gevers, Victor De Marez, Luna De Bruyne, Walter Daelemans</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23779">https://arxiv.org/abs/2503.23779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23779">https://arxiv.org/pdf/2503.23779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23779]] WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization(https://arxiv.org/abs/2503.23779)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In this study, we take a closer look at how Winograd schema challenges can be used to evaluate common sense reasoning in LLMs. Specifically, we evaluate generative models of different sizes on the popular WinoGrande benchmark. We release WinoWhat, a new corpus, in which each instance of the WinoGrande validation set is paraphrased. Additionally, we evaluate the performance on the challenge across five common sense knowledge categories, giving more fine-grained insights on what types of knowledge are more challenging for LLMs. Surprisingly, all models perform significantly worse on WinoWhat, implying that LLM reasoning capabilities are overestimated on WinoGrande. To verify whether this is an effect of benchmark memorization, we match benchmark instances to LLM trainingdata and create two test-suites. We observe that memorization has a minimal effect on model performance on WinoGrande.</li>
<li><strong>摘要：</strong>在这项研究中，我们仔细研究了如何使用Winograd模式挑战来评估LLMS中的常识推理。具体而言，我们评估了流行的Winogrande基准测试中不同尺寸的生成模型。我们发布了一个新的语料库Winowhat，其中Winogrande验证集的每个实例都进行了解释。此外，我们评估了五个常识知识类别的挑战表现，从而对LLM的知识类型更具挑战性提供了更多细粒度的见解。令人惊讶的是，所有模型在Winowhat上的表现都明显差，这意味着Winogrande上LLM推理能力被高估了。为了验证这是否是基准记忆的效果，我们将基准实例与LLM TrainingData匹配并创建两个测试套件。我们观察到记忆对Winogrande的模型性能的影响最小。</li>
</ul>

<h3>Title: Adaptive Layer-skipping in Pre-trained LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuan Luo, Weizhi Wang, Xifeng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23798">https://arxiv.org/abs/2503.23798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23798">https://arxiv.org/pdf/2503.23798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23798]] Adaptive Layer-skipping in Pre-trained LLMs(https://arxiv.org/abs/2503.23798)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.</li>
<li><strong>摘要：</strong>已经提出了各种鞋底方法，以加速大型语言模型（LLMS）的代币产生。但是，他们忽略了一个基本问题：在不同代币的一代中，计算需求如何变化？在这项工作中，我们介绍了FlexidePth，该方法可以动态调整文本生成中使用的变压器层数。通过合并插件路由器和适配器，FlexidePth可以在LLMS中启用自适应层跳动，而无需修改其原始参数。将FlexidePth引入Llama-3-8B模型的32层跳过8层的层跳过，同时保持了完整的100 \％基准性能。 flexidepth的实验结果表明，LLM的计算需求根据令牌类型而显着变化。具体而言，生成重复令牌或固定短语需要更少的层，而产生涉及计算或高不确定性的令牌需要更多的层。有趣的是，这种适应性分配模式与人类的直觉保持一致。为了推进这一领域的研究，我们开源的flexidepth和一个数据集记录了FlexidePth的层分配模式，以供将来探索。</li>
</ul>

<h3>Title: Did ChatGPT or Copilot use alter the style of internet news headlines? A time series regression analysis</h3>
<ul>
<li><strong>Authors: </strong>Chris Brogly, Connor McElroy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23811">https://arxiv.org/abs/2503.23811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23811">https://arxiv.org/pdf/2503.23811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23811]] Did ChatGPT or Copilot use alter the style of internet news headlines? A time series regression analysis(https://arxiv.org/abs/2503.23811)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The release of advanced Large Language Models (LLMs) such as ChatGPT and Copilot is changing the way text is created and may influence the content that we find on the web. This study investigated whether the release of these two popular LLMs coincided with a change in writing style in headlines and links on worldwide news websites. 175 NLP features were obtained for each text in a dataset of 451 million headlines/links. An interrupted time series analysis was applied for each of the 175 NLP features to evaluate whether there were any statistically significant sustained changes after the release dates of ChatGPT and/or Copilot. There were a total of 44 features that did not appear to have any significant sustained change after the release of ChatGPT/Copilot. A total of 91 other features did show significant change with ChatGPT and/or Copilot although significance with earlier control LLM release dates (GPT-1/2/3, Gopher) removed them from consideration. This initial analysis suggests these language models may have had a limited impact on the style of individual news headlines/links, with respect to only some NLP measures.</li>
<li><strong>摘要：</strong>高级大语言模型（LLM）（例如Chatgpt和Copilot）的发布正在改变创建文本的方式，并可能影响我们在Web上找到的内容。这项研究调查了这两个受欢迎的LLM的发布是否与全球新闻网站上的标题和链接的写作风格变化相吻合。在4.51亿个标题/链接的数据集中，为每个文本获得了175个NLP功能。对175个NLP功能中的每一个都应用了中断的时间序列分析，以评估在Chatgpt和/或副驾驶发行日期之后是否存在任何统计学上的持续变化。在Chatgpt/Copilot发行后，总共有44个功能似乎没有发生任何重大持续变化。尽管较早的控制LLM释放日期（GPT-1/2/3，Gopher），总共有91个其他功能在Chatgpt和/或Copilot中确实显示出重大变化，但将其从考虑方面删除。最初的分析表明，对于仅一些NLP措施，这些语言模型可能对单个新闻头条/链接的风格产生了有限的影响。</li>
</ul>

<h3>Title: Expanding RL with Verifiable Rewards Across Diverse Domains</h3>
<ul>
<li><strong>Authors: </strong>Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23829">https://arxiv.org/abs/2503.23829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23829">https://arxiv.org/pdf/2503.23829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23829]] Expanding RL with Verifiable Rewards Across Diverse Domains(https://arxiv.org/abs/2503.23829)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.</li>
<li><strong>摘要：</strong>具有可验证奖励（RLVR）的强化学习（RL）在可用结构良好的参考答案的数学推理和编码任务中显示出令人鼓舞的结果。但是，它对更宽的领域的适用性仍然没有被逐出解散。在这项工作中，我们研究了RLVR的扩展到医学，化学，心理学和经济学等更多样化的领域。当存在客观的参考答案时，我们观察到在不同大型语言模型（LLMS）跨二元判断的高度同意，这挑战了对培训领域特定奖励模型的大规模注释的必要性。为了解决处理非结构化参考答案时二进制奖励的局限性，我们将基于模型的软分评分纳入RLVR，以提高其灵活性。我们的实验表明，蒸馏的生成奖励模型可以用作有效的跨域验证器，为RL提供可靠的奖励信号，而无需特定域的注释。通过使用针对我们的奖励模型进行各种RL算法对基本7B模型进行微调，我们获得的策略优于最先进的开源llm，例如QWEN2.5-72B教学和DeepSeek-R1-Distill-Qwen-32b在自由构架答案设置中的跨域中的大量范围。这也增强了RLVR的鲁棒性和可扩展性，突出了其对具有嘈杂或弱标签的现实应用的潜力。</li>
</ul>

<h3>Title: SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to Accelerate Your Speech-LLM Development</h3>
<ul>
<li><strong>Authors: </strong>Minghan Wang, Ye Bai, Yuxia Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23848">https://arxiv.org/abs/2503.23848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23848">https://arxiv.org/pdf/2503.23848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23848]] SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to Accelerate Your Speech-LLM Development(https://arxiv.org/abs/2503.23848)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>High-quality speech dialogue datasets are crucial for Speech-LLM development, yet existing acquisition methods face significant limitations. Human recordings incur high costs and privacy concerns, while synthetic approaches often lack conversational authenticity. To address these challenges, we introduce \textsc{SpeechDialogueFactory}, a production-ready framework for generating natural speech dialogues efficiently. Our solution employs a comprehensive pipeline including metadata generation, dialogue scripting, paralinguistic-enriched utterance simulation, and natural speech synthesis with voice cloning. Additionally, the system provides an interactive UI for detailed sample inspection and a high-throughput batch synthesis mode. Evaluations show that dialogues generated by our system achieve a quality comparable to human recordings while significantly reducing production costs. We release our work as an open-source toolkit, alongside example datasets available in English and Chinese, empowering researchers and developers in Speech-LLM research and development.</li>
<li><strong>摘要：</strong>高质量的语音对话数据集对于语音-LLM开发至关重要，但是现有的获取方法面临重大局限性。人类记录引起了高昂的成本和隐私问题，而合成方法通常缺乏对话真实性。为了应对这些挑战，我们介绍了\ textsc {SecemDialogueFactory}，这是一个有效生成自然语音对话的框架。我们的解决方案采用了一条全面的管道，包括元数据产生，对话脚本，副语言增强的话语模拟以及语音克隆的自然语音综合。此外，该系统为详细的样本检查和高通量批处理合成模式提供了交互式UI。评估表明，我们系统产生的对话实现了与人类记录相当的质量，同时大大降低了生产成本。我们发布了作为开源工具包的工作，以及用英语和汉语提供的示例数据集，赋予了语音-LLM研发中的研究人员和开发人员的能力。</li>
</ul>

<h3>Title: Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23895">https://arxiv.org/abs/2503.23895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23895">https://arxiv.org/pdf/2503.23895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23895]] Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement(https://arxiv.org/abs/2503.23895)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>检索演示的生成（RAG）通过从外部来源检索相关文档并将其纳入上下文来增强大语言模型（LLMS）。尽管它通过提供事实文本来提高可靠性，但随着上下文长度的增长并引入了具有挑战性的RAG幻觉问题，它主要增加了推理成本，这主要是由于LLMS中缺乏相应的参数知识而引起的。有效的解决方案是在测试时间增强LLM的知识。参数抹布（Prag）通过将文档嵌入LLMS参数中以执行测试时间知识增强，从而通过离线培训有效降低推理成本来解决此问题。但是，其高培训和存储成本以及有限的概括能力大大限制了其实际采用。为了应对这些挑战，我们提出了动态参数抹布（DYPRAG），这是一个新型框架，该框架利用轻量级参数转换器模型有效地将文档转换为参数知识。 DyPrag不仅降低了推理，培训和存储成本，而且还会动态生成参数知识，无缝增强LLM的知识，并在测试时间内以插件的方式解决知识冲突。在多个数据集上进行的广泛实验证明了DyPrag的有效性和概括能力，提供了强大而实用的RAG范式，该范式可实现优越的知识融合，并减轻现实世界应用中的RAG幻觉。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset</h3>
<ul>
<li><strong>Authors: </strong>Diana Galvan-Sosa, Gabrielle Gaudeau, Pride Kavumba, Yunmeng Li, Hongyi gu, Zheng Yuan, Keisuke Sakaguchi, Paula Buttery</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23899">https://arxiv.org/abs/2503.23899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23899">https://arxiv.org/pdf/2503.23899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23899]] Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset(https://arxiv.org/abs/2503.23899)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The performance and usability of Large-Language Models (LLMs) are driving their use in explanation generation tasks. However, despite their widespread adoption, LLM explanations have been found to be unreliable, making it difficult for users to distinguish good from bad explanations. To address this issue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of 26k explanations, written and later quality-annotated using the rubric by both humans and six open- and closed-source LLMs. The CUBE dataset focuses on two reasoning and two language tasks, providing the necessary diversity for us to effectively test our proposed rubric. Using Rubrik, we find that explanations are influenced by both task and perceived difficulty. Low quality stems primarily from a lack of conciseness in LLM-generated explanations, rather than cohesion and word choice. The full dataset, rubric, and code will be made available upon acceptance.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的性能和可用性正在推动其在解释生成任务中的使用。但是，尽管采用了广泛的采用，但LLM的解释仍然不可靠，这使用户很难区分好的解释。为了解决这个问题，我们介绍了Rubrik的Cube，这是一个受过教育风格的标语和26K解释的数据集，并使用人类和六个开放式和封闭式的LLM进行了书面和后来的质量宣布。 Cube数据集专注于两个推理和两个语言任务，为我们提供了有效测试我们提议的标题的必要多样性。使用Rubrik，我们发现解释受到任务和感知困难的影响。低质量主要是由于LLM生成的解释缺乏简洁性，而不是凝聚力和单词选择。完整的数据集，标准和代码将在接受后提供。</li>
</ul>

<h3>Title: Entropy-Based Adaptive Weighting for Self-Training</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxuan Wang, Yihe Deng, Mingyu Derek Ma, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23913">https://arxiv.org/abs/2503.23913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23913">https://arxiv.org/pdf/2503.23913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23913]] Entropy-Based Adaptive Weighting for Self-Training(https://arxiv.org/abs/2503.23913)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.</li>
<li><strong>摘要：</strong>大语言模型的数学解决能力已成为研究的焦点，随着利用自我生成的推理路径的兴趣越来越多，这是完善和增强这些模型的有希望的方法。这些路径逐步捕获逻辑过程，同时仅需要正确的答案进行监督。自训练方法已被证明在推理任务中有效，同时消除了对外部模型和手动注释的需求。但是，优化使用自我生成的数据进行模型培训仍然是一个开放的挑战。在这项工作中，我们提出了基于熵的自适应加权来进行自我训练（EAST），这是一种自适应加权策略，旨在在自训练期间优先考虑不确定的数据。具体而言，EAST采用可调参数的映射函数，该函数控制权重的清晰度，将更高的权重分配给模型表现出更大不确定性的数据。这种方法指导该模型专注于更有益和具有挑战性的例子，从而增强其推理能力。我们在GSM8K和数学基准测试中评估了我们的方法。经验结果表明，虽然香草方法几乎没有提高数学的改善（0％），但East却比主链模型获得了1％的增长。在GSM8K上，与香草方法相比，East又获得了1-2％的性能提升。</li>
</ul>

<h3>Title: Model Hemorrhage and the Robustness Limits of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23924">https://arxiv.org/abs/2503.23924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23924">https://arxiv.org/pdf/2503.23924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23924]] Model Hemorrhage and the Robustness Limits of Large Language Models(https://arxiv.org/abs/2503.23924)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在自然语言处理任务中表现出强大的性能，但在通过量化，修剪或解码策略调整进行部署时，经过明显的性能降解。我们将这种现象定义为模型出血 - 参数改变和体系结构变化引起的性能下降。通过对各种LLM框架的系统分析，我们确定关键漏洞模式：层膨胀经常破坏注意机制，压缩技术会诱导信息丢失级联，并解码调整会放大预测差异。我们的研究揭示了变压器体系结构表现出固有的鲁棒性阈值，这些阈值决定了修饰类型的出血严重程度。我们提出了三种缓解策略：梯度感知的修剪可以保留临界权重途径，动态量化缩放保持激活完整性，并且解码校准将生成轨迹与原始模型分布保持一致。这项工作为评估适应过程中模型稳定性的基础指标建立了基础指标，为维持绩效的实用指南提供了有效的LLM部署。我们的发现提高了对建筑转变下的神经网络弹性的了解，尤其是对于大型语言模型。</li>
</ul>

<h3>Title: BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion Recognition in Conversation</h3>
<ul>
<li><strong>Authors: </strong>Yumeng Fu, Junjie Wu, Zhongjie Wang, Meishan Zhang, Yulin Wu, Bingquan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.23990">https://arxiv.org/abs/2503.23990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.23990">https://arxiv.org/pdf/2503.23990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.23990]] BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion Recognition in Conversation(https://arxiv.org/abs/2503.23990)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Multimodal emotion recognition in conversation (MERC), the task of identifying the emotion label for each utterance in a conversation, is vital for developing empathetic machines. Current MLLM-based MERC studies focus mainly on capturing the speaker's textual or vocal characteristics, but ignore the significance of video-derived behavior information. Different from text and audio inputs, learning videos with rich facial expression, body language and posture, provides emotion trigger signals to the models for more accurate emotion predictions. In this paper, we propose a novel behavior-aware MLLM-based framework (BeMERC) to incorporate speaker's behaviors, including subtle facial micro-expression, body language and posture, into a vanilla MLLM-based MERC model, thereby facilitating the modeling of emotional dynamics during a conversation. Furthermore, BeMERC adopts a two-stage instruction tuning strategy to extend the model to the conversations scenario for end-to-end training of a MERC predictor. Experiments demonstrate that BeMERC achieves superior performance than the state-of-the-art methods on two benchmark datasets, and also provides a detailed discussion on the significance of video-derived behavior information in MERC.</li>
<li><strong>摘要：</strong>对话（MERC）中的多模式情感识别是对话中每种话语的情感标签的任务，对于开发善解人意的机器至关重要。当前基于MLLM的MERC研究主要侧重于捕获说话者的文字或人声特征，但忽略了视频衍生的行为信息的重要性。与文本和音频输入不同，具有丰富面部表情，肢体语言和姿势的学习视频为模型提供了情感触发信号，以进行更准确的情感预测。在本文中，我们提出了一种新型的基于MLLM的行为感知的框架（BEMERC），以结合说话者的行为，包括微妙的面部微型表达，肢体语言和姿势，将其纳入基于香草MLLM的MERC模型中，从而在对话中促进了情绪动态的建模。此外，贝尔克（Bemerc）采用了两阶段的指令调整策略，将模型扩展到对话方案，以进行MERC预测指标的端到端培训。实验表明，Bemerc比两个基准数据集上的最先进方法取得了出色的性能，并且还提供了有关MERC中视频衍生行为信息的重要性的详细讨论。</li>
</ul>

<h3>Title: Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Mohammadi, Tommaso Romano, Samira Maghool, Paolo Ceravolo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24062">https://arxiv.org/abs/2503.24062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24062">https://arxiv.org/pdf/2503.24062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24062]] Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data(https://arxiv.org/abs/2503.24062)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Collecting high-quality training data is essential for fine-tuning Large Language Models (LLMs). However, acquiring such data is often costly and time-consuming, especially for non-English languages such as Italian. Recently, researchers have begun to explore the use of LLMs to generate synthetic datasets as a viable alternative. This study proposes a pipeline for generating synthetic data and a comprehensive approach for investigating the factors that influence the validity of synthetic data generated by LLMs by examining how model performance is affected by metrics such as prompt strategy, text length and target position in a specific task, i.e. inclusive language detection in Italian job advertisements. Our results show that, in most cases and across different metrics, the fine-tuned models trained on synthetic data consistently outperformed other models on both real and synthetic test datasets. The study discusses the practical implications and limitations of using synthetic data for language detection tasks with LLMs.</li>
<li><strong>摘要：</strong>收集高质量培训数据对于微调大语言模型（LLM）至关重要。但是，获取此类数据通常是昂贵且耗时的，尤其是对于意大利语等非英语语言。最近，研究人员已开始探索LLM的使用来生成合成数据集作为可行的替代方案。这项研究提出了一条用于生成合成数据的管道，以及一种全面的方法，用于调查LLMS生成的合成数据有效性的因素，方法是研究模型绩效如何受到诸如及时策略，文本长度和目标位置在特定任务（即意大利招聘广告中的包容性语言检测）中的及时策略，文本长度和目标位置的影响。我们的结果表明，在大多数情况下，在不同的指标中，经过合成数据训练的微调模型在真实和合成测试数据集上始终优于其他模型。该研究讨论了使用合成数据用于使用LLM的语言检测任务的实际含义和局限性。</li>
</ul>

<h3>Title: Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?</h3>
<ul>
<li><strong>Authors: </strong>Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, Tegawendé F. Bissyandé, Jacques Klein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24102">https://arxiv.org/abs/2503.24102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24102">https://arxiv.org/pdf/2503.24102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24102]] Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?(https://arxiv.org/abs/2503.24102)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Low-Resource Languages (LRLs) present significant challenges in natural language processing due to their limited linguistic resources and underrepresentation in standard datasets. While recent advancements in Large Language Models (LLMs) and Neural Machine Translation (NMT) have substantially improved translation capabilities for high-resource languages, performance disparities persist for LRLs, particularly impacting privacy-sensitive and resource-constrained scenarios. This paper systematically evaluates the limitations of current LLMs across 200 languages using benchmarks such as FLORES-200. We also explore alternative data sources, including news articles and bilingual dictionaries, and demonstrate how knowledge distillation from large pre-trained models can significantly improve smaller LRL translations. Additionally, we investigate various fine-tuning strategies, revealing that incremental enhancements markedly reduce performance gaps on smaller LLMs.</li>
<li><strong>摘要：</strong>低资源语言（LRLS）由于语言资源有限和标准数据集中的代表性不足，在自然语言处理中提出了重大挑战。尽管大型语言模型（LLM）和神经机器翻译（NMT）的最新进展已大大提高了高资源语言的翻译能力，但LRL的性能差异持续存在，尤其是影响对隐私敏感和资源约束的情况。本文系统地评估了使用Flores-200等基准测试的200种语言中当前LLM的局限性。我们还探讨了替代数据源，包括新闻文章和双语词典，并证明了大型预训练模型的知识蒸馏如何显着改善较小的LRL翻译。此外，我们研究了各种微调策略，揭示了增量增强功能明显减少了较小的LLM上的性能差距。</li>
</ul>

<h3>Title: TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24115">https://arxiv.org/abs/2503.24115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24115">https://arxiv.org/pdf/2503.24115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24115]] TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection(https://arxiv.org/abs/2503.24115)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at this https URL.</li>
<li><strong>摘要：</strong>由于缺乏将音频信号与以推理为导向的文本分析相结合的高质量多模式训练数据，因此对电信欺诈的检测面临重大挑战。为了解决这一差距，我们提出了Teleantifraud-28K，这是第一个专门为自动电信欺诈分析设计的开源音频 - 文本慢思维数据集。我们的数据集是通过三种策略来构建的：（1）使用自动语音识别（ASR）转录的呼叫记录（带有匿名的原始音频）的隐私文本真实示例生成，从而确保通过文本对语音（TTS）模型再生来确保现实世界的一致性； （2）基于大语言模型（LLM）基于真实ASR输出的基于大语言模型（LLM）的自我实施采样的语义增强； （3）通过预定义的通信场景和欺诈类型模拟新兴欺诈策略的多代理对抗性综合。生成的数据集包含28,511个严格处理的语音文本对，并配有欺诈推理的详细注释。数据集分为三个任务：方案分类，欺诈检测，欺诈类型分类。此外，我们构建了TeleAntifraud-Bench，这是一个标准化的评估基准，该基准包括来自数据集的成比例采样实例，以促进在电信欺诈检测任务上对模型性能进行系统测试。我们还贡献了对混合实际/合成数据培训的培训的生产优化的监督微调（SFT）模型，同时开源数据处理框架以实现社区驱动的数据集扩展。这项工作为多模式反欺诈研究建立了一个基本框架，同时解决了数据隐私和情景多样性的关键挑战。该项目将在此HTTPS URL上发布。</li>
</ul>

<h3>Title: Multi-Task Learning for Extracting Menstrual Characteristics from Clinical Notes</h3>
<ul>
<li><strong>Authors: </strong>Anna Shopova, Cristoph Lippert, Leslee J. Shaw, Eugenia Alleva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24116">https://arxiv.org/abs/2503.24116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24116">https://arxiv.org/pdf/2503.24116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24116]] Multi-Task Learning for Extracting Menstrual Characteristics from Clinical Notes(https://arxiv.org/abs/2503.24116)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Menstrual health is a critical yet often overlooked aspect of women's healthcare. Despite its clinical relevance, detailed data on menstrual characteristics is rarely available in structured medical records. To address this gap, we propose a novel Natural Language Processing pipeline to extract key menstrual cycle attributes -- dysmenorrhea, regularity, flow volume, and intermenstrual bleeding. Our approach utilizes the GatorTron model with Multi-Task Prompt-based Learning, enhanced by a hybrid retrieval preprocessing step to identify relevant text segments. It out- performs baseline methods, achieving an average F1-score of 90% across all menstrual characteristics, despite being trained on fewer than 100 annotated clinical notes. The retrieval step consistently improves performance across all approaches, allowing the model to focus on the most relevant segments of lengthy clinical notes. These results show that combining multi-task learning with retrieval improves generalization and performance across menstrual charac- teristics, advancing automated extraction from clinical notes and supporting women's health research.</li>
<li><strong>摘要：</strong>月经健康是妇女医疗保健的关键但经常被忽视的方面。尽管它具有临床意义，但在结构化病历中很少获得有关月经特征的详细数据。为了解决这一差距，我们提出了一种新型的自然语言处理管道，以提取关键的月经周期属性 - 痛经，规律性，流量和月经间出血。我们的方法利用基于多任务及时的学习来利用Gatortron模型，通过混合检索预处理步骤来增强，以识别相关的文本段。它超出了基线方法，尽管接受了少于100个注释的临床注释，但在所有月经特征中的平均F1得分为90％。检索步骤始终提高所有方法的性能，从而使模型可以专注于冗长的临床注释的最相关部分。这些结果表明，将多任务学习与检索结合起来，可以提高月经特征的概括和表现，从而推进了从临床笔记中自动提取并支持妇女健康研究。</li>
</ul>

<h3>Title: Implicit In-Context Learning: Evidence from Artificial Language Experiments</h3>
<ul>
<li><strong>Authors: </strong>Xiaomeng Ma, Qihui Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24190">https://arxiv.org/abs/2503.24190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24190">https://arxiv.org/pdf/2503.24190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24190]] Implicit In-Context Learning: Evidence from Artificial Language Experiments(https://arxiv.org/abs/2503.24190)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Humans acquire language through implicit learning, absorbing complex patterns without explicit awareness. While LLMs demonstrate impressive linguistic capabilities, it remains unclear whether they exhibit human-like pattern recognition during in-context learning at inferencing level. We adapted three classic artificial language learning experiments spanning morphology, morphosyntax, and syntax to systematically evaluate implicit learning at inferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini. Our results reveal linguistic domain-specific alignment between models and human behaviors, o3-mini aligns better in morphology while both models align in syntax.</li>
<li><strong>摘要：</strong>人类通过隐性学习获取语言，吸收复杂的模式而没有明确的意识。尽管LLM具有令人印象深刻的语言能力，但尚不清楚它们在推论级别的内在学习过程中是否表现出类似人类的模式识别。我们调整了三个经典的人工语言学习实验，涵盖形态学，形态和语法，以在两个最先进的OpenAI模型中系统地评估推论水平上的隐式学习：GPT-4O和O3-Mini。我们的结果揭示了模型和人类行为之间语言领域特异性的一致性，O3-Mini在形态学上更好地对齐，而两个模型在语法中都保持一致。</li>
</ul>

<h3>Title: TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24198">https://arxiv.org/abs/2503.24198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24198">https://arxiv.org/pdf/2503.24198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24198]] TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance(https://arxiv.org/abs/2503.24198)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made significant strides in problem-solving by incorporating reasoning processes. However, this enhanced reasoning capability results in an increased number of output tokens during inference, leading to higher computational costs. To address this challenge, we propose TwT (Thinking without Tokens), a method that reduces inference-time costs through habitual reasoning distillation with multi-teachers' guidance, while maintaining high performance. Our approach introduces a Habitual Reasoning Distillation method, which internalizes explicit reasoning into the model's habitual behavior through a Teacher-Guided compression strategy inspired by human cognition. Additionally, we propose Dual-Criteria Rejection Sampling (DCRS), a technique that generates a high-quality and diverse distillation dataset using multiple teacher models, making our method suitable for unsupervised scenarios. Experimental results demonstrate that TwT effectively reduces inference costs while preserving superior performance, achieving up to a 13.6% improvement in accuracy with fewer output tokens compared to other distillation methods, offering a highly practical solution for efficient LLM deployment.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过合并推理过程在解决问题方面取得了重大进步。但是，这种增强的推理能力会导致推断期间的产出代币数量增加，从而导致更高的计算成本。为了应对这一挑战，我们提出了TWT（无需代币的思考），这种方法通过习惯推理蒸馏使用多教老师的指导来降低推理时间成本，同时保持高性能。我们的方法引入了一种惯常的推理蒸馏方法，该方法将明确的推理通过受人类认知启发的教师指导的压缩策略内化为模型的习惯行为。此外，我们建议使用多个教师模型生成高质量且多样化的蒸馏数据集，使我们的方法适合于无监督的方案，从而生成高质量且多样化的蒸馏数据集。实验结果表明，与其他蒸馏方法相比，TWT可以有效地降低推理成本，同时保持卓越的性能，而产生令牌的准确性提高了13.6％，输出令牌较少，从而为有效的LLM部署提供了高度实用的解决方案。</li>
</ul>

<h3>Title: Synthetic News Generation for Fake News Classification</h3>
<ul>
<li><strong>Authors: </strong>Abdul Sittar, Luka Golob, Mateja Smiljanic</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24206">https://arxiv.org/abs/2503.24206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24206">https://arxiv.org/pdf/2503.24206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24206]] Synthetic News Generation for Fake News Classification(https://arxiv.org/abs/2503.24206)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study explores the generation and evaluation of synthetic fake news through fact based manipulations using large language models (LLMs). We introduce a novel methodology that extracts key facts from real articles, modifies them, and regenerates content to simulate fake news while maintaining coherence. To assess the quality of the generated content, we propose a set of evaluation metrics coherence, dissimilarity, and correctness. The research also investigates the application of synthetic data in fake news classification, comparing traditional machine learning models with transformer based models such as BERT. Our experiments demonstrate that transformer models, especially BERT, effectively leverage synthetic data for fake news detection, showing improvements with smaller proportions of synthetic data. Additionally, we find that fact verification features, which focus on identifying factual inconsistencies, provide the most promising results in distinguishing synthetic fake news. The study highlights the potential of synthetic data to enhance fake news detection systems, offering valuable insights for future research and suggesting that targeted improvements in synthetic data generation can further strengthen detection models.</li>
<li><strong>摘要：</strong>这项研究通过使用大型语言模型（LLMS）的基于事实的操纵来探讨合成假新闻的产生和评估。我们介绍了一种新颖的方法，该方法从真实文章中提取关键事实，修改它们并再生内容以模拟假新闻，同时保持连贯性。为了评估生成内容的质量，我们提出了一组评估指标的连贯性，差异性和正确性。该研究还研究了合成数据在虚假新闻分类中的应用，将传统的机器学习模型与基于变压器的模型（例如BERT）进行了比较。我们的实验表明，变压器模型，尤其是BERT，有效地利用合成数据来进行虚假新闻检测，显示了较小比例的合成数据的改进。此外，我们发现，侧重于确定事实不一致的事实验证功能为区分合成假新闻提供了最有希望的结果。该研究强调了合成数据增强假新闻检测系统的潜力，为未来的研究提供了宝贵的见解，并表明对合成数据生成的有针对性改进可以进一步增强检测模型。</li>
</ul>

<h3>Title: What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24235">https://arxiv.org/abs/2503.24235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24235">https://arxiv.org/pdf/2503.24235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24235]] What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models(https://arxiv.org/abs/2503.24235)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.</li>
<li><strong>摘要：</strong>随着对计算缩放计算（数据和参数）的热情逐渐减少，测试时间缩放（TTS），也称为``测试时间计算''的热情已成为一个突出的研究重点。最近的研究表明，TTS可以进一步引起大语模型（LLM）的解决问题的能力，这不仅可以在专门的推理任务（例如数学和编码）中进行重大突破，还可以在开放式Q＆A等一般任务中进行。但是，尽管最近在这一领域努力爆炸了，但仍需要进行全面了解的全面调查。为了填补这一空白，我们提出了一个沿TTS研究的四个核心维度构建的统一的多维框架：扩展的规模，如何扩展，尺度扩展位置以及规模范围的范围。在这种分类法的基础上，我们对方法，应用方案和评估方面进行了广泛的审查，并提出了有组织的分解，该分解突出了更广泛的TTS景观中各个技术的独特功能作用。通过此分析，我们将TTS的主要发展轨迹提炼出来，并提供实践部署指南。此外，我们确定了一些开放的挑战，并为有希望的未来方向提供了见解，包括进一步扩展，澄清技术的功能本质，推广到更多任务以及更多属性。</li>
</ul>

<h3>Title: Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, Jianzhong (Charlie)Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24245">https://arxiv.org/abs/2503.24245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24245">https://arxiv.org/pdf/2503.24245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24245]] Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation(https://arxiv.org/abs/2503.24245)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made significant progress in general-purpose natural language processing tasks. However, LLMs are still facing challenges when applied to domain-specific areas like telecommunications, which demands specialized expertise and adaptability to evolving standards. This paper presents a novel framework that combines knowledge graph (KG) and retrieval-augmented generation (RAG) techniques to enhance LLM performance in the telecom domain. The framework leverages a KG to capture structured, domain-specific information about network protocols, standards, and other telecom-related entities, comprehensively representing their relationships. By integrating KG with RAG, LLMs can dynamically access and utilize the most relevant and up-to-date knowledge during response generation. This hybrid approach bridges the gap between structured knowledge representation and the generative capabilities of LLMs, significantly enhancing accuracy, adaptability, and domain-specific comprehension. Our results demonstrate the effectiveness of the KG-RAG framework in addressing complex technical queries with precision. The proposed KG-RAG model attained an accuracy of 88% for question answering tasks on a frequently used telecom-specific dataset, compared to 82% for the RAG-only and 48% for the LLM-only approaches.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在通用自然语言处理任务方面取得了重大进展。但是，LLM仍面临挑战，当时是电信（例如电信），该领域需要专业知识和适应性的不断发展的标准。本文提出了一个新颖的框架，该框架结合了知识图（kg）和检索功能生成（RAG）技术，以增强电信域中的LLM性能。该框架利用公园来捕获有关网络协议，标准和其他与电信相关的实体的结构化特定于域的信息，从而全面地表示其关系。通过将KG与抹布整合在一起，LLM可以在响应生成期间动态访问并利用最相关和最新的知识。这种混合方法弥合了结构化知识表示与LLM的生成能力之间的差距，从而显着提高了准确性，适应性和特定于领域的理解。我们的结果证明了KG-rag框架在精确地解决复杂技术查询方面的有效性。提出的KG-rag模型的准确度为88％，用于在经常使用的特定于特定电信的数据集上回答任务的准确性，而仅抹布为82％，仅使用LLM的方法为48％。</li>
</ul>

<h3>Title: Is analogy enough to draw novel adjective-noun inferences?</h3>
<ul>
<li><strong>Authors: </strong>Hayley Ross, Kathryn Davidson, Najoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24293">https://arxiv.org/abs/2503.24293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24293">https://arxiv.org/pdf/2503.24293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24293]] Is analogy enough to draw novel adjective-noun inferences?(https://arxiv.org/abs/2503.24293)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recent work (Ross et al., 2025, 2024) has argued that the ability of humans and LLMs respectively to generalize to novel adjective-noun combinations shows that they each have access to a compositional mechanism to determine the phrase's meaning and derive inferences. We study whether these inferences can instead be derived by analogy to known inferences, without need for composition. We investigate this by (1) building a model of analogical reasoning using similarity over lexical items, and (2) asking human participants to reason by analogy. While we find that this strategy works well for a large proportion of the dataset of Ross et al. (2025), there are novel combinations for which both humans and LLMs derive convergent inferences but which are not well handled by analogy. We thus conclude that the mechanism humans and LLMs use to generalize in these cases cannot be fully reduced to analogy, and likely involves composition.</li>
<li><strong>摘要：</strong>最近的工作（Ross等，2025，2024）认为，人类和LLM的能力分别概括为新型形容词 - 名称组合表明，他们每个人都有访问组成机制来确定词组的含义并得出推论。我们研究这些推论是否可以通过类似于已知推论而不需要组成而得出。我们通过（1）使用与词汇项目相似的相似性来构建类似推理的模型，以及（2）要求人类参与者通过类比进行推理。尽管我们发现该策略适合Ross等人的大部分数据集。 （2025），有一些新型组合，人类和LLM都会得出收敛的推论，但不能通过类比来很好地处理。因此，我们得出的结论是，在这些情况下，人类和LLM用于概括的机制不能完全简化为类比，并且可能涉及组成。</li>
</ul>

<h3>Title: A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG</h3>
<ul>
<li><strong>Authors: </strong>Arshia Kermani, Veronica Perez-Rosas, Vangelis Metsis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24307">https://arxiv.org/abs/2503.24307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24307">https://arxiv.org/pdf/2503.24307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24307]] A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG(https://arxiv.org/abs/2503.24307)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>This study presents a systematic comparison of three approaches for the analysis of mental health text using large language models (LLMs): prompt engineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA 3, we evaluate these approaches on emotion classification and mental health condition detection tasks across two datasets. Fine-tuning achieves the highest accuracy (91% for emotion classification, 80% for mental health conditions) but requires substantial computational resources and large training sets, while prompt engineering and RAG offer more flexible deployment with moderate performance (40-68% accuracy). Our findings provide practical insights for implementing LLM-based solutions in mental health applications, highlighting the trade-offs between accuracy, computational requirements, and deployment flexibility.</li>
<li><strong>摘要：</strong>这项研究对使用大语言模型（LLMS）进行分析的三种方法进行了系统的比较：及时的工程，检索增强发电（RAG）和微调。使用Llama 3，我们对两个数据集的情绪分类和心理健康状况检测任务进行评估。微调达到了最高的准确性（情绪分类为91％，精神健康状况为80％），但需要大量的计算资源和大型培训集，而迅速的工程和抹布则提供了更加灵活的部署，具有中等的性能（40-68％的精度）。我们的发现为在心理健康应用中实施基于LLM的解决方案提供了实用的见解，突出了准确性，计算需求和部署灵活性之间的权衡。</li>
</ul>

<h3>Title: BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24310">https://arxiv.org/abs/2503.24310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24310">https://arxiv.org/pdf/2503.24310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24310]] BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models(https://arxiv.org/abs/2503.24310)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.</li>
<li><strong>摘要：</strong>在这项研究中，我们介绍了Beats，这是一个新颖的框架，用于评估大语言模型（LLMS）中的偏见，道德，公平和事实。在Beats框架的基础上，我们为LLM提供了一个偏差基准，该基准测量了29个不同指标的性能。这些指标涵盖了广泛的特征，包括人口统计，认知和社会偏见，以及对道德推理，群体公平和与事实相关的误解风险的度量。这些指标能够对LLM产生的反应的程度进行定量评估，使社会偏见具有加强或扩大全身不平等的社会偏见。为了在此基准上获得高分，LLM必须在其响应中表现出非常公平的行为，这使其成为负责AI评估的严格标准。基于我们实验的数据的经验结果表明，由行业领先模型产生的输出中有37.65％包含某种形式的偏见，这突出了在关键决策制定系统中使用这些模型的很大风险。 Beats框架和基准测试为基准LLM提供了可扩展且统计上严格的方法，诊断驱动偏见并制定缓解策略的因素。借助Beats框架，我们的目标是帮助发展更具社会负责的AI模型。</li>
</ul>

<h3>Title: Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.24377">https://arxiv.org/abs/2503.24377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.24377">https://arxiv.org/pdf/2503.24377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.24377]] Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models(https://arxiv.org/abs/2503.24377)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展显着增强了他们执行复杂的推理任务的能力，从快速，直觉的思维过渡到缓慢而深层的推理（系统2）。尽管系统2推理提高了任务准确性，但由于其思维性质缓慢，效率低下或不必要的推理行为，它通常会造成大量的计算成本。相反，系统1推理在计算上是有效的，但导致了次优性能。因此，在绩效（收益）和计算成本（预算）之间平衡权衡取舍至关重要，从而产生了推理经济的概念。在这项调查中，我们对LLM的培训后和测试时间推理阶段中的推理经济进行了全面分析，包括i）推理效率低下的原因，ii）对不同推理模式的行为分析以及iii）实现推理经济的潜在解决方案。通过提供可行的见解并强调开放的挑战，我们旨在阐明改善LLMS推理经济的策略，从而成为推进该不断发展的地区研究的宝贵资源。我们还提供了一个公共存储库，以不断跟踪这个快速发展的领域的发展。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
