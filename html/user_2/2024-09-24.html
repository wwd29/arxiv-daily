<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-24</h1>
<h3>Title: A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Shuo Yu (1 and 2), Mingyue Cheng (1 and 2), Jiqian Yang (1 and 2), Jie Ouyang (1 and 2) ((1) Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China (2) State Key Laboratory of Cognitive Intelligence)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13694">https://arxiv.org/abs/2409.13694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13694">https://arxiv.org/pdf/2409.13694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13694]] A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation(https://arxiv.org/abs/2409.13694)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances generative models by integrating retrieval mechanisms, which allow these models to access and utilize external knowledge sources. Despite its advantages, RAG encounters significant challenges, particularly in effectively handling real-world queries and mitigating hallucinations. The KDD Cup 2024 CRAG competition brings these issues to the forefront by incorporating both web pages and a mock API as knowledge sources, adding the complexity of parsing HTML before large language models (LLMs) can process the information. In this paper, we propose a novel RAG benchmark designed to address these challenges. Our work provides a comprehensive set of experimental results, offering valuable insights for the study of RAG. We thoroughly examine the entire RAG process, including knowledge source selection, retrieval, organization, and reasoning. Key findings from our study include the impact of automated knowledge source selection using agents and the influence of noise chunks on RAG reasoning. Additionally, we conduct detailed experiments to analyze the effects of various hyperparameters on RAG performance. To support further research, we have made our results, the associated code, and a parsed version of the CRAG dataset publicly available\footnote{this https URL}, contributing to the advancement of RAG methodologies and establishing a solid foundation for future work in this domain.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过集成检索机制来增强生成模型，从而使这些模型能够访问和利用外部知识源。尽管 RAG 具有诸多优势，但它也面临重大挑战，尤其是在有效处理现实世界的查询和减轻幻觉方面。KDD Cup 2024 CRAG 竞赛通过将网页和模拟 API 作为知识源，将这些问题推到了风口浪尖，增加了在大型语言模型 (LLM) 处理信息之前解析 HTML 的复杂性。在本文中，我们提出了一种旨在应对这些挑战的新型 RAG 基准。我们的工作提供了一套全面的实验结果，为 RAG 的研究提供了宝贵的见解。我们彻底检查了整个 RAG 过程，包括知识源选择、检索、组织和推理。我们研究的主要发现包括使用代理自动选择知识源的影响以及噪声块对 RAG 推理的影响。此外，我们还进行了详细的实验来分析各种超参数对 RAG 性能的影响。为了支持进一步的研究，我们已将我们的研究结果、相关代码和 CRAG 数据集的解析版本公开\footnote{此 https URL}，为 RAG 方法的进步做出了贡献，并为该领域的未来工作奠定了坚实的基础。</li>
</ul>

<h3>Title: You Only Use Reactive Attention Slice For Long Context Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yun Joon Soh, Hanxian Huang, Yuandong Tian, Jishen Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13695">https://arxiv.org/abs/2409.13695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13695">https://arxiv.org/pdf/2409.13695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13695]] You Only Use Reactive Attention Slice For Long Context Retrieval(https://arxiv.org/abs/2409.13695)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Supporting longer context for Large Language Models (LLM) is a promising direction to advance LLMs. As training a model for a longer context window is computationally expensive, many alternative solutions, such as Retrieval Augmented Generation (RAG), have been used. However, most existing RAG methods adopt embedding-based retrieval that falls short on long contexts. To address such challenges, we propose an attention-based retrieval technique, You Only Use Reactive Attention slice (YOURA). YOURA leverages a novel retrieval heuristic called reaction score to rank the relevance of each sentence in the input context with the query sentence. Intuitively, we measure how the per-token attention score "reacts" to the query and greedily retrieves the most reactive sentences. Internally, YOURA generates a token-indexed vector (called reaction vector) for the whole input context. To map each sentence to the token-indexed vector, we propose an Embedding-Agnostic Sentence Yield (EASY), a best-effort token wiggling algorithm. We evaluate our retrieval technique on three open-source pre-trained LLM models across six LongBench QA datasets. Our technique achieves up to 30% vLLM inference throughput improvement for serving long-context queries with a nearly identical quality score to the simple yet effective truncate-middle approach.</li>
<li><strong>摘要：</strong>支持大型语言模型 (LLM) 的较长上下文是推动 LLM 发展的一个有前途的方向。由于训练较长上下文窗口的模型在计算上非常昂贵，因此人们使用了许多替代解决方案，例如检索增强生成 (RAG)。然而，大多数现有的 RAG 方法都采用基于嵌入的检索，而这种检索在较长上下文中存在不足。为了应对此类挑战，我们提出了一种基于注意力的检索技术，即 You Only Use Reactive Attention 切片 (YOURA)。YOURA 利用一种称为反应分数的新型检索启发式方法对输入上下文中每个句子与查询句子的相关性进行排序。直观地说，我们测量每个标记的注意力分数如何“反应”查询，并贪婪地检索最具反应性的句子。在内部，YOURA 为整个输入上下文生成一个标记索引向量（称为反应向量）。为了将每个句子映射到标记索引向量，我们提出了一种 Embedding-Agnostic Sentence Yield (EASY)，这是一种尽力而为的标记摆动算法。我们在六个 LongBench QA 数据集上的三个开源预训练 LLM 模型上评估了我们的检索技术。我们的技术在处理长上下文查询时实现了高达 30% 的 vLLM 推理吞吐量改进，其质量得分几乎与简单但有效的截断中间方法相同。</li>
</ul>

<h3>Title: Prompt Baking</h3>
<ul>
<li><strong>Authors: </strong>Aman Bhargava, Cameron Witkowski, Alexander Detkov, Matt Thomson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13697">https://arxiv.org/abs/2409.13697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13697">https://arxiv.org/pdf/2409.13697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13697]] Prompt Baking(https://arxiv.org/abs/2409.13697)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Two primary ways to change LLM behavior are prompting and weight updates (e.g., fine-tuning). Prompting LLMs is simple and effective, specifying the desired changes explicitly in natural language, whereas weight updates provide more expressive and permanent behavior changes, specified implicitly via training on large datasets. We present a technique for "baking" prompts into the weights of an LLM. Prompt Baking converts a prompt $u$ and initial weights $\theta$ to a new set of weights $\theta_u$ such that new "baked" LLM behaves like the original prompted LLM. Mathematically, we minimize the KL divergence between $P_\theta(\cdot | u)$ and $P_{\theta_u}(\cdot)$, where $P$ is the LLM's probability distribution over token sequences. Across all our experiments, we find prompts can be readily baked into weight updates. Baking chain-of-thought prompts improves zero-shot performance on GSM8K, ASDiv, MBPP, ARC-Easy, ARC-Challenge, and CommonsenseQA benchmarks. Baking news headlines directly updates an LLM's knowledge. And baking instructions & personas alleviates "prompt forgetting" over long sequences. Furthermore, stopping baking early creates "half-baked" models, continuously scaling prompt strength. Baked models retain their sensitivity to further prompting and baking, including re-prompting with the baked-in prompt. Surprisingly, the re-prompted models yield further performance gains in instruction following, as well as math reasoning and coding benchmarks. Taking re-prompting and re-baking to the limit yields a form of iterative self-improvement we call Prompt Pursuit, and preliminary results on instruction following exhibit dramatic performance gains. Finally, we discuss implications for AI safety, continuous model updating, enhancing real-time learning capabilities in LLM-based agents, and generating more stable AI personas.</li>
<li><strong>摘要：</strong>改变 LLM 行为的两种主要方法是提示和权重更新（例如，微调）。提示 LLM 简单有效，用自然语言明确指定所需的更改，而权重更新提供更具表现力和持久的行为更改，通过对大型数据集进行训练隐式指定。我们提出了一种将提示“烘焙”到 LLM 权重中的技术。提示烘焙将提示 $u$ 和初始权重 $\theta$ 转换为一组新的权重 $\theta_u$，使得新的“烘焙”LLM 的行为与原始提示的 LLM 一样。从数学上讲，我们最小化了 $P_\theta(\cdot | u)$ 和 $P_{\theta_u}(\cdot)$ 之间的 KL 散度，其中 $P$ 是 LLM 在标记序列上的概率分布。在我们所有的实验中，我们发现提示可以很容易地烘焙到权重更新中。烘焙思路链提示可提高 GSM8K、ASDiv、MBPP、ARC-Easy、ARC-Challenge 和 CommonsenseQA 基准的零样本性能。烘焙新闻标题可直接更新 LLM 的知识。烘焙说明和角色可缓解长序列中的“提示遗忘”。此外，提前停止烘焙会产生“半生不熟”的模型，不断扩大提示强度。烘焙模型保留了对进一步提示和烘焙的敏感性，包括使用烘焙提示重新提示。令人惊讶的是，重新提示的模型在指令遵循以及数学推理和编码基准方面获得了进一步的性能提升。将重新提示和重新烘焙发挥到极致会产生一种我们称之为“提示追求”的迭代自我改进形式，指令遵循的初步结果显示出显着的性能提升。最后，我们讨论了对 AI 安全、持续模型更新、增强基于 LLM 的代理的实时学习能力以及生成更稳定的 AI 角色的影响。</li>
</ul>

<h3>Title: CA-BERT: Leveraging Context Awareness for Enhanced Multi-Turn Chat Interaction</h3>
<ul>
<li><strong>Authors: </strong>Minghao Liu, Mingxiu Sui, Cangqing Wang, Zhejie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13701">https://arxiv.org/abs/2409.13701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13701">https://arxiv.org/pdf/2409.13701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13701]] CA-BERT: Leveraging Context Awareness for Enhanced Multi-Turn Chat Interaction(https://arxiv.org/abs/2409.13701)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Effective communication in automated chat systems hinges on the ability to understand and respond to context. Traditional models often struggle with determining when additional context is necessary for generating appropriate responses. This paper introduces Context-Aware BERT (CA-BERT), a transformer-based model specifically fine-tuned to address this challenge. CA-BERT innovatively applies deep learning techniques to discern context necessity in multi-turn chat interactions, enhancing both the relevance and accuracy of responses. We describe the development of CA-BERT, which adapts the robust architecture of BERT with a novel training regimen focused on a specialized dataset of chat dialogues. The model is evaluated on its ability to classify context necessity, demonstrating superior performance over baseline BERT models in terms of accuracy and efficiency. Furthermore, CA-BERT's implementation showcases significant reductions in training time and resource usage, making it feasible for real-time applications. The results indicate that CA-BERT can effectively enhance the functionality of chatbots by providing a nuanced understanding of context, thereby improving user experience and interaction quality in automated systems. This study not only advances the field of NLP in chat applications but also provides a framework for future research into context-sensitive AI developments.</li>
<li><strong>摘要：</strong>自动聊天系统中的有效沟通取决于理解和响应上下文的能力。传统模型通常难以确定何时需要额外的上下文来生成适当的响应。本文介绍了上下文感知 BERT (CA-BERT)，这是一种基于转换器的模型，专门针对这一挑战进行了微调。CA-BERT 创新地应用深度学习技术来辨别多轮聊天交互中的上下文必要性，从而提高了响应的相关性和准确性。我们描述了 CA-BERT 的开发，它采用了 BERT 的强大架构，并采用了一种新的训练方案，该方案专注于专门的聊天对话数据集。该模型根据其对上下文必要性进行分类的能力进行评估，在准确性和效率方面表现出优于基线 BERT 模型的性能。此外，CA-BERT 的实施展示了训练时间和资源使用量的显着减少，使其适用于实时应用。结果表明，CA-BERT 可以通过提供对上下文的细致理解来有效增强聊天机器人的功能，从而改善自动化系统中的用户体验和交互质量。这项研究不仅推动了聊天应用中的 NLP 领域的发展，而且还为未来情境敏感型人工智能发展的研究提供了一个框架。</li>
</ul>

<h3>Title: Shaping the Future of Endangered and Low-Resource Languages -- Our Role in the Age of LLMs: A Keynote at ECIR 2024</h3>
<ul>
<li><strong>Authors: </strong>Josiane Mothe (IRIT-SIG)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13702">https://arxiv.org/abs/2409.13702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13702">https://arxiv.org/pdf/2409.13702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13702]] Shaping the Future of Endangered and Low-Resource Languages -- Our Role in the Age of LLMs: A Keynote at ECIR 2024(https://arxiv.org/abs/2409.13702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Isidore of Seville is credited with the adage that it is language that gives birth to a people, and not the other way around , underlining the profound role played by language in the formation of cultural and social identity. Today, of the more than 7100 languages listed, a significant number are endangered. Since the 1970s, linguists, information seekers and enthusiasts have helped develop digital resources and automatic tools to support a wide range of languages, including endangered ones. The advent of Large Language Model (LLM) technologies holds both promise and peril. They offer unprecedented possibilities for the translation and generation of content and resources, key elements in the preservation and revitalisation of languages. They also present threat of homogenisation, cultural oversimplification and the further marginalisation of already vulnerable languages. The talk this paper is based on has proposed an initiatory journey, exploring the potential paths and partnerships between technology and tradition, with a particular focus on the Occitan language. Occitan is a language from Southern France, parts of Spain and Italy that played a major cultural and economic role, particularly in the Middle Ages. It is now endangered according to UNESCO. The talk critically has examined how human expertise and artificial intelligence can work together to offer hope for preserving the linguistic diversity that forms the foundation of our global and especially our European heritage while addressing some of the ethical and practical challenges that accompany the use of these powerful technologies. This paper is based on the keynote I gave at the 46th European Conference on Information Retrieval (ECIR 2024). As an alternative to reading this paper, a video talk is available online. 1 Date: 26 March 2024.</li>
<li><strong>摘要：</strong>塞维利亚的伊西多尔有句名言：“语言孕育了一个民族，而不是相反”，强调了语言在文化和社会认同形成中发挥的重大作用。如今，在列出的 7100 多种语言中，有相当一部分濒临灭绝。自 1970 年代以来，语言学家、信息搜索者和爱好者帮助开发了数字资源和自动化工具，以支持包括濒危语言在内的多种语言。大型语言模型 (LLM) 技术的出现既有希望，也有危险。它们为内容和资源的翻译和生成提供了前所未有的可能性，而这些是保护和振兴语言的关键要素。它们也带来了同质化、文化过度简单化和本已脆弱的语言进一步边缘化的威胁。本文基于此演讲，提出了一次开创性的旅程，探索技术与传统之间的潜在路径和伙伴关系，特别关注奥克语。奥克语是一种来自法国南部、西班牙和意大利部分地区的语言，在中世纪发挥了重要的文化和经济作用。联合国教科文组织认为，这种语言现在濒临灭绝。本次演讲批判性地研究了人类专业知识和人工智能如何共同努力，为保护构成我们全球特别是欧洲遗产基础的语言多样性带来希望，同时解决使用这些强大技术所带来的一些道德和实际挑战。本文基于我在第 46 届欧洲信息检索会议 (ECIR 2024) 上的主题演讲。除了阅读本文外，您还可以在线观看视频演讲。1 日期：2024 年 3 月 26 日。</li>
</ul>

<h3>Title: Entity Extraction from High-Level Corruption Schemes via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Koletsis, Panagiotis-Konstantinos Gemos, Christos Chronis, Iraklis Varlamis, Vasilis Efthymiou, Georgios Th. Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13704">https://arxiv.org/abs/2409.13704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13704">https://arxiv.org/pdf/2409.13704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13704]] Entity Extraction from High-Level Corruption Schemes via Large Language Models(https://arxiv.org/abs/2409.13704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rise of financial crime that has been observed in recent years has created an increasing concern around the topic and many people, organizations and governments are more and more frequently trying to combat it. Despite the increase of interest in this area, there is a lack of specialized datasets that can be used to train and evaluate works that try to tackle those problems. This article proposes a new micro-benchmark dataset for algorithms and models that identify individuals and organizations, and their multiple writings, in news articles, and presents an approach that assists in its creation. Experimental efforts are also reported, using this dataset, to identify individuals and organizations in financial-crime-related articles using various low-billion parameter Large Language Models (LLMs). For these experiments, standard metrics (Accuracy, Precision, Recall, F1 Score) are reported and various prompt variants comprising the best practices of prompt engineering are tested. In addition, to address the problem of ambiguous entity mentions, a simple, yet effective LLM-based disambiguation method is proposed, ensuring that the evaluation aligns with reality. Finally, the proposed approach is compared against a widely used state-of-the-art open-source baseline, showing the superiority of the proposed method.</li>
<li><strong>摘要：</strong>近年来，金融犯罪的增多引起了人们对这一主题的日益关注，许多人、组织和政府也越来越多地试图打击金融犯罪。尽管人们对这一领域的兴趣日益增加，但仍然缺乏专门的数据集来训练和评估试图解决这些问题的作品。本文提出了一种新的微基准数据集，用于识别新闻文章中的个人和组织及其多篇文章的算法和模型，并提出了一种有助于创建该数据集的方法。还报告了使用该数据集使用各种低十亿参数大型语言模型 (LLM) 在金融犯罪相关文章中识别个人和组织的实验努力。对于这些实验，报告了标准指标（准确度、精确度、召回率、F1 分数），并测试了包含提示工程最佳实践的各种提示变体。此外，为了解决实体提及不明确的问题，提出了一种简单但有效的基于 LLM 的消歧方法，确保评估与现实相符。最后，将所提出的方法与广泛使用的最先进的开源基线进行了比较，显示了所提出方法的优越性。</li>
</ul>

<h3>Title: Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Olivia Sturman, Aparna Joshi, Bhaktipriya Radharapu, Piyush Kumar, Renee Shelby</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13705">https://arxiv.org/abs/2409.13705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13705">https://arxiv.org/pdf/2409.13705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13705]] Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble(https://arxiv.org/abs/2409.13705)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Increasing use of large language models (LLMs) demand performant guardrails to ensure the safety of inputs and outputs of LLMs. When these safeguards are trained on imbalanced data, they can learn the societal biases. We present a light-weight, post-processing method for mitigating counterfactual fairness in closed-source text safety classifiers. Our approach involves building an ensemble that not only outperforms the input classifiers and policy-aligns them, but also acts as a debiasing regularizer. We introduce two threshold-agnostic metrics to assess the counterfactual fairness of a model, and demonstrate how combining these metrics with Fair Data Reweighting (FDW) helps mitigate biases. We create an expanded Open AI dataset, and a new templated LLM-generated dataset based on user-prompts, both of which are counterfactually balanced across identity groups and cover four key areas of safety; we will work towards publicly releasing these datasets. Our results show that our approach improves counterfactual fairness with minimal impact on model performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的使用越来越多，需要高性能护栏来确保 LLM 输入和输出的安全。当这些保护措施在不平衡的数据上进行训练时，它们可以学习社会偏见。我们提出了一种轻量级的后处理方法来减轻闭源文本安全分类器中的反事实公平性。我们的方法包括构建一个集成，它不仅优于输入分类器并对其进行策略对齐，而且还充当去偏差正则化器。我们引入了两个阈值无关的指标来评估模型的反事实公平性，并展示了如何将这些指标与公平数据重新加权 (FDW) 相结合来帮助减轻偏见。我们创建了一个扩展的 Open AI 数据集和一个基于用户提示的新模板化 LLM 生成的数据集，这两个数据集在身份组之间都是反事实平衡的，并涵盖了四个关键的安全领域；我们将努力公开发布这些数据集。我们的结果表明，我们的方法提高了反事实公平性，对模型性能的影响最小。</li>
</ul>

<h3>Title: Towards Safe Multilingual Frontier AI</h3>
<ul>
<li><strong>Authors: </strong>Artūrs Kanepajs, Vladimir Ivanov, Richard Moulange</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13708">https://arxiv.org/abs/2409.13708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13708">https://arxiv.org/pdf/2409.13708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13708]] Towards Safe Multilingual Frontier AI(https://arxiv.org/abs/2409.13708)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Linguistically inclusive LLMs -- which maintain good performance regardless of the language with which they are prompted -- are necessary for the diffusion of AI benefits around the world. Multilingual jailbreaks that rely on language translation to evade safety measures undermine the safe and inclusive deployment of AI systems. We provide policy recommendations to enhance the multilingual capabilities of AI while mitigating the risks of multilingual jailbreaks. We quantitatively assess the relationship between language resourcedness and model vulnerabilities to multilingual jailbreaks for five frontier large language models across 24 official EU languages. Building on prior research, we propose policy actions that align with the EU legal landscape and institutional framework to address multilingual jailbreaks, while promoting linguistic inclusivity. These include mandatory assessments of multilingual capabilities and vulnerabilities, public opinion research, and state support for multilingual AI development. The measures aim to improve AI safety and functionality through EU policy initiatives, guiding the implementation of the EU AI Act and informing regulatory efforts of the European AI Office.</li>
<li><strong>摘要：</strong>语言包容性的法学硕士（无论使用哪种语言进行提示都能保持良好的性能）对于在世界范围内传播人工智能的优势是必不可少的。依赖语言翻译来逃避安全措施的多语言越狱破坏了人工智能系统的安全和包容性部署。我们提供政策建议，以增强人工智能的多语言能力，同时降低多语言越狱的风险。我们定量评估了 24 种欧盟官方语言中的五种前沿大型语言模型的语言资源和模型对多语言越狱的脆弱性之间的关系。在先前研究的基础上，我们提出了符合欧盟法律环境和体制框架的政策行动，以解决多语言越狱问题，同时促进语言包容性。这些措施包括对多语言能力和脆弱性的强制性评估、民意调查以及国家对多语言人工智能发展的支持。这些措施旨在通过欧盟政策举措提高人工智能的安全性和功能性，指导欧盟人工智能法案的实施，并为欧洲人工智能办公室的监管工作提供信息。</li>
</ul>

<h3>Title: Column Vocabulary Association (CVA): semantic interpretation of dataless tables</h3>
<ul>
<li><strong>Authors: </strong>Margherita Martorana, Xueli Pan, Benno Kruit, Tobias Kuhn, Jacco van Ossenbruggen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13709">https://arxiv.org/abs/2409.13709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13709">https://arxiv.org/pdf/2409.13709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13709]] Column Vocabulary Association (CVA): semantic interpretation of dataless tables(https://arxiv.org/abs/2409.13709)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Traditional Semantic Table Interpretation (STI) methods rely primarily on the underlying table data to create semantic annotations. This year's SemTab challenge introduced the ``Metadata to KG'' track, which focuses on performing STI by using only metadata information, without access to the underlying data. In response to this new challenge, we introduce a new term: Column Vocabulary Association (CVA). This term refers to the task of semantic annotation of column headers solely based on metadata information. In this study, we evaluate the performance of various methods in executing the CVA task, including a Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) approach, as well as a more traditional similarity approach with SemanticBERT. Our methodology uses a zero-shot setting, with no pretraining or examples passed to the Large Language Models (LLMs), as we aim to avoid a domain-specific setting. We investigate a total of 7 different LLMs, of which three commercial GPT models (i.e. gpt-3.5-turbo-0.125, gpt-4o and gpt-4-turbo) and four open source models (i.e. llama3-80b, llama3-7b, gemma-7b and mixtral-8x7b). We integrate this models with RAG systems, and we explore how variations in temperature settings affect performances. Moreover, we continue our investigation by performing the CVA task utilizing SemanticBERT, analyzing how various metadata information influence its performance. Initial findings indicate that LLMs generally perform well at temperatures below 1.0, achieving an accuracy of 100\% in certain cases. Nevertheless, our investigation also reveal that the nature of the data significantly influences CVA task outcomes. In fact, in cases where the input data and glossary are related (for example by being created by the same organizations) traditional methods appear to surpass the performance of LLMs.</li>
<li><strong>摘要：</strong>传统的语义表解释 (STI) 方法主要依赖于底层表数据来创建语义注释。今年的 SemTab 挑战赛引入了“元数据到 KG”轨道，该轨道专注于仅使用元数据信息执行 STI，而无需访问底层数据。为了应对这一新挑战，我们引入了一个新术语：列词汇关联 (CVA)。此术语指仅基于元数据信息对列标题进行语义注释的任务。在本研究中，我们评估了执行 CVA 任务的各种方法的性能，包括大型语言模型 (LLM) 和检索增强生成 (RAG) 方法，以及使用 SemanticBERT 的更传统的相似性方法。我们的方法使用零样本设置，没有预训练或将示例传递给大型语言模型 (LLM)，因为我们的目标是避免特定于领域的设置。我们总共调查了 7 种不同的 LLM，其中包括三种商业 GPT 模型（即 gpt-3.5-turbo-0.125、gpt-4o 和 gpt-4-turbo）和四种开源模型（即 llama3-80b、llama3-7b、gemma-7b 和 mixtral-8x7b）。我们将这些模型与 RAG 系统集成，并探索温度设置的变化如何影响性能。此外，我们继续调查，利用 SemanticBERT 执行 CVA 任务，分析各种元数据信息如何影响其性能。初步结果表明，LLM 在低于 1.0 的温度下通常表现良好，在某些情况下可达到 100\% 的准确率。然而，我们的调查还表明，数据的性质会显著影响 CVA 任务结果。事实上，在输入数据和词汇表相关的情况下（例如由同一组织创建），传统方法似乎超越了 LLM 的性能。</li>
</ul>

<h3>Title: You can remove GPT2's LayerNorm by fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Stefan Heimersheim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13710">https://arxiv.org/abs/2409.13710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13710">https://arxiv.org/pdf/2409.13710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13710]] You can remove GPT2's LayerNorm by fine-tuning(https://arxiv.org/abs/2409.13710)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>The LayerNorm (LN) layer in GPT-style transformer models has long been a hindrance to mechanistic interpretability. LN is a crucial component required to stabilize the training of large language models, and LN or the similar RMSNorm have been used in practically all large language models based on the transformer architecture. The non-linear nature of the LN layers is a hindrance for mechanistic interpretability as it hinders interpretation of the residual stream, and makes it difficult to decompose the model into circuits. Some research have gone so far as to name "reasons interpretability researchers hate layer norm". In this paper we show that it is possible to remove the LN layers from a pre-trained GPT2-small model by fine-tuning on a fraction (500M tokens) of the training data. We demonstrate that this LN-free model achieves similar performance to the original model on the OpenWebText and ThePile datasets (-0.05 cross-entropy loss), and the Hellaswag benchmark (-0.5% accuracy). We provide the fine-tuning procedure and a Hugging Face repository with the fine-tuned GPT2-small models. Our work not only provides a simplified model for mechanistic interpretability research, but also provides evidence that the LN layers, at inference time, do not play a crucial role in transformer models.</li>
<li><strong>摘要：</strong>GPT 风格 Transformer 模型中的 LayerNorm (LN) 层长期以来一直是机械可解释性的障碍。LN 是稳定大型语言模型训练所需的关键组件，几乎所有基于 Transformer 架构的大型语言模型都使用了 LN 或类似的 RMNSorm。LN 层的非线性特性阻碍了机械可解释性，因为它阻碍了对残差流的解释，并使模型难以分解为电路。一些研究甚至指出了“可解释性研究人员讨厌层范数的原因”。在本文中，我们展示了可以通过对训练数据的一小部分（5 亿个 token）进行微调，从预训练的 GPT2-small 模型中删除 LN 层。我们证明这个无 LN 模型在 OpenWebText 和 ThePile 数据集（-0.05 交叉熵损失）和 Hellaswag 基准（-0.5% 准确率）上实现了与原始模型类似的性能。我们提供了微调程序和包含微调后的 GPT2-small 模型的 Hugging Face 存储库。我们的工作不仅为机械可解释性研究提供了一个简化的模型，而且还提供了证据，证明 LN 层在推理时在 Transformer 模型中并不起关键作用。</li>
</ul>

<h3>Title: Good Idea or Not, Representation of LLM Could Tell</h3>
<ul>
<li><strong>Authors: </strong>Yi Xu, Bo Xue, Shuqian Sheng, Cheng Deng, Jiaxin Ding, Zanwei Shen, Luoyi Fu, Xinbing Wang, Chenghu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13712">https://arxiv.org/abs/2409.13712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13712">https://arxiv.org/pdf/2409.13712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13712]] Good Idea or Not, Representation of LLM Could Tell(https://arxiv.org/abs/2409.13712)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the ever-expanding landscape of academic research, the proliferation of ideas presents a significant challenge for researchers: discerning valuable ideas from the less impactful ones. The ability to efficiently evaluate the potential of these ideas is crucial for the advancement of science and paper review. In this work, we focus on idea assessment, which aims to leverage the knowledge of large language models to assess the merit of scientific ideas. First, we investigate existing text evaluation research and define the problem of quantitative evaluation of ideas. Second, we curate and release a benchmark dataset from nearly four thousand manuscript papers with full texts, meticulously designed to train and evaluate the performance of different approaches to this task. Third, we establish a framework for quantifying the value of ideas by employing representations in a specific layer of large language models. Experimental results show that the scores predicted by our method are relatively consistent with those of humans. Our findings suggest that the representations of large language models hold more potential in quantifying the value of ideas than their generative outputs, demonstrating a promising avenue for automating the idea assessment process.</li>
<li><strong>摘要：</strong>在不断扩展的学术研究领域中，思想的激增给研究人员带来了重大挑战：从影响较小的思想中辨别出有价值的思想。有效评估这些思想潜力的能力对于科学的进步和论文评审至关重要。在这项工作中，我们专注于思想评估，旨在利用大型语言模型的知识来评估科学思想的价值。首先，我们调查现有的文本评估研究并定义思想定量评估的问题。其次，我们从近四千份带有全文的手稿论文中整理并发布了一个基准数据集，精心设计用于训练和评估不同方法执行此任务的性能。第三，我们通过在大型语言模型的特定层中使用表示来建立一个量化思想价值的框架。实验结果表明，我们的方法预测的分数与人类的分数相对一致。我们的研究结果表明，大型语言模型的表示在量化思想价值方面比其生成输出更具潜力，为自动化思想评估过程提供了一条有希望的途径。</li>
</ul>

<h3>Title: TracrBench: Generating Interpretability Testbeds with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hannes Thurnherr, Jérémy Scheurer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13714">https://arxiv.org/abs/2409.13714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13714">https://arxiv.org/pdf/2409.13714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13714]] TracrBench: Generating Interpretability Testbeds with Large Language Models(https://arxiv.org/abs/2409.13714)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Achieving a mechanistic understanding of transformer-based language models is an open challenge, especially due to their large number of parameters. Moreover, the lack of ground truth mappings between model weights and their functional roles hinders the effective evaluation of interpretability methods, impeding overall progress. Tracr, a method for generating compiled transformers with inherent ground truth mappings in RASP, has been proposed to address this issue. However, manually creating a large number of models needed for verifying interpretability methods is labour-intensive and time-consuming. In this work, we present a novel approach for generating interpretability test beds using large language models (LLMs) and introduce TracrBench, a novel dataset consisting of 121 manually written and LLM-generated, human-validated RASP programs and their corresponding transformer weights. During this process, we evaluate the ability of frontier LLMs to autonomously generate RASP programs and find that this task poses significant challenges. GPT-4-turbo, with a 20-shot prompt and best-of-5 sampling, correctly implements only 57 out of 101 test programs, necessitating the manual implementation of the remaining programs. With its 121 samples, TracrBench aims to serve as a valuable testbed for evaluating and comparing interpretability methods.</li>
<li><strong>摘要：</strong>从机制上理解基于转换器的语言模型是一项开放性挑战，尤其是由于其参数数量众多。此外，模型权重与其功能角色之间缺乏基本事实映射，这阻碍了对可解释性方法的有效评估，从而阻碍了整体进展。Tracr 是一种在 RASP 中生成具有固有基本事实映射的编译转换器的方法，已被提出用于解决此问题。但是，手动创建验证可解释性方法所需的大量模型既费力又费时。在这项工作中，我们提出了一种使用大型语言模型 (LLM) 生成可解释性测试平台的新方法，并介绍了 TracrBench，这是一个由 121 个手动编写和 LLM 生成的、人工验证的 RASP 程序及其相应的转换器权重组成的新数据集。在此过程中，我们评估了前沿 LLM 自主生成 RASP 程序的能力，并发现这项任务带来了重大挑战。 GPT-4-turbo 具有 20 次提示和 5 次最佳采样，在 101 个测试程序中仅正确执行了 57 个，其余程序需要手动执行。TracrBench 拥有 121 个样本，旨在成为评估和比较可解释性方法的宝贵测试平台。</li>
</ul>

<h3>Title: Constrained Multi-Layer Contrastive Learning for Implicit Discourse Relationship Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yiheng Wu, Junhui Li, Muhua Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13716">https://arxiv.org/abs/2409.13716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13716">https://arxiv.org/pdf/2409.13716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13716]] Constrained Multi-Layer Contrastive Learning for Implicit Discourse Relationship Recognition(https://arxiv.org/abs/2409.13716)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Previous approaches to the task of implicit discourse relation recognition (IDRR) generally view it as a classification task. Even with pre-trained language models, like BERT and RoBERTa, IDRR still relies on complicated neural networks with multiple intermediate layers to proper capture the interaction between two discourse units. As a result, the outputs of these intermediate layers may have different capability in discriminating instances of different classes. To this end, we propose to adapt a supervised contrastive learning (CL) method, label- and instance-centered CL, to enhance representation learning. Moreover, we propose a novel constrained multi-layer CL approach to properly impose a constraint that the contrastive loss of higher layers should be smaller than that of lower layers. Experimental results on PDTB 2.0 and PDTB 3.0 show that our approach can significantly improve the performance on both multi-class classification and binary classification.</li>
<li><strong>摘要：</strong>先前针对隐式语篇关系识别 (IDRR) 任务的方法通常将其视为分类任务。即使使用预先训练的语言模型，如 BERT 和 RoBERTa，IDRR 仍然依赖于具有多个中间层的复杂神经网络来正确捕捉两个语篇单元之间的交互。因此，这些中间层的输出在区分不同类别的实例方面可能具有不同的能力。为此，我们建议采用一种监督对比学习 (CL) 方法，即以标签和实例为中心的 CL，以增强表示学习。此外，我们提出了一种新颖的约束多层 CL 方法来适当地施加约束，即较高层的对比损失应小于较低层的对比损失。在 PDTB 2.0 和 PDTB 3.0 上的实验结果表明，我们的方法可以显着提高多类分类和二元分类的性能。</li>
</ul>

<h3>Title: DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yiheng Wu, Roman Yangarber, Xian Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13717">https://arxiv.org/abs/2409.13717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13717">https://arxiv.org/pdf/2409.13717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13717]] DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level Relation Extraction(https://arxiv.org/abs/2409.13717)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The remarkable capabilities of Large Language Models (LLMs) in text comprehension and generation have revolutionized Information Extraction (IE). One such advancement is in Document-level Relation Triplet Extraction (DocRTE), a critical task in information systems that aims to extract entities and their semantic relationships from documents. However, existing methods are primarily designed for Sentence level Relation Triplet Extraction (SentRTE), which typically handles a limited set of relations and triplet facts within a single sentence. Additionally, some approaches treat relations as candidate choices integrated into prompt templates, resulting in inefficient processing and suboptimal performance when determining the relation elements in triplets. To address these limitations, we introduce a Discriminative and Voice Aware Paradigm DiVA. DiVA involves only two steps: performing document-level relation extraction (DocRE) and then identifying the subject object entities based on the relation. No additional processing is required simply input the document to directly obtain the triplets. This streamlined process more accurately reflects real-world scenarios for triplet extraction. Our innovation lies in transforming DocRE into a discriminative task, where the model pays attention to each relation and to the often overlooked issue of active vs. passive voice within the triplet. Our experiments on the Re-DocRED and DocRED datasets demonstrate state-of-the-art results for the DocRTE task.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在文本理解和生成方面的卓越能力彻底改变了信息提取 (IE)。其中一项进步是文档级关系三元组提取 (DocRTE)，这是信息系统中的一项关键任务，旨在从文档中提取实体及其语义关系。然而，现有方法主要针对句子级关系三元组提取 (SentRTE)，它通常处理单个句子中的一组有限的关系和三元组事实。此外，一些方法将关系视为集成到提示模板中的候选选择，导致在确定三元组中的关系元素时处理效率低下且性能不佳。为了解决这些限制，我们引入了判别和语音感知范式 DiVA。DiVA 仅涉及两个步骤：执行文档级关系提取 (DocRE)，然后根据关系识别主题对象实体。无需额外处理，只需输入文档即可直接获得三元组。这个简化的过程更准确地反映了三元组提取的真实场景。我们的创新之处在于将 DocRE 转变为一项判别性任务，其中模型会关注每一种关系以及三元组中经常被忽视的主动语态与被动语态问题。我们在 Re-DocRED 和 DocRED 数据集上进行的实验展示了 DocRTE 任务的最新成果。</li>
</ul>

<h3>Title: LegiLM: A Fine-Tuned Legal Language Model for Data Compliance</h3>
<ul>
<li><strong>Authors: </strong>Linkai Zhu, Lu Yang, Chaofan Li, Shanwen Hu, Lu Liu, Bin Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13721">https://arxiv.org/abs/2409.13721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13721">https://arxiv.org/pdf/2409.13721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13721]] LegiLM: A Fine-Tuned Legal Language Model for Data Compliance(https://arxiv.org/abs/2409.13721)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Ensuring compliance with international data protection standards for privacy and data security is a crucial but complex task, often requiring substantial legal expertise. This paper introduces LegiLM, a novel legal language model specifically tailored for consulting on data or information compliance. LegiLM leverages a pre-trained GDPR Fines dataset and has been fine-tuned to automatically assess whether particular actions or events breach data security and privacy regulations. By incorporating a specialized dataset that includes global data protection laws, meticulously annotated policy documents, and relevant privacy policies, LegiLM is optimized for addressing data compliance challenges. The model integrates advanced legal reasoning methods and information retrieval enhancements to enhance accuracy and reliability in practical legal consulting scenarios. Our evaluation using a custom benchmark dataset demonstrates that LegiLM excels in detecting data regulation breaches, offering sound legal justifications, and recommending necessary compliance modifications, setting a new benchmark for AI-driven legal compliance solutions. Our resources are publicly available at this https URL</li>
<li><strong>摘要：</strong>确保遵守隐私和数据安全的国际数据保护标准是一项至关重要但复杂的任务，通常需要大量的法律专业知识。本文介绍了 LegiLM，这是一种专为数据或信息合规咨询而量身定制的新型法律语言模型。LegiLM 利用预先训练的 GDPR 罚款数据集，并经过微调，可以自动评估特定行为或事件是否违反数据安全和隐私法规。通过整合包含全球数据保护法、精心注释的政策文件和相关隐私政策的专门数据集，LegiLM 针对解决数据合规挑战进行了优化。该模型集成了先进的法律推理方法和信息检索增强功能，以提高实际法律咨询场景的准确性和可靠性。我们使用自定义基准数据集进行的评估表明，LegiLM 在检测数据法规违规行为、提供合理的法律依据和推荐必要的合规性修改方面表现出色，为人工智能驱动的法律合规解决方案树立了新的标杆。我们的资源可在此 https URL 上公开获取</li>
</ul>

<h3>Title: Logically Consistent Language Models via Neuro-Symbolic Integration</h3>
<ul>
<li><strong>Authors: </strong>Diego Calanzone, Stefano Teso, Antonio Vergari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13724">https://arxiv.org/abs/2409.13724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13724">https://arxiv.org/pdf/2409.13724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13724]] Logically Consistent Language Models via Neuro-Symbolic Integration(https://arxiv.org/abs/2409.13724)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are a promising venue for natural language understanding and generation. However, current LLMs are far from reliable: they are prone to generating non-factual information and, more crucially, to contradicting themselves when prompted to reason about relations between entities of the world. These problems are currently addressed with large scale fine-tuning or by delegating reasoning to external tools. In this work, we strive for a middle ground and introduce a loss based on neuro-symbolic reasoning that teaches an LLM to be logically consistent with an external set of facts and rules and improves self-consistency even when the LLM is fine-tuned on a limited set of facts. Our approach also allows to easily combine multiple logical constraints at once in a principled way, delivering LLMs that are more consistent w.r.t. all constraints and improve over several baselines w.r.t. a given constraint. Moreover, our method allows LLMs to extrapolate to unseen but semantically similar factual knowledge, represented in unseen datasets, more systematically.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是自然语言理解和生成的一个有前途的平台。然而，目前的 LLM 远非可​​靠：它们容易生成非事实信息，更重要的是，当被提示推理世界实体之间的关系时，它们会自相矛盾。这些问题目前通过大规模微调或将推理委托给外部工具来解决。在这项工作中，我们努力寻找一个中间立场，并引入基于神经符号推理的损失，它教导 LLM 在逻辑上与外部事实和规则集保持一致，并提高自洽性，即使 LLM 在有限的事实集上进行微调也是如此。我们的方法还允许以原则性的方式轻松地同时组合多个逻辑约束，提供与所有约束更一致的 LLM，并在与给定约束相关的多个基线上有所改进。此外，我们的方法允许 LLM 更系统地推断出在看不见的数据集中表示的看不见但语义相似的事实知识。</li>
</ul>

<h3>Title: Classification performance and reproducibility of GPT-4 omni for information extraction from veterinary electronic health records</h3>
<ul>
<li><strong>Authors: </strong>Judit M Wulcan, Kevin L Jacques, Mary Ann Lee, Samantha L Kovacs, Nicole Dausend, Lauren E Prince, Jonatan Wulcan, Sina Marsilio, Stefan M Keller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13727">https://arxiv.org/abs/2409.13727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13727">https://arxiv.org/pdf/2409.13727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13727]] Classification performance and reproducibility of GPT-4 omni for information extraction from veterinary electronic health records(https://arxiv.org/abs/2409.13727)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can extract information from veterinary electronic health records (EHRs), but performance differences between models, the effect of temperature settings, and the influence of text ambiguity have not been previously evaluated. This study addresses these gaps by comparing the performance of GPT-4 omni (GPT-4o) and GPT-3.5 Turbo under different conditions and investigating the relationship between human interobserver agreement and LLM errors. The LLMs and five humans were tasked with identifying six clinical signs associated with Feline chronic enteropathy in 250 EHRs from a veterinary referral hospital. At temperature 0, the performance of GPT-4o compared to the majority opinion of human respondents, achieved 96.9% sensitivity (interquartile range [IQR] 92.9-99.3%), 97.6% specificity (IQR 96.5-98.5%), 80.7% positive predictive value (IQR 70.8-84.6%), 99.5% negative predictive value (IQR 99.0-99.9%), 84.4% F1 score (IQR 77.3-90.4%), and 96.3% balanced accuracy (IQR 95.0-97.9%). The performance of GPT-4o was significantly better than that of its predecessor, GPT-3.5 Turbo, particularly with respect to sensitivity where GPT-3.5 Turbo only achieved 81.7% (IQR 78.9-84.8%). Adjusting the temperature for GPT-4o did not significantly impact classification performance. GPT-4o demonstrated greater reproducibility than human pairs regardless of temperature, with an average Cohen's kappa of 0.98 (IQR 0.98-0.99) at temperature 0 compared to 0.8 (IQR 0.78-0.81) for humans. Most GPT-4o errors occurred in instances where humans disagreed (35/43 errors, 81.4%), suggesting that these errors were more likely caused by ambiguity of the EHR than explicit model faults. Using GPT-4o to automate information extraction from veterinary EHRs is a viable alternative to manual extraction.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可以从兽医电子健康记录 (EHR) 中提取信息，但之前尚未评估模型之间的性能差异、温度设置的影响以及文本歧义的影响。本研究通过比较 GPT-4 omni (GPT-4o) 和 GPT-3.5 Turbo 在不同条件下的性能并研究人类观察者间一致性与 LLM 误差之间的关系来解决这些差距。LLM 和五名人类的任务是在一家兽医转诊医院的 250 份 EHR 中识别与猫慢性肠病相关的六种临床症状。在温度为 0 时，GPT-4o 与人类受访者的多数意见相比，其灵敏度达到 96.9%（四分位距 [IQR] 92.9-99.3%），特异性达到 97.6%（IQR 96.5-98.5%），阳性预测值达到 80.7%（IQR 70.8-84.6%），阴性预测值达到 99.5%（IQR 99.0-99.9%），F1 得分达到 84.4%（IQR 77.3-90.4%），平衡准确度达到 96.3%（IQR 95.0-97.9%）。GPT-4o 的性能明显优于其前身 GPT-3.5 Turbo，特别是在灵敏度方面，GPT-3.5 Turbo 仅达到 81.7%（IQR 78.9-84.8%）。调整 GPT-4o 的温度不会显著影响分类性能。无论温度如何，GPT-4o 都表现出比人类配对更高的可重复性，温度为 0 时的平均 Cohen's kappa 为 0.98（IQR 0.98-0.99），而人类为 0.8（IQR 0.78-0.81）。大多数 GPT-4o 错误发生在人类意见不一致的情况下（35/43 个错误，81.4%），这表明这些错误更可能是由 EHR 的模糊性而不是明确的模型故障引起的。使用 GPT-4o 自动从兽医 EHR 中提取信息是手动提取的可行替代方案。</li>
</ul>

<h3>Title: Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts</h3>
<ul>
<li><strong>Authors: </strong>Anna Mészáros, Szilvia Ujváry, Wieland Brendel, Patrik Reizinger, Ferenc Huszár</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13728">https://arxiv.org/abs/2409.13728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13728">https://arxiv.org/pdf/2409.13728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13728]] Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts(https://arxiv.org/abs/2409.13728)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>LLMs show remarkable emergent abilities, such as inferring concepts from presumably out-of-distribution prompts, known as in-context learning. Though this success is often attributed to the Transformer architecture, our systematic understanding is limited. In complex real-world data sets, even defining what is out-of-distribution is not obvious. To better understand the OOD behaviour of autoregressive LLMs, we focus on formal languages, which are defined by the intersection of rules. We define a new scenario of OOD compositional generalization, termed rule extrapolation. Rule extrapolation describes OOD scenarios, where the prompt violates at least one rule. We evaluate rule extrapolation in formal languages with varying complexity in linear and recurrent architectures, the Transformer, and state space models to understand the architectures' influence on rule extrapolation. We also lay the first stones of a normative theory of rule extrapolation, inspired by the Solomonoff prior in algorithmic information theory.</li>
<li><strong>摘要：</strong>LLM 表现出非凡的新兴能力，例如从可能超出分布的提示中推断概念，这被称为上下文学习。虽然这种成功通常归功于 Transformer 架构，但我们的系统理解有限。在复杂的现实世界数据集中，甚至定义什么是超出分布的都不明显。为了更好地理解自回归 LLM 的 OOD 行为，我们专注于形式语言，它由规则的交集定义。我们定义了一种新的 OOD 组合泛化场景，称为规则外推。规则外推描述了提示违反至少一条规则的 OOD 场景。我们评估了线性和循环架构、Transformer 和状态空间模型中复杂度各异的形式语言中的规则外推，以了解架构对规则外推的影响。我们还奠定了规则外推的规范理论的第一块基石，该理论受到算法信息论中的 Solomonoff 先验的启发。</li>
</ul>

<h3>Title: MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Yuxiao Dong, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13729">https://arxiv.org/abs/2409.13729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13729">https://arxiv.org/pdf/2409.13729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13729]] MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model(https://arxiv.org/abs/2409.13729)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated significant capabilities in mathematical reasoning, particularly with text-based mathematical problems. However, current multi-modal large language models (MLLMs), especially those specialized in mathematics, tend to focus predominantly on solving geometric problems but ignore the diversity of visual information available in other areas of mathematics. Moreover, the geometric information for these specialized mathematical MLLMs is derived from several public datasets, which are typically limited in diversity and complexity. To address these limitations, we aim to construct a fine-tuning dataset named MathVL, and develop a series of specialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised Fine-Tuning (SFT) on MathVL with various parameter-scale backbones. To extensively evaluate the effectiveness of MathGLM-Vision, we conduct experiments on several public benchmarks and our curated MathVL-test consisting of 2,000 problems. Experimental results demonstrate that MathGLM-Vision achieves significant improvements compared with some existing models, including backbone models and open-source mathematical MLLMs. These findings indicate the importance of diversity dataset in enhancing the mathematical reasoning abilities of MLLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出在数学推理方面的强大能力，尤其是在基于文本的数学问题方面。然而，当前的多模态大型语言模型 (MLLM)，尤其是专门用于数学的模型，往往主要侧重于解决几何问题，而忽略了数学其他领域可用的视觉信息的多样性。此外，这些专门的数学 MLLM 的几何信息来自几个公共数据集，这些数据集通常在多样性和复杂性方面受到限制。为了解决这些限制，我们旨在构建一个名为 MathVL 的微调数据集，并通过对 MathVL 使用各种参数尺度主干进行监督微调 (SFT) 来开发一系列称为 MathGLM-Vision 的专门数学 MLLM。为了广泛评估 MathGLM-Vision 的有效性，我们对几个公共基准和我们精心挑选的由 2,000 个问题组成的 MathVL 测试进行了实验。实验结果表明，与一些现有模型（包括主干模型和开源数学 MLLM）相比，MathGLM-Vision 取得了显着的改进。这些发现表明多样性数据集对于增强 MLLM 的数学推理能力的重要性。</li>
</ul>

<h3>Title: KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Wen Zhang, Huajun Chen, Zhiqiang Zhang, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13731">https://arxiv.org/abs/2409.13731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13731">https://arxiv.org/pdf/2409.13731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13731]] KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation(https://arxiv.org/abs/2409.13731)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The recently developed retrieval-augmented generation (RAG) technology enables the efficient construction of domain-specific applications. However, it faces limitations due to fuzzy retrieval processes, the "hallucination" problem of understanding and reasoning capabilities of general language models, and cascading losses in complex systems. These challenges hinder the effectiveness of specialized knowledge services. However, in scenarios such as scientific computing, medicine, and law, the accuracy of knowledge, the completeness of information, and the logical rigor of rules, time, and values are particularly critical. We Introduce professional domain knowledge service framework: Knowledge Augmented Generation(KAG) to improve generation and reasoning performance by bidirectionally enhancing large language model(LLM)s and knowledge graph(KG)s, including five key enhancements: 1) LLM-friendly knowledge semantic representation, 2) mutual indexing between knowledge graph and original chunks, 3) logicalform-guided hybrid reasoning and solving, 4) Knowledge alignment based on semantic reasoning, 5) Model for KAG. We compared KAG with existing RAG methods in multi-hop question answering. The results show that KAG performs significantly better than the state-of-the-art methods, with a relative improvement from 19.6% to 33.4% in F1. We apply KAG to two professional knowledge Q&A tasks of Ant Group, including E-Goverment Q&A and E-Health Q&A, and has achieved significant improvement in professionalism compared with NaiveRAG. We will soon natively support KAG on the open source KG engine OpenSPG, allowing developers to more easily build rigorous knowledge decision-making or convenient information retrieval services.</li>
<li><strong>摘要：</strong>近期发展的检索增强生成（RAG）技术使得领域特定应用的构建变得高效，然而由于检索过程的模糊性、通用语言模型理解和推理能力的“幻觉”问题以及复杂系统中的级联损失等问题，该技术面临诸多限制，阻碍了专业化知识服务的有效性。然而，在科学计算、医学、法律等场景中，知识的准确性、信息的完备性以及规则、时间和值的逻辑严谨性尤为关键。我们引入专业领域知识服务框架：知识增强生成（KAG），通过双向增强大型语言模型（LLM）和知识图谱（KG）来提升生成和推理性能，包括五项关键增强：1）LLM友好的知识语义表示，2）知识图谱与原始块之间的相互索引，3）逻辑形式引导的混合推理与求解，4）基于语义推理的知识对齐，5）KAG模型。我们将 KAG 与现有的 RAG 方法在多跳问答中进行了比较，结果表明 KAG 的表现明显优于当前最佳方法，F1 相对提升从 19.6% 到 33.4%。我们将 KAG 应用于蚂蚁集团的两项专业知识问答任务，包括电子政务问答和电子医疗问答，与 NaiveRAG 相比，在专业性上取得了显著的提升。我们将很快在开源 KG 引擎 OpenSPG 上原生支持 KAG，让开发者可以更轻松地构建严谨的知识决策或便捷的信息检索服务。</li>
</ul>

<h3>Title: TopoChat: Enhancing Topological Materials Retrieval With Large Language Model and Multi-Source Knowledge</h3>
<ul>
<li><strong>Authors: </strong>HuangChao Xu, Baohua Zhang, Zhong Jin, Tiannian Zhu, Quansheng Wu, Hongming Weng</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13732">https://arxiv.org/abs/2409.13732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13732">https://arxiv.org/pdf/2409.13732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13732]] TopoChat: Enhancing Topological Materials Retrieval With Large Language Model and Multi-Source Knowledge(https://arxiv.org/abs/2409.13732)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), such as ChatGPT, have demonstrated impressive performance in the text generation task, showing the ability to understand and respond to complex instructions. However, the performance of naive LLMs in speciffc domains is limited due to the scarcity of domain-speciffc corpora and specialized training. Moreover, training a specialized large-scale model necessitates signiffcant hardware resources, which restricts researchers from leveraging such models to drive advances. Hence, it is crucial to further improve and optimize LLMs to meet speciffc domain demands and enhance their scalability. Based on the condensed matter data center, we establish a material knowledge graph (MaterialsKG) and integrate it with literature. Using large language models and prompt learning, we develop a specialized dialogue system for topological materials called TopoChat. Compared to naive LLMs, TopoChat exhibits superior performance in structural and property querying, material recommendation, and complex relational reasoning. This system enables efffcient and precise retrieval of information and facilitates knowledge interaction, thereby encouraging the advancement on the ffeld of condensed matter materials.</li>
<li><strong>摘要：</strong>大型语言模型（LLM），例如 ChatGPT，在文本生成任务中表现出色，展现出理解和响应复杂指令的能力。然而，由于领域特定语料库和专业训练的稀缺，朴素 LLM 在特定领域的表现有限。此外，训练专门的大型模型需要大量硬件资源，这限制了研究人员利用此类模型推动进步。因此，进一步改进和优化 LLM 以满足特定领域的需求并增强其可扩展性至关重要。基于凝聚态数据中心，我们建立了材料知识图谱（MaterialsKG）并将其与文献相结合。利用大型语言模型和快速学习，我们开发了一个专门针对拓扑材料的对话系统 TopoChat。与朴素 LLM 相比，TopoChat 在结构和属性查询、材料推荐和复杂关系推理方面表现出色。该系统能够高效、精准地检索信息，促进知识交互，从而促进凝聚态材料领域的进步。</li>
</ul>

<h3>Title: RNR: Teaching Large Language Models to Follow Roles and Rules</h3>
<ul>
<li><strong>Authors: </strong>Kuan Wang, Alexander Bukharin, Haoming Jiang, Qingyu Yin, Zhengyang Wang, Tuo Zhao, Jingbo Shang, Chao Zhang, Bing Yin, Xian Li, Jianshu Chen, Shiyang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13733">https://arxiv.org/abs/2409.13733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13733">https://arxiv.org/pdf/2409.13733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13733]] RNR: Teaching Large Language Models to Follow Roles and Rules(https://arxiv.org/abs/2409.13733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Instruction fine-tuning (IFT) elicits instruction following capabilities and steers the behavior of large language models (LLMs) via supervised learning. However, existing models trained on open-source IFT datasets only have the ability to follow instructions from users, and often fail to follow complex role and rules specified by developers, a.k.a. system prompts. The ability to follow these roles and rules is essential for deployment, as it ensures that the model safely interacts with users within developer defined guidelines. To improve such role and rule following ability, we propose \model, an automated data generation pipeline that generates diverse roles and rules from existing IFT instructions, along with corresponding responses. This data can then be used to train models that follow complex system prompts. The models are evaluated on our newly created benchmarks for role and rule following ability, as well as standard instruction-following benchmarks and general NLP tasks. Our framework significantly improves role and rule following capability in LLMs, as evidenced by over 25% increase in pass-rate on rule adherence, i.e. following all requirements, in our experiments with the Alpaca and Ultrachat datasets. Moreover, our models achieves this increase without any regression on popular instruction following benchmarks.</li>
<li><strong>摘要：</strong>指令微调 (IFT) 通过监督学习引出指令遵循能力并控制大型语言模型 (LLM) 的行为。然而，在开源 IFT 数据集上训练的现有模型仅具有遵循用户指令的能力，并且经常无法遵循开发人员指定的复杂角色和规则，即系统提示。遵循这些角色和规则的能力对于部署至关重要，因为它可以确保模型在开发人员定义的准则内安全地与用户交互。为了提高这种角色和规则遵循能力，我们提出了 \model，这是一种自动数据生成管道，可从现有的 IFT 指令生成各种角色和规则以及相应的响应。然后可以使用这些数据来训练遵循复杂系统提示的模型。这些模型将根据我们新创建的角色和规则遵循能力基准以及标准指令遵循基准和一般 NLP 任务进行评估。我们的框架显著提高了 LLM 中的角色和规则遵循能力，在我们对 Alpaca 和 Ultrachat 数据集进行的实验中，规则遵循（即遵循所有要求）的通过率提高了 25% 以上。此外，我们的模型实现了这一提升，而没有在流行指令遵循基准上出现任何回归。</li>
</ul>

<h3>Title: Analysis of Socially Unacceptable Discourse with Zero-shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Rayane Ghilene, Dimitra Niaouri, Michele Linardi, Julien Longhi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13735">https://arxiv.org/abs/2409.13735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13735">https://arxiv.org/pdf/2409.13735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13735]] Analysis of Socially Unacceptable Discourse with Zero-shot Learning(https://arxiv.org/abs/2409.13735)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Socially Unacceptable Discourse (SUD) analysis is crucial for maintaining online positive environments. We investigate the effectiveness of Entailment-based zero-shot text classification (unsupervised method) for SUD detection and characterization by leveraging pre-trained transformer models and prompting techniques. The results demonstrate good generalization capabilities of these models to unseen data and highlight the promising nature of this approach for generating labeled datasets for the analysis and characterization of extremist narratives. The findings of this research contribute to the development of robust tools for studying SUD and promoting responsible communication online.</li>
<li><strong>摘要：</strong>社会不可接受的话语 (SUD) 分析对于维护积极的在线环境至关重要。我们利用预先训练的转换器模型和提示技术，研究了基于蕴涵的零样本文本分类（无监督方法）对 SUD 检测和表征的有效性。结果证明了这些模型对未见数据的良好泛化能力，并强调了这种方法在生成标记数据集以分析和表征极端主义叙事方面的前景。这项研究的结果有助于开发用于研究 SUD 和促进负责任的在线交流的强大工具。</li>
</ul>

<h3>Title: NLP4PBM: A Systematic Review on Process Extraction using Natural Language Processing with Rule-based, Machine and Deep Learning Methods</h3>
<ul>
<li><strong>Authors: </strong>William Van Woensel, Soroor Motie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13738">https://arxiv.org/abs/2409.13738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13738">https://arxiv.org/pdf/2409.13738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13738]] NLP4PBM: A Systematic Review on Process Extraction using Natural Language Processing with Rule-based, Machine and Deep Learning Methods(https://arxiv.org/abs/2409.13738)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>This literature review studies the field of automated process extraction, i.e., transforming textual descriptions into structured processes using Natural Language Processing (NLP). We found that Machine Learning (ML) / Deep Learning (DL) methods are being increasingly used for the NLP component. In some cases, they were chosen for their suitability towards process extraction, and results show that they can outperform classic rule-based methods. We also found a paucity of gold-standard, scalable annotated datasets, which currently hinders objective evaluations as well as the training or fine-tuning of ML / DL methods. Finally, we discuss preliminary work on the application of LLMs for automated process extraction, as well as promising developments in this field.</li>
<li><strong>摘要：</strong>本文献综述研究了自动流程提取领域，即使用自然语言处理 (NLP) 将文本描述转换为结构化流程。我们发现机器学习 (ML)/深度学习 (DL) 方法越来越多地用于 NLP 组件。在某些情况下，选择它们是因为它们适合流程提取，结果表明它们可以胜过经典的基于规则的方法。我们还发现缺乏黄金标准、可扩展的注释数据集，这目前阻碍了客观评估以及 ML/DL 方法的训练或微调。最后，我们讨论了 LLM 在自动流程提取中的应用的初步工作，以及该领域的有希望的发展。</li>
</ul>

<h3>Title: Language agents achieve superhuman synthesis of scientific knowledge</h3>
<ul>
<li><strong>Authors: </strong>Michael D. Skarlinski, Sam Cox, Jon M. Laurent, James D. Braza, Michaela Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, Andrew D. White</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13740">https://arxiv.org/abs/2409.13740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13740">https://arxiv.org/pdf/2409.13740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13740]] Language agents achieve superhuman synthesis of scientific knowledge(https://arxiv.org/abs/2409.13740)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Language models are known to produce incorrect information, and their accuracy and reliability for scientific research are still in question. We developed a detailed human-AI comparison method to evaluate language models on real-world literature search tasks, including information retrieval, summarization, and contradiction detection. Our findings show that PaperQA2, an advanced language model focused on improving factual accuracy, matches or outperforms subject matter experts on three realistic literature search tasks, with no restrictions on human participants (full internet access, search tools, and time). PaperQA2 generates cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than current human-written Wikipedia entries. We also present LitQA2, a new benchmark for scientific literature research, which shaped the development of PaperQA2 and contributed to its superior performance. Additionally, PaperQA2 identifies contradictions in scientific literature, a challenging task for humans. It finds an average of 2.34 +/- 1.99 contradictions per paper in a random sample of biology papers, with 70% of these contradictions validated by human experts. These results show that language models can now surpass domain experts in important scientific literature tasks.</li>
<li><strong>摘要：</strong>众所周知，语言模型会产生错误的信息，而且它们在科学研究中的准确性和可靠性仍存在疑问。我们开发了一种详细的人机对比方法来评估语言模型在现实世界文献搜索任务中的表现，包括信息检索、总结和矛盾检测。我们的研究结果表明，PaperQA2 是一种专注于提高事实准确性的高级语言模型，它在三个现实的文献搜索任务中的表现与主题专家相当甚至更好，而且对人类参与者没有任何限制（完全的互联网访问、搜索工具和时间）。PaperQA2 生成的科学主题的引用、维基百科风格的摘要比目前人类编写的维基百科条目准确得多。我们还提出了 LitQA2，这是科学文献研究的新基准，它塑造了 PaperQA2 的发展并为其卓越的性能做出了贡献。此外，PaperQA2 可以识别科学文献中的矛盾，这对人类来说是一项具有挑战性的任务。它在随机的生物学论文样本中平均每篇论文发现 2.34 +/- 1.99 个矛盾，其中 70% 的矛盾得到了人类专家的验证。这些结果表明，语言模型现在可以在重要的科学文献任务中超越领域专家。</li>
</ul>

<h3>Title: Knowing When to Ask -- Bridging Large Language Models and Data</h3>
<ul>
<li><strong>Authors: </strong>Prashanth Radhakrishnan, Jennifer Chen, Bo Xu, Prem Ramaswami, Hannah Pho, Adriana Olmos, James Manyika, R. V. Guha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13741">https://arxiv.org/abs/2409.13741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13741">https://arxiv.org/pdf/2409.13741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13741]] Knowing When to Ask -- Bridging Large Language Models and Data(https://arxiv.org/abs/2409.13741)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are prone to generating factually incorrect information when responding to queries that involve numerical and statistical data or other timely facts. In this paper, we present an approach for enhancing the accuracy of LLMs by integrating them with Data Commons, a vast, open-source repository of public statistics from trusted organizations like the United Nations (UN), Center for Disease Control and Prevention (CDC) and global census bureaus. We explore two primary methods: Retrieval Interleaved Generation (RIG), where the LLM is trained to produce natural language queries to retrieve data from Data Commons, and Retrieval Augmented Generation (RAG), where relevant data tables are fetched from Data Commons and used to augment the LLM's prompt. We evaluate these methods on a diverse set of queries, demonstrating their effectiveness in improving the factual accuracy of LLM outputs. Our work represents an early step towards building more trustworthy and reliable LLMs that are grounded in verifiable statistical data and capable of complex factual reasoning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在响应涉及数字和统计数据或其他及时事实的查询时容易生成事实错误的信息。在本文中，我们提出了一种提高 LLM 准确性的方法，即将 LLM 与 Data Commons 集成，Data Commons 是一个庞大的开源公共统计数据存储库，包含来自联合国 (UN)、疾病控制和预防中心 (CDC) 和全球人口普查局等可信组织的公共统计数据。我们探索了两种主要方法：检索交错生成 (RIG)，其中训练 LLM 生成自然语言查询以从 Data Commons 检索数据，以及检索增强生成 (RAG)，其中从 Data Commons 获取相关数据表并用于增强 LLM 的提示。我们在一组不同的查询上评估了这些方法，证明了它们在提高 LLM 输出的事实准确性方面的有效性。我们的工作代表着朝着构建更值得信赖和可靠的 LLM 迈出了第一步，这些 LLM 以可验证的统计数据为基础，能够进行复杂的事实推理。</li>
</ul>

<h3>Title: A Simplified Retriever to Improve Accuracy of Phenotype Normalizations by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel B. Hier, Thanh Son Do, Tayo Obafemi-Ajayi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13744">https://arxiv.org/abs/2409.13744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13744">https://arxiv.org/pdf/2409.13744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13744]] A Simplified Retriever to Improve Accuracy of Phenotype Normalizations by Large Language Models(https://arxiv.org/abs/2409.13744)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown improved accuracy in phenotype term normalization tasks when augmented with retrievers that suggest candidate normalizations based on term definitions. In this work, we introduce a simplified retriever that enhances LLM accuracy by searching the Human Phenotype Ontology (HPO) for candidate matches using contextual word embeddings from BioBERT without the need for explicit term definitions. Testing this method on terms derived from the clinical synopses of Online Mendelian Inheritance in Man (OMIM), we demonstrate that the normalization accuracy of a state-of-the-art LLM increases from a baseline of 62.3% without augmentation to 90.3% with retriever augmentation. This approach is potentially generalizable to other biomedical term normalization tasks and offers an efficient alternative to more complex retrieval methods.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在表型术语规范化任务中表现出更高的准确性，因为使用基于术语定义建议候选规范化的检索器可以增强这种准确性。在这项工作中，我们引入了一种简化的检索器，它通过使用来自 BioBERT 的上下文词嵌入在人类表型本体 (HPO) 中搜索候选匹配项，而无需明确的术语定义，从而提高了 LLM 的准确性。在从在线孟德尔遗传 (OMIM) 的临床概要中得出的术语上测试此方法后，我们证明了最先进的 LLM 的规范化准确性从没有增强的基线 62.3% 提高到有增强检索器的 90.3%。这种方法可能可以推广到其他生物医学术语规范化任务，并为更复杂的检索方法提供了一种有效的替代方案。</li>
</ul>

<h3>Title: Context-Aware Membership Inference Attacks against Pre-trained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyan Chang, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, Reza Shokri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13745">https://arxiv.org/abs/2409.13745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13745">https://arxiv.org/pdf/2409.13745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13745]] Context-Aware Membership Inference Attacks against Pre-trained Large Language Models(https://arxiv.org/abs/2409.13745)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Prior Membership Inference Attacks (MIAs) on pre-trained Large Language Models (LLMs), adapted from classification model attacks, fail due to ignoring the generative process of LLMs across token sequences. In this paper, we present a novel attack that adapts MIA statistical tests to the perplexity dynamics of subsequences within a data point. Our method significantly outperforms prior loss-based approaches, revealing context-dependent memorization patterns in pre-trained LLMs.</li>
<li><strong>摘要：</strong>先前成员推理攻击 (MIA) 针对预训练的大型语言模型 (LLM)，改编自分类模型攻击，由于忽略了 LLM 在标记序列中的生成过程而失败。在本文中，我们提出了一种新颖的攻击，该攻击将 MIA 统计测试调整为数据点内子序列的困惑度动态。我们的方法明显优于先前基于损失的方法，揭示了预训练 LLM 中依赖于上下文的记忆模式。</li>
</ul>

<h3>Title: When Less Is Not More: Large Language Models Normalize Less-Frequent Terms with Lower Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Daniel B. Hier, Thanh Son Do, Tayo Obafemi-Ajayi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13746">https://arxiv.org/abs/2409.13746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13746">https://arxiv.org/pdf/2409.13746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13746]] When Less Is Not More: Large Language Models Normalize Less-Frequent Terms with Lower Accuracy(https://arxiv.org/abs/2409.13746)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Term normalization is the process of mapping a term from free text to a standardized concept and its machine-readable code in an ontology. Accurate normalization of terms that capture phenotypic differences between patients and diseases is critical to the success of precision medicine initiatives. A large language model (LLM), such as GPT-4o, can normalize terms to the Human Phenotype Ontology (HPO), but it may retrieve incorrect HPO IDs. Reported accuracy rates for LLMs on these tasks may be inflated due to imbalanced test datasets skewed towards high-frequency terms. In our study, using a comprehensive dataset of 268,776 phenotype annotations for 12,655 diseases from the HPO, GPT-4o achieved an accuracy of 13.1% in normalizing 11,225 unique terms. However, the accuracy was unevenly distributed, with higher-frequency and shorter terms normalized more accurately than lower-frequency and longer terms. Feature importance analysis, using SHAP and permutation methods, identified low-term frequency as the most significant predictor of normalization errors. These findings suggest that training and evaluation datasets for LLM-based term normalization should balance low- and high-frequency terms to improve model performance, particularly for infrequent terms critical to precision medicine.</li>
<li><strong>摘要：</strong>术语规范化是将术语从自由文本映射到本体中的标准化概念及其机器可读代码的过程。准确规范化术语以捕捉患者和疾病之间的表型差异对于精准医疗计划的成功至关重要。大型语言模型 (LLM)（例如 GPT-4o）可以将术语规范化为人类表型本体 (HPO)，但它可能会检索到错误的 HPO ID。由于不平衡的测试数据集偏向高频术语，这些任务中 LLM 的报告准确率可能会被夸大。在我们的研究中，使用来自 HPO 的 12,655 种疾病的 268,776 个表型注释的综合数据集，GPT-4o 在规范化 11,225 个唯一术语时实现了 13.1% 的准确率。然而，准确度分布不均，高频和较短术语的规范化准确度高于低频和较长术语。使用 SHAP 和置换方法进行的特征重要性分析确定了低词频是归一化误差最重要的预测因素。这些发现表明，基于 LLM 的词归一化的训练和评估数据集应平衡低频词和高频词，以提高模型性能，尤其是对于精准医疗至关重要的不常见词。</li>
</ul>

<h3>Title: Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder</h3>
<ul>
<li><strong>Authors: </strong>Abhinav P.M., SujayKumar Reddy M, Oswald Christopher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13747">https://arxiv.org/abs/2409.13747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13747">https://arxiv.org/pdf/2409.13747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13747]] Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder(https://arxiv.org/abs/2409.13747)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This project, titled "Machine Translation with Large Language Models: Decoder-only vs. Encoder-Decoder," aims to develop a multilingual machine translation (MT) model. Focused on Indian regional languages, especially Telugu, Tamil, and Malayalam, the model seeks to enable accurate and contextually appropriate translations across diverse language pairs. By comparing Decoder-only and Encoder-Decoder architectures, the project aims to optimize translation quality and efficiency, advancing cross-linguistic communication tools.The primary objective is to develop a model capable of delivering high-quality translations that are accurate and contextually appropriate. By leveraging large language models, specifically comparing the effectiveness of Decoder-only and Encoder-Decoder architectures, the project seeks to optimize translation performance and efficiency across multilingual contexts. Through rigorous experimentation and analysis, this project aims to advance the field of machine translation, contributing valuable insights into the effectiveness of different model architectures and paving the way for enhanced cross-linguistic communication tools.</li>
<li><strong>摘要：</strong>该项目名为“大型语言模型的机器翻译：仅解码器与编码器-解码器”，旨在开发多语言机器翻译 (MT) 模型。该模型专注于印度地区语言，尤其是泰卢固语、泰米尔语和马拉雅拉姆语，旨在实现跨不同语言对的准确且符合语境的翻译。通过比较仅解码器和编码器-解码器架构，该项目旨在优化翻译质量和效率，推动跨语言交流工具的发展。主要目标是开发一种能够提供准确且符合语境的高质量翻译的模型。通过利用大型语言模型，特别是比较仅解码器和编码器-解码器架构的有效性，该项目旨在优化多语言环境中的翻译性能和效率。通过严格的实验和分析，该项目旨在推动机器翻译领域的发展，为不同模型架构的有效性提供宝贵见解，并为增强跨语言交流工具铺平道路。</li>
</ul>

<h3>Title: TheraGen: Therapy for Every Generation</h3>
<ul>
<li><strong>Authors: </strong>Kartikey Doshi, Jimit Shah, Narendra Shekokar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13748">https://arxiv.org/abs/2409.13748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13748">https://arxiv.org/pdf/2409.13748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13748]] TheraGen: Therapy for Every Generation(https://arxiv.org/abs/2409.13748)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>We present TheraGen, an advanced AI-powered mental health chatbot utilizing the LLaMA 2 7B model. This approach builds upon recent advancements in language models and transformer architectures. TheraGen provides all-day personalized, compassionate mental health care by leveraging a large dataset of 1 million conversational entries, combining anonymized therapy transcripts, online mental health discussions, and psychological literature, including APA resources. Our implementation employs transfer learning, fine-tuning, and advanced training techniques to optimize performance. TheraGen offers a user-friendly interface for seamless interaction, providing empathetic responses and evidence-based coping strategies. Evaluation results demonstrate high user satisfaction rates, with 94% of users reporting improved mental well-being. The system achieved a BLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response accuracy. With an average response time of 1395 milliseconds, TheraGen ensures real-time, efficient support. While not a replacement for professional therapy, TheraGen serves as a valuable complementary tool, significantly improving user well-being and addressing the accessibility gap in mental health treatments. This paper details TheraGen's architecture, training methodology, ethical considerations, and future directions, contributing to the growing field of AI-assisted mental healthcare and offering a scalable solution to the pressing need for mental health support.</li>
<li><strong>摘要：</strong>我们推出了 TheraGen，这是一款采用 LLaMA 2 7B 模型的先进 AI 驱动心理健康聊天机器人。这种方法建立在语言模型和变压器架构的最新进展之上。TheraGen 利用 100 万个对话条目的大型数据集，结合匿名治疗记录、在线心理健康讨论和心理文献（包括 APA 资源），提供全天候个性化、富有同情心的心理健康护理。我们的实施采用迁移学习、微调和高级训练技术来优化性能。TheraGen 提供用户友好的界面以实现无缝交互，提供富有同理心的响应和基于证据的应对策略。评估结果显示用户满意度很高，94% 的用户报告心理健康有所改善。该系统的 BLEU 得分为 0.67，ROUGE 得分为 0.62，表明响应准确度很高。TheraGen 的平均响应时间为 1395 毫秒，可确保实时、高效的支持。 TheraGen 虽然不能替代专业治疗，但它是一种有价值的补充工具，可显著改善用户的健康状况并解决心理健康治疗的可及性差距。本文详细介绍了 TheraGen 的架构、培训方法、道德考量和未来发展方向，为不断发展的 AI 辅助心理健康保健领域做出了贡献，并为迫切的心理健康支持需求提供了可扩展的解决方案。</li>
</ul>

<h3>Title: KodeXv0.1: A Family of State-of-the-Art Financial Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Neel Rajani, Lilli Kiessling, Aleksandr Ogaltsov, Claus Lang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13749">https://arxiv.org/abs/2409.13749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13749">https://arxiv.org/pdf/2409.13749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13749]] KodeXv0.1: A Family of State-of-the-Art Financial Large Language Models(https://arxiv.org/abs/2409.13749)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Although powerful, current cutting-edge LLMs may not fulfil the needs of highly specialised sectors. We introduce KodeXv0.1, a family of large language models that outclass GPT-4 in financial question answering. We utilise the base variants of Llama 3.1 8B and 70B and adapt them to the financial domain through a custom training regime. To this end, we collect and process a large number of publicly available financial documents such as earnings calls and business reports. These are used to generate a high-quality, synthetic dataset consisting of Context-Question-Answer triplets which closely mirror real-world financial tasks. Using the train split of this dataset, we perform RAG-aware 4bit LoRA instruction tuning runs of Llama 3.1 base variants to produce KodeX-8Bv0.1 and KodeX-70Bv0.1. We then complete extensive model evaluations using FinanceBench, FinQABench and the withheld test split of our dataset. Our results show that KodeX-8Bv0.1 is more reliable in financial contexts than cutting-edge instruct models in the same parameter regime, surpassing them by up to 9.24%. In addition, it is even capable of outperforming state-of-the-art proprietary models such as GPT-4 by up to 7.07%. KodeX-70Bv0.1 represents a further improvement upon this, exceeding GPT-4's performance on every tested benchmark.</li>
<li><strong>摘要：</strong>尽管功能强大，但当前最先进的 LLM 可能无法满足高度专业化行业的需求。我们推出了 KodeXv0.1，这是一系列大型语言模型，在金融问答方面胜过 GPT-4。我们利用 Llama 3.1 8B 和 70B 的基本变体，并通过自定义训练机制使其适应金融领域。为此，我们收集和处理大量公开的财务文件，例如收益电话会议和业务报告。这些文件用于生成由上下文-问题-答案三元组组成的高质量合成数据集，这些三元组与现实世界的财务任务非常相似。使用此数据集的训练分割，我们对 Llama 3.1 基本变体执行 RAG 感知 4 位 LoRA 指令调整运行，以生成 KodeX-8Bv0.1 和 KodeX-70Bv0.1。然后，我们使用 FinanceBench、FinQABench 和我们数据集的保留测试分割完成广泛的模型评估。我们的结果表明，在金融环境下，KodeX-8Bv0.1 的可靠性比相同参数范围内的前沿指令模型高出 9.24%。此外，它甚至能够比 GPT-4 等最先进的专有模型高出 7.07%。KodeX-70Bv0.1 在此基础上进一步改进，在每个测试基准上都超过了 GPT-4 的性能。</li>
</ul>

<h3>Title: Thinking Before Speaking: A Role-playing Model with Mindset</h3>
<ul>
<li><strong>Authors: </strong>Baohua Zhang, Yongyi Huang, Wenyao Cui, Huaping Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13752">https://arxiv.org/abs/2409.13752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13752">https://arxiv.org/pdf/2409.13752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13752]] Thinking Before Speaking: A Role-playing Model with Mindset(https://arxiv.org/abs/2409.13752)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Role-playing is an easy task for Large Language Models (LLMs), as they are skilled at simulating human behaviors. Many current studies have enabled LLMs to generate responses in the tone of a specific role by fine-tuning the models or using specialized prompts. However, it is typically easy to recognize when a role is being played by LLMs. These models tend to perform poorly when confronted with knowledge that the assumed role does not possess, or a question that requires the specific experience or logic of the role to answer. To address this problem and make LLMs act more like real roles, we propose a Thinking Before Speaking (TBS) model in this paper. Unlike other studies, we first extend the data based on the character's real-life scenarios and the historical dialogue, supplementing each pair of dialogue with the character's mindset. Then we add few data points that include elements beyond the role's knowledge, and fine-tune the LLMs. This approach can help LLMs adopt the role's thought process and logic, avoiding responses that fall outside the role's knowledge base. We have also prepared a dataset and evaluation metrics to test these capabilities. Experimental results show that our TBS model can better emulate a role in terms of tone, knowledge, and mindset.</li>
<li><strong>摘要：</strong>角色扮演对于大型语言模型 (LLM) 来说是一项简单的任务，因为它们擅长模拟人类行为。许多当前的研究已经使 LLM 能够通过微调模型或使用专门的提示来生成具有特定角色语气的响应。然而，通常很容易识别 LLM 正在扮演的角色。当面对所扮演角色不具备的知识，或者需要角色的特定经验或逻辑来回答的问题时，这些模型往往表现不佳。为了解决这个问题并使 LLM 更像真实的角色，我们在本文中提出了一个三思而后行 (TBS) 模型。与其他研究不同，我们首先根据角色​​的真实生活场景和历史对话扩展数据，用角色的心态补充每对对话。然后我们添加一些包含角色知识之外元素的数据点，并微调 LLM。这种方法可以帮助 LLM 采用角色的思维过程和逻辑，避免超出角色知识库的响应。我们还准备了数据集和评估指标来测试这些功能。实验结果表明，我们的 TBS 模型在语气、知识和心态方面可以更好地模拟角色。</li>
</ul>

<h3>Title: Language Models Learn Metadata: Political Stance Detection Case Study</h3>
<ul>
<li><strong>Authors: </strong>Stanley Cao, Felix Drinkall</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13756">https://arxiv.org/abs/2409.13756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13756">https://arxiv.org/pdf/2409.13756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13756]] Language Models Learn Metadata: Political Stance Detection Case Study(https://arxiv.org/abs/2409.13756)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Stance detection is a crucial NLP task with numerous applications in social science, from analyzing online discussions to assessing political campaigns. This paper investigates the optimal way to incorporate metadata into a political stance detection task. We demonstrate that previous methods combining metadata with language-based data for political stance detection have not fully utilized the metadata information; our simple baseline, using only party membership information, surpasses the current state-of-the-art. We then show that prepending metadata (e.g., party and policy) to political speeches performs best, outperforming all baselines, indicating that complex metadata inclusion systems may not learn the task optimally.</li>
<li><strong>摘要：</strong>立场检测是一项至关重要的 NLP 任务，在社会科学中有着广泛的应用，从分析在线讨论到评估政治运动。本文探讨了将元数据纳入政治立场检测任务的最佳方式。我们表明，以前将元数据与基于语言的数据相结合进行政治立场检测的方法并没有充分利用元数据信息；我们仅使用党派成员信息的简单基线超越了当前最先进的技术。然后，我们表明，在政治演讲前添加元数据（例如，政党和政策）效果最好，优于所有基线，这表明复杂的元数据包含系统可能无法最佳地学习该任务。</li>
</ul>

<h3>Title: Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with Selective Cloud Assistance</h3>
<ul>
<li><strong>Authors: </strong>Adarsh MS, Jithin VG, Ditto PS</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13757">https://arxiv.org/abs/2409.13757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13757">https://arxiv.org/pdf/2409.13757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13757]] Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with Selective Cloud Assistance(https://arxiv.org/abs/2409.13757)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known for their exceptional performance across a range of natural language processing tasks, but their deployment comes at a high computational and financial cost. On the other hand, smaller language models (SLMs), which can be deployed on lower-cost edge devices, struggle to match the performance of their larger counterparts. This paper presents a novel hybrid inference approach that leverages the strengths of both model types while minimizing reliance on costly cloud-based LLMs. Unlike existing methods that route entire queries to either an SLM or a cloud LLM, our approach introduces a reward-based mechanism to dynamically determine the involvement of the cloud LLM during token generation. Specifically, each token predicted by the SLM is evaluated against a reward score, and only when this score falls below a certain threshold is the cloud LLM consulted for assistance in the next token prediction. This method not only reduces the traffic to the cloud LLM, thereby lowering costs, but also allows for flexible control over response quality depending on the reward score threshold. Experimental results demonstrate that our approach significantly reduces cloud LLM usage with minimal impact on overall response quality, offering a cost-effective solution for deploying high-performance language models</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其在一系列自然语言处理任务中的出色表现而闻名，但其部署需要很高的计算和财务成本。另一方面，可以部署在低成本边缘设备上的小型语言模型 (SLM) 难以与大型模型的性能相匹配。本文介绍了一种新颖的混合推理方法，该方法充分利用了两种模型类型的优势，同时最大限度地减少了对昂贵的基于云的 LLM 的依赖。与将整个查询路由到 SLM 或云 LLM 的现有方法不同，我们的方法引入了一种基于奖励的机制，以动态确定云 LLM 在令牌生成过程中的参与程度。具体而言，SLM 预测的每个令牌都会根据奖励分数进行评估，只有当该分数低于某个阈值时，才会咨询云 LLM 以协助进行下一个令牌预测。这种方法不仅可以减少到云 LLM 的流量，从而降低成本，而且还可以根据奖励分数阈值灵活控制响应质量。实验结果表明，我们的方法显著减少了云 LLM 的使用量，同时对整体响应质量的影响极小，为部署高性能语言模型提供了一种经济高效的解决方案</li>
</ul>

<h3>Title: Do Large Language Models Need a Content Delivery Network?</h3>
<ul>
<li><strong>Authors: </strong>Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13761">https://arxiv.org/abs/2409.13761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13761">https://arxiv.org/pdf/2409.13761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13761]] Do Large Language Models Need a Content Delivery Network?(https://arxiv.org/abs/2409.13761)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling flexible and efficient injection of new knowledge in LLM inference is critical. Three high-level options exist: (i) embedding the knowledge in LLM's weights (i.e., fine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e., in-context learning), or (iii) injecting the KV caches of the new knowledge to LLM during prefill. This paper argues that, although fine-tuning and in-context learning are popular, using KV caches as the medium of knowledge could simultaneously enable more modular management of knowledge injection and more efficient LLM serving with low cost and fast response. To realize these benefits, we envision a Knowledge Delivery Network (KDN), a new system component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. We believe that, just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. We have open-sourced a KDN prototype at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的使用范围迅速扩大，补充各种 LLM 查询所需的知识范围也在迅速扩大。因此，在 LLM 推理中灵活高效地注入新知识至关重要。存在三种高级选项：(i) 将知识嵌入 LLM 的权重中（即微调），(ii) 将知识作为 LLM 文本输入的一部分（即上下文学习），或 (iii) 在预填充期间将新知识的 KV 缓存注入 LLM。本文认为，尽管微调和上下文学习很流行，但使用 KV 缓存作为知识媒介可以同时实现知识注入的更模块化管理和更高效的 LLM 服务，同时降低成本并加快响应速度。为了实现这些好处，我们设想了一个知识交付网络 (KDN)，这是 LLM 服务中的一个新的系统组件，可动态优化 LLM 引擎和其他计算和存储资源之间的 KV 缓存的存储、传输和组合。我们相信，就像 Akamai 等内容交付网络 (CDN) 通过高效的数据交付推动了互联网生态系统的成功一样，KDN 也将通过高效的知识交付对 LLM 应用程序的成功至关重要。我们已在此 https URL 上开源了一个 KDN 原型。</li>
</ul>

<h3>Title: Local Explanations and Self-Explanations for Assessing Faithfulness in black-box LLMs</h3>
<ul>
<li><strong>Authors: </strong>Christos Fragkathoulas, Odysseas S. Chlapanis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13764">https://arxiv.org/abs/2409.13764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13764">https://arxiv.org/pdf/2409.13764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13764]] Local Explanations and Self-Explanations for Assessing Faithfulness in black-box LLMs(https://arxiv.org/abs/2409.13764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel task to assess the faithfulness of large language models (LLMs) using local perturbations and self-explanations. Many LLMs often require additional context to answer certain questions correctly. For this purpose, we propose a new efficient alternative explainability technique, inspired by the commonly used leave-one-out approach. Using this approach, we identify the sufficient and necessary parts for the LLM to generate correct answers, serving as explanations. We propose a metric for assessing faithfulness that compares these crucial parts with the self-explanations of the model. Using the Natural Questions dataset, we validate our approach, demonstrating its effectiveness in explaining model decisions and assessing faithfulness.</li>
<li><strong>摘要：</strong>本文介绍了一种新颖的任务，即使用局部扰动和自我解释来评估大型语言模型 (LLM) 的忠实度。许多 LLM 通常需要额外的背景才能正确回答某些问题。为此，我们提出了一种新的高效替代可解释性技术，该技术受到常用的留一法的启发。使用这种方法，我们可以确定 LLM 生成正确答案的充分和必要部分，作为解释。我们提出了一个评估忠实度的指标，将这些关键部分与模型的自我解释进行比较。使用 Natural Questions 数据集，我们验证了我们的方法，证明了它在解释模型决策和评估忠实度方面的有效性。</li>
</ul>

<h3>Title: Measuring Copyright Risks of Large Language Model via Partial Information Probing</h3>
<ul>
<li><strong>Authors: </strong>Weijie Zhao, Huajie Shao, Zhaozhuo Xu, Suzhen Duan, Denghui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13831">https://arxiv.org/abs/2409.13831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13831">https://arxiv.org/pdf/2409.13831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13831]] Measuring Copyright Risks of Large Language Model via Partial Information Probing(https://arxiv.org/abs/2409.13831)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Exploring the data sources used to train Large Language Models (LLMs) is a crucial direction in investigating potential copyright infringement by these models. While this approach can identify the possible use of copyrighted materials in training data, it does not directly measure infringing risks. Recent research has shifted towards testing whether LLMs can directly output copyrighted content. Addressing this direction, we investigate and assess LLMs' capacity to generate infringing content by providing them with partial information from copyrighted materials, and try to use iterative prompting to get LLMs to generate more infringing content. Specifically, we input a portion of a copyrighted text into LLMs, prompt them to complete it, and then analyze the overlap between the generated content and the original copyrighted material. Our findings demonstrate that LLMs can indeed generate content highly overlapping with copyrighted materials based on these partial inputs.</li>
<li><strong>摘要：</strong>探索用于训练大型语言模型 (LLM) 的数据来源是调查这些模型是否存在版权侵权行为的一个重要方向。虽然这种方法可以识别训练数据中可能使用的版权材料，但并不能直接衡量侵权风险。最近的研究转向测试 LLM 是否可以直接输出版权内容。针对这个方向，我们通过向 LLM 提供版权材料的部分信息来调查和评估 LLM 生成侵权内容的能力，并尝试使用迭代提示让 LLM 生成更多的侵权内容。具体来说，我们将一部分版权文本输入 LLM，提示它们完成，然后分析生成的内容与原始版权材料的重叠程度。我们的研究结果表明，LLM 确实可以根据这些部分输入生成与版权材料高度重叠的内容。</li>
</ul>

<h3>Title: STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions</h3>
<ul>
<li><strong>Authors: </strong>Robert Morabito, Sangmitra Madhusudan, Tyler McDonald, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13843">https://arxiv.org/abs/2409.13843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13843">https://arxiv.org/pdf/2409.13843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13843]] STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions(https://arxiv.org/abs/2409.13843)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Mitigating explicit and implicit biases in Large Language Models (LLMs) has become a critical focus in the field of natural language processing. However, many current methodologies evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. To address this, we introduce the Sensitivity Testing on Offensive Progressions (STOP) dataset, which includes 450 offensive progressions containing 2,700 unique sentences of varying severity that progressively escalate from less to more explicitly offensive. Covering a broad spectrum of 9 demographics and 46 sub-demographics, STOP ensures inclusivity and comprehensive coverage. We evaluate several leading closed- and open-source models, including GPT-4, Mixtral, and Llama 3. Our findings reveal that even the best-performing models detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We also demonstrate how aligning models with human judgments on STOP can improve model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs by up to 191%, while maintaining or even improving performance. STOP presents a novel framework for assessing the complex nature of biases in LLMs, which will enable more effective bias mitigation strategies and facilitates the creation of fairer language models.</li>
<li><strong>摘要：</strong>减轻大型语言模型 (LLM) 中的显性和隐性偏见已成为自然语言处理领域的一个关键重点。然而，许多当前的方法都是孤立地评估场景，而不考虑更广泛的背景或每种情况下的潜在偏见范围。为了解决这个问题，我们引入了对攻击性进展的敏感性测试 (STOP) 数据集，其中包括 450 个攻击性进展，其中包含 2,700 个不同严重程度的独特句子，这些句子的攻击性从较轻的攻击性逐渐升级到较明显的攻击性。STOP 涵盖了 9 个人口统计数据和 46 个子人口统计数据，确保了包容性和全面覆盖。我们评估了几个领先的闭源和开源模型，包括 GPT-4、Mixtral 和 Llama 3。我们的研究结果表明，即使是表现最好的模型检测偏见也不一致，成功率从 19.3% 到 69.8% 不等。我们还展示了如何将模型与 STOP 上的人类判断相结合，从而将 BBQ、StereoSet 和 CrowS-Pairs 等敏感任务的模型回答率提高高达 191%，同时保持甚至提高性能。STOP 提出了一个用于评估 LLM 中偏见复杂性质的新框架，这将实现更有效的偏见缓解策略，并促进创建更公平的语言模型。</li>
</ul>

<h3>Title: Do language models practice what they preach? Examining language ideologies about gendered language reform encoded in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Julia Watson, Sophia Lee, Barend Beekhuizen, Suzanne Stevenson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13852">https://arxiv.org/abs/2409.13852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13852">https://arxiv.org/pdf/2409.13852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13852]] Do language models practice what they preach? Examining language ideologies about gendered language reform encoded in LLMs(https://arxiv.org/abs/2409.13852)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We study language ideologies in text produced by LLMs through a case study on English gendered language reform (related to role nouns like congressperson/-woman/-man, and singular they). First, we find political bias: when asked to use language that is "correct" or "natural", LLMs use language most similarly to when asked to align with conservative (vs. progressive) values. This shows how LLMs' metalinguistic preferences can implicitly communicate the language ideologies of a particular political group, even in seemingly non-political contexts. Second, we find LLMs exhibit internal inconsistency: LLMs use gender-neutral variants more often when more explicit metalinguistic context is provided. This shows how the language ideologies expressed in text produced by LLMs can vary, which may be unexpected to users. We discuss the broader implications of these findings for value alignment.</li>
<li><strong>摘要：</strong>我们通过英语性别语言改革案例研究（与角色名词如国会议员/-woman/-man 和单数 they 相关）来研究 LLM 所写文本中的语言意识形态。首先，我们发现政治偏见：当被要求使用“正确”或“自然”的语言时，LLM 所用的语言与被要求与保守（而非进步）价值观保持一致时使用的语言最为相似。这表明 LLM 的元语言偏好可以隐含地传达特定政治群体的语言意识形态，即使在看似非政治的背景下也是如此。其次，我们发现 LLM 表现出内部不一致：当提供更明确的元语言背景时，LLM 会更频繁地使用性别中立的变体。这表明 LLM 所写文本中表达的语言意识形态可能会有所不同，这可能会让用户感到意外。我们讨论了这些发现对价值观一致的更广泛影响。</li>
</ul>

<h3>Title: Unlocking Memorization in Large Language Models with Dynamic Soft Prompting</h3>
<ul>
<li><strong>Authors: </strong>Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, Yanfu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13853">https://arxiv.org/abs/2409.13853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13853">https://arxiv.org/pdf/2409.13853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13853]] Unlocking Memorization in Large Language Models with Dynamic Soft Prompting(https://arxiv.org/abs/2409.13853)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize training data, leading to potential privacy breaches and copyright infringement. Accurate measurement of this memorization is essential to evaluate and mitigate these potential risks. However, previous attempts to characterize memorization are constrained by either using prefixes only or by prepending a constant soft prompt to the prefixes, which cannot react to changes in input. To address this challenge, we propose a novel method for estimating LLM memorization using dynamic, prefix-dependent soft prompts. Our approach involves training a transformer-based generator to produce soft prompts that adapt to changes in input, thereby enabling more accurate extraction of memorized data. Our method not only addresses the limitations of previous methods but also demonstrates superior performance in diverse experimental settings compared to state-of-the-art techniques. In particular, our method can achieve the maximum relative improvement of 112.75% and 32.26% over the vanilla baseline in terms of discoverable memorization rate for the text generation task and code generation task respectively.</li>
<li><strong>摘要：</strong>预训练大型语言模型 (LLM) 彻底改变了自然语言处理 (NLP) 任务，例如摘要、问答和翻译。然而，由于 LLM 倾向于记忆训练数据，因此存在重大安全风险，从而可能导致隐私泄露和版权侵权。准确测量这种记忆对于评估和减轻这些潜在风险至关重要。然而，以前表征记忆的尝试受到限制，要么只使用前缀，要么在前缀前添加一个恒定的软提示，而这些提示无法对输入的变化做出反应。为了应对这一挑战，我们提出了一种使用动态、前缀相关的软提示来估计 LLM 记忆的新方法。我们的方法包括训练基于变压器的生成器以产生适应输入变化的软提示，从而能够更准确地提取记忆数据。我们的方法不仅解决了以前方法的局限性，而且与最先进的技术相比，在各种实验环境中都表现出卓越的性能。具体来说，我们的方法在文本生成任务和代码生成任务的可发现记忆率方面分别比 vanilla 基线实现 112.75% 和 32.26% 的最大相对改进。</li>
</ul>

<h3>Title: Instruct-Tuning Pretrained Causal Language Models for Ancient Greek Papyrology and Epigraphy</h3>
<ul>
<li><strong>Authors: </strong>Eric Cullhed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13870">https://arxiv.org/abs/2409.13870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13870">https://arxiv.org/pdf/2409.13870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13870]] Instruct-Tuning Pretrained Causal Language Models for Ancient Greek Papyrology and Epigraphy(https://arxiv.org/abs/2409.13870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>This article presents an experiment in fine-tuning a pretrained causal language model (Meta's Llama 3.1 8B Instruct) for aiding in three fundamental tasks of philological research: chronological and geographic attribution as well as text restoration in ancient Greek inscriptions and documentary papyri. Using a prompt-based instruct approach, the fine-tuned models surpass the state of the art in key metrics. For inscriptions, the models achieve a lower average character error rate (CER) of 22.5% (vs. 26.3%), while closely matching top-1 accuracy (60.9% vs. 61.8%) and top-20 accuracy (77.5% vs. 78.3%) for sequences up to 10 characters. They also provide a practical advantage by ignoring spaces during reconstruction, aligning better with the scriptio continua typically used in ancient written artifacts. In geographic attribution, the model outperforms previous benchmarks with a top-1 accuracy of 75.0% (vs. 70.8%) and a top-3 accuracy of 83.7% (vs. 82.1%). For dating, it achieves an average deviation of 26.2 years (vs. 29.3) and a median deviation of 1 year (vs. 3) from the actual date range. The models also set new baselines for documentary papyri, with a CER of 16.3%, a top-1 accuracy of 71.3%, and top-20 of 85.0% in text reconstruction; a top-1 accuracy of 66.4% and top-3 of 79.9% in geographic attribution; and, in chronological attribution, a deviation of 21.7 years from the actual termini post/ante quem, with a median deviation of 0 years.</li>
<li><strong>摘要：</strong>本文介绍了一项实验，对预训练的因果语言模型 (Meta 的 Llama 3.1 8B Instruct) 进行微调，以协助完成语言学研究的三项基本任务：年代和地理归属以及古希腊铭文和文献纸莎草纸中的文本恢复。使用基于提示的指示方法，经过微调的模型在关键指标上超越了最先进的技术。对于铭文，这些模型实现了较低的平均字符错误率 (CER) 22.5%（vs. 26.3%），同时对于最多 10 个字符的序列，top-1 准确率（60.9% vs. 61.8%）和 top-20 准确率（77.5% vs. 78.3%）非常接近。它们还通过在重建过程中忽略空格来提供实际优势，更好地与古代书面文物中通常使用的 scriptio continua 保持一致。在地理归因方面，该模型的表现优于之前的基准，其 top-1 准确率为 75.0%（对比 70.8%），top-3 准确率为 83.7%（对比 82.1%）。对于年代测定，它与实际日期范围的平均偏差为 26.2 年（对比 29.3 年），中位偏差为 1 年（对比 3 年）。这些模型还为文献纸莎草纸设定了新的基准，文本重建的 CER 为 16.3%，top-1 准确率为 71.3%，top-20 准确率为 85.0%；地理归因的 top-1 准确率为 66.4%，top-3 准确率为 79.9%；在时间归因方面，与实际事后/事前终点的偏差为 21.7 年，中位偏差为 0 年。</li>
</ul>

<h3>Title: "I Never Said That": A dataset, taxonomy and baselines on response clarity classification</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Thomas, Giorgos Filandrianos, Maria Lymperaiou, Chrysoula Zerva, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13879">https://arxiv.org/abs/2409.13879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13879">https://arxiv.org/pdf/2409.13879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13879]] "I Never Said That": A dataset, taxonomy and baselines on response clarity classification(https://arxiv.org/abs/2409.13879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Equivocation and ambiguity in public speech are well-studied discourse phenomena, especially in political science and analysis of political interviews. Inspired by the well-grounded theory on equivocation, we aim to resolve the closely related problem of response clarity in questions extracted from political interviews, leveraging the capabilities of Large Language Models (LLMs) and human expertise. To this end, we introduce a novel taxonomy that frames the task of detecting and classifying response clarity and a corresponding clarity classification dataset which consists of question-answer (QA) pairs drawn from political interviews and annotated accordingly. Our proposed two-level taxonomy addresses the clarity of a response in terms of the information provided for a given question (high-level) and also provides a fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous responses (lower-level). We combine ChatGPT and human annotators to collect, validate and annotate discrete QA pairs from political interviews, to be used for our newly introduced response clarity task. We provide a detailed analysis and conduct several experiments with different model architectures, sizes and adaptation methods to gain insights and establish new baselines over the proposed dataset and task.</li>
<li><strong>摘要：</strong>公共演讲中的模棱两可和歧义是研究得很好的话语现象，尤其是在政治学和政治访谈分析中。受模棱两可理论的启发，我们旨在利用大型语言模型 (LLM) 和人类专业知识的能力，解决从政治访谈中提取的问题中密切相关的回答清晰度问题。为此，我们引入了一种新的分类法，该分类法构成了检测和分类回答清晰度的任务，以及相应的清晰度分类数据集，该数据集由从政治访谈中提取并进行相应注释的问答 (QA) 对组成。我们提出的两级分类法根据为给定问题提供的信息 (高级) 来解决回答的清晰度问题，还提供了与不清楚、模棱两可的回答 (低级) 相关的逃避技术的细粒度分类法。我们将 ChatGPT 和人类注释者结合起来，从政治访谈中收集、验证和注释离散的 QA 对，以用于我们新引入的回答清晰度任务。我们提供了详细的分析，并使用不同的模型架构、大小和适应方法进行了几项实验，以获得见解并为所提出的数据集和任务建立新的基线。</li>
</ul>

<h3>Title: A Multi-LLM Debiasing Framework</h3>
<ul>
<li><strong>Authors: </strong>Deonna M. Owens, Ryan A. Rossi, Sungchul Kim, Tong Yu, Franck Dernoncourt, Xiang Chen, Ruiyi Zhang, Jiuxiang Gu, Hanieh Deilamsalehy, Nedim Lipka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13884">https://arxiv.org/abs/2409.13884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13884">https://arxiv.org/pdf/2409.13884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13884]] A Multi-LLM Debiasing Framework(https://arxiv.org/abs/2409.13884)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are powerful tools with the potential to benefit society immensely, yet, they have demonstrated biases that perpetuate societal inequalities. Despite significant advancements in bias mitigation techniques using data augmentation, zero-shot prompting, and model fine-tuning, biases continuously persist, including subtle biases that may elude human detection. Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs. Building on this approach, we propose a novel multi-LLM debiasing framework aimed at reducing bias in LLMs. Our work is the first to introduce and evaluate two distinct approaches within this framework for debiasing LLMs: a centralized method, where the conversation is facilitated by a single central LLM, and a decentralized method, where all models communicate directly. Our findings reveal that our multi-LLM framework significantly reduces bias in LLMs, outperforming the baseline method across several social groups.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是一种强大的工具，有可能极大地造福社会，然而，它们也表现出了加剧社会不平等的偏见。尽管使用数据增强、零样本提示和模型微调的偏见缓解技术取得了重大进展，但偏见仍然存在，包括可能逃避人类检测的细微偏见。最近的研究表明，人们对多 LLM 方法的兴趣日益浓厚，这些方法已被证明可以有效提高 LLM 中的推理质量和事实性。基于这种方法，我们提出了一种新颖的多 LLM 去偏框架，旨在减少 LLM 中的偏见。我们的工作是首次在此框架内引入和评估两种不同的 LLM 去偏方法：一种是集中式方法，其中对话由单个中央 LLM 促进；另一种是分散式方法，其中所有模型都直接通信。我们的研究结果表明，我们的多 LLM 框架显着减少了 LLM 中的偏见，在多个社会群体中的表现优于基线方法。</li>
</ul>

<h3>Title: Transfer Learning with Clinical Concept Embeddings from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuhe Gao, Runxue Bao, Yuelyu Ji, Yiming Sun, Chenxi Song, Jeffrey P. Ferraro, Ye Ye</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13893">https://arxiv.org/abs/2409.13893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13893">https://arxiv.org/pdf/2409.13893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13893]] Transfer Learning with Clinical Concept Embeddings from Large Language Models(https://arxiv.org/abs/2409.13893)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge sharing is crucial in healthcare, especially when leveraging data from multiple clinical sites to address data scarcity, reduce costs, and enable timely interventions. Transfer learning can facilitate cross-site knowledge transfer, but a major challenge is heterogeneity in clinical concepts across different sites. Large Language Models (LLMs) show significant potential of capturing the semantic meaning of clinical concepts and reducing heterogeneity. This study analyzed electronic health records from two large healthcare systems to assess the impact of semantic embeddings from LLMs on local, shared, and transfer learning models. Results indicate that domain-specific LLMs, such as Med-BERT, consistently outperform in local and direct transfer scenarios, while generic models like OpenAI embeddings require fine-tuning for optimal performance. However, excessive tuning of models with biomedical embeddings may reduce effectiveness, emphasizing the need for balance. This study highlights the importance of domain-specific embeddings and careful model tuning for effective knowledge transfer in healthcare.</li>
<li><strong>摘要：</strong>知识共享在医疗保健领域至关重要，尤其是在利用来自多个临床站点的数据来解决数据稀缺、降低成本和实现及时干预时。迁移学习可以促进跨站点知识转移，但一个主要挑战是不同站点之间临床概念的异质性。大型语言模型 (LLM) 显示出捕捉临床概念语义含义和减少异质性的巨大潜力。本研究分析了两个大型医疗保健系统的电子健康记录，以评估 LLM 的语义嵌入对本地、共享和迁移学习模型的影响。结果表明，特定领域的 LLM（例如 Med-BERT）在本地和直接传输场景中始终表现优异，而通用模型（例如 OpenAI 嵌入）需要进行微调才能获得最佳性能。然而，使用生物医学嵌入过度调整模型可能会降低有效性，强调平衡的必要性。本研究强调了特定领域的嵌入和仔细的模型调整对于医疗保健中有效知识转移的重要性。</li>
</ul>

<h3>Title: LLM for Everyone: Representing the Underrepresented in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Samuel Cahyawijaya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13897">https://arxiv.org/abs/2409.13897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13897">https://arxiv.org/pdf/2409.13897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13897]] LLM for Everyone: Representing the Underrepresented in Large Language Models(https://arxiv.org/abs/2409.13897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Natural language processing (NLP) has witnessed a profound impact of large language models (LLMs) that excel in a multitude of tasks. However, the limitation of LLMs in multilingual settings, particularly in underrepresented languages, remains a significant hurdle. This thesis aims to bridge the gap in NLP research and development by focusing on underrepresented languages. A comprehensive evaluation of LLMs is conducted to assess their capabilities in these languages, revealing the challenges of multilingual and multicultural generalization. Addressing the multilingual generalization gap, this thesis proposes data-and-compute-efficient methods to mitigate the disparity in LLM ability in underrepresented languages, allowing better generalization on underrepresented languages without the loss of task generalization ability. The proposed solutions cover cross-lingual continual instruction tuning, retrieval-based cross-lingual in-context learning, and in-context query alignment. Furthermore, a novel method to measure cultural values alignment between LLMs operating in different languages is proposed, ensuring cultural sensitivity and inclusivity. These contributions aim to enhance the multilingual and multicultural alignment of LLMs in underrepresented languages, ultimately advancing the NLP field toward greater equality and inclusiveness.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 已经见证了大型语言模型 (LLM) 的深远影响，这些模型在众多任务中表现出色。然而，LLM 在多语言环境中的局限性，特别是在代表性不足的语言中，仍然是一个重大障碍。本论文旨在通过关注代表性不足的语言来弥合 NLP 研究和开发方面的差距。对 LLM 进行了全面评估，以评估它们在这些语言中的能力，揭示了多语言和多文化泛化的挑战。为了解决多语言泛化差距，本论文提出了数据和计算效率高的方法来缓解代表性不足的语言中 LLM 能力的差异，从而可以在不损失任务泛化能力的情况下更好地泛化代表性不足的语言。提出的解决方案包括跨语言持续指令调整、基于检索的跨语言上下文学习和上下文查询对齐。此外，还提出了一种衡量不同语言中 LLM 之间的文化价值观一致性的新方法，以确保文化敏感性和包容性。这些贡献旨在增强代表性不足的语言的法学硕士的多语言和多元文化一致性，最终推动 NLP 领域走向更加平等和包容。</li>
</ul>

<h3>Title: Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology</h3>
<ul>
<li><strong>Authors: </strong>Aidan Gilson, Xuguang Ai, Thilaka Arunachalam, Ziyou Chen, Ki Xiong Cheong, Amisha Dave, Cameron Duic, Mercy Kibe, Annette Kaminaka, Minali Prasad, Fares Siddig, Maxwell Singer, Wendy Wong, Qiao Jin, Tiarnan D.L. Keenan, Xia Hu, Emily Y. Chew, Zhiyong Lu, Hua Xu, Ron A. Adelman, Yih-Chung Tham, Qingyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13902">https://arxiv.org/abs/2409.13902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13902">https://arxiv.org/pdf/2409.13902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13902]] Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology(https://arxiv.org/abs/2409.13902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Despite the potential of Large Language Models (LLMs) in medicine, they may generate responses lacking supporting evidence or based on hallucinated evidence. While Retrieval Augment Generation (RAG) is popular to address this issue, few studies implemented and evaluated RAG in downstream domain-specific applications. We developed a RAG pipeline with 70,000 ophthalmology-specific documents that retrieve relevant documents to augment LLMs during inference time. In a case study on long-form consumer health questions, we systematically evaluated the responses including over 500 references of LLMs with and without RAG on 100 questions with 10 healthcare professionals. The evaluation focuses on factuality of evidence, selection and ranking of evidence, attribution of evidence, and answer accuracy and completeness. LLMs without RAG provided 252 references in total. Of which, 45.3% hallucinated, 34.1% consisted of minor errors, and 20.6% were correct. In contrast, LLMs with RAG significantly improved accuracy (54.5% being correct) and reduced error rates (18.8% with minor hallucinations and 26.7% with errors). 62.5% of the top 10 documents retrieved by RAG were selected as the top references in the LLM response, with an average ranking of 4.9. The use of RAG also improved evidence attribution (increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight decreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47 to 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited hallucinated and erroneous evidence in the responses, raising concerns for downstream applications in the medical domain. RAG substantially reduced the proportion of such evidence but encountered challenges.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 在医学领域具有巨大潜力，但它们可能会生成缺乏支持证据或基于幻觉证据的答案。虽然检索增强生成 (RAG) 是解决此问题的流行方法，但很少有研究在下游领域特定应用中实施和评估 RAG。我们开发了一个包含 70,000 份眼科专用文档的 RAG 管道，可在推理时间内检索相关文档以增强 LLM。在一项关于长篇消费者健康问题的案例研究中，我们系统地评估了 10 名医疗保健专业人士对 100 个问题的 500 多个带有和不带有 RAG 的 LLM 参考资料。评估侧重于证据的真实性、证据的选择和排名、证据的归因以及答案的准确性和完整性。没有 RAG 的 LLM 总共提供了 252 个参考资料。其中，45.3% 是幻觉，34.1% 是小错误，20.6% 是正确的。相比之下，使用 RAG 的 LLM 显著提高了准确率（54.5% 为正确）并降低了错误率（18.8% 为轻微幻觉，26.7% 为错误）。通过 RAG 检索到的前 10 篇文档中有 62.5% 被选为 LLM 回复中的顶级参考文献，平均排名为 4.9。使用 RAG 还改善了证据归因（在 5 分量表中从 1.85 增加到 2.49，P<0.001），尽管准确率（从 3.52 到 3.23，P=0.03）和完整性（从 3.47 到 3.27，P=0.17）略有下降。结果表明，LLM 在回复中经常表现出幻觉和错误的证据，这引起了人们对医学领域下游应用的担忧。RAG 大大降低了此类证据的比例，但也遇到了挑战。</li>
</ul>

<h3>Title: One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Nehrdich, Oliver Hellwig, Kurt Keutzer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13920">https://arxiv.org/abs/2409.13920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13920">https://arxiv.org/pdf/2409.13920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13920]] One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks(https://arxiv.org/abs/2409.13920)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Morphologically rich languages are notoriously challenging to process for downstream NLP applications. This paper presents a new pretrained language model, ByT5-Sanskrit, designed for NLP applications involving the morphologically rich language Sanskrit. We evaluate ByT5-Sanskrit on established Sanskrit word segmentation tasks, where it outperforms previous data-driven approaches by a considerable margin and matches the performance of the current best lexicon-based model. It is easier to deploy and more robust to data not covered by external linguistic resources. It also achieves new state-of-the-art results in Vedic Sanskrit dependency parsing and OCR post-correction tasks. Additionally, based on the Digital Corpus of Sanskrit, we introduce a novel multitask dataset for the joint training of Sanskrit word segmentation, lemmatization, and morphosyntactic tagging tasks. We fine-tune ByT5-Sanskrit on this dataset, creating a versatile multitask model for various downstream Sanskrit applications. We have used this model in Sanskrit linguistic annotation projects, in information retrieval setups, and as a preprocessing step in a Sanskrit machine translation pipeline. We also show that our approach yields new best scores for lemmatization and dependency parsing of other morphologically rich languages. We thus demonstrate that byte-level pretrained language models can achieve excellent performance for morphologically rich languages, outperforming tokenizer-based models and presenting an important vector of exploration when constructing NLP pipelines for such languages.</li>
<li><strong>摘要：</strong>形态丰富的语言对于下游 NLP 应用程序而言极具挑战性。本文介绍了一种新的预训练语言模型 ByT5-Sanskrit，专为涉及形态丰富的梵语的 NLP 应用程序而设计。我们在成熟的梵语分词任务上对 ByT5-Sanskrit 进行了评估，其表现远超以前的数据驱动方法，并与当前最佳的基于词典的模型的性能相当。它更易于部署，并且对于外部语言资源未涵盖的数据更具鲁棒性。它还在吠陀梵语依存关系解析和 OCR 后校正任务中取得了新的最佳结果。此外，基于梵语数字语料库，我们引入了一种新颖的多任务数据集，用于梵语分词、词形还原和形态句法标记任务的联合训练。我们在此数据集上对 ByT5-Sanskrit 进行了微调，为各种下游梵语应用程序创建了一个多功能的多任务模型。我们已将此模型用于梵语语言注释项目、信息检索设置以及梵语机器翻译流程中的预处理步骤。我们还表明，我们的方法在其他形态丰富的语言的词形还原和依存关系解析中获得了新的最佳分数。因此，我们证明了字节级预训练语言模型可以为形态丰富的语言实现出色的性能，优于基于标记器的模型，并且在为此类语言构建 NLP 流程时提供了重要的探索方向。</li>
</ul>

<h3>Title: MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sarfaroz Yunusov, Hamza Sidat, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13935">https://arxiv.org/abs/2409.13935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13935">https://arxiv.org/pdf/2409.13935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13935]] MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models(https://arxiv.org/abs/2409.13935)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study explores the effectiveness of Large Language Models (LLMs) in creating personalized "mirror stories" that reflect and resonate with individual readers' identities, addressing the significant lack of diversity in literature. We present MirrorStories, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and story moral. We demonstrate that LLMs can effectively incorporate diverse identity elements into narratives, with human evaluators identifying personalized elements in the stories with high accuracy. Through a comprehensive evaluation involving 26 diverse human judges, we compare the effectiveness of MirrorStories against generic narratives. We find that personalized LLM-generated stories not only outscore generic human-written and LLM-generated ones across all metrics of engagement (with average ratings of 4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity while preserving the intended moral. We also provide analyses that include bias assessments and a study on the potential for integrating images into personalized stories.</li>
<li><strong>摘要：</strong>本研究探讨了大型语言模型 (LLM) 在创建个性化“镜像故事”方面的有效性，这些故事反映了个人读者的身份并与之产生共鸣，解决了文学中多样性严重缺乏的问题。我们提出了 MirrorStories，这是一个由 1,500 个个性化短篇故事组成的语料库，通过整合姓名、性别、年龄、种族、读者兴趣和故事寓意等元素生成。我们证明 LLM 可以有效地将不同的身份元素融入叙事中，人类评估者可以高精度地识别故事中的个性化元素。通过涉及 26 位不同人类评委的全面评估，我们将 MirrorStories 与一般叙事的有效性进行了比较。我们发现个性化的 LLM 生成的故事不仅在所有参与度指标上都超过了一般的人工编写和 LLM 生成的故事（平均评分为 4.22 对 3.37，满分为 5 分），而且在保留预期寓意的同时实现了更高的文本多样性。我们还提供了包括偏见评估的分析和关于将图像整合到个性化故事中的潜力的研究。</li>
</ul>

<h3>Title: Aligning Language Models Using Follow-up Likelihood as Reward Signal</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhang, Dading Chong, Feng Jiang, Chengguang Tang, Anningzhe Gao, Guohua Tang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13948">https://arxiv.org/abs/2409.13948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13948">https://arxiv.org/pdf/2409.13948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13948]] Aligning Language Models Using Follow-up Likelihood as Reward Signal(https://arxiv.org/abs/2409.13948)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other non-verbal cues. Similarly, in human-machine interactions, the machine can leverage the user's follow-up utterances as feedback signals to assess whether it has appropriately addressed the user's request. Therefore, we propose using the likelihood of follow-up utterances as rewards to differentiate preferred responses from less favored ones, without relying on human or commercial LLM-based preference annotations. Our proposed reward mechanism, ``Follow-up Likelihood as Reward" (FLR), matches the performance of strong reward models trained on large-scale human or GPT-4 annotated data on 8 pairwise-preference and 4 rating-based benchmarks. Building upon the FLR mechanism, we propose to automatically mine preference data from the online generations of a base policy model. The preference data are subsequently used to boost the helpfulness of the base model through direct alignment from preference (DAP) methods, such as direct preference optimization (DPO). Lastly, we demonstrate that fine-tuning the language model that provides follow-up likelihood with natural language feedback significantly enhances FLR's performance on reward modeling benchmarks and effectiveness in aligning the base policy model's helpfulness.</li>
<li><strong>摘要：</strong>在自然的人际对话中，参与者通常会根据他们的后续反应收到对方的反馈信号。这些反应可以包括口头回应、面部表情、情绪状态变化和其他非语言暗示。同样，在人机交互中，机器可以利用用户的后续话语作为反馈信号来评估它是否适当地满足了用户的请求。因此，我们建议使用后续话语的可能性作为奖励来区分偏好的回应和不太偏好的回应，而不依赖于人类或商业的基于 LLM 的偏好注释。我们提出的奖励机制“后续可能性奖励”（FLR）在 8 个成对偏好和 4 个基于评级的基准上的表现可与在大规模人工或 GPT-4 注释数据上训练的强奖励模型相媲美。基于 FLR 机制，我们建议从基础策略模型的在线生成中自动挖掘偏好数据。随后，偏好数据将通过直接偏好对齐 (DAP) 方法（例如直接偏好优化 (DPO)）用于提升基础模型的有用性。最后，我们证明，通过自然语言反馈对提供后续可能性的语言模型进行微调可显著提高 FLR 在奖励建模基准上的表现以及对齐基础策略模型有用性的有效性。</li>
</ul>

<h3>Title: Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM</h3>
<ul>
<li><strong>Authors: </strong>Zheng Wei Lim, Nitish Gupta, Honglin Yu, Trevor Cohn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13949">https://arxiv.org/abs/2409.13949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13949">https://arxiv.org/pdf/2409.13949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13949]] Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM(https://arxiv.org/abs/2409.13949)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) are great translators, but this is largely limited to high-resource languages. For many LLMs, translating in and out of low-resource languages remains a challenging task. To maximize data efficiency in this low-resource setting, we introduce Mufu, which includes a selection of automatically generated multilingual candidates and an instruction to correct inaccurate translations in the prompt. Mufu prompts turn a translation task into a postediting one, and seek to harness the LLM's reasoning capability with auxiliary translation candidates, from which the model is required to assess the input quality, align the semantics cross-lingually, copy from relevant inputs and override instances that are incorrect. Our experiments on En-XX translations over the Flores-200 dataset show LLMs finetuned against Mufu-style prompts are robust to poor quality auxiliary translation candidates, achieving performance superior to NLLB 1.3B distilled model in 64% of low- and very-low-resource language pairs. We then distill these models to reduce inference cost, while maintaining on average 3.1 chrF improvement over finetune-only baseline in low-resource translations.</li>
<li><strong>摘要：</strong>多语言大型语言模型 (LLM) 是出色的翻译工具，但这主要限于高资源语言。对于许多 LLM 来说，翻译低资源语言仍然是一项艰巨的任务。为了在这种低资源环境中最大限度地提高数据效率，我们引入了 Mufu，其中包括自动生成的多语言候选项和在提示中纠正不准确翻译的指令。Mufu 提示将翻译任务转变为后编辑任务，并试图利用辅助翻译候选项来利用 LLM 的推理能力，模型需要从中评估输入质量、跨语言对齐语义、从相关输入中复制并覆盖不正确的实例。我们在 Flores-200 数据集上对 En-XX 翻译进行的实验表明，针对 Mufu 风格提示进行微调的 LLM 对质量较差的辅助翻译候选项具有很强的鲁棒性，在 64% 的低资源和极低资源语言对中实现了优于 NLLB 1.3B 提炼模型的性能。然后，我们提炼这些模型以降低推理成本，同时在低资源翻译中保持比仅微调基线平均 3.1 chrF 的改进。</li>
</ul>

<h3>Title: Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank</h3>
<ul>
<li><strong>Authors: </strong>Jaewook Lee, Hunter McNichols, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13952">https://arxiv.org/abs/2409.13952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13952">https://arxiv.org/pdf/2409.13952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13952]] Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank(https://arxiv.org/abs/2409.13952)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we study an under-explored area of language and vocabulary learning: keyword mnemonics, a technique for memorizing vocabulary through memorable associations with a target word via a verbal cue. Typically, creating verbal cues requires extensive human effort and is quite time-consuming, necessitating an automated method that is more scalable. We propose a novel overgenerate-and-rank method via prompting large language models (LLMs) to generate verbal cues and then ranking them according to psycholinguistic measures and takeaways from a pilot user study. To assess cue quality, we conduct both an automated evaluation of imageability and coherence, as well as a human evaluation involving English teachers and learners. Results show that LLM-generated mnemonics are comparable to human-generated ones in terms of imageability, coherence, and perceived usefulness, but there remains plenty of room for improvement due to the diversity in background and preference among language learners.</li>
<li><strong>摘要：</strong>在本文中，我们研究了语言和词汇学习中一个尚未得到充分探索的领域：关键词助记符，这是一种通过口头提示与目标词建立难忘的关联来记忆词汇的技术。通常，创建口头提示需要大量的人力，而且非常耗时，因此需要一种更具可扩展性的自动化方法。我们提出了一种新颖的过度生成和排名方法，通过提示大型语言模型 (LLM) 生成口头提示，然后根据心理语言学指标和试点用户研究的收获对其进行排名。为了评估提示质量，我们进行了自动可想象性和连贯性的评估，以及涉及英语教师和学习者的人工评估。结果表明，LLM 生成的助记符在可想象性、连贯性和感知有用性方面与人类生成的助记符相当，但由于语言学习者的背景和偏好各不相同，仍有很大改进空间。</li>
</ul>

<h3>Title: Can Language Model Understand Word Semantics as A Chatbot? An Empirical Study of Language Model Internal External Mismatch</h3>
<ul>
<li><strong>Authors: </strong>Jinman Zhao, Xueyan Zhang, Xingyu Yue, Weizhe Chen, Zifan Qian, Ruiyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13972">https://arxiv.org/abs/2409.13972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13972">https://arxiv.org/pdf/2409.13972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13972]] Can Language Model Understand Word Semantics as A Chatbot? An Empirical Study of Language Model Internal External Mismatch(https://arxiv.org/abs/2409.13972)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chat</a></li>
<li><strong>Abstract: </strong>Current common interactions with language models is through full inference. This approach may not necessarily align with the model's internal knowledge. Studies show discrepancies between prompts and internal representations. Most focus on sentence understanding. We study the discrepancy of word semantics understanding in internal and external mismatch across Encoder-only, Decoder-only, and Encoder-Decoder pre-trained language models.</li>
<li><strong>摘要：</strong>目前与语言模型的常见交互是通过完全推理。这种方法可能不一定与模型的内部知识相一致。研究表明提示和内部表示之间存在差异。大多数研究都集中在句子理解上。我们研究了在仅编码器、仅解码器和编码器-解码器预训练语言模型中，内部和外部不匹配的单词语义理解的差异。</li>
</ul>

<h3>Title: Bias and Toxicity in Role-Play Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jinman Zhao, Zifan Qian, Linbo Cao, Yining Wang, Yitian Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13979">https://arxiv.org/abs/2409.13979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13979">https://arxiv.org/pdf/2409.13979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13979]] Bias and Toxicity in Role-Play Reasoning(https://arxiv.org/abs/2409.13979)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Role-play in the Large Language Model (LLM) is a crucial technique that enables models to adopt specific perspectives, enhancing their ability to generate contextually relevant and accurate responses. By simulating different roles, theis approach improves reasoning capabilities across various NLP benchmarks, making the model's output more aligned with diverse scenarios. However, in this work, we demonstrate that role-play also carries potential risks. We systematically evaluate the impact of role-play by asking the language model to adopt different roles and testing it on multiple benchmarks that contain stereotypical and harmful questions. Despite the significant fluctuations in the benchmark results in different experiments, we find that applying role-play often increases the overall likelihood of generating stereotypical and harmful outputs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的角色扮演是一项关键技术，它使模型能够采用特定的观点，从而增强其生成与上下文相关且准确的响应的能力。通过模拟不同的角色，这种方法提高了各种 NLP 基准的推理能力，使模型的输出更符合不同的场景。然而，在这项工作中，我们证明了角色扮演也存在潜在风险。我们通过要求语言模型扮演不同的角色并在包含刻板和有害问题的多个基准上对其进行测试，系统地评估了角色扮演的影响。尽管不同实验中的基准结果存在显着波动，但我们发现应用角色扮演通常会增加生成刻板和有害输出的总体可能性。</li>
</ul>

<h3>Title: ChemEval: A Comprehensive Multi-Level Chemical Evaluation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong Liang, Yi Li, Jian Cui, Zimu Liu, Shijin Wang, Guoping Hu, Guiquan Liu, Qi Liu, Defu Lian, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13989">https://arxiv.org/abs/2409.13989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13989">https://arxiv.org/pdf/2409.13989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13989]] ChemEval: A Comprehensive Multi-Level Chemical Evaluation for Large Language Models(https://arxiv.org/abs/2409.13989)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>There is a growing interest in the role that LLMs play in chemistry which lead to an increased focus on the development of LLMs benchmarks tailored to chemical domains to assess the performance of LLMs across a spectrum of chemical tasks varying in type and complexity. However, existing benchmarks in this domain fail to adequately meet the specific requirements of chemical research professionals. To this end, we propose \textbf{\textit{ChemEval}}, which provides a comprehensive assessment of the capabilities of LLMs across a wide range of chemical domain tasks. Specifically, ChemEval identified 4 crucial progressive levels in chemistry, assessing 12 dimensions of LLMs across 42 distinct chemical tasks which are informed by open-source data and the data meticulously crafted by chemical experts, ensuring that the tasks have practical value and can effectively evaluate the capabilities of LLMs. In the experiment, we evaluate 12 mainstream LLMs on ChemEval under zero-shot and few-shot learning contexts, which included carefully selected demonstration examples and carefully designed prompts. The results show that while general LLMs like GPT-4 and Claude-3.5 excel in literature understanding and instruction following, they fall short in tasks demanding advanced chemical knowledge. Conversely, specialized LLMs exhibit enhanced chemical competencies, albeit with reduced literary comprehension. This suggests that LLMs have significant potential for enhancement when tackling sophisticated tasks in the field of chemistry. We believe our work will facilitate the exploration of their potential to drive progress in chemistry. Our benchmark and analysis will be available at {\color{blue} \url{this https URL}}.</li>
<li><strong>摘要：</strong>人们对 LLM 在化学领域的作用越来越感兴趣，这导致人们更加关注开发针对化学领域的 LLM 基准，以评估 LLM 在各种类型和复杂性的化学任务中的表现。然而，该领域现有的基准未能充分满足化学研究专业人员的特定要求。为此，我们提出了 \textbf{\textit{ChemEval}}，它对 LLM 在广泛的化学领域任务中的能力进行了全面的评估。具体来说，ChemEval 确定了化学领域中的 4 个关键进步水平，评估了 42 个不同化学任务中 LLM 的 12 个维度，这些数据来自开源数据和化学专家精心制作的数据，确保这些任务具有实用价值并能有效评估 LLM 的能力。在实验中，我们在零样本和少样本学习环境下评估了 ChemEval 上的 12 个主流 LLM，其中包括精心选择的演示示例和精心设计的提示。结果表明，虽然 GPT-4 和 Claude-3.5 等普通法学硕士在文献理解和指导跟进方面表现出色，但在需要高级化学知识的任务中却表现不佳。相反，专业法学硕士表现出增强的化学能力，尽管文学理解能力有所降低。这表明法学硕士在处理化学领域的复杂任务时具有巨大的提升潜力。我们相信我们的工作将有助于探索它们推动化学进步的潜力。我们的基准和分析将在 {\color{blue} \url{this https URL}} 上提供。</li>
</ul>

<h3>Title: SMART-RAG: Selection using Determinantal Matrices for Augmented Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jiatao Li, Xinyu Hu, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13992">https://arxiv.org/abs/2409.13992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13992">https://arxiv.org/pdf/2409.13992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13992]] SMART-RAG: Selection using Determinantal Matrices for Augmented Retrieval(https://arxiv.org/abs/2409.13992)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has greatly improved large language models (LLMs) by enabling them to generate accurate, contextually grounded responses through the integration of external information. However, conventional RAG approaches, which prioritize top-ranked documents based solely on query-context relevance, often introduce redundancy and conflicting information. This issue is particularly evident in unsupervised retrieval settings, where there are no mechanisms to effectively mitigate these problems, leading to suboptimal context selection. To address this, we propose Selection using Matrices for Augmented Retrieval (SMART) in question answering tasks, a fully unsupervised and training-free framework designed to optimize context selection in RAG. SMART leverages Determinantal Point Processes (DPPs) to simultaneously model relevance, diversity and conflict, ensuring the selection of potentially high-quality contexts. Experimental results across multiple datasets demonstrate that SMART significantly enhances QA performance and surpasses previous unsupervised context selection methods, showing a promising strategy for RAG.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 极大地改进了大型语言模型 (LLM)，使其能够通过整合外部信息生成准确、基于上下文的响应。然而，传统的 RAG 方法仅根据查询上下文相关性对排名靠前的文档进行优先排序，这通常会引入冗余和冲突信息。这个问题在无监督检索设置中尤其明显，因为没有有效缓解这些问题的机制，导致上下文选择不理想。为了解决这个问题，我们在问答任务中提出了使用增强检索矩阵进行选择 (SMART)，这是一个完全无监督且无需训练的框架，旨在优化 RAG 中的上下文选择。SMART 利用行列式点过程 (DPP) 同时对相关性、多样性和冲突进行建模，确保选择潜在的高质量上下文。跨多个数据集的实验结果表明，SMART 显著提高了 QA 性能并超越了以前的无监督上下文选择方法，为 RAG 展示了一种有前途的策略。</li>
</ul>

<h3>Title: Contrastive Learning for Knowledge-Based Question Generation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenhong Zhang, Jiajing Chen, Weiyan Shi, Lingjie Yi, Chihang Wang, Qian Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.13994">https://arxiv.org/abs/2409.13994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.13994">https://arxiv.org/pdf/2409.13994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.13994]] Contrastive Learning for Knowledge-Based Question Generation in Large Language Models(https://arxiv.org/abs/2409.13994)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specific texts or knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointly mine domain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.</li>
<li><strong>摘要：</strong>随着人工智能技术的快速发展，特别是问答系统的应用日益广泛，高质量的问题生成成为支撑问答系统发展的关键要素。本文重点研究基于知识库的问题生成技术，旨在使计算机能够基于对特定文本或知识库的理解，模拟人类的提问过程。针对大规模语言模型应用于知识密集型任务时出现的幻读和知识缺口问题，本文提出了一种融合对比学习的增强型问题生成方法。该方法利用多个模型联合挖掘领域知识，利用对比学习指导模型减少生成过程中的噪音和幻读。实验结果表明，通过设计包含对比示例的提示，模型的问题生成性能得到明显提升，尤其在同时使用对比说明和示例时，生成的问题质量最高，准确率也得到提升。结果表明，本研究提出的结合对比上下文和思路链提示的方法可以有效提高问题生成的质量和实用性。</li>
</ul>

<h3>Title: Uncovering Latent Chain of Thought Vectors in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jason Zhang, Scott Viteri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14026">https://arxiv.org/abs/2409.14026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14026">https://arxiv.org/pdf/2409.14026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14026]] Uncovering Latent Chain of Thought Vectors in Language Models(https://arxiv.org/abs/2409.14026)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>As language models grow more influential and trusted in our society, our ability to reliably steer them toward favorable behaviors becomes increasingly paramount. For this, we investigate the technique of steering vectors: biasing the forward pass of language models using a "steering vector" derived from a specific task. We apply them to steer language models toward performing Chain of Thought (CoT) Reasoning without the need to prompt through natural language. We demonstrate this approach on Llama3 8b and Mistral 7b v0.2, and obtain competitive results compared to CoT-prompted performances on a series of reasoning benchmarks (GSM8k, MMLU, AGI Eval, ARC AI2) and qualitative examples. We find this approach yields consistent steering towards CoT responses and takes less compute than traditional methods of fine-tuning models towards CoT.</li>
<li><strong>摘要：</strong>随着语言模型在社会中的影响力和可信度越来越高，我们能否可靠地引导它们朝着有利的方向发展变得越来越重要。为此，我们研究了引导向量技术：使用从特定任务中派生出的“引导向量”来偏向语言模型的前向传递。我们应用它们来引导语言模型执行思路链 (CoT) 推理，而无需通过自然语言提示。我们在 Llama3 8b 和 Mistral 7b v0.2 上演示了这种方法，并在一系列推理基准（GSM8k、MMLU、AGI Eval、ARC AI2）和定性示例上获得了与 CoT 提示的性能相比具有竞争力的结果。我们发现这种方法可以一致地引导 CoT 响应，并且比传统的微调模型以达到 CoT 所需的计算量更少。</li>
</ul>

<h3>Title: Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators</h3>
<ul>
<li><strong>Authors: </strong>Prasoon Bajpai, Niladri Chatterjee, Subhabrata Dutta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14037">https://arxiv.org/abs/2409.14037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14037">https://arxiv.org/pdf/2409.14037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14037]] Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators(https://arxiv.org/abs/2409.14037)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific questionanswering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 和由这些模型驱动的 AI 助手在专家和业余用户中的使用率呈指数级增长。在这项工作中，我们专注于评估当前 LLM 作为科学传播者的可靠性。与现有基准不同，我们的方法强调在需要细致理解和意识到可回答性的科学问答任务上评估这些模型。我们引入了一个新数据集 SCiPS-QA，其中包含嵌入在复杂科学概念中的 742 个是/否查询，以及一个基准测试套件，用于评估 LLM 在各种标准下的正确性和一致性。我们对 OpenAI GPT 系列的三个专有 LLM 和 Meta Llama-2、Llama-3 和 Mistral 系列的 13 个开放访问 LLM 进行了基准测试。虽然大多数开放访问模型的表现明显低于 GPT-4 Turbo，但我们的实验表明 Llama-3-70B 是一个强大的竞争对手，通常在各种评估方面都超越 GPT-4 Turbo。我们还发现，即使是 GPT 模型在可靠地验证 LLM 答案方面也表现出普遍的无能。此外，我们观察到一个令人担忧的趋势，即人类评估者被 GPT-4 Turbo 的错误答案欺骗。</li>
</ul>

<h3>Title: GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion</h3>
<ul>
<li><strong>Authors: </strong>Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, Jing Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14051">https://arxiv.org/abs/2409.14051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14051">https://arxiv.org/pdf/2409.14051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14051]] GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion(https://arxiv.org/abs/2409.14051)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, tree-of-thought, agent</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse NLP tasks. Extensive research has explored how to enhance the logical reasoning abilities such as Chain-of-Thought, Chain-of-Thought with Self-Consistency, Tree-Of-Thoughts, and multi-agent debates. In the context of multi-agent debates, significant performance improvements can be achieved with an increasing number of agents and debate rounds. However, the escalation in the number of agents and debate rounds can drastically raise the tokens cost of debates, thereby limiting the scalability of the multi-agent debate technique. To better harness the advantages of multi-agent debates in logical reasoning tasks, this paper proposes a method to significantly reduce token cost in multi-agent debates. This approach involves dividing all agents into multiple debate groups, with agents engaging in debates within their respective groups and sharing interim debate results between groups. Comparative experiments across multiple datasets have demonstrated that this method can reduce the total tokens by up to 51.7% during debates and while potentially enhancing accuracy by as much as 25%. Our method significantly enhances the performance and efficiency of interactions in the multi-agent debate.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 在各种 NLP 任务中展现出了卓越的能力。大量研究探索了如何增强逻辑推理能力，例如思路链、具有自洽性的思路链、思路树和多智能体辩论。在多智能体辩论的背景下，随着智能体数量和辩论轮次的增加，可以实现显著的性能提升。然而，智能体数量和辩论轮次的增加会大幅提高辩论的代币成本，从而限制多智能体辩论技术的可扩展性。为了更好地利用多智能体辩论在逻辑推理任务中的优势，本文提出了一种显著降低多智能体辩论代币成本的方法。该方法涉及将所有智能体分成多个辩论组，智能体在各自的组内参与辩论，并在组间共享中期辩论结果。多个数据集的对比实验表明，该方法可以在辩论过程中减少最多 51.7% 的 token 总量，同时最大程度地提高准确率。我们的方法显著提高了多智能体辩论中的交互性能和效率。</li>
</ul>

<h3>Title: Co-occurrence is not Factual Association in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zhang, Miao Li, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14057">https://arxiv.org/abs/2409.14057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14057">https://arxiv.org/pdf/2409.14057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14057]] Co-occurrence is not Factual Association in Language Models(https://arxiv.org/abs/2409.14057)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.</li>
<li><strong>摘要：</strong>预训练语言模型可以编码大量知识并将其用于各种推理任务，但它们仍然很难通过对有限的文本演示进行微调来有效地学习新的事实知识。在这项工作中，我们表明，造成这种缺陷的原因是语言模型倾向于学习词语共现统计，而不是真正的事实关联。我们确定了语言模型中两种知识表示形式的差异：以共现统计形式呈现的知识编码在 Transformer 模型的中间层，不能很好地推广到简单问答以外的推理场景，而真正的事实关联编码在较低层，可以自由地用于各种推理任务。基于这些观察，我们提出了两种策略来改进语言模型中事实关联的学习。我们表明，对具有隐性而非显性事实关联的文本进行训练可以迫使模型学习事实关联而不是共现统计，从而显著提高新学习知识的泛化能力。我们还提出了一种简单的训练方法，即主动忘记已学习的共现统计数据，这可以在对纯叙述文本进行训练时解除并增强事实关联的学习。在合成语料库和现实世界语料库中，这两种提出的策略都提高了在微调过程中学到的知识对间接和多跳问答等推理场景的泛化能力。</li>
</ul>

<h3>Title: Temporally Consistent Factuality Probing for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14065">https://arxiv.org/abs/2409.14065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14065">https://arxiv.org/pdf/2409.14065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14065]] Temporally Consistent Factuality Probing for Large Language Models(https://arxiv.org/abs/2409.14065)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The prolific use of Large Language Models (LLMs) as an alternate knowledge base requires them to be factually consistent, necessitating both correctness and consistency traits for paraphrased queries. Recently, significant attempts have been made to benchmark datasets and metrics to evaluate LLMs for these traits. However, structural simplicity (subject-relation-object) and contemporary association in their query formulation limit the broader definition of factuality and consistency. In this study, we introduce TeCFaP, a novel Temporally Consistent Factuality Probe task to expand the consistent factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC, a high-quality dataset of prefix-style English query paraphrases. Subsequently, we extend the definitions of existing metrics to represent consistent factuality across temporal dimension. We experiment with a diverse set of LLMs and find most of them performing poorly on TeCFaP. Next, we propose a novel solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining multi-task instruction tuning (MT-IT) with consistent-time-sensitive reinforcement learning (CTSRL) to improve temporally consistent factuality in LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several baselines.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 作为替代知识库的广泛使用要求它们具有事实一致性，这就要求释义查询同时具备正确性和一致性特征。最近，人们做出了重大尝试来对数据集和指标进行基准测试，以评估 LLM 的这些特征。然而，查询公式中的结构简单性（主语-关系-宾语）和当代关联限制了事实性和一致性的更广泛定义。在本研究中，我们引入了 TeCFaP，一种新颖的时间一致性事实性探测任务，以扩展时间维度上的一致性事实性探测。为此，我们提出了 TEMP-COFAC，这是一个高质量的前缀式英语查询释义数据集。随后，我们扩展了现有指标的定义，以表示跨时间维度的一致性事实性。我们尝试了一组不同的 LLM，发现它们中的大多数在 TeCFaP 上的表现都很差。接下来，我们提出了一种新颖的解决方案 CoTSeLF（一致时间敏感学习框架），将多任务指令调整 (MT-IT) 与一致时间敏感强化学习 (CTSRL) 相结合，以提高 LLM 中时间一致性事实性。我们的实验证明了 CoTSeLF 在多个基线上的有效性。</li>
</ul>

<h3>Title: PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Ruilin Luo, Liyuan Wang, Binghuai Lin, Zicheng Lin, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14082">https://arxiv.org/abs/2409.14082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14082">https://arxiv.org/pdf/2409.14082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14082]] PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL(https://arxiv.org/abs/2409.14082)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problems and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements, mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为文本到 SQL 任务的强大工具，表现出卓越的推理能力。与数学应用题和常识推理等任务不同，SQL 解决方案具有相对固定的模式。这有助于研究 LLM 是否可以从分类思维中受益，反映出人类如何通过基于可比示例的归纳推理获取知识。在本研究中，我们提出使用查询组分区允许 LLM 专注于学习特定于单一问题类型的思维过程，从而增强其在不同难度级别和问题类别中的推理能力。我们的实验表明，多个高级 LLM 在配备 PTD-SQL 后，可以在 Spider 和 BIRD 数据集上超越或匹配以前最先进的 (SOTA) 方法。有趣的是，具有不同初始性能的模型在有针对性的钻探后表现出显着的改进，主要是在其能力的边界上，这表明与人类的进步相似。代码可在此 https URL 上获得。</li>
</ul>

<h3>Title: Probing Context Localization of Polysemous Words in Pre-trained Language Model Sub-Layers</h3>
<ul>
<li><strong>Authors: </strong>Soniya Vijayakumar, Josef van Genabith, Simon Ostermann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14097">https://arxiv.org/abs/2409.14097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14097">https://arxiv.org/pdf/2409.14097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14097]] Probing Context Localization of Polysemous Words in Pre-trained Language Model Sub-Layers(https://arxiv.org/abs/2409.14097)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the era of high performing Large Language Models, researchers have widely acknowledged that contextual word representations are one of the key drivers in achieving top performances in downstream tasks. In this work, we investigate the degree of contextualization encoded in the fine-grained sub-layer representations of a Pre-trained Language Model (PLM) by empirical experiments using linear probes. Unlike previous work, we are particularly interested in identifying the strength of contextualization across PLM sub-layer representations (i.e. Self-Attention, Feed-Forward Activation and Output sub-layers). To identify the main contributions of sub-layers to contextualisation, we first extract the sub-layer representations of polysemous words in minimally different sentence pairs, and compare how these representations change through the forward pass of the PLM network. Second, by probing on a sense identification classification task, we try to empirically localize the strength of contextualization information encoded in these sub-layer representations. With these probing experiments, we also try to gain a better understanding of the influence of context length and context richness on the degree of contextualization. Our main conclusion is cautionary: BERT demonstrates a high degree of contextualization in the top sub-layers if the word in question is in a specific position in the sentence with a shorter context window, but this does not systematically generalize across different word positions and context sizes.</li>
<li><strong>摘要：</strong>在高性能大型语言模型时代，研究人员普遍承认上下文词语表征是实现下游任务顶级性能的关键驱动因素之一。在这项工作中，我们通过使用线性探针的实证实验研究了预训练语言模型 (PLM) 的细粒度子层表征中编码的语境化程度。与以前的工作不同，我们特别感兴趣的是识别跨 PLM 子层表征（即自注意力、前馈激活和输出子层）的语境化强度。为了确定子层对语境化的主要贡献，我们首先提取最小差异句子对中多义词的子层表征，并比较这些表征如何通过 PLM 网络的前向传递而变化。其次，通过探索意义识别分类任务，我们尝试实证定位这些子层表征中编码的语境化信息的强度。通过这些探索性实验，我们还试图更好地理解上下文长度和上下文丰富度对上下文化程度的影响。我们的主要结论是谨慎的：如果所讨论的单词位于句子中的特定位置且上下文窗口较短，则 BERT 在顶部子层中表现出较高的上下文化程度，但这并不能系统地推广到不同的单词位置和上下文大小。</li>
</ul>

<h3>Title: Routing in Sparsely-gated Language Models responds to Context</h3>
<ul>
<li><strong>Authors: </strong>Stefan Arnold, Marian Fietta, Dilara Yesilbas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14107">https://arxiv.org/abs/2409.14107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14107">https://arxiv.org/pdf/2409.14107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14107]] Routing in Sparsely-gated Language Models responds to Context(https://arxiv.org/abs/2409.14107)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) recently incorporate mixture-of-experts layers consisting of a router and a collection of experts to scale up their parameter count given a fixed computational budget. Building on previous efforts indicating that token-expert assignments are predominantly influenced by token identities and positions, we trace routing decisions of similarity-annotated text pairs to evaluate the context sensitivity of learned token-expert assignments. We observe that routing in encoder layers mainly depends on (semantic) associations, but contextual cues provide an additional layer of refinement. Conversely, routing in decoder layers is more variable and markedly less sensitive to context.</li>
<li><strong>摘要：</strong>语言模型 (LM) 最近采用了由路由器和专家集合组成的混合专家层，以在给定固定计算预算的情况下扩大其参数数量。基于先前的努力，表明 token-expert 分配主要受 token 身份和位置的影响，我们跟踪相似性注释文本对的路由决策，以评估学习到的 token-expert 分配的上下文敏感性。我们观察到编码器层中的路由主要依赖于（语义）关联，但上下文线索提供了额外的细化层。相反，解码器层中的路由变化更大，对上下文的敏感度明显较低。</li>
</ul>

<h3>Title: Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Jaehan Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14119">https://arxiv.org/abs/2409.14119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14119">https://arxiv.org/pdf/2409.14119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14119]] Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm(https://arxiv.org/abs/2409.14119)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) has become a key training strategy for large language models. However, its reliance on fewer trainable parameters poses security risks, such as task-agnostic backdoors. Despite their severe impact on a wide range of tasks, there is no practical defense solution available that effectively counters task-agnostic backdoors within the context of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor defense. We develop two techniques aimed at amplifying benign neurons within PEFT layers and penalizing the influence of trigger tokens. Our evaluations across three major PEFT architectures show that our method can significantly reduce the attack success rate of the state-of-the-art task-agnostic backdoors (83.6%$\downarrow$). Furthermore, our method exhibits robust defense capabilities against both task-specific backdoors and adaptive attacks. Source code will be obtained at this https URL.</li>
<li><strong>摘要：</strong>参数高效微调 (PEFT) 已成为大型语言模型的关键训练策略。然而，它对较少可训练参数的依赖带来了安全风险，例如任务无关的后门。尽管它们对各种任务都有严重影响，但目前还没有一种实用的防御解决方案能够有效地对抗 PEFT 环境中的任务无关后门。在本研究中，我们介绍了一种可与 PEFT 集成的后门防御 Obliviate。我们开发了两种技术，旨在放大 PEFT 层内的良性神经元并惩罚触发标记的影响。我们对三种主要 PEFT 架构的评估表明，我们的方法可以显著降低最先进的任务无关后门的攻击成功率 (83.6%$\downarrow$)。此外，我们的方法对特定于任务的后门和自适应攻击都表现出强大的防御能力。源代码将在此 https URL 中获取。</li>
</ul>

<h3>Title: Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zeping Yu, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14144">https://arxiv.org/abs/2409.14144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14144">https://arxiv.org/pdf/2409.14144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14144]] Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis(https://arxiv.org/abs/2409.14144)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations. To delve into the reason, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons. Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages. These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions. Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias. Code is on this https URL.</li>
<li><strong>摘要：</strong>我们发现算术能力存在于有限数量的注意力头中，每个注意力头专门负责不同的操作。为了深入研究原因，我们引入了比较神经元分析 (CNA) 方法，该方法确定了从输入到预测由四个不同阶段组成的内部逻辑链：使用浅 FFN 神经元进行特征增强、通过浅层注意力层进行特征转移、通过算术头进行特征预测以及在深层 FFN 神经元之间进行预测增强。此外，我们在特征增强和特征预测阶段都确定了人类可解释的 FFN 神经元。这些发现促使我们研究 LoRA 的机制，揭示了它通过放大与预测相关的 FFN 神经元的系数分数来提高预测概率。最后，我们将我们的方法应用于算术任务的模型修剪和减少性别偏见的模型编辑。代码位于此 https URL 上。</li>
</ul>

<h3>Title: QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling</h3>
<ul>
<li><strong>Authors: </strong>Blessed Guda, Gabrial Zencha A., Lawrence Francis, Carlee Joe-Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14175">https://arxiv.org/abs/2409.14175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14175">https://arxiv.org/pdf/2409.14175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14175]] QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling(https://arxiv.org/abs/2409.14175)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language models (LLMs) have brought about substantial advancements in the field of Question Answering (QA) systems. These models do remarkably well in addressing intricate inquiries in a variety of disciplines. However, because of domain-specific vocabulary, complex technological concepts, and the requirement for exact responses applying LLMs to specialized sectors like telecommunications presents additional obstacles. GPT-3.5 has been used in recent work, to obtain noteworthy accuracy for telecom-related questions in a Retrieval Augmented Generation (RAG) framework. Notwithstanding these developments, the practical use of models such as GPT-3.5 is restricted by their proprietary nature and high computing demands. This paper introduces QMOS, an innovative approach which uses a Question-Masked loss and Option Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice Questions in the telecommunications domain. Our focus was on using opensource, smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework. Our multi-faceted approach involves several enhancements to the whole LLM-RAG pipeline of finetuning, retrieval, prompt engineering and inference. Our approaches significantly outperform existing results, achieving accuracy improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07% to 84.65% with Phi-2.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 为问答 (QA) 系统领域带来了重大进步。这些模型在解决各种学科的复杂查询方面表现出色。然而，由于领域特定词汇、复杂的技术概念以及对精确响应的要求，将 LLM 应用于电信等专业领域带来了额外的障碍。GPT-3.5 已在最近的工作中用于在检索增强生成 (RAG) 框架中获得与电信相关的问题的显著准确性。尽管取得了这些进展，但 GPT-3.5 等模型的实际使用受到其专有性质和高计算要求的限制。本文介绍了 QMOS，这是一种创新方法，它使用问题掩蔽损失和选项改组技巧来提高 LLM 在电信领域回答多项选择题时的性能。我们的重点是在增强的 RAG 框架中使用开源、较小的语言模型（Phi-2 和 Falcon-7B）。我们的多方面方法涉及对整个 LLM-RAG 流程的多项增强，包括微调、检索、快速工程和推理。我们的方法明显优于现有结果，使用 Falcon-7B 时准确率从基线的 24.70% 提高到 49.30%，使用 Phi-2 时准确率从 42.07% 提高到 84.65%。</li>
</ul>

<h3>Title: Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic Extraction</h3>
<ul>
<li><strong>Authors: </strong>Hossein Sholehrasa, Sanaz Saki Norouzi, Pascal Hitzler, Majid Jaberi-Douraki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14192">https://arxiv.org/abs/2409.14192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14192">https://arxiv.org/pdf/2409.14192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14192]] Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic Extraction(https://arxiv.org/abs/2409.14192)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Integrating structured knowledge from tabular formats poses significant challenges within natural language processing (NLP), mainly when dealing with complex, semi-structured tables like those found in the FeTaQA dataset. These tables require advanced methods to interpret and generate meaningful responses accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully capture the semantics of such data, especially in the presence of irregular table structures like web tables. This paper addresses these challenges by proposing a novel approach that extracts triples straightforward from tabular data and integrates it with a retrieval-augmented generation (RAG) model to enhance the accuracy, coherence, and contextual richness of responses generated by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly outperforms existing baselines on the FeTaQA dataset, particularly excelling in Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate and detailed long-form answers from tables, showcasing its strength in complex data interpretation.</li>
<li><strong>摘要：</strong>在自然语言处理 (NLP) 中，从表格格式中集成结构化知识带来了重大挑战，尤其是在处理 FeTaQA 数据集中复杂的半结构化表格时。这些表格需要高级方法来准确解释和生成有意义的响应。传统方法（例如 SQL 和 SPARQL）通常无法完全捕获此类数据的语义，尤其是在存在不规则的表格结构（例如 Web 表格）的情况下。本文通过提出一种新方法来应对这些挑战，该方法直接从表格数据中提取三元组并将其与检索增强生成 (RAG) 模型集成，以增强由微调的 GPT-3.5-turbo-0125 模型生成的响应的准确性、连贯性和上下文丰富性。我们的方法在 FeTaQA 数据集上的表现明显优于现有基线，尤其是在 Sacre-BLEU 和 ROUGE 指标方面表现出色。它可以有效地从表格中生成上下文准确且详细的长格式答案，展示了其在复杂数据解释方面的优势。</li>
</ul>

<h3>Title: The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends</h3>
<ul>
<li><strong>Authors: </strong>Xinghua Zhang, Haiyang Yu, Yongbin Li, Minzheng Wang, Longze Chen, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14195">https://arxiv.org/abs/2409.14195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14195">https://arxiv.org/pdf/2409.14195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14195]] The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends(https://arxiv.org/abs/2409.14195)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the era of large language models (LLMs), a vast amount of conversation logs will be accumulated thanks to the rapid development trend of language UI. Conversation Analysis (CA) strives to uncover and analyze critical information from conversation data, streamlining manual processes and supporting business insights and decision-making. The need for CA to extract actionable insights and drive empowerment is becoming increasingly prominent and attracting widespread attention. However, the lack of a clear scope for CA leads to a dispersion of various techniques, making it difficult to form a systematic technical synergy to empower business applications. In this paper, we perform a thorough review and systematize CA task to summarize the existing related work. Specifically, we formally define CA task to confront the fragmented and chaotic landscape in this field, and derive four key steps of CA from conversation scene reconstruction, to in-depth attribution analysis, and then to performing targeted training, finally generating conversations based on the targeted training for achieving the specific goals. In addition, we showcase the relevant benchmarks, discuss potential challenges and point out future directions in both industry and academia. In view of current advancements, it is evident that the majority of efforts are still concentrated on the analysis of shallow conversation elements, which presents a considerable gap between the research and business, and with the assist of LLMs, recent work has shown a trend towards research on causality and strategic tasks which are sophisticated and high-level. The analyzed experiences and insights will inevitably have broader application value in business operations that target conversation logs.</li>
<li><strong>摘要：</strong>在大型语言模型（LLM）时代，语言用户界面的快速发展趋势将积累大量的对话日志。对话分析（CA）致力于从对话数据中发现和分析关键信息，简化手动流程并支持业务洞察和决策。对 CA 提取可操作洞察和驱动赋能的需求日益突出并受到广泛关注。然而，由于缺乏明确的 CA 范围，导致各种技术分散，难以形成系统的技术协同来赋能业务应用。在本文中，我们对 CA 任务进行了彻底的回顾和系统化，以总结现有的相关工作。具体来说，我们正式定义 CA 任务以应对该领域的碎片化和混乱局面，并推导出 CA 的四个关键步骤，从对话场景重建到深入的归因分析，再到进行有针对性的训练，最后根据有针对性的训练生成对话以实现特定目标。此外，我们展示了相关的基准，讨论了潜在的挑战，并指出了工业界和学术界的未来方向。从目前的进展来看，显然大部分的努力仍然集中在浅显的对话要素分析上，这在研究和商业之间留下了相当大的空白，而在法学硕士的协助下，最近的工作已经呈现出对因果关系和策略任务的研究趋势，这些研究更为复杂和高水平。分析的经验和见解必然会在以对话日志为目标的商业运作中具有更广泛的应用价值。</li>
</ul>

<h3>Title: Data-centric NLP Backdoor Defense from the Lens of Memorization</h3>
<ul>
<li><strong>Authors: </strong>Zhenting Wang, Zhizhi Wang, Mingyu Jin, Mengnan Du, Juan Zhai, Shiqing Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14200">https://arxiv.org/abs/2409.14200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14200">https://arxiv.org/pdf/2409.14200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14200]] Data-centric NLP Backdoor Defense from the Lens of Memorization(https://arxiv.org/abs/2409.14200)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Backdoor attack is a severe threat to the trustworthiness of DNN-based language models. In this paper, we first extend the definition of memorization of language models from sample-wise to more fine-grained sentence element-wise (e.g., word, phrase, structure, and style), and then point out that language model backdoors are a type of element-wise memorization. Through further analysis, we find that the strength of such memorization is positively correlated to the frequency of duplicated elements in the training dataset. In conclusion, duplicated sentence elements are necessary for successful backdoor attacks. Based on this, we propose a data-centric defense. We first detect trigger candidates in training data by finding memorizable elements, i.e., duplicated elements, and then confirm real triggers by testing if the candidates can activate backdoor behaviors (i.e., malicious elements). Results show that our method outperforms state-of-the-art defenses in defending against different types of NLP backdoors.</li>
<li><strong>摘要：</strong>后门攻击是对基于 DNN 的语言模型可信度的严重威胁。在本文中，我们首先将语言模型的记忆定义从样本级扩展到更细粒度的句子元素级（例如单词、短语、结构和风格），然后指出语言模型后门是一种元素级记忆。通过进一步分析，我们发现这种记忆的强度与训练数据集中重复元素的频率呈正相关。综上所述，重复的句子元素是成功的后门攻击的必要条件。基于此，我们提出了一种以数据为中心的防御方法。我们首先通过查找可记忆的元素（即重复元素）来检测训练数据中的触发候选，然后通过测试候选是否能够激活后门行为（即恶意元素）来确认真正的触发因素。结果表明，我们的方法在防御不同类型的 NLP 后门方面优于最先进的防御方法。</li>
</ul>

<h3>Title: Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Javier Chiyah-Garcia, Alessandro Suglia, Arash Eshghi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14247">https://arxiv.org/abs/2409.14247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14247">https://arxiv.org/pdf/2409.14247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14247]] Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models(https://arxiv.org/abs/2409.14247)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generisability. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction.</li>
<li><strong>摘要：</strong>在对话中，接收者最初可能会误解说话者的意思并做出错误回应，这通常会促使说话者在下一轮使用第三位置修复 (TPR) 来纠正误解。因此，处理此类修复序列并做出适当响应的能力在对话式 AI 系统中至关重要。在本文中，我们首先收集、分析并公开发布 BlockWorld-Repairs：一个多模态 TPR 序列数据集，用于指令跟随操作任务，该任务在设计上充满了指称歧义。我们使用此数据集在多种设置中评估几种最先进的视觉和语言模型 (VLM)，重点关注它们处理和准确响应 TPR 的能力，从而从错误传达中恢复。我们发现，与人类相比，所有模型在这项任务中的表现都明显不佳。然后，我们表明，VLM 可以从微调过程中针对相关标记的专门损失中受益，从而实现更好的性能和可通用性。我们的结果表明，这些模型尚未准备好部署在经常进行修复的多模式协作环境中，并强调需要设计有利于从交互中学习的训练制度和目标。</li>
</ul>

<h3>Title: Instruction Following without Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>John Hewitt, Nelson F. Liu, Percy Liang, Christopher D. Manning</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14254">https://arxiv.org/abs/2409.14254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14254">https://arxiv.org/pdf/2409.14254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14254]] Instruction Following without Instruction Tuning(https://arxiv.org/abs/2409.14254)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning commonly means finetuning a language model on instruction-response pairs. We discover two forms of adaptation (tuning) that are deficient compared to instruction tuning, yet still yield instruction following; we call this implicit instruction tuning. We first find that instruction-response pairs are not necessary: training solely on responses, without any corresponding instructions, yields instruction following. This suggests pretrained models have an instruction-response mapping which is revealed by teaching the model the desired distribution of responses. However, we then find it's not necessary to teach the desired distribution of responses: instruction-response training on narrow-domain data like poetry still leads to broad instruction-following behavior like recipe generation. In particular, when instructions are very different from those in the narrow finetuning domain, models' responses do not adhere to the style of the finetuning domain. To begin to explain implicit instruction tuning, we hypothesize that very simple changes to a language model's distribution yield instruction following. We support this by hand-writing a rule-based language model which yields instruction following in a product-of-experts with a pretrained model. The rules are to slowly increase the probability of ending the sequence, penalize repetition, and uniformly change 15 words' probabilities. In summary, adaptations made without being designed to yield instruction following can do so implicitly.</li>
<li><strong>摘要：</strong>指令调整通常意味着对指令-响应对语言模型进行微调。我们发现两种形式的适应（调整）与指令调整相比有所欠缺，但仍能产生指令跟随；我们称之为隐式指令调整。我们首先发现指令-响应对不是必需的：仅对响应进行训练，没有任何相应的指令，就能产生指令跟随。这表明预训练模型具有指令-响应映射，通过向模型传授所需的响应分布可以揭示该映射。然而，我们发现没有必要传授所需的响应分布：对诗歌等窄域数据进行指令-响应训练仍然会导致广泛的指令跟随行为，如食谱生成。特别是，当指令与窄微调域中的指令非常不同时，模型的响应不遵循微调域的风格。为了开始解释隐式指令调整，我们假设对语言模型分布的非常简单的更改会产生指令跟随。我们通过手写基于规则的语言模型来支持这一点，该模型使用预训练模型在专家产品中产生指令跟随。规则是慢慢增加结束序列的概率，惩罚重复，并统一改变 15 个单词的概率。总之，没有设计为产生指令跟随的调整可以隐式地产生指令跟随。</li>
</ul>

<h3>Title: ESPERANTO: Evaluating Synthesized Phrases to Enhance Robustness in AI Detection for Text Origination</h3>
<ul>
<li><strong>Authors: </strong>Navid Ayoobi, Lily Knab, Wen Cheng, David Pantoja, Hamidreza Alikhani, Sylvain Flamant, Jin Kim, Arjun Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14285">https://arxiv.org/abs/2409.14285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14285">https://arxiv.org/pdf/2409.14285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14285]] ESPERANTO: Evaluating Synthesized Phrases to Enhance Robustness in AI Detection for Text Origination(https://arxiv.org/abs/2409.14285)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) exhibit significant utility across various domains, they simultaneously are susceptible to exploitation for unethical purposes, including academic misconduct and dissemination of misinformation. Consequently, AI-generated text detection systems have emerged as a countermeasure. However, these detection mechanisms demonstrate vulnerability to evasion techniques and lack robustness against textual manipulations. This paper introduces back-translation as a novel technique for evading detection, underscoring the need to enhance the robustness of current detection systems. The proposed method involves translating AI-generated text through multiple languages before back-translating to English. We present a model that combines these back-translated texts to produce a manipulated version of the original AI-generated text. Our findings demonstrate that the manipulated text retains the original semantics while significantly reducing the true positive rate (TPR) of existing detection methods. We evaluate this technique on nine AI detectors, including six open-source and three proprietary systems, revealing their susceptibility to back-translation manipulation. In response to the identified shortcomings of existing AI text detectors, we present a countermeasure to improve the robustness against this form of manipulation. Our results indicate that the TPR of the proposed method declines by only 1.85% after back-translation manipulation. Furthermore, we build a large dataset of 720k texts using eight different LLMs. Our dataset contains both human-authored and LLM-generated texts in various domains and writing styles to assess the performance of our method and existing detectors. This dataset is publicly shared for the benefit of the research community.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在各个领域都表现出显著的实用性，但它们同时也容易被利用来达到不道德的目的，包括学术不端行为和传播错误信息。因此，人工智能生成的文本检测系统应运而生，成为一种应对措施。然而，这些检测机制表现出对逃避技术的脆弱性，并且缺乏对文本操纵的鲁棒性。本文介绍了反向翻译作为一种逃避检测的新技术，强调需要提高当前检测系统的鲁棒性。所提出的方法包括将人工智能生成的文本翻译成多种语言，然后再翻译成英语。我们提出了一个模型，该模型将这些反向翻译的文本结合起来，生成原始人工智能生成文本的操纵版本。我们的研究结果表明，操纵后的文本保留了原始语义，同时显著降低了现有检测方法的真正率 (TPR)。我们在九个人工智能检测器上评估了这项技术，包括六个开源系统和三个专有系统，揭示了它们对反向翻译操纵的敏感性。针对现有 AI 文本检测器的缺陷，我们提出了一种对策来提高对这种操纵形式的鲁棒性。我们的结果表明，在反向翻译操纵之后，所提出方法的 TPR 仅下降了 1.85%。此外，我们使用八个不同的 LLM 构建了一个包含 720k 文本的大型数据集。我们的数据集包含各种领域和写作风格的人工编写文本和 LLM 生成的文本，以评估我们的方法和现有检测器的性能。该数据集公开共享，以造福研究界。</li>
</ul>

<h3>Title: PretextTrans: Investigating Medical Factual Knowledge Mastery of LLMs with Predicate-text Dual Transformation</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Zhou, Xien Liu, Chen Ning, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14302">https://arxiv.org/abs/2409.14302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14302">https://arxiv.org/pdf/2409.14302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14302]] PretextTrans: Investigating Medical Factual Knowledge Mastery of LLMs with Predicate-text Dual Transformation(https://arxiv.org/abs/2409.14302)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In the study, we aim to investigate current LLMs' mastery of medical factual knowledge with a dynamic evaluation schema, which can automatically generate multiple test samples for each medical factual knowledge point. Test samples produced directly by LLMs always introduce factual errors and lack diversity in the manner of knowledge expression. To overcome the drawbacks, here we propose a novel evaluation method, Predicate-text Dual Transformation (PretextTrans), by introducing predicate transformations into the dynamic evaluation schema. Specifically, each medical knowledge point is firstly transformed into a predicate expression; then, the predicate expression derives a series of variants through predicate transformations; lastly, the produced predicate variants are transformed back into textual expressions, resulting in a series of test samples with both factual reliability and expression diversity. Using the proposed PretextTrans method, we systematically investigate 12 well-known LLMs' mastery of medical factual knowledge based on two medical datasets. The comparison results show that current LLMs still have significant deficiencies in fully mastering medical knowledge, which may illustrate why current LLMs still perform unsatisfactorily in real-world medical scenarios despite having achieved considerable performance on public benchmarks. Our proposed method serves as an effective solution for evaluation of LLMs in medical domain and offers valuable insights for developing medical-specific LLMs.</li>
<li><strong>摘要：</strong>本研究旨在通过一种动态评估模式考察当前法学硕士对医学事实知识的掌握程度，该模式可以为每个医学事实知识点自动生成多个测试样本。直接由法学硕士生成的测试样本往往会引入事实错误，并且在知识表达方式上缺乏多样性。为了克服这些缺点，我们提出了一种新的评估方法——谓词-文本双重转换（PretextTrans），通过在动态评估模式中引入谓词转换。具体来说，每个医学知识点首先被转换成谓词表达式；然后，谓词表达式通过谓词转换派生出一系列变体；最后，将产生的谓词变体转换回文本表达式，从而产生一系列既具有事实可靠性又具有表达多样性的测试样本。使用所提出的PretextTrans方法，我们基于两个医学数据集系统地考察了12位知名法学硕士对医学事实知识的掌握程度。比较结果表明，目前的法学硕士在全面掌握医学知识方面仍然存在重大缺陷，这可能解释了为什么尽管目前的法学硕士在公共基准上取得了相当不错的成绩，但在现实医学场景中的表现仍然不尽如人意。我们提出的方法为评估医学领域的法学硕士提供了一种有效的解决方案，并为开发医学专用的法学硕士提供了宝贵的见解。</li>
</ul>

<h3>Title: Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses</h3>
<ul>
<li><strong>Authors: </strong>Hung-Ting Su, Ya-Ching Hsu, Xudong Lin, Xiang-Qian Shi, Yulei Niu, Han-Yuan Hsu, Hung-yi Lee, Winston H. Hsu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14324">https://arxiv.org/abs/2409.14324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14324">https://arxiv.org/pdf/2409.14324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14324]] Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses(https://arxiv.org/abs/2409.14324)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting have shown significant multi-step reasoning capabilities in factual content like mathematics, commonsense, and logic. However, their performance in narrative reasoning, which demands greater abstraction capabilities, remains unexplored. This study utilizes tropes in movie synopses to assess the abstract reasoning abilities of state-of-the-art LLMs and uncovers their low performance. We introduce a trope-wise querying approach to address these challenges and boost the F1 score by 11.8 points. Moreover, while prior studies suggest that CoT enhances multi-step reasoning, this study shows CoT can cause hallucinations in narrative content, reducing GPT-4's performance. We also introduce an Adversarial Injection method to embed trope-related text tokens into movie synopses without explicit tropes, revealing CoT's heightened sensitivity to such injections. Our comprehensive analysis provides insights for future research directions.</li>
<li><strong>摘要：</strong>配备思路链 (CoT) 提示的大型语言模型 (LLM) 在数学、常识和逻辑等事实内容中表现出显著的多步推理能力。然而，它们在需要更高抽象能力的叙事推理中的表现仍未得到探索。本研究利用电影概要中的比喻来评估最先进的 LLM 的抽象推理能力，并揭示了它们的低性能。我们引入了一种逐个比喻的查询方法来应对这些挑战，并将 F1 分数提高了 11.8 分。此外，虽然先前的研究表明 CoT 增强了多步推理能力，但这项研究表明 CoT 会在叙事内容中引起幻觉，从而降低 GPT-4 的性能。我们还引入了一种对抗性注入方法，将与比喻相关的文本标记嵌入没有明确比喻的电影概要中，揭示了 CoT 对此类注入的高度敏感性。我们的全面分析为未来的研究方向提供了见解。</li>
</ul>

<h3>Title: MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Lu, Liang Ding, Kanjian Zhang, Jinxia Zhang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14335">https://arxiv.org/abs/2409.14335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14335">https://arxiv.org/pdf/2409.14335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14335]] MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators(https://arxiv.org/abs/2409.14335)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown significant potential as judges for Machine Translation (MT) quality assessment, providing both scores and fine-grained feedback. Although approaches such as GEMBA-MQM has shown SOTA performance on reference-free evaluation, the predicted errors do not align well with those annotated by human, limiting their interpretability as feedback signals. To enhance the quality of error annotations predicted by LLM evaluators, we introduce a universal and training-free framework, $\textbf{MQM-APE}$, based on the idea of filtering out non-impactful errors by Automatically Post-Editing (APE) the original translation based on each error, leaving only those errors that contribute to quality improvement. Specifically, we prompt the LLM to act as 1) $\textit{evaluator}$ to provide error annotations, 2) $\textit{post-editor}$ to determine whether errors impact quality improvement and 3) $\textit{pairwise quality verifier}$ as the error filter. Experiments show that our approach consistently improves both the reliability and quality of error spans against GEMBA-MQM, across eight LLMs in both high- and low-resource languages. Orthogonal to trained approaches, MQM-APE complements translation-specific evaluators such as Tower, highlighting its broad applicability. Further analysis confirm the effectiveness of each module and offer valuable insights into evaluator design and LLMs selection. The code will be released to facilitate the community.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出作为机器翻译 (MT) 质量评估评判者的巨大潜力，既能提供分数，又能提供细粒度的反馈。尽管 GEMBA-MQM 等方法在无参考评估中表现出 SOTA 性能，但预测的错误与人类注释的错误并不完全一致，限制了它们作为反馈信号的可解释性。为了提高 LLM 评估器预测的错误注释的质量，我们引入了一个通用且无需训练的框架 $\textbf{MQM-APE}$，其理念是通过根据每个错误自动后期编辑 (APE) 原始翻译来过滤掉不具影响的错误，只留下有助于质量改进的错误。具体来说，我们提示 LLM 充当 1) $\textit{评估器}$ 以提供错误注释，2) $\textit{后期编辑器}$ 以确定错误是否影响质量改进，以及 3) $\textit{成对质量验证器}$ 作为错误过滤器。实验表明，我们的方法在高资源和低资源语言的 8 个 LLM 中持续提高了与 GEMBA-MQM 相比的错误跨度可靠性和质量。MQM-APE 与经过训练的方法正交，是对 Tower 等翻译专用评估器的补充，突出了其广泛的适用性。进一步的分析证实了每个模块的有效性，并为评估器设计和 LLM 选择提供了宝贵的见解。代码将发布以方便社区使用。</li>
</ul>

<h3>Title: More Effective LLM Compressed Tokens with Uniformly Spread Position Identifiers and Compression Loss</h3>
<ul>
<li><strong>Authors: </strong>Runsong Zhao, Pengcheng Huang, Xinyu Liu, Chunyang Xiao, Tong Xiao, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14364">https://arxiv.org/abs/2409.14364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14364">https://arxiv.org/pdf/2409.14364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14364]] More Effective LLM Compressed Tokens with Uniformly Spread Position Identifiers and Compression Loss(https://arxiv.org/abs/2409.14364)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Compressing Transformer inputs into compressd tokens allows running LLMs with improved speed and cost efficiency. Based on the compression method ICAE, we carefully examine the position identifier choices for compressed tokens and also propose a new compression loss. We demonstrate empirically that our proposed methods achieve significantly higher compression ratios (15x compared to 4x for ICAE), while being able to attain comparable reconstruction performance.</li>
<li><strong>摘要：</strong>将 Transformer 输入压缩为压缩标记可以提高 LLM 的运行速度和成本效率。基于压缩方法 ICAE，我们仔细研究了压缩标记的位置标识符选择，并提出了一种新的压缩损失。我们通过实证证明，我们提出的方法实现了显著更高的压缩比（15 倍，而 ICAE 为 4 倍），同时能够获得相当的重建性能。</li>
</ul>

<h3>Title: The Ability of Large Language Models to Evaluate Constraint-satisfaction in Agent Responses to Open-ended Requests</h3>
<ul>
<li><strong>Authors: </strong>Lior Madmoni, Amir Zait, Ilia Labzovsky, Danny Karmon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14371">https://arxiv.org/abs/2409.14371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14371">https://arxiv.org/pdf/2409.14371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14371]] The Ability of Large Language Models to Evaluate Constraint-satisfaction in Agent Responses to Open-ended Requests(https://arxiv.org/abs/2409.14371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Generative AI agents are often expected to respond to complex user requests that have No One Right Answer (NORA), e.g., "design a vegetarian meal plan below 1800 calories". Such requests may entail a set of constraints that the agent should adhere to. To successfully develop agents for NORA scenarios, an accurate automatic evaluation framework is essential, and specifically - one capable of validating the satisfaction of constraints in the agent's response. Recently, large language models (LLMs) have been adopted as versatile evaluators for many NORA tasks, but their ability to evaluate constraint-satisfaction in generated text remains unclear. To study this, we develop and release a novel Arithmetic Constraint-Satisfaction (ACS) benchmarking dataset. The dataset consists of complex user requests with corresponding constraints, agent responses and human labels indicating each constraint's satisfaction level in the response. A unique property of this dataset is that validating many of its constraints requires reviewing the response as a whole (in contrast to many other benchmarks that require the validation of a single independent item). Moreover, it assesses LLMs in performing reasoning, in-context data extraction, arithmetic calculations, and counting. We then benchmark both open and proprietary LLMs on evaluating constraint-satisfaction, and show that most models still have a significant headroom for improvement, and that errors primarily stem from reasoning issues. In addition, most models exhibit a skewed constraint-satisfaction prediction pattern, with higher accuracy where the ground-truth label is "satisfied". Lastly, few-shot prompting for our task proved to be rather challenging, since many of the studied models showed a degradation in performance when it was introduced.</li>
<li><strong>摘要：</strong>生成式 AI 代理通常需要响应没有唯一正确答案 (NORA) 的复杂用户请求，例如“设计一份低于 1800 卡路里的素食餐计划”。此类请求可能需要代理必须遵守的一组约束。要成功开发用于 NORA 场景的代理，准确的自动评估框架必不可少，具体来说，该框架能够验证代理响应中约束的满足情况。最近，大型语言模型 (LLM) 已被用作许多 NORA 任务的多功能评估器，但它们评估生成文本中约束满足情况的能力仍不清楚。为了研究这一点，我们开发并发布了一个新颖的算术约束满足 (ACS) 基准数据集。该数据集由复杂的用户请求和相应的约束、代理响应和指示响应中每个约束的满足程度的人工标签组成。该数据集的一个独特属性是，验证其许多约束需要审查整个响应（与许多其他需要验证单个独立项目的基准不同）。此外，它还评估了 LLM 在执行推理、上下文数据提取、算术计算和计数方面的能力。然后，我们对开放和专有 LLM 进行了基准测试，以评估约束满足度，并表明大多数模型仍有很大改进空间，错误主要源于推理问题。此外，大多数模型都表现出一种倾斜的约束满足度预测模式，当基本事实标签为“满足”时，准确率更高。最后，对于我们的任务来说，小样本提示被证明是相当具有挑战性的，因为许多研究模型在引入它时都表现出性能下降。</li>
</ul>

<h3>Title: Investigating Layer Importance in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Yanfei Dong, Kenji Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14381">https://arxiv.org/abs/2409.14381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14381">https://arxiv.org/pdf/2409.14381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14381]] Investigating Layer Importance in Large Language Models(https://arxiv.org/abs/2409.14381)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in safety-critical scenarios and hindered the development of better models. In this study, we advance the understanding of LLM by investigating the significance of individual layers in LLMs. We propose an efficient sampling method to faithfully evaluate the importance of layers using Shapley values, a widely used explanation framework in feature attribution and data valuation. In addition, we conduct layer ablation experiments to assess the performance degradation resulting from the exclusion of specific layers. Our findings reveal the existence of cornerstone layers, wherein certain early layers can exhibit a dominant contribution over others. Removing one cornerstone layer leads to a drastic collapse of the model performance, often reducing it to random guessing. Conversely, removing non-cornerstone layers results in only marginal performance changes. This study identifies cornerstone layers in LLMs and underscores their critical role for future research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其出色的文本理解和处理能力而受到越来越多的关注。然而，LLM 在很大程度上仍然不透明。对 LLM 缺乏了解阻碍了其在安全关键场景中的部署，并阻碍了更好模型的开发。在本研究中，我们通过研究 LLM 中各个层的重要性来加深对 LLM 的理解。我们提出了一种有效的采样方法，使用 Shapley 值忠实地评估层的重要性，Shapley 值是特征归因和数据评估中广泛使用的解释框架。此外，我们进行了层消融实验，以评估排除特定层导致的性能下降。我们的研究结果揭示了基石层的存在，其中某些早期层可以表现出比其他层更突出的贡献。删除一个基石层会导致模型性能急剧下降，通常会将其降低为随机猜测。相反，删除非基石层只会导致性能略有变化。本研究确定了 LLM 中的基石层，并强调了它们在未来研究中的关键作用。</li>
</ul>

<h3>Title: Predicting User Stances from Target-Agnostic Information using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Brandon Loh, Liang Ze Wong, Prasanta Bhattacharya, Joseph Simons, Wei Gao, Hong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14395">https://arxiv.org/abs/2409.14395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14395">https://arxiv.org/pdf/2409.14395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14395]] Predicting User Stances from Target-Agnostic Information using Large Language Models(https://arxiv.org/abs/2409.14395)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We investigate Large Language Models' (LLMs) ability to predict a user's stance on a target given a collection of his/her target-agnostic social media posts (i.e., user-level stance prediction). While we show early evidence that LLMs are capable of this task, we highlight considerable variability in the performance of the model across (i) the type of stance target, (ii) the prediction strategy and (iii) the number of target-agnostic posts supplied. Post-hoc analyses further hint at the usefulness of target-agnostic posts in providing relevant information to LLMs through the presence of both surface-level (e.g., target-relevant keywords) and user-level features (e.g., encoding users' moral values). Overall, our findings suggest that LLMs might offer a viable method for determining public stances towards new topics based on historical and target-agnostic data. At the same time, we also call for further research to better understand LLMs' strong performance on the stance prediction task and how their effectiveness varies across task contexts.</li>
<li><strong>摘要：</strong>我们研究了大型语言模型 (LLM) 预测用户对目标的立场的能力，给定一组目标无关的社交媒体帖子（即用户级立场预测）。虽然我们展示了 LLM 能够完成这项任务的早期证据，但我们强调了模型在 (i) 立场目标类型、(ii) 预测策略和 (iii) 提供的目标无关帖子数量方面的表现存在相当大的差异。事后分析进一步暗示了目标无关帖子在通过表面级（例如，目标相关关键字）和用户级特征（例如，编码用户的道德价值观）的存在为 LLM 提供相关信息方面的实用性。总体而言，我们的研究结果表明 LLM 可能提供一种可行的方法，可根据历史和目标无关数据确定公众对新主题的立场。同时，我们也呼吁进一步研究，以更好地了解 LLM 在立场预测任务上的强劲表现，以及它们的有效性如何随着任务环境的变化而变化。</li>
</ul>

<h3>Title: Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations</h3>
<ul>
<li><strong>Authors: </strong>Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14399">https://arxiv.org/abs/2409.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14399">https://arxiv.org/pdf/2409.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14399]] Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations(https://arxiv.org/abs/2409.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>With the aid of large language models, current conversational recommender system (CRS) has gaining strong abilities to persuade users to accept recommended items. While these CRSs are highly persuasive, they can mislead users by incorporating incredible information in their explanations, ultimately damaging the long-term trust between users and the CRS. To address this, we propose a simple yet effective method, called PC-CRS, to enhance the credibility of CRS's explanations during persuasion. It guides the explanation generation through our proposed credibility-aware persuasive strategies and then gradually refines explanations via post-hoc self-reflection. Experimental results demonstrate the efficacy of PC-CRS in promoting persuasive and credible explanations. Further analysis reveals the reason behind current methods producing incredible explanations and the potential of credible explanations to improve recommendation accuracy.</li>
<li><strong>摘要：</strong>借助大型语言模型，当前的对话式推荐系统 (CRS) 已经获得了强大的说服用户接受推荐项目的能力。虽然这些 CRS 具有很强的说服力，但它们可能会通过在解释中加入令人难以置信的信息来误导用户，最终损害用户和 CRS 之间的长期信任。为了解决这个问题，我们提出了一种简单而有效的方法，称为 PC-CRS，以在说服过程中提高 CRS 解释的可信度。它通过我们提出的可信度感知说服策略指导解释生成，然后通过事后自我反思逐步完善解释。实验结果证明了 PC-CRS 在促进有说服力和可信的解释方面的有效性。进一步的分析揭示了当前方法产生令人难以置信的解释背后的原因以及可信解释提高推荐准确性的潜力。</li>
</ul>

<h3>Title: Automotive innovation landscaping using LLM</h3>
<ul>
<li><strong>Authors: </strong>Raju Gorain, Omkar Salunke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14436">https://arxiv.org/abs/2409.14436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14436">https://arxiv.org/pdf/2409.14436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14436]] Automotive innovation landscaping using LLM(https://arxiv.org/abs/2409.14436)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The process of landscaping automotive innovation through patent analysis is crucial for Research and Development teams. It aids in comprehending innovation trends, technological advancements, and the latest technologies from competitors. Traditionally, this process required intensive manual efforts. However, with the advent of Large Language Models (LLMs), it can now be automated, leading to faster and more efficient patent categorization & state-of-the-art of inventive concept extraction. This automation can assist various R\&D teams in extracting relevant information from extensive patent databases. This paper introduces a method based on prompt engineering to extract essential information for landscaping. The information includes the problem addressed by the patent, the technology utilized, and the area of innovation within the vehicle ecosystem (such as safety, Advanced Driver Assistance Systems and more).The result demonstrates the implementation of this method to create a landscape of fuel cell technology using open-source patent data. This approach provides a comprehensive overview of the current state of fuel cell technology, offering valuable insights for future research and development in this field.</li>
<li><strong>摘要：</strong>通过专利分析来对汽车创新进行景观设计的过程对于研发团队至关重要。它有助于理解创新趋势、技术进步以及竞争对手的最新技术。传统上，这个过程需要大量的人工工作。然而，随着大型语言模型 (LLM) 的出现，现在可以实现自动化，从而实现更快、更高效的专利分类和最先进的发明概念提取。这种自动化可以帮助各种研发团队从广泛的专利数据库中提取相关信息。本文介绍了一种基于快速工程的方法来提取景观设计的基本信息。这些信息包括专利解决的问题、所使用的技术以及车辆生态系统中的创新领域（例如安全性、高级驾驶辅助系统等）。结果展示了该方法的实施，即使用开源专利数据来创建燃料电池技术的景观。这种方法全面概述了燃料电池技术的现状，为该领域未来的研究和开发提供了宝贵的见解。</li>
</ul>

<h3>Title: Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis</h3>
<ul>
<li><strong>Authors: </strong>Daoyang Li, Mingyu Jin, Qingcheng Zeng, Haiyan Zhao, Mengnan Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14459">https://arxiv.org/abs/2409.14459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14459">https://arxiv.org/pdf/2409.14459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14459]] Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis(https://arxiv.org/abs/2409.14459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Probing techniques for large language models (LLMs) have primarily focused on English, overlooking the vast majority of the world's languages. In this paper, we extend these probing methods to a multilingual context, investigating the behaviors of LLMs across diverse languages. We conduct experiments on several open-source LLM models, analyzing probing accuracy, trends across layers, and similarities between probing vectors for multiple languages. Our key findings reveal: (1) a consistent performance gap between high-resource and low-resource languages, with high-resource languages achieving significantly higher probing accuracy; (2) divergent layer-wise accuracy trends, where high-resource languages show substantial improvement in deeper layers similar to English; and (3) higher representational similarities among high-resource languages, with low-resource languages demonstrating lower similarities both among themselves and with high-resource languages. These results highlight significant disparities in LLMs' multilingual capabilities and emphasize the need for improved modeling of low-resource languages.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的探测技术主要集中在英语上，忽略了世界上绝大多数语言。在本文中，我们将这些探测方法扩展到多语言环境，研究 LLM 在不同语言中的行为。我们对几个开源 LLM 模型进行了实验，分析了探测准确性、跨层趋势以及多种语言探测向量之间的相似性。我们的主要发现表明：(1) 高资源语言和低资源语言之间存在一致的性能差距，高资源语言的探测准确性明显更高；(2) 分层准确性趋势不同，高资源语言在更深的层次上显示出与英语类似的显着改进；(3) 高资源语言之间的表征相似性更高，低资源语言在它们之间以及与高资源语言之间都表现出较低的相似性。这些结果突出了 LLM 多语言能力的显著差异，并强调需要改进低资源语言的建模。</li>
</ul>

<h3>Title: Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints</h3>
<ul>
<li><strong>Authors: </strong>Kaikai An, Shuzheng Si, Helan Hu, Haozhe Zhao, Yuchi Wang, Qingyan Guo, Baobao Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14469">https://arxiv.org/abs/2409.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14469">https://arxiv.org/pdf/2409.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14469]] Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints(https://arxiv.org/abs/2409.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Semantic Parsing aims to capture the meaning of a sentence and convert it into a logical, structured form. Previous studies show that semantic parsing enhances the performance of smaller models (e.g., BERT) on downstream tasks. However, it remains unclear whether the improvements extend similarly to LLMs. In this paper, our empirical findings reveal that, unlike smaller models, directly adding semantic parsing results into LLMs reduces their performance. To overcome this, we propose SENSE, a novel prompting approach that embeds semantic hints within the prompt. Experiments show that SENSE consistently improves LLMs' performance across various tasks, highlighting the potential of integrating semantic information to improve LLM capabilities.</li>
<li><strong>摘要：</strong>语义解析旨在捕捉句子的含义并将其转换为逻辑结构化的形式。先前的研究表明，语义解析可以提高小型模型（例如 BERT）在下游任务上的性能。然而，目前尚不清楚这种改进是否也适用于 LLM。在本文中，我们的实证结果表明，与小型模型不同，直接将语义解析结果添加到 LLM 中会降低其性能。为了克服这个问题，我们提出了 SENSE，这是一种新颖的提示方法，可在提示中嵌入语义提示。实验表明，SENSE 可以持续提高 LLM 在各种任务上的性能，凸显了集成语义信息以提高 LLM 功能的潜力。</li>
</ul>

<h3>Title: Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Chenxu Wang, Ping Jian, Yang Zhen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14495">https://arxiv.org/abs/2409.14495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14495">https://arxiv.org/pdf/2409.14495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14495]] Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension(https://arxiv.org/abs/2409.14495)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Logical reading comprehension is a challenging task that entails grasping the underlying semantics of text and applying reasoning to deduce the correct answer. Prior researches have primarily focused on enhancing logical reasoning capabilities through Chain-of-Thought (CoT) or data augmentation. However, previous work constructing chain-of-thought rationales concentrates solely on analyzing correct options, neglecting the incorrect alternatives. Addtionally, earlier efforts on data augmentation by altering contexts rely on rule-based methods, which result in generated contexts that lack diversity and coherence. To address these issues, we propose a Premise-Oriented Data Augmentation (PODA) framework. This framework can generate CoT rationales including analyses for both correct and incorrect options, while constructing diverse and high-quality counterfactual contexts from incorrect candidate options. We integrate summarizing premises and identifying premises for each option into rationales. Subsequently, we employ multi-step prompts with identified premises to construct counterfactual context. To facilitate the model's capabilities to better differentiate the reasoning process associated with each option, we introduce a novel thought-path contrastive learning method that compares reasoning paths between the original and counterfactual samples. Experimental results on three representative LLMs demonstrate that our method can improve the baselines substantially across two challenging logical reasoning benchmarks (ReClor and LogiQA 2.0). The data and code are released at this https URL.</li>
<li><strong>摘要：</strong>逻辑阅读理解是一项具有挑战性的任务，它需要掌握文本的底层语义并运用推理来推断出正确答案。先前的研究主要集中在通过思路链 (CoT) 或数据增强来增强逻辑推理能力。然而，以前构建思路链原理的工作仅仅集中在分析正确的选项，而忽略了错误的选项。此外，早期通过改变上下文进行数据增强的努力依赖于基于规则的方法，这导致生成的上下文缺乏多样性和连贯性。为了解决这些问题，我们提出了一个面向前提的数据增强 (PODA) 框架。该框架可以生成 CoT 原理，包括对正确和错误选项的分析，同时从错误的候选选项中构建多样化和高质量的反事实上下文。我们将总结前提和识别每个选项的前提整合到原理中。随后，我们使用具有已识别前提的多步骤提示来构建反事实上下文。为了提高模型更好地区分与每个选项相关的推理过程的能力，我们引入了一种新颖的思维路径对比学习方法，该方法比较原始样本和反事实样本之间的推理路径。在三个代表性 LLM 上的实验结果表明，我们的方法可以在两个具有挑战性的逻辑推理基准（ReClor 和 LogiQA 2.0）中显著提高基线。数据和代码在此 https URL 上发布。</li>
</ul>

<h3>Title: A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, Joseph Bloom</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14507">https://arxiv.org/abs/2409.14507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14507">https://arxiv.org/pdf/2409.14507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14507]] A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders(https://arxiv.org/abs/2409.14507)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to what extent does varying the sparsity or the size of the SAE affect monosemanticity / interpretability? By investigating these questions in the context of a simple first-letter identification task where we have complete access to ground truth labels for all tokens in the vocabulary, we are able to provide more detail than prior investigations. Critically, we identify a problematic form of feature-splitting we call feature absorption where seemingly monosemantic latents fail to fire in cases where they clearly should. Our investigation suggests that varying SAE size or sparsity is insufficient to solve this issue, and that there are deeper conceptual issues in need of resolution.</li>
<li><strong>摘要：</strong>稀疏自动编码器 (SAE) 已成为一种有前途的方法，可将大型语言模型 (LLM) 的激活分解为人类可解释的潜在激活。在本文中，我们提出了两个问题。首先，SAE 在多大程度上提取了单义和可解释的潜在激活？其次，改变 SAE 的稀疏度或大小会在多大程度上影响单义性 / 可解释性？通过在简单的首字母识别任务的背景下调查这些问题，我们可以完全访问词汇表中所有标记的真实标签，我们能够提供比之前调查更详细的信息。至关重要的是，我们发现了一种有问题的特征分裂形式，我们称之为特征吸收，在这种形式中，看似单义的潜在激活在应该激活的情况下却无法激活。我们的调查表明，改变 SAE 的大小或稀疏度不足以解决这个问题，还有更深层次的概念问题需要解决。</li>
</ul>

<h3>Title: Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits</h3>
<ul>
<li><strong>Authors: </strong>Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14509">https://arxiv.org/abs/2409.14509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14509">https://arxiv.org/pdf/2409.14509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14509]] Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits(https://arxiv.org/abs/2409.14509)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>LLM-based applications are helping people write, and LLM-generated text is making its way into social media, journalism, and our classrooms. However, the differences between LLM-generated and human-written text remain unclear. To explore this, we hired professional writers to edit paragraphs in several creative domains. We first found these writers agree on undesirable idiosyncrasies in LLM-generated text, formalizing it into a seven-category taxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP corpus: 1,057 LLM-generated paragraphs edited by professional writers according to our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our study (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms of writing quality, revealing common limitations across model families. Third, we explored automatic editing methods to improve LLM-generated text. A large-scale preference annotation confirms that although experts largely prefer text edited by other experts, automatic editing methods show promise in improving alignment between LLM-generated and human-written text.</li>
<li><strong>摘要：</strong>基于 LLM 的应用程序正在帮助人们写作，而 LLM 生成的文本正在进入社交媒体、新闻业和我们的课堂。然而，LLM 生成的文本和人工编写的文本之间的差异仍然不清楚。为了探索这一点，我们聘请了专业作家来编辑几个创意领域的段落。我们首先发现这些作家一致认为 LLM 生成的文本中存在不良特质，并将其形式化为七类分类法（例如陈词滥调、不必要的说明）。其次，我们整理了 LAMP 语料库：1,057 个 LLM 生成的段落由专业作家根据我们的分类法编辑。对 LAMP 的分析表明，我们研究中使用的 LLM（GPT4o、Claude-3.5-Sonnet、Llama-3.1-70b）在写作质量方面没有一个表现得更好，揭示了模型系列之间的共同局限性。第三，我们探索了自动编辑方法来改进 LLM 生成的文本。大规模偏好注释证实，尽管专家大多偏好由其他专家编辑的文本，但自动编辑方法有望改善 LLM 生成的文本与人类编写的文本之间的一致性。</li>
</ul>

<h3>Title: Evaluating the Performance and Robustness of LLMs in Materials Science Q&A and Property Predictions</h3>
<ul>
<li><strong>Authors: </strong>Hongchen Wang, Kangming Li, Scott Ramsay, Yao Fehlis, Edward Kim, Jason Hattrick-Simpers</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14572">https://arxiv.org/abs/2409.14572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14572">https://arxiv.org/pdf/2409.14572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14572]] Evaluating the Performance and Robustness of LLMs in Materials Science Q&A and Property Predictions(https://arxiv.org/abs/2409.14572)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have the potential to revolutionize scientific research, yet their robustness and reliability in domain-specific applications remain insufficiently explored. This study conducts a comprehensive evaluation and robustness analysis of LLMs within the field of materials science, focusing on domain-specific question answering and materials property prediction. Three distinct datasets are used in this study: 1) a set of multiple-choice questions from undergraduate-level materials science courses, 2) a dataset including various steel compositions and yield strengths, and 3) a band gap dataset, containing textual descriptions of material crystal structures and band gap values. The performance of LLMs is assessed using various prompting strategies, including zero-shot chain-of-thought, expert prompting, and few-shot in-context learning. The robustness of these models is tested against various forms of 'noise', ranging from realistic disturbances to intentionally adversarial manipulations, to evaluate their resilience and reliability under real-world conditions. Additionally, the study uncovers unique phenomena of LLMs during predictive tasks, such as mode collapse behavior when the proximity of prompt examples is altered and performance enhancement from train/test mismatch. The findings aim to provide informed skepticism for the broad use of LLMs in materials science and to inspire advancements that enhance their robustness and reliability for practical applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 有可能彻底改变科学研究，但它们在特定领域应用中的稳健性和可靠性仍未得到充分探索。本研究对材料科学领域的 LLM 进行了全面的评估和稳健性分析，重点关注特定领域的问答和材料性能预测。本研究使用了三个不同的数据集：1) 一组来自本科材料科学课程的多项选择题，2) 一个包括各种钢成分和屈服强度的数据集，3) 一个带隙数据集，包含材料晶体结构和带隙值的文本描述。使用各种提示策略评估 LLM 的性能，包括零样本思维链、专家提示和少量上下文学习。这些模型的稳健性针对各种形式的“噪声”进行了测试，从现实干扰到故意对抗的操纵，以评估它们在现实条件下的弹性和可靠性。此外，该研究还揭示了 LLM 在预测任务中的独特现象，例如当提示示例的接近度发生变化时模式崩溃的行为以及训练/测试不匹配导致的性能增强。研究结果旨在为 LLM 在材料科学中的广泛应用提供明智的怀疑态度，并激发改进，提高其在实际应用中的稳健性和可靠性。</li>
</ul>

<h3>Title: EchoAtt: Attend, Copy, then Adjust for More Efficient Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hossein Rajabzadeh, Aref Jafari, Aman Sharma, Benyamin Jami, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14595">https://arxiv.org/abs/2409.14595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14595">https://arxiv.org/pdf/2409.14595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14595]] EchoAtt: Attend, Copy, then Adjust for More Efficient Large Language Models(https://arxiv.org/abs/2409.14595)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), with their increasing depth and number of parameters, have demonstrated outstanding performance across a variety of natural language processing tasks. However, this growth in scale leads to increased computational demands, particularly during inference and fine-tuning. To address these challenges, we introduce EchoAtt, a novel framework aimed at optimizing transformer-based models by analyzing and leveraging the similarity of attention patterns across layers. Our analysis reveals that many inner layers in LLMs, especially larger ones, exhibit highly similar attention matrices. By exploiting this similarity, EchoAtt enables the sharing of attention matrices in less critical layers, significantly reducing computational requirements without compromising performance. We incorporate this approach within a knowledge distillation setup, where a pre-trained teacher model guides the training of a smaller student model. The student model selectively shares attention matrices in layers with high similarity while inheriting key parameters from the teacher. Our best results with TinyLLaMA-1.1B demonstrate that EchoAtt improves inference speed by 15\%, training speed by 25\%, and reduces the number of parameters by approximately 4\%, all while improving zero-shot performance. These findings highlight the potential of attention matrix sharing to enhance the efficiency of LLMs, making them more practical for real-time and resource-limited applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的深度和参数数量不断增加，在各种自然语言处理任务中表现出色。然而，这种规模的增长导致计算需求增加，特别是在推理和微调期间。为了应对这些挑战，我们推出了 EchoAtt，这是一个新颖的框架，旨在通过分析和利用跨层注意力模式的相似性来优化基于 Transformer 的模型。我们的分析表明，LLM 中的许多内层，尤其是较大的内层，表现出高度相似的注意力矩阵。通过利用这种相似性，EchoAtt 能够在较不重要的层中共享注意力矩阵，从而显着降低计算要求而不会影响性能。我们将这种方法融入知识蒸馏设置中，其中预先训练的教师模型指导较小的学生模型的训练。学生模型有选择地在具有高相似性的层中共享注意力矩阵，同时从教师那里继承关键参数。我们使用 TinyLLaMA-1.1B 获得的最佳结果表明，EchoAtt 将推理速度提高了 15%，训练速度提高了 25%，并将参数数量减少了约 4%，同时还提高了零样本性能。这些发现凸显了注意力矩阵共享在提高 LLM 效率方面的潜力，使其更适用于实时和资源有限的应用。</li>
</ul>

<h3>Title: Can pre-trained language models generate titles for research papers?</h3>
<ul>
<li><strong>Authors: </strong>Tohida Rehman, Debarshi Kumar Sanyal, Samiran Chattopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14602">https://arxiv.org/abs/2409.14602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14602">https://arxiv.org/pdf/2409.14602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14602]] Can pre-trained language models generate titles for research papers?(https://arxiv.org/abs/2409.14602)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>The title of a research paper communicates in a succinct style the main theme and, sometimes, the findings of the paper. Coming up with the right title is often an arduous task, and therefore, it would be beneficial to authors if title generation can be automated. In this paper, we fine-tune pre-trained and large language models to generate titles of papers from their abstracts. We also use ChatGPT in a zero-shot setting to generate paper titles. The performance of the models is measured with ROUGE, METEOR, MoverScore, BERTScore and SciBERTScore metrics.</li>
<li><strong>摘要：</strong>研究论文的标题以简洁的风格传达了论文的主题，有时还传达了论文的研究结果。想出合适的标题通常是一项艰巨的任务，因此，如果标题生成可以自动化，对作者来说将大有裨益。在本文中，我们对预训练和大型语言模型进行了微调，以根据论文摘要生成论文标题。我们还在零样本设置中使用 ChatGPT 来生成论文标题。使用 ROUGE、METEOR、MoverScore、BERTScore 和 SciBERTScore 指标来衡量模型的性能。</li>
</ul>

<h3>Title: Harmonising the Clinical Melody: Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding</h3>
<ul>
<li><strong>Authors: </strong>Bokang Bi, Leibo Liu, Oscar Perez-Concha, Sanja Lujic, Louisa Jorm</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14638">https://arxiv.org/abs/2409.14638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14638">https://arxiv.org/pdf/2409.14638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14638]] Harmonising the Clinical Melody: Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding(https://arxiv.org/abs/2409.14638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The increasing volume and complexity of clinical documentation in Electronic Medical Records systems pose significant challenges for clinical coders, who must mentally process and summarise vast amounts of clinical text to extract essential information needed for coding tasks. While large language models have been successfully applied to shorter summarisation tasks in recent years, the challenge of summarising a hospital course remains an open area for further research and development. In this study, we adapted three pre trained LLMs, Llama 3, BioMistral, Mistral Instruct v0.1 for the hospital course summarisation task, using Quantized Low Rank Adaptation fine tuning. We created a free text clinical dataset from MIMIC III data by concatenating various clinical notes as the input clinical text, paired with ground truth Brief Hospital Course sections extracted from the discharge summaries for model training. The fine tuned models were evaluated using BERTScore and ROUGE metrics to assess the effectiveness of clinical domain fine tuning. Additionally, we validated their practical utility using a novel hospital course summary assessment metric specifically tailored for clinical coding. Our findings indicate that fine tuning pre trained LLMs for the clinical domain can significantly enhance their performance in hospital course summarisation and suggest their potential as assistive tools for clinical coding. Future work should focus on refining data curation methods to create higher quality clinical datasets tailored for hospital course summary tasks and adapting more advanced open source LLMs comparable to proprietary models to further advance this research.</li>
<li><strong>摘要：</strong>电子医疗记录系统中临床文档的数量和复杂性不断增加，这对临床编码员提出了重大挑战，他们必须在脑中处理和总结大量临床文本，以提取编码任务所需的基本信息。虽然近年来大型语言模型已成功应用于较短的总结任务，但总结医院病程的挑战仍然是一个有待进一步研究和开发的开放领域。在本研究中，我们使用量化低秩自适应微调，调整了三个预训练的 LLM，Llama 3、BioMistral、Mistral Instruct v0.1，以完成医院病程总结任务。我们从 MIMIC III 数据中创建了一个自由文本临床数据集，将各种临床笔记连接起来作为输入临床文本，并与从出院总结中提取的基本事实简要医院病程部分配对，以进行模型训练。使用 BERTScore 和 ROUGE 指标评估微调后的模型，以评估临床领域微调的有效性。此外，我们使用专门为临床编码量身定制的新型医院病程总结评估指标来验证它们的实际效用。我们的研究结果表明，针对临床领域对预训练的 LLM 进行微调可以显著提高其在医院病程总结中的表现，并表明其具有作为临床编码辅助工具的潜力。未来的工作应侧重于改进数据管理方法，以创建针对医院病程总结任务量身定制的更高质量的临床数据集，并采用与专有模型相当的更先进的开源 LLM，以进一步推进这项研究。</li>
</ul>

<h3>Title: Direct Judgement Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14664">https://arxiv.org/abs/2409.14664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14664">https://arxiv.org/pdf/2409.14664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14664]] Direct Judgement Preference Optimization(https://arxiv.org/abs/2409.14664)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Auto-evaluation is crucial for assessing response quality and offering feedback for model development. Recent studies have explored training large language models (LLMs) as generative judges to evaluate and critique other models' outputs. In this work, we investigate the idea of learning from both positive and negative data with preference optimization to enhance the evaluation capabilities of LLM judges across an array of different use cases. We achieve this by employing three approaches to collect the preference pairs for different use cases, each aimed at improving our generative judge from a different perspective. Our comprehensive study over a wide range of benchmarks demonstrates the effectiveness of our method. In particular, our generative judge achieves the best performance on 10 out of 13 benchmarks, outperforming strong baselines like GPT-4o and specialized judge models. Further analysis show that our judge model robustly counters inherent biases such as position and length bias, flexibly adapts to any evaluation protocol specified by practitioners, and provides helpful language feedback for improving downstream generator models.</li>
<li><strong>摘要：</strong>自动评估对于评估响应质量和为模型开发提供反馈至关重要。最近的研究探索了训练大型语言模型 (LLM) 作为生成性判断者来评估和批评其他模型的输出。在这项工作中，我们研究了通过偏好优化从正面和负面数据中学习的想法，以增强 LLM 判断者在一系列不同用例中的评估能力。我们通过采用三种方法收集不同用例的偏好对来实现这一点，每种方法都旨在从不同的角度改进我们的生成性判断者。我们对广泛基准的全面研究证明了我们方法的有效性。特别是，我们的生成性判断者在 13 个基准中的 10 个上取得了最佳表现，优于 GPT-4o 等强大的基线和专门的判断模型。进一步的分析表明，我们的判断模型可以稳健地对抗位置和长度偏差等固有偏差，灵活地适应从业者指定的任何评估协议，并为改进下游生成器模型提供有用的语言反馈。</li>
</ul>

<h3>Title: Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</h3>
<ul>
<li><strong>Authors: </strong>Taihang Wang, Xiaoman Xu, Yimin Wang, Ye Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14673">https://arxiv.org/abs/2409.14673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14673">https://arxiv.org/pdf/2409.14673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14673]] Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science(https://arxiv.org/abs/2409.14673)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Real-world applications of large language models (LLMs) in computational social science (CSS) tasks primarily depend on the effectiveness of instruction tuning (IT) or in-context learning (ICL). While IT has shown highly effective at fine-tuning LLMs for various tasks, ICL offers a rapid alternative for task adaptation by learning from examples without explicit gradient updates. In this paper, we evaluate the classification performance of LLMs using IT versus ICL in few-shot CSS tasks. The experimental results indicate that ICL consistently outperforms IT in most CSS tasks. Additionally, we investigate the relationship between the increasing number of training samples and LLM performance. Our findings show that simply increasing the number of samples without considering their quality does not consistently enhance the performance of LLMs with either ICL or IT and can sometimes even result in a performance decline. Finally, we compare three prompting strategies, demonstrating that ICL is more effective than zero-shot and Chain-of-Thought (CoT). Our research highlights the significant advantages of ICL in handling CSS tasks in few-shot settings and emphasizes the importance of optimizing sample quality and prompting strategies to improve LLM classification performance. The code will be made available.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在计算社会科学 (CSS) 任务中的实际应用主要取决于指令调整 (IT) 或上下文学习 (ICL) 的有效性。虽然 IT 在针对各种任务对 LLM 进行微调方面表现出了高度的有效性，但 ICL 提供了一种快速的任务适应替代方案，它通过从示例中学习而无需显式梯度更新。在本文中，我们评估了在少样本 CSS 任务中使用 IT 与 ICL 的 LLM 的分类性能。实验结果表明，在大多数 CSS 任务中，ICL 的表现始终优于 IT。此外，我们还研究了训练样本数量的增加与 LLM 性能之间的关系。我们的研究结果表明，仅仅增加样本数量而不考虑样本质量并不能持续提高使用 ICL 或 IT 的 LLM 的性能，有时甚至会导致性能下降。最后，我们比较了三种提示策略，表明 ICL 比零样本和思想链 (CoT) 更有效。我们的研究突出了 ICL 在处理少样本设置中的 CSS 任务方面的显著优势，并强调了优化样本质量和提示策略对提高 LLM 分类性能的重要性。代码将公开。</li>
</ul>

<h3>Title: Target-Aware Language Modeling via Granular Data Sampling</h3>
<ul>
<li><strong>Authors: </strong>Ernie Chang, Pin-Jie Lin, Yang Li, Changsheng Zhao, Daeil Kim, Rastislav Rabatin, Zechun Liu, Yangyang Shi, Vikas Chandra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14705">https://arxiv.org/abs/2409.14705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14705">https://arxiv.org/pdf/2409.14705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14705]] Target-Aware Language Modeling via Granular Data Sampling(https://arxiv.org/abs/2409.14705)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language model pretraining generally targets a broad range of use cases and incorporates data from diverse sources. However, there are instances where we desire a model that excels in specific areas without markedly compromising performance in other areas. A cost-effective and straightforward approach is sampling with low-dimensional data features, which allows to select large-scale pretraining data for domain-specific use cases. In this work, we revisit importance sampling with n-gram features consisting of multi-granular tokens, which strikes a good balance between sentence compression and representation capabilities. We observed the sampled data to have a high correlation with the target downstream task performance while preserving its effectiveness on other tasks. This leads to the proposed data sampling paradigm where language models can be pretrained more efficiently on selected documents. On eight benchmarks we demonstrate with $\sim$1% of the data, pretrained models perform on par with the full RefinedWeb data and outperform randomly selected samples for model sizes ranging from 125M to 1.5B.</li>
<li><strong>摘要：</strong>语言模型预训练通常针对广泛的用例，并整合来自不同来源的数据。然而，在某些情况下，我们希望模型在特定领域表现出色，而不会明显损害其他领域的性能。一种经济高效且直接的方法是使用低维数据特征进行采样，这允许为特定领域的用例选择大规模预训练数据。在这项工作中，我们重新审视了由多粒度标记组成的 n-gram 特征的重要性采样，这在句子压缩和表示能力之间取得了良好的平衡。我们观察到采样数据与目标下游任务性能具有高度相关性，同时保留了其在其他任务上的有效性。这导致了提出的数据采样范例，其中语言模型可以在选定的文档上更有效地进行预训练。在八个基准测试中，我们用 $\sim$1% 的数据证明，预训练模型的性能与完整的 RefinedWeb 数据相当，并且优于随机选择的样本，模型大小从 125M 到 1.5B。</li>
</ul>

<h3>Title: ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning</h3>
<ul>
<li><strong>Authors: </strong>Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14710">https://arxiv.org/abs/2409.14710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14710">https://arxiv.org/pdf/2409.14710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14710]] ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning(https://arxiv.org/abs/2409.14710)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Role-playing is an emerging application in the field of Human-Computer Interaction (HCI), primarily implemented through the alignment training of a large language model (LLM) with assigned characters. Despite significant progress, role-playing agents (RPLAs) still struggle with maintaining role-consistency across conversations, particularly when confronted with boundary queries subtly related to character attributes. In this paper, we present ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities through boundary-aware learning. ERABAL encompasses a generation pipeline for role-specific dialogues and a concomitant methodology for alignment training. Through comprehensive evaluations, we demonstrate that ERABAL is both efficient and effective. By training with significantly fewer dialogues than those used in leading approaches, ERABAL achieves notable improvements across WikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared to the generalist baseline models. Our code and datasets will be made publicly available to support further research.</li>
<li><strong>摘要：</strong>角色扮演是人机交互 (HCI) 领域的一种新兴应用，主要通过对大型语言模型 (LLM) 与指定角色进行对齐训练来实现。尽管取得了重大进展，但角色扮演代理 (RPLA) 仍然难以在对话中保持角色一致性，尤其是在面对与角色属性微妙相关的边界查询时。在本文中，我们介绍了 ERABAL，这是一个旨在通过边界感知学习增强 RPLA 角色扮演能力的框架。ERABAL 包含角色特定对话的生成管道和对齐训练的伴随方法。通过全面的评估，我们证明了 ERABAL 既高效又有效。通过使用比领先方法中使用的对话少得多的对话进行训练，ERABAL 在 WikiRoleEval、CharacterEval 和 MT-Bench 的角色扮演子集上与通用基线模型相比取得了显着的改进。我们的代码和数据集将公开提供以支持进一步的研究。</li>
</ul>

<h3>Title: LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sihui Yang, Keping Bi, Wanqing Cui, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14744">https://arxiv.org/abs/2409.14744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14744">https://arxiv.org/pdf/2409.14744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14744]] LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs(https://arxiv.org/abs/2409.14744)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to diverse potential answers and no objective criterion. The commonly used automatic evaluation metrics like ROUGE or BERTScore cannot accurately measure semantic similarities or answers from different perspectives. Recently, Large Language Models (LLMs) have been resorted to for NFQA evaluation due to their compelling performance on various NLP tasks. Common approaches include pointwise scoring of each candidate answer and pairwise comparisons between answers. Inspired by the evolution from pointwise to pairwise to listwise in learning-to-rank methods, we propose a novel listwise NFQA evaluation approach, that utilizes LLMs to rank candidate answers in a list of reference answers sorted by descending quality. Moreover, for NF questions that do not have multi-grade or any golden answers, we leverage LLMs to generate the reference answer list of various quality to facilitate the listwise evaluation. Extensive experimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and WebGLM show that our method has significantly higher correlations with human annotations compared to automatic scores and common pointwise and pairwise approaches.</li>
<li><strong>摘要：</strong>非事实类 (NF) 问答 (QA) 的评估具有挑战性，因为潜在答案多种多样且没有客观标准。常用的自动评估指标（如 ROUGE 或 BERTScore）无法准确衡量语义相似性或不同角度的答案。最近，大型语言模型 (LLM) 因其在各种 NLP 任务上的出色表现而被用于 NFQA 评估。常用方法包括对每个候选答案进行逐点评分和对答案进行成对比较。受学习排序方法从逐点到成对再到列表的演变启发，我们提出了一种新颖的列表式 NFQA 评估方法，该方法利用 LLM 对按质量降序排序的参考答案列表中的候选答案进行排序。此外，对于没有多级或任何黄金答案的 NF 问题，我们利用 LLM 生成各种质量的参考答案列表以促进列表式评估。在三个 NFQA 数据集（即 ANTIQUE、TREC-DL-NF 和 WebGLM）上进行的大量实验结果表明，与自动评分和常见的逐点和成对方法相比，我们的方法与人工注释的相关性明显更高。</li>
</ul>

<h3>Title: Do Large Language Models have Problem-Solving Capability under Incomplete Information Scenarios?</h3>
<ul>
<li><strong>Authors: </strong>Yuyan Chen, Tianhao Yu, Yueze Li, Songzhou Yan, Sijia Liu, Jiaqing Liang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14762">https://arxiv.org/abs/2409.14762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14762">https://arxiv.org/pdf/2409.14762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14762]] Do Large Language Models have Problem-Solving Capability under Incomplete Information Scenarios?(https://arxiv.org/abs/2409.14762)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The evaluation of the problem-solving capability under incomplete information scenarios of Large Language Models (LLMs) is increasingly important, encompassing capabilities such as questioning, knowledge search, error detection, and path planning. Current research mainly focus on LLMs' problem-solving capability such as ``Twenty Questions''. However, these kinds of games do not require recognizing misleading cues which are necessary in the incomplete information scenario. Moreover, the existing game such as ``Who is undercover'' are highly subjective, making it challenging for evaluation. Therefore, in this paper, we introduce a novel game named BrainKing based on the ``Who is undercover'' and ``Twenty Questions'' for evaluating LLM capabilities under incomplete information scenarios. It requires LLMs to identify target entities with limited yes-or-no questions and potential misleading answers. By setting up easy, medium, and hard difficulty modes, we comprehensively assess the performance of LLMs across various aspects. Our results reveal the capabilities and limitations of LLMs in BrainKing, providing significant insights of LLM problem-solving levels.</li>
<li><strong>摘要：</strong>对大型语言模型 (LLM) 在不完全信息场景下解决问题的能力的评估越来越重要，包括提问、知识搜索、错误检测和路径规划等能力。当前的研究主要集中在 LLM 的解决问题能力上，例如“二十个问题”。然而，这类游戏不需要识别不完全信息场景中必需的误导性线索。此外，现有的游戏如“谁是卧底”具有很强的主观性，评估起来具有挑战性。因此，在本文中，我们介绍了一款基于“谁是卧底”和“二十个问题”的新游戏 BrainKing，用于评估不完全信息场景下的 LLM 能力。它要求 LLM 用有限的是非问题和潜在的误导性答案来识别目标实体。通过设置简单、中等和困难难度模式，我们全面评估 LLM 在各个方面的性能。我们的研究结果揭示了 BrainKing 中 LLM 的能力和局限性，为 LLM 问题解决水平提供了重要的见解。</li>
</ul>

<h3>Title: Language-Agnostic Analysis of Speech Depression Detection</h3>
<ul>
<li><strong>Authors: </strong>Sona Binu, Jismi Jose, Fathima Shimna K V, Alino Luke Hans, Reni K. Cherian, Starlet Ben Alex, Priyanka Srivastava, Chiranjeevi Yarra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14769">https://arxiv.org/abs/2409.14769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14769">https://arxiv.org/pdf/2409.14769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14769]] Language-Agnostic Analysis of Speech Depression Detection(https://arxiv.org/abs/2409.14769)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>The people with Major Depressive Disorder (MDD) exhibit the symptoms of tonal variations in their speech compared to the healthy counterparts. However, these tonal variations not only confine to the state of MDD but also on the language, which has unique tonal patterns. This work analyzes automatic speech-based depression detection across two languages, English and Malayalam, which exhibits distinctive prosodic and phonemic characteristics. We propose an approach that utilizes speech data collected along with self-reported labels from participants reading sentences from IViE corpus, in both English and Malayalam. The IViE corpus consists of five sets of sentences: simple sentences, WH-questions, questions without morphosyntactic markers, inversion questions and coordinations, that can naturally prompt speakers to speak in different tonal patterns. Convolutional Neural Networks (CNNs) are employed for detecting depression from speech. The CNN model is trained to identify acoustic features associated with depression in speech, focusing on both languages. The model's performance is evaluated on the collected dataset containing recordings from both depressed and non-depressed speakers, analyzing its effectiveness in detecting depression across the two languages. Our findings and collected data could contribute to the development of language-agnostic speech-based depression detection systems, thereby enhancing accessibility for diverse populations.</li>
<li><strong>摘要：</strong>与健康人相比，重度抑郁症 (MDD) 患者在言语中表现出音调变化的症状。然而，这些音调变化不仅限于 MDD 状态，也存在于具有独特音调模式的语言中。这项研究分析了两种语言（英语和马拉雅拉姆语）的自动语音抑郁症检测，这两种语言表现出独特的韵律和音位特征。我们提出了一种方法，利用从阅读英语和马拉雅拉姆语 IViE 语料库中的句子的参与者收集的语音数据以及自我报告的标签。IViE 语料库由五组句子组成：简单句、WH 问题、没有形态句法标记的问题、倒装问题和协调，这些句子可以自然地促使说话者以不同的音调模式说话。卷积神经网络 (CNN) 用于从语音中检测抑郁症。CNN 模型经过训练，可以识别与语音抑郁症相关的声学特征，重点关注两种语言。该模型的性能在收集的数据集上进行评估，该数据集包含抑郁和非抑郁说话者的录音，分析了其在两种语言中检测抑郁症的有效性。我们的发现和收集的数据可能有助于开发与语言无关的基于语音的抑郁症检测系统，从而提高不同人群的可访问性。</li>
</ul>

<h3>Title: OMPar: Automatic Parallelization with AI-Driven Source-to-Source Compilation</h3>
<ul>
<li><strong>Authors: </strong>Tal Kadosh, Niranjan Hasabnis, Prema Soundararajan, Vy A. Vo, Mihai Capota, Nesreen Ahmed, Yuval Pinter, Gal Oren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14771">https://arxiv.org/abs/2409.14771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14771">https://arxiv.org/pdf/2409.14771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14771]] OMPar: Automatic Parallelization with AI-Driven Source-to-Source Compilation(https://arxiv.org/abs/2409.14771)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Manual parallelization of code remains a significant challenge due to the complexities of modern software systems and the widespread adoption of multi-core architectures. This paper introduces OMPar, an AI-driven tool designed to automate the parallelization of C/C++ code using OpenMP pragmas. OMPar integrates Large Language Models (LLMs) through two key components: OMPify, which assesses loop parallelization potential, and MonoCoder-OMP, a new fine-tuned model which generates precise OpenMP pragmas. The evaluation of OMPar follows the same rigorous process applied to traditional tools like source-to-source AutoPar and ICPC compilers: (1) ensuring the generated code compiles and runs correctly in serial form, (2) assessing performance with the gradual addition of threads and corresponding physical cores, and (3) verifying and validating the correctness of the code's output. Benchmarks from HeCBench and ParEval are used to evaluate accuracy and performance. Experimental results demonstrate that OMPar significantly outperforms traditional methods, achieving higher accuracy in identifying parallelizable loops and generating efficient pragmas. Beyond accuracy, OMPar offers advantages such as the ability to work on partial or incomplete codebases and the capacity to continuously learn from new code patterns, enhancing its parallelization capabilities over time. These results underscore the potential of LLMs in revolutionizing automatic parallelization techniques, paving the way for more efficient and scalable parallel computing systems.</li>
<li><strong>摘要：</strong>由于现代软件系统的复杂性和多核架构的广泛采用，代码的手动并行化仍然是一项重大挑战。本文介绍了 OMPar，这是一种 AI 驱动的工具，旨在使用 OpenMP 指令自动并行化 C/C++ 代码。OMPar 通过两个关键组件集成大型语言模型 (LLM)：OMPify（评估循环并行化潜力）和 MonoCoder-OMP（一种新的微调模型，可生成精确的 OpenMP 指令）。OMPar 的评估遵循与传统工具（如源到源 AutoPar 和 ICPC 编译器）相同的严格流程：(1) 确保生成的代码以串行形式正确编译和运行，(2) 通过逐步添加线程和相应的物理核心来评估性能，以及 (3) 验证和确认代码输出的正确性。HeCBench 和 ParEval 的基准用于评估准确性和性能。实验结果表明，OMPar 的表现明显优于传统方法，在识别可并行化循环和生成高效指令方面具有更高的准确性。除了准确性之外，OMPar 还具有其他优势，例如能够处理部分或不完整的代码库，并能够不断从新的代码模式中学习，从而随着时间的推移增强其并行化能力。这些结果凸显了 LLM 在革新自动并行化技术方面的潜力，为更高效、可扩展的并行计算系统铺平了道路。</li>
</ul>

<h3>Title: Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method</h3>
<ul>
<li><strong>Authors: </strong>Weichao Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14781">https://arxiv.org/abs/2409.14781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14781">https://arxiv.org/pdf/2409.14781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14781]] Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method(https://arxiv.org/abs/2409.14781)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As the scale of training corpora for large language models (LLMs) grows, model developers become increasingly reluctant to disclose details on their data. This lack of transparency poses challenges to scientific evaluation and ethical deployment. Recently, pretraining data detection approaches, which infer whether a given text was part of an LLM's training data through black-box access, have been explored. The Min-K% Prob method, which has achieved state-of-the-art results, assumes that a non-training example tends to contain a few outlier words with low token probabilities. However, the effectiveness may be limited as it tends to misclassify non-training texts that contain many common words with high probabilities predicted by LLMs. To address this issue, we introduce a divergence-based calibration method, inspired by the divergence-from-randomness concept, to calibrate token probabilities for pretraining data detection. We compute the cross-entropy (i.e., the divergence) between the token probability distribution and the token frequency distribution to derive a detection score.We have developed a Chinese-language benchmark, PatentMIA, to assess the performance of detection approaches for LLMs on Chinese text. Experimental results on English-language benchmarks and PatentMIA demonstrate that our proposed method significantly outperforms existing methods. Our code and PatentMIA benchmark are available at this https URL</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 训练语料库规模的不断扩大，模型开发人员越来越不愿意透露其数据的详细信息。这种缺乏透明度对科学评估和道德部署构成了挑战。最近，人们探索了预训练数据检测方法，该方法通过黑盒访问推断给定文本是否是 LLM 训练数据的一部分。已取得最佳效果的 Min-K% Prob 方法假设非训练示例往往包含一些具有低标记概率的异常词。然而，这种方法的有效性可能有限，因为它往往会对包含许多 LLM 预测概率较高的常用词的非训练文本进行错误分类。为了解决这个问题，我们引入了一种基于散度的校准方法，该方法受到随机散度概念的启发，用于校准预训练数据检测的标记概率。我们计算 token 概率分布和 token 频率分布之间的交叉熵（即散度）来得出检测分数。我们开发了一个中文基准 PatentMIA，以评估 LLM 在中文文本上的检测方法的性能。在英语基准和 PatentMIA 上的实验结果表明，我们提出的方法明显优于现有方法。我们的代码和 PatentMIA 基准可在此 https URL 上找到</li>
</ul>

<h3>Title: Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Amadeus Irawan, Genta Indra Winata, Samuel Cahyawijaya, Ayu Purwarianti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14785">https://arxiv.org/abs/2409.14785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14785">https://arxiv.org/pdf/2409.14785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14785]] Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models(https://arxiv.org/abs/2409.14785)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Natural Language Explanation (NLE) aims to elucidate the decision-making process by providing detailed, human-friendly explanations in natural language. It helps demystify the decision-making processes of large vision-language models (LVLMs) through the use of language models. While existing methods for creating a Vision Question-Answering with Natural Language Explanation (VQA-NLE) datasets can provide explanations, they heavily rely on human annotations that are time-consuming and costly. In this study, we propose a novel approach that leverages LVLMs to efficiently generate high-quality synthetic VQA-NLE datasets. By evaluating our synthetic data, we showcase how advanced prompting techniques can lead to the production of high-quality VQA-NLE data. Our findings indicate that this proposed method achieves up to 20x faster than human annotation, with only a minimal decrease in qualitative metrics, achieving robust quality that is nearly equivalent to human-annotated data. Furthermore, we show that incorporating visual prompts significantly enhances the relevance of text generation. Our study paves the way for a more efficient and robust automated generation of multi-modal NLE data, offering a promising solution to the problem.</li>
<li><strong>摘要：</strong>自然语言解释 (NLE) 旨在通过提供详细的、人性化的自然语言解释来阐明决策过程。它有助于通过使用语言模型揭开大型视觉语言模型 (LVLM) 决策过程的神秘面纱。虽然现有的创建自然语言解释视觉问答 (VQA-NLE) 数据集的方法可以提供解释，但它们严重依赖于耗时且昂贵的人工注释。在本研究中，我们提出了一种利用 LVLM 高效生成高质量合成 VQA-NLE 数据集的新方法。通过评估我们的合成数据，我们展示了先进的提示技术如何生成高质量的 VQA-NLE 数据。我们的研究结果表明，这种提出的方​​法比人工注释快 20 倍，定性指标仅略有下降，实现了几乎与人工注释数据相当的稳健质量。此外，我们表明，结合视觉提示可显著增强文本生成的相关性。我们的研究为更高效、更强大的多模态 NLE 数据自动生成铺平了道路，为该问题提供了一个有希望的解决方案。</li>
</ul>

<h3>Title: MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations</h3>
<ul>
<li><strong>Authors: </strong>Gia-Bao Dinh Ho, Chang Wei Tan, Zahra Zamanzadeh Darban, Mahsa Salehi, Gholamreza Haffari, Wray Buntine</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14801">https://arxiv.org/abs/2409.14801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14801">https://arxiv.org/pdf/2409.14801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14801]] MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations(https://arxiv.org/abs/2409.14801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Detecting critical moments, such as emotional outbursts or changes in decisions during conversations, is crucial for understanding shifts in human behavior and their consequences. Our work introduces a novel problem setting focusing on these moments as turning points (TPs), accompanied by a meticulously curated, high-consensus, human-annotated multi-modal dataset. We provide precise timestamps, descriptions, and visual-textual evidence high-lighting changes in emotions, behaviors, perspectives, and decisions at these turning points. We also propose a framework, TPMaven, utilizing state-of-the-art vision-language models to construct a narrative from the videos and large language models to classify and detect turning points in our multi-modal dataset. Evaluation results show that TPMaven achieves an F1-score of 0.88 in classification and 0.61 in detection, with additional explanations aligning with human expectations.</li>
<li><strong>摘要：</strong>检测关键时刻（例如对话过程中的情绪爆发或决策变化）对于理解人类行为的变化及其后果至关重要。我们的工作引入了一种新颖的问题设置，重点关注这些时刻作为转折点 (TP)，并附带精心策划、高度一致、人工注释的多模态数据集。我们提供精确的时间戳、描述和视觉文本证据，突出显示这些转折点时情绪、行为、观点和决策的变化。我们还提出了一个框架 TPMaven，利用最先进的视觉语言模型从视频和大型语言模型构建叙述，以对我们的多模态数据集中的转折点进行分类和检测。评估结果表明，TPMaven 在分类中获得了 0.88 的 F1 分数，在检测中获得了 0.61 的 F1 分数，并提供了符合人类期望的其他解释。</li>
</ul>

<h3>Title: MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding</h3>
<ul>
<li><strong>Authors: </strong>Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Shuo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14818">https://arxiv.org/abs/2409.14818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14818">https://arxiv.org/pdf/2409.14818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14818]] MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding(https://arxiv.org/abs/2409.14818)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Recently, mobile AI agents based on VLMs have been gaining increasing attention. These works typically utilize VLM as a foundation, fine-tuning it with instruction-based mobile datasets. However, these VLMs are typically pre-trained on general-domain data, which often results in a lack of fundamental capabilities specific to the mobile domain. Therefore, they may struggle to recognize specific UI elements and understand intra-UI fine-grained information. In addition, the current fine-tuning task focuses on interacting with the most relevant element for the given instruction. These fine-tuned VLMs may still ignore the relationships between UI pages, neglect the roles of elements in page transitions and lack inter-UI understanding. To address issues, we propose a VLM called MobileVLM, which includes two additional pre-training stages to enhance both intra- and inter-UI understanding. We defined four UI-based pre-training tasks, enabling the model to better perceive fine-grained elements and capture page transition actions. To address the lack of mobile pre-training data, we built a large Chinese mobile dataset Mobile3M from scratch, which contains 3 million UI pages, and real-world transition actions, forming a directed graph structure. Experimental results show MobileVLM excels on both our test set and public mobile benchmarks, outperforming existing VLMs.</li>
<li><strong>摘要：</strong>最近，基于 VLM 的移动 AI 代理越来越受到关注。这些工作通常以 VLM 为基础，使用基于指令的移动数据集对其进行微调。然而，这些 VLM 通常是在通用领域数据上进行预训练的，这通常会导致缺乏特定于移动领域的基本功能。因此，它们可能难以识别特定的 UI 元素并理解 UI 内的细粒度信息。此外，当前的微调任务侧重于与给定指令最相关的元素进行交互。这些经过微调的 VLM 可能仍然会忽略 UI 页面之间的关系，忽略元素在页面转换中的作用，并且缺乏 UI 之间的理解。为了解决问题，我们提出了一种名为 MobileVLM 的 VLM，它包括两个额外的预训练阶段，以增强 UI 内和 UI 之间的理解。我们定义了四个基于 UI 的预训练任务，使模型能够更好地感知细粒度元素并捕获页面转换操作。为了解决移动端预训练数据不足的问题，我们从零开始构建了一个大型中国移动端数据集 Mobile3M，其中包含 300 万个 UI 页面和真实世界的转换动作，形成有向图结构。实验结果表明，MobileVLM 在我们的测试集和公开的移动端基准测试中均表现出色，优于现有的 VLM。</li>
</ul>

<h3>Title: Past Meets Present: Creating Historical Analogy with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nianqi Li, Siyu Yuan, Jiangjie Chen, Jiaqing Liang, Feng Wei, Zujie Liang, Deqing Yang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14820">https://arxiv.org/abs/2409.14820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14820">https://arxiv.org/pdf/2409.14820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14820]] Past Meets Present: Creating Historical Analogy with Large Language Models(https://arxiv.org/abs/2409.14820)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Historical analogies, which compare known past events with contemporary but unfamiliar events, are important abilities that help people make decisions and understand the world. However, research in applied history suggests that people have difficulty finding appropriate analogies. And previous studies in the AI community have also overlooked historical analogies. To fill this gap, in this paper, we focus on the historical analogy acquisition task, which aims to acquire analogous historical events for a given event. We explore retrieval and generation methods for acquiring historical analogies based on different large language models (LLMs). Furthermore, we propose a self-reflection method to mitigate hallucinations and stereotypes when LLMs generate historical analogies. Through human evaluations and our specially designed automatic multi-dimensional assessment, we find that LLMs generally have a good potential for historical analogies. And the performance of the models can be further improved by using our self-reflection method.</li>
<li><strong>摘要：</strong>历史类比是将已知的过去事件与当代但不熟悉的事件进行比较，是帮助人们做出决策和理解世界的重要能力。然而，应用历史的研究表明，人们很难找到合适的类比。而人工智能界以前的研究也忽视了历史类比。为了填补这一空白，在本文中，我们专注于历史类比获取任务，旨在为给定事件获取类似的历史事件。我们探索基于不同大型语言模型 (LLM) 获取历史类比的检索和生成方法。此外，我们提出了一种自我反思方法来减轻 LLM 生成历史类比时的幻觉和刻板印象。通过人工评估和我们专门设计的自动多维评估，我们发现 LLM 通常具有良好的历史类比潜力。并且可以通过使用我们的自我反思方法进一步提高模型的性能。</li>
</ul>

<h3>Title: ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback</h3>
<ul>
<li><strong>Authors: </strong>Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14826">https://arxiv.org/abs/2409.14826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14826">https://arxiv.org/pdf/2409.14826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14826]] ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback(https://arxiv.org/abs/2409.14826)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recently, tool-augmented LLMs have gained increasing attention. Given an instruction, tool-augmented LLMs can interact with various external tools in multiple rounds and provide a final answer. However, previous LLMs were trained on overly detailed instructions, which included API names or parameters, while real users would not explicitly mention these API details. This leads to a gap between trained LLMs and real-world scenarios. In addition, most works ignore whether the interaction process follows the instruction. To address these issues, we constructed a training dataset called MGToolBench, which contains statement and category-level instructions to better reflect real-world scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement learning framework that utilizes path planning and two feedback mechanisms to enhance the LLM's task completion and instruction-following capabilities. Experimental results show that ToolPlanner significantly improves the Match Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA model. Human evaluation verifies that the multi-granularity instructions can better align with users' usage habits. Our data and code will be released upon acceptance.</li>
<li><strong>摘要：</strong>近来，工具增强型LLM受到越来越多的关注。给定一个指令，工具增强型LLM可以与各种外部工具进行多轮交互并提供最终答案。然而，以前的LLM是在过于详细的指令上进行训练的，这些指令包括API名称或参数，而真实用户不会明确提到这些API细节。这导致训练出来的LLM与真实场景之间存在差距。此外，大多数工作都忽略了交互过程是否遵循指令。针对这些问题，我们构建了一个名为MGToolBench的训练数据集，其中包含语句和类别级别的指令，以更好地反映真实场景。此外，我们提出了ToolPlanner，这是一个两阶段强化学习框架，利用路径规划和两种反馈机制来增强LLM的任务完成和指令遵循能力。实验结果表明，与SOTA模型相比，ToolPlanner显著提高了匹配率、通过率和胜率，分别提高了26.8%、20.2%和5.6%。经过人工评估，多粒度指令更符合用户的使用习惯，数据和代码将在接受后发布。</li>
</ul>

<h3>Title: Privacy Policy Analysis through Prompt Engineering for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Arda Goknil, Femke B. Gelderblom, Simeon Tverdal, Shukun Tokas, Hui Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14879">https://arxiv.org/abs/2409.14879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14879">https://arxiv.org/pdf/2409.14879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14879]] Privacy Policy Analysis through Prompt Engineering for LLMs(https://arxiv.org/abs/2409.14879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Privacy policies are often obfuscated by their complexity, which impedes transparency and informed consent. Conventional machine learning approaches for automatically analyzing these policies demand significant resources and substantial domain-specific training, causing adaptability issues. Moreover, they depend on extensive datasets that may require regular maintenance due to changing privacy concerns. In this paper, we propose, apply, and assess PAPEL (Privacy Policy Analysis through Prompt Engineering for LLMs), a framework harnessing the power of Large Language Models (LLMs) through prompt engineering to automate the analysis of privacy policies. PAPEL aims to streamline the extraction, annotation, and summarization of information from these policies, enhancing their accessibility and comprehensibility without requiring additional model training. By integrating zero-shot, one-shot, and few-shot learning approaches and the chain-of-thought prompting in creating predefined prompts and prompt templates, PAPEL guides LLMs to efficiently dissect, interpret, and synthesize the critical aspects of privacy policies into user-friendly summaries. We demonstrate the effectiveness of PAPEL with two applications: (i) annotation and (ii) contradiction analysis. We assess the ability of several LLaMa and GPT models to identify and articulate data handling practices, offering insights comparable to existing automated analysis approaches while reducing training efforts and increasing the adaptability to new analytical needs. The experiments demonstrate that the LLMs PAPEL utilizes (LLaMA and Chat GPT models) achieve robust performance in privacy policy annotation, with F1 scores reaching 0.8 and above (using the OPP-115 gold standard), underscoring the effectiveness of simpler prompts across various advanced language models.</li>
<li><strong>摘要：</strong>隐私政策通常因其复杂性而令人困惑，这妨碍了透明度和知情同意。用于自动分析这些政策的传统机器学习方法需要大量资源和大量特定领域的培训，从而导致适应性问题。此外，它们依赖于大量数据集，由于隐私问题的变化，这些数据集可能需要定期维护。在本文中，我们提出、应用和评估了 PAPEL（通过 LLM 的快速工程进行隐私政策分析），这是一个通过快速工程利用大型语言模型 (LLM) 的强大功能来自动分析隐私政策的框架。PAPEL 旨在简化从这些政策中提取、注释和汇总信息的过程，从而无需额外的模型训练即可增强其可访问性和可理解性。通过整合零样本、单样本和少样本学习方法以及思路链提示来创建预定义的提示和提示模板，PA​​PEL 指导 LLM 有效地剖析、解释和综合隐私政策的关键方面，使其成为用户友好的摘要。我们通过两个应用证明了 PAPEL 的有效性：(i) 注释和 (ii) 矛盾分析。我们评估了几种 LLaMa 和 GPT 模型识别和表达数据处理实践的能力，这些模型提供了与现有自动分析方法相当的见解，同时减少了训练工作量并提高了对新分析需求的适应性。实验表明，PAPEL 使用的 LLM（LLaMA 和 Chat GPT 模型）在隐私政策注释方面取得了稳健的表现，F1 分数达到 0.8 及以上（使用 OPP-115 黄金标准），凸显了更简单的提示在各种高级语言模型中的有效性。</li>
</ul>

<h3>Title: End-to-End Graph Flattening Method for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bin Hong, Jinze Wu, Jiayu Liu, Liang Ding, Jing Sha, Kai Zhang, Shijin Wang, Zhenya Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14880">https://arxiv.org/abs/2409.14880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14880">https://arxiv.org/pdf/2409.14880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14880]] End-to-End Graph Flattening Method for Large Language Models(https://arxiv.org/abs/2409.14880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In recent years, the breakthrough of Large Language Models (LLMs) offers new ideas for achieving universal methods on graph data. The common practice of converting graphs into natural language for LLMs, which refers to graph flattening, exhibits good generalizability and interpretability. However, the poor organization of the textual format results in poor performance in long-distance scenario understanding. Inspired by human cognitive reasoning habits, we propose a novel method for graph flattening to fit LLMs, termed as End-to-End DAG-Path prompting (EEDP). Experiments on real-world datasets show that EEDP enhances the reasoning performance of LLMs in long-distance scenarios while maintaining excellent performance in short-distance scenarios, demonstrating good robustness in the face of distance variations.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLM）的突破为实现在图数据上进行通用化的方法提供了新思路。LLM 将图转化为自然语言的常用做法是将图展平，具有很好的通用性和可解释性。然而，文本格式的组织性较差导致其在长距离场景理解中表现不佳。我们受到人类认知推理习惯的启发，提出了一种适合 LLM 的图展平方法，即端到端 DAG 路径提示（EEDP）。在真实数据集上的实验表明，EEDP 提升了 LLM 在长距离场景下的推理性能，同时在短距离场景下保持了优异的性能，面对距离变化表现出良好的鲁棒性。</li>
</ul>

<h3>Title: DSG-KD: Knowledge Distillation from Domain-Specific to General Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sangyeon Cho, Jangyeong Jeon, Dongjoon Lee, Changhee Lee, Junyeong Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14904">https://arxiv.org/abs/2409.14904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14904">https://arxiv.org/pdf/2409.14904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14904]] DSG-KD: Knowledge Distillation from Domain-Specific to General Language Models(https://arxiv.org/abs/2409.14904)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The use of pre-trained language models fine-tuned to address specific downstream tasks is a common approach in natural language processing (NLP). However, acquiring domain-specific knowledge via fine-tuning is challenging. Traditional methods involve pretraining language models using vast amounts of domain-specific data before fine-tuning for particular tasks. This study investigates emergency/non-emergency classification tasks based on electronic medical record (EMR) data obtained from pediatric emergency departments (PEDs) in Korea. Our findings reveal that existing domain-specific pre-trained language models underperform compared to general language models in handling N-lingual free-text data characteristics of non-English-speaking regions. To address these limitations, we propose a domain knowledge transfer methodology that leverages knowledge distillation to infuse general language models with domain-specific knowledge via fine-tuning. This study demonstrates the effective transfer of specialized knowledge between models by defining a general language model as the student model and a domain-specific pre-trained model as the teacher model. In particular, we address the complexities of EMR data obtained from PEDs in non-English-speaking regions, such as Korea, and demonstrate that the proposed method enhances classification performance in such contexts. The proposed methodology not only outperforms baseline models on Korean PED EMR data, but also promises broader applicability in various professional and technical domains. In future works, we intend to extend this methodology to include diverse non-English-speaking regions and address additional downstream tasks, with the aim of developing advanced model architectures using state-of-the-art KD techniques. The code is available in this https URL.</li>
<li><strong>摘要：</strong>使用经过微调的预训练语言模型来解决特定的下游任务是自然语言处理 (NLP) 中的一种常见方法。然而，通过微调获取领域特定知识具有挑战性。传统方法涉及使用大量领域特定数据对语言模型进行预训练，然后针对特定任务进行微调。本研究基于从韩国儿科急诊部 (PED) 获得的电子病历 (EMR) 数据，调查了紧急/非紧急分类任务。我们的研究结果表明，现有的领域特定预训练语言模型在处理非英语地区的 N 语言自由文本数据特征时表现不佳，而通用语言模型则不然。为了解决这些限制，我们提出了一种领域知识转移方法，该方法利用知识提炼通过微调将领域特定知识注入通用语言模型。本研究通过将通用语言模型定义为学生模型，将领域特定预训练模型定义为教师模型，展示了模型之间专业知识的有效转移。具体来说，我们解决了从非英语地区（例如韩国）的 PED 获得的 EMR 数据的复杂性，并证明了所提出的方法可以提高此类情况下的分类性能。所提出的方法不仅在韩国 PED EMR 数据上的表现优于基线模型，而且有望在各种专业和技术领域得到更广泛的应用。在未来的工作中，我们打算将这种方法扩展到包括不同的非英语地区并解决其他下游任务，目的是使用最先进的 KD 技术开发先进的模型架构。代码可在此 https URL 中找到。</li>
</ul>

<h3>Title: Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization</h3>
<ul>
<li><strong>Authors: </strong>Aseem Srivastava, Smriti Joshi, Tanmoy Chakraborty, Md Shad Akhtar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14907">https://arxiv.org/abs/2409.14907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14907">https://arxiv.org/pdf/2409.14907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14907]] Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization(https://arxiv.org/abs/2409.14907)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In mental health counseling, condensing dialogues into concise and relevant summaries (aka counseling notes) holds pivotal significance. Large Language Models (LLMs) exhibit remarkable capabilities in various generative tasks; however, their adaptation to domain-specific intricacies remains challenging, especially within mental health contexts. Unlike standard LLMs, mental health experts first plan to apply domain knowledge in writing summaries. Our work enhances LLMs' ability by introducing a novel planning engine to orchestrate structuring knowledge alignment. To achieve high-order planning, we divide knowledge encapsulation into two major phases: (i) holding dialogue structure and (ii) incorporating domain-specific knowledge. We employ a planning engine on Llama-2, resulting in a novel framework, PIECE. Our proposed system employs knowledge filtering-cum-scaffolding to encapsulate domain knowledge. Additionally, PIECE leverages sheaf convolution learning to enhance its understanding of the dialogue's structural nuances. We compare PIECE with 14 baseline methods and observe a significant improvement across ROUGE and Bleurt scores. Further, expert evaluation and analyses validate the generation quality to be effective, sometimes even surpassing the gold standard. We further benchmark PIECE with other LLMs and report improvement, including Llama-2 (+2.72%), Mistral (+2.04%), and Zephyr (+1.59%), to justify the generalizability of the planning engine.</li>
<li><strong>摘要：</strong>在心理健康咨询中，将对话浓缩为简洁而相关的摘要（又称咨询笔记）具有至关重要的意义。大型语言模型 (LLM) 在各种生成任务中表现出非凡的能力；然而，它们对特定领域复杂性的适应仍然具有挑战性，尤其是在心理健康环境中。与标准 LLM 不同，心理健康专家首先计划将领域知识应用于撰写摘要。我们的工作通过引入一种新颖的规划引擎来协调结构化知识对齐，从而增强了 LLM 的能力。为了实现高阶规划，我们将知识封装分为两个主要阶段：(i) 保持对话结构和 (ii) 整合领域特定知识。我们在 Llama-2 上使用了一个规划引擎，从而产生了一个新颖的框架 PIECE。我们提出的系统采用知识过滤兼支架来封装领域知识。此外，PIECE 利用层卷积学习来增强其对对话结构细微差别的理解。我们将 PIECE 与 14 种基线方法进行比较，发现 ROUGE 和 Bleurt 得分均有显著提高。此外，专家评估和分析证实了生成质量是有效的，有时甚至超过了黄金标准。我们进一步将 PIECE 与其他 LLM 进行了对比，并报告了改进情况，包括 Llama-2（+2.72%）、Mistral（+2.04%）和 Zephyr（+1.59%），以证明规划引擎的通用性。</li>
</ul>

<h3>Title: Towards a Realistic Long-Term Benchmark for Open-Web Research Agents</h3>
<ul>
<li><strong>Authors: </strong>Peter Mühlbacher, Nikos I. Bosse, Lawrence Phillips</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14913">https://arxiv.org/abs/2409.14913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14913">https://arxiv.org/pdf/2409.14913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14913]] Towards a Realistic Long-Term Benchmark for Open-Web Research Agents(https://arxiv.org/abs/2409.14913)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>We present initial results of a forthcoming benchmark for evaluating LLM agents on white-collar tasks of economic value. We evaluate eight realistic and ``messy'' tasks that are routine in finance and consulting, drawn from real-world cases from our customers. We lay the groundwork for an LLM agent evaluation suite where good performance directly corresponds to a large economic and societal impact. This fills a gap in existing benchmarks with tasks like ``order a pizza to the following address'' that do not constitute real-human work of economic value. Our evaluations assign credit to agents for partially solving tasks. By doing that, this initial evaluation, and the forthcoming benchmark, allow us to more accurately extrapolate performance of LLM-based agents on economically valuable tasks. We built and tested several architectures with GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini, ensuring that failure to solve a task was due to failures of reasoning and planning, rather than due to common failures like e.g. the inability to parse a website. On average, LLM agents powered by Claude-3.5 Sonnet substantially outperformed agents using GPT-4o, with agents based on Llama 3.1 (405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct architecture with the ability to delegate subtasks to subagents performed best. In addition to quantitative evaluations, we qualitatively assessed the performance of the LLM agents by inspecting their traces and reflecting on their observations.</li>
<li><strong>摘要：</strong>我们介绍了即将推出的基准的初步结果，该基准用于评估 LLM 代理在具有经济价值的白领任务上的表现。我们评估了八项现实且“混乱”的任务，这些任务在金融和咨询领域很常见，这些任务来自我们客户的真实案例。我们为 LLM 代理评估套件奠定了基础，其中良好的表现直接对应于巨大的经济和社会影响。这填补了现有基准中的空白，其中诸如“向以下地址订购披萨”之类的任务不构成具有经济价值的真实人类工作。我们的评估将功劳归于代理部分解决任务。通过这样做，这个初步评估和即将推出的基准使我们能够更准确地推断基于 LLM 的代理在具有经济价值的任务上的表现。我们用 GPT-4o、Claude-3.5 Sonnet、Llama 3.1 (405b) 和 GPT-4o-mini 构建并测试了几种架构，确保无法解决任务是由于推理和计划失败，而不是由于常见的失败，例如无法解析网站。平均而言，由 Claude-3.5 Sonnet 驱动的 LLM 代理的表现明显优于使用 GPT-4o 的代理，而基于 Llama 3.1 (405b) 和 GPT-4o-mini 的代理则明显落后。在 LLM 中，具有将子任务委派给子代理能力的 ReAct 架构表现最佳。除了定量评估外，我们还通过检查 LLM 代理的踪迹并反思其观察结果来定性评估其性能。</li>
</ul>

<h3>Title: With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tyler Loakman, Yucheng Li, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14917">https://arxiv.org/abs/2409.14917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14917">https://arxiv.org/pdf/2409.14917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14917]] With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models(https://arxiv.org/abs/2409.14917)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to ``hear'' via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks, and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.</li>
<li><strong>摘要：</strong>最近，大型语言模型 (LLM) 和视觉语言模型 (VLM) 已证明有能力在测试心理语言现象的实验中替代人类参与者。然而，一个尚未得到充分研究的问题是，仅能访问视觉和文本模态的模型在多大程度上能够仅通过从正字法和图像进行抽象推理来隐式理解基于声音的现象。为了研究这个问题，我们分析了 VLM 和 LLM 展示声音象征意义的能力（即识别声音和概念之间的非任意联系），以及它们通过开放和封闭源多模态模型的语言和视觉模块的相互作用“听”的能力。我们进行了多项实验，包括复制经典的 Kiki-Bouba 和 Mil-Mal 形状和幅度象征意义任务，并将人类对语言象似性的判断与 LLM 的判断进行比较。我们的结果表明，VLM 与人类标签的一致性程度各不相同，并且与人类同行相比，VLM 可能需要更多的任务信息来进行计算机实验。我们还通过更高的最大一致性水平发现，对于 VLM 来说，量级象征比形状象征更容易识别，并且对语言象似性的理解高度依赖于模型大小。</li>
</ul>

<h3>Title: Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely</h3>
<ul>
<li><strong>Authors: </strong>Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14924">https://arxiv.org/abs/2409.14924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14924">https://arxiv.org/pdf/2409.14924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14924]] Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely(https://arxiv.org/abs/2409.14924)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.</li>
<li><strong>摘要：</strong>借助外部数据增强的大型语言模型 (LLM) 已展现出在完成实际任务方面的卓越能力。将外部数据集成到 LLM 中的技术（例如检索增强生成 (RAG) 和微调）正受到越来越多的关注和广泛应用。尽管如此，在各个专业领域有效部署数据增强型 LLM 仍面临巨大挑战。这些挑战涵盖了广泛的问题，从检索相关数据和准确解释用户意图到充分利用 LLM 的推理能力来完成复杂任务。我们认为，对于数据增强型 LLM 应用，没有一刀切的解决方案。在实践中，表现不佳通常是由于未能正确识别任务的核心重点，或者因为任务本身需要多种功能的混合，而这些功能必须解开才能获得更好的解决方案。在本调查中，我们提出了一种 RAG 任务分类方法，根据所需外部数据类型和任务的主要重点将用户查询分为四个级别：显性事实查询、隐性事实查询、可解释的原理查询和隐藏的原理查询。我们定义了这些查询级别，提供了相关数据集，并总结了解决这些挑战的关键挑战和最有效的技术。最后，我们讨论了将外部数据集成到 LLM 中的三种主要形式：上下文、小模型和微调，重点介绍了它们各自的优势、局限性以及它们适合解决的问题类型。这项工作旨在帮助读者彻底理解和分解构建 LLM 应用程序中的数据需求和关键瓶颈，为不同的挑战提供解决方案，并为系统地开发此类应用程序提供指南。</li>
</ul>

<h3>Title: Evaluating Theory of (an uncertain) Mind: Predicting the Uncertain Beliefs of Others in Conversation Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Anthony Sicilia, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14986">https://arxiv.org/abs/2409.14986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14986">https://arxiv.org/pdf/2409.14986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14986]] Evaluating Theory of (an uncertain) Mind: Predicting the Uncertain Beliefs of Others in Conversation Forecasting(https://arxiv.org/abs/2409.14986)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Typically, when evaluating Theory of Mind, we consider the beliefs of others to be binary: held or not held. But what if someone is unsure about their own beliefs? How can we quantify this uncertainty? We propose a new suite of tasks, challenging language models (LMs) to model the uncertainty of others in dialogue. We design these tasks around conversation forecasting, wherein an agent forecasts an unobserved outcome to a conversation. Uniquely, we view interlocutors themselves as forecasters, asking an LM to predict the uncertainty of the interlocutors (a probability). We experiment with re-scaling methods, variance reduction strategies, and demographic context, for this regression task, conducting experiments on three dialogue corpora (social, negotiation, task-oriented) with eight LMs. While LMs can explain up to 7% variance in the uncertainty of others, we highlight the difficulty of the tasks and room for future work, especially in practical applications, like anticipating ``false</li>
<li><strong>摘要：</strong>通常，在评估心智理论时，我们认为他人的信念是二元的：持有或不持有。但是，如果有人不确定自己的信念怎么办？我们如何量化这种不确定性？我们提出了一套新的任务，挑战语言模型 (LM) 来模拟对话中他人的不确定性。我们围绕对话预测设计这些任务，其中代理预测对话的未观察到的结果。独特的是，我们将对话者本身视为预测者，要求 LM 预测对话者的不确定性（概率）。对于这个回归任务，我们尝试了重新缩放方法、方差减少策略和人口统计背景，使用八个 LM 在三个对话语料库（社交、谈判、任务导向）上进行实验。虽然 LM 可以解释他人不确定性高达 7% 的方差，但我们强调了任务的难度和未来工作的空间，特别是在实际应用中，例如预测“错误</li>
</ul>

<h3>Title: Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs</h3>
<ul>
<li><strong>Authors: </strong>Clément Christophe, Tathagata Raha, Svetlana Maslenkova, Muhammad Umar Salman, Praveen K Kanithi, Marco AF Pimentel, Shadab Khan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14988">https://arxiv.org/abs/2409.14988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14988">https://arxiv.org/pdf/2409.14988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14988]] Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs(https://arxiv.org/abs/2409.14988)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant potential in transforming clinical applications. In this study, we investigate the efficacy of four techniques in adapting LLMs for clinical use-cases: continuous pretraining, instruct fine-tuning, NEFTune, and prompt engineering. We employ these methods on Mistral 7B and Mixtral 8x7B models, leveraging a large-scale clinical pretraining dataset of 50 billion tokens and an instruct fine-tuning dataset of 500 million tokens. Our evaluation across various clinical tasks reveals the impact of each technique. While continuous pretraining beyond 250 billion tokens yields marginal improvements on its own, it establishes a strong foundation for instruct fine-tuning. Notably, NEFTune, designed primarily to enhance generation quality, surprisingly demonstrates additional gains on our benchmark. Complex prompt engineering methods further enhance performance. These findings show the importance of tailoring fine-tuning strategies and exploring innovative techniques to optimize LLM performance in the clinical domain.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出在临床应用转型方面的巨大潜力。在本研究中，我们研究了四种技术在将 LLM 应用于临床用例方面的有效性：连续预训练、指令微调、NEFTune 和提示工程。我们在 Mistral 7B 和 Mixtral 8x7B 模型上采用了这些方法，利用了 500 亿个标记的大规模临床预训练数据集和 5 亿个标记的指令微调数据集。我们对各种临床任务的评估揭示了每种技术的影响。虽然超过 2500 亿个标记的连续预训练本身会产生微小的改进，但它为指令微调奠定了坚实的基础。值得注意的是，主要用于提高生成质量的 NEFTune 在我们的基准上令人惊讶地显示出额外的收益。复杂的提示工程方法进一步提高了性能。这些发现表明了定制微调策略和探索创新技术以优化临床领域的 LLM 性能的重要性。</li>
</ul>

<h3>Title: Enhancing Aspect-based Sentiment Analysis in Tourism Using Large Language Models and Positional Information</h3>
<ul>
<li><strong>Authors: </strong>Chun Xu, Mengmeng Wang, Yan Ren, Shaolin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.14997">https://arxiv.org/abs/2409.14997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.14997">https://arxiv.org/pdf/2409.14997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.14997]] Enhancing Aspect-based Sentiment Analysis in Tourism Using Large Language Models and Positional Information(https://arxiv.org/abs/2409.14997)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) in tourism plays a significant role in understanding tourists' evaluations of specific aspects of attractions, which is crucial for driving innovation and development in the tourism industry. However, traditional pipeline models are afflicted by issues such as error propagation and incomplete extraction of sentiment elements. To alleviate this issue, this paper proposes an aspect-based sentiment analysis model, ACOS_LLM, for Aspect-Category-Opinion-Sentiment Quadruple Extraction (ACOSQE). The model comprises two key stages: auxiliary knowledge generation and ACOSQE. Firstly, Adalora is used to fine-tune large language models for generating high-quality auxiliary knowledge. To enhance model efficiency, Sparsegpt is utilized to compress the fine-tuned model to 50% sparsity. Subsequently, Positional information and sequence modeling are employed to achieve the ACOSQE task, with auxiliary knowledge and the original text as inputs. Experiments are conducted on both self-created tourism datasets and publicly available datasets, Rest15 and Rest16. Results demonstrate the model's superior performance, with an F1 improvement of 7.49% compared to other models on the tourism dataset. Additionally, there is an F1 improvement of 0.05% and 1.06% on the Rest15 and Rest16 datasets, respectively.</li>
<li><strong>摘要：</strong>基于方面的情绪分析 (ABSA) 在旅游业中发挥着重要作用，有助于了解游客对景点特定方面的评价，这对于推动旅游业的创新和发展至关重要。然而，传统的管道模型受到错误传播和情绪元素提取不完整等问题的困扰。为了缓解这一问题，本文提出了一种基于方面的情绪分析模型 ACOS_LLM，用于方面-类别-观点-情绪四重提取 (ACOSQE)。该模型包括两个关键阶段：辅助知识生成和 ACOSQE。首先，使用 Adalora 对大型语言模型进行微调，以生成高质量的辅助知识。为了提高模型效率，使用 Sparsegpt 将微调后的模型压缩到 50% 的稀疏度。随后，使用位置信息和序列建模来实现 ACOSQE 任务，以辅助知识和原始文本作为输入。实验在自建旅游数据集和公开可用的数据集 Rest15 和 Rest16 上进行。结果证明了该模型的卓越性能，与其他模型相比，旅游数据集上的 F1 提高了 7.49%。此外，在 Rest15 和 Rest16 数据集上，F1 分别提高了 0.05% 和 1.06%。</li>
</ul>

<h3>Title: Inference-Friendly Models With MixAttention</h3>
<ul>
<li><strong>Authors: </strong>Shashank Rajput, Ying Sheng, Sean Owen, Vitaliy Chiley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15012">https://arxiv.org/abs/2409.15012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15012">https://arxiv.org/pdf/2409.15012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15012]] Inference-Friendly Models With MixAttention(https://arxiv.org/abs/2409.15012)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The size of the key-value (KV) cache plays a critical role in determining both the maximum context length and the number of concurrent requests supported during inference in modern language models. The KV cache size grows proportionally with the number of attention heads and the tokens processed, leading to increased memory consumption and slower inference for long inputs. In this work, we explore the use of MixAttention, a model architecture modification closely related to a blog published by this http URL. MixAttention combines sliding window attention, where only a small subset of recent tokens is stored in the KV cache, with KV cache sharing across layers. Our experiments demonstrate that MixAttention significantly reduces memory usage and improves inference speed without sacrificing model performance in both short and long-context tasks. We also explore various configurations of this architecture, identifying those that maintain quality across evaluation metrics while optimizing resource efficiency.</li>
<li><strong>摘要：</strong>在现代语言模型中，键值 (KV) 缓存的大小在确定最大上下文长度和推理期间支持的并发请求数方面起着关键作用。KV 缓存大小与注意力头和处理的标记数量成比例增长，导致内存消耗增加，长输入的推理速度变慢。在这项工作中，我们探索了 MixAttention 的使用，MixAttention 是一种模型架构修改，与此 http URL 发布的博客密切相关。MixAttention 结合了滑动窗口注意力，其中只有一小部分最近的标记存储在 KV 缓存中，并且 KV 缓存在各层之间共享。我们的实验表明，MixAttention 显著降低了内存使用量并提高了推理速度，而不会牺牲模型在短上下文和长上下文任务中的性能。我们还探索了这种架构的各种配置，确定了那些在优化资源效率的同时保持评估指标质量的配置。</li>
</ul>

<h3>Title: Generative LLM Powered Conversational AI Application for Personalized Risk Assessment: A Case Study in COVID-19</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Amin Roshani, Xiangyu Zhou, Yao Qiang, Srinivasan Suresh, Steve Hicks, Usha Sethuraman, Dongxiao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15027">https://arxiv.org/abs/2409.15027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15027">https://arxiv.org/pdf/2409.15027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15027]] Generative LLM Powered Conversational AI Application for Personalized Risk Assessment: A Case Study in COVID-19(https://arxiv.org/abs/2409.15027)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable capabilities in various natural language tasks and are increasingly being applied in healthcare domains. This work demonstrates a new LLM-powered disease risk assessment approach via streaming human-AI conversation, eliminating the need for programming required by traditional machine learning approaches. In a COVID-19 severity risk assessment case study, we fine-tune pre-trained generative LLMs (e.g., Llama2-7b and Flan-t5-xl) using a few shots of natural language examples, comparing their performance with traditional classifiers (i.e., Logistic Regression, XGBoost, Random Forest) that are trained de novo using tabular data across various experimental settings. We develop a mobile application that uses these fine-tuned LLMs as its generative AI (GenAI) core to facilitate real-time interaction between clinicians and patients, providing no-code risk assessment through conversational interfaces. This integration not only allows for the use of streaming Questions and Answers (QA) as inputs but also offers personalized feature importance analysis derived from the LLM's attention layers, enhancing the interpretability of risk assessments. By achieving high Area Under the Curve (AUC) scores with a limited number of fine-tuning samples, our results demonstrate the potential of generative LLMs to outperform discriminative classification methods in low-data regimes, highlighting their real-world adaptability and effectiveness. This work aims to fill the existing gap in leveraging generative LLMs for interactive no-code risk assessment and to encourage further research in this emerging field.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种自然语言任务中表现出非凡的能力，并越来越多地应用于医疗保健领域。这项工作通过流式人机对话展示了一种新的 LLM 驱动的疾病风险评估方法，消除了传统机器学习方法所需的编程需求。在 COVID-19 严重程度风险评估案例研究中，我们使用一些自然语言示例对预训练的生成式 LLM（例如 Llama2-7b 和 Flan-t5-xl）进行微调，将其性能与使用表格数据在各种实验环境中进行从头训练的传统分类器（即 Logistic 回归、XGBoost、随机森林）进行比较。我们开发了一个移动应用程序，该应用程序使用这些经过微调的 LLM 作为其生成式 AI (GenAI) 核心，以促进临床医生和患者之间的实时互动，通过对话界面提供无代码风险评估。这种集成不仅允许使用流式问答 (QA) 作为输入，还提供来自 LLM 注意力层的个性化特征重要性分析，从而增强风险评估的可解释性。通过使用有限数量的微调样本实现高曲线下面积 (AUC) 分数，我们的结果证明了生成式 LLM 在低数据情况下胜过判别式分类方法的潜力，凸显了其在现实世界中的适应性和有效性。这项工作旨在填补利用生成式 LLM 进行交互式无代码风险评估的现有空白，并鼓励在这一新兴领域开展进一步研究。</li>
</ul>

<h3>Title: Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task</h3>
<ul>
<li><strong>Authors: </strong>Gaëtan Caillaut, Raheel Qader, Mariam Nakhlé, Jingshu Liu, Jean-Gabriel Barthélemy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15051">https://arxiv.org/abs/2409.15051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15051">https://arxiv.org/pdf/2409.15051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15051]] Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task(https://arxiv.org/abs/2409.15051)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent studies have showcased remarkable capabilities of decoder-only models in many NLP tasks, including translation. Yet, the machine translation field has been largely dominated by encoder-decoder models based on the Transformer architecture. As a consequence, scaling laws of encoder-decoder models for neural machine translation have already been well studied, but decoder-only models have received less attention. This work explores the scaling laws of decoder-only models on the multilingual and multidomain translation task. We trained a collection of six decoder-only models, ranging from 70M to 7B parameters, on a sentence-level, multilingual and multidomain dataset. We conducted a series of experiments showing that the loss of decoder-only models can be estimated using a scaling law similar to the one discovered for large language models, but we also show that this scaling law has difficulties to generalize to too large models or to a different data distribution. We also study different scaling methods and show that scaling the depth and the width of a model lead to similar test loss improvements, but with different impact on the model's efficiency.</li>
<li><strong>摘要：</strong>最近的研究展示了仅解码器模型在许多 NLP 任务（包括翻译）中的卓越能力。然而，机器翻译领域主要由基于 Transformer 架构的编码器-解码器模型主导。因此，用于神经机器翻译的编码器-解码器模型的缩放规律已经得到很好的研究，但仅解码器模型受到的关注较少。这项工作探索了仅解码器模型在多语言和多领域翻译任务上的缩放规律。我们在句子级、多语言和多领域数据集上训练了六个仅解码器模型的集合，参数范围从 70M 到 7B。我们进行了一系列实验，表明可以使用类似于大型语言模型的缩放规律来估计仅解码器模型的损失，但我们也表明这种缩放规律难以推广到过大的模型或不同的数据分布。我们还研究了不同的缩放方法，并表明缩放模型的深度和宽度会导致类似的测试损失改进，但对模型效率的影响不同。</li>
</ul>

<h3>Title: Brotherhood at WMT 2024: Leveraging LLM-Generated Contextual Conversations for Cross-Lingual Image Captioning</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Betala, Ishan Chokshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15052">https://arxiv.org/abs/2409.15052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15052">https://arxiv.org/pdf/2409.15052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15052]] Brotherhood at WMT 2024: Leveraging LLM-Generated Contextual Conversations for Cross-Lingual Image Captioning(https://arxiv.org/abs/2409.15052)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we describe our system under the team name Brotherhood for the English-to-Lowres Multi-Modal Translation Task. We participate in the multi-modal translation tasks for English-Hindi, English-Hausa, English-Bengali, and English-Malayalam language pairs. We present a method leveraging multi-modal Large Language Models (LLMs), specifically GPT-4o and Claude 3.5 Sonnet, to enhance cross-lingual image captioning without traditional training or fine-tuning. Our approach utilizes instruction-tuned prompting to generate rich, contextual conversations about cropped images, using their English captions as additional context. These synthetic conversations are then translated into the target languages. Finally, we employ a weighted prompting strategy, balancing the original English caption with the translated conversation to generate captions in the target language. This method achieved competitive results, scoring 37.90 BLEU on the English-Hindi Challenge Set and ranking first and second for English-Hausa on the Challenge and Evaluation Leaderboards, respectively. We conduct additional experiments on a subset of 250 images, exploring the trade-offs between BLEU scores and semantic similarity across various weighting schemes.</li>
<li><strong>摘要：</strong>在本文中，我们以 Brotherhood 的团队名称描述了我们的英语到 Lowres 多模态翻译任务系统。我们参与了英语-印地语、英语-豪萨语、英语-孟加拉语和英语-马拉雅拉姆语语言对的多模态翻译任务。我们提出了一种利用多模态大型语言模型 (LLM)（特别是 GPT-4o 和 Claude 3.5 Sonnet）来增强跨语言图像字幕的方法，而无需进行传统的训练或微调。我们的方法利用指令调整的提示来生成关于裁剪图像的丰富上下文对话，并使用其英文字幕作为附加上下文。然后将这些合成对话翻译成目标语言。最后，我们采用加权提示策略，平衡原始英文字幕和翻译对话以生成目标语言字幕。该方法取得了有竞争力的结果，在英语-印地语挑战集上获得 37.90 BLEU，在挑战和评估排行榜上分别排名英语-豪萨语第一和第二。我们对 250 张图像的子集进行了额外的实验，探索了不同加权方案中 BLEU 分数和语义相似度之间的权衡。</li>
</ul>

<h3>Title: Enhancing Scientific Reproducibility Through Automated BioCompute Object Creation Using Retrieval-Augmented Generation from Publications</h3>
<ul>
<li><strong>Authors: </strong>Sean Kim, Raja Mazumder</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.OT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15076">https://arxiv.org/abs/2409.15076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15076">https://arxiv.org/pdf/2409.15076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15076]] Enhancing Scientific Reproducibility Through Automated BioCompute Object Creation Using Retrieval-Augmented Generation from Publications(https://arxiv.org/abs/2409.15076)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The exponential growth in computational power and accessibility has transformed the complexity and scale of bioinformatics research, necessitating standardized documentation for transparency, reproducibility, and regulatory compliance. The IEEE BioCompute Object (BCO) standard addresses this need but faces adoption challenges due to the overhead of creating compliant documentation, especially for legacy research. This paper presents a novel approach to automate the creation of BCOs from scientific papers using Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). We describe the development of the BCO assistant tool that leverages RAG to extract relevant information from source papers and associated code repositories, addressing key challenges such as LLM hallucination and long-context understanding. The implementation incorporates optimized retrieval processes, including a two-pass retrieval with re-ranking, and employs carefully engineered prompts for each BCO domain. We discuss the tool's architecture, extensibility, and evaluation methods, including automated and manual assessment approaches. The BCO assistant demonstrates the potential to significantly reduce the time and effort required for retroactive documentation of bioinformatics research while maintaining compliance with the standard. This approach opens avenues for AI-assisted scientific documentation and knowledge extraction from publications thereby enhancing scientific reproducibility. The BCO assistant tool and documentation is available at this https URL.</li>
<li><strong>摘要：</strong>计算能力和可访问性的迅猛增长改变了生物信息学研究的复杂性和规模，因此需要标准化文档以确保透明度、可重复性和法规遵从性。IEEE BioCompute Object (BCO) 标准满足了这一需求，但由于创建合规文档的开销（尤其是对于传统研究而言），它面临着采用方面的挑战。本文介绍了一种使用检索增强生成 (RAG) 和大型语言模型 (LLM) 自动从科学论文中创建 BCO 的新方法。我们描述了 BCO 辅助工具的开发，该工具利用 RAG 从源论文和相关代码存储库中提取相关信息，解决了 LLM 幻觉和长上下文理解等关键挑战。该实现结合了优化的检索过程，包括重新排序的两遍检索，并为每个 BCO 域采用了精心设计的提示。我们讨论了该工具的架构、可扩展性和评估方法，包括自动和手动评估方法。 BCO 助手展示了在保持符合标准的同时，大幅减少追溯生物信息学研究所需的时间和精力的潜力。这种方法为人工智能辅助科学文献和从出版物中提取知识开辟了途径，从而提高了科学的可重复性。BCO 助手工具和文档可在此 https URL 上找到。</li>
</ul>

<h3>Title: Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory</h3>
<ul>
<li><strong>Authors: </strong>Kunyao Lan, Bingui Jin, Zichen Zhu, Siyuan Chen, Shu Zhang, Kenny Q. Zhu, Mengyue Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15084">https://arxiv.org/abs/2409.15084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15084">https://arxiv.org/pdf/2409.15084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15084]] Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory(https://arxiv.org/abs/2409.15084)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Mental health issues, particularly depressive disorders, present significant challenges in contemporary society, necessitating the development of effective automated diagnostic methods. This paper introduces the Agent Mental Clinic (AMC), a self-improving conversational agent system designed to enhance depression diagnosis through simulated dialogues between patient and psychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we design a psychiatrist agent consisting of a tertiary memory structure, a dialogue control and reflect plugin that acts as ``supervisor'' and a memory sampling module, fully leveraging the skills reflected by the psychiatrist agent, achieving great accuracy on depression risk and suicide risk diagnosis via conversation. Experiment results on datasets collected in real-life scenarios demonstrate that the system, simulating the procedure of training psychiatrists, can be a promising optimization method for aligning LLMs with real-life distribution in specific domains without modifying the weights of LLMs, even when only a few representative labeled cases are available.</li>
<li><strong>摘要：</strong>精神健康问题，特别是抑郁症，是当代社会面临的重大挑战，亟待开发有效的自动化诊断方法。本​​文介绍了 Agent Mental Clinic (AMC)，这是一个自我改进的对话代理系统，旨在通过模拟患者和精神科医生代理之间的对话来增强抑郁症诊断。为了提高对话质量和诊断准确性，我们设计了一个精神科医生代理，由三级记忆结构、充当“监督者”的对话控制和反映插件以及记忆采样模块组成，充分利用精神科医生代理反映的技能，通过对话在抑郁风险和自杀风险诊断方面取得了很高的准确性。在现实场景中收集的数据集上的实验结果表明，该系统模拟了精神科医生的培训过程，可以成为一种有前途的优化方法，用于将 LLM 与特定领域中的真实分布对齐，而无需修改 LLM 的权重，即使在只有少数有代表性的标记案例可用的情况下也是如此。</li>
</ul>

<h3>Title: Scientific Cross-Document Coreference and Hierarchy with Definition-Augmented Relational Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lior Forer, Tom Hope</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15113">https://arxiv.org/abs/2409.15113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15113">https://arxiv.org/pdf/2409.15113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15113]] Scientific Cross-Document Coreference and Hierarchy with Definition-Augmented Relational Reasoning(https://arxiv.org/abs/2409.15113)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We address the fundamental task of inferring cross-document coreference and hierarchy in scientific texts, which has important applications in knowledge graph construction, search, recommendation and discovery. LLMs still struggle when faced with many long-tail technical concepts with nuanced variations. We present a novel method which generates context-dependent definitions of concept mentions by retrieving full-text literature, and uses the definitions to enhance detection of cross-document mention relations. We further generate relational definitions, which describe how two concept mentions are related or different, and design an efficient re-ranking approach to address the combinatorial explosion involved in inferring links across papers. In both fine-tuning and in-context learning settings we achieve large gains in performance. We provide analysis of generated definitions, shedding light on the relational reasoning ability of LLMs over fine-grained scientific texts.</li>
<li><strong>摘要：</strong>我们解决了推断科学文本中跨文档共指和层次结构的基本任务，这在知识图谱构建、搜索、推荐和发现中具有重要应用。当面对许多具有细微变化的长尾技术概念时，LLM 仍然举步维艰。我们提出了一种新方法，该方法通过检索全文文献来生成概念提及的上下文相关定义，并使用这些定义来增强对跨文档提及关系的检测。我们进一步生成关系定义，描述两个概念提及如何相关或不同，并设计一种有效的重新排名方法来解决推断论文间链接所涉及的组合爆炸。在微调和上下文学习设置中，我们都实现了性能的大幅提升。我们对生成的定义进行了分析，揭示了 LLM 在细粒度科学文本上的关系推理能力。</li>
</ul>

<h3>Title: Lessons Learned on Information Retrieval in Electronic Health Records: A Comparison of Embedding Models and Pooling Strategies</h3>
<ul>
<li><strong>Authors: </strong>Skatje Myers, Timothy A. Miller, Yanjun Gao, Matthew M. Churpek, Anoop Mayampurath, Dmitriy Dligach, Majid Afshar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15163">https://arxiv.org/abs/2409.15163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15163">https://arxiv.org/pdf/2409.15163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15163]] Lessons Learned on Information Retrieval in Electronic Health Records: A Comparison of Embedding Models and Pooling Strategies(https://arxiv.org/abs/2409.15163)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Objective: Applying large language models (LLMs) to the clinical domain is challenging due to the context-heavy nature of processing medical records. Retrieval-augmented generation (RAG) offers a solution by facilitating reasoning over large text sources. However, there are many parameters to optimize in just the retrieval system alone. This paper presents an ablation study exploring how different embedding models and pooling methods affect information retrieval for the clinical domain. Methods: Evaluating on three retrieval tasks on two electronic health record (EHR) data sources, we compared seven models, including medical- and general-domain models, specialized encoder embedding models, and off-the-shelf decoder LLMs. We also examine the choice of embedding pooling strategy for each model, independently on the query and the text to retrieve. Results: We found that the choice of embedding model significantly impacts retrieval performance, with BGE, a comparatively small general-domain model, consistently outperforming all others, including medical-specific models. However, our findings also revealed substantial variability across datasets and query text phrasings. We also determined the best pooling methods for each of these models to guide future design of retrieval systems. Discussion: The choice of embedding model, pooling strategy, and query formulation can significantly impact retrieval performance and the performance of these models on other public benchmarks does not necessarily transfer to new domains. Further studies such as this one are vital for guiding empirically-grounded development of retrieval frameworks, such as in the context of RAG, for the clinical domain.</li>
<li><strong>摘要：</strong>目标：由于处理医疗记录的语境繁重，将大型语言模型 (LLM) 应用于临床领域具有挑战性。检索增强生成 (RAG) 通过促进对大型文本源的推理提供了一种解决方案。但是，仅在检索系统中就有许多参数需要优化。本文介绍了一项消融研究，探讨了不同的嵌入模型和池化方法如何影响临床领域的信息检索。方法：在两个电子健康记录 (EHR) 数据源上的三个检索任务上进行评估，我们比较了七种模型，包括医疗和通用领域模型、专用编码器嵌入模型和现成的解码器 LLM。我们还根据查询和要检索的文本独立检查了每个模型的嵌入池化策略的选择。结果：我们发现嵌入模型的选择会显著影响检索性能，其中 BGE 是一个相对较小的通用领域模型，其表现始终优于所有其他模型，包括医疗专用模型。然而，我们的研究结果还揭示了数据集和查询文本措辞之间的巨大差异。我们还确定了每个模型的最佳池化方法，以指导未来的检索系统设计。讨论：嵌入模型、池化策略和查询公式的选择会显著影响检索性能，这些模型在其他公共基准上的性能不一定会转移到新领域。像这样的进一步研究对于指导临床领域的基于经验的检索框架的开发至关重要，例如在 RAG 的背景下。</li>
</ul>

<h3>Title: PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron, Tabor Flickinger, Laura E. Barnes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15188">https://arxiv.org/abs/2409.15188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15188">https://arxiv.org/pdf/2409.15188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15188]] PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models(https://arxiv.org/abs/2409.15188)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues. Although existing natural language processing (NLP) techniques show promise, they struggle with the nuances of clinical communication and require sensitive clinical data for training, reducing their effectiveness in real-world applications. Emerging large language models (LLMs) offer a new approach to assessing complex communication metrics, with the potential to advance the field through integration into passive sensing and just-in-time intervention systems. This study explores LLMs as evaluators of palliative care communication quality, leveraging their linguistic, in-context learning, and reasoning capabilities. Specifically, using simulated scripts crafted and labeled by healthcare professionals, we test proprietary models (e.g., GPT-4) and fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset generated by GPT-4 to evaluate clinical conversations, to identify key metrics such as `understanding' and `empathy'. Our findings demonstrated LLMs' superior performance in evaluating clinical communication, providing actionable feedback with reasoning, and demonstrating the feasibility and practical viability of developing in-house LLMs. This research highlights LLMs' potential to enhance patient-provider interactions and lays the groundwork for downstream steps in developing LLM-empowered clinical health systems.</li>
<li><strong>摘要：</strong>有效的医患沟通对临床护理至关重要，直接影响患者的治疗结果和生活质量。传统的评估方法，例如人工评分、患者反馈和提供者自我评估，通常受到高成本和可扩展性问题的限制。尽管现有的自然语言处理 (NLP) 技术前景光明，但它们难以处理临床沟通的细微差别，并且需要敏感的临床数据进行训练，从而降低了它们在实际应用中的有效性。新兴的大型语言模型 (LLM) 提供了一种评估复杂沟通指标的新方法，有可能通过集成到被动感知和即时干预系统中来推动该领域的发展。本研究探索了 LLM 作为姑息治疗沟通质量评估者的作用，利用其语言、情境学习和推理能力。具体来说，我们使用由医疗保健专业人员编写和标记的模拟脚本，测试专有模型（例如 GPT-4）并使用 GPT-4 生成的合成数据集微调开源 LLM（例如 LLaMA2）以评估临床对话，以确定“理解”和“同理心”等关键指标。我们的研究结果表明，LLM 在评估临床沟通、提供具有推理能力的可行反馈以及展示开发内部 LLM 的可行性和实际可行性方面表现出色。这项研究突出了 LLM 增强医患互动的潜力，并为开发 LLM 赋能的临床医疗系统的后续步骤奠定了基础。</li>
</ul>

<h3>Title: Learning from Contrastive Prompts: Automated Optimization and Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Mingqi Li, Karan Aggarwal, Yong Xie, Aitzaz Ahmad, Stephen Lau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15199">https://arxiv.org/abs/2409.15199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15199">https://arxiv.org/pdf/2409.15199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15199]] Learning from Contrastive Prompts: Automated Optimization and Adaptation(https://arxiv.org/abs/2409.15199)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>As LLMs evolve, significant effort is spent on manually crafting prompts. While existing prompt optimization methods automate this process, they rely solely on learning from incorrect samples, leading to a sub-optimal performance. Additionally, an unexplored challenge in the literature is prompts effective for prior models may not perform well on newer versions or different languages. We propose the Learning from Contrastive Prompts (LCP) framework to address these gaps, enhancing both prompt optimization and adaptation. LCP employs contrastive learning to generate effective prompts by analyzing patterns in good and bad prompt examples. Our evaluation on the Big-Bench Hard dataset shows that LCP has a win rate of over 76% over existing methods in prompt optimization and demonstrates strong adaptability across different model versions, families, and languages. LCP offers a systematic approach to prompt engineering, reducing manual effort in deploying LLMs across varied contexts.</li>
<li><strong>摘要：</strong>随着 LLM 的发展，人们在手动制作提示上投入了大量精力。虽然现有的提示优化方法可以自动执行此过程，但它们仅依赖于从不正确的样本中学习，导致性能不佳。此外，文献中尚未探索的一个挑战是，对先前模型有效的提示可能在较新版本或不同语言上表现不佳。我们提出了从对比提示中学习 (LCP) 框架来解决这些差距，增强提示优化和适应性。LCP 采用对比学习通过分析好提示和坏提示示例中的模式来生成有效提示。我们对 Big-Bench Hard 数据集的评估表明，LCP 在提示优化方面比现有方法的胜率超过 76%，并且在不同的模型版本、系列和语言中表现出很强的适应性。LCP 提供了一种系统的提示工程方法，减少了在不同环境中部署 LLM 的手动工作量。</li>
</ul>

<h3>Title: MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Junqing He, Liang Zhu, Qi Wei, Rui Wang, Jiaxing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15240">https://arxiv.org/abs/2409.15240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15240">https://arxiv.org/pdf/2409.15240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15240]] MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems(https://arxiv.org/abs/2409.15240)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, chat</a></li>
<li><strong>Abstract: </strong>Long-term memory is so important for chatbots and dialogue systems (DS) that researchers have developed numerous memory-augmented DS. However, their evaluation methods are different from the real situation in human conversation. They only measured the accuracy of factual information or the perplexity of generated responses given a query, which hardly reflected their performance. Moreover, they only consider passive memory retrieval based on similarity, neglecting diverse memory-recalling paradigms in humans, e.g. emotions and surroundings. To bridge the gap, we construct a novel benchmark covering various memory recalling paradigms based on cognitive science and psychology theory. The Memory Benchmark (MemBench) contains two tasks according to the two-phrase theory in cognitive science: memory retrieval, memory recognition and injection. The benchmark considers both passive and proactive memory recalling based on meta information for the first time. In addition, novel scoring aspects are proposed to comprehensively measure the generated responses. Results from the strongest embedding models and LLMs on MemBench show that there is plenty of room for improvement in existing dialogue systems. Extensive experiments also reveal the correlation between memory injection and emotion supporting (ES) skillfulness, and intimacy. Our code and dataset will be released.</li>
<li><strong>摘要：</strong>长期记忆对于聊天机器人和对话系统 (DS) 非常重要，因此研究人员开发了许多增强记忆的 DS。然而，它们的评估方法与人类对话的实际情况不同。它们仅测量事实信息的准确性或给定查询的生成响应的困惑度，这几乎不能反映它们的性能。此外，它们仅考虑基于相似性的被动记忆检索，而忽略了人类的各种记忆回忆范式，例如情绪和周围环境。为了弥补这一差距，我们构建了一个基于认知科学和心理学理论的新基准，涵盖了各种记忆回忆范式。根据认知科学中的两阶段理论，记忆基准 (MemBench) 包含两个任务：记忆检索、记忆识别和注入。该基准首次考虑了基于元信息的被动和主动记忆回忆。此外，还提出了新颖的评分方面来全面衡量生成的响应。MemBench 上最强的嵌入模型和 LLM 的结果表明，现有对话系统有很大的改进空间。大量实验还揭示了记忆注入与情感支持 (ES) 技巧和亲密度之间的相关性。我们的代码和数据集即将发布。</li>
</ul>

<h3>Title: Behavioral Bias of Vision-Language Models: A Behavioral Finance View</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Xiao, Yudi Lin, Ming-Chang Chiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15256">https://arxiv.org/abs/2409.15256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15256">https://arxiv.org/pdf/2409.15256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15256]] Behavioral Bias of Vision-Language Models: A Behavioral Finance View(https://arxiv.org/abs/2409.15256)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models. However, we should carefully evaluate their applications in different domains, as they may possess undesired biases. Our work studies the potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary subject that jointly considers finance and psychology. We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs' reasoning capabilities and the dynamic behaviors manifested in two established human financial behavioral biases: recency bias and authority bias. Our evaluations find that recent open-source LVLMs such as LLaVA-NeXT, MobileVLM-V2, Mini-Gemini, MiniCPM-Llama3-V 2.5 and Phi-3-vision-128k suffer significantly from these two biases, while the proprietary model GPT-4o is negligibly impacted. Our observations highlight directions in which open-source models can improve. The code is available at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 配备视觉模块以创建更像人类的模型，大型视觉语言模型 (LVLM) 也迅速发展。然而，我们应该仔细评估它们在不同领域的应用，因为它们可能具有不良偏见。我们的工作从行为金融学的角度研究了 LVLM 的潜在行为偏见，行为金融学是一门综合考虑金融和心理学的跨学科学科。我们提出了一个端到端的框架，从数据收集到新的评估指标，以评估 LVLM 的推理能力和两种既定的人类金融行为偏见所表现的动态行为：近因偏见和权威偏见。我们的评估发现，最近的开源 LVLM，如 LLaVA-NeXT、MobileVLM-V2、Mini-Gemini、MiniCPM-Llama3-V 2.5 和 Phi-3-vision-128k 都受到这两种偏见的严重影响，而专有模型 GPT-4o 受到的影响可以忽略不计。我们的观察突出了开源模型可以改进的方向。代码可在此 https URL 上获取。</li>
</ul>

<h3>Title: OmniBench: Towards The Future of Universal Omni-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15272">https://arxiv.org/abs/2409.15272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15272">https://arxiv.org/pdf/2409.15272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15272]] OmniBench: Towards The Future of Universal Omni-Language Models(https://arxiv.org/abs/2409.15272)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define models capable of such tri-modal processing as omni-language models (OLMs). OmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that: i) open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; and ii) the baseline models perform poorly (below 50% accuracy) even when provided with alternative textual representations of images and audio. These results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. We advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities. The codes and live leaderboard could be found at this https URL.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 的最新进展旨在整合和解释跨多种模态的数据。然而，这些模型同时处理和推理多种模态的能力仍未得到充分探索，部分原因是缺乏全面的模态基准。我们引入了 OmniBench，这是一种新颖的基准，旨在严格评估模型同时识别、解释和推理视觉、听觉和文本输入的能力。我们将能够进行这种三模态处理的模型定义为全语言模型 (OLM)。OmniBench 以高质量的人工注释而著称，确保准确的响应需要跨所有三种模态的综合理解和推理。我们的主要发现表明：i) 开源 OLM 在三模态环境中的指令遵循和推理能力方面表现出严重限制；ii) 即使提供图像和音频的替代文本表示，基线模型的表现也很差（准确率低于 50%）。这些结果表明，现有的 MLLM 训练范式经常忽视从文本、图像和音频构建一致上下文的能力。我们主张未来的研究重点是开发更强大的三模态集成技术和训练策略，以增强 OLM 在不同模态下的性能。代码和实时排行榜可在此 https URL 中找到。</li>
</ul>

<h3>Title: A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?</h3>
<ul>
<li><strong>Authors: </strong>Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15277">https://arxiv.org/abs/2409.15277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15277">https://arxiv.org/pdf/2409.15277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15277]] A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?(https://arxiv.org/abs/2409.15277)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs at this https URL for future research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各个领域和任务中都表现出了卓越的能力，突破了我们在学习和认知方面的知识界限。最新模型 OpenAI 的 o1 脱颖而出，成为第一个使用强化学习策略的内化思维链技术的 LLM。虽然它在各种一般语言任务上表现出令人惊讶的强大能力，但它在医学等专业领域的表现仍然未知。为此，本报告对 o1 在不同医疗场景中的表现进行了全面探索，研究了 3 个关键方面：理解、推理和多语言性。具体来说，我们的评估涵盖了 6 个任务，使用来自 37 个医学数据集的数据，其中包括两个基于《新英格兰医学杂志》(NEJM) 和《柳叶刀》的专业医学测验新构建的更具挑战性的问答 (QA) 任务。与 MedQA 等标准医学 QA 基准相比，这些数据集具有更高的临床相关性，可以更有效地转化为现实世界的临床效用。我们对 o1 的分析表明，LLM 的推理能力增强可能会（显著）有利于他们理解各种医疗指令和推理复杂临床场景的能力。值得注意的是，在 19 个数据集和两个新创建的复杂 QA 场景中，o1 的准确率平均比之前的 GPT-4 高出 6.2% 和 6.6%。但与此同时，我们发现模型能力和现有评估协议都存在一些弱点，包括幻觉、多语言能力不一致以及评估指标不一致。我们在此 https URL 上发布原始数据和模型输出，以供将来研究。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
