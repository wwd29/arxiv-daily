<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-05</h1>
<h3>Title: Euskarazko lehen C1 ebaluatzaile automatikoa</h3>
<ul>
<li><strong>Authors: </strong>Ekhi Azurmendi, Oier Lopez de Lacalle</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01851">https://arxiv.org/abs/2503.01851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01851">https://arxiv.org/pdf/2503.01851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01851]] Euskarazko lehen C1 ebaluatzaile automatikoa(https://arxiv.org/abs/2503.01851)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Throughout this project, we have attempted to develop an automatic evaluator that determines whether Basque language compositions meet the C1 level. To achieve our goal, we obtained 10,000 transcribed compositions through an agreement between HABE and HiTZ to train our system. We have developed different techniques to avoid data scarcity and system overfitting: EDA, SCL and regulation; We have also conducted tests with different Language Models to analyze their behavior. Finally, we have also performed analyses of different system behaviors to measure model calibration and the impact of artifacts. -- Proiektu honetan zehar euskarazko idazlanek C1 maila duten edo ez zehazten duen ebaluatzaile automatiko bat garatzen saiatu gara. Gure helburua betetzeko HABE eta HiTZ arteko hitzarmenaren bitartez 10.000 transkribatutako idazlan eskuratu ditugu gure sistema entrenatzeko. Datu eskasia eta sistemaren gaindoitzea ekiditeko teknika ezberdinak landu ditugu: EDA, SCL eta erregulazioa; Hizkuntza Eredu ezberdinekin ere probak egin ditugu duten portaera aztertzeko. Azkenik, sistema ezberdinen portaeren analisiak ere egin ditugu, ereduen kalibrazioa eta artefaktuen eragina neurtzeko.</li>
<li><strong>摘要：</strong>在整个项目中，我们试图开发一个自动评估者，该评估者确定巴斯克语言构图是否符合C1水平。为了实现我们的目标，我们通过Habe和Hitz之间的协议获得了10,000份转录的作品，以训练我们的系统。我们已经开发了不同的技术来避免数据稀缺和系统过度拟合：EDA，SCL和调节；我们还使用不同的语言模型进行了测试，以分析其行为。最后，我们还对不同系统行为进行了分析，以衡量模型校准和伪影的影响。 -  Proiektu Honetan Zehar Euskarazko Idazlanek C1 Maila Duten Edo Ez ez Zehazten duen ebaluatzaile autautiko automatiko bat garatzen saiatu gara。 Gure Helurua Betetzeko Habe Eta Hitz hitz arteko hitzarmenaren bitartez 10.000 transkribatutako idazlan eskuratu ditugu ditugu gure sistema intrenatzeko。 datu eskasia eta sistemaren gaindoitzea ekiditeko teknika ezberdinak landu ditugu：eda，scl eta eTa erregulazioa; Hizkuntza eredu ezberdinekin erere erak egin ditugu duten portaera aztertzeko。 Azkenik，Sistema Ezberdinen Portaeren Analisiak Ere egin Ditugu，Ereduen Kalibrazioa Eta Artefaktuen Eragina eragina neurtzeko。</li>
</ul>

<h3>Title: A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Geng, Qing Li, Herbert Woisetschlaeger, Zongxiong Chen, Yuxia Wang, Preslav Nakov, Hans-Arno Jacobsen, Fakhri Karray</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01854">https://arxiv.org/abs/2503.01854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01854">https://arxiv.org/pdf/2503.01854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01854]] A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models(https://arxiv.org/abs/2503.01854)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.</li>
<li><strong>摘要：</strong>这项研究研究了在大语言模型（LLMS）的上下文中的机器学习技术，称为\ textit {llm uncorning}。 LLM Uncorning提供了一种原则性的方法，可以从LLMS中删除不良数据（例如敏感或非法信息）的影响，同时保留其整体实用程序而无需进行全面重新培训。尽管研究的兴趣日益增加，但没有系统地组织现有工作并提炼关键见解的全面调查。在这里，我们的目标是弥合这一差距。我们首先介绍LLM学习的定义和范例，然后是现有未学习研究的全面分类法。接下来，我们将当前的学习方法分类，总结其优势和局限性。此外，我们回顾了评估指标和基准，提供了当前评估方法的结构化概述。最后，我们概述了未来研究的有希望的方向，强调了该领域的主要挑战和机遇。</li>
</ul>

<h3>Title: Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition Learning</h3>
<ul>
<li><strong>Authors: </strong>Jeremi I. Kaczmarek, Jakub Pokrywka, Krzysztof Biedalak, Grzegorz Kurzyp, Łukasz Grzybowski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01859">https://arxiv.org/abs/2503.01859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01859">https://arxiv.org/pdf/2503.01859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01859]] Optimizing Retrieval-Augmented Generation of Medical Content for Spaced Repetition Learning(https://arxiv.org/abs/2503.01859)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Advances in Large Language Models revolutionized medical education by enabling scalable and efficient learning solutions. This paper presents a pipeline employing Retrieval-Augmented Generation (RAG) system to prepare comments generation for Poland's State Specialization Examination (PES) based on verified resources. The system integrates these generated comments and source documents with a spaced repetition learning algorithm to enhance knowledge retention while minimizing cognitive overload. By employing a refined retrieval system, query rephraser, and an advanced reranker, our modified RAG solution promotes accuracy more than efficiency. Rigorous evaluation by medical annotators demonstrates improvements in key metrics such as document relevance, credibility, and logical coherence of generated content, proven by a series of experiments presented in the paper. This study highlights the potential of RAG systems to provide scalable, high-quality, and individualized educational resources, addressing non-English speaking users.</li>
<li><strong>摘要：</strong>大型语言模型的进步通过实现可扩展有效的学习解决方案彻底改变了医学教育。本文提出了一条采用检索功能的生成系统（RAG）系统的管道，根据经过验证的资源为波兰州专业化考试（PES）准备评论生成。该系统将这些生成的注释和源文档与间隔重复学习算法集成在一起，以增强知识的保留，同时最大程度地减少认知过载。通过采用精致的检索系统，查询改造器和高级读取器，我们修改的抹布解决方案比效率促进了准确性。医学注释者进行严格的评估表明，关键指标的改进，例如文档相关性，可信度和逻辑连贯性，这是通过本文中提出的一系列实验证明的。这项研究强调了抹布系统提供可扩展，高质量和个性化的教育资源的潜力，从而解决了非英语用户。</li>
</ul>

<h3>Title: From Small to Large Language Models: Revisiting the Federalist Papers</h3>
<ul>
<li><strong>Authors: </strong>So Won Jeong, Veronika Rockova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01869">https://arxiv.org/abs/2503.01869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01869">https://arxiv.org/pdf/2503.01869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01869]] From Small to Large Language Models: Revisiting the Federalist Papers(https://arxiv.org/abs/2503.01869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>For a long time, the authorship of the Federalist Papers had been a subject of inquiry and debate, not only by linguists and historians but also by statisticians. In what was arguably the first Bayesian case study, Mosteller and Wallace (1963) provided the first statistical evidence for attributing all disputed papers to Madison. Our paper revisits this historical dataset but from a lens of modern language models, both small and large. We review some of the more popular Large Language Model (LLM) tools and examine them from a statistical point of view in the context of text classification. We investigate whether, without any attempt to fine-tune, the general embedding constructs can be useful for stylometry and attribution. We explain differences between various word/phrase embeddings and discuss how to aggregate them in a document. Contrary to our expectations, we exemplify that dimension expansion with word embeddings may not always be beneficial for attribution relative to dimension reduction with topic embeddings. Our experiments demonstrate that default LLM embeddings (even after manual fine-tuning) may not consistently improve authorship attribution accuracy. Instead, Bayesian analysis with topic embeddings trained on ``function words" yields superior out-of-sample classification performance. This suggests that traditional (small) statistical language models, with their interpretability and solid theoretical foundation, can offer significant advantages in authorship attribution tasks. The code used in this analysis is available at this http URL</li>
<li><strong>摘要：</strong>长期以来，联邦主义者论文的作者一直是询问和辩论的主题，不仅是语言学家和历史学家，而且是统计学家的主题。在可以说是第一个贝叶斯案例研究的情况下，Mosteller and Wallace（1963）提供了将所有有争议的论文归因于麦迪逊的第一个统计证据。我们的论文重新审视了这个历史数据集，但从大小的现代语言模型中。我们回顾一些更流行的大型语言模型（LLM）工具，并在文本分类的背景下从统计的角度检查它们。我们调查是否没有尝试进行任何尝试，一般嵌入构建体可用于风格和归因。我们解释了各种单词/短语嵌入之间的差异，并讨论了如何在文档中汇总它们。与我们的期望相反，我们说明了用单词嵌入的维度扩展可能并不总是有益于归因于降低主题嵌入的尺寸。我们的实验表明，默认的LLM嵌入（即使在手动微调之后）也可能无法始终提高作者身份归因精度。取而代之的是，对``功能词''训练的主题嵌入的贝叶斯分析得出了出色的样本外分类性能。这表明传统（小）统计语言模型具有可解释性和稳固的理论基础，可以在作者身份归因任务中具有很大的优势。此分析可在此分析中使用HTTP URL。</li>
</ul>

<h3>Title: Can Large Language Models Extract Customer Needs as well as Professional Analysts?</h3>
<ul>
<li><strong>Authors: </strong>Artem Timoshenko, Chengfeng Mao, John R. Hauser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01870">https://arxiv.org/abs/2503.01870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01870">https://arxiv.org/pdf/2503.01870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01870]] Can Large Language Models Extract Customer Needs as well as Professional Analysts?(https://arxiv.org/abs/2503.01870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Identifying customer needs (CNs) is important for product management, product development, and marketing. Applications rely on professional analysts interpreting textual data (e.g., interview transcripts, online reviews) to understand the nuances of customer experience and concisely formulate "jobs to be done." The task is cognitively complex and time-consuming. Current practice facilitates the process with keyword search and machine learning but relies on human judgment to formulate CNs. We examine whether Large Language Models (LLMs) can automatically extract CNs. Because evaluating CNs requires professional judgment, we partnered with a marketing consulting firm to conduct a blind study of CNs extracted by: (1) a foundational LLM with prompt engineering only (Base LLM), (2) an LLM fine-tuned with professionally identified CNs (SFT LLM), and (3) professional analysts. The SFT LLM performs as well as or better than professional analysts when extracting CNs. The extracted CNs are well-formulated, sufficiently specific to identify opportunities, and justified by source content (no hallucinations). The SFT LLM is efficient and provides more complete coverage of CNs. The Base LLM was not sufficiently accurate or specific. Organizations can rely on SFT LLMs to reduce manual effort, enhance the precision of CN articulation, and provide improved insight for innovation and marketing strategy.</li>
<li><strong>摘要：</strong>确定客户需求（CNS）对于产品管理，产品开发和营销非常重要。应用程序依靠专业分析师解释文本数据（例如，面试成绩单，在线评论）来了解客户体验的细微差别，并简单地提出“工作要做”。该任务在认知上复杂且耗时。当前的实践通过关键字搜索和机器学习促进了该过程，但依靠人类的判断来制定CNS。我们检查大型语言模型（LLMS）是否可以自动提取CNS。由于评估CNS需要专业判断，因此我们与一家营销咨询公司合作，对提取的CNS进行了盲目研究：（1）具有及时工程（基本LLM）的基础LLM（基本LLM），（2）使用专业识别的CNS（SFT LLM）和（3）专业分析师进行专业识别的LLM。提取CNS时，SFT LLM的性能比专业分析师的性能和更好。提取的中枢神经系统已良好，足以确定机会，并通过源含量（无幻觉）证明。 SFT LLM是有效的，并提供了CNS的更完整覆盖范围。基础LLM不够精确或具体。组织可以依靠SFT LLM来减少手动努力，提高CN发音的精度，并为创新和营销策略提供改进的见解。</li>
</ul>

<h3>Title: Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yaxuan Kong, Yiyuan Yang, Yoontae Hwang, Wenjie Du, Stefan Zohren, Zhangyang Wang, Ming Jin, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01875">https://arxiv.org/abs/2503.01875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01875">https://arxiv.org/pdf/2503.01875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01875]] Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement(https://arxiv.org/abs/2503.01875)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Time series data are foundational in finance, healthcare, and energy domains. However, most existing methods and datasets remain focused on a narrow spectrum of tasks, such as forecasting or anomaly detection. To bridge this gap, we introduce Time Series Multi-Task Question Answering (Time-MQA), a unified framework that enables natural language queries across multiple time series tasks - numerical analytical tasks and open-ended question answering with reasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset containing $\sim$200k question-answer pairs derived from diverse time series spanning environment, traffic, etc. This comprehensive resource covers various time series lengths and promotes robust model development. We further demonstrate how continually pre-training large language models (Mistral 7B, Llama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning capabilities, moving beyond mere numeric tasks and enabling more advanced and intuitive interactions with temporal data. The complete TSQA dataset, models, executable codes, user study questionnaires for evaluation, and results have all been open-sourced.</li>
<li><strong>摘要：</strong>时间序列数据是金融，医疗保健和能源领域的基础。但是，大多数现有的方法和数据集仍集中在狭窄的任务范围上，例如预测或异常检测。为了弥合这一差距，我们介绍了时间序列多任务问题回答（Time-MQA），这是一个统一的框架，可以在多个时间序列任务中进行自然语言查询 - 数值分析任务和通过推理回答的开放式问题。 Time-MQA的核心是TSQA数据集，TSQA数据集是一个大规模数据集，其中包含$ \ sim $ \ sim $ 200K的问题 - 答案对，源自不同的时间序列跨度环境，流量等。这种综合资源涵盖了各种时间序列的长度，并促进了强大的模型开发。我们进一步展示了TSQA数据集增强时间序列推理能力，超越单纯的数字任务并促进与时间数据更高级和直觉的交互作用，如何在TSQA数据集增强时间序列推理功能上不断预训练大型语言模型（Mistral 7b，Llama-3 8b和Qwen-2.5 7b）。完整的TSQA数据集，模型，可执行的代码，用于评估的用户研究问卷以及结果都是开源的。</li>
</ul>

<h3>Title: BEYONDWORDS is All You Need: Agentic Generative AI based Social Media Themes Extractor</h3>
<ul>
<li><strong>Authors: </strong>Mohammed-Khalil Ghali, Abdelrahman Farrag, Sarah Lam, Daehan Won</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01880">https://arxiv.org/abs/2503.01880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01880">https://arxiv.org/pdf/2503.01880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01880]] BEYONDWORDS is All You Need: Agentic Generative AI based Social Media Themes Extractor(https://arxiv.org/abs/2503.01880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Thematic analysis of social media posts provides a major understanding of public discourse, yet traditional methods often struggle to capture the complexity and nuance of unstructured, large-scale text data. This study introduces a novel methodology for thematic analysis that integrates tweet embeddings from pre-trained language models, dimensionality reduction using and matrix factorization, and generative AI to identify and refine latent themes. Our approach clusters compressed tweet representations and employs generative AI to extract and articulate themes through an agentic Chain of Thought (CoT) prompting, with a secondary LLM for quality assurance. This methodology is applied to tweets from the autistic community, a group that increasingly uses social media to discuss their experiences and challenges. By automating the thematic extraction process, the aim is to uncover key insights while maintaining the richness of the original discourse. This autism case study demonstrates the utility of the proposed approach in improving thematic analysis of social media data, offering a scalable and adaptable framework that can be applied to diverse contexts. The results highlight the potential of combining machine learning and Generative AI to enhance the depth and accuracy of theme identification in online communities.</li>
<li><strong>摘要：</strong>社交媒体帖子的主题分析提供了对公共话语的主要理解，但是传统方法通常难以捕获非结构化的大规模文本数据的复杂性和细微差别。这项研究介绍了一种新型的主题分析方法，该方法将来自预训练的语言模型的推文嵌入，使用和基质分解降低维度，以及生成的AI，以识别和完善潜在主题。我们的方法群集压缩了推文表示，并采用生成的AI通过代理思想链（COT）提示提取和阐明主题，并具有辅助LLM以进行质量保证。该方法应用于自闭症社区的推文，该小组越来越多地利用社交媒体讨论他们的经验和挑战。通过自动化主题提取过程，目的是在保持原始话语的丰富性的同时发现关键的见解。这项自闭症案例研究证明了拟议方法在改善社交媒体数据的主题分析方面的实用性，提供了可扩展和适应性的框架，可应用于不同的环境。结果突出了将机器学习和生成AI结合起来的潜力，以增强在线社区中主题识别的深度和准确性。</li>
</ul>

<h3>Title: An Empirical Analysis of LLMs for Countering Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Adiba Mahbub Proma, Neeley Pate, James Druckman, Gourab Ghoshal, Hangfeng He, Ehsan Hoque</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01902">https://arxiv.org/abs/2503.01902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01902">https://arxiv.org/pdf/2503.01902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01902]] An Empirical Analysis of LLMs for Countering Misinformation(https://arxiv.org/abs/2503.01902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) can amplify online misinformation, they also show promise in tackling misinformation. In this paper, we empirically study the capabilities of three LLMs -- ChatGPT, Gemini, and Claude -- in countering political misinformation. We implement a two-step, chain-of-thought prompting approach, where models first identify credible sources for a given claim and then generate persuasive responses. Our findings suggest that models struggle to ground their responses in real news sources, and tend to prefer citing left-leaning sources. We also observe varying degrees of response diversity among models. Our findings highlight concerns about using LLMs for fact-checking through only prompt-engineering, emphasizing the need for more robust guardrails. Our results have implications for both researchers and non-technical users.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）可以扩大在线错误信息，但它们也表现出解决错误信息的希望。在本文中，我们从经验上研究了三个LLM的能力 -  Chatgpt，Gemini和Claude-反对政治错误信息。我们实施了两步，经过深思熟虑的提示方法，模型首先确定了给定索赔的可靠来源，然后产生有说服力的响应。我们的发现表明，模型努力在真实新闻来源中对他们的回应进行基础，并且倾向于引用左倾来源。我们还观察到模型之间不同程度的响应多样性。我们的发现强调了仅通过及时工程来使用LLM进行事实检查的担忧，强调需要更强大的护栏。我们的结果对研究人员和非技术用户都有影响。</li>
</ul>

<h3>Title: PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice</h3>
<ul>
<li><strong>Authors: </strong>Ruoxi Wang, Shuyu Liu, Ling Zhang, Xuequan Zhu, Rui Yang, Xinzhu Zhou, Fei Wu, Zhi Yang, Cheng Jin, Gang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01903">https://arxiv.org/abs/2503.01903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01903">https://arxiv.org/pdf/2503.01903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01903]] PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice(https://arxiv.org/abs/2503.01903)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) offers potential solutions to address problems such as shortage of medical resources and low diagnostic consistency in psychiatric clinical practice. Despite this potential, a robust and comprehensive benchmarking framework to assess the efficacy of LLMs in authentic psychiatric clinical environments is absent. This has impeded the advancement of specialized LLMs tailored to psychiatric applications. In response to this gap, by incorporating clinical demands in psychiatry and clinical data, we proposed a benchmarking system, PsychBench, to evaluate the practical performance of LLMs in psychiatric clinical settings. We conducted a comprehensive quantitative evaluation of 16 LLMs using PsychBench, and investigated the impact of prompt design, chain-of-thought reasoning, input text length, and domain-specific knowledge fine-tuning on model performance. Through detailed error analysis, we identified strengths and potential limitations of the existing models and suggested directions for improvement. Subsequently, a clinical reader study involving 60 psychiatrists of varying seniority was conducted to further explore the practical benefits of existing LLMs as supportive tools for psychiatrists of varying seniority. Through the quantitative and reader evaluation, we show that while existing models demonstrate significant potential, they are not yet adequate as decision-making tools in psychiatric clinical practice. The reader study further indicates that, as an auxiliary tool, LLM could provide particularly notable support for junior psychiatrists, effectively enhancing their work efficiency and overall clinical quality. To promote research in this area, we will make the dataset and evaluation framework publicly available, with the hope of advancing the application of LLMs in psychiatric clinical settings.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的出现提供了潜在的解决方案，以解决诸如医疗资源短缺和精神病临床实践中诊断一致性较低的问题。尽管存在这一潜力，但仍不存在一个稳健而全面的基准测试框架，以评估LLM在正宗的精神病临床环境中的功效。这阻碍了针对精神病应用量身定制的专业LLMS的进步。为了应对这一差距，通过将临床需求纳入精神病学和临床数据中，我们提出了一个基准测试系统Psych Bench，以评估LLM在精神病临床环境中的实际表现。我们使用Psychbench对16个LLM进行了全面的定量评估，并研究了及时设计，经过思考推理，输入文本长度和特定领域的知识对模型性能的影响。通过详细的错误分析，我们确定了现有模型的优势和潜在局限性以及改进的建议指示。随后，进行了一项涉及60位不同资历的精神科医生的临床读者研究，以进一步探索现有LLMS作为不同资历的精神病医生的支持工具的实际好处。通过定量和读者的评估，我们表明，尽管现有模型具有巨大的潜力，但它们在精神病临床实践中尚未足够作为决策工具。读者的研究进一步表明，作为一种辅助工具，LLM可以为初级精神科医生提供特别值得注意的支持，从而有效提高其工作效率和整体临床质量。为了促进该领域的研究，我们将公开提供数据集和评估框架，希望推进LLM在精神病临床环境中的应用。</li>
</ul>

<h3>Title: NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT</h3>
<ul>
<li><strong>Authors: </strong>Jiaying Hong, Thanet Markchom, Jianfei Xu, Tong Wu, Huizhi Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01921">https://arxiv.org/abs/2503.01921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01921">https://arxiv.org/pdf/2503.01921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01921]] NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT(https://arxiv.org/abs/2503.01921)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in content generated by various large language models (LLMs) across multiple languages. This task involves not only identifying the presence of hallucinations but also pinpointing their specific occurrences. To tackle this challenge, this study introduces two methods: modified RefChecker and modified SelfCheckGPT. The modified RefChecker integrates prompt-based factual verification into References, structuring them as claim-based tests rather than single external knowledge sources. The modified SelfCheckGPT incorporates external knowledge to overcome its reliance on internal knowledge. In addition, both methods' original prompt designs are enhanced to identify hallucinated words within LLM-generated texts. Experimental results demonstrate the effectiveness of the approach, achieving a high ranking on the test dataset in detecting hallucinations across various languages, with an average IoU of 0.5310 and an average COR of 0.5669.</li>
<li><strong>摘要：</strong>Semeval-2025 Task 3（MU浴室）重点是检测多种语言各种大型语言模型（LLM）产生的内容中的幻觉。这项任务不仅涉及确定幻觉的存在，还涉及指出其特定事件。为了应对这一挑战，这项研究介绍了两种方法：修改后的refchecker和修改后的自我检查。修改后的Refchecker将基于及时的事实验证集成到参考文献中，将其构成基于索赔的测试，而不是单个外部知识源。修改后的自我检查符合外部知识，以克服其对内部知识的依赖。此外，两种方法的原始提示设计都得到了增强，以识别LLM生成的文本中的幻觉单词。实验结果证明了该方法的有效性，在检测各种语言的幻觉方面在测试数据集上取得了很高的排名，平均IOU为0.5310，平均COR为0.5669。</li>
</ul>

<h3>Title: Output Length Effect on DeepSeek-R1's Safety in Forced Thinking</h3>
<ul>
<li><strong>Authors: </strong>Xuying Li, Zhuo Li, Yuji Kosuga, Victor Bian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01923">https://arxiv.org/abs/2503.01923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01923">https://arxiv.org/pdf/2503.01923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01923]] Output Length Effect on DeepSeek-R1's Safety in Forced Thinking(https://arxiv.org/abs/2503.01923)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong reasoning capabilities, but their safety under adversarial conditions remains a challenge. This study examines the impact of output length on the robustness of DeepSeek-R1, particularly in Forced Thinking scenarios. We analyze responses across various adversarial prompts and find that while longer outputs can improve safety through self-correction, certain attack types exploit extended generations. Our findings suggest that output length should be dynamically controlled to balance reasoning effectiveness and security. We propose reinforcement learning-based policy adjustments and adaptive token length regulation to enhance LLM safety.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）表现出强大的推理能力，但是它们在对抗条件下的安全仍然是一个挑战。这项研究研究了输出长度对DeepSeek-R1稳健性的影响，尤其是在强迫思维方案中。我们分析了各种对抗性提示中的响应，发现较长的输出可以通过自我纠正来提高安全性，但某些攻击类型利用了延长的世代。我们的发现表明，应动态控制输出长度，以平衡推理有效性和安全性。我们建议基于学习的政策调整和适应性令牌长度调节，以提高LLM安全性。</li>
</ul>

<h3>Title: Unnatural Languages Are Not Bugs but Features for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Keyu Duan, Yiran Zhao, Zhili Feng, Jinjie Ni, Tianyu Pang, Qian Liu, Tianle Cai, Longxu Dou, Kenji Kawaguchi, Anirudh Goyal, J. Zico Kolter, Michael Qizhe Shieh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01926">https://arxiv.org/abs/2503.01926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01926">https://arxiv.org/pdf/2503.01926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01926]] Unnatural Languages Are Not Bugs but Features for LLMs(https://arxiv.org/abs/2503.01926)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving 49.71 win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis, we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words.</li>
<li><strong>摘要：</strong>已经观察到大型语言模型（LLM）处理非人类可读文本序列（例如越狱提示），通常被视为对齐LLM的错误。在这项工作中，我们提出了一项系统的调查，挑战了这种看法，表明不自然的语言 - 人类似乎无法理解的字符串，但要维持LLMS的语义含义 - 包含模型可用的潜在特征。值得注意的是，不自然的语言具有潜在特征，可以在推理过程中跨越不同的模型和任务。此外，在不自然版本的指令数据集上进行了微调的模型与接受自然语言培训的模型在PAR上执行了PAR，在各种基础模型中平均达到了长度控制的Alpacaeval 2.0的49.71获胜率。此外，通过全面的分析，我们通过过滤噪声并从过滤单词中推断上下文含义来证明LLMS来处理不自然的语言。</li>
</ul>

<h3>Title: AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification</h3>
<ul>
<li><strong>Authors: </strong>Xuan Zhang, Yongliang Shen, Zhe Zheng, Linjuan Wu, Wenqi Zhang, Yuchen Yan, Qiuying Peng, Jun Wang, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01940">https://arxiv.org/abs/2503.01940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01940">https://arxiv.org/pdf/2503.01940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01940]] AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification(https://arxiv.org/abs/2503.01940)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in tool learning. In real-world scenarios, user queries are often ambiguous and incomplete, requiring effective clarification. However, existing interactive clarification approaches face two critical limitations: reliance on manually constructed datasets and lack of error correction mechanisms during multi-turn clarification. We present AskToAct, which addresses these challenges by exploiting the structural mapping between queries and their tool invocation solutions. Our key insight is that tool parameters naturally represent explicit user intents. By systematically removing key parameters from queries while retaining them as ground truth, we enable automated construction of high-quality training data. We further enhance model robustness by fine-tuning on error-correction augmented data using selective masking mechanism, enabling dynamic error detection during clarification interactions. Comprehensive experiments demonstrate that AskToAct significantly outperforms existing approaches, achieving above 79% accuracy in recovering critical unspecified intents and enhancing clarification efficiency by an average of 48.34% while maintaining high accuracy in tool invocation. Our framework exhibits robust performance across varying complexity levels and successfully generalizes to entirely unseen APIs without additional training, achieving performance comparable to GPT-4 with substantially fewer computational resources.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在工具学习中表现出了显着的功能。在实际情况下，用户查询通常是模棱两可和不完整的，需要有效的澄清。但是，现有的交互式澄清方法面临两个关键局限性：依赖手动构造的数据集以及在多转移澄清过程中缺乏误差校正机制。我们提出asktoact，它通过利用查询及其工具调用解决方案之间的结构映射来解决这些挑战。我们的关键见解是工具参数自然代表明确的用户意图。通过系统地从查询中删除关键参数，同时将其保留为地面真理，我们可以自动构造高质量的培训数据。我们通过使用选择性掩蔽机制对错误校正增强数据进行微调来进一步增强模型鲁棒性，从而在澄清相互作用期间实现动态误差检测。全面的实验表明，Ask施用明显胜过现有方法，在恢复关键的未指定意图并平均提高澄清效率48.34％的同时，同时保持高精度的工具调用准确性。我们的框架在不同的复杂性水平上表现出良好的性能，并成功地概括为完全看不见的API，而无需额外的培训，从而达到了与GPT-4相当的计算资源的性能。</li>
</ul>

<h3>Title: Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts</h3>
<ul>
<li><strong>Authors: </strong>Akito Nakanishi, Yukie Sano, Geng Liu, Francesco Pierri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01947">https://arxiv.org/abs/2503.01947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01947">https://arxiv.org/pdf/2503.01947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01947]] Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts(https://arxiv.org/abs/2503.01947)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLMs) have attracted growing interest for their significant potential, though concerns have rapidly emerged regarding unsafe behaviors stemming from inherent stereotypes and this http URL research on stereotypes in LLMs has primarily relied on indirect evaluation setups, in which models are prompted to select between pairs of sentences associated with particular social groups. Recently, direct evaluation methods have emerged, examining open-ended model responses to overcome limitations of previous approaches, such as annotator this http URL existing studies have focused on English-centric LLMs, whereas research on non-English models--particularly Japanese--remains sparse, despite the growing development and adoption of these this http URL study examines the safety of Japanese LLMs when responding to stereotype-triggering prompts in direct this http URL constructed 3,612 prompts by combining 301 social group terms--categorized by age, gender, and other attributes--with 12 stereotype-inducing templates in this http URL were analyzed from three foundational models trained respectively on Japanese, English, and Chinese this http URL findings reveal that LLM-jp, a Japanese native model, exhibits the lowest refusal rate and is more likely to generate toxic and negative responses compared to other this http URL, prompt format significantly influence the output of all models, and the generated responses include exaggerated reactions toward specific social groups, varying across this http URL findings underscore the insufficient ethical safety mechanisms in Japanese LLMs and demonstrate that even high-accuracy models can produce biased outputs when processing Japanese-language this http URL advocate for improving safety mechanisms and bias mitigation strategies in Japanese LLMs, contributing to ongoing discussions on AI ethics beyond linguistic boundaries.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLMS）引起了人们对其巨大潜力的日益兴趣，尽管人们对源于固有的刻板印象的不安全行为的担忧迅速出现，而这项HTTP URL对LLMS中的刻板印象的研究主要依赖于间隔评估设置，以促使与特定社交之间的选择相关，以提示与特定的社交相关的组成部分。最近，出现了直接评估方法，研究了对克服先前方法的限制的开放式模型的响应，例如注释者，现有的研究集中在以英语为中心的LLM上，而对非英国模型的研究 - 尽管如此，对这些HTTP的安全性却不断提高，但对日本的越来越多的日语进行了逐渐探讨，但这些模型稀少了，这是众多的研究。直接在此HTTP URL中提示，通过结合301个社会群体术语（按年龄，性别和其他属性进行分类），该提示构建了3,612个提示，该术语与12个刻板印象诱导的HTTP URL中的12个基础模型分别对日语，英语和中国式凝固型ARM-a a a a a a http url分析了http url的刻板印象，该模型均分别培训。与其他HTTP URL相比，速率和更可能产生有毒和负面反应，及时格式会显着影响所有模型的产出，并且产生的响应包括对特定社会群体的夸大反应，在此HTTP URL发现中，在这种http url发现中有所不同。 URL倡导改善日本LLM的安全机制和缓解偏见策略，这有助于对语言界限以外的AI伦理的持续讨论。</li>
</ul>

<h3>Title: Adaptively evaluating models with task elicitation</h3>
<ul>
<li><strong>Authors: </strong>Davis Brown, Prithvi Balehannina, Helen Jin, Shreya Havaldar, Hamed Hassani, Eric Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01986">https://arxiv.org/abs/2503.01986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01986">https://arxiv.org/pdf/2503.01986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01986]] Adaptively evaluating models with task elicitation(https://arxiv.org/abs/2503.01986)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Manual curation of evaluation datasets is struggling to keep up with the rapidly expanding capabilities and deployment scenarios of language models. Towards scalable model profiling, we introduce and validate a framework for evaluating LLMs, called Adaptive Evaluations. Adaptive evaluations use scaffolded language models (evaluator agents) to search through a target model's behavior on a domain dataset and create difficult questions (tasks) that can discover and probe the model's failure modes. We find that frontier models lack consistency when adaptively probed with our framework on a diverse suite of datasets and tasks, including but not limited to legal reasoning, forecasting, and online harassment. Generated questions pass human validity checks and often transfer to other models with different capability profiles, demonstrating that adaptive evaluations can also be used to create difficult domain-specific datasets.</li>
<li><strong>摘要：</strong>评估数据集的手动策划正在努力跟上语言模型的快速扩展功能和部署方案。为了进行可扩展模型分析，我们介绍并验证一个评估LLM的框架，称为自适应评估。自适应评估使用脚手架语言模型（评估器代理）在域数据集上搜索目标模型的行为，并创建难题（任务），以发现和探测模型的故障模式。我们发现，当我们在各种数据集和任务上使用框架进行自适应探索时，Frontier模型缺乏一致性，包括但不限于法律推理，预测和在线骚扰。生成的问题通过了人体有效性检查，并经常转移到具有不同能力概况的其他模型中，这表明自适应评估也可以用于创建困难的域特异性数据集。</li>
</ul>

<h3>Title: One ruler to measure them all: Benchmarking multilingual long-context language models</h3>
<ul>
<li><strong>Authors: </strong>Yekyung Kim, Jenna Russell, Marzena Karpinska, Mohit Iyyer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.01996">https://arxiv.org/abs/2503.01996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.01996">https://arxiv.org/pdf/2503.01996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.01996]] One ruler to measure them all: Benchmarking multilingual long-context language models(https://arxiv.org/abs/2503.01996)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present ONERULER, a multilingual benchmark designed to evaluate long-context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test both retrieval and aggregation, including new variations of the "needle-in-a-haystack" task that allow for the possibility of a nonexistent needle. We create ONERULER through a two-step process, first writing English instructions for each task and then collaborating with native speakers to translate them into 25 additional languages. Experiments with both open-weight and closed LLMs reveal a widening performance gap between low- and high-resource languages as context length increases from 8K to 128K tokens. Surprisingly, English is not the top-performing language on long-context tasks (ranked 6th out of 26), with Polish emerging as the top language. Our experiments also show that many LLMs (particularly OpenAI's o3-mini-high) incorrectly predict the absence of an answer, even in high-resource languages. Finally, in cross-lingual scenarios where instructions and context appear in different languages, performance can fluctuate by up to 20% depending on the instruction language. We hope the release of ONERULER will facilitate future research into improving multilingual and cross-lingual long-context training pipelines.</li>
<li><strong>摘要：</strong>我们提出了Oneruler，这是一种多种语言基准，旨在评估26种语言的长篇小写语言模型。 Oneruler通过包括七个测试检索和聚集的合成任务，包括唯一的统治者基准（Hsieh等，2024），包括“针中的针中的针刺”任务的新变化，这些任务允许允许无针头的针刺。我们通过两步过程创建Oneruler，首先为每个任务编写英语说明，然后与母语人士合作，将其翻译成25种其他语言。进行开放权重和闭合LLM的实验表明，随着上下文长度从8k增加到128K令牌，低资源和高资源语言之间的性能差距扩大。令人惊讶的是，英语不是长篇文章任务的表现最佳的语言（在26个中排名第六），而波兰语则是顶级语言。我们的实验还表明，即使在高资源语言中，许多LLM（尤其是Openai的O3-Mini-High）也错误地预测了没有答案的情况。最后，在跨语言的情况下，指令和上下文以不同的语言出现，根据说明语言，性能最多可以波动20％。我们希望Oneruler的发布能够促进未来的研究，以改善多语言和跨语义的长篇小写培训管道。</li>
</ul>

<h3>Title: HoT: Highlighted Chain of Thought for Referencing Supportive Facts from Inputs</h3>
<ul>
<li><strong>Authors: </strong>Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02003">https://arxiv.org/abs/2503.02003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02003">https://arxiv.org/pdf/2503.02003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02003]] HoT: Highlighted Chain of Thought for Referencing Supportive Facts from Inputs(https://arxiv.org/abs/2503.02003)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的致命脚跟是他们幻觉非事实陈述的趋势。事实和非事实陈述的回应构成了人类验证并准确基于决策的挑战。为了解决这个问题，我们提出了突出显示的链条链（HOT），这是一种提示LLMS使用XML标签生成响应的技术，这些标签将事实扎根于查询中提供的事实。也就是说，给出一个输入问题，LLMS将首先重新格式化问题，以添加XML标签突出关键事实，然后在输入中引用的事实中生成一个响应。有趣的是，在几个设置中，热门的表现优于促进思想链（COT），从算术，阅读理解到逻辑推理的17个任务上，均超过了思想链（COT）。当要求人类验证LLM响应时，突出显示有助于时间限制的参与者更准确，更有效地识别LLM何时正确。但是，令人惊讶的是，当LLM错误时，HOTS倾向于使用户相信答案是正确的。</li>
</ul>

<h3>Title: Mind the (Belief) Gap: Group Identity in the World of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Angana Borah, Marwa Houalla, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02016">https://arxiv.org/abs/2503.02016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02016">https://arxiv.org/pdf/2503.02016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02016]] Mind the (Belief) Gap: Group Identity in the World of LLMs(https://arxiv.org/abs/2503.02016)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Social biases and belief-driven behaviors can significantly impact Large Language Models (LLMs) decisions on several tasks. As LLMs are increasingly used in multi-agent systems for societal simulations, their ability to model fundamental group psychological characteristics remains critical yet under-explored. In this study, we present a multi-agent framework that simulates belief congruence, a classical group psychology theory that plays a crucial role in shaping societal interactions and preferences. Our findings reveal that LLMs exhibit amplified belief congruence compared to humans, across diverse contexts. We further investigate the implications of this behavior on two downstream tasks: (1) misinformation dissemination and (2) LLM learning, finding that belief congruence in LLMs increases misinformation dissemination and impedes learning. To mitigate these negative impacts, we propose strategies inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global citizenship framework. Our results show that the best strategies reduce misinformation dissemination by up to 37% and enhance learning by 11%. Bridging social psychology and AI, our work provides insights to navigate real-world interactions using LLMs while addressing belief-driven biases.</li>
<li><strong>摘要：</strong>社会偏见和信仰驱动的行为可以显着影响大型语言模型（LLMS）的决定。由于LLM越来越多地用于社会模拟的多代理系统中，因此它们对基本群体心理特征进行建模的能力仍然是至关重要的，但仍未探索。在这项研究中，我们提出了一个多代理框架，该框架模拟了信仰一致性，这是一种古典群体心理学理论，在塑造社会互动和偏好中起着至关重要的作用。我们的发现表明，与人类相比，LLM在各种环境中表现出放大的信念一致性。我们进一步研究了这种行为对两个下游任务的含义：（1）误导性传播和（2）LLM学习，发现LLMS中的信念一致性增加了错误信息传播并阻碍了学习。为了减轻这些负面影响，我们提出了受以下方式启发的策略：（1）联系假设，（2）准确性nudges和（3）全球公民身份框架。我们的结果表明，最佳策略最多将错误信息传播减少了37％，并将学习提高了11％。弥合社会心理学和人工智能，我们的工作提供了见解，可以在解决信念驱动的偏见的同时，使用LLM在现实世界中进行互动。</li>
</ul>

<h3>Title: Comparative Analysis of OpenAI GPT-4o and DeepSeek R1 for Scientific Text Categorization Using Prompt Engineering</h3>
<ul>
<li><strong>Authors: </strong>Aniruddha Maiti, Samuel Adewumi, Temesgen Alemayehu Tikure, Zichun Wang, Niladri Sengupta, Anastasiia Sukhanova, Ananya Jana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02032">https://arxiv.org/abs/2503.02032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02032">https://arxiv.org/pdf/2503.02032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02032]] Comparative Analysis of OpenAI GPT-4o and DeepSeek R1 for Scientific Text Categorization Using Prompt Engineering(https://arxiv.org/abs/2503.02032)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>This study examines how large language models categorize sentences from scientific papers using prompt engineering. We use two advanced web-based models, GPT-4o (by OpenAI) and DeepSeek R1, to classify sentences into predefined relationship categories. DeepSeek R1 has been tested on benchmark datasets in its technical report. However, its performance in scientific text categorization remains unexplored. To address this gap, we introduce a new evaluation method designed specifically for this task. We also compile a dataset of cleaned scientific papers from diverse domains. This dataset provides a platform for comparing the two models. Using this dataset, we analyze their effectiveness and consistency in categorization.</li>
<li><strong>摘要：</strong>这项研究研究了大型语言模型是如何使用及时工程从科学论文中分类句子的。我们使用两个基于Web的高级模型GPT-4O（由OpenAI）和DeepSeek R1，将句子分类为预定义的关系类别。 DeepSeek R1在其技术报告中已在基准数据集上进行了测试。但是，它在科学文本分类中的表现仍未开发。为了解决此差距，我们介绍了一种专门为此任务设计的新评估方法。我们还编译了来自不同领域的清洁科学论文数据集。该数据集提供了一个用于比较这两个模型的平台。使用此数据集，我们分析了它们在分类中的有效性和一致性。</li>
</ul>

<h3>Title: Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions</h3>
<ul>
<li><strong>Authors: </strong>Angana Borah, Rada Mihalcea, Verónica Pérez-Rosas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02038">https://arxiv.org/abs/2503.02038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02038">https://arxiv.org/pdf/2503.02038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02038]] Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions(https://arxiv.org/abs/2503.02038)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Existing challenges in misinformation exposure and susceptibility vary across demographic groups, as some populations are more vulnerable to misinformation than others. Large language models (LLMs) introduce new dimensions to these challenges through their ability to generate persuasive content at scale and reinforcing existing biases. This study investigates the bidirectional persuasion dynamics between LLMs and humans when exposed to misinformative content. We analyze human-to-LLM influence using human-stance datasets and assess LLM-to-human influence by generating LLM-based persuasive arguments. Additionally, we use a multi-agent LLM framework to analyze the spread of misinformation under persuasion among demographic-oriented LLM agents. Our findings show that demographic factors influence susceptibility to misinformation in LLMs, closely reflecting the demographic-based patterns seen in human susceptibility. We also find that, similar to human demographic groups, multi-agent LLMs exhibit echo chamber behavior. This research explores the interplay between humans and LLMs, highlighting demographic differences in the context of misinformation and offering insights for future interventions.</li>
<li><strong>摘要：</strong>在人群群体中，现有的错误信息暴露和敏感性各不相同，因为某些人群比其他人群更容易受到错误信息的影响。大型语言模型（LLMS）通过其规模生成有说服力的内容并加强现有偏见的能力来引入新的方面。这项研究调查了LLM和人类之间的双向说服动力学，暴露于错误的信息含量时。我们使用人类静态数据集分析了人与LLM的影响，并通过产生基于LLM的有说服力的论点来评估LLM至人类的影响。此外，我们使用多代理LLM框架来分析说服力面向人口统计学的LLM药物之间错误信息的传播。我们的发现表明，人口统计学因素会影响LLM中错误信息的敏感性，密切反映了人类易感性中基于人口的模式。我们还发现，与人类人群类似，多代理LLM表现出回声室行为。这项研究探讨了人类和LLM之间的相互作用，突出了人口统计学差异，在错误信息的背景下，为将来的干预提供了见解。</li>
</ul>

<h3>Title: Hebbian learning the local structure of language</h3>
<ul>
<li><strong>Authors: </strong>P. Myles Eugenio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02057">https://arxiv.org/abs/2503.02057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02057">https://arxiv.org/pdf/2503.02057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02057]] Hebbian learning the local structure of language(https://arxiv.org/abs/2503.02057)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Learning in the brain is local and unsupervised (Hebbian). We derive the foundations of an effective human language model inspired by these microscopic constraints. It has two parts: (1) a hierarchy of neurons which learns to tokenize words from text (whichiswhatyoudowhenyoureadthis); and (2) additional neurons which bind the learned symanticless patterns of the tokenizer into a symanticful token (an embedding). The model permits continuous parallel learning without forgetting; and is a powerful tokenizer which performs renormalization group. This allows it to exploit redundancy, such that it generates tokens which are always decomposable into a basis set (e.g an alphabet), and can mix features learned from multiple languages. We find that the structure of this model allows it to learn a natural language morphology WITHOUT data. The language data generated by this model predicts the correct distribution of word-forming patterns observed in real languages, and further demonstrates why microscopically human speech is broken up into words. This model provides the basis for understanding the microscopic origins of language and human creativity.</li>
<li><strong>摘要：</strong>在大脑中学习是本地的，无监督的（Hebbian）。我们得出了受这些微观约束启发的有效人类语言模型的基础。它有两个部分：（1）神经元的层次结构，该层次结构从文本（WhatyOudoWhenyoureadthis）中学会化为单词； （2）将引人入胜的代币模式结合到同意的令牌（嵌入）中的其他神经元。该模型允许在不忘记的情况下进行连续的并行学习。并且是一个强大的令牌仪，可执行重归其化组。这使其可以利用冗余，从而生成代币，这些令牌总是可以分解为基集（例如字母），并且可以混合从多种语言中学到的功能。我们发现该模型的结构使其可以在没有数据的情况下学习自然语言形态。该模型生成的语言数据预测了以真实语言观察到的单词形成模式的正确分布，并进一步证明了为什么显微镜上的人类语音被分解为单词。该模型为理解语言和人类创造力的微观起源提供了基础。</li>
</ul>

<h3>Title: Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Jacobi, Gal Niv</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02078">https://arxiv.org/abs/2503.02078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02078">https://arxiv.org/pdf/2503.02078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02078]] Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation(https://arxiv.org/abs/2503.02078)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Understanding and interpreting the internal representations of large language models (LLMs) remains an open challenge. Patchscopes introduced a method for probing internal activations by patching them into new prompts, prompting models to self-explain their hidden representations. We introduce Superscopes, a technique that systematically amplifies superposed features in MLP outputs (multilayer perceptron) and hidden states before patching them into new contexts. Inspired by the "features as directions" perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training. This approach provides new insights into how LLMs build context and represent complex concepts, further advancing mechanistic interpretability.</li>
<li><strong>摘要：</strong>理解和解释大语言模型（LLM）的内部表示仍然是一个开放的挑战。 PatchScopes引入了一种通过将其修补为新提示来探测内部激活的方法，并提示模型自我解释其隐藏的表示形式。我们介绍了SuperScopes，这是一种系统地放大MLP输出（多层perceptron）和隐藏状态的技术，然后将其修补成新的上下文。受到扩散模型的“作为方向”的观点和无分类器指导（CFG）方法的启发，SuperScopes放大了弱但有意义的特征，从而使以前方法无法解释的内部表示形式解释了所有人，而无需进行其他培训。这种方法为LLM如何构建背景和代表复杂的概念，进一步提高机械解释性提供了新的见解。</li>
</ul>

<h3>Title: Linear Representations of Political Perspective Emerge in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junsol Kim, James Evans, Aaron Schein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02080">https://arxiv.org/abs/2503.02080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02080">https://arxiv.org/pdf/2503.02080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02080]] Linear Representations of Political Perspective Emerge in Large Language Models(https://arxiv.org/abs/2503.02080)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives in American politics. We show that LLMs possess linear representations of political perspectives within activation space, wherein more similar perspectives are represented closer together. To do so, we probe the attention heads across the layers of three open transformer-based LLMs (\texttt{Llama-2-7b-chat}, \texttt{Mistral-7b-instruct}, \texttt{Vicuna-7b}). We first prompt models to generate text from the perspectives of different U.S.~lawmakers. We then identify sets of attention heads whose activations linearly predict those lawmakers' DW-NOMINATE scores, a widely-used and validated measure of political ideology. We find that highly predictive heads are primarily located in the middle layers, often speculated to encode high-level concepts and tasks. Using probes only trained to predict lawmakers' ideology, we then show that the same probes can predict measures of news outlets' slant from the activations of models prompted to simulate text from those news outlets. These linear probes allow us to visualize, interpret, and monitor ideological stances implicitly adopted by an LLM as it generates open-ended responses. Finally, we demonstrate that by applying linear interventions to these attention heads, we can steer the model outputs toward a more liberal or conservative stance. Overall, our research suggests that LLMs possess a high-level linear representation of American political ideology and that by leveraging recent advances in mechanistic interpretability, we can identify, monitor, and steer the subjective perspective underlying generated text.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）已经证明了能够生成文本的能力，这些文本实际上反映了一系列不同的主观人类观点。本文研究了LLM在美国政治中如何反映出更加自由主义者和更保守的观点的方式。我们表明，LLM在激活空间内具有政治观点的线性表示，其中更相似的观点被更近在咫尺。为此，我们探测了三个基于开放变压器的LLM（\ texttt {llama-2-7b-chat}}的注意力头，\ texttt {mistral-7b-instruct}，\ texttt {vicuna-7b}）。我们首先促使模型从美国不同的立法者的角度生成文本。然后，我们确定了一组注意力头，其激活线性地预测了这些立法者的DW-Nominate分数，这是一种广泛使用的和有验证的政治意识形态衡量标准。我们发现高度预测的头主要位于中层，通常被推测用于编码高级概念和任务。然后，使用培训来预测立法者的意识形态，我们表明，相同的探针可以预测新闻媒体的倾向的度量，这些模型的激活导致促使这些新闻媒体的文本模拟文本。这些线性探针使我们能够在LLM产生开放式响应时可视化，解释和监视意识形态立场。最后，我们证明，通过将线性干预应用于这些注意力头，我们可以将模型输出引导到更加自由或保守的立场。总体而言，我们的研究表明，LLM具有对美国政治意识形态的高级线性表示，并且通过利用最新的机械性解释性进步，我们可以识别，监控和指导基本产生的文本的主观观点。</li>
</ul>

<h3>Title: Provable Benefits of Task-Specific Prompts for In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Chang, Yingcong Li, Muti Kara, Samet Oymak, Amit K. Roy-Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02102">https://arxiv.org/abs/2503.02102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02102">https://arxiv.org/pdf/2503.02102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02102]] Provable Benefits of Task-Specific Prompts for In-context Learning(https://arxiv.org/abs/2503.02102)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The in-context learning capabilities of modern language models have motivated a deeper mathematical understanding of sequence models. A line of recent work has shown that linear attention models can emulate projected gradient descent iterations to implicitly learn the task vector from the data provided in the context window. In this work, we consider a novel setting where the global task distribution can be partitioned into a union of conditional task distributions. We then examine the use of task-specific prompts and prediction heads for learning the prior information associated with the conditional task distribution using a one-layer attention model. Our results on loss landscape show that task-specific prompts facilitate a covariance-mean decoupling where prompt-tuning explains the conditional mean of the distribution whereas the variance is learned/explained through in-context learning. Incorporating task-specific head further aids this process by entirely decoupling estimation of mean and variance components. This covariance-mean perspective similarly explains how jointly training prompt and attention weights can provably help over fine-tuning after pretraining.</li>
<li><strong>摘要：</strong>现代语言模型的内在学习能力激发了对序列模型的更深入的数学理解。最近的一系列工作表明，线性注意模型可以模拟投射的梯度下降迭代，以隐式从上下文窗口中提供的数据中隐含地学习任务向量。在这项工作中，我们考虑了一个新颖的环境，可以将全球任务分布分配到有条件的任务分布的结合中。然后，我们检查了特定于任务的提示和预测头的使用，以学习使用单层注意模型与条件任务分布相关的先前信息。我们在损失景观方面的结果表明，特定于任务的提示促进了协方差的脱钩，其中迅速调整解释了分布的条件平均值，而通过封闭式学习来学习/解释差异。合并特定于任务的头部通过完全取消对均值和方差成分的估计来进一步帮助这一过程。这种协方差的观点类似地说明了在训练后，共同训练的提示和注意力重量如何有助于对微调进行微调。</li>
</ul>

<h3>Title: Superficial Self-Improved Reasoners Benefit from Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Xiangchi Yuan, Chunhui Zhang, Zheyuan Liu, Dachuan Shi, Soroush Vosoughi, Wenke Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02103">https://arxiv.org/abs/2503.02103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02103">https://arxiv.org/pdf/2503.02103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02103]] Superficial Self-Improved Reasoners Benefit from Model Merging(https://arxiv.org/abs/2503.02103)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As scaled language models (LMs) approach human-level reasoning capabilities, self-improvement emerges as a solution to synthesizing high-quality data corpus. While previous research has identified model collapse as a risk in self-improvement, where model outputs become increasingly deterministic, we discover a more fundamental challenge: the superficial self-improved reasoners phenomenon. In particular, our analysis reveals that even when LMs show improved in-domain (ID) reasoning accuracy, they actually compromise their generalized reasoning capabilities on out-of-domain (OOD) tasks due to memorization rather than genuine. Through a systematic investigation of LM architecture, we discover that during self-improvement, LM weight updates are concentrated in less reasoning-critical layers, leading to superficial learning. To address this, we propose Iterative Model Merging (IMM), a method that strategically combines weights from original and self-improved models to preserve generalization while incorporating genuine reasoning improvements. Our approach effectively mitigates both LM collapse and superficial learning, moving towards more stable self-improving systems.</li>
<li><strong>摘要：</strong>随着缩放语言模型（LMS）接近人类水平的推理能力，自我完善是作为综合高质量数据语料库的解决方案而出现的。尽管以前的研究已经确定模型崩溃是自我完善的风险，而模型输出变得越来越确定性，但我们发现了一个更根本的挑战：肤浅的自我改善的推理者现象。特别是，我们的分析表明，即使LMS显示出改善的（ID）推理精度，它们实际上会损害由于记忆而不是真实的，而不是真实的，它们在室外（OOD）任务上的广义推理能力也损害了其广义推理能力。通过对LM体系结构的系统研究，我们发现在自我完善期间，LM重量更新集中在较少的推理关键层中，从而导致肤浅的学习。为了解决这个问题，我们提出了迭代模型合并（IMM），这种方法从策略性地结合了原始和自我改良模型的权重，以保留概括，同时结合真正的推理改进。我们的方法有效地减轻了LM崩溃和肤浅的学习，朝着更稳定的自我改善系统迈进。</li>
</ul>

<h3>Title: Measuring Intrinsic Dimension of Token Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Takuya Kataiwa, Cho Hakaze, Tetsushi Ohki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02142">https://arxiv.org/abs/2503.02142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02142">https://arxiv.org/pdf/2503.02142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02142]] Measuring Intrinsic Dimension of Token Embeddings(https://arxiv.org/abs/2503.02142)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this study, we measure the Intrinsic Dimension (ID) of token embedding to estimate the intrinsic dimensions of the manifolds spanned by the representations, so as to evaluate their redundancy quantitatively compared to their extrinsic dimensionality. In detail, (1) we estimate the ID of token embeddings in small-scale language models and also modern large language models, finding that the embedding spaces often reside on lower-dimensional manifolds compared to their extrinsic dimensionality; (2) we measure the ID across various model sizes and observe an increase in redundancy rates as the model scale grows; (3) we measure the dynamics of IDs during the training process, and find a rapid ID drop in the early stages of training. Moreover, (4) when LoRA is applied to the embedding layers, we observe a sudden drop in perplexity around the estimated IDs, suggesting that the ID can serve as a useful guideline for LoRA application.</li>
<li><strong>摘要：</strong>在这项研究中，我们测量了令牌嵌入的固有维度（ID），以估计表示形式跨越的流形的固有维度，以便与其外在维度相比评估其冗余性。详细说明，（1）我们估算了小型语言模型和现代大语模型中令牌嵌入的ID，发现嵌入空间通常位于较低维的流形上，与它们的外在维度相比； （2）我们测量各种模型大小的ID，并观察到随着模型量表的增长，冗余率的增加； （3）我们在训练过程中测量ID的动态，并在培训的早期阶段找到快速ID下降。此外，（4）将洛拉应用于嵌入层时，我们观察到估计的ID周围的困惑突然下降，这表明该ID可以作为Lora应用的有用指南。</li>
</ul>

<h3>Title: Adversarial Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Renato Lui Geh, Zilei Shao, Guy Van den Broeck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02174">https://arxiv.org/abs/2503.02174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02174">https://arxiv.org/pdf/2503.02174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02174]] Adversarial Tokenization(https://arxiv.org/abs/2503.02174)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference. For example, the standard Llama3 tokenization of penguin is [p,enguin], yet [peng,uin] is another perfectly valid alternative. In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety. Put succinctly, we answer the following question: can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions? We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request. We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models.</li>
<li><strong>摘要：</strong>当前的LLM管道仅解释了一个给定的字符串的一个可能的令牌化，在训练和推理过程中毫无natized忽略了许多替代引物。例如，企鹅的标准Llama3令牌是[P，Enguin]，但是[Peng，UIN]是另一个完全有效的选择。在本文中，我们表明，尽管LLMS仅接受了一个令牌化培训，但他们仍然保留对其他引物的语义理解，从而提出了有关其在LLM安全性的影响的问题。简而言之，我们回答以下问题：我们能否在对抗性地将明显的恶意绳子示出来逃避安全性和一致性限制吗？我们表明，对抗性令牌不仅是有效但先前被忽视的攻击轴，而且在不改变有害要求的文本的情况下，它与现有的最新对抗方法具有竞争力。我们从经验上验证了在三个最先进的LLM和对抗数据集中的这种利用，从而揭示了子词模型中以前未知的漏洞。</li>
</ul>

<h3>Title: ATLaS: Agent Tuning via Learning Critical Steps</h3>
<ul>
<li><strong>Authors: </strong>Zhixun Chen, Ming Li, Yuxuan Huang, Yali Du, Meng Fang, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02197">https://arxiv.org/abs/2503.02197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02197">https://arxiv.org/pdf/2503.02197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02197]] ATLaS: Agent Tuning via Learning Critical Steps(https://arxiv.org/abs/2503.02197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）代理在多域任务中表现出显着的概括能力。现有的代理调整方法通常在整个专家轨迹上采用监督的固定。但是，全面轨迹的行为限制会引入专家偏见，并削弱专家数据所涵盖的状态。此外，关键步骤，例如计划，中间子任务的复杂推理和战略决策对于代理任务的成功至关重要，因此学习这些步骤是改善LLM代理的关键。为了进行更有效和有效的代理调整，我们建议的地图集仅在成本降低的这些步骤上识别专家轨迹和Finetunes llms的关键步骤。通过将培训的重点转移到一些关键步骤中，我们的方法减轻了整个轨迹过度拟合并促进跨不同环境和任务的概括的风险。在广泛的实验中，仅在Atlas选择的30％关键步骤上进行的LLM填充均优于所有步骤和最近的开源LLM代理的LLM易位。随着通才代理人与各种环境互动，Atlas维持并提高了LLM的基本LLM技能。</li>
</ul>

<h3>Title: Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02233">https://arxiv.org/abs/2503.02233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02233">https://arxiv.org/pdf/2503.02233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02233]] Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling(https://arxiv.org/abs/2503.02233)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) frequently hallucinate due to misaligned self-awareness, generating erroneous outputs when addressing queries beyond their knowledge boundaries. While existing approaches mitigate hallucinations via uncertainty estimation or query rejection, they suffer from computational inefficiency or sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate use of high-confidence outputs. For uncertain predictions, a slow refinement model conducts targeted reasoning to improve accuracy. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. Our work establishes a scalable paradigm for advancing LLM reliability and balancing accuracy and practical utility in error-sensitive applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）经常由于自我意识未对准而幻觉，在解决其知识边界以外的查询时产生错误的输出。尽管现有方法通过不确定性估计或拒绝来减轻幻觉，但它们会遭受计算效率低下或牺牲的帮助。为了解决这些问题，我们提出了明确的知识边界建模（EKBM）框架，将快速和缓慢的推理系统集成以协调可靠性和可用性。该框架首先采用快速思考的模型来生成置信标记的响应，从而立即使用高信心输出。对于不确定的预测，缓慢的改进模型会进行有针对性的推理以提高准确性。为了使模型行为与我们提出的对象保持一致，我们提出了一条混合训练管道，增强自我意识而不会降低任务绩效。对话状态跟踪任务的评估表明，EKBM比基于不确定性的基准实现了卓越的模型可靠性。进一步的分析表明，改进可以大大提高准确性，同时保持低计算开销。我们的工作建立了一个可扩展的范式，以提高LLM的可靠性，并在误差敏感应用中平衡准确性和实用性。</li>
</ul>

<h3>Title: Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient and Feasible Multitasking with Time Constraints Between Actions</h3>
<ul>
<li><strong>Authors: </strong>Zirui Wu, Xiao Liu, Jiayi Li, Lingpeng Kong, Yansong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02238">https://arxiv.org/abs/2503.02238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02238">https://arxiv.org/pdf/2503.02238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02238]] Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient and Feasible Multitasking with Time Constraints Between Actions(https://arxiv.org/abs/2503.02238)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>While Large Language Model-based agents have demonstrated substantial progress in task completion, existing evaluation benchmarks tend to overemphasize single-task performance, with insufficient attention given to the crucial aspects of multitask planning and execution efficiency required in real-world scenarios. To bridge this gap, we present Recipe2Plan, a novel benchmark framework based on real-world cooking scenarios. Unlike conventional benchmarks, Recipe2Plan challenges agents to optimize cooking time through parallel task execution while respecting temporal constraints i.e. specific actions need to be performed within a particular time intervals following the preceding steps. Overly aggressive local parallelization may disrupt this constraint, potentially compromising the entire cooking process. This strict time constraint between actions raises a unique challenge for agents to balance between maximizing concurrent operations and adhering to critical timing constraints. Extensive experiments with state-of-the-art models reveal challenges in maintaining this balance between efficiency and feasibility. The results highlight the need for improved temporal awareness and global multitasking capabilities in large language models. We open-source our benchmark and code at this https URL.</li>
<li><strong>摘要：</strong>尽管基于语言模型的大型代理在任务完成方面已取得了很大的进步，但现有的评估基准倾向于过分强调单任务的性能，而在现实世界情景中，对多任务计划和执行效率的关键方面的关注不足。为了弥合这一差距，我们提出了Copipe2plan，这是一个基于现实世界烹饪方案的新型基准框架。与传统的基准不同，配方2plan挑战代理通过并行任务执行来优化烹饪时间，同时尊重时间约束，即需要在前面的步骤之后的特定时间间隔内执行特定的操作。过于激进的局部并行化可能会破坏这种约束，并可能损害整个烹饪过程。动作之间的严格时间限制为代理人在最大化并发操作和遵守关键时序约束之间平衡的独特挑战。对最先进模型的广泛实验揭示了在效率和可行性之间保持这种平衡的挑战。结果强调了在大型语言模型中需要提高时间意识和全球多任务功能的必要性。我们在此HTTPS URL上开源的基准和代码。</li>
</ul>

<h3>Title: OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Li, Shang Wu, Xiaokang Zhang, Xinmei Huang, Jing Zhang, Fuxin Jiang, Shuai Wang, Tieying Zhang, Jianjun Chen, Rui Shi, Hong Chen, Cuiping Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02240">https://arxiv.org/abs/2503.02240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02240">https://arxiv.org/pdf/2503.02240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02240]] OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale(https://arxiv.org/abs/2503.02240)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Text-to-SQL, the task of translating natural language questions into SQL queries, plays a crucial role in enabling non-experts to interact with databases. While recent advancements in large language models (LLMs) have significantly enhanced text-to-SQL performance, existing approaches face notable limitations in real-world text-to-SQL applications. Prompting-based methods often depend on closed-source LLMs, which are expensive, raise privacy concerns, and lack customization. Fine-tuning-based methods, on the other hand, suffer from poor generalizability due to the limited coverage of publicly available training data. To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution. Leveraging SynSQL-2.5M, we develop OmniSQL, a powerful open-source text-to-SQL model available in three sizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate that OmniSQL achieves state-of-the-art performance, matching or surpassing leading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3, despite its smaller size. We release all code, datasets, and models to support further research.</li>
<li><strong>摘要：</strong>将自然语言问题转化为SQL查询的文本到SQL在使非专家与数据库互动中起着至关重要的作用。尽管大型语言模型（LLMS）的最新进展具有显着提高的文本到SQL性能，但现有方法在现实世界中文本到SQL应用程序中面临着明显的限制。基于促进的方法通常取决于封闭源的LLM，这些LLM昂贵，提高隐私问题并缺乏自定义。另一方面，由于公共可用培训数据的覆盖率有限，基于微调的方法的普遍性差。为了克服这些挑战，我们提出了一个新颖且可扩展的文本到SQL数据合成框架，以自动合成大型，高质量和不同的数据集，而无需大量的人类干预。使用此框架，我们引入了Synsql-2.5m，这是第一个百万尺度的文本到SQL数据集，其中包含250万个跨越16,000个合成数据库的样本。每个样本包括一个数据库，SQL查询，自然语言问题和思想链（COT）解决方案。利用Synsql-2.5m，我们开发了OmnisQL，这是一种功能强大的开源文本到SQL模型，分为三种尺寸：7b，14b和32b。对九个数据集进行了广泛的评估表明，OmnisQL达到了最先进的性能，匹配或超过领先的封闭源和开源LLM，包括GPT-4O和DeepSeek-V3，尽管尺寸较小。我们发布所有代码，数据集和模型，以支持进一步的研究。</li>
</ul>

<h3>Title: PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xueliang Zhao, Wei Wu, Jian Guan, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02324">https://arxiv.org/abs/2503.02324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02324">https://arxiv.org/pdf/2503.02324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02324]] PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models(https://arxiv.org/abs/2503.02324)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The ability of large language models to solve complex mathematical problems has progressed significantly, particularly for tasks requiring advanced reasoning. However, the scarcity of sufficiently challenging problems, particularly at the Olympiad level, hinders further advancements. In this work, we introduce PromptCoT, a novel approach for automatically generating high-quality Olympiad-level math problems. The proposed method synthesizes complex problems based on mathematical concepts and the rationale behind problem construction, emulating the thought processes of experienced problem designers. We provide a theoretical analysis demonstrating that an optimal rationale should maximize both the likelihood of rationale generation given the associated concepts and the likelihood of problem generation conditioned on both the rationale and the concepts. Our method is evaluated on standard benchmarks including GSM8K, MATH-500, and AIME2024, where it consistently outperforms existing problem generation methods. Furthermore, we demonstrate that PromptCoT exhibits superior data scalability, consistently maintaining high performance as the dataset size increases, outperforming the baselines. The implementation is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型解决复杂数学问题的能力已经显着发展，尤其是对于需要高级推理的任务。但是，缺乏足够挑战的问题，尤其是在奥林匹亚级别上，阻碍了进一步的进步。在这项工作中，我们介绍了RespentCot，这是一种新型的方法，用于自动产生高质量的奥林匹克级数学问题。提出的方法基于数​​学概念和问题构建背后的理由综合了复杂的问题，从而模拟了经验丰富的问题​​设计师的思维过程。我们提供了理论分析，表明最佳理由应最大程度地提高鉴于相关概念的可能性以及问题产生的可能性在基本原理和概念方面的可能性。我们的方法对包括GSM8K，Math-500和AIME2024在内的标准基准进行了评估，在该基准中，它始终优于现有的问题生成方法。此外，我们证明了下午的数据可扩展性，随着数据集尺寸的增加而始终保持高性能，表现优于基线。该实现可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: Limited Effectiveness of LLM-based Data Augmentation for COVID-19 Misinformation Stance Detection</h3>
<ul>
<li><strong>Authors: </strong>Eun Cheol Choi, Ashwin Balasubramanian, Jinhu Qi, Emilio Ferrara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02328">https://arxiv.org/abs/2503.02328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02328">https://arxiv.org/pdf/2503.02328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02328]] Limited Effectiveness of LLM-based Data Augmentation for COVID-19 Misinformation Stance Detection(https://arxiv.org/abs/2503.02328)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Misinformation surrounding emerging outbreaks poses a serious societal threat, making robust countermeasures essential. One promising approach is stance detection (SD), which identifies whether social media posts support or oppose misleading claims. In this work, we finetune classifiers on COVID-19 misinformation SD datasets consisting of claims and corresponding tweets. Specifically, we test controllable misinformation generation (CMG) using large language models (LLMs) as a method for data augmentation. While CMG demonstrates the potential for expanding training datasets, our experiments reveal that performance gains over traditional augmentation methods are often minimal and inconsistent, primarily due to built-in safeguards within LLMs. We release our code and datasets to facilitate further research on misinformation detection and generation.</li>
<li><strong>摘要：</strong>围绕新兴爆发的错误信息构成了严重的社会威胁，这使得对策至关重要。一种有前途的方法是立场检测（SD），它确定了社交媒体帖子是否支持还是反对误导性主张。在这项工作中，我们在COVID-19错误信息信息中列出了分类器SD数据集，该数据集由索赔和相应的推文组成。具体而言，我们使用大语言模型（LLMS）作为数据增强方法测试可控的错误信息生成（CMG）。尽管CMG证明了扩大培训数据集的潜力，但我们的实验表明，在传统增强方法上的性能增长通常是最小且不一致的，这主要是由于LLMS内的内置保障措施。我们发布代码和数据集，以促进有关错误信息检测和生成的进一步研究。</li>
</ul>

<h3>Title: DeLTa: A Decoding Strategy based on Logit Trajectory Prediction Improves Factuality and Reasoning Ability</h3>
<ul>
<li><strong>Authors: </strong>Yunzhen He, Yusuke Takase, Yoichi Ishibashi, Hidetoshi Shimodaira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02343">https://arxiv.org/abs/2503.02343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02343">https://arxiv.org/pdf/2503.02343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02343]] DeLTa: A Decoding Strategy based on Logit Trajectory Prediction Improves Factuality and Reasoning Ability(https://arxiv.org/abs/2503.02343)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly being used in real-world applications. However, concerns about the reliability of the content they generate persist, as it frequently deviates from factual correctness or exhibits deficiencies in logical reasoning. This paper proposes a novel decoding strategy aimed at enhancing both factual accuracy and inferential reasoning without requiring any modifications to the architecture or pre-trained parameters of LLMs. Our approach adjusts next-token probabilities by analyzing the trajectory of logits from lower to higher layers in Transformers and applying linear regression. We find that this Decoding by Logit Trajectory-based approach (DeLTa) effectively reinforces factuality and reasoning while mitigating incorrect generation. Experiments on TruthfulQA demonstrate that DeLTa attains up to a 4.9% improvement over the baseline. Furthermore, it enhances performance by up to 8.1% on StrategyQA and 7.3% on GSM8K, both of which demand strong reasoning capabilities.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用于现实世界应用。但是，人们对它们产生的内容的可靠性的担忧经常偏离事实正确性或逻辑推理中的缺陷。本文提出了一种新颖的解码策略，旨在提高事实准确性和推理推理，而无需对LLM的体系结构或预训练的参数进行任何修改。我们的方法通过分析变压器中较低层到更高层的逻辑轨迹并应用线性回归来调整下一概率。我们发现，基于logit轨迹的方法（DELTA）的解码有效地增强了事实和推理，同时减轻了不正确的产生。关于真实性的实验表明，三角洲比基线提高了4.9％。此外，策略QA的性能最多可提高8.1％，而GSM8K的绩效提高了7.3％，这两者都需要强大的推理能力。</li>
</ul>

<h3>Title: Add-One-In: Incremental Sample Selection for Large Language Models via a Choice-Based Greedy Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Li, Yuhao Du, Xiaoqi Jiao, Yiwen Guo, Yuege Feng, Xiang Wan, Anningzhe Gao, Jinpeng Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02359">https://arxiv.org/abs/2503.02359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02359">https://arxiv.org/pdf/2503.02359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02359]] Add-One-In: Incremental Sample Selection for Large Language Models via a Choice-Based Greedy Paradigm(https://arxiv.org/abs/2503.02359)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Selecting high-quality and diverse training samples from extensive datasets plays a crucial role in reducing training overhead and enhancing the performance of Large Language Models (LLMs). However, existing studies fall short in assessing the overall value of selected data, focusing primarily on individual quality, and struggle to strike an effective balance between ensuring diversity and minimizing data point traversals. Therefore, this paper introduces a novel choice-based sample selection framework that shifts the focus from evaluating individual sample quality to comparing the contribution value of different samples when incorporated into the subset. Thanks to the advanced language understanding capabilities of LLMs, we utilize LLMs to evaluate the value of each option during the selection process. Furthermore, we design a greedy sampling process where samples are incrementally added to the subset, thereby improving efficiency by eliminating the need for exhaustive traversal of the entire dataset with the limited budget. Extensive experiments demonstrate that selected data from our method not only surpass the performance of the full dataset but also achieves competitive results with state-of-the-art (SOTA) studies, while requiring fewer selections. Moreover, we validate our approach on a larger medical dataset, highlighting its practical applicability in real-world applications.</li>
<li><strong>摘要：</strong>从广泛的数据集中选择高质量和多样化的培训样本在减少培训开销和增强大语模型（LLMS）的性能方面起着至关重要的作用。但是，现有研究在评估选定数据的总体价值，主要集中于个人质量方面，并在确保多样性和最小化数据点遍历之间取得有效的平衡方面缺乏。因此，本文介绍了一个新型的基于选择的样本选择框架，该框架将焦点从评估单个样本质量转变为在集合到子集中时的不同样品的贡献价值。得益于LLM的高级语言理解能力，我们利用LLM在选择过程中评估每个选项的价值。此外，我们设计了一个贪婪的抽样过程，在该过程中，将样品逐步添加到子集中，从而通过消除预算有限的整个数据集的必要性来提高效率。广泛的实验表明，从我们的方法中选择的数据不仅超过了完整数据集的性能，而且还通过最先进的研究（SOTA）研究获得了竞争成果，同时需要更少的选择。此外，我们在较大的医疗数据集上验证了我们的方法，从而突出了其在现实应用程序中的实际适用性。</li>
</ul>

<h3>Title: Iterative Value Function Optimization for Guided Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhenhua Liu, Lijun Li, Ruizhe Chen, Yuxian Jiang, Tong Zhu, Wenliang Chen, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02368">https://arxiv.org/abs/2503.02368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02368">https://arxiv.org/pdf/2503.02368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02368]] Iterative Value Function Optimization for Guided Decoding(https://arxiv.org/abs/2503.02368)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control.</li>
<li><strong>摘要：</strong>尽管从人类反馈中学习（RLHF）已成为控制语言模型输出的主要方法，但它具有高计算成本和培训不稳定的损失。指导解码，特别是价值指导的方法，通过控制输出而无需重新训练模型，提供了一种具有成本效益的替代方法。但是，值函数的准确性对于价值引导的解码至关重要，因为不准确会导致次优决策和降级性能。现有方法努力准确估计最佳价值函数，从而导致效率较低的控制。我们提出了迭代价值函数优化，这是一个新的框架，通过两个关键组成部分来解决这些局限性：蒙特卡洛价值估计，从而通过探索各种轨迹来降低估计方差，并迭代式的上液体优化，从而逐步改善了价值估计，从而通过收集价值领域的收集轨迹来改善轨迹估计。关于文本摘要，多转向对话和指令的广泛实验，证明了在对齐语言模型中的价值指导解码方法的有效性。这些方法不仅可以实现对齐方式，而且通过利用原则性价值功能优化的有效控制来显着降低计算成本。</li>
</ul>

<h3>Title: MedEthicEval: Evaluating Large Language Models Based on Chinese Medical Ethics</h3>
<ul>
<li><strong>Authors: </strong>Haoan Jin, Jiacheng Shi, Hanhui Xu, Kenny Q. Zhu, Mengyue Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02374">https://arxiv.org/abs/2503.02374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02374">https://arxiv.org/pdf/2503.02374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02374]] MedEthicEval: Evaluating Large Language Models Based on Chinese Medical Ethics(https://arxiv.org/abs/2503.02374)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate significant potential in advancing medical applications, yet their capabilities in addressing medical ethics challenges remain underexplored. This paper introduces MedEthicEval, a novel benchmark designed to systematically evaluate LLMs in the domain of medical ethics. Our framework encompasses two key components: knowledge, assessing the models' grasp of medical ethics principles, and application, focusing on their ability to apply these principles across diverse scenarios. To support this benchmark, we consulted with medical ethics researchers and developed three datasets addressing distinct ethical challenges: blatant violations of medical ethics, priority dilemmas with clear inclinations, and equilibrium dilemmas without obvious resolutions. MedEthicEval serves as a critical tool for understanding LLMs' ethical reasoning in healthcare, paving the way for their responsible and effective use in medical contexts.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在推进医疗应用方面具有巨大的潜力，但是它们在解决医学伦理挑战方面的能力仍然没有得到充实。本文介绍了Medethiceval，这是一种新型基准测试，旨在系统地评估医学伦理领域的LLM。我们的框架涵盖了两个关键组成部分：知识，评估模型对医学伦理原则的掌握以及应用，重点关注其在各种情况下应用这些原理的能力。为了支持这一基准，我们与医学伦理学研究人员进行了咨询，并开发了三个针对不同道德挑战的数据集：公然违反医学伦理的行为，明显的倾向和平衡困境，没有明显的决心。 Medethiceval是了解LLMS在医疗保健中的道德推理的关键工具，为在医疗环境中负责和有效使用铺平了道路。</li>
</ul>

<h3>Title: An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wei Sun, Qianlong Du, Fuwei Cui, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02382">https://arxiv.org/abs/2503.02382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02382">https://arxiv.org/pdf/2503.02382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02382]] An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning(https://arxiv.org/abs/2503.02382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) is of great scientific and practical significance. Researchers typically employ process-supervised reward models (PRMs) to guide the reasoning process, effectively improving the models' reasoning abilities. However, existing methods for constructing process supervision training data, such as manual annotation and per-step Monte Carlo estimation, are often costly or suffer from poor quality. To address these challenges, this paper introduces a framework called EpicPRM, which annotates each intermediate reasoning step based on its quantified contribution and uses an adaptive binary search algorithm to enhance both annotation precision and efficiency. Using this approach, we efficiently construct a high-quality process supervision training dataset named Epic50k, consisting of 50k annotated intermediate steps. Compared to other publicly available datasets, the PRM trained on Epic50k demonstrates significantly superior performance. Getting Epic50k at this https URL.</li>
<li><strong>摘要：</strong>增强大语言模型（LLM）的数学推理能力具有很大的科学和实际意义。研究人员通常采用程序监督奖励模型（PRM）来指导推理过程，从而有效地提高了模型的推理能力。但是，现有的构建过程监督培训数据的方法，例如手动注释和每步蒙特卡洛的估计，通常是昂贵的或质量差。为了应对这些挑战，本文介绍了一个名为Epicprm的框架，该框架根据其量化的贡献来注释每个中间推理步骤，并使用自适应二进制搜索算法来增强注释精度和效率。使用这种方法，我们有效地构建了一个名为EPIC50K的高质量过程监督培训数据集，该数据集由50K注释的中间步骤组成。与其他公开可用的数据集相比，接受EPIC50K培训的PRM表现出明显优越的性能。在此HTTPS URL中获得EPIC50K。</li>
</ul>

<h3>Title: AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and Translation Strategies for Multilingual Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Dimitra Karkani, Maria Lymperaiou, Giorgos Filandrianos, Nikolaos Spanos, Athanasios Voulodimos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02442">https://arxiv.org/abs/2503.02442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02442">https://arxiv.org/pdf/2503.02442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02442]] AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and Translation Strategies for Multilingual Hallucination Detection(https://arxiv.org/abs/2503.02442)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Multilingual hallucination detection stands as an underexplored challenge, which the Mu-SHROOM shared task seeks to address. In this work, we propose an efficient, training-free LLM prompting strategy that enhances detection by translating multilingual text spans into English. Our approach achieves competitive rankings across multiple languages, securing two first positions in low-resource languages. The consistency of our results highlights the effectiveness of our translation strategy for hallucination detection, demonstrating its applicability regardless of the source language.</li>
<li><strong>摘要：</strong>多语言幻觉检测是一个毫无争议的挑战，MU棚共享的任务寻求解决。在这项工作中，我们提出了一种高效，无训练的LLM提示策略，该策略通过将多语言文本跨度翻译成英语来增强检测。我们的方法可以在多种语言中获得竞争性排名，并以低资源语言确保了两个第一名。我们的结果的一致性突出了我们的翻译策略在幻觉检测中的有效性，无论源语言如何，都证明了其适用性。</li>
</ul>

<h3>Title: AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking</h3>
<ul>
<li><strong>Authors: </strong>Iraklis Premptis, Maria Lymperaiou, Giorgos Filandrianos, Orfeas Menis Mastromichalakis, Athanasios Voulodimos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02443">https://arxiv.org/abs/2503.02443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02443">https://arxiv.org/pdf/2503.02443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02443]] AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking(https://arxiv.org/abs/2503.02443)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The Unlearning Sensitive Content from Large Language Models task aims to remove targeted datapoints from trained models while minimally affecting their general knowledge. In our work, we leverage parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To further enhance unlearning effectiveness, we employ data chunking, splitting forget data into disjoint partitions and merging them with cyclically sampled retain samples at a pre-defined ratio. Our task-agnostic method achieves an outstanding forget-retain balance, ranking first on leaderboards and significantly outperforming baselines and competing systems.</li>
<li><strong>摘要：</strong>大型语言模型任务的敏感内容旨在从受过训练的模型中删除目标数据点，同时最小化其常识。在我们的工作中，我们利用低级别（LORA）适应和以层为中心的微调来利用基于梯度的参数，基于梯度的学习。为了进一步提高学习效率，我们采用了数据块，将忘记数据分解为不相交的分区，并将它们与周期性采样的保留样品合并，以预定的比例。我们的任务不足的方法实现了出色的忘记余额，在排行榜上排名第一，并且表现出色的基线和竞争系统。</li>
</ul>

<h3>Title: Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yilun Qiu, Xiaoyan Zhao, Yang Zhang, Yimeng Bai, Wenjie Wang, Hong Cheng, Fuli Feng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02450">https://arxiv.org/abs/2503.02450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02450">https://arxiv.org/pdf/2503.02450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02450]] Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization(https://arxiv.org/abs/2503.02450)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Personalizing Large Language Models (LLMs) has become a critical step in facilitating their widespread application to enhance individual life experiences. In pursuit of personalization, distilling key preference information from an individual's historical data as instructional preference context to customize LLM generation has emerged as a promising direction. However, these methods face a fundamental limitation by overlooking the inter-user comparative analysis, which is essential for identifying the inter-user differences that truly shape preferences. To address this limitation, we propose Difference-aware Personalization Learning (DPL), a novel approach that emphasizes extracting inter-user differences to enhance LLM personalization. DPL strategically selects representative users for comparison and establishes a structured standard to extract meaningful, task-relevant differences for customizing LLM generation. Extensive experiments on real-world datasets demonstrate that DPL significantly enhances LLM personalization. We release our code at this https URL.</li>
<li><strong>摘要：</strong>个性化大型语言模型（LLM）已成为促进其广泛应用以增强个人生活体验的关键一步。为了追求个性化，将关键的偏好信息从个人的历史数据中提取为指导性偏好上下文，以自定义LLM生成是一个有希望的方向。但是，这些方法通过忽略用户间比较分析而面临基本限制，这对于确定真正塑造偏好的用户间差异至关重要。为了解决这一限制，我们提出了差异感知的个性化学习（DPL），这种新方法强调提取用户间差异以增强LLM个性化。 DPL策略性地选择代表性用户进行比较，并建立一个结构化标准，以提取有意义的，任务相关的差异以自定义LLM生成。对现实世界数据集的广泛实验表明，DPL显着增强了LLM个性化。我们在此HTTPS URL上发布代码。</li>
</ul>

<h3>Title: It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate Mutually via Selective Rationale Optimisation</h3>
<ul>
<li><strong>Authors: </strong>Sohan Patnaik, Milan Aggarwal, Sumit Bhatia, Balaji Krishnamurthy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02463">https://arxiv.org/abs/2503.02463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02463">https://arxiv.org/pdf/2503.02463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02463]] It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate Mutually via Selective Rationale Optimisation(https://arxiv.org/abs/2503.02463)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Very large language models (LLMs) such as GPT-4 have shown the ability to handle complex tasks by generating and self-refining step-by-step rationales. Smaller language models (SLMs), typically with < 13B parameters, have been improved by using the data generated from very-large LMs through knowledge distillation. However, various practical constraints such as API costs, copyright, legal and ethical policies restrict using large (often opaque) models to train smaller models for commercial use. Limited success has been achieved at improving the ability of an SLM to explore the space of possible rationales and evaluate them by itself through self-deliberation. To address this, we propose COALITION, a trainable framework that facilitates interaction between two variants of the same SLM and trains them to generate and refine rationales optimized for the end-task. The variants exhibit different behaviors to produce a set of diverse candidate rationales during the generation and refinement steps. The model is then trained via Selective Rationale Optimization (SRO) to prefer generating rationale candidates that maximize the likelihood of producing the ground-truth answer. During inference, COALITION employs a controller to select the suitable variant for generating and refining the rationales. On five different datasets covering mathematical problems, commonsense reasoning, and natural language inference, COALITION outperforms several baselines by up to 5%. Our ablation studies reveal that cross-communication between the two variants performs better than using the single model to self-refine the rationales. We also demonstrate the applicability of COALITION for LMs of varying scales (4B to 14B parameters) and model families (Mistral, Llama, Qwen, Phi). We release the code for this work at this https URL.</li>
<li><strong>摘要：</strong>GPT-4等非常大的语言模型（LLM）表明了通过产生和自我改进的逐步理解来处理复杂任务的能力。通常使用从非常大的LMS通过知识蒸馏生成的数据，可以改善较小的语言模型（SLM），通常具有<13B参数。但是，诸如API成本，版权，法律和道德政策之类的各种实际限制限制了使用大型（通常不透明）模型来训练较小模型以供商业使用。在提高SLM探索可能理由的空间并通过自我解释本身评估的能力方面取得了有限的成功。为了解决这个问题，我们提出了联盟，这是一个可训练的框架，促进了同一SLM的两个变体之间的相互作用，并训练它们以生成和完善针对终端任务优化的理由。这些变体表现出不同的行为，可以在一代和改进步骤中产生一组不同的候选原理。然后，通过选择性理由优化（SRO）对模型进行训练，以更喜欢生成候选候选者，从而最大程度地产生基本真相答案。在推论期间，联盟采用控制者来选择合适的变体来产生和完善理由。在涉及数学问题，常识性推理和自然语言推论的五个不同数据集中，联盟的表现高达多达5％。我们的消融研究表明，两种变体之间的交叉通信性能要比使用单个模型自我重新理解理由更好。我们还证明了联盟对不同量表（4b至14b参数）和模型家族（Mistral，Llama，Qwen，Phi）的适用性。我们在此HTTPS URL上发布此工作的代码。</li>
</ul>

<h3>Title: LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jianghao Chen, Junhong Wu, Yangyifan Xu, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02502">https://arxiv.org/abs/2503.02502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02502">https://arxiv.org/pdf/2503.02502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02502]] LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs(https://arxiv.org/abs/2503.02502)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Long-context modeling has drawn more and more attention in the area of Large Language Models (LLMs). Continual training with long-context data becomes the de-facto method to equip LLMs with the ability to process long inputs. However, it still remains an open challenge to measure the quality of long-context training data. To address this issue, we propose a Long-context data selection framework with Attention-based Dependency Measurement (LADM), which can efficiently identify high-quality long-context data from a large-scale, multi-domain pre-training corpus. LADM leverages the retrieval capabilities of the attention mechanism to capture contextual dependencies, ensuring a comprehensive quality measurement of long-context data. Experimental results show that our LADM framework significantly boosts the performance of LLMs on multiple long-context tasks with only 1B tokens for continual training.</li>
<li><strong>摘要：</strong>在大型语言模型（LLMS）领域，长篇小说建模引起了越来越多的关注。使用长篇文化数据的持续训练成为装备LLM具有处理长输入能力的事实上的方法。但是，衡量长篇文本培训数据的质量仍然是一个开放的挑战。为了解决这个问题，我们提出了一个具有基于注意力的依赖性测量（LADM）的长篇小说数据选择框架，该框架可以有效地从大规模的多域中训练前体中有效地识别高质量的长篇小说数据。 LADM利用注意机制的检索能力来捕获上下文依赖性，从而确保对长篇文化数据进行全面的质量测量。实验结果表明，我们的LADM框架大大提高了LLM在多个长篇文章任务上的性能，只有1B代币进行连续培训。</li>
</ul>

<h3>Title: Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent</h3>
<ul>
<li><strong>Authors: </strong>Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02519">https://arxiv.org/abs/2503.02519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02519">https://arxiv.org/pdf/2503.02519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02519]] Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent(https://arxiv.org/abs/2503.02519)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）代理通常采用逐步推理框架，在该框架中，他们交织了思考和行动以完成给定任务的过程。但是，该范式面临着根深蒂固的一通问题，每个产生的中间思想都会插入轨迹中，无论其正确性如何，这可能会导致不可逆的错误传播。为了解决这个问题，本文提出了一个新颖的框架，称为“发电机辅助逐步回滚”（GA-ROLLBACK），以诱导LLM代理的更好决策。特别是，Ga-Rollback利用发电机与环境进行交互，助手检查发电机产生的每个动作，助手在发现不正确的动作后会触发回滚操作。此外，我们介绍了针对回滚场景量身定制的另外两种策略，以进一步提高其有效性。广泛的实验表明，Ga-Rollback在三个广泛使用的基准测试的基准上取得了重大改进。我们的分析进一步表明，GA-ROLLBACK可以用作强大的插件模块，并与其他方法无缝集成。</li>
</ul>

<h3>Title: MciteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Caiyu Hu, Yikai Zhang, Tinghui Zhu, Yiwei Ye, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02589">https://arxiv.org/abs/2503.02589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02589">https://arxiv.org/pdf/2503.02589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02589]] MciteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs(https://arxiv.org/abs/2503.02589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have advanced in integrating diverse modalities but frequently suffer from hallucination. A promising solution to mitigate this issue is to generate text with citations, providing a transparent chain for verification. However, existing work primarily focuses on generating citations for text-only content, overlooking the challenges and opportunities of multimodal contexts. To address this gap, we introduce MCiteBench, the first benchmark designed to evaluate and analyze the multimodal citation text generation ability of MLLMs. Our benchmark comprises data derived from academic papers and review-rebuttal interactions, featuring diverse information sources and multimodal content. We comprehensively evaluate models from multiple dimensions, including citation quality, source reliability, and answer accuracy. Through extensive experiments, we observe that MLLMs struggle with multimodal citation text generation. We also conduct deep analyses of models' performance, revealing that the bottleneck lies in attributing the correct sources rather than understanding the multimodal content.</li>
<li><strong>摘要：</strong>多模式的大语言模型（MLLM）在整合多种方式方面已经进步，但经常遭受幻觉。缓解此问题的有前途的解决方案是用引用生成文本，提供透明的链条进行验证。但是，现有工作主要集中于为仅文本内容引用引用，忽视了多模式环境的挑战和机会。为了解决这一差距，我们介绍了MciteBench，这是第一个旨在评估和分析MLLM的多模式引用文本生成能力的基准。我们的基准包括来自学术论文和评论 - 雷神交互的数据，这些数据具有多种信息源和多模式内容。我们全面评估了来自多个维度的模型，包括引文质量，源可靠性和答案准确性。通过广泛的实验，我们观察到MLLM与多模式引用文本生成斗争。我们还对模型的性能进行了深入的分析，表明瓶颈在于归因于正确的来源，而不是理解多模式内容。</li>
</ul>

<h3>Title: OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query Processing</h3>
<ul>
<li><strong>Authors: </strong>Yulong Hui, Yihao Liu, Yao Lu, Huanchen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02603">https://arxiv.org/abs/2503.02603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02603">https://arxiv.org/pdf/2503.02603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02603]] OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query Processing(https://arxiv.org/abs/2503.02603)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) encounter challenges in efficiently processing long-text queries, as seen in applications like enterprise document analysis and financial report comprehension. While conventional solutions employ long-context processing or Retrieval-Augmented Generation (RAG), they suffer from prohibitive input expenses or incomplete information. Recent advancements adopt context compression and dynamic retrieval loops, but still sacrifice critical details or incur iterative this http URL address these limitations, we propose OkraLong, a novel framework that flexibly optimizes the entire processing workflow. Unlike prior static or coarse-grained adaptive strategies, OkraLong adopts fine-grained orchestration through three synergistic components: analyzer, organizer and executor. The analyzer characterizes the task states, which guide the organizer in dynamically scheduling the workflow. The executor carries out the execution and generates the final answer. Experimental results demonstrate that OkraLong not only enhances answer accuracy but also achieves cost-effectiveness across a variety of datasets.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在有效处理长文本查询方面遇到挑战，如企业文档分析和财务报告理解等应用程序所示。尽管常规解决方案采用长篇文章处理或检索命名的发电（RAG），但它们遭受了投入费用过高或不完整的信息。最近的进步采用了上下文压缩和动态检索循环，但仍然牺牲关键细节或迭代此HTTP URL解决这些局限性，我们提出了Okralong，这是一个新颖的框架，可以灵活地优化整个处理工作流程。与先前的静态或粗粒剂自适应策略不同，Okralong通过三个协同组件采用细粒度的编排：分析仪，组织者和执行者。分析仪表征任务状态，该任务状态指导组织者动态安排工作流程。执行人执行执行并生成最终答案。实验结果表明，Okralong不仅增强了答案的准确性，而且还可以在各种数据集中实现成本效益。</li>
</ul>

<h3>Title: Rewarding Doubt: A Reinforcement Learning Approach to Confidence Calibration of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Paul Stangel, David Bani-Harouni, Chantal Pellegrini, Ege Özsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02623">https://arxiv.org/abs/2503.02623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02623">https://arxiv.org/pdf/2503.02623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02623]] Rewarding Doubt: A Reinforcement Learning Approach to Confidence Calibration of Large Language Models(https://arxiv.org/abs/2503.02623)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We introduce a novel Reinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs to elicit calibrated confidence estimations in their answers to factual questions. We model the problem as a betting game where the model predicts a confidence score together with every answer, and design a reward function that penalizes both over and under-confidence. We prove that under our reward design an optimal policy would result in a perfectly calibrated confidence estimation. Our experiments demonstrate significantly improved confidence calibration and generalization to new tasks without re-training, indicating that our approach teaches a general confidence awareness. This approach enables the training of inherently calibrated LLMs.</li>
<li><strong>摘要：</strong>安全且值得信赖的大语模型（LLM）需要准确表达其答案的信心。我们介绍了一种新颖的加强学习方法（RL）进行LLM校准，该方法微调LLMS在其对事实问题的回答中引起了校准的置信度估计。我们将问题建模为博彩游戏，该游戏模型将置信度得分与每个答案一起预测，并设计奖励功能，以惩罚过度和不信任的奖励功能。我们证明，在我们的奖励设计下，最佳政策将导致置信度的估计。我们的实验表明，在不重新训练的情况下，明显提高了对新任务的置信度校准和概括，表明我们的方法传达了一般的信心意识。这种方法可以训练固有的校准LLM。</li>
</ul>

<h3>Title: Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Liu, Zixuan Li, Long Bai, Yuxin Zuo, Daozhu Xu, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02628">https://arxiv.org/abs/2503.02628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02628">https://arxiv.org/pdf/2503.02628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02628]] Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction(https://arxiv.org/abs/2503.02628)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Developing a general-purpose extraction system that can extract events with massive types is a long-standing target in Event Extraction (EE). In doing so, the challenge comes from two aspects: 1) The absence of an efficient and effective annotation method. 2) The absence of a powerful extraction method can handle massive types. For the first challenge, we propose a collaborative annotation method based on Large Language Models (LLMs). Through collaboration among multiple LLMs, it first refines annotations of trigger words from distant supervision and then carries out argument annotation. Next, a voting phase consolidates the annotation preferences across different LLMs. Finally, we create the EEMT dataset, the largest EE dataset to date, featuring over 200,000 samples, 3,465 event types, and 6,297 role types. For the second challenge, we propose an LLM-based Partitioning EE method called LLM-PEE. To overcome the limited context length of LLMs, LLM-PEE first recalls candidate event types and then splits them into multiple partitions for LLMs to extract events. The results in the supervised setting show that LLM-PEE outperforms the state-of-the-art methods by 5.4 in event detection and 6.1 in argument extraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement compared to mainstream LLMs, demonstrating its strong generalization capabilities.</li>
<li><strong>摘要：</strong>在事件提取（EE）中，开发一种可以提取大量类型的事件的通用提取系统（EE）是一个长期存在的目标。在这样做的过程中，挑战来自两个方面：1）缺乏有效的注释方法。 2）缺乏强大的提取方法可以处理大量类型。对于第一个挑战，我们提出了一种基于大语言模型（LLM）的协作注释方法。通过在多个LLMS之间的协作，它首先完善了从遥远的监督中触发单词的注释，然后进行了参数注释。接下来，投票阶段巩固了不同LLM的注释偏好。最后，我们创建了EEMT数据集，即迄今为止最大的EE数据集，其中包含超过200,000个样本，3,465个事件类型和6,297个角色类型。对于第二个挑战，我们提出了一种基于LLM的分区EE方法，称为LLM-PEE。为了克服LLM的有限上下文长度，LLM-PEE首先回忆候选事件类型，然后将它们分成多个分区以供LLMS提取事件。监督设置中的结果表明，LLM-PEE在事件检测中优于5.4的最新方法，而参数提取中的6.1。与主流LLM相比，LLM-PEE在零拍摄的环境中可提高12.9，表明其强大的概括能力。</li>
</ul>

<h3>Title: Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks</h3>
<ul>
<li><strong>Authors: </strong>Paul Suganthan, Fedor Moiseev, Le Yan, Junru Wu, Jianmo Ni, Jay Han, Imed Zitouni, Enrique Alfonseca, Xuanhui Wang, Zhe Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02656">https://arxiv.org/abs/2503.02656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02656">https://arxiv.org/pdf/2503.02656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02656]] Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks(https://arxiv.org/abs/2503.02656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Decoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in tasks like classification, regression, and ranking. This is primarily due to the inherent structure of decoder-based models, which limits their direct applicability to these tasks. In this paper, we introduce Gemma Encoder, adapting the powerful Gemma decoder model to an encoder architecture, thereby unlocking its potential for a wider range of non-generative applications. To optimize the adaptation from decoder to encoder, we systematically analyze various pooling strategies, attention mechanisms, and hyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder against established approaches on the GLUE benchmarks, and MS MARCO ranking benchmark, demonstrating its effectiveness and versatility.</li>
<li><strong>摘要：</strong>基于解码器的变压器在将语言建模和扩展为巨大尺寸的同时，在自然语言处理中尚未完全取代编码器体系结构。具体而言，仅编码模型在分类，回归和排名等任务中仍然占主导地位。这主要是由于基于解码器的模型的固有结构，该结构限制了其直接适用于这些任务。在本文中，我们介绍了Gemma编码器，将功能强大的Gemma解码器模型调整为编码器体系结构，从而解开其对更广泛的非生成应用的潜力。为了优化从解码器到编码器的适应，我们系统地分析了各种汇总策略，注意机制和超参数（例如，辍学率）。此外，我们基于针对胶水基准的既定方法进行了基准编码，而Marco对基准进行了排名，这表明了其有效性和多功能性。</li>
</ul>

<h3>Title: LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pengwei Tang, Yong Liu, Dongjie Zhang, Xing Wu, Debing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02659">https://arxiv.org/abs/2503.02659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02659">https://arxiv.org/pdf/2503.02659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02659]] LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models(https://arxiv.org/abs/2503.02659)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is the leading parameter-efficient fine-tuning method for Large Language Models (LLMs). However, the fine-tuned LLMs encounter the issue of catastrophic forgetting of the pre-trained world knowledge. To address this issue, inspired by theoretical insights of null space, we propose LoRA-Null, i.e., Low-Rank Adaptation via null space, which builds adapters initialized from the null space of the pre-trained knowledge activation. Concretely, we randomly collect a few data samples and capture their activations after passing through the LLM layer. We perform Singular Value Decomposition on the input activations to obtain their null space. We use the projection of the pre-trained weights onto the null space as the initialization for adapters. Experimental results demonstrate that this initialization approach can effectively preserve the original pre-trained world knowledge of the LLMs during fine-tuning. Additionally, if we freeze the values of the down-projection matrices during fine-tuning, it achieves even better preservation of the pre-trained world knowledge. LoRA-Null effectively preserves pre-trained world knowledge while maintaining strong fine-tuning performance, as validated by extensive experiments on LLaMA series (LLaMA2, LLaMA3, LLaMA3.1, and LLaMA3.2) across Code, Math, and Instruction Following tasks. We also provide a theoretical guarantee for the capacity of LoRA-Null to retain pre-trained knowledge. Code is in this https URL.</li>
<li><strong>摘要：</strong>低级适应性（LORA）是大型语言模型（LLMS）的领先参数有效的微调方法。但是，微调的LLMS遇到了灾难性忘记预训练的世界知识的问题。为了解决这个问题，灵感来自零空间的理论见解，我们提出了Lora-null，即通过Null Space的低级别适应性，该空间构建了从预训练的知识激活的空空间初始化的适配器。具体而言，我们在通过LLM层后随机收集一些数据样本并捕获其激活。我们在输入激活上执行奇异的值分解以获得其空空间。我们将预先训练的权重投影在空空间中作为适配器的初始化。实验结果表明，这种初始化方法可以有效地保留微调过程中LLM的原始预训练世界知识。此外，如果我们在微调过程中冻结了下降矩阵的值，它可以更好地保存预训练的世界知识。洛拉·纳尔（Lora-Null）有效地保留了预培训的世界知识，同时保持了强大的微调表现，这是通过在代码，数学和指导下进行的Llama Series（Llama2，Llama3，Llama3.1和Llama3.2）进行的广泛实验（Llama2，Llama3，Llama3.1和Llama3.2）的验证。我们还为洛拉·纳尔（Lora-Null）保留预培训知识的能力提供了理论保证。代码在此HTTPS URL中。</li>
</ul>

<h3>Title: Multidimensional Consistency Improves Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huiyuan Lai, Xiao Zhang, Malvina Nissim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02670">https://arxiv.org/abs/2503.02670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02670">https://arxiv.org/pdf/2503.02670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02670]] Multidimensional Consistency Improves Reasoning in Language Models(https://arxiv.org/abs/2503.02670)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across input variations can thus be taken as a sign of stronger confidence. Leveraging this insight, we introduce a framework, {\em Multidimensional Reasoning Consistency} where, focusing on math problems, models are systematically pushed to diversify solution paths towards a final answer, thereby testing them for answer consistency across multiple input variations. We induce variations in (i) order of shots in prompt, (ii) problem phrasing, and (iii) languages used. Extensive experiments on a large range of open-source state-of-the-art LLMs of various sizes show that reasoning consistency differs by variation dimension, and that by aggregating consistency across dimensions, our framework consistently enhances mathematical reasoning performance on both monolingual dataset GSM8K and multilingual dataset MGSM, especially for smaller models.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）已证明能够解决某些复杂的推理任务，但我们也知道它们对输入变化高度敏感，这可能导致不同的解决方案路径和最终答案。因此，可以将跨输入变化的答案一致性视为具有更强置信度的迹象。利用这种见解，我们介绍了一个框架，{\ em多维推理一致性}在其中，专注于数学问题，系统地推动模型以多元化的解决方案路径向最终答案进行多元化，从而对它们进行测试，从而在多个输入变化上测试它们以确保答案的一致性。我们在提示，（ii）问题措辞和（iii）使用的语言中引起（i）镜头顺序的变化。对各种尺寸的各种开源的最先进的LLM进行了广泛的实验表明，推理一致性因变化维度而有所不同，并且通过跨维度的一致性汇总一致性，我们的框架一致地增强了数学推理性能在单语言数据集GSM8K和多语言数据集MGSM上，尤其是较小的型号。</li>
</ul>

<h3>Title: MPO: Boosting LLM Agents with Meta Plan Optimization</h3>
<ul>
<li><strong>Authors: </strong>Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02682">https://arxiv.org/abs/2503.02682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02682">https://arxiv.org/pdf/2503.02682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02682]] MPO: Boosting LLM Agents with Meta Plan Optimization(https://arxiv.org/abs/2503.02682)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展使基于LLM的代理能够成功处理互动计划任务。但是，尽管取得了成功，但现有的方法通常会遭受计划幻觉的困扰，并且需要为每个新代理商进行再培训。为了应对这些挑战，我们提出了META计划优化（MPO）框架，该框架通过直接合并明确的指导来增强代理计划功能。与以前依靠复杂知识（需要大量人力努力或缺乏质量保证的方法）的方法不同，MPO通过META计划利用高水平的一般指导来协助代理计划，并基于代理商任务执行的反馈来持续优化元计划。我们对两个代表性任务进行的实验表明，MPO明显优于现有基准。此外，我们的分析表明，MPO提供了一个插件解决方案，可在以前的看不见的情况下提高任务完成效率和概括能力。</li>
</ul>

<h3>Title: Evaluating Knowledge Generation and Self-Refinement Strategies for LLM-based Column Type Annotation</h3>
<ul>
<li><strong>Authors: </strong>Keti Korini, Christian Bizer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02718">https://arxiv.org/abs/2503.02718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02718">https://arxiv.org/pdf/2503.02718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02718]] Evaluating Knowledge Generation and Self-Refinement Strategies for LLM-based Column Type Annotation(https://arxiv.org/abs/2503.02718)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Understanding the semantics of columns in relational tables is an important pre-processing step for indexing data lakes in order to provide rich data search. An approach to establishing such understanding is column type annotation (CTA) where the goal is to annotate table columns with terms from a given vocabulary. This paper experimentally compares different knowledge generation and self-refinement strategies for LLM-based column type annotation. The strategies include using LLMs to generate term definitions, error-based refinement of term definitions, self-correction, and fine-tuning using examples and term definitions. We evaluate these strategies along two dimensions: effectiveness measured as F1 performance and efficiency measured in terms of token usage and cost. Our experiments show that the best performing strategy depends on the model/dataset combination. We find that using training data to generate label definitions outperforms using the same data as demonstrations for in-context learning for two out of three datasets using OpenAI models. The experiments further show that using the LLMs to refine label definitions brings an average increase of 3.9% F1 in 10 out of 12 setups compared to the performance of the non-refined definitions. Combining fine-tuned models with self-refined term definitions results in the overall highest performance, outperforming zero-shot prompting fine-tuned models by at least 3% in F1 score. The costs analysis shows that while reaching similar F1 score, self-refinement via prompting is more cost efficient for use cases requiring smaller amounts of tables to be annotated while fine-tuning is more efficient for large amounts of tables.</li>
<li><strong>摘要：</strong>了解关系表中的列的语义是索引数据湖以提供丰富数据搜索的重要预处理步骤。建立这种理解的方法是列类型注释（CTA），其中的目标是用给定词汇中的术语注释表列。本文在实验上比较了基于LLM的列类型注释的不同知识产生和自我注册策略。这些策略包括使用LLMS生成术语定义，基于错误的术语定义的完善，自我纠正以及使用示例和术语定义进行微调。我们沿两个维度评估了这些策略：根据令牌的使用和成本衡量的有效性，以F1的性能和效率来衡量。我们的实验表明，最佳性能策略取决于模型/数据集组合。我们发现，使用培训数据使用openai模型，使用与在三个数据集中的两个数据集中进行的相同的数据生成标签定义的表现优于外观学习的相同数据。该实验进一步表明，与未精制定义的性能相比，使用LLMS来完善标签定义的定义在12个设置中的10个中的平均增加3.9％。将微调模型与自我精制的项定义结合起来会导致总体性能最高，超过零拍摄，促使微型模型在F1分数中至少提高3％。成本分析表明，在达到相似的F1分数时，通过提示进行自我进行的自我进行更有效的效率对于需要少量的表格注释的用例，而微调对于大量表来说更有效。</li>
</ul>

<h3>Title: Large Language Models for Multilingual Previously Fact-Checked Claim Detection</h3>
<ul>
<li><strong>Authors: </strong>Ivan Vykopal, Matúš Pikuliak, Simon Ostermann, Tatiana Anikina, Michal Gregor, Marián Šimko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02737">https://arxiv.org/abs/2503.02737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02737">https://arxiv.org/pdf/2503.02737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02737]] Large Language Models for Multilingual Previously Fact-Checked Claim Detection(https://arxiv.org/abs/2503.02737)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In our era of widespread false information, human fact-checkers often face the challenge of duplicating efforts when verifying claims that may have already been addressed in other countries or languages. As false information transcends linguistic boundaries, the ability to automatically detect previously fact-checked claims across languages has become an increasingly important task. This paper presents the first comprehensive evaluation of large language models (LLMs) for multilingual previously fact-checked claim detection. We assess seven LLMs across 20 languages in both monolingual and cross-lingual settings. Our results show that while LLMs perform well for high-resource languages, they struggle with low-resource languages. Moreover, translating original texts into English proved to be beneficial for low-resource languages. These findings highlight the potential of LLMs for multilingual previously fact-checked claim detection and provide a foundation for further research on this promising application of LLMs.</li>
<li><strong>摘要：</strong>在我们广泛的虚假信息时代，当验证其他国家或语言已经解决的主张时，人类事实检查者通常会面临重复努力的挑战。随着虚假信息超越语言界限，自动检测到以前事实核对主张的能力已成为越来越重要的任务。本文介绍了对大语模型（LLMS）的首次全面评估，用于多语种以前的事实检查的索赔检测。我们在单语言和跨语性设置中评估了20种语言的七个LLM。我们的结果表明，虽然LLM在高资源语言方面表现良好，但它们在低资源语言中挣扎。此外，将原始文本翻译成英文被证明对低资源语言有益。这些发现凸显了LLM在多语言以前事实检查的索赔检测中的潜力，并为这一有希望的LLMS应用提供了进一步研究的基础。</li>
</ul>

<h3>Title: BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched Prompting and Prompt Compression</h3>
<ul>
<li><strong>Authors: </strong>Daniil Larionov, Steffen Eger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02756">https://arxiv.org/abs/2503.02756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02756">https://arxiv.org/pdf/2503.02756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02756]] BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched Prompting and Prompt Compression(https://arxiv.org/abs/2503.02756)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Model (LLM)-based Natural Language Generation evaluation have largely focused on single-example prompting, resulting in significant token overhead and computational inefficiencies. In this work, we introduce BatchGEMBA-MQM, a framework that integrates batched prompting with the GEMBA-MQM metric for machine translation evaluation. Our approach aggregates multiple translation examples into a single prompt, reducing token usage by 2-4 times (depending on the batch size) relative to single-example prompting. Furthermore, we propose a batching-aware prompt compression model that achieves an additional token reduction of 13-15% on average while also showing ability to help mitigate batching-induced quality degradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral Small, Phi4, and CommandR7B) and varying batch sizes reveal that while batching generally negatively affects quality (but sometimes not substantially), prompt compression does not degrade further, and in some cases, recovers quality loss. For instance, GPT-4o retains over 90% of its baseline performance at a batch size of 4 when compression is applied, compared to a 44.6% drop without compression. We plan to release our code and trained models at this https URL to support future research in this domain.</li>
<li><strong>摘要：</strong>基于大语言模型（LLM）的自然语言生成评估的最新进展主要集中在单个例子的提示上，导致了大量的令牌开销和计算效率低下。在这项工作中，我们介绍了Batchgemba-MQM，该框架将批处理的提示与GEMBA-MQM Metric集成了用于机器翻译评估的gemba-MQM指标。我们的方法将多个翻译示例汇总到一个提示中，相对于单个例子提示，将令牌用法减少了2-4次（取决于批处理大小）。此外，我们提出了一个批处理感知的及时压缩模型，该模型平均达到了另外的标记减少13-15％，同时还显示出帮助减轻批处理引起的质量降解的能力。跨多个LLM（GPT-4O，GPT-4O-MINI，MISTRAL SMILLE，PHI4和COMMANRR7B）和不同批量尺寸的评估表明，批处理通常会对质量产生负面影响（但有时不会实质性地影响质量），但及时压缩并不会进一步降低，在某些情况下会降低质量损失。例如，GPT-4O在施加压缩时将其基线性能的90％以上的批次大小保留为4，而没有压缩的44.6％的下降。我们计划在此HTTPS URL上发布我们的代码和经过训练的模型，以支持该领域的未来研究。</li>
</ul>

<h3>Title: From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine Symbolic Language for Modern Clinical Relevance</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Tang, Nankai Wu, Fan Gao, Chengxiao Dai, Mengyao Zhao, Xinjie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02760">https://arxiv.org/abs/2503.02760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02760">https://arxiv.org/pdf/2503.02760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02760]] From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine Symbolic Language for Modern Clinical Relevance(https://arxiv.org/abs/2503.02760)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM), conveying complex disease mechanisms and holistic health concepts through culturally rich and often abstract terminology. Bridging these metaphors to anatomically driven Western medical (WM) concepts poses significant challenges for both automated language processing and real-world clinical practice. To address this gap, we propose a novel multi-agent and chain-of-thought (CoT) framework designed to interpret TCM metaphors accurately and map them to WM pathophysiology. Specifically, our approach combines domain-specialized agents (TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise chain-of-thought prompts to ensure transparent reasoning and conflict resolution. We detail a methodology for building a metaphor-rich TCM dataset, discuss strategies for effectively integrating multi-agent collaboration and CoT reasoning, and articulate the theoretical underpinnings that guide metaphor interpretation across distinct medical paradigms. We present a comprehensive system design and highlight both the potential benefits and limitations of our approach, while leaving placeholders for future experimental validation. Our work aims to support clinical decision-making, cross-system educational initiatives, and integrated healthcare research, ultimately offering a robust scaffold for reconciling TCM's symbolic language with the mechanistic focus of Western medicine.</li>
<li><strong>摘要：</strong>传统中药（TCM）中的隐喻表达丰富，通过文化丰富且通常是抽象的术语传达复杂的疾病机制和整体健康概念。将这些隐喻桥接到解剖学驱动的西方医学概念（WM）概念对自动化语言处理和现实世界临床实践构成了重大挑战。为了解决这一差距，我们提出了一种新型的多试剂和链链（COT）框架，旨在准确解释TCM隐喻并将其映射到WM病理生理学。具体而言，我们的方法将领域专题化的代理（TCM专家，WM Expert）与协调剂相结合，利用了逐步的思维链提示，以确保透明的推理和冲突解决。我们详细介绍了一种构建隐喻丰富的TCM数据集的方法，讨论了有效整合多代理协作和COT推理的策略，并阐明了理论基础，从而指导跨不同医疗范式的隐喻解释。我们介绍了全面的系统设计，并突出了我们方法的潜在好处和局限性，同时让占位符进行未来的实验验证。我们的工作旨在支持临床决策，跨系统的教育计划和综合医疗研究，最终为将TCM的象征性语言与西方医学的机械重点相结合提供了强大的脚手架。</li>
</ul>

<h3>Title: Implicit Bias in LLMs: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Xinru Lin, Luyang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02776">https://arxiv.org/abs/2503.02776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02776">https://arxiv.org/pdf/2503.02776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02776]] Implicit Bias in LLMs: A Survey(https://arxiv.org/abs/2503.02776)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Due to the implement of guardrails by developers, Large language models (LLMs) have demonstrated exceptional performance in explicit bias tests. However, bias in LLMs may occur not only explicitly, but also implicitly, much like humans who consciously strive for impartiality yet still harbor implicit bias. The unconscious and automatic nature of implicit bias makes it particularly challenging to study. This paper provides a comprehensive review of the existing literature on implicit bias in LLMs. We begin by introducing key concepts, theories and methods related to implicit bias in psychology, extending them from humans to LLMs. Drawing on the Implicit Association Test (IAT) and other psychological frameworks, we categorize detection methods into three primary approaches: word association, task-oriented text generation and decision-making. We divide our taxonomy of evaluation metrics for implicit bias into two categories: single-value-based metrics and comparison-value-based metrics. We classify datasets into two types: sentences with masked tokens and complete sentences, incorporating datasets from various domains to reflect the broad application of LLMs. Although research on mitigating implicit bias in LLMs is still limited, we summarize existing efforts and offer insights on future challenges. We aim for this work to serve as a clear guide for researchers and inspire innovative ideas to advance exploration in this task.</li>
<li><strong>摘要：</strong>由于开发人员实施了护栏，因此大型语言模型（LLMS）在显式偏置测试中表现出了出色的性能。但是，LLM的偏见可能不仅明确地发生，而且是隐式的，就像人类有意识地努力争取公正性但仍然具有隐性偏见的人类一样。隐性偏见的无意识和自动性质使研究特别具有挑战性。本文对LLMS中隐性偏见的现有文献进行了全面综述。我们首先介绍与心理学上隐性偏见有关的关键概念，理论和方法，从而将其从人类扩展到LLM。利用隐式关联测试（IAT）和其他心理框架，我们将检测方法分为三种主要方法：单词关联，面向任务的文本生成和决策。我们将对隐性偏见的评估指标的分类学分为两类：基于单值的指标和基于比较值的指标。我们将数据集分类为两种类型：带有蒙版令牌和完整句子的句子，结合了来自各个域的数据集，以反映LLMS的广泛应用。尽管关于减轻LLM中隐性偏见的研究仍然有限，但我们总结了现有的努力，并就未来的挑战提供了见解。我们的目标是这项工作是研究人员的明确指南，并激发创新的思想，以推进这项任务的探索。</li>
</ul>

<h3>Title: IterPref: Focal Preference Learning for Code Generation via Iterative Debugging</h3>
<ul>
<li><strong>Authors: </strong>Jie Wu, Haoling Li, Xin Zhang, Jianwen Luo, Yangyu Huang, Ruihang Chu, Yujiu Yang, Scarlett Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02783">https://arxiv.org/abs/2503.02783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02783">https://arxiv.org/pdf/2503.02783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02783]] IterPref: Focal Preference Learning for Code Generation via Iterative Debugging(https://arxiv.org/abs/2503.02783)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, as aligning failing code as a whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made publicaly available.</li>
<li><strong>摘要：</strong>偏好学习通过利用相对质量比较来增强代码LLM，超出监督微调。现有方法基于测试案例的成功构建了候选人的偏好对，将较高的及格率样本视为正，较低为负。但是，此方法并未查明代码中的特定错误，这阻止了模型学习更有信息的误差校正模式，因为整个代码整体上都缺乏捕获有意义的错误分辨率关系所需的粒度。为了解决这些问题，我们提出了ITERPREF，这是一个新的偏好对齐框架，模仿了人类迭代调试以完善代码LLMS。 iterpref明确定位误差区域，并通过量身定制的DPO算法对应着相应的令牌。为了生成信息对，我们介绍了CodeFlow数据集，其中样本进行了迭代完善，直到通过测试进行修改以捕获错误更正。广泛的实验表明，配备ITERPREF的各种代码LLM套件可在代码生成方面取得显着的性能，并改善了BigCodebench等挑战性任务。深入分析表明，iterPref产生的错误较少。我们的代码和数据将被提供公开。</li>
</ul>

<h3>Title: Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression</h3>
<ul>
<li><strong>Authors: </strong>Nathan Godey, Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini, Éric de la Clergerie, Benoît Sagot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02812">https://arxiv.org/abs/2503.02812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02812">https://arxiv.org/pdf/2503.02812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02812]] Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression(https://arxiv.org/abs/2503.02812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.</li>
<li><strong>摘要：</strong>自回归语言模型依赖于键值（KV）缓存，该缓存避免了在发电期间重新计算过去的隐藏状态，从而使其更快。随着模型尺寸和上下文长度的增长，KV缓存成为一个重要的内存瓶颈，该记忆瓶颈需要限制其在发电期间大小的压缩方法。在本文中，我们发现查询（q）和键（k）向量的令人惊讶的属性，使我们能够在不计算注意力图的情况下有效地近似关注得分。我们提出了Q滤波器，这是一种无训练的KV缓存压缩方法，该方法基于单个上下文 - 不平衡投影滤除了较少关键的键值对。与许多替代方案相反，Q滤波器与闪光兼容兼容，因为它不需要直接访问注意力重量。长篇文章设置中的实验结果表明，Q滤波器在检索任务中具有基于注意力的压缩方法（例如SNAPKV）具有竞争力，同时始终超过有效的压缩方案，例如生成设置中的流式压缩方案。值得注意的是，Q滤波器在X32压缩水平上实现了99％的核心任务，同时与流媒体LLM相比，文本生成中的产生困惑下降高达65％。</li>
</ul>

<h3>Title: AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation</h3>
<ul>
<li><strong>Authors: </strong>Songming Zhang, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02832">https://arxiv.org/abs/2503.02832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02832">https://arxiv.org/pdf/2503.02832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02832]] AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation(https://arxiv.org/abs/2503.02832)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.</li>
<li><strong>摘要：</strong>在现代的大语言模型（LLM）中，LLM对准至关重要，通常是通过诸如从人类反馈（RLHF）和直接偏好优化（DPO）等方法来实现的。但是，在大多数现有的LLM对齐方法中，使用稀疏，响应级奖励或偏好注释对响应中的所有令牌进行了优化。令牌级别的奖励的无知可能会错误地惩罚高质量的代币或鼓励低质量令牌，从而导致次优的性能和缓慢的收敛速度。为了解决这个问题，我们提出了AlignDistil，这是用于令牌级奖励优化的RLHF等效蒸馏方法。具体而言，我们将DPO学到的奖励介绍给RLHF目标，并从理论上证明了该目标和令牌级蒸馏过程之间的等价性，在该过程中，教师分布在其中线性地结合了DPO模型和参考模型的逻辑。在此基础上，我们通过使用正常和反向DPO模型建立对比度DPO奖励，进一步弥合DPO模型与纯奖励模型的奖励之间的准确性差距。此外，为了避免在不同令牌上不高度优化和过度优化，我们设计了一个令牌自适应logit外推机制，以为每个令牌构建适当的教师分布。实验结果表明，由于其令牌级别的分布奖励优化，我们对位属的优势比现有方法的优越性，并展示了快速收敛。</li>
</ul>

<h3>Title: Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02846">https://arxiv.org/abs/2503.02846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02846">https://arxiv.org/pdf/2503.02846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02846]] Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs(https://arxiv.org/abs/2503.02846)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在各个领域充当AI助手时表现出幻觉（即不忠或荒谬的信息）。由于幻觉总是在LLM响应中带有真实的内容，因此以前的事实一致性方法在培训过程中不可避免地会引入响应级别的偏好学习。因此，本文提出了一种基于直接偏好优化（DPO）的细粒度事实对准方法，称为Mask-DPO。将句子级别的事实纳入面具信号，面具-DPO只能从首选样本中的事实正确句子中学习，并防止对非首选样本中的事实内容的惩罚，从而解决了偏好学习中的歧义。广泛的实验结果表明，蒙版-DPO可以显着改善LLMS对问题的事实，尽管这些问题及其相应的主题在培训过程中却看不见。 ANAH测试集上的Llama3.1-8b构造的得分从49.19％提高到77.53％，甚至超过Llama3.1-70B教学的得分（53.44％），而在Biograch Biogractography ofdagraphy Datapography的情况下，llama3.1-8b的得分从49.19％提高到了77.53％，而在ANAH测试集上的得分从ANAH的训练套装中得到了训练，而在ANAH测试集上仅受过训练，而在30.29％的训练中，甚至超过了llama3.1-70b教学的得分（53.44％），而在未域名群中，则在30.29％上也改善了该数据量表的事实调查。我们使用不同的训练样本缩放策略进一步研究了面膜-DPO的概括属性，并发现比例比例比问题的数量更有效。我们提供了一个假设，即事实对齐与LLM的作用，对这种现象的含义，并进行概念验证实验以验证它。我们希望该方法和发现为未来的研究缩放事实一致性铺平了道路。</li>
</ul>

<h3>Title: Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers</h3>
<ul>
<li><strong>Authors: </strong>Zicong He, Boxuan Zhang, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02851">https://arxiv.org/abs/2503.02851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02851">https://arxiv.org/pdf/2503.02851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02851]] Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers(https://arxiv.org/abs/2503.02851)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to hallucinate, a phenomenon often linked to creativity. While previous research has primarily explored this connection through theoretical or qualitative lenses, our work takes a quantitative approach to systematically examine the relationship between hallucination and creativity in LLMs. Given the complex nature of creativity, we propose a narrow definition tailored to LLMs and introduce an evaluation framework, HCL, which quantifies Hallucination and Creativity across different Layers of LLMs during decoding. Our empirical analysis reveals a tradeoff between hallucination and creativity that is consistent across layer depth, model type, and model size. Notably, across different model architectures, we identify a specific layer at each model size that optimally balances this tradeoff. Additionally, the optimal layer tends to appear in the early layers of larger models, and the confidence of the model is also significantly higher at this layer. These findings provide a quantitative perspective that offers new insights into the interplay between LLM creativity and hallucination. The code and data for our experiments are available at this https URL.</li>
<li><strong>摘要：</strong>众所周知，大型语言模型（LLM）是幻觉，这一现象通常与创造力有关。虽然先前的研究主要通过理论或定性镜头探讨了这种联系，但我们的工作采用了定量方法来系统地检查LLM中幻觉与创造力之间的关系。鉴于创造力的复杂性质，我们提出了一个针对LLM的狭窄定义，并引入了评估框架HCL，该框架在解码过程中量化了不同LLM的不同层的幻觉和创造力。我们的经验分析揭示了幻觉和创造力之间的权衡，在层深，模型类型和模型大小之间是一致的。值得注意的是，在不同的模型体系结构中，我们在每个模型大小上确定一个特定层，可以最佳地平衡此权衡。此外，最佳层倾向于出现在较大模型的早期层中，并且该层的模型置信度也明显更高。这些发现提供了一种定量的观点，可为LLM创造力和幻觉之间的相互作用提供新的见解。我们的实验的代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: (How) Do Language Models Track State?</h3>
<ul>
<li><strong>Authors: </strong>Belinda Z. Li, Zifan Carl Guo, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02854">https://arxiv.org/abs/2503.02854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02854">https://arxiv.org/pdf/2503.02854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02854]] (How) Do Language Models Track State?(https://arxiv.org/abs/2503.02854)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Transformer language models (LMs) exhibit behaviors -- from storytelling to code generation -- that appear to require tracking the unobserved state of an evolving world. How do they do so? We study state tracking in LMs trained or fine-tuned to compose permutations (i.e., to compute the order of a set of objects after a sequence of swaps). Despite the simple algebraic structure of this problem, many other tasks (e.g., simulation of finite automata and evaluation of boolean expressions) can be reduced to permutation composition, making it a natural model for state tracking in general. We show that LMs consistently learn one of two state tracking mechanisms for this task. The first closely resembles the "associative scan" construction used in recent theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second uses an easy-to-compute feature (permutation parity) to partially prune the space of outputs, then refines this with an associative scan. The two mechanisms exhibit markedly different robustness properties, and we show how to steer LMs toward one or the other with intermediate training tasks that encourage or suppress the heuristics. Our results demonstrate that transformer LMs, whether pretrained or fine-tuned, can learn to implement efficient and interpretable state tracking mechanisms, and the emergence of these mechanisms can be predicted and controlled.</li>
<li><strong>摘要：</strong>变压器语言模型（LMS）表现出行为 - 从讲故事到代码生成 - 似乎需要跟踪不断发展的世界的未经观察状态。他们怎么做？我们研究了经过训练或微调的LMS的状态跟踪以组成排序（即，在一系列掉期之后计算一组对象的顺序）。尽管该问题的代数结构简单，但许多其他任务（例如，有限自动机的模拟和布尔表达式评估）可以减少为置换组成，从而使其成为一般状态跟踪的自然模型。我们表明，LM始终为此任务学习两种状态跟踪机制之一。第一个非常类似于Liu等人最近理论工作中使用的“关联扫描”结构。 （2023）和Merrill等。 （2024）。第二种使用易于计算的功能（置换率）部分修剪输出空间，然后通过关联扫描来完善此功能。这两种机制具有明显不同的鲁棒性特性，我们展示了如何使用中间训练任务将LMS引导到一个或另一个，以鼓励或抑制启发式方法。我们的结果表明，变压器LMS（无论是经过审计还是微调）都可以学会实施有效且可解释的状态跟踪机制，并且可以预测和控制这些机制的出现。</li>
</ul>

<h3>Title: Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt Aggregation Framework</h3>
<ul>
<li><strong>Authors: </strong>Ziang Zhou, Tianyuan Jin, Jieming Shi, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02863">https://arxiv.org/abs/2503.02863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02863">https://arxiv.org/pdf/2503.02863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02863]] Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt Aggregation Framework(https://arxiv.org/abs/2503.02863)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often exhibit misaligned confidence scores, usually overestimating the reliability of their predictions. While verbalized confidence in Large Language Models (LLMs) has gained attention, prior work remains divided on whether confidence scores can be systematically steered through prompting. Recent studies even argue that such prompt-induced confidence shifts are negligible, suggesting LLMs' confidence calibration is rigid to linguistic interventions. Contrary to these claims, we first rigorously confirm the existence of directional confidence shifts by probing three models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks, demonstrating that explicit instructions can inflate or deflate confidence scores in a regulated manner. Based on this observation, we propose a novel framework containing three components: confidence steering, steered confidence aggregation and steered answers selection, named SteeringConf. Our method, SteeringConf, leverages a confidence manipulation mechanism to steer the confidence scores of LLMs in several desired directions, followed by a summarization module that aggregates the steered confidence scores to produce a final prediction. We evaluate our method on 7 benchmarks and it consistently outperforms the baselines in terms of calibration metrics in task of confidence calibration and failure detection.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通常表现出未对准的置信度得分，通常高估了其预测的可靠性。尽管对大型语言模型（LLM）的口头信心引起了人们的关注，但先前的工作仍然划分了是否可以通过提示系统地转向信心得分。最近的研究甚至认为，这种迅速引起的置信度转移可以忽略不计，这表明LLMS的置信度校准是对语言干预的严格性。与这些主张相反，我们首先通过探测三个模型（包括gpt3.5，llama3-70b，gpt4）在7个基准上进行了严格确认定向置信度的存在，这表明明确指令可以以受调节的方式膨胀或偏向置信度得分。基于此观察，我们提出了一个新的框架，其中包含三个组成部分：信心转向，转向信心聚集和转向答案选择，名为StoeseringConf。我们的方法，SteeringConf利用置信操纵机制将LLMS的置信度分数转向多个所需的方向，然后是一个汇总模块，该模块汇总了转向的置信得分以产生最终预测。我们在7个基准测试中评估了我们的方法，并且在置信度校准和失败检测任务中，它在校准指标方面始终优于基准。</li>
</ul>

<h3>Title: FairSense-AI: Responsible AI Meets Sustainability</h3>
<ul>
<li><strong>Authors: </strong>Shaina Raza, Mukund Sayeeganesh Chettiar, Matin Yousefabadi, Tahniat Khan, Marcelo Lotif</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02865">https://arxiv.org/abs/2503.02865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02865">https://arxiv.org/pdf/2503.02865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02865]] FairSense-AI: Responsible AI Meets Sustainability(https://arxiv.org/abs/2503.02865)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. this https URL, this https URL</li>
<li><strong>摘要：</strong>在本文中，我们介绍了Fairsense-ai：一个多模式框架，旨在检测和减轻文本和图像的偏见。通过利用大型语言模型（LLM）和视觉语言模型（VLMS），Fairsense-Ai会发现可能出现在内容中的细微形式的偏见或刻板印象，从而为用户提供了偏见的分数，解释性的亮点，并自动提出了公平性增强的建议。此外，FairSense-AI集成了AI风险评估组件，该组件与MIT AI风险存储库和NIST AI风险管理框架等框架保持一致，从而实现了对道德和安全问题的结构化识别。该平台通过模型修剪和混合精确计算等技术进行了优化的能源效率，从而减少了其环境足迹。通过一系列案例研究和应用，我们通过解决公平的社会维度和在大规模AI部署中的可持续性需求来促进负责人的AI使用。此HTTPS URL，此HTTPS URL</li>
</ul>

<h3>Title: The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Ke Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie Wang, Junying Chen, Benyou Wang, Zhaopeng Tu, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02875">https://arxiv.org/abs/2503.02875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02875">https://arxiv.org/pdf/2503.02875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02875]] The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models(https://arxiv.org/abs/2503.02875)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Improving the reasoning capabilities of large language models (LLMs) typically requires supervised fine-tuning with labeled data or computationally expensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which leverages the observation of Prefix Self-Consistency -- the shared initial reasoning steps across diverse solution trajectories -- to enhance LLM reasoning efficiency. By training exclusively on the initial prefix substrings (as few as 8 tokens), UPFT removes the need for labeled data or exhaustive sampling. Experiments on reasoning benchmarks show that UPFT matches the performance of supervised methods such as Rejection Sampling Fine-Tuning, while reducing training time by 75% and sampling cost by 99%. Further analysis reveals that errors tend to appear in later stages of the reasoning process and that prefix-based training preserves the model's structural knowledge. This work demonstrates how minimal unsupervised fine-tuning can unlock substantial reasoning gains in LLMs, offering a scalable and resource-efficient alternative to conventional approaches.</li>
<li><strong>摘要：</strong>提高大语模型（LLM）的推理功能通常需要用标记的数据或计算昂贵的采样进行微调进行微调。我们介绍了无监督的前缀微调（上升），该调查利用前缀自稳态的观察 - 各种解决方案轨迹的共同初始推理步骤 - 以提高LLM推理效率。通过专门培训初始前缀substring（几乎8个令牌），Upft消除了对标记的数据或详尽抽样的需求。关于推理基准测试的实验表明，上升与监督方法的性能，例如拒绝采样进行微调，同时将训练时间降低了75％，并采样成本匹配99％。进一步的分析表明，错误倾向于在推理过程的后期出现，并且基于前缀的培训保留了模型的结构知识。这项工作表明了最小的无监督微调可以在LLM中解锁大量推理的收益，从而提供可扩展且有效的传统方法的替代品。</li>
</ul>

<h3>Title: Wikipedia in the Era of LLMs: Evolution and Risks</h3>
<ul>
<li><strong>Authors: </strong>Siming Huang, Yuliang Xu, Mingmeng Geng, Yao Wan, Dongping Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02879">https://arxiv.org/abs/2503.02879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02879">https://arxiv.org/pdf/2503.02879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02879]] Wikipedia in the Era of LLMs: Evolution and Risks(https://arxiv.org/abs/2503.02879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrieval-augmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedia's language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks.</li>
<li><strong>摘要：</strong>在本文中，我们对大语言模型（LLM）对Wikipedia的影响进行了详尽的分析，通过现有数据检查Wikipedia的演变，并使用模拟探索潜在的风险。我们首先分析页面视图和文章内容，以研究Wikipedia最近的变化并评估LLM的影响。随后，我们评估了LLM如何影响与Wikipedia相关的各种自然语言处理（NLP）任务，包括机器翻译和检索效果发电（RAG）。我们的发现和仿真结果表明，Wikipedia文章受LLM的影响，在某些类别中的影响约为1％-2％。如果基于Wikipedia的机器翻译基准受LLM的影响，则模型的得分可能会膨胀，并且模型之间的比较结果也可能会发生变化。此外，如果知识库被LLM生成的内容污染，则破布的有效性可能会降低。尽管LLM尚未完全改变Wikipedia的语言和知识结构，但我们认为我们的经验发现表明需要仔细考虑潜在的未来风险。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
