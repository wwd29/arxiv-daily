<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-24</h1>
<h3>Title: Memorization in Self-Supervised Learning Improves Downstream  Generalization</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wang, Muhammad Ahmad Kaleem, Adam Dziedzic, Michael Backes, Nicolas Papernot, Franziska Boenisch</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12233">https://arxiv.org/abs/2401.12233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12233">https://arxiv.org/pdf/2401.12233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12233]] Memorization in Self-Supervised Learning Improves Downstream  Generalization(https://arxiv.org/abs/2401.12233)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regularization techniques that reduce overfitting-still significant fractions of training data points experience high memorization. Through our empirical results, we show that this memorization is essential for encoders to achieve higher generalization performance on different downstream tasks.</li>
<li><strong>摘要：</strong>自监督学习（SSL）最近受到了极大的关注，因为它能够纯粹根据通常从互联网上抓取的未标记数据来训练高性能编码器。这些数据仍然可能是敏感的，经验证据表明 SSL 编码器会记住其训练数据的私人信息，并可以在推理时披露它们。由于现有的监督学习记忆理论定义依赖于标签，因此它们不会转移到 SSL。为了解决这个问题，我们提出了 SSLMem，一个在 SSL 中定义记忆的框架。我们的定义比较了数据点表示的对齐方式及其由在这些数据点上训练的编码器和未在这些数据点上训练的编码器返回的增强视图的差异。通过对不同编码器架构和数据集的全面实证分析，我们强调，尽管 SSL 依赖于大型数据集和强大的增强（这两种技术在监督学习中都被称为减少过度拟合的正则化技术），但仍有相当一部分训练数据点经历了高记忆。通过我们的实证结果，我们表明这种记忆对于编码器在不同下游任务上实现更高的泛化性能至关重要。</li>
</ul>

<h3>Title: Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot  Adaption via Contextual Meta Graph Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Bairong Deng, Tao Yu, Zhenning Pan, Xuehan Zhang, Yufeng Wu, Qiaoyi Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12235">https://arxiv.org/abs/2401.12235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12235">https://arxiv.org/pdf/2401.12235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12235]] Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot  Adaption via Contextual Meta Graph Reinforcement Learning(https://arxiv.org/abs/2401.12235)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Reinforcement learning is an emerging approaches to facilitate multi-stage sequential decision-making problems. This paper studies a real-time multi-stage stochastic power dispatch considering multivariate uncertainties. Current researches suffer from low generalization and practicality, that is, the learned dispatch policy can only handle a specific dispatch scenario, its performance degrades significantly if actual samples and training samples are inconsistent. To fill these gaps, a novel contextual meta graph reinforcement learning (Meta-GRL) for a highly generalized multi-stage optimal dispatch policy is proposed. Specifically, a more general contextual Markov decision process (MDP) and scalable graph representation are introduced to achieve a more generalized multi-stage stochastic power dispatch modeling. An upper meta-learner is proposed to encode context for different dispatch scenarios and learn how to achieve dispatch task identification while the lower policy learner learns context-specified dispatch policy. After sufficient offline learning, this approach can rapidly adapt to unseen and undefined scenarios with only a few updations of the hypothesis judgments generated by the meta-learner. Numerical comparisons with state-of-the-art policies and traditional reinforcement learning verify the optimality, efficiency, adaptability, and scalability of the proposed Meta-GRL.</li>
<li><strong>摘要：</strong>强化学习是一种促进多阶段顺序决策问题的新兴方法。本文研究了考虑多元不确定性的实时多级随机功率调度。目前的研究普遍性和实用性较低，即学习到的调度策略只能处理特定的调度场景，如果实际样本和训练样本不一致，其性能会显着下降。为了填补这些空白，提出了一种用于高度广义的多阶段最优调度策略的新颖的上下文元图强化学习（Meta-GRL）。具体来说，引入更通用的上下文马尔可夫决策过程（MDP）和可扩展图表示来实现更通用的多级随机功率调度建模。提出了上层元学习器对不同调度场景的上下文进行编码，并学习如何实现调度任务识别，而下层策略学习器则学习上下文指定的调度策略。经过充分的离线学习后，这种方法只需对元学习器生成的假设判断进行少量更新，就可以快速适应未见过和未定义的场景。与最先进的策略和传统强化学习的数值比较验证了所提出的 Meta-GRL 的最优性、效率、适应性和可扩展性。</li>
</ul>

<h3>Title: Orion-14B: Open-source Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, Kun Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12246">https://arxiv.org/abs/2401.12246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12246">https://arxiv.org/pdf/2401.12246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12246]] Orion-14B: Open-source Multilingual Large Language Models(https://arxiv.org/abs/2401.12246)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.</li>
<li><strong>摘要：</strong>在这项研究中，我们介绍了 Orion-14B，这是一个具有 140 亿个参数的多语言大型语言模型的集合。我们利用数据调度方法在包含 2.5 万亿个标记的多样化语料库上训练基础模型，这些标记来自英语、中文、日语、韩语和其他语言的文本。此外，我们还微调了一系列专为对话应用程序和其他特定用例定制的模型。我们的评估结果表明 Orion-14B 在广泛的任务中实现了最先进的性能。我们将 Orion-14B 模型系列及其相关代码公开访问 https://github.com/OrionStarAI/Orion，旨在激发该领域未来的研究和实际应用。</li>
</ul>

<h3>Title: Exploring consumers response to text-based chatbots in e-commerce: The  moderating role of task complexity and chatbot disclosure</h3>
<ul>
<li><strong>Authors: </strong>Xusen Cheng, Ying Bao, Alex Zarifis, Wankun Gong, Jian Mou</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12247">https://arxiv.org/abs/2401.12247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12247">https://arxiv.org/pdf/2401.12247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12247]] Exploring consumers response to text-based chatbots in e-commerce: The  moderating role of task complexity and chatbot disclosure(https://arxiv.org/abs/2401.12247)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Artificial intelligence based chatbots have brought unprecedented business potential. This study aims to explore consumers trust and response to a text-based chatbot in ecommerce, involving the moderating effects of task complexity and chatbot identity disclosure. A survey method with 299 useable responses was conducted in this research. This study adopted the ordinary least squares regression to test the hypotheses. First, the consumers perception of both the empathy and friendliness of the chatbot positively impacts their trust in it. Second, task complexity negatively moderates the relationship between friendliness and consumers trust. Third, disclosure of the text based chatbot negatively moderates the relationship between empathy and consumers trust, while it positively moderates the relationship between friendliness and consumers trust. Fourth, consumers trust in the chatbot increases their reliance on the chatbot and decreases their resistance to the chatbot in future interactions. Adopting the stimulus organism response framework, this study provides important insights on consumers perception and response to the text-based chatbot. The findings of this research also make suggestions that can increase consumers positive responses to text based chatbots. Extant studies have investigated the effects of automated bots attributes on consumers perceptions. However, the boundary conditions of these effects are largely ignored. This research is one of the first attempts to provide a deep understanding of consumers responses to a chatbot.</li>
<li><strong>摘要：</strong>基于人工智能的聊天机器人带来了前所未有的商业潜力。本研究旨在探讨消费者对电子商务中基于文本的聊天机器人的信任和反应，涉及任务复杂性和聊天机器人身份披露的调节作用。本研究采用了包含 299 个可用答案的调查方法。本研究采用普通最小二乘回归来检验假设。首先，消费者对聊天机器人的同理心和友好性的看法会积极影响他们对聊天机器人的信任。其次，任务复杂性对友善度和消费者信任之间的关系产生负面影响。第三，基于文本的聊天机器人的披露负面调节了同理心和消费者信任之间的关系，而正面调节了友善度和消费者信任之间的关系。第四，消费者对聊天机器人的信任增加了他们对聊天机器人的依赖，减少了他们在未来互动中对聊天机器人的抵制。这项研究采用刺激有机体反应框架，为消费者对基于文本的聊天机器人的感知和反应提供了重要的见解。这项研究的结果还提出了一些建议，可以增加消费者对基于文本的聊天机器人的积极反应。现有研究调查了自动化机器人属性对消费者认知的影响。然而，这些效应的边界条件在很大程度上被忽略了。这项研究是深入了解消费者对聊天机器人反应的首次尝试之一。</li>
</ul>

<h3>Title: GRATH: Gradual Self-Truthifying for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weixin Chen, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12292">https://arxiv.org/abs/2401.12292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12292">https://arxiv.org/pdf/2401.12292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12292]] GRATH: Gradual Self-Truthifying for Large Language Models(https://arxiv.org/abs/2401.12292)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the truthfulness data and optimizes the model, leading to a gradual improvement in model truthfulness. Empirically, we evaluate GRATH using different 7B-LLMs and compare with LLMs with similar or even larger sizes on benchmark datasets. Our results show that GRATH effectively improves LLMs' truthfulness without compromising other core capabilities. Notably, GRATH achieves state-of-the-art performance on TruthfulQA, with MC1 accuracy as 54.71% and MC2 accuracy as 69.10%, which even surpass those on larger-scale models, such as Llama2-Chat-70B, by 23.62% and 24.18%, respectively.</li>
<li><strong>摘要：</strong>对于大型语言模型 (LLM) 来说，真实性至关重要，因为它们越来越多地部署在现实世界的应用程序中。然而，现有的法学硕士仍然难以生成真实的答案和内容，他们在 TruthfulQA 等基准测试中的表现不佳就证明了这一点。为了解决这个问题，我们提出了 GRAdual self-truTHifying (GRATH)，这是一种新颖的后处理方法，用于增强法学硕士的真实性。 GRATH 利用域外问题提示生成相应答案，并通过直接偏好优化 (DPO) 自适应优化模型。请注意，在此过程中，GRATH 以自我监督的方式学习真实性，而不需要带注释的答案。特别是，GRATH 首先通过提示 LLM 本身生成成对的真实性训练数据，每对包含一个问题及其正确和错误答案。然后使用 DPO 对该模型进行微调，以了解答案对之间的差异。随后，GRATH迭代细化真实性数据并优化模型，导致模型真实性逐渐提高。根据经验，我们使用不同的 7B-LLM 评估 GRATH，并与基准数据集上具有相似甚至更大大小的 LLM 进行比较。我们的结果表明，GRATH 有效提高了法学硕士的真实性，同时又不影响其他核心能力。值得注意的是，GRATH 在 TruthfulQA 上实现了最先进的性能，MC1 准确度为 54.71%，MC2 准确度为 69.10%，甚至超过了 Llama2-Chat-70B 等更大规模模型的 23.62%，分别为24.18%。</li>
</ul>

<h3>Title: Cheap Learning: Maximising Performance of Language Models for Social  Data Science Using Minimal Data</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Castro-Gonzalez, Yi-Ling Chung, Hannak Rose Kirk, John Francis, Angus R. Williams, Pica Johansson, Jonathan Bright</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12295">https://arxiv.org/abs/2401.12295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12295">https://arxiv.org/pdf/2401.12295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12295]] Cheap Learning: Maximising Performance of Language Models for Social  Data Science Using Minimal Data(https://arxiv.org/abs/2401.12295)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code</a></li>
<li><strong>Abstract: </strong>The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very low cost. Our results are accompanied by a code repository to make it easy for others to duplicate our work and use it in their own research. Overall, our article is intended to stimulate further uptake of these techniques in the social sciences.</li>
<li><strong>摘要：</strong>机器学习领域最近在减少构建新模型时对标记训练数据的要求方面取得了重大进展。这些“更便宜”的学习技术在社会科学领域具有巨大的潜力，其中大型标记训练数据集的开发通常是使用机器学习进行分析任务的重大实际障碍。在本文中，我们回顾了近年来发展起来的三种“廉价”技术：弱监督、迁移学习和即时工程。对于后者，我们还回顾了大型语言模型的零样本提示的特殊情况。对于每种技术，我们提供了其工作原理的指南，并演示了其在六种不同的现实社会科学应用程序中的应用（两种不同的任务与三种不同的数据集组成配对）。我们展示了所有技术的良好性能，特别是我们展示了大型语言模型的提示如何以非常低的成本实现高精度。我们的结果附有代码存储库，以便其他人可以轻松复制我们的工作并将其用于自己的研究。总的来说，我们的文章旨在促进社会科学进一步采用这些技术。</li>
</ul>

<h3>Title: Fine-tuning Large Language Models for Multigenerator, Multidomain, and  Multilingual Machine-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Feng Xiong, Thanet Markchom, Ziwei Zheng, Subin Jung, Varun Ojha, Huizhi Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12326">https://arxiv.org/abs/2401.12326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12326">https://arxiv.org/pdf/2401.12326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12326]] Fine-tuning Large Language Models for Multigenerator, Multidomain, and  Multilingual Machine-Generated Text Detection(https://arxiv.org/abs/2401.12326)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>SemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A & B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.</li>
<li><strong>摘要：</strong>SemEval-2024 任务 8 引入了从各种语言和领域的各种大型语言模型 (LLM) 中识别机器生成的文本的挑战。该任务包含三个子任务：单语和多语的二元分类（子任务 A）、多类分类（子任务 B）和混合文本检测（子任务 C）。本文重点关注子任务 A 和 B。每个子任务由用于训练、开发和测试的三个数据集支持。为了解决这个任务，有两种方法：1) 使用传统的机器学习 (ML) 和自然语言预处理 (NLP) 进行特征提取，2) 微调 LLM 进行文本分类。结果表明，Transformer 模型（尤其是 LoRA-RoBERTa）在有效性上超过了传统的 ML 方法，多数投票在多语言环境中对于识别机器生成的文本特别有效。</li>
</ul>

<h3>Title: Subgraph Extraction-based Feedback-guided Iterative Scheduling for HLS</h3>
<ul>
<li><strong>Authors: </strong>Hanchen Ye, David Z. Pan, Chris Leary, Deming Chen, Xiaoqing Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12343">https://arxiv.org/abs/2401.12343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12343">https://arxiv.org/pdf/2401.12343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12343]] Subgraph Extraction-based Feedback-guided Iterative Scheduling for HLS(https://arxiv.org/abs/2401.12343)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper proposes ISDC, a novel feedback-guided iterative system of difference constraints (SDC) scheduling algorithm for high-level synthesis (HLS). ISDC leverages subgraph extraction-based low-level feedback from downstream tools like logic synthesizers to iteratively refine HLS scheduling. Technical innovations include: (1) An enhanced SDC formulation that effectively integrates low-level feedback into the linear-programming (LP) problem; (2) A fanout and window-based subgraph extraction mechanism driving the feedback cycle; (3) A no-human-in-loop ISDC flow compatible with a wide range of downstream tools and process design kits (PDKs). Evaluation shows that ISDC reduces register usage by 28.5% against an industrial-strength open-source HLS tool.</li>
<li><strong>摘要：</strong>本文提出了 ISDC，一种用于高级综合（HLS）的新型反馈引导差分约束迭代系统（SDC）调度算法。 ISDC 利用来自逻辑合成器等下游工具的基于子图提取的低级反馈来迭代地完善 HLS 调度。技术创新包括：（1）增强的SDC公式，有效地将低级反馈集成到线性规划（LP）问题中； (2) 驱动反馈循环的扇出和基于窗口的子图提取机制； (3) 无人参与的 ISDC 流程，与各种下游工具和工艺设计套件 (PDK) 兼容。评估表明，与工业级开源 HLS 工具相比，ISDC 将寄存器使用量减少了 28.5%。</li>
</ul>

<h3>Title: Efficient Collaborations through Weight-Driven Coalition Dynamics in  Federated Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Mohammed El Hanjri, Hamza Reguieg, Adil Attiaoui, Amine Abouaomar, Abdellatif Kobbane, Mohamed El Kamili</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12356">https://arxiv.org/abs/2401.12356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12356">https://arxiv.org/pdf/2401.12356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12356]] Efficient Collaborations through Weight-Driven Coalition Dynamics in  Federated Learning Systems(https://arxiv.org/abs/2401.12356)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In the era of the Internet of Things (IoT), decentralized paradigms for machine learning are gaining prominence. In this paper, we introduce a federated learning model that capitalizes on the Euclidean distance between device model weights to assess their similarity and disparity. This is foundational for our system, directing the formation of coalitions among devices based on the closeness of their model weights. Furthermore, the concept of a barycenter, representing the average of model weights, helps in the aggregation of updates from multiple devices. We evaluate our approach using homogeneous and heterogeneous data distribution, comparing it against traditional federated learning averaging algorithm. Numerical results demonstrate its potential in offering structured, outperformed and communication-efficient model for IoT-based machine learning.</li>
<li><strong>摘要：</strong>在物联网 (IoT) 时代，机器学习的去中心化范式日益受到重视。在本文中，我们介绍了一种联邦学习模型，该模型利用设备模型权重之间的欧几里得距离来评估它们的相似性和差异。这是我们系统的基础，根据模型权重的接近程度指导设备之间联盟的形成。此外，代表模型权重平均值的重心概念有助于聚合来自多个设备的更新。我们使用同质和异构数据分布来评估我们的方法，并将其与传统的联邦学习平均算法进行比较。数值结果证明了其为基于物联网的机器学习提供结构化、性能优越且通信高效的模型的潜力。</li>
</ul>

<h3>Title: Analyzing the Effectiveness of Large Language Models on Text-to-SQL  Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Richard Roberson, Gowtham Kaki, Ashutosh Trivedi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.DB, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12379">https://arxiv.org/abs/2401.12379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12379">https://arxiv.org/pdf/2401.12379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12379]] Analyzing the Effectiveness of Large Language Models on Text-to-SQL  Synthesis(https://arxiv.org/abs/2401.12379)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, lora, code</a></li>
<li><strong>Abstract: </strong>This study investigates various approaches to using Large Language Models (LLMs) for Text-to-SQL program synthesis, focusing on the outcomes and insights derived. Employing the popular Text-to-SQL dataset, spider, the goal was to input a natural language question along with the database schema and output the correct SQL SELECT query. The initial approach was to fine-tune a local and open-source model to generate the SELECT query. After QLoRa fine-tuning WizardLM's WizardCoder-15B model on the spider dataset, the execution accuracy for generated queries rose to a high of 61%. With the second approach, using the fine-tuned gpt-3.5-turbo-16k (Few-shot) + gpt-4-turbo (Zero-shot error correction), the execution accuracy reached a high of 82.1%. Of all the incorrect queries, most can be categorized into a seven different categories of what went wrong: selecting the wrong columns or wrong order of columns, grouping by the wrong column, predicting the wrong values in conditionals, using different aggregates than the ground truth, extra or too few JOIN clauses, inconsistencies in the Spider dataset, and lastly completely incorrect query structure. Most if not all of the queries fall into these categories and it is insightful to understanding where the faults still lie with LLM program synthesis and where they can be improved.</li>
<li><strong>摘要：</strong>本研究调查了使用大型语言模型 (LLM) 进行文本到 SQL 程序合成的各种方法，重点关注所得出的结果和见解。使用流行的文本到 SQL 数据集 Spider，目标是输入自然语言问题以及数据库模式并输出正确的 SQL SELECT 查询。最初的方法是微调本地开源模型来生成 SELECT 查询。 QLoRa 在蜘蛛数据集上对 WizardLM 的 WizardCoder-15B 模型进行微调后，生成的查询的执行准确度升至 61%。第二种方法，使用微调的 gpt-3.5-turbo-16k (Few-shot) + gpt-4-turbo (Zero-shot error Correction)，执行精度达到了 82.1% 的高值。在所有错误的查询中，大多数可以分为七种不同的错误类别：选择错误的列或错误的列顺序、按错误的列分组、预测条件中的错误值、使用与基本事实不同的聚合、额外或太少的 JOIN 子句、Spider 数据集中的不一致以及最后完全错误的查询结构。大多数（如果不是全部）查询都属于这些类别，了解 LLM 程序综合中仍然存在的错误以及可以改进的地方是很有洞察力的。</li>
</ul>

<h3>Title: Enhancing In-context Learning via Linear Probe Calibration</h3>
<ul>
<li><strong>Authors: </strong>Momin Abbas, Yi Zhou, Parikshit Ram, Nathalie Baracaldo, Horst Samulowitz, Theodoros Salonidis, Tianyi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12406">https://arxiv.org/abs/2401.12406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12406">https://arxiv.org/pdf/2401.12406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12406]] Enhancing In-context Learning via Linear Probe Calibration(https://arxiv.org/abs/2401.12406)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improvement of up to 21%, and up to a 50% improvement in some cases, and significantly boosts the performance of PEFT methods, especially in the low resource regime. Moreover, LinC achieves lower expected calibration error, and is highly robust to varying label proportions, prompt templates, and demonstration permutations. Our code is available at \url{https://github.com/mominabbass/LinC}.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 是一种新的自然语言处理范式，它利用类似生成预训练 Transformer (GPT) 的模型。此方法使用包含上下文演示的提示来为新查询输入生成相应的输出。然而，在实际案例中应用 ICL 并不会随着样本数量的增加而扩展，并且对不同的提示模板和演示排列缺乏鲁棒性。在本文中，我们首先证明，基于香农熵的新度量，使用 ICL 的类 GPT 模型会导致不可靠的预测。然后，为了解决这个问题，我们提出了一种称为线性探针校准（LinC）的新技术，该方法可以校准模型的输出概率，从而获得可靠的预测并提高性能，同时只需要最少的额外样本（少至五个标记）数据样本）。 LinC 显着增强了 GPT 模型在各种基准数据集上的 ICL 测试性能，平均提升高达 21%，某些情况下提升高达 50%，并显着提升了 PEFT 方法的性能，尤其是在资源匮乏的情况下政权。此外，LinC 实现了较低的预期校准误差，并且对于不同的标签比例、提示模板和演示排列具有高度鲁棒性。我们的代码可在 \url{https://github.com/mominabbass/LinC} 获取。</li>
</ul>

<h3>Title: How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual  Translation via Tiny Multi-Parallel Data</h3>
<ul>
<li><strong>Authors: </strong>Di Wu, Shaomu Tan, Yan Meng, David Stap, Christof Monz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12413">https://arxiv.org/abs/2401.12413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12413">https://arxiv.org/pdf/2401.12413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12413]] How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual  Translation via Tiny Multi-Parallel Data(https://arxiv.org/abs/2401.12413)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close to the upper bound (complete translation). Due to its high efficiency and practicality, we encourage the community 1) to consider the use of the fine-tuning method as a strong baseline for zero-shot translation and 2) to construct more comprehensive and high-quality multi-parallel data to cover real-world demand.</li>
<li><strong>摘要：</strong>零样本翻译是一个悬而未决的问题，旨在在多语言机器翻译（MMT）训练期间未见过的语言对之间进行翻译。一种常见的解决方案（尽管很消耗资源）是挖掘尽可能多的翻译方向以添加到平行语料库中。在本文中，我们表明，通过使用极少量的多并行数据进行微调，可以轻松增强以英语为中心的模型的零样本能力。例如，在 EC30 数据集上，我们表明仅使用 100 个多并行样本即可实现高达 +21.7 ChrF 非英语整体改进（870 个方向），同时保留以英语为中心的方向的能力。我们进一步研究微调数据的尺寸效应及其传输能力。令人惊讶的是，我们的实证分析表明，即使通过在小的随机采样方向集 (10%) 中进行微调，也可以实现类似的整体改进。此外，最终的非英语表现非常接近上限（完整翻译）。由于其高效性和实用性，我们鼓励社区1）考虑使用微调方法作为零镜头翻译的强大基线，2）构建更全面和高质量的多并行数据来覆盖现实世界的需求。</li>
</ul>

<h3>Title: Wasserstein Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Chengyi Yang, Jiayin Qi, Aimin Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12436">https://arxiv.org/abs/2401.12436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12436">https://arxiv.org/pdf/2401.12436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12436]] Wasserstein Differential Privacy(https://arxiv.org/abs/2401.12436)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) has achieved remarkable results in the field of privacy-preserving machine learning. However, existing DP frameworks do not satisfy all the conditions for becoming metrics, which prevents them from deriving better basic private properties and leads to exaggerated values on privacy budgets. We propose Wasserstein differential privacy (WDP), an alternative DP framework to measure the risk of privacy leakage, which satisfies the properties of symmetry and triangle inequality. We show and prove that WDP has 13 excellent properties, which can be theoretical supports for the better performance of WDP than other DP frameworks. In addition, we derive a general privacy accounting method called Wasserstein accountant, which enables WDP to be applied in stochastic gradient descent (SGD) scenarios containing sub-sampling. Experiments on basic mechanisms, compositions and deep learning show that the privacy budgets obtained by Wasserstein accountant are relatively stable and less influenced by order. Moreover, the overestimation on privacy budgets can be effectively alleviated. The code is available at https://github.com/Hifipsysta/WDP.</li>
<li><strong>摘要：</strong>差分隐私（DP）在隐私保护机器学习领域取得了显着的成果。然而，现有的DP框架并不能满足成为指标的所有条件，这使得它们无法推导出更好的基本私有属性，并导致隐私预算的价值被夸大。我们提出了 Wasserstein 差分隐私（WDP），这是一种衡量隐私泄露风险的替代 DP 框架，它满足对称性和三角不等式的性质。我们展示并证明了WDP具有13个优异的特性，这可以为WDP比其他DP框架更好的性能提供理论支持。此外，我们推导了一种称为 Wasserstein accountant 的通用隐私会计方法，该方法使 WDP 能够应用于包含子采样的随机梯度下降（SGD）场景。基本机制、组成和深度学习的实验表明，Wasserstein会计师得到的隐私预算相对稳定，受顺序影响较小。而且，可以有效缓解隐私预算的高估问题。该代码可在 https://github.com/Hifipsysta/WDP 上获取。</li>
</ul>

<h3>Title: Towards Socially and Morally Aware RL agent: Reward Design With LLM</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyue Wang</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12459">https://arxiv.org/abs/2401.12459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12459">https://arxiv.org/pdf/2401.12459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12459]] Towards Socially and Morally Aware RL agent: Reward Design With LLM(https://arxiv.org/abs/2401.12459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, rag, agent</a></li>
<li><strong>Abstract: </strong>When we design and deploy an Reinforcement Learning (RL) agent, reward functions motivates agents to achieve an objective. An incorrect or incomplete specification of the objective can result in behavior that does not align with human values - failing to adhere with social and moral norms that are ambiguous and context dependent, and cause undesired outcomes such as negative side effects and exploration that is unsafe. Previous work have manually defined reward functions to avoid negative side effects, use human oversight for safe exploration, or use foundation models as planning tools. This work studies the ability of leveraging Large Language Models (LLM)' understanding of morality and social norms on safe exploration augmented RL methods. This work evaluates language model's result against human feedbacks and demonstrates language model's capability as direct reward signals.</li>
<li><strong>摘要：</strong>当我们设计和部署强化学习（RL）代理时，奖励函数会激励代理实现目标。目标的不正确或不完整的说明可能会导致与人类价值观不符的行为——无法遵守模糊且依赖于环境的社会和道德规范，并导致不良后果，例如负面副作用和不安全的探索。以前的工作手动定义奖励函数以避免负面影响，使用人类监督进行安全探索，或使用基础模型作为规划工具。这项工作研究了利用大型语言模型 (LLM) 对安全探索增强强化学习方法的道德和社会规范的理解的能力。这项工作根据人类反馈评估语言模型的结果，并证明语言模型作为直接奖励信号的能力。</li>
</ul>

<h3>Title: Contrastive Learning in Distilled Models</h3>
<ul>
<li><strong>Authors: </strong>Valerie Lim, Kai Wen Ng, Kenneth Lim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12472">https://arxiv.org/abs/2401.12472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12472">https://arxiv.org/pdf/2401.12472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12472]] Contrastive Learning in Distilled Models(https://arxiv.org/abs/2401.12472)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Natural Language Processing models like BERT can provide state-of-the-art word embeddings for downstream NLP tasks. However, these models yet to perform well on Semantic Textual Similarity, and may be too large to be deployed as lightweight edge applications. We seek to apply a suitable contrastive learning method based on the SimCSE paper, to a model architecture adapted from a knowledge distillation based model, DistilBERT, to address these two issues. Our final lightweight model DistilFace achieves an average of 72.1 in Spearman's correlation on STS tasks, a 34.2 percent improvement over BERT base.</li>
<li><strong>摘要：</strong>BERT 等自然语言处理模型可以为下游 NLP 任务提供最先进的词嵌入。然而，这些模型在语义文本相似性方面尚未表现良好，并且可能太大而无法部署为轻量级边缘应用程序。我们寻求将基于 SimCSE 论文的合适对比学习方法应用于基于知识蒸馏的模型 DistilBERT 改编的模型架构，以解决这两个问题。我们最终的轻量级模型 DistilFace 在 STS 任务上的 Spearman 相关性平均达到 72.1，比 BERT 基础提高了 34.2%。</li>
</ul>

<h3>Title: Large Language Models are Superpositions of All Characters: Attaining  Arbitrary Role-play via Self-Alignment</h3>
<ul>
<li><strong>Authors: </strong>Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12474">https://arxiv.org/abs/2401.12474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12474">https://arxiv.org/pdf/2401.12474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12474]] Large Language Models are Superpositions of All Characters: Attaining  Arbitrary Role-play via Self-Alignment(https://arxiv.org/abs/2401.12474)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in various parameter scales, consistently maintains a consistent role identity and provides accurate role-specific knowledge in multi-turn role-play conversations. Notably, it outperforms all open-source role-play baselines, showcasing performance levels comparable to advanced proprietary chatbots. Furthermore, we present the first comprehensive cross-supervision alignment experiment in the role-play domain, revealing that the intrinsic capabilities of LLMs confine the knowledge within role-play. Meanwhile, the role-play styles can be easily acquired with the guidance of smaller models. We open-source related resources at https://github.com/OFA-Sys/Ditto.</li>
<li><strong>摘要：</strong>通过模拟专有模型，人们投入了大量的精力来提高开源大语言模型（LLM）的角色扮演能力。尽管如此，我们认为法学硕士本质上具有角色扮演能力，因为其庞大的训练语料库中根深蒂固地具有广泛的角色知识和潜在对话。因此，在本研究中，我们引入了 Ditto，一种角色扮演的自我调整方法。 Ditto 利用角色知识，鼓励遵循指令的法学硕士模拟角色扮演对话，作为阅读理解的一种变体。该方法创建了一个包含 4,000 个角色的角色扮演训练集，就角色数量而言，超出了当前可用数据集规模的十倍。随后，我们使用这个自行生成的数据集对法学硕士进行微调，以增强其角色扮演能力。在评估我们精心构建的、可重现的角色扮演基准和 MT-Bench 的角色扮演子集后，Ditto 在各种参数范围内始终保持一致的角色身份，并在多轮角色扮演对话中提供准确的特定于角色的知识。值得注意的是，它的性能优于所有开源角色扮演基准，表现出与高级专有聊天机器人相当的性能水平。此外，我们在角色扮演领域提出了第一个全面的交叉监督对齐实验，揭示了法学硕士的内在能力将知识限制在角色扮演中。同时，角色扮演风格可以在较小模型的指导下轻松习得。我们在 https://github.com/OFA-Sys/Ditto 开源了相关资源。</li>
</ul>

<h3>Title: Assessing and Understanding Creativity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yunpu Zhao, Rui Zhang, Wenyi Li, Di Huang, Jiaming Guo, Shaohui Peng, Yifan Hao, Yuanbo Wen, Xing Hu, Zidong Du, Qi Guo, Ling Li, Yunji Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12491">https://arxiv.org/abs/2401.12491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12491">https://arxiv.org/pdf/2401.12491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12491]] Assessing and Understanding Creativity in Large Language Models(https://arxiv.org/abs/2401.12491)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In the field of natural language processing, the rapid development of large language model (LLM) has attracted more and more attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. The assessment of LLM creativity needs to consider differences from humans, requiring multi-dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance Tests of Creative Thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including Fluency, Flexibility, Originality, and Elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs' responses to diverse prompts and role-play situations. We found that the creativity of LLMs primarily falls short in originality, while excelling in elaboration. Besides, the use of prompts and the role-play settings of the model significantly influence creativity. Additionally, the experimental results also indicate that collaboration among multiple LLMs can enhance originality. Notably, our findings reveal a consensus between human evaluations and LLMs regarding the personality traits that influence creativity. The findings underscore the significant impact of LLM design on creativity and bridges artificial intelligence and human creativity, offering insights into LLMs' creativity and potential applications.</li>
<li><strong>摘要：</strong>在自然语言处理领域，大语言模型（LLM）的快速发展引起了越来越多的关注。法学硕士在各种任务中表现出了高水平的创造力，但评估这种创造力的方法还不够充分。 LLM创造力的评估需要考虑与人类的差异，需要多维度衡量，同时平衡准确性和效率。本文旨在建立一个有效的框架来评估法学硕士的创造力水平。通过采用修改后的托伦斯创造性思维测试，该研究评估了不同法学硕士在 7 项任务中的创造性表现，强调了 4 个标准，包括流畅性、灵活性、原创性和精细化。在此背景下，我们开发了包含 700 个测试问题的综合数据集和基于法学硕士的评估方法。此外，本研究还对法学硕士对不同提示和角色扮演情况的反应进行了新颖的分析。我们发现法学硕士的创造力主要不足于独创性，而擅长于精细化。此外，提示的使用和模型的角色扮演设置也会显着影响创造力。此外，实验结果还表明，多个法学硕士之间的合作可以增强原创性。值得注意的是，我们的研究结果揭示了人类评估和法学硕士在影响创造力的人格特质方面达成了共识。研究结果强调了法学硕士设计对创造力的重大影响，并将人工智能和人类创造力联系起来，为法学硕士的创造力和潜在应用提供了见解。</li>
</ul>

<h3>Title: Comparing Human-Centered Language Modeling: Is it Better to Model  Groups, Individual Traits, or Both?</h3>
<ul>
<li><strong>Authors: </strong>Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz, Dirk Hovy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12492">https://arxiv.org/abs/2401.12492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12492">https://arxiv.org/pdf/2401.12492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12492]] Comparing Human-Centered Language Modeling: Is it Better to Model  Groups, Individual Traits, or Both?(https://arxiv.org/abs/2401.12492)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does well even without user's historical data.</li>
<li><strong>摘要：</strong>自然语言处理在将人类背景融入其模型方面取得了进展，但使用群体属性（例如 45 岁以上）或模型个体是否更有效仍然存在疑问。群组属性在技术上更简单，但比较粗糙：并非所有 45 岁的人都以相同的方式写作。相比之下，对个体进行建模可以捕捉每个人身份的复杂性。它允许更个性化的表示，但我们可能必须对无限数量的用户进行建模，并且需要可能无法获得的数据。我们比较通过群体属性、个人用户和组合方法对人类环境进行建模。结合群体和个人特征可以显着有利于用户级回归任务，例如根据用户文档进行年龄估计或个性评估。对单个用户进行建模可以显着提高单个文档级分类任务（例如立场和主题检测）的性能。我们还发现，即使没有用户的历史数据，个人用户建模也能表现良好。</li>
</ul>

<h3>Title: Building Minimal and Reusable Causal State Abstractions for  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zizhao Wang, Caroline Wang, Xuesu Xiao, Yuke Zhu, Peter Stone</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12497">https://arxiv.org/abs/2401.12497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12497">https://arxiv.org/pdf/2401.12497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12497]] Building Minimal and Reusable Causal State Abstractions for  Reinforcement Learning(https://arxiv.org/abs/2401.12497)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Two desiderata of reinforcement learning (RL) algorithms are the ability to learn from relatively little experience and the ability to learn policies that generalize to a range of problem specifications. In factored state spaces, one approach towards achieving both goals is to learn state abstractions, which only keep the necessary variables for learning the tasks at hand. This paper introduces Causal Bisimulation Modeling (CBM), a method that learns the causal relationships in the dynamics and reward functions for each task to derive a minimal, task-specific abstraction. CBM leverages and improves implicit modeling to train a high-fidelity causal dynamics model that can be reused for all tasks in the same environment. Empirical validation on manipulation environments and Deepmind Control Suite reveals that CBM's learned implicit dynamics models identify the underlying causal relationships and state abstractions more accurately than explicit ones. Furthermore, the derived state abstractions allow a task learner to achieve near-oracle levels of sample efficiency and outperform baselines on all tasks.</li>
<li><strong>摘要：</strong>强化学习 (RL) 算法的两个必要条件是能够从相对较少的经验中学习，以及能够学习泛化到一系列问题规范的策略。在因子状态空间中，实现这两个目标的一种方法是学习状态抽象，它只保留学习手头任务所需的变量。本文介绍了因果双模拟建模（CBM），这种方法可以学习每个任务的动态和奖励函数中的因果关系，从而得出最小的、特定于任务的抽象。 CBM 利用并改进隐式建模来训练高保真因果动力学模型，该模型可重复用于同一环境中的所有任务。对操纵环境和 Deepmind Control Suite 的实证验证表明，CBM 学习的隐式动力学模型比显式模型更准确地识别潜在的因果关系和状态抽象。此外，派生的状态抽象允许任务学习者实现接近神谕级别的样本效率，并在所有任务上超越基线。</li>
</ul>

<h3>Title: DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing  High-Quality Implicit Neural Representations</h3>
<ul>
<li><strong>Authors: </strong>Dogyun Park, Sihyeon Kim, Sojin Lee, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12517">https://arxiv.org/abs/2401.12517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12517">https://arxiv.org/pdf/2401.12517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12517]] DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing  High-Quality Implicit Neural Representations(https://arxiv.org/abs/2401.12517)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel conditioning mechanism for evaluating INRs with the hierarchically decomposed PEs to further enhance expressive power. Extensive experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets, demonstrate the versatility of DDMI and its superior performance compared to the existing INR generative models.</li>
<li><strong>摘要：</strong>最近的研究引入了一类新的生成模型，用于合成隐式神经表示（INR），捕获各个领域中的任意连续信号。这些模型为与领域无关的生成模型打开了大门，但它们往往无法实现高质量的生成。我们观察到现有方法生成神经网络的权重来参数化 INR 并使用固定位置嵌入（PE）评估网络。可以说，这种架构限制了生成模型的表达能力并导致低质量的 INR 生成。为了解决这个限制，我们提出了 INR 的领域无关的潜在扩散模型（DDMI），它生成自适应位置嵌入而不是神经网络的权重。具体来说，我们开发了一种离散到连续空间变分自动编码器（D2C-VAE），它无缝连接共享潜在空间中的离散数据和连续信号函数。此外，我们引入了一种新颖的条件机制，用于使用分层分解的 PE 来评估 INR，以进一步增强表达能力。跨四种模式（例如 2D 图像、3D 形状、神经辐射场和视频）以及七个基准数据集的广泛实验证明了 DDMI 的多功能性及其与现有 INR 生成模型相比的卓越性能。</li>
</ul>

<h3>Title: Key Information Retrieval to Classify the Unstructured Data Content of  Preferential Trade Agreements</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Zhao, Ziyi Meng, Stepan Gordeev, Zijie Pan, Dongjin Song, Sandro Steinbach, Caiwen Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12520">https://arxiv.org/abs/2401.12520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12520">https://arxiv.org/pdf/2401.12520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12520]] Key Information Retrieval to Classify the Unstructured Data Content of  Preferential Trade Agreements(https://arxiv.org/abs/2401.12520)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>With the rapid proliferation of textual data, predicting long texts has emerged as a significant challenge in the domain of natural language processing. Traditional text prediction methods encounter substantial difficulties when grappling with long texts, primarily due to the presence of redundant and irrelevant information, which impedes the model's capacity to capture pivotal insights from the text. To address this issue, we introduce a novel approach to long-text classification and prediction. Initially, we employ embedding techniques to condense the long texts, aiming to diminish the redundancy therein. Subsequently,the Bidirectional Encoder Representations from Transformers (BERT) embedding method is utilized for text classification training. Experimental outcomes indicate that our method realizes considerable performance enhancements in classifying long texts of Preferential Trade Agreements. Furthermore, the condensation of text through embedding methods not only augments prediction accuracy but also substantially reduces computational complexity. Overall, this paper presents a strategy for long-text prediction, offering a valuable reference for researchers and engineers in the natural language processing sphere.</li>
<li><strong>摘要：</strong>随着文本数据的快速增长，预测长文本已成为自然语言处理领域的重大挑战。传统的文本预测方法在处理长文本时遇到很大的困难，这主要是由于冗余和不相关信息的存在，这阻碍了模型从文本中捕获关键见解的能力。为了解决这个问题，我们引入了一种新的长文本分类和预测方法。最初，我们采用嵌入技术来压缩长文本，旨在减少其中的冗余。随后，利用来自 Transformers 的双向编码器表示（BERT）嵌入方法进行文本分类训练。实验结果表明，我们的方法在对优惠贸易协议的长文本进行分类方面实现了相当大的性能提升。此外，通过嵌入方法压缩文本不仅提高了预测精度，而且还大大降低了计算复杂性。总的来说，本文提出了一种长文本预测策略，为自然语言处理领域的研究人员和工程师提供了有价值的参考。</li>
</ul>

<h3>Title: BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12522">https://arxiv.org/abs/2401.12522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12522">https://arxiv.org/pdf/2401.12522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12522]] BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language  Models(https://arxiv.org/abs/2401.12522)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments confirm our method surpasses state-of-the-art acceleration techniques.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常在推理过程中采用自回归生成，导致内存带宽需求较高，从而导致延迟延长。为了缓解这种低效率，我们提出了无损加速双向调整 (BiTA)，这是一种通过简化的半自回归生成和草稿验证来加速 LLM 的创新方法。受即时调优概念的启发，我们通过称为双向调优的参数高效设计增强了法学硕士的半自回归生成能力。该模型采用高效的基于树的解码，并行执行候选草稿生成和验证，确保输出与贪婪采样下的自回归副本相同。 BiTA 作为一个轻量级插件模块，可以无缝地提高现有 LLM 的推理效率，而无需额外的辅助模型或产生大量额外的内存成本。应用建议的 BiTA，LLaMA-2-70B-Chat 在 MT-Bench 基准测试中实现了 2.7$\times$ 的加速。大量的实验证实我们的方法超越了最先进的加速技术。</li>
</ul>

<h3>Title: DAFA: Distance-Aware Fair Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Hyungyu Lee, Saehyung Lee, Hyemi Jang, Junsung Park, Ho Bae, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12532">https://arxiv.org/abs/2401.12532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12532">https://arxiv.org/pdf/2401.12532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12532]] DAFA: Distance-Aware Fair Adversarial Training(https://arxiv.org/abs/2401.12532)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The disparity in accuracy between classes in standard training is amplified during adversarial training, a phenomenon termed the robust fairness problem. Existing methodologies aimed to enhance robust fairness by sacrificing the model's performance on easier classes in order to improve its performance on harder ones. However, we observe that under adversarial attacks, the majority of the model's predictions for samples from the worst class are biased towards classes similar to the worst class, rather than towards the easy classes. Through theoretical and empirical analysis, we demonstrate that robust fairness deteriorates as the distance between classes decreases. Motivated by these insights, we introduce the Distance-Aware Fair Adversarial training (DAFA) methodology, which addresses robust fairness by taking into account the similarities between classes. Specifically, our method assigns distinct loss weights and adversarial margins to each class and adjusts them to encourage a trade-off in robustness among similar classes. Experimental results across various datasets demonstrate that our method not only maintains average robust accuracy but also significantly improves the worst robust accuracy, indicating a marked improvement in robust fairness compared to existing methods.</li>
<li><strong>摘要：</strong>标准训练中类别之间的准确度差异在对抗性训练期间被放大，这种现象被称为鲁棒公平问题。现有的方法旨在通过牺牲模型在较简单的类别上的性能来提高其在较难的类别上的性能，从而增强稳健的公平性。然而，我们观察到，在对抗性攻击下，模型对最差类别样本的大多数预测都偏向于与最差类别相似的类别，而不是容易类别。通过理论和实证分析，我们证明鲁棒公平性随着类之间距离的减小而恶化。受这些见解的启发，我们引入了距离感知公平对抗训练（DAFA）方法，该方法通过考虑类别之间的相似性来解决稳健的公平性。具体来说，我们的方法为每个类别分配不同的损失权重和对抗性裕度，并对其进行调整以鼓励相似类别之间稳健性的权衡。各种数据集的实验结果表明，我们的方法不仅保持了平均鲁棒精度，而且还显着提高了最差鲁棒精度，表明与现有方法相比，鲁棒公平性有了显着提高。</li>
</ul>

<h3>Title: Graph Contrastive Invariant Learning from the Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yanhu Mo, Xiao Wang, Shaohua Fan, Chuan Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12564">https://arxiv.org/abs/2401.12564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12564">https://arxiv.org/pdf/2401.12564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12564]] Graph Contrastive Invariant Learning from the Causal Perspective(https://arxiv.org/abs/2401.12564)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the encoder to capture the invariant information contained in causal variables, and (ii) the independence objective aims to reduce the influence of confounders on the causal variables. Experimental results demonstrate the effectiveness of our approach on node classification tasks.</li>
<li><strong>摘要：</strong>图对比学习（GCL）通过以自监督的方式对比两个增强图来学习节点表示，引起了相当大的关注。通常认为 GCL 可以学习不变表示。然而，这种理解在实践中总是成立吗？在本文中，我们首先从因果关系的角度研究GCL。通过使用结构因果模型（SCM）分析 GCL，我们发现由于图中包含非因果信息，传统的 GCL 可能无法很好地学习不变表示。我们如何解决这个问题并鼓励当前的 GCL 学习更好的不变表示？ SCM 提出了两个要求，并促使我们提出一种新颖的 GCL 方法。特别是，我们引入了谱图增强来模拟对非因果因素的干预。然后我们设计不变性目标和独立性目标以更好地捕获因果因素。具体来说，（i）不变性目标鼓励编码器捕获因果变量中包含的不变信息，（ii）独立性目标旨在减少混杂因素对因果变量的影响。实验结果证明了我们的方法在节点分类任务上的有效性。</li>
</ul>

<h3>Title: Automated Fact-Checking of Climate Change Claims with Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Markus Leippold, Saeid Ashraf Vaghefi, Dominik Stammbach, Veruska Muccione, Julia Bingler, Jingwei Ni, Chiara Colesanti-Senni, Tobias Wekhof, Tobias Schimanski, Glen Gostlow, Tingyu Yu, Juerg Luterbacher, Christian Huggel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12566">https://arxiv.org/abs/2401.12566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12566">https://arxiv.org/pdf/2401.12566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12566]] Automated Fact-Checking of Climate Change Claims with Large Language  Models(https://arxiv.org/abs/2401.12566)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>This paper presents Climinator, a novel AI-based tool designed to automate the fact-checking of climate change claims. Utilizing an array of Large Language Models (LLMs) informed by authoritative sources like the IPCC reports and peer-reviewed scientific literature, Climinator employs an innovative Mediator-Advocate framework. This design allows Climinator to effectively synthesize varying scientific perspectives, leading to robust, evidence-based evaluations. Our model demonstrates remarkable accuracy when testing claims collected from Climate Feedback and Skeptical Science. Notably, when integrating an advocate with a climate science denial perspective in our framework, Climinator's iterative debate process reliably converges towards scientific consensus, underscoring its adeptness at reconciling diverse viewpoints into science-based, factual conclusions. While our research is subject to certain limitations and necessitates careful interpretation, our approach holds significant potential. We hope to stimulate further research and encourage exploring its applicability in other contexts, including political fact-checking and legal domains.</li>
<li><strong>摘要：</strong>本文介绍了 Climinator，这是一种基于人工智能的新型工具，旨在自动对气候变化声明进行事实核查。 Climinator 利用 IPCC 报告和同行评审的科学文献等权威来源提供的一系列大型语言模型 (LLM)，采用创新的调解者-倡导者框架。这种设计使 Climinator 能够有效地综合不同的科学观点，从而进行稳健、基于证据的评估。在测试从气候反馈和怀疑科学收集的声明时，我们的模型表现出了惊人的准确性。值得注意的是，当将倡导者与气候科学否认观点整合到我们的框架中时，Climinator 的迭代辩论过程可靠地收敛于科学共识，强调了其将不同观点调和为基于科学的事实结论的能力。虽然我们的研究受到某些限制并且需要仔细解释，但我们的方法具有巨大的潜力。我们希望刺激进一步的研究，并鼓励探索其在其他领域的适用性，包括政治事实核查和法律领域。</li>
</ul>

<h3>Title: LLMCheckup: Conversational Examination of Large Language Models via  Interpretability Tools</h3>
<ul>
<li><strong>Authors: </strong>Qianli Wang, Tatiana Anikina, Nils Feldhus, Josef van Genabith, Leonhard Hennig, Sebastian Möller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12576">https://arxiv.org/abs/2401.12576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12576">https://arxiv.org/pdf/2401.12576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12576]] LLMCheckup: Conversational Examination of Large Language Models via  Interpretability Tools(https://arxiv.org/abs/2401.12576)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup provides tutorials for operations available in the system, catering to individuals with varying levels of expertise in XAI and supports multiple input modalities. We introduce a new parsing strategy called multi-prompt parsing substantially enhancing the parsing accuracy of LLMs. Finally, we showcase the tasks of fact checking and commonsense question answering.</li>
<li><strong>摘要：</strong>以对话形式提供解释的可解释性工具已证明其在增强用户理解方面的功效，因为一次性解释有时可能无法向用户提供足够的信息。然而，当前基于对话的解释的解决方案需要许多依赖性，并且不容易转移到它们不是为之设计的任务。通过 LLMCheckup，我们提供了一个易于访问的工具，允许用户与任何最先进的大型语言模型 (LLM) 讨论其行为。我们使法学硕士能够自行生成所有解释，并通过将其与广泛的可解释人工智能 (XAI) 工具连接起来，无需进行微调即可处理意图识别。特征归因、基于嵌入的相似性以及反事实和理由生成的提示策略。 LLM（自我）解释以互动对话的形式呈现，支持后续问题并生成建议。 LLMCheckup 提供系统中可用操作的教程，适合具有不同 XAI 专业知识水平的个人，并支持多种输入模式。我们引入了一种称为多提示解析的新解析策略，大大提高了 LLM 的解析准确性。最后，我们展示事实核查和常识性问答的任务。</li>
</ul>

<h3>Title: SLANG: New Concept Comprehension of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12585">https://arxiv.org/abs/2401.12585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12585">https://arxiv.org/pdf/2401.12585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12585]] SLANG: New Concept Comprehension of Large Language Models(https://arxiv.org/abs/2401.12585)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their intended meanings. The empirical analysis shows that our causal inference-based approach outperforms the traditional models in terms of precision and relevance in the interpretation of Internet slang and memes.</li>
<li><strong>摘要：</strong>语言的动态特性，在互联网上的俚语和模因领域尤其明显，对大型语言模型（LLM）的适应性提出了严峻的挑战。传统上，这些模型以静态数据集为基础，通常难以跟上在线社区快速语言演变的特征。这项研究解决了弥合这一差距的迫切需要，旨在增强法学硕士对互联网上不断发展的新概念的理解，而无需持续再培训的高成本和不切实际的情况。为了解决这个问题，我们提出了一个新的基准 $\textbf{SLANG}$ 来评估法学硕士在理解新兴语言趋势方面的熟练程度，并提出了一个基线方法 $\textbf{FOCUS}$ ，该方法使用因果推理来增强法学硕士理解新短语的能力和使用模式。这种方法涉及仔细审查现实世界中的语言转变实例，作为语境信标，在新出现的表达方式与其预期含义之间形成更精确和与语境相关的联系。实证分析表明，我们基于因果推理的方法在互联网俚语和模因解释的精度和相关性方面优于传统模型。</li>
</ul>

<h3>Title: Interpreting Equivariant Representations</h3>
<ul>
<li><strong>Authors: </strong>Andreas Abildtrup Hansen, Anna Calissano, Aasa Feragen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12588">https://arxiv.org/abs/2401.12588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12588">https://arxiv.org/pdf/2401.12588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12588]] Interpreting Equivariant Representations(https://arxiv.org/abs/2401.12588)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Latent representations are used extensively for downstream tasks, such as visualization, interpolation or feature extraction of deep learning models. Invariant and equivariant neural networks are powerful and well-established models for enforcing inductive biases. In this paper, we demonstrate that the inductive bias imposed on the by an equivariant model must also be taken into account when using latent representations. We show how not accounting for the inductive biases leads to decreased performance on downstream tasks, and vice versa, how accounting for inductive biases can be done effectively by using an invariant projection of the latent representations. We propose principles for how to choose such a projection, and show the impact of using these principles in two common examples: First, we study a permutation equivariant variational auto-encoder trained for molecule graph generation; here we show that invariant projections can be designed that incur no loss of information in the resulting invariant representation. Next, we study a rotation-equivariant representation used for image classification. Here, we illustrate how random invariant projections can be used to obtain an invariant representation with a high degree of retained information. In both cases, the analysis of invariant latent representations proves superior to their equivariant counterparts. Finally, we illustrate that the phenomena documented here for equivariant neural networks have counterparts in standard neural networks where invariance is encouraged via augmentation. Thus, while these ambiguities may be known by experienced developers of equivariant models, we make both the knowledge as well as effective tools to handle the ambiguities available to the broader community.</li>
<li><strong>摘要：</strong>潜在表示广泛用于下游任务，例如深度学习模型的可视化、插值或特征提取。不变和等变神经网络是用于强制归纳偏差的强大且完善的模型。在本文中，我们证明，在使用潜在表示时，还必须考虑等变模型施加的归纳偏差。我们展示了不考虑归纳偏差如何导致下游任务的性能下降，反之亦然，如何通过使用潜在表示的不变投影来有效地考虑归纳偏差。我们提出了如何选择这种投影的原则，并在两个常见示例中展示了使用这些原则的影响：首先，我们研究了一种用于分子图生成的排列等变变分自动编码器；在这里，我们表明可以设计不变投影，从而在所得的不变表示中不会丢失信息。接下来，我们研究用于图像分类的旋转等变表示。在这里，我们说明如何使用随机不变投影来获得具有高度保留信息的不变表示。在这两种情况下，对不变潜在表示的分析都证明优于其等变对应物。最后，我们说明这里记录的等变神经网络的现象在标准神经网络中也有对应的现象，其中通过增强来鼓励不变性。因此，虽然经验丰富的等变模型开发人员可能知道这些模糊性，但我们为更广泛的社区提供了知识和有效的工具来处理这些模糊性。</li>
</ul>

<h3>Title: Revolutionizing Retrieval-Augmented Generation with Enhanced PDF  Structure Recognition</h3>
<ul>
<li><strong>Authors: </strong>Demiao Lin (chatdoc.com)</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12599">https://arxiv.org/abs/2401.12599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12599">https://arxiv.org/pdf/2401.12599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12599]] Revolutionizing Retrieval-Augmented Generation with Enhanced PDF  Structure Recognition(https://arxiv.org/abs/2401.12599)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. We conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panoptic and pinpoint PDF parser, retrieves more accurate and complete segments, and thus better answers. Empirical experiments show that ChatDOC is superior to baseline on nearly 47% of questions, ties for 38% of cases, and falls short on only 15% of cases. It shows that we may revolutionize RAG with enhanced PDF structure recognition.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的快速发展，检索增强生成（RAG）已成为基于专业知识的问答领域的主导方法。目前各大基础模型公司都已经开放了Embedding和Chat API接口，LangChain等框架也已经集成了RAG流程。看来RAG的关键模型和步骤已经解决了，那么问题来了：专业知识QA体系现在是否已经接近完美？本文发现，目前的主要方法都依赖于访问高质量文本语料库的前提。然而，由于专业文档主要存储在PDF中，PDF解析的准确性较低，严重影响了基于专业知识的QA的有效性。我们对来自相应现实世界专业文档的数百个问题进行了实证 RAG 实验。结果表明，ChatDOC（一种配备全景和精确 PDF 解析器的 RAG 系统）可以检索更准确和完整的片段，从而获得更好的答案。实证实验表明，ChatDOC 在近 47% 的问题上优于基线，在 38% 的案例上表现平平，仅在 15% 的案例上表现不佳。它表明我们可以通过增强 PDF 结构识别来彻底改变 RAG。</li>
</ul>

<h3>Title: Prompt Smells: An Omen for Undesirable Generative AI Outputs</h3>
<ul>
<li><strong>Authors: </strong>Krishna Ronanki, Beatriz Cabrero-Daniel, Christian Berger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12611">https://arxiv.org/abs/2401.12611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12611">https://arxiv.org/pdf/2401.12611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12611]] Prompt Smells: An Omen for Undesirable Generative AI Outputs(https://arxiv.org/abs/2401.12611)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination, prompt, code</a></li>
<li><strong>Abstract: </strong>Recent Generative Artificial Intelligence (GenAI) trends focus on various applications, including creating stories, illustrations, poems, articles, computer code, music compositions, and videos. Extrinsic hallucinations are a critical limitation of such GenAI, which can lead to significant challenges in achieving and maintaining the trustworthiness of GenAI. In this paper, we propose two new concepts that we believe will aid the research community in addressing limitations associated with the application of GenAI models. First, we propose a definition for the "desirability" of GenAI outputs and three factors which are observed to influence it. Second, drawing inspiration from Martin Fowler's code smells, we propose the concept of "prompt smells" and the adverse effects they are observed to have on the desirability of GenAI outputs. We expect our work will contribute to the ongoing conversation about the desirability of GenAI outputs and help advance the field in a meaningful way.</li>
<li><strong>摘要：</strong>最近的生成人工智能 (GenAI) 趋势侧重于各种应用，包括创作故事、插图、诗歌、文章、计算机代码、音乐作品和视频。外在幻觉是此类 GenAI 的一个关键限制，这可能会给实现和维持 GenAI 的可信度带来重大挑战。在本文中，我们提出了两个新概念，我们相信这将有助于研究界解决与 GenAI 模型应用相关的局限性。首先，我们提出了 GenAI 输出的“可取性”的定义以及观察到的影响它的三个因素。其次，从 Martin Fowler 的代码气味中汲取灵感，我们提出了“即时气味”的概念以及观察到的它们对 GenAI 输出的期望产生的不利影响。我们希望我们的工作能够为有关 GenAI 输出的可取性的持续对话做出贡献，并以有意义的方式帮助推动该领域的发展。</li>
</ul>

<h3>Title: Knowledge Distillation from Language-Oriented to Emergent Communication  for Multi-Agent Remote Control</h3>
<ul>
<li><strong>Authors: </strong>Yongjun Kim, Sejin Seo, Jihong Park, Mehdi Bennis, Seong-Lyun Kim, Junil Choi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.IT, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12624">https://arxiv.org/abs/2401.12624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12624">https://arxiv.org/pdf/2401.12624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12624]] Knowledge Distillation from Language-Oriented to Emergent Communication  for Multi-Agent Remote Control(https://arxiv.org/abs/2401.12624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.</li>
<li><strong>摘要：</strong>在这项工作中，我们比较了基于多智能体深度强化学习 (MADRL) 的紧急通信 (EC) 和由使用人类语言的预训练大语言模型 (LLM) 提供支持的面向语言的语义通信 (LSC)。在多智能体远程导航任务中，多模态输入数据包括位置和通道图，结果表明，EC 在使用多模态数据时会产生很高的训练成本并且很困难，而 LSC 由于 LLM 的规模较大，因此会产生很高的推理计算成本。为了解决它们各自的瓶颈，我们提出了一种新的语言引导 EC (LEC) 框架，通过知识蒸馏 (KD) 使用 LSC 指导 EC 训练。仿真证实，与 EC 相比，LEC 实现了更快的传输时间，同时避开了信道条件较差的区域，并将 MADRL 训练收敛速度加快了 61.8%。</li>
</ul>

<h3>Title: Binary Feature Mask Optimization for Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Mehmet E. Lorasdagi, Mehmet Y. Turali, Ali T. Koc, Suleyman S. Kozat</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12644">https://arxiv.org/abs/2401.12644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12644">https://arxiv.org/pdf/2401.12644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12644]] Binary Feature Mask Optimization for Feature Selection(https://arxiv.org/abs/2401.12644)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>We investigate feature selection problem for generic machine learning (ML) models. We introduce a novel framework that selects features considering the predictions of the model. Our framework innovates by using a novel feature masking approach to eliminate the features during the selection process, instead of completely removing them from the dataset. This allows us to use the same ML model during feature selection, unlike other feature selection methods where we need to train the ML model again as the dataset has different dimensions on each iteration. We obtain the mask operator using the predictions of the ML model, which offers a comprehensive view on the subsets of the features essential for the predictive performance of the model. A variety of approaches exist in the feature selection literature. However, no study has introduced a training-free framework for a generic ML model to select features while considering the importance of the feature subsets as a whole, instead of focusing on the individual features. We demonstrate significant performance improvements on the real-life datasets under different settings using LightGBM and Multi-Layer Perceptron as our ML models. Additionally, we openly share the implementation code for our methods to encourage the research and the contributions in this area.</li>
<li><strong>摘要：</strong>我们研究通用机器学习（ML）模型的特征选择问题。我们引入了一种新颖的框架，该框架根据模型的预测来选择特征。我们的框架进行了创新，使用一种新颖的特征屏蔽方法在选择过程中消除特征，而不是从数据集中完全删除它们。这允许我们在特征选择过程中使用相同的 ML 模型，这与其他特征选择方法不同，在其他特征选择方法中，我们需要再次训练 ML 模型，因为数据集在每次迭代中具有不同的维度。我们使用 ML 模型的预测获得掩模运算符，该运算符提供了对模型预测性能至关重要的特征子集的全面视图。特征选择文献中存在多种方法。然而，还没有研究引入通用机器学习模型的免训练框架来选择特征，同时考虑整个特征子集的重要性，而不是关注单个特征。我们使用 LightGBM 和多层感知器作为 ML 模型，在不同设置下展示了现实数据集的显着性能改进。此外，我们公开分享我们方法的实现代码，以鼓励该领域的研究和贡献。</li>
</ul>

<h3>Title: Consistency Enhancement-Based Deep Multiview Clustering via Contrastive  Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Hua Mao, Wai Lok Woo, Jie Chen, Xi Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12648">https://arxiv.org/abs/2401.12648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12648">https://arxiv.org/pdf/2401.12648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12648]] Consistency Enhancement-Based Deep Multiview Clustering via Contrastive  Learning(https://arxiv.org/abs/2401.12648)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiments conducted on five datasets demonstrate the effectiveness and superiority of our method in comparison with the state-of-the-art (SOTA) methods. The code for this method can be accessed at https://anonymous.4open.science/r/CCEC-E84E/.</li>
<li><strong>摘要：</strong>多视图聚类 (MVC) 通过综合多个视图的信息，将数据样本分成有意义的簇。此外，基于深度学习的方法在MVC场景中展示了其强大的特征学习能力。然而，在保持一致性的同时有效地概括特征表示仍然是一个棘手的问题。此外，现有的大多数基于对比学习的深度聚类方法都忽略了聚类过程中聚类表示的一致性。在本文中，我们展示了如何克服上述问题，并通过对比学习（CCEC）提出了一种基于一致增强的深度 MVC 方法。具体来说，语义连接块被合并到特征表示中以保留多个视图之间的一致信息。此外，通过谱聚类增强了聚类的表示过程，并且提高了多个视图之间的一致性。在五个数据集上进行的实验证明了我们的方法与最先进的（SOTA）方法相比的有效性和优越性。此方法的代码可以访问 https://anonymous.4open.science/r/CCEC-E84E/。</li>
</ul>

<h3>Title: Context Matters: Pushing the Boundaries of Open-Ended Answer Generation  with Graph-Structured Knowledge Context</h3>
<ul>
<li><strong>Authors: </strong>Somnath Banerjee, Amruit Sahoo, Sayan Layek, Avik Dutta, Rima Hazra, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12671">https://arxiv.org/abs/2401.12671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12671">https://arxiv.org/pdf/2401.12671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12671]] Context Matters: Pushing the Boundaries of Open-Ended Answer Generation  with Graph-Structured Knowledge Context(https://arxiv.org/abs/2401.12671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the continuously advancing AI landscape, crafting context-rich and meaningful responses via Large Language Models (LLMs) is essential. Researchers are becoming more aware of the challenges that LLMs with fewer parameters encounter when trying to provide suitable answers to open-ended questions. To address these hurdles, the integration of cutting-edge strategies, augmentation of rich external domain knowledge to LLMs, offers significant improvements. This paper introduces a novel framework that combines graph-driven context retrieval in conjunction to knowledge graphs based enhancement, honing the proficiency of LLMs, especially in domain specific community question answering platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on various LLMs with different parameter sizes to evaluate their ability to ground knowledge and determine factual accuracy in answers to open-ended questions. Our methodology GraphContextGen consistently outperforms dominant text-based retrieval systems, demonstrating its robustness and adaptability to a larger number of use cases. This advancement highlights the importance of pairing context rich data retrieval with LLMs, offering a renewed approach to knowledge sourcing and generation in AI systems. We also show that, due to rich contextual data retrieval, the crucial entities, along with the generated answer, remain factually coherent with the gold answer.</li>
<li><strong>摘要：</strong>在不断发展的人工智能领域，通过大型语言模型 (LLM) 制作上下文丰富且有意义的响应至关重要。研究人员越来越意识到参数较少的法学硕士在尝试为开放式问题提供合适答案时遇到的挑战。为了解决这些障碍，前沿策略的整合、法学硕士丰富的外部领域知识的增强，可以带来显着的改进。本文介绍了一种新颖的框架，它将图驱动的上下文检索与基于知识图的增强相结合，提高了法学硕士的熟练程度，特别是在特定领域的社区问答平台（如 AskUbuntu、Unix 和 ServerFault）中。我们对具有不同参数大小的各种法学硕士进行实验，以评估他们奠定知识基础并确定开放式问题答案的事实准确性的能力。我们的方法 GraphContextGen 始终优于主流的基于文本的检索系统，证明了其对大量用例的稳健性和适应性。这一进步凸显了将上下文丰富的数据检索与法学硕士相结合的重要性，为人工智能系统中的知识获取和生成提供了一种新的方法。我们还表明，由于丰富的上下文数据检索，关键实体以及生成的答案实际上与黄金答案保持一致。</li>
</ul>

<h3>Title: ChatGraph: Chat with Your Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yun Peng, Sen Lin, Qian Chen, Lyu Xu, Xiaojun Ren, Yafei Li, Jianliang Xu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12672">https://arxiv.org/abs/2401.12672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12672">https://arxiv.org/pdf/2401.12672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12672]] ChatGraph: Chat with Your Graphs(https://arxiv.org/abs/2401.12672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat, rag</a></li>
<li><strong>Abstract: </strong>Graph analysis is fundamental in real-world applications. Traditional approaches rely on SPARQL-like languages or clicking-and-dragging interfaces to interact with graph data. However, these methods either require users to possess high programming skills or support only a limited range of graph analysis functionalities. To address the limitations, we propose a large language model (LLM)-based framework called ChatGraph. With ChatGraph, users can interact with graphs through natural language, making it easier to use and more flexible than traditional approaches. The core of ChatGraph lies in generating chains of graph analysis APIs based on the understanding of the texts and graphs inputted in the user prompts. To achieve this, ChatGraph consists of three main modules: an API retrieval module that searches for relevant APIs, a graph-aware LLM module that enables the LLM to comprehend graphs, and an API chain-oriented finetuning module that guides the LLM in generating API chains.</li>
<li><strong>摘要：</strong>图分析是现实应用中的基础。传统方法依赖类似 SPARQL 的语言或单击并拖动界面来与图形数据交互。然而，这些方法要么要求用户具备较高的编程技能，要么仅支持有限范围的图分析功能。为了解决这些限制，我们提出了一个基于大型语言模型（LLM）的框架，称为 ChatGraph。通过 ChatGraph，用户可以通过自然语言与图形进行交互，比传统方法更易于使用且更灵活。 ChatGraph的核心在于根据对用户提示中输入的文本和图形的理解生成图形分析API链。为了实现这一目标，ChatGraph由三个主要模块组成：API检索模块，用于搜索相关API；图形感知LLM模块，使LLM能够理解图形；以及面向API链的微调模块，指导LLM生成API链。</li>
</ul>

<h3>Title: Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical  Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhishuai Li, Yunhao Nie, Ziyue Li, Lei Bai, Yisheng Lv, Rui Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12681">https://arxiv.org/abs/2401.12681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12681">https://arxiv.org/pdf/2401.12681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12681]] Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical  Learning(https://arxiv.org/abs/2401.12681)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Kriging aims at estimating the attributes of unsampled geo-locations from observations in the spatial vicinity or physical connections, which helps mitigate skewed monitoring caused by under-deployed sensors. Existing works assume that neighbors' information offers the basis for estimating the attributes of the unobserved target while ignoring non-neighbors. However, non-neighbors could also offer constructive information, and neighbors could also be misleading. To this end, we propose ``Contrastive-Prototypical'' self-supervised learning for Kriging (KCP) to refine valuable information from neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we conduct the Kriging task from a new perspective of representation: we aim to first learn robust and general representations and then recover attributes from representations. A neighboring contrastive module is designed that coarsely learns the representations by narrowing the representation distance between the target and its neighbors while pushing away the non-neighbors. In parallel, a prototypical module is introduced to identify similar representations via exchanged prediction, thus refining the misleading neighbors and recycling the useful non-neighbors from the neighboring contrast component. As a result, not all the neighbors and some of the non-neighbors will be used to infer the target. To encourage the two modules above to learn general and robust representations, we design an adaptive augmentation module that incorporates data-driven attribute augmentation and centrality-based topology augmentation over the spatiotemporal Kriging graph data. Extensive experiments on real-world datasets demonstrate the superior performance of KCP compared to its peers with 6% improvements and exceptional transferability and robustness. The code is available at https://github.com/bonaldli/KCP</li>
<li><strong>摘要：</strong>克里金法旨在根据空间附近或物理连接的观测来估计未采样地理位置的属性，这有助于减轻因部署不足的传感器而导致的监测偏差。现有的工作假设邻居的信息为估计未观察目标的属性提供了基础，同时忽略了非邻居。然而，非邻居也可以提供建设性信息，而邻居也可能会产生误导。为此，我们提出了克里金法的“对比原型”自监督学习（KCP），以提炼来自邻居的有价值的信息并回收来自非邻居的信息。作为预训练的范例，我们从新的表示角度进行克里格任务：我们的目标是首先学习鲁棒且通用的表示，然后从表示中恢复属性。设计了邻近对比模块，通过缩小目标与其邻居之间的表示距离同时推开非邻居来粗略地学习表示。同时，引入了一个原型模块来通过交换预测来识别相似的表示，从而细化误导的邻居并从相邻对比分量中回收有用的非邻居。因此，并非所有邻居和一些非邻居都会被用来推断目标。为了鼓励上述两个模块学习通用且鲁棒的表示，我们设计了一个自适应增强模块，该模块在时空克里金图数据上结合了数据驱动的属性增强和基于中心性的拓扑增强。对真实世界数据集的大量实验表明，与同类产品相比，KCP 具有 6% 的性能提升以及卓越的可转移性和鲁棒性。代码可在 https://github.com/bonaldli/KCP 获取</li>
</ul>

<h3>Title: Energy-based Automated Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12689">https://arxiv.org/abs/2401.12689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12689">https://arxiv.org/pdf/2401.12689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12689]] Energy-based Automated Model Evaluation(https://arxiv.org/abs/2401.12689)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels.</li>
<li><strong>摘要：</strong>机器学习模型的传统评估协议严重依赖于带标签的、独立同分布假设的测试数据集，而这在现实世界的应用中并不常见。自动模型评估 (AutoEval) 展示了这种传统工作流程的替代方案，通过在不存在真实标签的情况下形成测试性能的近端预测管道。尽管 AutoEval 框架最近取得了成功，但仍然存在过度自信问题、大量存储和计算成本。在这方面，我们提出了一种新颖的措施——元分布式能源（MDE）——使 AutoEval 框架更加高效和有效。 MDE 的核心是针对与各个样本相关的信息（能量）建立元分布统计，然后通过基于能量的学习提供更平滑的表示。我们通过将 MDE 与分类损失联系起来，进一步提供我们的理论见解。我们提供了跨模式、数据集和不同架构主干的广泛实验，以验证 MDE 的有效性以及与先前方法相比的优越性。我们还通过展示 MDE 与大型模型的无缝集成以及轻松适应带有噪声或不平衡标签的学习场景来证明 MDE 的多功能性。</li>
</ul>

<h3>Title: Deep Neural Network Benchmarks for Selective Classification</h3>
<ul>
<li><strong>Authors: </strong>Andrea Pugnana, Lorenzo Perini, Jesse Davis, Salvatore Ruggieri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12708">https://arxiv.org/abs/2401.12708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12708">https://arxiv.org/pdf/2401.12708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12708]] Deep Neural Network Benchmarks for Selective Classification(https://arxiv.org/abs/2401.12708)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into their relative merits. We fill this gap by benchmarking 18 baselines on a diverse set of 44 datasets that includes both image and tabular data. Moreover, there is a mix of binary and multiclass tasks. We evaluate these approaches using several criteria, including selective error rate, empirical coverage, distribution of rejected instance's classes, and performance on out-of-distribution instances. The results indicate that there is not a single clear winner among the surveyed baselines, and the best method depends on the users' objectives.</li>
<li><strong>摘要：</strong>随着机器学习模型在许多社会敏感任务中的部署不断增加，对可靠且值得信赖的预测的需求也不断增长。实现这些要求的一种方法是允许模型在犯错误的风险很高时放弃进行预测。这需要向模型添加选择机制，该机制选择模型将提供预测的那些示例。选择性分类框架旨在设计一种机制，平衡拒绝预测的比例（即模型未做出预测的示例的比例）与所选预测的预测性能的改进。存在多种选择性分类框架，其中大多数依赖于深度神经网络架构。然而，对现有方法的实证评估仍然仅限于方法和设置之间的部分比较，使实践者无法深入了解其相对优点。我们通过对 44 个不同数据集（包括图像和表格数据）上的 18 个基线进行基准测试来填补这一空白。此外，还有二进制任务和多类任务的混合。我们使用几个标准来评估这些方法，包括选择性错误率、经验覆盖率、被拒绝实例类的分布以及分布外实例的性能。结果表明，在调查的基线中没有一个明显的赢家，最佳方法取决于用户的目标。</li>
</ul>

<h3>Title: Generating Unsupervised Abstractive Explanations for Rumour Verification</h3>
<ul>
<li><strong>Authors: </strong>Iman Munire Bilal, Preslav Nakov, Rob Procter, Maria Liakata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12713">https://arxiv.org/abs/2401.12713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12713">https://arxiv.org/pdf/2401.12713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12713]] Generating Unsupervised Abstractive Explanations for Rumour Verification(https://arxiv.org/abs/2401.12713)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.</li>
<li><strong>摘要：</strong>社交媒体中谣言验证的任务涉及根据由此产生的对话线索来评估声明的真实性。虽然之前的工作重点是预测真实性标签，但在这里我们重新制定了任务，以生成以模型为中心的、对谣言真实性的自由​​文本解释。我们遵循无监督的方法，首先利用事后可解释性方法对线程中最重要的帖子进行评分，然后通过模板引导的摘要使用这些帖子生成信息丰富的解释性摘要。为了评估解释性摘要的信息量，我们利用大型语言模型（LLM）的小样本学习能力。我们的实验表明，法学硕士在评估摘要方面与人类具有相似的一致性。重要的是，我们表明，与仅使用线程中排名最高的帖子相比，解释性抽象摘要信息更丰富，并且更好地反映了预测的谣言准确性。</li>
</ul>

<h3>Title: A Comprehensive View of the Biases of Toxicity and Sentiment Analysis  Methods Towards Utterances with African American English Expressions</h3>
<ul>
<li><strong>Authors: </strong>Guilherme H. Resende, Luiz F. Nery, Fabrício Benevenuto, Savvas Zannettou, Flavio Figueiredo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12720">https://arxiv.org/abs/2401.12720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12720">https://arxiv.org/pdf/2401.12720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12720]] A Comprehensive View of the Biases of Toxicity and Sentiment Analysis  Methods Towards Utterances with African American English Expressions(https://arxiv.org/abs/2401.12720)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language is a dynamic aspect of our culture that changes when expressed in different technologies/communities. Online social networks have enabled the diffusion and evolution of different dialects, including African American English (AAE). However, this increased usage is not without barriers. One particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity (Google's Perspective and the open-source Detoxify) methods present biases towards utterances with AAE expressions. Consider Google's Perspective to understand bias. Here, an utterance such as ``All n*ggers deserve to die respectfully. The police murder us.'' it reaches a higher toxicity than ``African-Americans deserve to die respectfully. The police murder us.''. This score difference likely arises because the tool cannot understand the re-appropriation of the term ``n*gger''. One explanation for this bias is that AI models are trained on limited datasets, and using such a term in training data is more likely to appear in a toxic utterance. While this may be plausible, the tool will make mistakes regardless. Here, we study bias on two Web-based (YouTube and Twitter) datasets and two spoken English datasets. Our analysis shows how most models present biases towards AAE in most settings. We isolate the impact of AAE expression usage via linguistic control features from the Linguistic Inquiry and Word Count (LIWC) software, grammatical control features extracted via Part-of-Speech (PoS) tagging from Natural Language Processing (NLP) models, and the semantic of utterances by comparing sentence embeddings from recent language models. We present consistent results on how a heavy usage of AAE expressions may cause the speaker to be considered substantially more toxic, even when speaking about nearly the same subject. Our study complements similar analyses focusing on small datasets and/or one method only.</li>
<li><strong>摘要：</strong>语言是我们文化的一个动态方面，当在不同的技术/社区中表达时，语言会发生变化。在线社交网络促进了不同方言的传播和演变，其中包括非裔美国英语 (AAE)。然而，这种使用量的增加并非没有障碍。一个特殊的障碍是情感（Vader、TextBlob 和 Flair）和毒性（Google 的 Perspective 和开源 Detoxify）方法如何对 AAE 表达式的话语产生偏见。考虑一下谷歌的观点来理解偏见。在这里，有这样一句话：“所有黑人都应该有尊严地死去。”警察谋杀了我们。”它的毒性比“非裔美国人应该有尊严地死去”​​还要高。警察谋杀了我们。”出现这种分数差异可能是因为该工具无法理解术语“n*gger”的重新占用。对这种偏见的一种解释是，人工智能模型是在有限的数据集上进行训练的，在训练数据中使用这样的术语更有可能出现在有毒的话语中。虽然这可能是合理的，但该工具无论如何都会犯错误。在这里，我们研究两个基于网络（YouTube 和 Twitter）的数据集和两个英语口语数据集的偏见。我们的分析表明，大多数模型在大多数情况下都对 AAE 存在偏见。我们通过语言查询和字数统计 (LIWC) 软件中的语言控制特征、通过自然语言处理 (NLP) 模型中的词性 (PoS) 标记提取的语法控制特征以及语义来隔离 AAE 表达使用的影响。通过比较最近语言模型中的句子嵌入来分析话语。我们得出了一致的结果，表明大量使用 AAE 表达方式可能会导致说话者被认为更具毒性，即使在谈论几乎相同的主题时也是如此。我们的研究补充了专注于小数据集和/或仅一种方法的类似分析。</li>
</ul>

<h3>Title: DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for  Alleviating Over-squashing</h3>
<ul>
<li><strong>Authors: </strong>Li Sun, Zhenhao Huang, Hua Wu, Junda Ye, Hao Peng, Zhengtao Yu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12780">https://arxiv.org/abs/2401.12780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12780">https://arxiv.org/pdf/2401.12780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12780]] DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for  Alleviating Over-squashing(https://arxiv.org/abs/2401.12780)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have shown great power for learning and mining on graphs, and Graph Structure Learning (GSL) plays an important role in boosting GNNs with a refined graph. In the literature, most GSL solutions either primarily focus on structure refinement with task-specific supervision (i.e., node classification), or overlook the inherent weakness of GNNs themselves (e.g., over-squashing), resulting in suboptimal performance despite sophisticated designs. In light of these limitations, we propose to study self-supervised graph structure-feature co-refinement for effectively alleviating the issue of over-squashing in typical GNNs. In this paper, we take a fundamentally different perspective of the Ricci curvature in Riemannian geometry, in which we encounter the challenges of modeling, utilizing and computing Ricci curvature. To tackle these challenges, we present a self-supervised Riemannian model, DeepRicci. Specifically, we introduce a latent Riemannian space of heterogeneous curvatures to model various Ricci curvatures, and propose a gyrovector feature mapping to utilize Ricci curvature for typical GNNs. Thereafter, we refine node features by geometric contrastive learning among different geometric views, and simultaneously refine graph structure by backward Ricci flow based on a novel formulation of differentiable Ricci curvature. Finally, extensive experiments on public datasets show the superiority of DeepRicci, and the connection between backward Ricci flow and over-squashing. Codes of our work are given in https://github.com/RiemanGraph/.</li>
<li><strong>摘要：</strong>图神经网络（GNN）在图的学习和挖掘方面表现出了强大的能力，而图结构学习（GSL）在通过精细图提升 GNN 方面发挥着重要作用。在文献中，大多数 GSL 解决方案要么主要关注特定任务监督（即节点分类）的结构细化，要么忽视 GNN 本身固有的弱点（例如过度挤压），导致尽管设计复杂，但性能仍不理想。鉴于这些局限性，我们建议研究自监督图结构-特征协同细化，以有效缓解典型 GNN 中的过度挤压问题。在本文中，我们对黎曼几何中的里奇曲率采取了根本不同的视角，其中我们遇到了建模、利用和计算里奇曲率的挑战。为了应对这些挑战，我们提出了一种自监督黎曼模型 DeepRicci。具体来说，我们引入了异构曲率的潜在黎曼空间来对各种 Ricci 曲率进行建模，并提出了一种陀螺矢量特征映射以将 Ricci 曲率用于典型的 GNN。此后，我们通过不同几何视图之间的几何对比学习来细化节点特征，并同时通过基于可微里奇曲率的新颖公式的向后里奇流来细化图结构。最后，对公共数据集的大量实验表明了 DeepRicci 的优越性，以及向后 Ricci 流和过度挤压之间的联系。我们的工作代码在 https://github.com/RiemanGraph/ 中给出。</li>
</ul>

<h3>Title: A Review of Deep Learning Methods for Photoplethysmography Data</h3>
<ul>
<li><strong>Authors: </strong>Guangkun Nie, Jiabao Zhu, Gongzheng Tang, Deyun Zhang, Shijia Geng, Qinghao Zhao, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12783">https://arxiv.org/abs/2401.12783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12783">https://arxiv.org/pdf/2401.12783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12783]] A Review of Deep Learning Methods for Photoplethysmography Data(https://arxiv.org/abs/2401.12783)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Photoplethysmography (PPG) is a highly promising device due to its advantages in portability, user-friendly operation, and non-invasive capabilities to measure a wide range of physiological information. Recent advancements in deep learning have demonstrated remarkable outcomes by leveraging PPG signals for tasks related to personal health management and other multifaceted applications. In this review, we systematically reviewed papers that applied deep learning models to process PPG data between January 1st of 2017 and July 31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed from three key perspectives: tasks, models, and data. We finally extracted 193 papers where different deep learning frameworks were used to process PPG signals. Based on the tasks addressed in these papers, we categorized them into two major groups: medical-related, and non-medical-related. The medical-related tasks were further divided into seven subgroups, including blood pressure analysis, cardiovascular monitoring and diagnosis, sleep health, mental health, respiratory monitoring and analysis, blood glucose analysis, as well as others. The non-medical-related tasks were divided into four subgroups, which encompass signal processing, biometric identification, electrocardiogram reconstruction, and human activity recognition. In conclusion, significant progress has been made in the field of using deep learning methods to process PPG data recently. This allows for a more thorough exploration and utilization of the information contained in PPG signals. However, challenges remain, such as limited quantity and quality of publicly available databases, a lack of effective validation in real-world scenarios, and concerns about the interpretability, scalability, and complexity of deep learning models. Moreover, there are still emerging research areas that require further investigation.</li>
<li><strong>摘要：</strong>光电体积描记法（PPG）因其便携性、用户友好的操作以及非侵入性测量广泛生理信息的能力而成为一种非常有前途的设备。深度学习的最新进展通过利用 PPG 信号执行与个人健康管理和其他多方面应用相关的任务，取得了显着的成果。在这篇综述中，我们系统地回顾了 2017 年 1 月 1 日至 2023 年 7 月 31 日期间来自 Google Scholar、PubMed 和 Dimensions 的应用深度学习模型处理 PPG 数据的论文。每篇论文都从三个关键角度进行分析：任务、模型和数据。我们最终提取了 193 篇论文，其中使用不同的深度学习框架来处理 PPG 信号。根据这些论文中涉及的任务，我们将它们分为两大类：医疗相关的和非医疗相关的。医疗相关任务进一步分为七个子组，包括血压分析、心血管监测与诊断、睡眠健康、心理健康、呼吸监测与分析、血糖分析等。非医学相关任务分为四个子组，包括信号处理、生物特征识别、心电图重建和人类活动识别。总之，最近在使用深度学习方法处理 PPG 数据领域取得了重大进展。这样可以更彻底地探索和利用 PPG 信号中包含的信息。然而，挑战仍然存在，例如公开数据库的数量和质量有限，在现实场景中缺乏有效的验证，以及对深度学习模型的可解释性、可扩展性和复杂性的担忧。此外，仍有一些新兴研究领域需要进一步研究。</li>
</ul>

<h3>Title: Multilingual and Fully Non-Autoregressive ASR with Large Language Model  Fusion: A Comprehensive Study</h3>
<ul>
<li><strong>Authors: </strong>W. Ronny Huang, Cyril Allauzen, Tongzhou Chen, Kilol Gupta, Ke Hu, James Qin, Yu Zhang, Yongqiang Wang, Shuo-Yiin Chang, Tara N. Sainath</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12789">https://arxiv.org/abs/2401.12789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12789">https://arxiv.org/pdf/2401.12789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12789]] Multilingual and Fully Non-Autoregressive ASR with Large Language Model  Fusion: A Comprehensive Study(https://arxiv.org/abs/2401.12789)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>In the era of large models, the autoregressive nature of decoding often results in latency serving as a significant bottleneck. We propose a non-autoregressive LM-fused ASR system that effectively leverages the parallelization capabilities of accelerator hardware. Our approach combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment scoring mode, achieving an average relative WER improvement across all languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our comprehensive ablation study analyzes key parameters such as LLM size, context length, vocabulary size, fusion methodology. For instance, we explore the impact of LLM size ranging from 128M to 340B parameters on ASR performance. This study provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems.</li>
<li><strong>摘要：</strong>在大型模型时代，解码的自回归性质通常会导致延迟成为一个重要的瓶颈。我们提出了一种非自回归 LM 融合 ASR 系统，该系统有效地利用了加速器硬件的并行化功能。我们的方法在每段评分模式下结合了通用语音模型 (USM) 和 PaLM 2 语言模型，在 FLEURS 上实现了 10.8% 的平均相对 WER 改进，在 YouTube 字幕上实现了 3.6% 的平均相对 WER 改进。此外，我们全面的消融研究分析了法学硕士规模、上下文长度、词汇量、融合方法等关键参数。例如，我们探讨了从 128M 到 340B 参数范围的 LLM 大小对 ASR 性能的影响。这项研究为影响实际大规模 LM 融合语音识别系统有效性的因素提供了有价值的见解。</li>
</ul>

<h3>Title: Benchmarking LLMs via Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12794">https://arxiv.org/abs/2401.12794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12794">https://arxiv.org/pdf/2401.12794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12794]] Benchmarking LLMs via Uncertainty Quantification(https://arxiv.org/abs/2401.12794)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty into account, our new UAcc metric can either amplify or diminish the relative improvement of one LLM over another and may even change the relative ranking of two LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs.</li>
<li><strong>摘要：</strong>各个机构开源大型语言模型（LLM）的激增凸显了对综合评估方法的迫切需要。然而，目前的评估平台，例如广受认可的HuggingFace开放式LLM排行榜，忽略了一个至关重要的方面——不确定性，而这对于全面评估LLM至关重要。为了弥补这一差距，我们为法学硕士引入了一种新的基准测试方法，该方法集成了不确定性量化。我们的考试涉及八个法学硕士（法学硕士系列），涵盖五个代表性的自然语言处理任务。此外，我们引入了一种不确定性感知评估指标 UAcc，它同时考虑了预测准确性和预测不确定性。我们的研究结果表明： I) 准确度较高的法学硕士可能表现出较低的确定性； II) 与规模较小的法学硕士相比，规模较大的法学硕士可能表现出更大的不确定性； III) 指令微调往往会增加法学硕士的不确定性。通过考虑不确定性，我们的新 UAcc 指标可以放大或减少一个法学硕士相对于另一个法学硕士的相对进步，甚至可能改变两个法学硕士的相对排名。这些结果强调了在法学硕士评估中纳入不确定性的重要性。</li>
</ul>

<h3>Title: Binary structured physics-informed neural networks for solving equations  with rapidly changing solutions</h3>
<ul>
<li><strong>Authors: </strong>Yanzhi Liu, Ruifan Wu, Ying Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12806">https://arxiv.org/abs/2401.12806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12806">https://arxiv.org/pdf/2401.12806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12806]] Binary structured physics-informed neural networks for solving equations  with rapidly changing solutions(https://arxiv.org/abs/2401.12806)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features of solutions more effectively and efficiently. These features are particularly crucial for learning the rapidly changing in the nature of solutions. In a series of numerical experiments solving Burgers equation, Euler equation, Helmholtz equation, and high-dimension Poisson equation, BsPINNs exhibit superior convergence speed and heightened accuracy compared to PINNs. From these experiments, we discover that BsPINNs resolve the issues caused by increased hidden layers in PINNs resulting in over-smoothing, and prevent the decline in accuracy due to non-smoothness of PDEs solutions.</li>
<li><strong>摘要：</strong>植根于深度学习的物理信息神经网络 (PINN) 已成为解决偏微分方程 (PDE) 的一种有前途的方法。通过将偏微分方程描述的物理信息嵌入到前馈神经网络中，PINN 被训练为代理模型来近似解决方案，而无需标签数据。然而，尽管 PINN 表现出了卓越的性能，但它们也可能面临困难，特别是在处理具有快速变化解的方程时。这些困难包括收敛速度慢、容易陷入局部极小值以及求解精度降低。为了解决这些问题，我们提出了一种二元结构物理信息神经网络（BsPINN）框架，该框架采用二元结构神经网络（BsNN）作为神经网络组件。与完全连接的神经网络相比，BsPINN 利用减少神经元间连接的二元结构，能够更有效地捕捉解决方案的局部特征。这些特征对于了解解决方案本质的快速变化尤其重要。在求解伯格斯方程、欧拉方程、亥姆霍兹方程和高维泊松方程的一系列数值实验中，与 PINN 相比，BsPINN 表现出优越的收敛速度和更高的精度。从这些实验中，我们发现 BsPINN 解决了​​ PINN 中隐藏层增加导致过度平滑的问题，并防止了由于偏微分方程解不平滑而导致的精度下降。</li>
</ul>

<h3>Title: Dynamic Layer Tying for Parameter-Efficient Transformers</h3>
<ul>
<li><strong>Authors: </strong>Tamir David Hay, Lior Wolf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12819">https://arxiv.org/abs/2401.12819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12819">https://arxiv.org/pdf/2401.12819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12819]] Dynamic Layer Tying for Parameter-Efficient Transformers(https://arxiv.org/abs/2401.12819)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.</li>
<li><strong>摘要：</strong>为了减少深度变压器网络中可训练参数的数量，我们采用强化学习在训练期间动态选择层并将它们连接在一起。每隔几次迭代，RL 代理就会被询问是否独立训练每一层 $i$，还是复制前一层 $j<i$ 的权重。这有利于权重共享，减少可训练参数的数量，并且还可以作为一种有效的正则化技术。实验评估验证我们的模型在困惑度方面略胜于基线 Transformer 模型，并大大减少了可训练参数的数量。特别是，训练期间的内存消耗比传统训练方法减少了一个数量级。</li>
</ul>

<h3>Title: How well can large language models explain business processes?</h3>
<ul>
<li><strong>Authors: </strong>Dirk Fahland, Fabian Fournier, Lior Limonad, Inna Skarbovsky, Ava J.E. Swevels</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12846">https://arxiv.org/abs/2401.12846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12846">https://arxiv.org/pdf/2401.12846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12846]] How well can large language models explain business processes?(https://arxiv.org/abs/2401.12846)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are likely to play a prominent role in future AI-augmented business process management systems (ABPMSs) catering functionalities across all system lifecycle stages. One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and yet human-interpretable explanations that take into account the process context in which the explained condition occurred. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason, we pursued a methodological evaluation of the quality of the generated explanations. To this aim, we developed a designated scale and conducted a rigorous user study. Our findings show that the input presented to the LLMs aided with the guard-railing of its performance, yielding SAX explanations having better-perceived fidelity. This improvement is moderated by the perception of trust and curiosity. More so, this improvement comes at the cost of the perceived interpretability of the explanation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可能会在未来的人工智能增强业务流程管理系统 (ABPMS) 中发挥重要作用，以满足所有系统生命周期阶段的功能。此类系统的功能之一是情境感知可解释性 (SAX)，它涉及生成因果合理且人类可解释的解释，并考虑所解释条件发生的流程上下文。在本文中，我们介绍了为生成 SAX 解释而开发的 SAX4BPM 框架。 SAX4BPM 套件由一组服务和一个中央知识库组成。这些服务的功能是引出 SAX 解释背后的各种知识成分。这些要素中的一个关键创新组成部分是因果流程执行视图。在这项工作中，我们将该框架与法学硕士集成，利用其综合各种输入成分的能力，以改进 SAX 解释。由于使用 LLM 进行 SAX 也伴随着对其充分实现 SAX 的能力的一定程度的怀疑，以及其产生幻觉的倾向和缺乏内在推理能力，因此我们对所生成的解释的质量进行了方法论评估。为此，我们制定了指定量表并进行了严格的用户研究。我们的研究结果表明，向法学硕士提供的输入有助于提高其性能，从而产生具有更好感知保真度的 SAX 解释。这种改进受到信任和好奇心的影响。更重要的是，这种改进是以解释的可解释性为代价的。</li>
</ul>

<h3>Title: Learning safety critics via a non-contractive binary bellman operator</h3>
<ul>
<li><strong>Authors: </strong>Agustin Castellano, Hancheng Min, Juan Andrés Bazerque, Enrique Mallada</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12849">https://arxiv.org/abs/2401.12849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12849">https://arxiv.org/pdf/2401.12849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12849]] Learning safety critics via a non-contractive binary bellman operator(https://arxiv.org/abs/2401.12849)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag</a></li>
<li><strong>Abstract: </strong>The inability to naturally enforce safety in Reinforcement Learning (RL), with limited failures, is a core challenge impeding its use in real-world applications. One notion of safety of vast practical relevance is the ability to avoid (unsafe) regions of the state space. Though such a safety goal can be captured by an action-value-like function, a.k.a. safety critics, the associated operator lacks the desired contraction and uniqueness properties that the classical Bellman operator enjoys. In this work, we overcome the non-contractiveness of safety critic operators by leveraging that safety is a binary property. To that end, we study the properties of the binary safety critic associated with a deterministic dynamical system that seeks to avoid reaching an unsafe region. We formulate the corresponding binary Bellman equation (B2E) for safety and study its properties. While the resulting operator is still non-contractive, we fully characterize its fixed points representing--except for a spurious solution--maximal persistently safe regions of the state space that can always avoid failure. We provide an algorithm that, by design, leverages axiomatic knowledge of safe data to avoid spurious fixed points.</li>
<li><strong>摘要：</strong>强化学习（RL）无法自然地加强安全性，并且故障有限，这是阻碍其在实际应用中使用的核心挑战。具有广泛实际意义的安全概念之一是避免状态空间（不安全）区域的能力。尽管这样的安全目标可以通过类似于动作值的函数（又名安全批评家）来捕获，但相关的算子缺乏经典贝尔曼算子所享有的所需的收缩性和唯一性属性。在这项工作中，我们通过利用安全性是二元属性来克服安全批评算子的非收缩性。为此，我们研究了与确定性动态系统相关的二元安全批评器的属性，该系统旨在避免到达不安全区域。我们制定了相应的安全二元贝尔曼方程（B2E）并研究了其性质。虽然得到的算子仍然是非收缩性的，但我们充分描述了它的不动点的特征，除了虚假的解决方案之外，状态空间的最大持久安全区域总是可以避免失败。我们提供了一种算法，通过设计，利用安全数据的公理知识来避免虚假的固定点。</li>
</ul>

<h3>Title: KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, Godawari Sudhakar Rao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12863">https://arxiv.org/abs/2401.12863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12863">https://arxiv.org/pdf/2401.12863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12863]] KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning(https://arxiv.org/abs/2401.12863)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers. Experimental findings show KAM-CoT outperforms the state-of-the-art methods. On the ScienceQA dataset, we achieve an average accuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by 10%. Remarkably, KAM-CoT achieves these results with only 280M trainable parameters at a time, demonstrating its cost-efficiency and effectiveness.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过利用支持逐步思考的思维链 (CoT)，在自然语言处理任务中展现了令人印象深刻的性能。利用多模式功能扩展法学硕士是最近的兴趣，但会产生计算成本并且需要大量的硬件资源。为了应对这些挑战，我们提出了 KAM-CoT 一个框架，该框架集成了 CoT 推理、知识图（KG）和多种模态，以全面理解多模态任务。 KAM-CoT 采用以 KG 为基础的两阶段训练过程，以产生有效的理由和答案。通过在推理过程中结合 KG 的外部知识，模型获得了更深入的上下文理解，减少了幻觉并提高了答案的质量。这种知识增强的 CoT 推理使模型能够处理需要外部上下文的问题，从而提供更明智的答案。实验结果表明 KAM-CoT 优于最先进的方法。在 ScienceQA 数据集上，我们的平均准确率达到 93.87%，比 GPT-3.5 (75.17%) 提高了 18%，比 GPT-4 (83.99%) 提高了 10%。值得注意的是，KAM-CoT 一次仅用 280M 个可训练参数就实现了这些结果，证明了其成本效益和有效性。</li>
</ul>

<h3>Title: Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported  Coordination of Mobile Crowdsourcing</h3>
<ul>
<li><strong>Authors: </strong>Ralf Bruns, Jeremias Dötterl, Jürgen Dunkel, Sascha Ossowski</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12866">https://arxiv.org/abs/2401.12866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12866">https://arxiv.org/pdf/2401.12866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12866]] Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported  Coordination of Mobile Crowdsourcing(https://arxiv.org/abs/2401.12866)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on the suggested prediction model, we propose and evaluate two different approaches for task coordination with different degrees of autonomy: an opportunistic approach for crowdshipping with collaborative, but non-autonomous workers, and a market-based model with autonomous workers for crowdsensing.</li>
<li><strong>摘要：</strong>移动众包是指完成任务必然需要众包工作者在按需劳动力中进行身体移动的系统。有证据表明，在此类系统中，任务通常会分配给难以成功完成这些任务的众包人员，从而导致高失败率和低服务质量。确保更高服务质量的一个有前途的解决方案是不断调整分配并通过将任务转移给使用不同路线或车辆的更适合的工人来响应导致故障的事件。然而，在移动众包中实现任务转移很困难，因为工作人员是自主的并且可能会拒绝转移请求。此外，任务结果是不确定的，需要预测。在本文中，我们提出了不同的机制来实现移动众包中的结果预测和任务协调。首先，我们分析用于预测任务结果的不同数据流学习方法。其次，根据建议的预测模型，我们提出并评估了两种不同的具有不同自主程度的任务协调方法：一种与协作但非自主的工人进行众包的机会主义方法，以及一种基于市场的由自主工人进行群体感知的模型。</li>
</ul>

<h3>Title: TroVE: Inducing Verifiable and Efficient Toolboxes for Solving  Programmatic Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zhiruo Wang, Daniel Fried, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12869">https://arxiv.org/abs/2401.12869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12869">https://arxiv.org/pdf/2401.12869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12869]] TroVE: Inducing Verifiable and Efficient Toolboxes for Solving  Programmatic Tasks(https://arxiv.org/abs/2401.12869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code</a></li>
<li><strong>Abstract: </strong>Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics.</li>
<li><strong>摘要：</strong>语言模型（LM）可以通过编写程序来解决诸如回答有关表格或图像的问题等任务。然而，使用原始函数通常会导致程序冗长且容易出错，而更高级别的函数需要专家设计。为了在无需人工的情况下实现更好的解决方案，我们要求代码语言模型来策划可重用的高级函数，并使用它们来编写解决方案。我们提出了 TROVE，一种无需训练的方法，通过使用、增长和定期修剪工具箱来生成可验证且高效的函数工具箱。在来自数学、表格问答和图像推理任务的 11 个数据集上，TROVE 始终能够产生比使用 ​​CODELLAMA 的基线和使用 GPT 的先前方法更简单、精度更高的解决方案，同时使用的工具箱小了 79-98%。 TROVE 的人工验证速度比基线快 31%，准确度高 13%。使用相同的管道，它可以为不同的任务和数据集创建不同的函数，从而深入了解它们的各自特征。</li>
</ul>

<h3>Title: Improving Machine Translation with Human Feedback: An Exploration of  Quality Estimation as a Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12873">https://arxiv.org/abs/2401.12873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12873">https://arxiv.org/pdf/2401.12873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12873]] Improving Machine Translation with Human Feedback: An Exploration of  Quality Estimation as a Reward Model(https://arxiv.org/abs/2401.12873)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the QE-based rewards for the detected incorrect translations. Experimental results show that the proposed QE-based feedback training achieves consistent and significant improvements across various settings, further verified through human preference studies. Our subsequent analysis demonstrates the high data efficiency of the proposed QE-based feedback training: the proposed approach using a small amount of monolingual data can outperform systems using larger parallel corpora.</li>
<li><strong>摘要：</strong>奖励模型中对人类偏好的建模不足是利用人类反馈来提高翻译质量的主要障碍。幸运的是，质量估计（QE）无需参考即可预测给定翻译的质量，在过去两年中与人类评估取得了令人印象深刻的一致性。在这项工作中，我们研究了使用 QE 模型作为奖励模型（基于 QE 的奖励模型）来预测人类对反馈训练的偏好的潜力。我们首先确定基于 QE 的反馈训练期间的过度优化问题，表现为奖励增加而翻译质量下降。我们研究了这个问题，并认为 QE 模型的漏洞可能会导致错误翻译获得高额奖励，从而导致过度优化和错误传播。为了解决这个问题，我们采用了一种简单而有效的方法，该方法使用启发式规则来检测不正确的翻译，并为检测到的不正确翻译的基于 QE 的奖励分配惩罚项。实验结果表明，所提出的基于 QE 的反馈训练在各种设置中实现了一致且显着的改进，并通过人类偏好研究进一步得到验证。我们随后的分析证明了所提出的基于 QE 的反馈训练的高数据效率：所提出的使用少量单语数据的方法可以优于使用较大并行语料库的系统。</li>
</ul>

<h3>Title: From Understanding to Utilization: A Survey on Explainability for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyan Luo, Lucia Specia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12874">https://arxiv.org/abs/2401.12874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12874">https://arxiv.org/pdf/2401.12874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12874]] From Understanding to Utilization: A Survey on Explainability for Large  Language Models(https://arxiv.org/abs/2401.12874)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering insights for future research and development in the field of LLM explainability.</li>
<li><strong>摘要：</strong>这篇调查论文深入研究了大型语言模型（LLM）的可解释性这一新兴领域，这是自然语言处理的一个关键但具有挑战性的方面。由于法学硕士在各种应用中发挥着关键作用，其“黑匣子”性质引起了人们对透明度和道德使用的担忧。本文强调了增强法学硕士可解释性的必要性，满足公众的信任和技术社区对这些模型更深入理解的需求。我们专注于基于 Transformer 的预训练法学硕士，例如 LLaMA，由于其规模和复杂性，它们带来了独特的可解释性挑战。我们的评论对现有的可解释性方法进行了分类，并讨论了它们在提高模型透明度和可靠性方面的应用。我们还讨论了代表性的评估方法，强调了它们的优点和局限性。这项调查的目的是弥合理论理解和实际应用之间的差距，为法学硕士可解释性领域的未来研究和发展提供见解。</li>
</ul>

<h3>Title: Model-Free $δ$-Policy Iteration Based on Damped Newton Method for  Nonlinear Continuous-Time H$\infty$ Tracking Control</h3>
<ul>
<li><strong>Authors: </strong>Qi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12882">https://arxiv.org/abs/2401.12882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12882">https://arxiv.org/pdf/2401.12882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12882]] Model-Free $δ$-Policy Iteration Based on Damped Newton Method for  Nonlinear Continuous-Time H$\infty$ Tracking Control(https://arxiv.org/abs/2401.12882)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>This paper presents a {\delta}-PI algorithm which is based on damped Newton method for the H{\infty} tracking control problem of unknown continuous-time nonlinear system. A discounted performance function and an augmented system are used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation. Tracking HJI equation is a nonlinear partial differential equation, traditional reinforcement learning methods for solving the tracking HJI equation are mostly based on the Newton method, which usually only satisfies local convergence and needs a good initial guess. Based upon the damped Newton iteration operator equation, a generalized tracking Bellman equation is derived firstly. The {\delta}-PI algorithm can seek the optimal solution of the tracking HJI equation by iteratively solving the generalized tracking Bellman equation. On-policy learning and off-policy learning {\delta}-PI reinforcement learning methods are provided, respectively. Off-policy version {\delta}-PI algorithm is a model-free algorithm which can be performed without making use of a priori knowledge of the system dynamics. NN-based implementation scheme for the off-policy {\delta}-PI algorithms is shown. The suitability of the model-free {\delta}-PI algorithm is illustrated with a nonlinear system simulation.</li>
<li><strong>摘要：</strong>针对未知连续时间非线性系统的H{\infty}跟踪控制问题，提出一种基于阻尼牛顿法的{\delta}-PI算法。使用折扣性能函数和增强系统来获得跟踪 Hamilton-Jacobi-Isaac (HJI) 方程。跟踪HJI方程是一个非线性偏微分方程，传统的求解跟踪HJI方程的强化学习方法大多基于牛顿法，通常只满足局部收敛，需要良好的初始猜测。基于阻尼牛顿迭代算子方程，首先推导了广义跟踪贝尔曼方程。 {δ}-PI算法可以通过迭代求解广义跟踪Bellman方程来寻求跟踪HJI方程的最优解。分别提供了在策略学习和离策略学习{\delta}-PI强化学习方法。离策略版本 {\delta}-PI 算法是一种无模型算法，可以在不使用系统动力学先验知识的情况下执行。显示了基于神经网络的离策略 {\delta}-PI 算法的实现方案。通过非线性系统仿真说明了无模型δ-PI算法的适用性。</li>
</ul>

<h3>Title: Red Teaming Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12915">https://arxiv.org/abs/2401.12915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12915">https://arxiv.org/pdf/2401.12915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12915]] Red Teaming Visual Language Models(https://arxiv.org/abs/2401.12915)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.</li>
<li><strong>摘要：</strong>VLM（视觉语言模型）扩展了 LLM（大型语言模型）的功能，以接受多模式输入。由于已经证实 LLM 可以通过特定的测试用例（称为红队）诱导生成有害或不准确的内容，因此 VLM 在类似场景中的表现如何，特别是在文本和视觉输入相结合的情况下，仍然是一个问题。为了探讨这个问题，我们提出了一个新颖的红队数据集 RTVLM，它包含 4 个主要方面（忠实、隐私、安全、公平）下的 10 个子任务（例如图像误导、多模式越狱、人脸公平等）。我们的 RTVLM 是第一个在这 4 个不同方面对当前 VLM 进行基准测试的红队数据集。详细分析显示，10 个著名的开源 VLM 都不同程度地与红队作斗争，与 GPT-4V 的性能差距高达 31%。此外，我们简单地使用 RTVLM 将红队对齐与监督微调 (SFT) 应用于 LLaVA-v1.5，这增强了模型的性能，在 RTVLM 测试集中提高了 10%，在 MM-Hal 中提高了 13%，并且没有明显的影响MM-Bench 的下降，超过了具有常规对齐数据的其他基于 LLaVA 的模型。这表明当前的开源 VLM 仍然缺乏红队协调。我们的代码和数据集将是开源的。</li>
</ul>

<h3>Title: Active Inference as a Model of Agency</h3>
<ul>
<li><strong>Authors: </strong>Lancelot Da Costa, Samuel Tenka, Dominic Zhao, Noor Sajid</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12917">https://arxiv.org/abs/2401.12917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12917">https://arxiv.org/pdf/2401.12917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12917]] Active Inference as a Model of Agency(https://arxiv.org/abs/2401.12917)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Is there a canonical way to think of agency beyond reward maximisation? In this paper, we show that any type of behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about states of the world. This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception originating in neuroscience. Active inference provides a normative Bayesian framework to simulate and model agency that is widely used in behavioural neuroscience, reinforcement learning (RL) and robotics. The usefulness of active inference for RL is three-fold. \emph{a}) Active inference provides a principled solution to the exploration-exploitation dilemma that usefully simulates biological agency. \emph{b}) It provides an explainable recipe to simulate behaviour, whence behaviour follows as an explainable mixture of exploration and exploitation under a generative world model, and all differences in behaviour are explicit in differences in world model. \emph{c}) This framework is universal in the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an active inference algorithm. Thus, active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency.</li>
<li><strong>摘要：</strong>除了奖励最大化之外，是否有一种规范的方式来思考代理？在本文中，我们表明，任何类型的行为都符合关于宏观生物体如何与世界相互作用的物理合理假设，在最大限度地减少世界状态的风险和模糊性的意义上，规范地整合了探索和利用。这种描述被称为主动推理，它完善了自由能原理，这是一种起源于神经科学的流行的动作和感知描述框架。主动推理提供了一个规范的贝叶斯框架来模拟和建模代理，广泛应用于行为神经科学、强化学习 (RL) 和机器人技术。强化学习的主动推理有三重用途。 \emph{a}）主动推理为有效模拟生物机构的探索-利用困境提供了原则性的解决方案。 \emph{b}）它提供了一个可解释的方法来模拟行为，因此行为是在生成世界模型下探索和利用的可解释的混合体，并且所有行为差异在世界模型的差异中都是明确的。 \emph{c}) 这个框架是通用的，因为理论上可以将任何符合主动推理描述性假设的强化学习算法重写为主动推理算法。因此，主动推理可以用作揭示和比较更具体的代理模型的承诺和假设的工具。</li>
</ul>

<h3>Title: Truck Parking Usage Prediction with Decomposed Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Rei Tamaru, Yang Cheng, Steven Parker, Ernie Perry, Bin Ran, Soyoung Ahn</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12920">https://arxiv.org/abs/2401.12920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12920">https://arxiv.org/pdf/2401.12920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12920]] Truck Parking Usage Prediction with Decomposed Graph Neural Networks(https://arxiv.org/abs/2401.12920)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Truck parking on freight corridors faces various challenges, such as insufficient parking spaces and compliance with Hour-of-Service (HOS) regulations. These constraints often result in unauthorized parking practices, causing safety concerns. To enhance the safety of freight operations, providing accurate parking usage prediction proves to be a cost-effective solution. Despite the existing research demonstrating satisfactory accuracy for predicting individual truck parking site usage, few approaches have been proposed for predicting usage with spatial dependencies of multiple truck parking sites. We present the Regional Temporal Graph Neural Network (RegT-GCN) as a predictive framework for assessing parking usage across the entire state to provide better truck parking information and mitigate unauthorized parking. The framework leverages the topological structures of truck parking site distributions and historical parking data to predict occupancy rates across a state. To achieve this, we introduce a Regional Decomposition approach, which effectively captures the geographical characteristics. We also introduce the spatial module working efficiently with the temporal module. Evaluation results demonstrate that the proposed model surpasses other baseline models, improving the performance by more than $20\%$ compared with the original model. The proposed model allows truck parking sites' percipience of the topological structures and provides higher performance.</li>
<li><strong>摘要：</strong>货运走廊上的卡车停车面临着各种挑战，例如停车位不足和遵守服务时间（HOS）法规。这些限制通常会导致未经授权的停车行为，从而引发安全问题。为了提高货运运营的安全性，提供准确的停车位使用预测被证明是一种经济高效的解决方案。尽管现有研究表明预测单个卡车停车位使用情况的准确性令人满意，但很少有人提出方法来预测多个卡车停车位空间依赖性的使用情况。我们提出区域时态图神经网络 (RegT-GCN) 作为预测框架，用于评估整个州的停车使用情况，以提供更好的卡车停车信息并减少未经授权的停车。该框架利用卡车停车位分布的拓扑结构和历史停车数据来预测整个州的占用率。为了实现这一目标，我们引入了区域分解方法，该方法有效地捕捉了地理特征。我们还介绍了与时间模块有效配合的空间模块。评估结果表明，所提出的模型超越了其他基线模型，与原始模型相比，性能提高了超过 $20\%$。所提出的模型允许卡车停车场感知拓扑结构并提供更高的性能。</li>
</ul>

<h3>Title: DsDm: Model-Aware Dataset Selection with Datamodels</h3>
<ul>
<li><strong>Authors: </strong>Logan Engstrom, Axel Feldmann, Aleksander Madry</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12926">https://arxiv.org/abs/2401.12926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12926">https://arxiv.org/pdf/2401.12926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12926]] DsDm: Model-Aware Dataset Selection with Datamodels(https://arxiv.org/abs/2401.12926)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with "high quality" data sources may not increase (and can even hurt) performance compared to randomly selecting data. To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.</li>
<li><strong>摘要：</strong>在选择用于训练大型模型的数据时，标准做法是过滤与人类数据质量概念相匹配的示例。这种过滤会产生高质量的干净数据点，直观上应该可以改善模型行为。然而，在实践中，经常会发生相反的情况：我们发现，与随机选择数据相比，根据“高质量”数据源的相似性进行选择可能不会提高（甚至可能会损害）性能。为了开发更好的数据选择方法，我们首先将数据集选择框架为我们可以直接解决的优化问题：给定目标任务、学习算法和候选数据，选择最大化模型性能的子集。因此，该框架避免了精心挑选的数据质量概念，而是明确地建模了学习过程如何使用训练数据点来预测目标任务。我们的方法极大地提高了语言模型（LM）在预先指定的任务和以前未见过的任务上的性能。具体来说，选择代表标准 LM 问题的目标任务并评估各种保留的基准，我们选择的数据集提供了比基线方法高 2 倍的计算乘数。</li>
</ul>

<h3>Title: Transformer-Based Models Are Not Yet Perfect At Learning to Emulate  Structural Recursion</h3>
<ul>
<li><strong>Authors: </strong>Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, Talia Ringer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.FL, cs.LO, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12947">https://arxiv.org/abs/2401.12947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12947">https://arxiv.org/pdf/2401.12947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12947]] Transformer-Based Models Are Not Yet Perfect At Learning to Emulate  Structural Recursion(https://arxiv.org/abs/2401.12947)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture. With our framework as a powerful conceptual tool, we identify different issues under various set-ups. The models trained to emulate recursive computations cannot fully capture the recursion yet instead fit short-cut algorithms and thus cannot solve certain edge cases that are under-represented in the training distribution. In addition, it is difficult for state-of-the-art large language models (LLMs) to mine recursive rules from in-context demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating reduction (step-wise computation) of the recursive function.</li>
<li><strong>摘要：</strong>本文研究了基于 Transformer 的模型从示例中学习结构递归的能力。递归是自然语言和形式语言中的普遍概念。结构递归是编程语言和形式数学任务的核心，符号工具目前在神经模型之外表现出色，例如推断数据类型之间的语义关系和模拟程序行为。我们引入了一个通用框架，它将编程语言领域中结构递归的抽象概念与具体的序列建模问题和学习模型的行为很好地联系起来。该框架包括一种捕获结构递归的一般 \textit{syntax} 的表示，以及两个不同的框架来理解它们的 \textit{semantics} ——一个从编程语言的角度来看更自然，另一个有助于弥合这一观点对底层变压器架构有机械的理解。借助我们的框架作为强大的概念工具，我们可以识别不同设置下的不同问题。经过训练来模拟递归计算的模型无法完全捕获递归，而是适合快捷算法，因此无法解决训练分布中代表性不足的某些边缘情况。此外，最先进的大型语言模型（LLM）很难从上下文演示中挖掘递归规则。与此同时，这些法学硕士在模拟递归函数的约简（逐步计算）时会以有趣的方式失败。</li>
</ul>

<h3>Title: Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding</h3>
<ul>
<li><strong>Authors: </strong>Mirac Suzgun, Adam Tauman Kalai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12954">https://arxiv.org/abs/2401.12954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12954">https://arxiv.org/pdf/2401.12954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12954]] Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding(https://arxiv.org/abs/2401.12954)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, rag</a></li>
<li><strong>Abstract: </strong>We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.</li>
<li><strong>摘要：</strong>我们引入元提示，这是一种有效的脚手架技术，旨在增强语言模型（LM）的功能。这种方法将单个 LM 转变为多面导体，擅长管理和集成多个独立的 LM 查询。通过使用高级指令，元提示引导 LM 将复杂的任务分解为更小、更易于管理的子任务。然后，这些子任务由同一 LM 的不同“专家”实例处理，每个实例都在特定的、定制的指令下运行。这个过程的核心是语言模型本身，它作为指挥者，确保无缝通信和这些专家模型输出的有效集成。它还利用其固有的批判性思维和强大的验证流程来完善和验证最终结果。这种协作提示方法使单个 LM 能够同时充当综合协调者和多元化专家小组，从而显着提高其在各种任务中的绩效。元提示的零样本、任务无关的性质通过消除对详细的、特定于任务的指令的需要，极大地简化了用户交互。此外，我们的研究表明，外部工具（例如Python解释器）可以无缝集成到元提示框架中，从而扩大了其适用性和实用性。通过 GPT-4 的严格实验，我们确立了元提示相对于传统脚手架方法的优越性：当对所有任务（包括 24 人游戏、Checkmate-in-One 和 Python 编程难题）进行平均时，元提示增强了Python 解释器功能，超出标准提示 17.1%，超出专家（动态）提示 17.3%，超出多人提示 15.2%。</li>
</ul>

<h3>Title: Raidar: geneRative AI Detection viA Rewriting</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Mao, Carl Vondrick, Hao Wang, Junfeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12970">https://arxiv.org/abs/2401.12970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12970">https://arxiv.org/pdf/2401.12970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12970]] Raidar: geneRative AI Detection viA Rewriting(https://arxiv.org/abs/2401.12970)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar. Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.</li>
<li><strong>摘要：</strong>我们发现，在执行重写任务时，大型语言模型 (LLM) 比人工智能生成的文本更有可能修改人类编写的文本。这种趋势的出现是因为法学硕士通常认为人工智能生成的文本质量很高，从而减少了修改。我们引入了一种通过提示法学硕士重写文本并计算输出的编辑距离来检测人工智能生成的内容的方法。我们将我们的生成人工智能检测通过重写方法称为Raidar。 Raidar 显着提高了现有人工智能内容检测模型（包括学术和商业）的 F1 检测分数，涉及多个领域，包括新闻、创意写作、学生论文、代码、Yelp 评论和 arXiv 论文，涨幅高达 29 分。我们的方法仅对没有高维特征的单词符号进行操作，与黑盒法学硕士兼容，并且对新内容具有固有的鲁棒性。我们的结果通过机器本身的镜头展示了机器生成文本的独特印记。</li>
</ul>

<h3>Title: In-Context Language Learning: Arhitectures and Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Ekin Akyürek, Bailin Wang, Yoon Kim, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12973">https://arxiv.org/abs/2401.12973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12973">https://arxiv.org/pdf/2401.12973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12973]] In-Context Language Learning: Arhitectures and Algorithms(https://arxiv.org/abs/2401.12973)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the "real" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including several RNNs, Transformers, and state-space model variants) on regular ICLL tasks, aiming to answer three questions: (1) Which model classes are empirically capable of ICLL? (2) What algorithmic solutions do successful models implement to perform ICLL? (3) What architectural changes can improve ICLL in less performant models? We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks. Next, we provide evidence that their ability to do so relies on specialized "n-gram heads" (higher-order variants of induction heads) that compute input-conditional next-token distributions. Finally, we show that hard-wiring these heads into recurrent and convolutional models improves performance not just on ICLL, but natural language modeling -- improving the perplexity of 340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.</li>
<li><strong>摘要：</strong>大规模神经语言模型表现出非凡的上下文学习（ICL）能力：它们可以从作为输入提供的数据集中推断新功能。我们目前对 ICL 何时以及如何产生的大部分理解都来自于接受过线性回归和联想回忆等极其简单的学习问题训练的 LM。这些模型问题与在大型文本语料库上训练的 LM 所展示的“真实”ICL 之间仍然存在显着差距，其中不仅涉及检索和函数逼近，还涉及语言和其他结构化输出的自由形式生成。在本文中，我们通过一系列新的模型问题（我们称之为情境语言学习（ICLL））的视角来研究 ICL。在 ICLL 中，LM 会收到一组来自形式语言的字符串，并且必须从同一语言生成其他字符串。我们专注于随机有限自动机生成的常规语言的上下文学习。我们在常规 ICLL 任务上评估了一组不同的神经序列模型（包括几个 RNN、Transformers 和状态空间模型变体），旨在回答三个问题：（1）哪些模型类在经验上能够执行 ICLL？ (2) 成功的模型采用什么算法解决方案来执行 ICLL？ (3) 哪些架构变化可以改善性能较差模型中的 ICLL？我们首先证明 Transformer 在 ICLL 任务上的性能显着优于具有循环或卷积表示的神经序列模型。接下来，我们提供证据表明，他们这样做的能力依赖于计算输入条件下一个令牌分布的专门“n-gram 头”（归纳头的高阶变体）。最后，我们表明，将这些头硬连接到循环模型和卷积模型中不仅可以提高 ICLL 的性能，还可以提高自然语言建模的性能——在 SlimPajama 数据集上将 340M 参数模型的困惑度提高多达 1.14 个点 (6.7%)。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
