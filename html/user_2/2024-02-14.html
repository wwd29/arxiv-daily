<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-14</h1>
<h3>Title: QACP: An Annotated Question Answering Dataset for Assisting Chinese  Python Programming Learners</h3>
<ul>
<li><strong>Authors: </strong>Rui Xiao, Lu Han, Xiaoying Zhou, Jiong Wang, Na Zong, Pengyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07913">https://arxiv.org/abs/2402.07913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07913">https://arxiv.org/pdf/2402.07913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07913]] QACP: An Annotated Question Answering Dataset for Assisting Chinese  Python Programming Learners(https://arxiv.org/abs/2402.07913)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing the programming teaching assists (TA). Furthermore, we conducted comprehensive evaluations of various LLMs proficient in processing and generating Chinese content, highlighting the potential limitations of general LLMs as intelligent teaching assistants in computer programming courses.</li>
<li><strong>摘要：</strong>在在线学习平台中，特别是在快速增长的计算机编程课程中，解决成千上万学生的学习查询需要相当大的人力成本。为编程教育量身定制的智能助理大语言模型（LLM）的创建需要独特的数据支持。然而，在实际应用场景中，用于训练此类LLM的数据资源相对匮乏。因此，针对编程智能教育系统中数据稀缺的问题，本文提出了一种新的Python学习者中文问答数据集。为保证试题来源的真实性和可靠性，我们从学生实际问题中收集试题，并按照试题类型、学习者类型等多个维度进行分类。该标注原则旨在提高在线编程教育的有效性和质量，为开发编程教学辅助工具（TA）提供坚实的数据基础。此外，我们对精通中文内容处理和生成的各类法学硕士进行了综合评估，突显了一般法学硕士作为计算机编程课程智能助教的潜在局限性。</li>
</ul>

<h3>Title: Point and Instruct: Enabling Precise Image Editing by Unifying Direct  Manipulation and Text Instructions</h3>
<ul>
<li><strong>Authors: </strong>Alec Helbling, Seongmin Lee, Polo Chau</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07925">https://arxiv.org/abs/2402.07925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07925">https://arxiv.org/pdf/2402.07925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07925]] Point and Instruct: Enabling Precise Image Editing by Unifying Direct  Manipulation and Text Instructions(https://arxiv.org/abs/2402.07925)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Machine learning has enabled the development of powerful systems capable of editing images from natural language instructions. However, in many common scenarios it is difficult for users to specify precise image transformations with text alone. For example, in an image with several dogs, it is difficult to select a particular dog and move it to a precise location. Doing this with text alone would require a complex prompt that disambiguates the target dog and describes the destination. However, direct manipulation is well suited to visual tasks like selecting objects and specifying locations. We introduce Point and Instruct, a system for seamlessly combining familiar direct manipulation and textual instructions to enable precise image manipulation. With our system, a user can visually mark objects and locations, and reference them in textual instructions. This allows users to benefit from both the visual descriptiveness of natural language and the spatial precision of direct manipulation.</li>
<li><strong>摘要：</strong>机器学习使得能够根据自然语言指令编辑图像的强大系统的开发成为可能。然而，在许多常见场景中，用户很难仅用文本来指定精确的图像转换。例如，在具有多只狗的图像中，很难选择特定的狗并将其移动到精确的位置。仅使用文本来完成此操作将需要一个复杂的提示来消除目标狗的歧义并描述目的地。然而，直接操作非常适合视觉任务，例如选择对象和指定位置。我们推出了 Point and Instruct，这是一个无缝结合熟悉的直接操作和文本指令以实现精确图像操作的系统。通过我们的系统，用户可以直观地标记对象和位置，并在文本说明中引用它们。这使用户能够受益于自然语言的视觉描述性和直接操作的空间精度。</li>
</ul>

<h3>Title: A Systematic Survey of Prompt Engineering in Large Language Models:  Techniques and Applications</h3>
<ul>
<li><strong>Authors: </strong>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07927">https://arxiv.org/abs/2402.07927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07927">https://arxiv.org/pdf/2402.07927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07927]] A Systematic Survey of Prompt Engineering in Large Language Models:  Techniques and Applications(https://arxiv.org/abs/2402.07927)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.</li>
<li><strong>摘要：</strong>快速工程已成为扩展大型语言模型 (LLM) 和视觉语言模型 (VLM) 功能不可或缺的技术。这种方法利用特定于任务的指令（称为提示）来增强模型功效，而无需修改核心模型参数。提示不是更新模型参数，而是仅根据给定的提示引发所需的模型行为，从而将预先训练的模型无缝集成到下游任务中。提示可以是提供上下文来指导模型的自然语言指令，也可以是激活相关知识的学习向量表示。这个新兴领域已经在从问答到常识推理的各种应用中取得了成功。然而，对于各种快速工程方法和技术仍然缺乏系统的组织和理解。本调查论文通过按应用领域分类，对即时工程的最新进展进行了结构化概述，从而弥补了这一差距。对于每种提示方法，我们提供了一个摘要，详细说明了提示方法、其应用、涉及的模型以及所使用的数据集。我们还深入研究了每种方法的优点和局限性，并提供了分类图和表格，总结了每种提示技术的数据集、模型和关键点。这种系统分析使人们能够更好地了解这个快速发展的领域，并通过阐明快速工程的开放挑战和机遇来促进未来的研究。</li>
</ul>

<h3>Title: SMX: Sequential Monte Carlo Planning for Expert Iteration</h3>
<ul>
<li><strong>Authors: </strong>Matthew V Macfarlane, Edan Toledo, Donal Byrne, Siddarth Singh, Paul Duckworth, Alexandre Laterre</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.07963">https://arxiv.org/abs/2402.07963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.07963">https://arxiv.org/pdf/2402.07963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.07963]] SMX: Sequential Monte Carlo Planning for Expert Iteration(https://arxiv.org/abs/2402.07963)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on hardware accelerators to optimise computing efficiency. SMX demonstrates a statistically significant improvement in performance compared to AlphaZero, as well as demonstrating its performance as an improvement operator for a model-free policy, matching or exceeding top model-free methods across both continuous and discrete environments.</li>
<li><strong>摘要：</strong>开发能够在决策和学习过程中利用规划能力的代理对于人工智能的进步至关重要。最近的工作证明了基于树的搜索方法和自我游戏学习机制相结合的有效性。然而，由于搜索的顺序性，这些方法通常面临扩展挑战。虽然实际的工程解决方案可以部分克服这个问题，但它们仍然需要大量的计算资源，这阻碍了它们的适用性。在本文中，我们介绍了 SMX，一种基于模型的规划算法，它利用可扩展的顺序蒙特卡罗方法来创建有效的自学习机制。 SMX 以控制作为推理的理论框架为基础，受益于强大的理论基础。其基于采样的搜索方法使其能够适应具有离散和连续动作空间的环境。此外，SMX 允许高度并行化，并且可以在硬件加速器上运行以优化计算效率。与 AlphaZero 相比，SMX 展示了统计上显着的性能改进，并展示了其作为无模型策略的改进算子的性能，在连续和离散环境中匹配或超过了顶级无模型方法。</li>
</ul>

<h3>Title: Refined Direct Preference Optimization with Synthetic Data for  Behavioral Alignment of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Víctor Gallego</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08005">https://arxiv.org/abs/2402.08005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08005">https://arxiv.org/pdf/2402.08005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08005]] Refined Direct Preference Optimization with Synthetic Data for  Behavioral Alignment of LLMs(https://arxiv.org/abs/2402.08005)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce \emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 \emph{refined Direct Preference Optimization} (rDPO)，这是一种无需人工注释数据即可改进大型语言模型 (LLM) 行为对齐的方法。该方法涉及使用 LLM 教师的自我批评提示来创建合成数据，然后利用广义 DPO 损失函数将其提炼为 LLM 学生。损失函数包含一个额外的外部奖励模型，以提高合成数据的质量，使 rDPO 对合成数据集中的潜在噪声具有鲁棒性。 rDPO 被证明在一系列不同的行为协调任务中是有效的，例如提高安全性、针对角色扮演的稳健性以及减少阿谀奉承。代码将在 https://github.com/vicgalle/refined-dpo 发布。</li>
</ul>

<h3>Title: Enhancing Amharic-LLaMA: Integrating Task Specific and Generative  Datasets</h3>
<ul>
<li><strong>Authors: </strong>Israel Abebe Azime, Mitiku Yohannes Fuge, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas Chanie, Walelign Tewabe Sewunetie, Seid Muhie Yimam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08015">https://arxiv.org/abs/2402.08015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08015">https://arxiv.org/pdf/2402.08015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08015]] Enhancing Amharic-LLaMA: Integrating Task Specific and Generative  Datasets(https://arxiv.org/abs/2402.08015)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）因其在理解和生成人类语言方面的卓越性能而在自然语言处理（NLP）研究中受到广泛关注。然而，低资源语言由于资源的缺乏而被抛在后面。在这项工作中，我们专注于通过集成特定任务和生成数据集来增强 LLaMA-2-Amharic 模型，以提高阿姆哈拉语的语言模型性能。我们编译了阿姆哈拉语指令微调数据集和微调的 LLaMA-2-Amharic 模型。微调模型在不同的 NLP 任务中显示出有希望的结果。我们开源数据集创建管道、指令数据集、训练模型和评估输出，以促进对这些模型的特定语言研究。</li>
</ul>

<h3>Title: Careless Whisper: Speech-to-Text Hallucination Harms</h3>
<ul>
<li><strong>Authors: </strong>Allison Koenecke, Anna Seo Gyeong Choi, Katelyn Mei, Hilke Schellmann, Mona Sloane</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08021">https://arxiv.org/abs/2402.08021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08021">https://arxiv.org/pdf/2402.08021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08021]] Careless Whisper: Speech-to-Text Hallucination Harms(https://arxiv.org/abs/2402.08021)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>Speech-to-text services aim to transcribe input audio as accurately as possible. They increasingly play a role in everyday life, for example in personal voice assistants or in customer-company interactions. We evaluate Open AI's Whisper, a state-of-the-art service outperforming industry competitors. While many of Whisper's transcriptions were highly accurate, we found that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences, which did not exist in any form in the underlying audio. We thematically analyze the Whisper-hallucinated content, finding that 38% of hallucinations include explicit harms such as violence, made up personal information, or false video-based authority. We further provide hypotheses on why hallucinations occur, uncovering potential disparities due to speech type by health status. We call on industry practitioners to ameliorate these language-model-based hallucinations in Whisper, and to raise awareness of potential biases in downstream applications of speech-to-text models.</li>
<li><strong>摘要：</strong>语音转文本服务旨在尽可能准确地转录输入音频。它们在日常生活中发挥着越来越重要的作用，例如在个人语音助理或客户与公司的互动中。我们评估了 Open AI 的 Whisper，这是一项超越行业竞争对手的最先进服务。虽然 Whisper 的许多转录非常准确，但我们发现大约 1% 的音频转录包含整个幻觉短语或句子，而这些短语或句子在底层音频中并不以任何形式存在。我们对 Whisper 幻觉内容进行了主题分析，发现 38% 的幻觉包含明显的伤害，例如暴力、编造的个人信息或基于视频的虚假权威。我们进一步提供了关于幻觉发生原因的假设，揭示了由于言语类型和健康状况而导致的潜在差异。我们呼吁行业从业者改善 Whisper 中这些基于语言模型的幻觉，并提高对语音转文本模型下游应用中潜在偏差的认识。</li>
</ul>

<h3>Title: Avoiding Catastrophe in Continuous Spaces by Asking for Help</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Plaut, Hanlin Zhu, Stuart Russell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08062">https://arxiv.org/abs/2402.08062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08062">https://arxiv.org/pdf/2402.08062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08062]] Avoiding Catastrophe in Continuous Spaces by Asking for Help(https://arxiv.org/abs/2402.08062)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly asks for help or is nearly guaranteed to cause catastrophe. Finally, we identify the key obstacle to generalizing our algorithm to a multi-dimensional state space.</li>
<li><strong>摘要：</strong>大多数带有正式后悔保证的强化学习算法都假设所有错误都是可逆的，并且基本上依赖于尝试所有可能的选项。当某些错误无法弥补甚至是灾难性的时，这种方法会导致不良结果。我们提出了上下文强盗问题的一种变体，其目标是最大限度地减少灾难的可能性。具体来说，我们假设每轮的收益代表了该轮避免灾难的机会，并尝试最大化收益的乘积（避免灾难的总体机会）。为了给代理一些成功的机会，我们允许向导师提出有限数量的查询，并假设利普希茨连续支付函数。我们提出了一种算法，假设连续的一维状态空间和相对“简单”的收益函数，随着时间范围的增长，遗憾和查询导师的比率都接近 0。我们还提供了一个匹配的下限：没有简单的假设：任何算法要么不断地寻求帮助，要么几乎肯定会导致灾难。最后，我们确定了将我们的算法推广到多维状态空间的关键障碍。</li>
</ul>

<h3>Title: Beyond LLMs: Advancing the Landscape of Complex Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jennifer Chu-Carroll, Andrew Beck, Greg Burnham, David OS Melville, David Nachman, A. Erdem Özcan, David Ferrucci</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08064">https://arxiv.org/abs/2402.08064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08064">https://arxiv.org/pdf/2402.08064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08064]] Beyond LLMs: Advancing the Landscape of Complex Reasoning(https://arxiv.org/abs/2402.08064)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Since the advent of Large Language Models a few years ago, they have often been considered the de facto solution for many AI problems. However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems. These problems are ubiquitous and current solutions are highly specialized and expensive to implement. At Elemental Cognition, we developed our EC AI platform which takes a neuro-symbolic approach to solving constraint satisfaction and optimization problems. The platform employs, at its core, a precise and high performance logical reasoning engine, and leverages LLMs for knowledge acquisition and user interaction. This platform supports developers in specifying application logic in natural and concise language while generating application user interfaces to interact with users effectively. We evaluated LLMs against systems built on the EC AI platform in three domains and found the EC AI systems to significantly outperform LLMs on constructing valid and optimal solutions, on validating proposed solutions, and on repairing invalid solutions.</li>
<li><strong>摘要：</strong>自从几年前大型语言模型出现以来，它们经常被认为是许多人工智能问题的实际解决方案。然而，除了法学硕士的许多缺陷（例如可靠性、成本和速度）阻碍了其广泛的行业采用之外，大型语言模型在一整类常见的现实问题上表现不佳，即约束满足和优化问题。这些问题普遍存在，当前的解决方案高度专业化且实施成本昂贵。在 Elemental Cognition，我们开发了 EC AI 平台，该平台采用神经符号方法来解决约束满足和优化问题。该平台的核心采用精确、高性能的逻辑推理引擎，并利用法学硕士进行知识获取和用户交互。该平台支持开发人员以自然简洁的语言指定应用程序逻辑，同时生成应用程序用户界面以与用户有效交互。我们在三个领域对基于 EC AI 平台构建的系统进行了法学硕士的评估，发现 EC AI 系统在构建有效和最优解决方案、验证提出的解决方案以及修复无效解决方案方面显着优于法学硕士。</li>
</ul>

<h3>Title: Grounding Data Science Code Generation with Input-Output Specifications</h3>
<ul>
<li><strong>Authors: </strong>Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri, Alex Polozov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08073">https://arxiv.org/abs/2402.08073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08073">https://arxiv.org/pdf/2402.08073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08073]] Grounding Data Science Code Generation with Input-Output Specifications(https://arxiv.org/abs/2402.08073)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data science benchmarks, Arcade and DS-1000. The results demonstrate a significant improvement in the LLM's ability to generate code that is not only executable but also accurately aligned with user specifications, substantially improving the quality of code generation for complex data science tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近展示了根据自然语言 (NL) 提示生成代码的卓越能力。然而，在现实世界中，NL 通常过于模糊，无法捕捉编程问题背后的真实意图，需要额外的输入输出 (I/O) 规范。不幸的是，法学硕士很难将其输出与 NL 提示和 I/O 规范保持一致。在本文中，我们提供了一种在数据科学编程背景下缓解此问题的方法，其中任务需要明确的 I/O 规范才能清晰。具体来说，我们提出了 GIFT4Code，这是一种根据 I/O 规范对 LLM 进行指令微调的新颖方法。我们的方法利用法学硕士本身产生的合成数据，并利用执行衍生的反馈作为关键的学习信号。这种反馈以程序 I/O 规范的形式提供给法学硕士，以促进指令微调。我们在两个具有挑战性的数据科学基准 Arcade 和 DS-1000 上评估了我们的方法。结果表明，法学硕士生成代码的能力得到了显着提高，这些代码不仅可以执行，而且可以准确地符合用户规范，从而大大提高了复杂数据科学任务的代码生成质量。</li>
</ul>

<h3>Title: Large Language Models as Agents in Two-Player Games</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Peng Sun, Hang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08078">https://arxiv.org/abs/2402.08078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08078">https://arxiv.org/pdf/2402.08078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08078]] Large Language Models as Agents in Two-Player Games(https://arxiv.org/abs/2402.08078)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.</li>
<li><strong>摘要：</strong>通过在单一统一的机器学习范式中正式定义大型语言模型 (LLM) 的训练过程（通常包括预训练、监督微调和基于人类反馈的强化学习），我们可以收集推进 LLM 的关键见解技术。本立场文件描述了法学硕士的训练方法与两人游戏中代理开发策略之间的相似之处，如博弈论、强化学习和多代理系统中所研究的那样。我们建议根据基于语言的游戏中的代理学习来重新概念化 LLM 学习过程。该框架揭示了法学硕士发展的成功和挑战的创新视角，为解决其他战略考虑因素中的一致性问题提供了新的理解。此外，我们的两人游戏方法揭示了用于培训法学硕士的新颖数据准备和机器学习技术。</li>
</ul>

<h3>Title: Text-centric Alignment for Multi-Modality Learning</h3>
<ul>
<li><strong>Authors: </strong>Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, Shou-De Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08086">https://arxiv.org/abs/2402.08086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08086">https://arxiv.org/pdf/2402.08086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08086]] Text-centric Alignment for Multi-Modality Learning(https://arxiv.org/abs/2402.08086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain.</li>
<li><strong>摘要：</strong>本研究论文解决了多模态学习中模态不匹配的挑战，其中推理过程中可用的模态与训练中可用的模态不同。我们提出了以文本为中心的多模态学习对齐（TAMML）方法，这是一种创新方法，利用具有上下文学习和基础模型的大型语言模型（LLM）来增强这些条件下多模态系统的通用性。通过利用文本作为统一语义空间的独特属性，TAMML 在处理看不见的、多样化的和不可预测的模态组合方面展示了显着的改进。 TAMML 不仅适应不同的模态，而且保持稳健的性能，展示了基础模型在克服传统固定模态框架在嵌入表示方面的局限性的潜力。这项研究为模态可用性动态且不确定的现实应用提供了灵活、有效的解决方案，为该领域做出了贡献。</li>
</ul>

<h3>Title: BASE TTS: Lessons from building a billion-parameter Text-to-Speech model  on 100K hours of data</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Łajszczak, Guillermo Cámbara, Yang Li, Fatih Beyhan, Arent van Korlaar, Fan Yang, Arnaud Joly, Álvaro Martín-Cortinas, Ammar Abbas, Adam Michalski, Alexis Moinet, Sri Karlapati, Ewa Muszyńska, Haohan Guo, Bartosz Putrycz, Soledad López Gambino, Kayeon Yoo, Elena Sokolova, Thomas Drugman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08093">https://arxiv.org/abs/2402.08093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08093">https://arxiv.org/pdf/2402.08093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08093]] BASE TTS: Lessons from building a billion-parameter Text-to-Speech model  on 100K hours of data(https://arxiv.org/abs/2402.08093)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.</li>
<li><strong>摘要：</strong>我们引入了一种名为 BASE TTS 的文本转语音 (TTS) 模型，它代表 $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E }$合并能力。 BASE TTS 是迄今为止最大的 TTS 模型，经过 10 万小时的公共领域语音数据训练，在语音自然度方面达到了新的最先进水平。它部署了一个 10 亿参数的自回归 Transformer，将原始文本转换为离散代码（“语音代码”），然后是基于卷积的解码器，以增量、可流式传输的方式将这些语音代码转换为波形。此外，我们的语音代码是使用一种新颖的语音标记化技术构建的，该技术具有说话人 ID 解开和字节对编码压缩的功能。与广泛报道的大型语言模型在不断增加的数据量上训练时的“涌现能力”相呼应，我们表明，用 10K 多个小时和 5 亿多个参数构建的 BASE TTS 变体开始在文本复杂的句子上展示自然韵律。我们设计并共享一个专门的数据集来衡量这些文本转语音的新兴能力。我们通过对照基线进行评估，展示 BASE TTS 最先进的自然性，其中包括公开可用的大型文本转语音系统：YourTTS、Bark 和 TortoiseTTS。可以在 https://amazon-ltts-paper.com/ 上听到该模型生成的音频样本。</li>
</ul>

<h3>Title: Investigating the Impact of Data Contamination of Large Language Models  in Text-to-SQL Translation</h3>
<ul>
<li><strong>Authors: </strong>Federico Ranaldi, Elena Sofia Ruzzetti, Dario Onorati, Leonardo Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08100">https://arxiv.org/abs/2402.08100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08100">https://arxiv.org/pdf/2402.08100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08100]] Investigating the Impact of Data Contamination of Large Language Models  in Text-to-SQL Translation(https://arxiv.org/abs/2402.08100)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination. In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to-SQL translation tasks.</li>
<li><strong>摘要：</strong>理解文本描述来生成代码似乎是零样本场景中指令跟踪大型语言模型（LLM）所实现的能力。然而，这种翻译能力很可能会受到看到目标文本描述和相关代码的影响。这种效应称为数据污染。在本研究中，我们研究了数据污染对 GPT-3.5 在文本到 SQL 代码生成任务中性能的影响。因此，我们引入了一种新颖的方法来检测 GPT 中的数据污染，并使用已知的 Spider 数据集和我们新的不熟悉的数据集 Termite 检查 GPT-3.5 的文本到 SQL 性能。此外，我们通过对抗性表断开 (ATD) 方法分析了 GPT-3.5 对修改信息的数据库的功效，通过从数据库中删除结构性信息，使文本到 SQL 任务变得复杂。我们的结果表明，即使进行了 ATD 修改，GPT-3.5 在不熟悉的 Termite 数据集上的性能也会显着下降，这凸显了文本到 SQL 翻译任务中数据污染对 LLM 的影响。</li>
</ul>

<h3>Title: A Competition Winning Deep Reinforcement Learning Agent in microRTS</h3>
<ul>
<li><strong>Authors: </strong>Scott Goodfriend</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08112">https://arxiv.org/abs/2402.08112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08112">https://arxiv.org/pdf/2402.08112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08112]] A Competition Winning Deep Reinforcement Learning Agent in microRTS(https://arxiv.org/abs/2402.08112)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation Learning using Behavior Cloning and fine-tuning these models with DRL has proven promising as an efficient way to bootstrap models with demonstrated, competitive behaviors.</li>
<li><strong>摘要：</strong>脚本代理在 CIG 和 CoG 主办的 IEEE microRTS ($\mu$RTS) 竞赛的前五届比赛中主要获胜。尽管深度强化学习（DRL）算法在实时策略（RTS）游戏中取得了重大进展，但由于需要大量的训练资源以及创建和调试此类代理所固有的复杂性，它们在主要学术竞赛中的采用受到限制。 RAISocketAI 是第一个赢得 IEEE microRTS 竞赛的 DRL 代理。在没有性能限制的基准测试中，RAISocketAI 经常击败之前的两个竞赛获胜者。第一个获奖的 DRL 提交可以成为未来 microRTS 竞赛的基准和未来 DRL 研究的起点。迭代地微调基本策略并将学习迁移到特定地图对于 RAISocketAI 的获胜表现至关重要。这些策略可用于经济地训练未来的 DRL 智能体。使用行为克隆进行模仿学习并使用 DRL 微调这些模型的进一步工作已被证明是一种很有前景的有效方法，可以引导具有已证明的竞争行为的模型。</li>
</ul>

<h3>Title: Addressing cognitive bias in medical language models</h3>
<ul>
<li><strong>Authors: </strong>Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, Rama Chellappa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08113">https://arxiv.org/abs/2402.08113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08113">https://arxiv.org/pdf/2402.08113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08113]] Addressing cognitive bias in medical language models(https://arxiv.org/abs/2402.08113)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3, modified to replicate common clinically-relevant cognitive biases. Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias. Our findings highlight the critical need for bias mitigation in the development of medical LLMs, pointing towards safer and more reliable applications in healthcare.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 与医学领域的集成因其在模拟临床决策环境中具有良好的准确性而受到广泛关注。然而，临床决策比模拟更复杂，因为医生的决策受到许多因素的影响，包括认知偏差的存在。然而，法学硕士在多大程度上容易受到影响人类临床医生的相同认知偏见的影响仍有待探索。我们的假设认为，当法学硕士面临包含认知偏差的临床问题时，与没有此类偏差的相同问题相比，他们给出的答案的准确度要低得多。在这项研究中，我们开发了 BiasMedQA，这是一种评估应用法学硕士认知偏差的新基准。到医疗任务。我们使用 BiasMedQA 评估了 6 个 LLM，即 GPT-4、Mixtral-8x70B、GPT-3.5、PaLM-2、Llama 2 70B-chat 和医学专业 PMC Llama 13B。我们对美国医疗执照考试 (USMLE) 第 1、2 和 3 步中的 1,273 个问题测试了这些模型，并进行了修改以复制常见的临床相关认知偏差。我们的分析揭示了偏见对这些 LLM 的不同影响，其中 GPT-4 因其对偏见的恢复能力而脱颖而出，而 Llama 2 70B-chat 和 PMC Llama 13B 则受到认知偏见的影响尤为严重。我们的研究结果强调了医学法学硕士发展中减少偏见的迫切需要，指出了医疗保健领域更安全、更可靠的应用。</li>
</ul>

<h3>Title: Active Preference Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>William Muldrew, Peter Hayes, Mingtian Zhang, David Barber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08114">https://arxiv.org/abs/2402.08114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08114">https://arxiv.org/pdf/2402.08114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08114]] Active Preference Learning for Large Language Models(https://arxiv.org/abs/2402.08114)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的能力变得越来越强大，用于与人类意图保持一致的微调技术变得越来越重要。调整这些模型的一个关键考虑因素是如何最有效地利用人力资源，或者在法学硕士本身被用作预言机的情况下如何最有效地利用人力资源或模型资源。根据人类或人工智能偏好进行强化学习 (RLHF/RLAIF) 是此类技术最突出的例子，但很复杂且往往不稳定。直接偏好优化（DPO）最近被提议作为一种更简单、更稳定的替代方案。在这项工作中，我们为 DPO 开发了一种主动学习策略，以更好地利用偏好标签。我们基于语言模型的预测熵和 DPO 优化的隐式偏好模型的确定性度量，提出了一种实用的提示/完成对习得函数。我们展示了我们的方法如何提高学习率和对成对偏好数据进行微调的最终性能。</li>
</ul>

<h3>Title: On the Self-Verification Limitations of Large Language Models on  Reasoning and Planning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08115">https://arxiv.org/abs/2402.08115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08115">https://arxiv.org/pdf/2402.08115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08115]] On the Self-Verification Limitations of Large Language Models on  Reasoning and Planning Tasks(https://arxiv.org/abs/2402.08115)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system. In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.</li>
<li><strong>摘要：</strong>关于大型语言模型（LLM）的推理能力存在很大的分歧。虽然最初对推理可能会随着规模自动出现的乐观情绪由于大量的反例（从乘法到简单的规划）而受到了影响，但人们仍然普遍相信法学硕士可以在迭代中进行自我批评并改进自己的解决方案。时尚。这种信念似乎基于这样的假设：正确性的验证应该比生成更容易——计算复杂性的一个相当经典的论点——这应该与法学硕士无关，因为他们所做的是近似检索。在本文中，我们着手系统地研究推理和规划背景下迭代提示的有效性。我们对 GPT-4 在三个领域的性能进行了原则性的实证研究：24 人游戏、图形着色和 STRIPS 规划。我们用模型批评自己的答案，并用外部正确推理器验证提出的解决方案进行实验。在每种情况下，我们都会分析批评的内容是否实际上会影响底线绩效，以及我们是否可以在不损失绩效的情况下消除增强系统的元素。我们观察到，通过自我批评，性能会显着下降；通过健全的外部验证，性能会显着提高，但批评的内容对系统的性能并不重要。事实上，仅仅用声音验证器重新提示就可以保留更多复杂设置的大部分好处。</li>
</ul>

<h3>Title: Recursive Joint Simulation in Games</h3>
<ul>
<li><strong>Authors: </strong>Vojtech Kovarik, Caspar Oesterheld, Vincent Conitzer</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08128">https://arxiv.org/abs/2402.08128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08128">https://arxiv.org/pdf/2402.08128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08128]] Recursive Joint Simulation in Games(https://arxiv.org/abs/2402.08128)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known. Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings. In this paper, we study an interaction between AI agents where the agents run a recursive joint simulation. That is, the agents first jointly observe a simulation of the situation they face. This simulation in turn recursively includes additional simulations (with a small chance of failure, to avoid infinite recursion), and the results of all these nested simulations are observed before an action is chosen. We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems.</li>
<li><strong>摘要：</strong>人工智能代理之间的博弈论动态可能在很多方面不同于传统的人与人交互。其中一个区别是，可以准确地模拟人工智能代理，例如因为它的源代码是已知的。我们的目标是探索如何利用这种可能性在战略环境中取得更多合作成果。在本文中，我们研究了人工智能代理之间的交互，其中代理运行递归联合模拟。也就是说，智能体首先共同观察他们所面临的情况的模拟。该模拟又递归地包括额外的模拟（失败的可能性很小，以避免无限递归），并且在选择操作之前观察所有这些嵌套模拟的结果。我们证明，由此产生的交互在策略上相当于原始游戏的无限重复版本，允许直接转移现有结果，例如各种民间定理。</li>
</ul>

<h3>Title: On the Resurgence of Recurrent Models for Long Sequences: Survey and  Research Opportunities in the Transformer Era</h3>
<ul>
<li><strong>Authors: </strong>Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, Stefano Melacci</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08132">https://arxiv.org/abs/2402.08132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08132">https://arxiv.org/pdf/2402.08132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08132]] On the Resurgence of Recurrent Models for Long Sequences: Survey and  Research Opportunities in the Transformer Era(https://arxiv.org/abs/2402.08132)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends framed under the unifying umbrella of Recurrence. Moreover, it emphasizes novel research opportunities that become prominent when abandoning the idea of processing long sequences whose length is known-in-advance for the more realistic setting of potentially infinite-length sequences, thus intersecting the field of lifelong-online learning from streamed data.</li>
<li><strong>摘要：</strong>机器学习社区面临的一个长期挑战是开发能够处理很长的数据序列并从中学习的模型。基于 Transformer 的网络（例如大型语言模型）的出色成果促进了并行注意力的理念成为应对此类挑战的关键，从而混淆了循环模型的经典顺序处理的作用。然而，在过去几年中，关注自注意力二次复杂性的研究人员提出了一种新的神经模型浪潮，该模型充分利用了 Transformers 和 Recurrent Nets 这两个领域的优点。与此同时，深层空间状态模型作为随着时间的推移函数逼近的稳健方法而出现，从而为从序列数据中学习开辟了新的视角，受到该领域许多人的追随，并被用来实现一类特殊的（线性）循环神经网络。这项调查旨在概述“复发”统一框架下的这些趋势。此外，它强调了新颖的研究机会，当放弃处理长度预先已知的长序列的想法以实现潜在无限长度序列的更现实设置时，这些机会变得突出，从而与流数据的终身在线学习领域相交叉。</li>
</ul>

<h3>Title: Epistemic Exploration for Generalizable Planning and Learning in  Non-Stationary Settings</h3>
<ul>
<li><strong>Authors: </strong>Rushang Karia, Pulkit Verma, Alberto Speranzon, Siddharth Srivastava</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08145">https://arxiv.org/abs/2402.08145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08145">https://arxiv.org/pdf/2402.08145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08145]] Epistemic Exploration for Generalizable Planning and Learning in  Non-Stationary Settings(https://arxiv.org/abs/2402.08145)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings. Theoretical results show that the system reverts to exhibit desirable convergence properties when stationarity holds.</li>
<li><strong>摘要：</strong>本文介绍了一种在使用关系表示表示的非平稳随机环境中进行持续规划和模型学习的新方法。这些能力对于在不确定、不断变化的现实世界中部署顺序决策系统至关重要。在具有未知（且非静态）转换系统和不断变化的任务的实际环境中工作，所提出的框架对智能体当前知识状态的差距进行建模，并利用它们进行有针对性的调查性探索。使用这些探索收集的数据用于学习可概括的概率模型，以解决当前任务，尽管环境动态不断变化。对多个基准领域的实证评估表明，就非平稳环境中的样本复杂性而言，该方法显着优于规划和强化学习基线。理论结果表明，当平稳性成立时，系统会恢复到表现出理想的收敛特性。</li>
</ul>

<h3>Title: Group Decision-Making among Privacy-Aware Agents</h3>
<ul>
<li><strong>Authors: </strong>Marios Papachristou, M. Amin Rahimian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.MA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08156">https://arxiv.org/abs/2402.08156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08156">https://arxiv.org/pdf/2402.08156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08156]] Group Decision-Making among Privacy-Aware Agents(https://arxiv.org/abs/2402.08156)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>How can individuals exchange information to learn from each other despite their privacy needs and security concerns? For example, consider individuals deliberating a contentious topic and being concerned about divulging their private experiences. Preserving individual privacy and enabling efficient social learning are both important desiderata but seem fundamentally at odds with each other and very hard to reconcile. We do so by controlling information leakage using rigorous statistical guarantees that are based on differential privacy (DP). Our agents use log-linear rules to update their beliefs after communicating with their neighbors. Adding DP randomization noise to beliefs provides communicating agents with plausible deniability with regard to their private information and their network neighborhoods. We consider two learning environments one for distributed maximum-likelihood estimation given a finite number of private signals and another for online learning from an infinite, intermittent signal stream. Noisy information aggregation in the finite case leads to interesting tradeoffs between rejecting low-quality states and making sure all high-quality states are accepted in the algorithm output. Our results flesh out the nature of the trade-offs in both cases between the quality of the group decision outcomes, learning accuracy, communication cost, and the level of privacy protections that the agents are afforded.</li>
<li><strong>摘要：</strong>尽管存在隐私需求和安全问题，个人如何交换信息以相互学习？例如，考虑个人正在审议一个有争议的话题并担心泄露他们的私人经历。保护个人隐私和实现高效的社会学习都是重要的需求，但似乎从根本上相互矛盾，而且很难调和。为此，我们使用基于差分隐私 (DP) 的严格统计保证来控制信息泄漏。我们的代理在与邻居沟通后使用对数线性规则来更新他们的信念。将 DP 随机化噪声添加到信念中，可以为通信主体提供关于其私人信息和网络邻居的合理否认性。我们考虑两种学习环境，一种用于给定有限数量的私有信号的分布式最大似然估计，另一种用于从无限的间歇信号流进行在线学习。有限情况下的噪声信息聚合会导致在拒绝低质量状态和确保算法输出中接受所有高质量状态之间进行有趣的权衡。我们的结果充实了这两种情况下群体决策结果的质量、学习准确性、通信成本和代理所提供的隐私保护水平之间权衡的本质。</li>
</ul>

<h3>Title: LLaGA: Large Language and Graph Assistant</h3>
<ul>
<li><strong>Authors: </strong>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08170">https://arxiv.org/abs/2402.08170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08170">https://arxiv.org/pdf/2402.08170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08170]] LLaGA: Large Language and Graph Assistant(https://arxiv.org/abs/2402.08170)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios. Our code is available at \url{https://github.com/ChenRunjin/LLaGA}</li>
<li><strong>摘要：</strong>图神经网络（GNN）推动了图结构数据分析的进步。最近，像 GPT-4 这样的大型语言模型 (LLM) 的兴起预示着深度学习的新时代的到来。然而，由于将图结构翻译成语言的固有困难，它们在图数据上的应用提出了明显的挑战。为此，我们引入了 \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA})，这是一种创新模型，有效地集成了 LLM 功能来处理图结构数据的复杂性。 LLaGA 保留了 LLM 的通用性质，同时将图形数据调整为与 LLM 输入兼容的格式。 LLaGA 通过将图节点重新组织为结构感知序列，然后通过多功能投影仪将它们映射到令牌嵌入空间来实现这一点。 LLaGA 在多功能性、通用性和可解释性方面表现出色，使其能够在不同的数据集和任务中始终如一地表现良好，将其能力扩展到未见过的数据集或任务，并为图形提供解释。我们在流行的图形基准上进行的广泛实验表明，LLaGA 使用一个模型在四个数据集和三个任务中提供了出色的性能，在监督和零样本场景中都超越了最先进的图形模型。我们的代码位于 \url{https://github.com/ChenRunjin/LLaGA}</li>
</ul>

<h3>Title: LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied  Agents</h3>
<ul>
<li><strong>Authors: </strong>Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08178">https://arxiv.org/abs/2402.08178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08178">https://arxiv.org/pdf/2402.08178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08178]] LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied  Agents(https://arxiv.org/abs/2402.08178)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）作为任务规划的替代解决方案最近受到了相当多的关注。然而，比较面向语言的任务规划器的性能变得困难，并且缺乏对预训练模型选择和提示构建等各种因素的影响的详细探索。为了解决这个问题，我们提出了一个基准系统，用于自动量化家庭服务具体代理的任务规划性能。任务规划器在两对数据集和模拟器上进行测试：1) ALFRED 和 AI2-THOR，2) Watch-And-Help 和 VirtualHome 的扩展。使用所提出的基准系统，我们对法学硕士和提示进行了广泛的实验，并探索了基线规划器的一些增强功能。我们期望所提出的基准工具将加速面向语言的任务规划器的开发。</li>
</ul>

<h3>Title: Pixel Sentence Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Xiao, Zhuoxu Huang, Danlu Chen, G Thomas Hudson, Yizhi Li, Haoran Duan, Chenghua Lin, Jie Fu, Jungong Han, Noura Al Moubayed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08183">https://arxiv.org/abs/2402.08183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08183">https://arxiv.org/pdf/2402.08183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08183]] Pixel Sentence Representation Learning(https://arxiv.org/abs/2402.08183)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pretrained language models are long known to be subpar in capturing sentence and document-level semantics. Though heavily investigated, transferring perturbation-based methods from unsupervised visual representation learning to NLP remains an unsolved problem. This is largely due to the discreteness of subword units brought by tokenization of language models, limiting small perturbations of inputs to form semantics-preserved positive pairs. In this work, we conceptualize the learning of sentence-level textual semantics as a visual representation learning process. Drawing from cognitive and linguistic sciences, we introduce an unsupervised visual sentence representation learning framework, employing visually-grounded text perturbation methods like typos and word order shuffling, resonating with human cognitive patterns, and enabling perturbation to texts to be perceived as continuous. Our approach is further bolstered by large-scale unsupervised topical alignment training and natural language inference supervision, achieving comparable performance in semantic textual similarity (STS) to existing state-of-the-art NLP methods. Additionally, we unveil our method's inherent zero-shot cross-lingual transferability and a unique leapfrogging pattern across languages during iterative training. To our knowledge, this is the first representation learning method devoid of traditional language models for understanding sentence and document semantics, marking a stride closer to human-like textual comprehension. Our code is available at https://github.com/gowitheflow-1998/Pixel-Linguist</li>
<li><strong>摘要：</strong>众所周知，预训练的语言模型在捕获句子和文档级语义方面表现不佳。尽管经过大量研究，将基于扰动的方法从无监督视觉表示学习转移到 NLP 仍然是一个未解决的问题。这主要是由于语言模型标记化带来的子词单元的离散性，限制了输入的小扰动以形成语义保留的正对。在这项工作中，我们将句子级文本语义的学习概念化为视觉表示学习过程。我们借鉴认知和语言科学的经验，引入了一种无监督的视觉句子表示学习框架，采用基于视觉的文本扰动方法（例如打字错误和词序洗牌），与人类认知模式产生共鸣，并使对文本的扰动能够被感知为连续的。我们的方法得到大规模无监督主题对齐训练和自然语言推理监督的进一步支持，在语义文本相似性（STS）方面实现了与现有最​​先进的 NLP 方法相当的性能。此外，我们还揭示了我们的方法固有的零样本跨语言可迁移性以及迭代训练过程中独特的跨语言跳跃模式。据我们所知，这是第一个无需传统语言模型来理解句子和文档语义的表示学习方法，标志着向类人文本理解迈出了一步。我们的代码位于 https://github.com/gowitheflow-1998/Pixel-Linguist</li>
</ul>

<h3>Title: Enabling Multi-Agent Transfer Reinforcement Learning via Scenario  Independent Representation</h3>
<ul>
<li><strong>Authors: </strong>Ayesha Siddika Nipu, Siming Liu, Anthony Harris</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08184">https://arxiv.org/abs/2402.08184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08184">https://arxiv.org/pdf/2402.08184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08184]] Enabling Multi-Agent Transfer Reinforcement Learning via Scenario  Independent Representation(https://arxiv.org/abs/2402.08184)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned from other scenarios compared to agents learning from scratch. Furthermore, we adopted Curriculum Transfer Learning (CTL), enabling our deep learning policy to progressively acquire knowledge and skills across pre-designed homogeneous learning scenarios organized by difficulty levels. This process promotes inter- and intra-agent knowledge transfer, leading to high multi-agent learning performance in more complicated heterogeneous scenarios.</li>
<li><strong>摘要：</strong>多智能体强化学习（MARL）算法被广泛应用于处理动态多智能体系统（MAS）中需要智能体之间协作和竞争的复杂任务。然而，从头开始学习此类任务是艰巨的，并且可能并不总是可行，特别是对于由于样本复杂性而具有大量交互代理的 MAS。因此，重用从过去的经验或其他代理中获得的知识可以有效地加速学习过程和升级 MARL 算法。在这项研究中，我们引入了一种新颖的框架，该框架通过将各种状态空间统一为固定大小的输入来实现 MARL 的迁移学习，从而允许一个统一的深度学习策略在 MAS 内的不同场景中可行。我们在星际争霸多智能体挑战（SMAC）环境中的一系列场景中评估了我们的方法，结果表明，与从头开始学习的智能体相比，使用从其他场景中学到的机动技能的多智能体学习性能显着增强。此外，我们采用了课程迁移学习（CTL），使我们的深度学习政策能够在按难度级别组织的预先设计的同质学习场景中逐步获取知识和技能。这个过程促进了智能体之间和智能体内部的知识转移，从而在更复杂的异构场景中实现了较高的多智能体学习性能。</li>
</ul>

<h3>Title: BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08219">https://arxiv.org/abs/2402.08219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08219">https://arxiv.org/pdf/2402.08219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08219]] BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models(https://arxiv.org/abs/2402.08219)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.</li>
<li><strong>摘要：</strong>使 GPT-4 和 Gemini 等最先进的大型语言模型 (LLM) 适应特定任务具有挑战性。由于其参数、嵌入甚至输出概率的不透明性，现有的微调适应方法不适用。因此，只有通过 API 服务才能适应这些黑盒法学硕士，这引发了人们对透明度、隐私和成本的担忧。为了应对这些挑战，我们推出了 BBox-Adapter，这是一种用于黑盒 LLM 的新型轻量级适配器。 BBox-Adapter 通过将目标数据视为正数据，将源数据视为负数据来区分目标域数据和源域数据。它采用基于排名的噪声对比估计（NCE）损失来提高目标域数据的可能性，同时惩罚源域数据的可能性。此外，它还具有在线适应机制，其中结合了来自地面实况、人类或人工智能反馈的实时正数据采样，以及之前适应的负数据。大量实验证明了 BBox-Adapter 的有效性和成本效率。它在不同的任务和领域中将模型性能提高了高达 6.77%，同时将训练和推理成本分别降低了 31.30 倍和 1.84 倍。</li>
</ul>

<h3>Title: Improving Black-box Robustness with In-Context Rewriting</h3>
<ul>
<li><strong>Authors: </strong>Kyle O'Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, Thomas Hartvigsen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08225">https://arxiv.org/abs/2402.08225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08225">https://arxiv.org/pdf/2402.08225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08225]] Improving Black-box Robustness with In-Context Rewriting(https://arxiv.org/abs/2402.08225)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID performance. We explore selectively augmenting inputs based on prediction entropy to reduce the rate of expensive LLM augmentations, allowing us to maintain performance gains while reducing the average number of generated augmentations by 57.76%. LLM-TTA is agnostic to the task model architecture, does not require OOD labels, and is effective across low and high-resource settings. We share our data, models, and code for reproducibility.</li>
<li><strong>摘要：</strong>机器学习模型通常擅长处理分布内 (ID) 数据，但难以处理看不见的分布外 (OOD) 输入。大多数提高 OOD 鲁棒性的技术不适用于模型实际上是黑盒的设置，例如当权重被冻结、重新训练成本高昂或通过 API 利用模型时。测试时间增强 (TTA) 是一种简单的事后技术，用于提高鲁棒性，通过聚合测试输入的多个增强的预测来避开黑盒约束。由于生成有效的自然语言增强的挑战，TTA 在 NLP 中的使用有限。在这项工作中，我们提出了 LLM-TTA，它使用 LLM 生成的增强作为 TTA 的增强函数。 LLM-TTA 在 BERT 和 T5 模型的情感、毒性和新闻分类任务上优于传统的增强函数，BERT 的 OOD 鲁棒性平均提高了 4.30 个百分点，而平均 ID 性能没有下降。我们探索基于预测熵的选择性增强输入，以降低昂贵的 LLM 增强率，使我们能够保持性能增益，同时将生成的增强的平均数量减少 57.76%。 LLM-TTA 与任务模型架构无关，不需要 OOD 标签，并且在低资源和高资源设置中都有效。我们共享数据、模型和代码以实现可重复性。</li>
</ul>

<h3>Title: Privacy-Preserving Language Model Inference with Instance Obfuscation</h3>
<ul>
<li><strong>Authors: </strong>Yixiang Yao, Fei Wang, Srivatsan Ravi, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08227">https://arxiv.org/abs/2402.08227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08227">https://arxiv.org/pdf/2402.08227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08227]] Privacy-Preserving Language Model Inference with Instance Obfuscation(https://arxiv.org/abs/2402.08227)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language Models as a Service (LMaaS) offers convenient access for developers and researchers to perform inference using pre-trained language models. Nonetheless, the input data and the inference results containing private information are exposed as plaintext during the service call, leading to privacy issues. Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page. In order to maintain the black-box manner of LMaaS, conducting data privacy protection, especially for the decision, is a challenging task because the process has to be seamless to the models and accompanied by limited communication and computation overhead. We thus propose Instance-Obfuscated Inference (IOI) method, which focuses on addressing the decision privacy issue of natural language understanding tasks in their complete life-cycle. Besides, we conduct comprehensive experiments to evaluate the performance as well as the privacy-protection strength of the proposed method on various benchmarking tasks.</li>
<li><strong>摘要：</strong>语言模型即服务 (LMaaS) 为开发人员和研究人员提供了使用预先训练的语言模型进行推理的便捷途径。然而，包含隐私信息的输入数据和推理结果在服务调用过程中以明文形式暴露，导致隐私问题。最近的研究开始通过噪声添加和内容扰动等技术将输入数据转换为来自用户端的隐私保护表示来解决隐私问题，而对推理结果保护（即决策隐私）的探索仍然是空白页。为了保持 LMaaS 的黑盒方式，进行数据隐私保护，尤其是决策数据隐私保护，是一项具有挑战性的任务，因为该过程必须与模型无缝连接，并且伴随着有限的通信和计算开销。因此，我们提出了实例模糊推理（IOI）方法，该方法专注于解决自然语言理解任务在其整个生命周期中的决策隐私问题。此外，我们还进行了全面的实验来评估所提出的方法在各种基准测试任务上的性能和隐私保护强度。</li>
</ul>

<h3>Title: A Survey of Table Reasoning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08259">https://arxiv.org/abs/2402.08259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08259">https://arxiv.org/pdf/2402.08259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08259]] A Survey of Table Reasoning with Large Language Models(https://arxiv.org/abs/2402.08259)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Table reasoning, which aims to generate the corresponding answer to the question following the user requirement according to the provided table, and optionally a text description of the table, effectively improving the efficiency of obtaining information. Recently, using Large Language Models (LLMs) has become the mainstream method for table reasoning, because it not only significantly reduces the annotation cost but also exceeds the performance of previous methods. However, existing research still lacks a summary of LLM-based table reasoning works. Due to the existing lack of research, questions about which techniques can improve table reasoning performance in the era of LLMs, why LLMs excel at table reasoning, and how to enhance table reasoning abilities in the future, remain largely unexplored. This gap significantly limits progress in research. To answer the above questions and advance table reasoning research with LLMs, we present this survey to analyze existing research, inspiring future work. In this paper, we analyze the mainstream techniques used to improve table reasoning performance in the LLM era, and the advantages of LLMs compared to pre-LLMs for solving table reasoning. We provide research directions from both the improvement of existing methods and the expansion of practical applications to inspire future research.</li>
<li><strong>摘要：</strong>表格推理，旨在根据提供的表格，以及可选的表格的文字描述，根据用户需求生成对应问题的答案，有效提高获取信息的效率。最近，使用大型语言模型（LLM）已成为表推理的主流方法，因为它不仅显着降低了注释成本，而且超过了以前方法的性能。然而，现有研究仍缺乏对基于LLM的表推理工作的总结。由于目前研究的缺乏，法学硕士时代哪些技术可以提高表推理性能、法学硕士为什么擅长表推理以及未来如何增强表推理能力等问题仍然很大程度上没有被探索。这一差距极大地限制了研究的进展。为了回答上述问题并推进法学硕士的表格推理研究，我们提出这项调查来分析现有研究，启发未来的工作。在本文中，我们分析了LLM时代用于提高表推理性能的主流技术，以及LLM与pre-LLM相比在解决表推理方面的优势。我们从改进现有方法和扩展实际应用两个方面提供研究方向，以启发未来的研究。</li>
</ul>

<h3>Title: World Model on Million-Length Video And Language With RingAttention</h3>
<ul>
<li><strong>Authors: </strong>Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08268">https://arxiv.org/abs/2402.08268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08268">https://arxiv.org/pdf/2402.08268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08268]] World Model on Million-Length Video And Language With RingAttention(https://arxiv.org/abs/2402.08268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.</li>
<li><strong>摘要：</strong>当前的语言模型在理解世界上不易用语言描述的方面方面存在不足，并且难以处理复杂的长格式任务。视频序列提供了语言和静态图像中所缺少的有价值的时间信息，这使得它们对于与语言的联合建模很有吸引力。这些模型可以加深对人类文本知识和物理世界的理解，从而实现更广泛的人工智能能力来帮助人类。然而，由于内存限制、计算复杂性和有限的数据集，从数百万个视频和语言序列的标记中学习提出了挑战。为了应对这些挑战，我们整理了一个包含各种视频和书籍的大型数据集，利用 RingAttention 技术对长序列进行可扩展训练，并逐渐将上下文大小从 4K 增加到 1M 令牌。本文做出以下贡献：（a）最大上下文大小神经网络：我们在长视频和语言序列上训练最大上下文大小变换器之一，为困难的检索任务和长视频理解设定了新的基准。 (b) 克服视觉语言训练挑战的解决方案，包括使用掩码序列打包来混合不同的序列长度、使用损失权重来平衡语言和视觉，以及使用模型生成的用于长序列聊天的 QA 数据集。 (c) 高度优化的实现，具有 RingAttention、屏蔽序列打包和其他关键功能，用于训练数百万长度的多模态序列。 (d) 完全开源一系列 7B 参数模型，能够处理超过 100 万代币的长文本文档（LWM-Text、LWM-Text-Chat）和视频（LWM、LWM-Chat）。这项工作为长视频和语言的海量数据集的训练铺平了道路，以发展对人类知识和多模式世界的理解以及更广泛的能力。</li>
</ul>

<h3>Title: Towards Faithful and Robust LLM Specialists for Evidence-Based  Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08277">https://arxiv.org/abs/2402.08277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08277">https://arxiv.org/pdf/2402.08277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08277]] Towards Faithful and Robust LLM Specialists for Evidence-Based  Question-Answering(https://arxiv.org/abs/2402.08277)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的更忠实和可追踪的答案的进步对于各种研究和实践活动至关重要。实现这一目标的途径之一是将答案建立在可靠的来源之上。然而，事实证明，这种基于证据的质量保证在引用正确的来源（来源质量）和真实地表示来源中的信息（答案归因）方面与法学硕士的配合不够。在这项工作中，我们系统地研究了如何稳健地微调法学硕士，以获得更好的来源质量和答案归因。具体来说，我们引入了带有自动数据质量过滤器的数据生成管道，它可以大规模合成多样化的高质量训练和测试数据。我们进一步引入了四个测试集来对微调专业模型的稳健性进行基准测试。广泛的评估表明，对合成数据进行微调可以提高分布内和分布外的性能。 %基于证据的 QA 案例。此外，我们表明，在改进基于证据的 QA 方面，数据质量（可以通过提出的质量过滤器大幅提高）比数量更重要。</li>
</ul>

<h3>Title: ChatCell: Facilitating Single-Cell Analysis with Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Yin Fang, Kangwei Liu, Ningyu Zhang, Xinle Deng, Penghui Yang, Zhuo Chen, Xiangru Tang, Mark Gerstein, Xiaohui Fan, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08303">https://arxiv.org/abs/2402.08303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08303">https://arxiv.org/pdf/2402.08303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08303]] ChatCell: Facilitating Single-Cell Analysis with Natural Language(https://arxiv.org/abs/2402.08303)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to deepen single-cell insights, paving the way for more accessible and intuitive exploration in this pivotal field. Our project homepage is available at https://zjunlp.github.io/project/ChatCell.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的迅速发展，它们对科学的影响变得越来越突出。法学硕士在任务概括和自由形式对话方面的新兴能力可以显着推进化学和生物学等领域的发展。然而，构成生物体基本组成部分的单细胞生物学领域仍然面临着一些挑战。当前方法中的高知识壁垒和有限的可扩展性限制了法学硕士在掌握单细胞数据方面的充分利用，阻碍了直接访问和快速迭代。为此，我们引入了 ChatCell，它通过利用自然语言促进单细胞分析来实现范式转变。利用词汇适应和统一序列生成，ChatCell 获得了单细胞生物学方面深厚的专业知识以及适应各种分析任务的能力。大量实验进一步证明了 ChatCell 的强大性能和深化单细胞洞察的潜力，为这一关键领域更容易、更直观的探索铺平了道路。我们的项目主页位于 https://zjunlp.github.io/project/ChatCell。</li>
</ul>

<h3>Title: Prompted Contextual Vectors for Spear-Phishing Detection</h3>
<ul>
<li><strong>Authors: </strong>Daniel Nahmias, Gal Engelberg, Dan Klein, Asaf Shabtai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08309">https://arxiv.org/abs/2402.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08309">https://arxiv.org/pdf/2402.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08309]] Prompted Contextual Vectors for Spear-Phishing Detection(https://arxiv.org/abs/2402.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizing LLM reasoning, a publicly available dataset of high-quality spear-phishing emails, and the demonstrated effectiveness of our method in detecting such emails. This methodology can be utilized for various document classification tasks, particularly in adversarial problem domains.</li>
<li><strong>摘要：</strong>鱼叉式网络钓鱼攻击带来了重大的安全挑战，大型语言模型 (LLM) 通过生成令人信服的电子邮件和促进目标侦察来升级威胁。为了解决这个问题，我们提出了一种基于新颖的文档向量化方法的检测方法，该方法利用 LLM 集合来创建表示向量。通过提示法学硕士推理并回答人为的问题，我们量化了电子邮件内容中常见说服原则的存在，为下游监督机器学习模型生成提示的上下文文档向量。我们使用由专有系统生成的独特数据集来评估我们的方法，该系统可自动执行目标侦察和鱼叉式网络钓鱼电子邮件创建。我们的方法在识别 LLM 生成的鱼叉式网络钓鱼电子邮件方面取得了 91% 的 F1 分数，训练集仅包含传统网络钓鱼和良性电子邮件。主要贡献包括利用 LLM 推理的创新文档矢量化方法、高质量鱼叉式网络钓鱼电子邮件的公开数据集，以及我们的方法在检测此类电子邮件方面已证明的有效性。该方法可用于各种文档分类任务，特别是在对抗性问题领域。</li>
</ul>

<h3>Title: Eliciting Big Five Personality Traits in Large Language Models: A  Textual Analysis with Classifier-Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Airlie Hilliard, Cristian Munoz, Zekun Wu, Adriano Soares Koshiyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08341">https://arxiv.org/abs/2402.08341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08341">https://arxiv.org/pdf/2402.08341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08341]] Eliciting Big Five Personality Traits in Large Language Models: A  Textual Analysis with Classifier-Driven Approach(https://arxiv.org/abs/2402.08341)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompted multiple LMs with different parameter sizes, including Llama-2, Falcon, Mistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined their personality using classifiers trained on the myPersonality dataset. Our results reveal that, generally, all LLMs demonstrate high openness and low extraversion. However, whereas LMs with fewer parameters exhibit similar behaviour in personality traits, newer and LMs with more parameters exhibit a broader range of personality traits, with increased agreeableness, emotional stability, and openness. Furthermore, a greater number of parameters is positively associated with openness and conscientiousness. Moreover, fine-tuned models exhibit minor modulations in their personality traits, contingent on the dataset. Implications and directions for future research are discussed.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地被候选人和雇主在招聘中使用。然而，随之而来的是许多道德问题，特别是与这些“黑匣子”模型缺乏透明度有关。尽管之前的研究试图通过调查法学硕士的人格特质来提高这些模型的透明度，但之前的许多研究都为他们提供了需要完成的人格评估。另一方面，本研究旨在通过检查基于不同输入提示的输出变化来更好地理解此类模型。具体来说，我们使用一种新颖的启发方法，使用来自常见面试问题的提示，以及旨在引出特定大五人格特征的提示，以检查模型是否像人类一样容易受到特征激活的影响，并根据他们的输出中使用的语言。为此，我们反复提示多个具有不同参数大小的 LM，包括 Llama-2、Falcon、Mistral、Bloom、GPT、OPT 和 XLNet（基本版本和微调版本），并使用在 myPersonality 数据集上训练的分类器检查它们的个性。我们的结果表明，一般来说，所有法学硕士都表现出较高的开放性和较低的外向性。然而，虽然参数较少的 LM 在人格特质方面表现出相似的行为，但参数较多的新型 LM 表现出更广泛的人格特质，具有更高的宜人性、情绪稳定性和开放性。此外，更多的参数与开放性和责任心呈正相关。此外，经过微调的模型根据数据集表现出人格特征的微小调整。讨论了未来研究的意义和方向。</li>
</ul>

<h3>Title: One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill</h3>
<ul>
<li><strong>Authors: </strong>Sangwoo Shin, Daehee Lee, Minjong Yoo, Woo Kyung Kim, Honguk Woo</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08369">https://arxiv.org/abs/2402.08369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08369">https://arxiv.org/pdf/2402.08369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08369]] One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill(https://arxiv.org/abs/2402.08369)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time. Specifically, we leverage a vision-language model to learn a semantic skill set from offline video datasets, where each skill is represented on the vision-language embedding space, and adapt meta-learning with dynamics inference to enable zero-shot skill adaptation. We evaluate our framework with various one-shot imitation scenarios for extended multi-stage Meta-world tasks, showing its superiority in learning complex tasks, generalizing to dynamics changes, and extending to different demonstration conditions and modalities, compared to other baselines.</li>
<li><strong>摘要：</strong>一次性模仿是从单个演示中学习一项新任务，但将其用于非平稳环境中固有的高领域多样性的复杂任务是一个具有挑战性的问题。为了解决这个问题，我们探索了复杂任务的组合性，并提出了一种新颖的基于技能的模仿学习框架，可以实现一次性模仿和零次适应；从复杂的看不见的任务的单个演示中，推断出语义技能序列，然后将序列中的每个技能转换为针对可能随时间变化的环境隐藏动态而优化的动作序列。具体来说，我们利用视觉语言模型从离线视频数据集中学习语义技能集，其中每种技能都在视觉语言嵌入空间上表示，并通过动态推理调整元学习以实现零样本技能适应。我们使用用于扩展多阶段元世界任务的各种一次性模仿场景来评估我们的框架，与其他基线相比，显示了其在学习复杂任务、泛化动态变化以及扩展到不同演示条件和模式方面的优越性。</li>
</ul>

<h3>Title: Punctuation Restoration Improves Structure Understanding without  Supervision</h3>
<ul>
<li><strong>Authors: </strong>Junghyun Min, Minho Lee, Woochul Lee, Yeonsoo Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08382">https://arxiv.org/abs/2402.08382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08382">https://arxiv.org/pdf/2402.08382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08382]] Punctuation Restoration Improves Structure Understanding without  Supervision(https://arxiv.org/abs/2402.08382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks. However, despite impressive conversational capabilities of recent large language model, their abilities to capture syntactic or semantic structure within text lag behind. We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. We show that punctuation restoration transfers to improvements in in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging. Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more robust structure-aware representations of natural language.</li>
<li><strong>摘要：</strong>语言建模和去噪等无监督学习目标构成了生成预训练模型的重要组成部分，这些模型执行从自然语言理解到会话任务的各种下游应用。然而，尽管最近的大型语言模型具有令人印象深刻的会话能力，但它们捕获文本中的句法或语义结构的能力却落后了。我们假设机器的语言性能和能力之间的不匹配可归因于语言结构知识向当前流行的预训练目标的计算系统的转移不足。我们表明，标点符号恢复可以改善结构相关任务（例如命名实体识别、开放信息提取、分块和词性标记）的分布内和分布外性能。标点符号恢复是一种有效的学习目标，可以提高结构理解并产生更强大的自然语言结构感知表示。</li>
</ul>

<h3>Title: Large Language Models as Minecraft Agents</h3>
<ul>
<li><strong>Authors: </strong>Chris Madge, Massimo Poesio</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08392">https://arxiv.org/abs/2402.08392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08392">https://arxiv.org/pdf/2402.08392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08392]] Large Language Models as Minecraft Agents(https://arxiv.org/abs/2402.08392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.</li>
<li><strong>摘要：</strong>在这项工作中，我们研究了大型语言模型 (LLM) 在充当 Minecraft 代理的挑战性环境中的使用。我们在建筑商和建筑师环境中应用和评估法学硕士，提出澄清问题并检查改进的挑战和机会。此外，我们还提供了一个与代理商在线互动的平台以及对之前作品的评估。</li>
</ul>

<h3>Title: LLMs and the Human Condition</h3>
<ul>
<li><strong>Authors: </strong>Peter Wallis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08403">https://arxiv.org/abs/2402.08403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08403">https://arxiv.org/pdf/2402.08403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08403]] LLMs and the Human Condition(https://arxiv.org/abs/2402.08403)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship with our world.</li>
<li><strong>摘要：</strong>本文提出了人类决策的三种既定理论，并描述了如何将它们整合起来以提供有目的的人类行为模型。认真对待语言作为动作的概念，然后将该模型应用于会话用户界面。基于理论的人工智能研究最近遇到了困难，这里的目的是重新激发人们对理解法学硕士实际上在做什么的兴趣，而不是对相关大型科技公司可以获取的所有数据运行不太了解的机器学习例程。当一台价格低于 50 美元的树莓派计算机比第一台商用 Cray 超级计算机快 400 倍时~\cite{crayVpi}，大型科技公司就可以真正接近拥有无限数量的猴子随机打字并生成文本，其中一些这是有道理的。通过了解 ChatGPT 表面智能的来源，也许我们可以用更少的资源施展魔法，同时对我们与世界的关系有一些了解。</li>
</ul>

<h3>Title: Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning  for Digital Twins</h3>
<ul>
<li><strong>Authors: </strong>Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, Hirley Alves</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08421">https://arxiv.org/abs/2402.08421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08421">https://arxiv.org/pdf/2402.08421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08421]] Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning  for Digital Twins(https://arxiv.org/abs/2402.08421)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme to the centralized training decentralized execution framework, allowing joint training of the agents' policies. The proposed MARL scheme, referred to as multi-agent conservative quantile regression (MA-CQR) addresses general risk-sensitive design criteria and is applied to the trajectory planning problem in drone networks, showcasing its advantages.</li>
<li><strong>摘要：</strong>数字孪生 (DT) 平台越来越被视为一种有前途的技术，用于控制、优化和监控下一代无线网络等复杂工程系统。采用DT解决方案的一个重要挑战是它们依赖离线收集的数据，缺乏对物理环境的直接访问。这种限制在多智能体系统中尤其严重，传统的多智能体强化（MARL）需要与环境进行在线交互。由于数据可用性有限所带来的认知不确定性，在线 MARL 方案直接应用于离线环境通常会失败。在这项工作中，我们提出了一种基于 DT 的无线网络的离线 MARL 方案，该方案集成了分布式 RL 和保守 Q 学习，以解决环境固有的任意不确定性和有限数据引起的认知不确定性。为了进一步利用离线数据，我们将所提出的方案适应集中训练去中心化执行框架，从而允许对代理策略进行联合训练。所提出的 MARL 方案，称为多智能体保守分位数回归（MA-CQR），解决了一般风险敏感的设计标准，并应用于无人机网络中的轨迹规划问题，展示了其优势。</li>
</ul>

<h3>Title: Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect  Disinformation Claims at Scale</h3>
<ul>
<li><strong>Authors: </strong>Freddy Heppell, Mehmet E. Bakir, Kalina Bontcheva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08467">https://arxiv.org/abs/2402.08467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08467">https://arxiv.org/pdf/2402.08467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08467]] Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect  Disinformation Claims at Scale(https://arxiv.org/abs/2402.08467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become more proficient, their misuse in large-scale viral disinformation campaigns is a growing concern. This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff, and evaluates whether such claims can be differentiated by human readers and automated tools from human-written ones. We compare war-related claims from ClaimReview, authored by IFCN-registered fact-checkers, and similar short-form content generated by ChatGPT. We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 变得越来越熟练，它们在大规模病毒式虚假信息活动中的滥用日益引起人们的关注。本研究探讨了 ChatGPT 生成有关乌克兰战争（超出其知识范围的事件）的无条件声明的能力，并评估人类读者和自动化工具是否可以将此类声明与人类编写的声明区分开来。我们比较了由 IFCN 注册的事实核查人员撰写的 ClaimReview 中与战争相关的主张，以及 ChatGPT 生成的类似简短内容。我们证明 ChatGPT 可以廉价、快速、大规模地产生真实的、针对特定目标的虚假信息，并且人类或现有的自动化工具无法可靠地区分这些声明。</li>
</ul>

<h3>Title: Large Language Models for the Automated Analysis of Optimization  Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Camilo Chacón Sartori, Christian Blum, Gabriela Ochoa</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08472">https://arxiv.org/abs/2402.08472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08472">https://arxiv.org/pdf/2402.08472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08472]] Large Language Models for the Automated Analysis of Optimization  Algorithms(https://arxiv.org/abs/2402.08472)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The ability of Large Language Models (LLMs) to generate high-quality text and code has fuelled their rise in popularity. In this paper, we aim to demonstrate the potential of LLMs within the realm of optimization algorithms by integrating them into STNWeb. This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior. Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted. In an attempt to bridge this knowledge gap, we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community. Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential of LLMs in this field.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 生成高质量文本和代码的能力推动了它们的普及。在本文中，我们旨在通过将法学硕士集成到 STNWeb 中来展示法学硕士在优化算法领域的潜力。这是一个基于网络的工具，用于生成搜索轨迹网络（STN），它是优化算法行为的可视化。尽管 STNWeb 生成的可视化对于算法设计者来说可以提供非常丰富的信息，但它们通常需要一定水平的先验知识才能解释。为了弥补这一知识差距，我们将 LLM（特别是 GPT-4）纳入 STNWeb，以生成大量书面报告，并辅以自动生成的图表，从而增强用户体验并减少采用该工具的障碍。研究社区。此外，我们的方法可以扩展到优化社区的其他工具，展示了法学硕士在该领域的多功能性和潜力。</li>
</ul>

<h3>Title: The Application of ChatGPT in Responding to Questions Related to the  Boston Bowel Preparation Scale</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqiang Liu, Yubin Wang, Zicheng Huang, Boming Xu, Yilin Zeng, Xinqi Chen, Zilong Wang, Enning Yang, Xiaoxuan Lei, Yisen Huang, Xiaobo Liu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08492">https://arxiv.org/abs/2402.08492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08492">https://arxiv.org/pdf/2402.08492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08492]] The Application of ChatGPT in Responding to Questions Related to the  Boston Bowel Preparation Scale(https://arxiv.org/abs/2402.08492)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation. ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications. This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment. Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023. These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists. Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning. Consistency was evaluated through two rounds of testing. Results: In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT shows promise in bowel preparation scoring, it currently does not match the accuracy and consistency of experienced endoscopists. Future research should focus on in-depth Fine-tuning.</li>
<li><strong>摘要：</strong>背景：结肠镜检查是胃肠病学的重要诊断工具，在很大程度上取决于良好的肠道准备。 ChatGPT，一种具有新兴智能的大型语言模型，在医疗应用中也展现出了潜力。本研究旨在评估 ChatGPT 使用波士顿肠道准备量表 (BBPS) 进行结肠镜检查评估的准确性和一致性。方法：我们回顾性收集了 2020 年至 2023 年的 233 幅结肠镜检查图像。这些图像由 3 名资深内镜医师和 3 名新手内镜医师使用 BBPS 进行评估。此外，ChatGPT 还评估了这些图像，将其分为三组并进行了特定的微调。通过两轮测试评估一致性。结果：在第一轮中，ChatGPT 的准确率在 48.93% 到 62.66% 之间变化，落后于内窥镜医生 76.68% 到 77.83% 的准确率。 ChatGPT 的 Kappa 值在 0.52 至 0.53 之间，而内窥镜医师的 Kappa 值在 0.75 至 0.87 之间。结论：虽然 ChatGPT 在肠道准备评分方面显示出良好的前景，但目前其准确性和一致性与经验丰富的内窥镜医师的准确性和一致性不符。未来的研究应该集中在深入的Fine-tuning上。</li>
</ul>

<h3>Title: A Systematic Review of Data-to-Text NLG</h3>
<ul>
<li><strong>Authors: </strong>Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08496">https://arxiv.org/abs/2402.08496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08496">https://arxiv.org/pdf/2402.08496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08496]] A Systematic Review of Data-to-Text NLG(https://arxiv.org/abs/2402.08496)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.</li>
<li><strong>摘要：</strong>本系统综述旨在对数据到文本生成研究的现状进行全面分析，重点是确定研究差距、提供未来方向并解决综述期间发现的挑战。我们彻底审查了文献，包括方法、数据集、评估指标、应用、多语言和幻觉缓解措施。我们的评论为这个快速发展的领域的未来研究提供了路线图。</li>
</ul>

<h3>Title: Auditing Counterfire: Evaluating Advanced Counterargument Generation  with Evidence and Style</h3>
<ul>
<li><strong>Authors: </strong>Preetika Verma, Kokil Jaidka, Svetlana Churina</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08498">https://arxiv.org/abs/2402.08498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08498">https://arxiv.org/pdf/2402.08498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08498]] Auditing Counterfire: Evaluating Advanced Counterargument Generation  with Evidence and Style(https://arxiv.org/abs/2402.08498)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8). In further analyses, reciprocity-style counterarguments display higher counts in most categories, possibly indicating a more creatively persuasive use of evidence. In contrast, human-written counterarguments exhibited greater argumentative richness and diversity across categories. Despite human-written arguments being favored as the most persuasive in human evaluation, the 'No Style' generated text surprisingly exhibited the highest score, prompting further exploration and investigation on the trade-offs in generation for facts and style.</li>
<li><strong>摘要：</strong>我们提出了一个新颖的数据集，用于控制反驳的组成，旨在进一步应用于论证提炼、挖掘和评估。我们的数据集构成了对 Reddit ChangeMyView 数据集中帖子的丰富反驳，这些反驳与从高质量来源检索到的证据相集成，并根据用户偏好生成，调整证据的关键属性和论证风格。由此产生的 Counterfire 语料库包含从 GPT-3.5 Turbo、Koala 和 PaLM 2 模型及其两个微调变体生成的参数 (N = 32,000)。模型评估表明，尽管单词重叠有限，但具有很强的证据释义能力，同时展示了高风格整合（“互惠”为 0.9682），显示了法学硕士吸收不同风格的能力。在所有模型中，GPT-3.5 Turbo 在论证质量评估中得分最高，表现出一致的准确性（得分 >0.8）。在进一步的分析中，互惠式反驳在大多数类别中显示出更高的计数，这可能表明证据的使用更具创造性和说服力。相比之下，人类撰写的反驳在不同类别中表现出更大的论证丰富性和多样性。尽管人类撰写的论点被认为是人类评估中最具说服力的，但“无风格”生成的文本令人惊讶地表现出最高分，这促使人们进一步探索和调查事实和风格生成的权衡。</li>
</ul>

<h3>Title: Provable Traffic Rule Compliance in Safe Reinforcement Learning on the  Open Sea</h3>
<ul>
<li><strong>Authors: </strong>Hanna Krasowski, Matthias Althoff</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08502">https://arxiv.org/abs/2402.08502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08502">https://arxiv.org/pdf/2402.08502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08502]] Provable Traffic Rule Compliance in Safe Reinforcement Learning on the  Open Sea(https://arxiv.org/abs/2402.08502)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Autonomous vehicles have to obey traffic rules. These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners. Reinforcement Learning (RL) is a promising method to find motion plans adhering to temporal logic specifications. However, vanilla RL algorithms are based on random exploration, which is inherently unsafe. To address this issue, we propose a provably safe RL approach that always complies with traffic rules. As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS). We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic. Our action verification is integrated into the RL process so that the agent only selects verified actions. In contrast to agents that only integrate the traffic rule information in the reward function, our provably safe agent always complies with the formalized rules in critical maritime traffic situations and, thus, never causes a collision.</li>
<li><strong>摘要：</strong>自动驾驶车辆必须遵守交通规则。这些规则通常使用时间逻辑来形式化，从而导致使用基于优化的运动规划器难以解决的约束。强化学习（RL）是一种很有前途的方法，可以找到符合时间逻辑规范的运动计划。然而，普通的强化学习算法基于随机探索，本质上是不安全的。为了解决这个问题，我们提出了一种可证明安全的强化学习方法，该方法始终遵守交通规则。作为一个具体的应用领域，我们考虑公海上的船舶，这些船舶必须遵守《国际海上避碰规则公约》(COLREGS)。我们引入了一种有效的验证方法，该方法可以确定操作是否符合使用时间逻辑形式化的 COLREGS。我们的动作验证已集成到 RL 流程中，以便代理仅选择经过验证的动作。与仅将交通规则信息集成到奖励函数中的代理相比，我们可证明安全的代理在关键的海上交通情况下始终遵守形式化规则，因此永远不会引起碰撞。</li>
</ul>

<h3>Title: Fairness Auditing with Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Martijn de Vos, Akash Dhasade, Jade Garcia Bourrée, Anne-Marie Kermarrec, Erwan Le Merrer, Benoit Rottembourg, Gilles Tredan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08522">https://arxiv.org/abs/2402.08522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08522">https://arxiv.org/pdf/2402.08522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08522]] Fairness Auditing with Multi-Agent Collaboration(https://arxiv.org/abs/2402.08522)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Existing work in fairness audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.</li>
<li><strong>摘要：</strong>现有的公平审计工作假设代理人独立运作。在本文中，我们考虑多个代理针对不同任务审核同一平台的情况。代理有两个杠杆：他们的协作策略（无论是否事先进行协调）以及他们的采样方法。我们从理论上研究了代理独立操作或协作时它们的相互作用。令人惊讶的是，我们证明，协调有时可能会损害审计准确性，而不协调的协作通常会产生良好的结果。对现实世界数据集的实验证实了这一观察结果，因为不协调协作的审计准确性与协作最优抽样的审计准确性相匹配。</li>
</ul>

<h3>Title: Higher Layers Need More LoRA Experts</h3>
<ul>
<li><strong>Authors: </strong>Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen Zhang, Xiaoyuan Guo, Jie Yang, VS Subrahmanian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08562">https://arxiv.org/abs/2402.08562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08562">https://arxiv.org/pdf/2402.08562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08562]] Higher Layers Need More LoRA Experts(https://arxiv.org/abs/2402.08562)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines. We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code is available at https://github.com/GCYZSL/MoLA.</li>
<li><strong>摘要：</strong>低秩适应 (LoRA) 等参数高效调整 (PEFT) 技术可提高大型语言模型的训练效率，但它们对模型性能的影响仍然有限。最近的努力将 LoRA 和专家混合 (MoE) 相结合，以提高 PEFT 方法的性能。尽管取得了令人鼓舞的成果，但利用 MoE 提高 LoRA 效率的研究仍处于早期阶段。最近的研究表明，教育部架构中的专家具有不同的优势，并且也表现出一些冗余。这个说法是否也适用于参数高效的 MoE？在本文中，我们介绍了一种新颖的参数高效 MoE 方法， \textit{\textbf{M}oE-L\textbf{o}RA 和 \textbf{L}ayer-wise Expert \textbf{A}llocation (MoLA)对于基于 Transformer 的模型，其中每个模型层都可以灵活地雇用不同数量的 LoRA 专家。我们研究了几种具有不同分层专家配置的架构。在六个著名的 NLP 和常识 QA 基准上进行的实验表明，与所有基准相比，MoLA 实现了相同或更好的性能。我们发现，将更多的 LoRA 专家分配到更高层可以进一步增强具有一定数量专家的模型的有效性。由于参数少得多，这种分配策略的性能优于每层专家数量相同的设置。这项工作可以作为一种即插即用的参数高效调整方法广泛应用于各种应用。代码可在 https://github.com/GCYZSL/MoLA 获取。</li>
</ul>

<h3>Title: Artificial Intelligence for Literature Reviews: Opportunities and  Challenges</h3>
<ul>
<li><strong>Authors: </strong>Francisco Bolanos, Angelo Salatino, Francesco Osborne, Enrico Motta</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08565">https://arxiv.org/abs/2402.08565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08565">https://arxiv.org/pdf/2402.08565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08565]] Artificial Intelligence for Literature Reviews: Opportunities and  Challenges(https://arxiv.org/abs/2402.08565)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research.</li>
<li><strong>摘要：</strong>这篇手稿对人工智能（AI）在系统文献综述（SLR）中的使用进行了全面的回顾。 SLR 是一种严格且有组织的方法，用于评估和整合先前针对给定主题的研究。人们已经开发了许多工具来辅助和部分自动化 SLR 流程。人工智能在这一领域的作用日益增强，在为研究人员提供更有效的支持方面显示出巨大的潜力，朝着半自动创建文献综述的方向发展。我们的研究重点是如何将人工智能技术应用于单反相机的半自动化，特别是在筛选和提取阶段。我们使用结合了 23 种传统功能和 11 种人工智能功能的框架来研究 21 种领先的 SLR 工具。我们还分析了 11 种最新工具，这些工具利用大型语言模型来搜索文献和协助学术写作。最后，本文讨论了该领域的当前趋势，概述了主要的研究挑战，并提出了未来研究的方向。</li>
</ul>

<h3>Title: Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM  Agents Exponentially Fast</h3>
<ul>
<li><strong>Authors: </strong>Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.CV, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08567">https://arxiv.org/abs/2402.08567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08567">https://arxiv.org/pdf/2402.08567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08567]] Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM  Agents Exponentially Fast(https://arxiv.org/abs/2402.08567)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.</li>
<li><strong>摘要：</strong>多模式大语言模型 (MLLM) 代理可以接收指令、捕获图像、从内存中检索历史记录并决定使用哪些工具。尽管如此，红队工作表明，对抗性图像/提示可能会越狱 MLLM 并导致不一致的行为。在这项工作中，我们报告了多代理环境中更严重的安全问题，称为传染性越狱。它需要对手简单地越狱单个代理，并且在没有对手的任何进一步干预的情况下，（几乎）所有代理都将呈指数级快速感染并表现出有害行为。为了验证传染性越狱的可行性，我们模拟了包含多达一百万个 LLaVA-1.5 代理的多代理环境，并采用随机配对聊天作为多代理交互的概念验证实例。我们的结果表明，将（传染性）对抗性图像输入任何随机选择的代理的内存中足以实现传染性越狱。最后，我们推导出一个简单的原则来确定防御机制是否可以证明抑制传染性越狱的传播，但如何设计满足该原则的实用防御仍然是一个有待研究的悬而未决的问题。我们的项目页面位于 https://sail-sg.github.io/Agent-Smith/。</li>
</ul>

<h3>Title: Test-Time Backdoor Attacks on Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08577">https://arxiv.org/abs/2402.08577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08577">https://arxiv.org/pdf/2402.08577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08577]] Test-Time Backdoor Attacks on Multimodal Large Language Models(https://arxiv.org/abs/2402.08577)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks. Our project page is available at https://sail-sg.github.io/AnyDoor/.</li>
<li><strong>摘要：</strong>后门攻击通常是通过污染训练数据来执行的，这样触发器就可以在测试阶段激活预定的有害影响。在这项工作中，我们提出了 AnyDoor，这是一种针对多模态大语言模型 (MLLM) 的测试时后门攻击，其中涉及使用对抗性测试图像（共享相同的通用扰动）将后门注入到文本模态中，无需访问或修改的训练数据。 AnyDoor 采用了通用对抗性攻击中使用的类似技术，但其独特之处在于其能够将有害影响的设置和激活时间解耦。在我们的实验中，我们针对 LLaVA-1.5、MiniGPT-4、InstructBLIP 和 BLIP-2 等流行的 MLLM 验证了 AnyDoor 的有效性，并提供了全面的消融研究。值得注意的是，由于后门是通过普遍扰动注入的，AnyDoor可以动态改变其后门触发提示/有害影响，这给防御后门攻击带来了新的挑战。我们的项目页面位于 https://sail-sg.github.io/AnyDoor/。</li>
</ul>

<h3>Title: Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Haeju Lee, Minchan Jeong, Se-Young Yun, Kee-Eung Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08594">https://arxiv.org/abs/2402.08594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08594">https://arxiv.org/pdf/2402.08594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08594]] Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning(https://arxiv.org/abs/2402.08594)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in a multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target prompt. We show extensive experimental results on the standard benchmark NLP tasks, where our Bayesian multi-task transfer learning approach outperforms the state-of-the-art methods in many settings. Furthermore, our approach requires no auxiliary models other than the prompt itself, achieving a high degree of parameter efficiency.</li>
<li><strong>摘要：</strong>提示调优，即对提示进行优化，使大规模预训练语言模型适应下游任务，而不是微调整个模型参数，已被证明在多任务迁移学习中训练提示时特别有效环境。这些方法通常涉及对每个源任务单独训练提示，然后将它们聚合以提供目标任务提示的初始化。然而，这种方法严重忽略了这样一个事实，即某些源任务可能会相互产生负面或正面干扰。我们认为，当我们通过训练源提示从源任务中提取知识时，我们需要考虑源任务之间的这种相关性，以便更好地迁移到目标任务。为此，我们提出了一种贝叶斯方法，我们使用跨源任务的提示的后验分布。我们利用斯坦因变分梯度下降获得与后验样本相对应的代表性源提示，然后将其聚合以构成初始目标提示。我们展示了标准基准 NLP 任务的广泛实验结果，其中我们的贝叶斯多任务迁移学习方法在许多设置中优于最先进的方法。此外，我们的方法除了提示本身之外不需要任何辅助模型，从而实现了高度的参数效率。</li>
</ul>

<h3>Title: Knowledge Editing on Black-box Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshuai Song, Zhengyang Wang, Keqing He, Guanting Dong, Jinxu Zhao, Weiran Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08631">https://arxiv.org/abs/2402.08631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08631">https://arxiv.org/pdf/2402.08631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08631]] Knowledge Editing on Black-box Large Language Models(https://arxiv.org/abs/2402.08631)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average $+20.82\%\uparrow$).</li>
<li><strong>摘要：</strong>知识编辑（KE）旨在高效、精确地修改大型语言模型（LLM）的行为，以更新特定知识，而不会对其他知识产生负面影响。目前的研究主要集中在白盒法学硕士编辑上，忽略了一个重要的场景：黑盒法学硕士编辑，即通过界面访问法学硕士，仅提供文本输出。针对现有评估不适用于黑盒LLM编辑且缺乏全面性的局限性，我们提出了多视角评估框架，首次纳入风格保留评估。为了解决当前方法中编辑数据和样式过度编辑的隐私泄露问题，我们引入了一种新颖的 postEdit 框架，通过下游后处理解决隐私问题，并通过对原始响应进行细粒度编辑来保持文本样式的一致性。对两个基准的实验和分析表明，postEdit 优于所有基线并实现了很强的泛化性，特别是在风格保留方面有巨大的改进（平均 $+20.82\%\uparrow$）。</li>
</ul>

<h3>Title: SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14  Languages</h3>
<ul>
<li><strong>Authors: </strong>Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir Araujo, Abinew Ali Ayele, Pavan Baswani, Meriem Beloucif, Chris Biemann, Sofia Bourhim, Christine De Kock, Genet Shanko Dekebo, Oumaima Hourrane, Gopichand Kanumolu, Lokesh Madasu, Samuel Rutunda, Manish Shrivastava, Thamar Solorio, Nirmal Surange, Hailegnaw Getaneh Tilaye, Krishnapriya Vishnubhotla, Genta Winata, Seid Muhie Yimam, Saif M. Mohammad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08638">https://arxiv.org/abs/2402.08638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08638">https://arxiv.org/pdf/2402.08638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08638]] SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14  Languages(https://arxiv.org/abs/2402.08638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. The scores are obtained using a comparative annotation framework. We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP. We further report experiments for each language and across the different languages.</li>
<li><strong>摘要：</strong>探索和量化语义相关性是表示语言的核心。它对各种 NLP 任务具有重大影响，包括提供对大型语言模型 (LLM) 的功能和性能的见解。虽然早期的 NLP 研究主要关注语义相似性（通常是在英语语言环境中），但我们转而研究更广泛的语义相关性现象。在本文中，我们提出了 SemRel，这是一个由 14 种语言的母语人士注释的新语义相关性数据集集合：南非荷兰语、阿尔及利亚阿拉伯语、阿姆哈拉语、英语、豪萨语、印地语、印度尼西亚语、基尼亚卢旺达语、马拉地语、摩洛哥阿拉伯语、现代标准阿拉伯语、旁遮普语、西班牙语和泰卢固语。这些语言起源于五个不同的语系，主要在非洲和亚洲使用，这些地区的 NLP 资源相对有限。 SemRel 数据集中的每个实例都是一个与分数关联的句子对，该分数表示两个句子之间的语义文本相关程度。分数是使用比较注释框架获得的。我们描述了数据收集和注释过程、构建数据集时的相关挑战及其在 NLP 中的影响和实用性。我们进一步报告每种语言以及跨不同语言的实验。</li>
</ul>

<h3>Title: Tandem Transformers for Inference Efficient LLMs</h3>
<ul>
<li><strong>Authors: </strong>Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08644">https://arxiv.org/abs/2402.08644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08644">https://arxiv.org/pdf/2402.08644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08644]] Tandem Transformers for Inference Efficient LLMs(https://arxiv.org/abs/2402.08644)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.</li>
<li><strong>摘要：</strong>传统大型语言模型 (LLM) 的自回归本质本质上限制了推理速度，因为标记是按顺序生成的。虽然推测和并行解码技术试图缓解这一问题，但它们面临着局限性：要么依赖不太准确的较小模型来生成，要么无法充分利用基础 LLM 的表示。我们引入了一种新颖的架构，即串联变压器来解决这些问题。该架构独特地结合了（1）小型自回归模型和（2）以块模式运行的大型模型（同时处理多个令牌）。通过将注意力集中在大型模型更丰富的表示上，小型模型的预测准确性得到了显着提高。在 PaLM2 预训练数据集上，PaLM2-Bison 和 PaLM2-Gecko 的串联显示，与独立的 PaLM2-Gecko 相比，下一个标记的预测精度提高了 3.3%，与下游性能相当的 PaLM2-Otter 模型相比，速度提高了 1.16 倍。我们进一步将串联模型合并到推测解码（SPEED）框架中，其中大模型验证来自小模型的令牌。这确保了 PaLM2-Bison 和 PaLM2-Gecko 的串联实现了显着的加速（比在 SPEED 中使用普通 PaLM2-Gecko 快约 1.14 倍），同时保持相同的下游任务精度。</li>
</ul>

<h3>Title: Improving Generalization in Semantic Parsing by Increasing Natural  Language Variation</h3>
<ul>
<li><strong>Authors: </strong>Irina Saparina, Mirella Lapata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08666">https://arxiv.org/abs/2402.08666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08666">https://arxiv.org/pdf/2402.08666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08666]] Improving Generalization in Semantic Parsing by Increasing Natural  Language Variation(https://arxiv.org/abs/2402.08666)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Text-to-SQL semantic parsing has made significant progress in recent years, with various models demonstrating impressive performance on the challenging Spider benchmark. However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions. This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation. In this work, we use data augmentation to enhance the robustness of text-to-SQL parsers against natural language variations. Existing approaches generate question reformulations either via models trained on Spider or only introduce local changes. In contrast, we leverage the capabilities of large language models to generate more realistic and diverse questions. Using only a few prompts, we achieve a two-fold increase in the number of questions in Spider. Training on this augmented dataset yields substantial improvements on a range of evaluation sets, including robustness benchmarks and out-of-domain data.</li>
<li><strong>摘要：</strong>近年来，文本到 SQL 语义解析取得了重大进展，各种模型在具有挑战性的 Spider 基准测试中展示了令人印象深刻的性能。然而，研究还表明，即使面对先前（准确）解析的表达式的小扰动，这些模型也常常难以泛化。这主要是由于Spider中问题的语言形式过于具体、不自然、变化有限。在这项工作中，我们使用数据增强来增强文本到 SQL 解析器针对自然语言变化的鲁棒性。现有方法通过在 Spider 上训练的模型或仅引入局部更改来生成问题重新表述。相比之下，我们利用大型语言模型的功能来生成更现实和多样化的问题。仅使用一些提示，我们就将 Spider 中的问题数量增加了两倍。对该增强数据集的训练对一系列评估集产生了重大改进，包括鲁棒性基准和域外数据。</li>
</ul>

<h3>Title: Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuqing Liu, Yu Wang, Lichao Sun, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08670">https://arxiv.org/abs/2402.08670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08670">https://arxiv.org/pdf/2402.08670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08670]] Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models(https://arxiv.org/abs/2402.08670)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics. However, the application of LVLMs in this field is still limited due to the following complexities: First, LVLMs lack user preference knowledge as they are trained from vast general datasets. Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences. To overcome these issues, we propose the novel reasoning scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation. We utilize user history as in-context user preferences to address the first challenge. Next, we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences over candidate items. We conduct comprehensive experiments across four datasets with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results indicate the efficacy of VST.</li>
<li><strong>摘要：</strong>大型视觉语言模型（LVLM）的开发提供了解决传统多模态推荐所面临的挑战的潜力，这要归功于它们对静态图像和文本动态的熟练理解。然而，由于以下复杂性，LVLM 在该领域的应用仍然受到限制：首先，LVLM 缺乏用户偏好知识，因为它们是从大量通用数据集进行训练的。其次，LVLM 在解决涉及离散、噪声和冗余图像序列的场景中的多个图像动态方面遇到了挫折。为了克服这些问题，我们提出了名为 Rec-GPT4V 的新颖推理方案：利用大型视觉语言模型进行多模态推荐的视觉摘要思想（VST）。我们利用用户历史记录作为上下文中的用户偏好来解决第一个挑战。接下来，我们提示 LVLM 生成项目图像摘要，并利用自然语言空间中的图像理解结合项目标题来查询用户对候选项目的偏好。我们使用三个 LVLM 在四个数据集上进行了全面的实验：GPT4-V、LLaVa-7b 和 LLaVa-13b。数值结果表明了 VST 的功效。</li>
</ul>

<h3>Title: COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability</h3>
<ul>
<li><strong>Authors: </strong>Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08679">https://arxiv.org/abs/2402.08679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08679">https://arxiv.org/pdf/2402.08679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08679]] COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability(https://arxiv.org/abs/2402.08679)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的越狱最近受到越来越多的关注。为了全面评估LLM安全性，必须考虑具有不同属性的越狱，例如上下文连贯性和情感/风格变化，因此研究可控越狱，即如何对LLM攻击实施控制是有益的。在本文中，我们正式阐述了可控攻击生成问题，并在该问题与可控文本生成（自然语言处理的一个深入研究的主题）之间建立了新颖的联系。基于这种联系，我们采用了 Langevin Dynamics 的基于能量的约束解码（COLD），这是一种最先进、高效的可控文本生成算法，并引入了 COLD-Attack 框架，该框架统一并自动化了搜索在流畅性、隐秘性、情感和左右连贯性等各种控制要求下进行对抗性 LLM 攻击。 COLD-Attack 带来的可控性带来了多种新的越狱场景，这些场景不仅涵盖了生成流畅后缀攻击的标准设置，而且还允许我们解决新的可控攻击设置，例如以最少的释义对抗性地修改用户查询，以及插入隐秘内容在左右连贯的背景下进行攻击。我们对各种LLM（Llama-2、Mistral、Vicuna、Guanaco、GPT-3.5）进行的大量实验表明COLD-Attack具有广泛的适用性、强可控性、高成功率和攻击可转移性。我们的代码可在 https://github.com/Yu-Fangxu/COLD-Attack 获取。</li>
</ul>

<h3>Title: Mitigating Object Hallucination in Large Vision-Language Models via  Classifier-Free Guidance</h3>
<ul>
<li><strong>Authors: </strong>Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08680">https://arxiv.org/abs/2402.08680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08680">https://arxiv.org/pdf/2402.08680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08680]] Mitigating Object Hallucination in Large Vision-Language Models via  Classifier-Free Guidance(https://arxiv.org/abs/2402.08680)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V.</li>
<li><strong>摘要：</strong>大视觉语言模型（LVLM）的进步日益凸显了它们容易产生图像中不存在物体的幻觉这一关键问题。为了解决这个问题，以前的工作重点是使用专门策划的数据集或强大的 LLM（例如 GPT-3.5）来纠正 LVLM 的输出。然而，这些方法需要昂贵的训练/微调或 API 访问高级 LLM 来纠正模型的生成后输出。在本文中，我们通过引入一个名为 Mitigating幻觉通过无分类引导（MARINE）的框架来应对这一挑战，该框架既无需训练，也无需API，并且可以有效且高效地减少生成过程中的物体幻觉。具体来说，MARINE 通过集成现有的开源视觉模型丰富了 LVLM 的视觉环境，并采用无分类器引导来合并额外的对象接地功能，以提高 LVLM 各代的精度。通过使用多种评估指标对 6 美元流行的 LVLM 进行综合评估，我们证明了 MARINE 的有效性，它甚至优于现有的基于微调的方法。值得注意的是，根据 GPT-4V 的评估，它不仅减少了幻觉，还提高了 LVLM 生成的细节。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
