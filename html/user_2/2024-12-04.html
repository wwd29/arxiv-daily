<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-04</h1>
<h3>Title: The use of large language models to enhance cancer clinical trial educational materials</h3>
<ul>
<li><strong>Authors: </strong>Mingye Gao, Aman Varshney, Shan Chen, Vikram Goddla, Jack Gallifant, Patrick Doyle, Claire Novack, Maeve Dillon-Martin, Teresia Perkins, Xinrong Correia, Erik Duhaime, Howard Isenstein, Elad Sharon, Lisa Soleymani Lehmann, David Kozono, Brian Anthony, Dmitriy Dligach, Danielle S. Bitterman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01955">https://arxiv.org/abs/2412.01955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01955">https://arxiv.org/pdf/2412.01955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01955]] The use of large language models to enhance cancer clinical trial educational materials(https://arxiv.org/abs/2412.01955)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Cancer clinical trials often face challenges in recruitment and engagement due to a lack of participant-facing informational and educational resources. This study investigated the potential of Large Language Models (LLMs), specifically GPT4, in generating patient-friendly educational content from clinical trial informed consent forms. Using data from this http URL, we employed zero-shot learning for creating trial summaries and one-shot learning for developing multiple-choice questions, evaluating their effectiveness through patient surveys and crowdsourced annotation. Results showed that GPT4-generated summaries were both readable and comprehensive, and may improve patients' understanding and interest in clinical trials. The multiple-choice questions demonstrated high accuracy and agreement with crowdsourced annotators. For both resource types, hallucinations were identified that require ongoing human oversight. The findings demonstrate the potential of LLMs "out-of-the-box" to support the generation of clinical trial education materials with minimal trial-specific engineering, but implementation with a human-in-the-loop is still needed to avoid misinformation risks.</li>
<li><strong>摘要：</strong>由于缺乏面向参与者的信息和教育资源，癌症临床试验在招募和参与方面经常面临挑战。本研究调查了大型语言模型 (LLM)，特别是 GPT4，在从临床试验知情同意书中生成患者友好的教育内容方面的潜力。使用来自此 http URL 的数据，我们采用零样本学习来创建试验摘要，采用一样本学习来开发多项选择题，并通过患者调查和众包注释评估其有效性。结果表明，GPT4 生成的摘要既可读又全面，可以提高患者对临床试验的理解和兴趣。多项选择题表现出很高的准确性和与众包注释者的一致性。对于这两种资源类型，都发现了需要持续人工监督的幻觉。研究结果表明，LLM 具有“开箱即用”的潜力，可以以最少的试验特定工程支持生成临床试验教育材料，但仍需要以人为本的方式实施，以避免错误信息风险。</li>
</ul>

<h3>Title: A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil and Sinhala</h3>
<ul>
<li><strong>Authors: </strong>Surangika Ranathunga, Asanka Ranasinghea, Janaka Shamala, Ayodya Dandeniyaa, Rashmi Galappaththia, Malithi Samaraweeraa</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02056">https://arxiv.org/abs/2412.02056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02056">https://arxiv.org/pdf/2412.02056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02056]] A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil and Sinhala(https://arxiv.org/abs/2412.02056)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents a multi-way parallel English-Tamil-Sinhala corpus annotated with Named Entities (NEs), where Sinhala and Tamil are low-resource languages. Using pre-trained multilingual Language Models (mLMs), we establish new benchmark Named Entity Recognition (NER) results on this dataset for Sinhala and Tamil. We also carry out a detailed investigation on the NER capabilities of different types of mLMs. Finally, we demonstrate the utility of our NER system on a low-resource Neural Machine Translation (NMT) task. Our dataset is publicly released: this https URL.</li>
<li><strong>摘要：</strong>本文介绍了一个多向并行英语-泰米尔语-僧伽罗语语料库，该语料库带有命名实体 (NE) 注释，其中僧伽罗语和泰米尔语是资源匮乏的语言。使用预先训练的多语言语言模型 (mLM)，我们在此数据集上为僧伽罗语和泰米尔语建立了新的基准命名实体识别 (NER) 结果。我们还对不同类型的 mLM 的 NER 功能进行了详细调查。最后，我们展示了我们的 NER 系统在资源匮乏的神经机器翻译 (NMT) 任务上的实用性。我们的数据集已公开发布：此 https URL。</li>
</ul>

<h3>Title: Let's Think Var-by-Var: Large Language Models Enable Ad Hoc Probabilistic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shepard Xia, Brian Lu, Jason Eisner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02081">https://arxiv.org/abs/2412.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02081">https://arxiv.org/pdf/2412.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02081]] Let's Think Var-by-Var: Large Language Models Enable Ad Hoc Probabilistic Reasoning(https://arxiv.org/abs/2412.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>A hallmark of intelligence is the ability to flesh out underspecified situations using "common sense." We propose to extract that common sense from large language models (LLMs), in a form that can feed into probabilistic inference. We focus our investigation on $\textit{guesstimation}$ questions such as "How much are Airbnb listings in Newark, NJ?" Formulating a sensible answer without access to data requires drawing on, and integrating, bits of common knowledge about how $\texttt{Price}$ and $\texttt{Location}$ may relate to other variables, such as $\texttt{Property Type}$. Our framework answers such a question by synthesizing an $\textit{ad hoc}$ probabilistic model. First we prompt an LLM to propose a set of random variables relevant to the question, followed by moment constraints on their joint distribution. We then optimize the joint distribution $p$ within a log-linear family to maximize the overall constraint satisfaction. Our experiments show that LLMs can successfully be prompted to propose reasonable variables, and while the proposed numerical constraints can be noisy, jointly optimizing for their satisfaction reconciles them. When evaluated on probabilistic questions derived from three real-world tabular datasets, we find that our framework performs comparably to a direct prompting baseline in terms of total variation distance from the dataset distribution, and is similarly robust to noise.</li>
<li><strong>摘要：</strong>智能的一个标志是能够使用“常识”充实未指定的情况。我们建议从大型语言模型 (LLM) 中提取常识，以可用于概率推理的形式。我们将调查重点放在“猜测”问题上，例如“新泽西州纽瓦克的 Airbnb 房源多少钱？”。在没有数据的情况下制定合理的答案需要利用和整合关于“价格”和“位置”可能与其他变量（例如“房产类型”）相关的一些常识。我们的框架通过合成一个“临时”概率模型来回答这样的问题。首先，我们提示 LLM 提出一组与问题相关的随机变量，然后对它们的联合分布进行矩约束。然后，我们在对数线性族中优化联合分布 $p$，以最大化整体约束满足度。我们的实验表明，可以成功提示 LLM 提出合理的变量，尽管所提出的数值约束可能存在噪声，但联合优化以满足这些约束可以协调它们。当对来自三个真实世界表格数据集的概率问题进行评估时，我们发现我们的框架在与数据集分布的总变异距离方面的表现与直接提示基线相当，并且同样具有抗噪声能力。</li>
</ul>

<h3>Title: Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian, Kun Wang, Yong Liu, Jing Shao, Hui Xiong, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02104">https://arxiv.org/abs/2412.02104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02104">https://arxiv.org/pdf/2412.02104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02104]] Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey(https://arxiv.org/abs/2412.02104)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid development of Artificial Intelligence (AI) has revolutionized numerous fields, with large language models (LLMs) and computer vision (CV) systems driving advancements in natural language understanding and visual processing, respectively. The convergence of these technologies has catalyzed the rise of multimodal AI, enabling richer, cross-modal understanding that spans text, vision, audio, and video modalities. Multimodal large language models (MLLMs), in particular, have emerged as a powerful framework, demonstrating impressive capabilities in tasks like image-text generation, visual question answering, and cross-modal retrieval. Despite these advancements, the complexity and scale of MLLMs introduce significant challenges in interpretability and explainability, essential for establishing transparency, trustworthiness, and reliability in high-stakes applications. This paper provides a comprehensive survey on the interpretability and explainability of MLLMs, proposing a novel framework that categorizes existing research across three perspectives: (I) Data, (II) Model, (III) Training \& Inference. We systematically analyze interpretability from token-level to embedding-level representations, assess approaches related to both architecture analysis and design, and explore training and inference strategies that enhance transparency. By comparing various methodologies, we identify their strengths and limitations and propose future research directions to address unresolved challenges in multimodal explainability. This survey offers a foundational resource for advancing interpretability and transparency in MLLMs, guiding researchers and practitioners toward developing more accountable and robust multimodal AI systems.</li>
<li><strong>摘要：</strong>人工智能 (AI) 的快速发展彻底改变了许多领域，大型语言模型 (LLM) 和计算机视觉 (CV) 系统分别推动了自然语言理解和视觉处理的进步。这些技术的融合催化了多模态 AI 的兴起，实现了涵盖文本、视觉、音频和视频模态的更丰富的跨模态理解。多模态大型语言模型 (MLLM) 尤其已经成为一个强大的框架，在图像文本生成、视觉问答和跨模态检索等任务中展示了令人印象深刻的功能。尽管取得了这些进步，但 MLLM 的复杂性和规模给可解释性和可解释性带来了重大挑战，而可解释性和可解释性对于在高风险应用中建立透明度、可信度和可靠性至关重要。本文对 MLLM 的可解释性和可解释性进行了全面调查，提出了一个新颖的框架，将现有研究分为三个角度：(I) 数据、(II) 模型、(III) 训练和推理。我们系统地分析了从 token 级到 embedding 级表示的可解释性，评估了与架构分析和设计相关的方法，并探索了增强透明度的训练和推理策略。通过比较各种方法，我们确定了它们的优势和局限性，并提出了未来的研究方向，以解决多模态可解释性中尚未解决的挑战。这项调查为提高 MLLM 的可解释性和透明度提供了基础资源，指导研究人员和从业者开发更可靠、更强大的多模态 AI 系统。</li>
</ul>

<h3>Title: Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Fernando Gabriela Garcia, Spencer Burns, Harrison Fuller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02149">https://arxiv.org/abs/2412.02149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02149">https://arxiv.org/pdf/2412.02149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02149]] Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms(https://arxiv.org/abs/2412.02149)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce ChatCite, a novel method leveraging large language models (LLMs) for generating comparative literature summaries. The ability to summarize research papers with a focus on key comparisons between studies is an essential task in academic research. Existing summarization models, while effective at generating concise summaries, fail to provide deep comparative insights. ChatCite addresses this limitation by incorporating a multi-step reasoning mechanism that extracts critical elements from papers, incrementally builds a comparative summary, and refines the output through a reflective memory process. We evaluate ChatCite on a custom dataset, CompLit-LongContext, consisting of 1000 research papers with annotated comparative summaries. Experimental results show that ChatCite outperforms several baseline methods, including GPT-4, BART, T5, and CoT, across various automatic evaluation metrics such as ROUGE and the newly proposed G-Score. Human evaluation further confirms that ChatCite generates more coherent, insightful, and fluent summaries compared to these baseline models. Our method provides a significant advancement in automatic literature review generation, offering researchers a powerful tool for efficiently comparing and synthesizing scientific research.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 ChatCite，这是一种利用大型语言模型 (LLM) 生成比较文献摘要的新方法。能够总结研究论文并重点关注研究之间的关键比较是学术研究中的一项基本任务。现有的摘要模型虽然能够有效地生成简洁的摘要，但无法提供深入的比较见解。ChatCite 通过结合多步骤推理机制来解决这一限制，该机制从论文中提取关键要素，逐步构建比较摘要，并通过反射记忆过程细化输出。我们在自定义数据集 CompLit-LongContext 上评估 ChatCite，该数据集包含 1000 篇带有注释的比较摘要的研究论文。实验结果表明，ChatCite 在各种自动评估指标（例如 ROUGE 和新提出的 G-Score）上的表现优于几种基线方法，包括 GPT-4、BART、T5 和 CoT。人工评估进一步证实，与这些基线模型相比，ChatCite 生成的摘要更连贯、更有见地、更流畅。我们的方法在自动文献综述生成方面取得了重大进步，为研究人员提供了有效比较和综合科学研究的有力工具。</li>
</ul>

<h3>Title: BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Quanjiang Guo, Yihong Dong, Ling Tian, Zhao Kang, Yu Zhang, Sijie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02228">https://arxiv.org/abs/2412.02228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02228">https://arxiv.org/pdf/2412.02228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02228]] BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition(https://arxiv.org/abs/2412.02228)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Despite the recent success of two-stage prototypical networks in few-shot named entity recognition (NER), challenges such as over/under-detected false spans in the span detection stage and unaligned entity prototypes in the type classification stage persist. Additionally, LLMs have not proven to be effective few-shot information extractors in general. In this paper, we propose an approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to address these issues. We introduce a boundary-aware contrastive learning strategy to enhance the LLM's ability to perceive entity boundaries for generalized entity spans. Additionally, we utilize LoRAHub to align information from the target domain to the source domain, thereby enhancing adaptive cross-domain classification capabilities. Extensive experiments across various benchmarks demonstrate that our framework outperforms prior methods, validating its effectiveness. In particular, the proposed strategies demonstrate effectiveness across a range of LLM architectures. The code and data are released on this https URL.</li>
<li><strong>摘要：</strong>尽管两阶段原型网络最近在少样本命名实体识别 (NER) 中取得了成功，但诸如跨度检测阶段的过度/不足检测错误跨度和类型分类阶段的未对齐实体原型等挑战仍然存在。此外，LLM 已被证明不是有效的少样本信息提取器。在本文中，我们提出了一种称为边界感知 LLM 的少样本命名实体识别方法来解决这些问题。我们引入了一种边界感知对比学习策略，以增强 LLM 感知广义实体跨度的实体边界的能力。此外，我们利用 LoRAHub 将信息从目标域对齐到源域，从而增强自适应跨域分类能力。在各种基准测试中进行的大量实验表明，我们的框架优于以前的方法，验证了它的有效性。特别是，所提出的策略在一系列 LLM 架构中都表现出了有效性。代码和数据在此 https URL 上发布。</li>
</ul>

<h3>Title: Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</h3>
<ul>
<li><strong>Authors: </strong>Da Ma, Lu Chen, Situo Zhang, Yuxun Miao, Su Zhu, Zhi Chen, Hongshen Xu, Hanqi Li, Shuai Fan, Lei Pan, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02252">https://arxiv.org/abs/2412.02252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02252">https://arxiv.org/pdf/2412.02252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02252]] Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity(https://arxiv.org/abs/2412.02252)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The increasing context window size in Large Language Models (LLMs), such as the GPT and LLaMA series, has improved their ability to tackle complex, long-text tasks, but at the cost of inference efficiency, particularly regarding memory and computational complexity. Existing methods, including selective token retention and window-based attention, improve efficiency but risk discarding important tokens needed for future text generation. In this paper, we propose an approach that enhances LLM efficiency without token loss by reducing the memory and computational load of less important tokens, rather than discarding this http URL address two challenges: 1) investigating the distribution of important tokens in the context, discovering recent tokens are more important than distant tokens in context, and 2) optimizing resources for distant tokens by sharing attention scores across layers. The experiments show that our method saves $35\%$ KV cache without compromising the performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM)（例如 GPT 和 LLaMA 系列）中上下文窗口大小的增加提高了它们处理复杂长文本任务的能力，但却以牺牲推理效率为代价，特别是在内存和计算复杂度方面。现有方法（包括选择性标记保留和基于窗口的注意）提高了效率，但存在丢弃未来文本生成所需的重要标记的风险。在本文中，我们提出了一种方法，通过减少不太重要的标记的内存和计算负载而不是丢弃此 http URL 来提高 LLM 效率而不会丢失标记，这解决了两个挑战：1）调查上下文中重要标记的分布，发现最近的标记比上下文中的远程标记更重要，2）通过跨层共享注意力分数来优化远程标记的资源。实验表明，我们的方法在不影响性能的情况下节省了 $35\%$ KV 缓存。</li>
</ul>

<h3>Title: MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines</h3>
<ul>
<li><strong>Authors: </strong>Preetika Verma, Kokil Jaidka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02271">https://arxiv.org/abs/2412.02271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02271">https://arxiv.org/pdf/2412.02271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02271]] MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines(https://arxiv.org/abs/2412.02271)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce the MediaSpin dataset aiming to help in the development of models that can detect different forms of media bias present in news headlines, developed through human-supervised and -validated Large Language Model (LLM) labeling of media bias. This corpus comprises 78,910 pairs of news headlines and annotations with explanations of the 13 distinct types of media bias categories assigned. We demonstrate the usefulness of our dataset for automated bias detection in news edits.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 MediaSpin 数据集，旨在帮助开发能够检测新闻标题中存在的不同形式媒体偏见的模型，该模型是通过人工监督和验证的大型语言模型 (LLM) 媒体偏见标记开发的。该语料库包含 78,910 对新闻标题和注释，并解释了分配的 13 种不同类型的媒体偏见类别。我们展示了我们的数据集在新闻编辑中自动检测偏见的实用性。</li>
</ul>

<h3>Title: A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Changzhi Zhou, Dandan Song, Yuhang Tian, Zhijing Wu, Hao Wang, Xinyu Zhang, Jun Yang, Ziyi Yang, Shuhao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02279">https://arxiv.org/abs/2412.02279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02279">https://arxiv.org/pdf/2412.02279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02279]] A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis(https://arxiv.org/abs/2412.02279)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have garnered increasing attention in the field of natural language processing, revolutionizing numerous downstream tasks with powerful reasoning and generation abilities. For example, In-Context Learning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box LLMs to execute downstream tasks by analogy learning without any fine-tuning. Besides, in a fine-tuning-dependent paradigm where substantial training data exists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods, enable LLMs to achieve excellent performance comparable to full fine-tuning. However, these fascinating techniques employed by LLMs have not been fully exploited in the ABSA field. Previous works probe LLMs in ABSA by merely using randomly selected input-output pairs as demonstrations in ICL, resulting in an incomplete and superficial evaluation. In this paper, we shed light on a comprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8 ABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation to unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.'' For the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using instruction-based multi-task learning. For the fine-tuning-free paradigm, we propose 3 demonstration selection strategies to stimulate the few-shot abilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a new state-of-the-art performance compared to fine-tuned Small Language Models (SLMs) in the fine-tuning-dependent paradigm. More importantly, in the fine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still showcase impressive potential and even compete with fine-tuned SLMs on some ABSA subtasks.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLM）在自然语言处理领域引起越来越多的关注，并以强大的推理和生成能力彻底改变了许多下游任务。例如，上下文学习（ICL）引入了一种无需微调的范式，允许开箱即用的LLM通过类比学习执行下游任务，而无需任何微调。此外，在存在大量训练数据的依赖微调的范式中，参数高效微调（PEFT）作为经济有效的方法，使LLM能够达到与完全微调相当的优异性能。然而，LLM 采用的这些迷人技术尚未在ABSA领域得到充分利用。以前的研究仅使用随机选择的输入输出对作为ICL中的演示来探究ABSA中的LLM，从而导致评估不完整且肤浅。在本文中，我们对 ABSA 领域的 LLM 进行了全面评估，涉及 13 个数据集、8 个 ABSA 子任务和 6 个 LLM。具体来说，我们设计了一个统一的任务公式来统一“多个范式中多个 ABSA 子任务的多个 LLM”。对于依赖微调的范式，我们使用基于指令的多任务学习有效地微调 LLM。对于无微调范式，我们提出了 3 种演示选择策略来刺激 LLM 的少样本能力。我们大量的实验表明，与微调依赖范式中的微调小语言模型 (SLM) 相比，LLM 实现了新的最佳性能。更重要的是，在 SLM 无效的无微调范式中，具有 ICL 的 LLM 仍然展现出令人印象深刻的潜力，甚至可以在某些 ABSA 子任务上与微调的 SLM 相媲美。</li>
</ul>

<h3>Title: Pay Attention to the Robustness of Chinese Minority Language Models! Syllable-level Textual Adversarial Attack on Tibetan Script</h3>
<ul>
<li><strong>Authors: </strong>Xi Cao, Dolma Dawa, Nuo Qun, Trashi Nyima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02323">https://arxiv.org/abs/2412.02323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02323">https://arxiv.org/pdf/2412.02323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02323]] Pay Attention to the Robustness of Chinese Minority Language Models! Syllable-level Textual Adversarial Attack on Tibetan Script(https://arxiv.org/abs/2412.02323)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The textual adversarial attack refers to an attack method in which the attacker adds imperceptible perturbations to the original texts by elaborate design so that the NLP (natural language processing) model produces false judgments. This method is also used to evaluate the robustness of NLP models. Currently, most of the research in this field focuses on English, and there is also a certain amount of research on Chinese. However, to the best of our knowledge, there is little research targeting Chinese minority languages. Textual adversarial attacks are a new challenge for the information processing of Chinese minority languages. In response to this situation, we propose a Tibetan syllable-level black-box textual adversarial attack called TSAttacker based on syllable cosine distance and scoring mechanism. And then, we conduct TSAttacker on six models generated by fine-tuning two PLMs (pre-trained language models) for three downstream tasks. The experiment results show that TSAttacker is effective and generates high-quality adversarial samples. In addition, the robustness of the involved models still has much room for improvement.</li>
<li><strong>摘要：</strong>文本对抗攻击是指攻击者通过精心设计，对原始文本添加不可察觉的扰动，从而使NLP（自然语言处理）模型产生错误判断的一种攻击方式，该方法也用于评估NLP模型的鲁棒性。目前该领域的研究大多集中在英文上，对中文也有一定研究，但据我们所知，针对中国少数民族语言的研究很少。文本对抗攻击是中国少数民族语言信息处理面临的新挑战。针对此情况，我们提出了一种基于音节余弦距离和评分机制的藏文音节级黑盒文本对抗攻击TSAttacker。然后，我们对由两个预训练语言模型（PLM）微调生成的六个模型针对三个下游任务进行了TSAttacker实验。实验结果表明TSAttacker是有效的，并能生成高质量的对抗样本。此外，所涉及模型的鲁棒性仍有很大提升空间。</li>
</ul>

<h3>Title: Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xi Cao, Nuo Qun, Quzong Gesang, Yulei Zhu, Trashi Nyima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02343">https://arxiv.org/abs/2412.02343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02343">https://arxiv.org/pdf/2412.02343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02343]] Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked Language Model(https://arxiv.org/abs/2412.02343)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In social media, neural network models have been applied to hate speech detection, sentiment analysis, etc., but neural network models are susceptible to adversarial attacks. For instance, in a text classification task, the attacker elaborately introduces perturbations to the original texts that hardly alter the original semantics in order to trick the model into making different predictions. By studying textual adversarial attack methods, the robustness of language models can be evaluated and then improved. Currently, most of the research in this field focuses on English, and there is also a certain amount of research on Chinese. However, there is little research targeting Chinese minority languages. With the rapid development of artificial intelligence technology and the emergence of Chinese minority language models, textual adversarial attacks become a new challenge for the information processing of Chinese minority languages. In response to this situation, we propose a multi-granularity Tibetan textual adversarial attack method based on masked language models called TSTricker. We utilize the masked language models to generate candidate substitution syllables or words, adopt the scoring mechanism to determine the substitution order, and then conduct the attack method on several fine-tuned victim models. The experimental results show that TSTricker reduces the accuracy of the classification models by more than 28.70% and makes the classification models change the predictions of more than 90.60% of the samples, which has an evidently higher attack effect than the baseline method.</li>
<li><strong>摘要：</strong>在社交媒体中，神经网络模型已被应用于仇恨言论检测、情感分析等，但神经网络模型易受到对抗性攻击。例如在文本分类任务中，攻击者精心设计对原文进行扰动，而这些扰动几乎不改变原文的语义，以此诱使模型做出不同的预测。通过研究文本对抗性攻击方法，可以评估并提高语言模型的鲁棒性。目前该领域的研究大多集中在英文，对中文也有一定研究，而针对中国少数民族语言的研究较少。随着人工智能技术的快速发展和中国少数民族语言模型的出现，文本对抗性攻击成为中国少数民族语言信息处理的新挑战。针对此情况，我们提出了一种基于掩码语言模型的多粒度藏文文本对抗性攻击方法TSTricker。我们利用掩码语言模型生成候选替代音节或单词，采用评分机制确定替代顺序，然后对若干经过微调的受害者模型实施攻击方法。实验结果表明，TSTricker 使分类模型的准确率降低了 28.70% 以上，并使分类模型改变了 90.60% 以上样本的预测，攻击效果明显高于基线方法。</li>
</ul>

<h3>Title: TSCheater: Generating High-Quality Tibetan Adversarial Texts via Visual Similarity</h3>
<ul>
<li><strong>Authors: </strong>Xi Cao, Quzong Gesang, Yuan Sun, Nuo Qun, Tashi Nyima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02371">https://arxiv.org/abs/2412.02371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02371">https://arxiv.org/pdf/2412.02371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02371]] TSCheater: Generating High-Quality Tibetan Adversarial Texts via Visual Similarity(https://arxiv.org/abs/2412.02371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models based on deep neural networks are vulnerable to textual adversarial attacks. While rich-resource languages like English are receiving focused attention, Tibetan, a cross-border language, is gradually being studied due to its abundant ancient literature and critical language strategy. Currently, there are several Tibetan adversarial text generation methods, but they do not fully consider the textual features of Tibetan script and overestimate the quality of generated adversarial texts. To address this issue, we propose a novel Tibetan adversarial text generation method called TSCheater, which considers the characteristic of Tibetan encoding and the feature that visually similar syllables have similar semantics. This method can also be transferred to other abugidas, such as Devanagari script. We utilize a self-constructed Tibetan syllable visual similarity database called TSVSDB to generate substitution candidates and adopt a greedy algorithm-based scoring mechanism to determine substitution order. After that, we conduct the method on eight victim language models. Experimentally, TSCheater outperforms existing methods in attack effectiveness, perturbation magnitude, semantic similarity, visual similarity, and human acceptance. Finally, we construct the first Tibetan adversarial robustness evaluation benchmark called AdvTS, which is generated by existing methods and proofread by humans.</li>
<li><strong>摘要：</strong>基于深度神经网络的语言模型易受文本对抗攻击。在英语等资源丰富的语言受到关注的同时，跨境语言藏语由于其丰富的古文献和批判性语言策略也逐渐受到研究。目前，藏语对抗文本生成方法已有多种，但均未充分考虑藏文的文本特征，高估了生成的对抗文本的质量。针对这一问题，我们提出了一种新的藏文对抗文本生成方法TSCheater，该方法考虑了藏文编码的特点以及视觉相似的音节具有相似的语义特点。该方法也可以迁移到其他元音附标文字，如天城文。我们利用自建的藏文音节视觉相似性数据库TSVSDB生成替换候选，并采用基于贪婪算法的评分机制确定替换顺序。之后，我们在八个受害语言模型上进行了该方法的测试。实验表明，TSCheater 在攻击有效性、扰动幅度、语义相似性、视觉相似性和人类接受度方面均优于现有方法。最后，我们构建了第一个藏文对抗鲁棒性评估基准 AdvTS，该基准由现有方法生成并由人类校对。</li>
</ul>

<h3>Title: GerPS-Compare: Comparing NER methods for legal norm analysis</h3>
<ul>
<li><strong>Authors: </strong>Sarah T. Bachinger, Christoph Unger, Robin Erd, Leila Feddoul, Clara Lachenmaier, Sina Zarrieß, Birgitta König-Ries</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02427">https://arxiv.org/abs/2412.02427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02427">https://arxiv.org/pdf/2412.02427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02427]] GerPS-Compare: Comparing NER methods for legal norm analysis(https://arxiv.org/abs/2412.02427)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We apply NER to a particular sub-genre of legal texts in German: the genre of legal norms regulating administrative processes in public service administration. The analysis of such texts involves identifying stretches of text that instantiate one of ten classes identified by public service administration professionals. We investigate and compare three methods for performing Named Entity Recognition (NER) to detect these classes: a Rule-based system, deep discriminative models, and a deep generative model. Our results show that Deep Discriminative models outperform both the Rule-based system as well as the Deep Generative model, the latter two roughly performing equally well, outperforming each other in different classes. The main cause for this somewhat surprising result is arguably the fact that the classes used in the analysis are semantically and syntactically heterogeneous, in contrast to the classes used in more standard NER tasks. Deep Discriminative models appear to be better equipped for dealing with this heterogenerity than both generic LLMs and human linguists designing rule-based NER systems.</li>
<li><strong>摘要：</strong>我们将 NER 应用于德语法律文本的一个特定子类别：规范公共服务管理中行政程序的法律规范类别。此类文本的分析涉及识别文本片段，这些片段实例化了公共服务管理专业人员确定的十个类别之一。我们研究并比较了三种执行命名实体识别 (NER) 来检测这些类别的方法：基于规则的系统、深度判别模型和深度生成模型。我们的结果表明，深度判别模型的表现优于基于规则的系统和深度生成模型，后两者的表现大致相同，在不同类别中表现优于彼此。造成这种有些令人惊讶的结果的主要原因可以说是分析中使用的类别在语义和句法上是异构的，与更标准的 NER 任务中使用的类别形成鲜明对比。与通用 LLM 和设计基于规则的 NER 系统的人类语言学家相比，深度判别模型似乎更适合处理这种异构性。</li>
</ul>

<h3>Title: Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining</h3>
<ul>
<li><strong>Authors: </strong>Zongru Wu, Pengzhou Cheng, Lingyong Fang, Zhuosheng Zhang, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02454">https://arxiv.org/abs/2412.02454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02454">https://arxiv.org/pdf/2412.02454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02454]] Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining(https://arxiv.org/abs/2412.02454)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Backdoor attacks remain significant security threats to generative large language models (LLMs). Since generative LLMs output sequences of high-dimensional token logits instead of low-dimensional classification logits, most existing backdoor defense methods designed for discriminative models like BERT are ineffective for generative LLMs. Inspired by the observed differences in learning behavior between backdoor and clean mapping in the frequency space, we transform gradients of each training sample, directly influencing parameter updates, into the frequency space. Our findings reveal a distinct separation between the gradients of backdoor and clean samples in the frequency space. Based on this phenomenon, we propose Gradient Clustering in the Frequency Space for Backdoor Sample Filtering (GraCeFul), which leverages sample-wise gradients in the frequency space to effectively identify backdoor samples without requiring retraining LLMs. Experimental results show that GraCeFul outperforms baselines significantly. Notably, GraCeFul exhibits remarkable computational efficiency, achieving nearly 100% recall and F1 scores in identifying backdoor samples, reducing the average success rate of various backdoor attacks to 0% with negligible drops in clean accuracy across multiple free-style question answering datasets. Additionally, GraCeFul generalizes to Llama-2 and Vicuna. The codes are publicly available at this https URL.</li>
<li><strong>摘要：</strong>后门攻击仍然是生成式大型语言模型 (LLM) 面临的重大安全威胁。由于生成式 LLM 输出的是高维标记逻辑序列而不是低维分类逻辑序列，因此大多数现有的为 BERT 等判别模型设计的后门防御方法对生成式 LLM 都无效。受观察到的后门和干净映射在频率空间中的学习行为差异的启发，我们将直接影响参数更新的每个训练样本的梯度转换到频率空间中。我们的研究结果揭示了后门和干净样本在频率空间中的梯度有明显的分离。基于这一现象，我们提出了用于后门样本过滤的频率空间梯度聚类 (GraCeFul)，它利用频率空间中的样本梯度来有效识别后门样本，而无需重新训练 LLM。实验结果表明，GraCeFul 的表现明显优于基线。值得注意的是，GraCeFul 表现出了卓越的计算效率，在识别后门样本时实现了近 100% 的召回率和 F1 分数，将各种后门攻击的平均成功率降低到 0%，而在多个自由式问答数据集中，清洁准确率的下降可以忽略不计。此外，GraCeFul 还推广到 Llama-2 和 Vicuna。代码可在此 https URL 上公开获取。</li>
</ul>

<h3>Title: Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Q. Shormani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02466">https://arxiv.org/abs/2412.02466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02466">https://arxiv.org/pdf/2412.02466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02466]] Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths(https://arxiv.org/abs/2412.02466)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>This study sets out to answer one major question: Can ChatGPT capture swearing nuances? It presents an empirical study on the ability of ChatGPT to translate Arabic oath expressions into English. 30 Arabic oath expressions were collected from the literature. These 30 oaths were first translated via ChatGPT and then analyzed and compared to the human translation in terms of types of gaps left unfulfilled by ChatGPT. Specifically, the gaps involved are: religious gap, cultural gap, both religious and cultural gaps, no gap, using non-oath particles, redundancy and noncapturing of Arabic script diacritics. It concludes that ChatGPT translation of oaths is still much unsatisfactory, unveiling the need of further developments of ChatGPT, and the inclusion of Arabic data on which ChatGPT should be trained including oath expressions, oath nuances, rituals, and practices.</li>
<li><strong>摘要：</strong>本研究旨在回答一个主要问题：ChatGPT 能否捕捉到咒骂的细微差别？它对 ChatGPT 将阿拉伯语誓言表达翻译成英语的能力进行了实证研究。从文献中收集了 30 种阿拉伯语誓言表达。这 30 种誓言首先通过 ChatGPT 翻译，然后根据 ChatGPT 未填补的空白类型与人工翻译进行比较。具体而言，所涉及的空白包括：宗教空白、文化空白、宗教和文化空白、无空白、使用非誓言助词、冗余和未捕捉阿拉伯语字母变音符号。研究得出的结论是，ChatGPT 对誓言的翻译仍然不尽如人意，这表明需要进一步开发 ChatGPT，并纳入阿拉伯语数据以训练 ChatGPT，包括誓言表达、誓言细微差别、仪式和习俗。</li>
</ul>

<h3>Title: Patent-CR: A Dataset for Patent Claim Revision</h3>
<ul>
<li><strong>Authors: </strong>Lekang Jiang, Pascal A Scherz, Stephan Goetz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02549">https://arxiv.org/abs/2412.02549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02549">https://arxiv.org/pdf/2412.02549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02549]] Patent-CR: A Dataset for Patent Claim Revision(https://arxiv.org/abs/2412.02549)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models. Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions. In addition, domain-specific models and the method of fine-tuning show promising results. Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard. Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment. This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision.</li>
<li><strong>摘要：</strong>本文介绍了 Patent-CR，这是第一个为英文专利权利要求修改任务创建的数据集。它既包括被专利审查员拒绝的初始专利申请，也包括最终授权的版本。与主要侧重于提高句子质量（例如语法校正和连贯性改进）的普通文本修改任务不同，专利权利要求修改旨在确保权利要求符合严格的法律标准。这些标准超越了新颖性和创造性，包括范围的清晰度、技术准确性、语言精确性和法律稳健性。我们通过专业的人工评估来评估各种大型语言模型 (LLM)，包括具有不同大小和架构的通用 LLM、文本修订模型和领域特定模型。我们的结果表明，LLM 通常会带来偏离目标修订的无效编辑。此外，领域特定模型和微调方法显示出令人鼓舞的结果。值得注意的是，GPT-4 的表现优于其他经过测试的 LLM，但仍需要进一步修改才能达到审查标准。此外，我们展示了自动评估结果与人工评估结果之间的不一致，表明基于 GPT-4 的自动评估与人工判断的相关性最高。该数据集以及我们初步的实证研究为进一步探索专利权利要求修改提供了宝贵的见解。</li>
</ul>

<h3>Title: Semantic Tokens in Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Joel Suro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02563">https://arxiv.org/abs/2412.02563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02563">https://arxiv.org/pdf/2412.02563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02563]] Semantic Tokens in Retrieval Augmented Generation(https://arxiv.org/abs/2412.02563)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 架构最近因其在自然语言处理任务中改善事实依据和连贯性的能力而备受关注。然而，随着 RAG 系统访问的数据量增加，其生成准确答案的可靠性会降低。即使数据集较小，这些系统有时也无法解决简单查询。这个问题源于它们对最先进的大型语言模型 (LLM) 的依赖，这可能会给系统的输出带来不确定性。在这项工作中，我提出了一种新颖的比较 RAG 系统，该系统引入了一个评估器模块来弥合概率 RAG 系统和确定性可验证响应之间的差距。评估器将外部建议与检索到的文档块进行比较，从而增加了一个决策层，以提高系统的可靠性。这种方法确保检索到的块在语义上与确定性见解相关且在逻辑上一致，从而提高 RAG 系统的准确性和整体效率。该框架为在需要高精度和可验证性的领域中实现更可靠、更可扩展的问答应用铺平了道路。</li>
</ul>

<h3>Title: CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs</h3>
<ul>
<li><strong>Authors: </strong>Abhas Kumar, Kapil Pathak, Rajesh Kavuru, Prabhakar Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02602">https://arxiv.org/abs/2412.02602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02602">https://arxiv.org/pdf/2412.02602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02602]] CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs(https://arxiv.org/abs/2412.02602)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper analyzes the performance of Small Language Models (SLMs) and Vision Language Models (VLMs) and evaluates the trade-off between model performance and carbon emissions across 4 essential tasks: Image Captioning, Visual Question Answering (VQA), Dialogue Summarization and Text-to-SQL conversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture family are chosen and variants based on model size in terms of the number of parameters, quantization level and fine-tuning parameters are evaluated. The model variant's performance and carbon emissions are calculated. To quantify the trade-off between model performance and carbon emissions, we introduce a novel metric called CEGI (Carbon Efficient Gain Index). This metric represents the carbon emission per unit percentage gain per million trainable parameters . This metric provides a normalized measure to compare model's efficiency in terms of performance improvement relative to their environmental cost. The experiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve performance levels comparable to Large Language Models (LLMs) while producing significantly less carbon emissions. Our findings suggest that the marginal gains in accuracy from larger models do not justify the substantial increase in carbon emissions. Leveraging lower-bit quantization levels, the proposed metric further enhances energy efficiency without compromising performance. This study highlights balancing high performance and environmental sustainability. It offers a valuable metric for selecting models suitable for environmentally-friendly AI development.</li>
<li><strong>摘要：</strong>本文分析了小型语言模型 (SLM) 和视觉语言模型 (VLM) 的性能，并评估了四项基本任务中模型性能和碳排放之间的权衡：图像字幕、视觉问答 (VQA)、对话摘要和文本到 SQL 转换。选择了属于 Qwen 和 LLaMA 架构系列的各种 SLM 和 VLM，并评估了基于模型大小的变体（包括参数数量、量化级别和微调参数）。计算了模型变体的性能和碳排放量。为了量化模型性能和碳排放之间的权衡，我们引入了一个称为 CEGI（碳效率增益指数）的新指标。该指标表示每百万可训练参数每单位百分比增益的碳排放量。该指标提供了一个标准化度量，以比较模型在性能改进相对于环境成本方面的效率。实验结果表明，微调 SLM 和 VLM 可以实现与大型语言模型 (LLM) 相当的性能水平，同时产生更少的碳排放。我们的研究结果表明，大型模型的准确度边际提升并不能证明碳排放的大幅增加是合理的。利用较低位量化级别，所提出的指标进一步提高了能源效率，同时不影响性能。这项研究强调了在高性能和环境可持续性之间取得平衡。它为选择适合环保型 AI 开发的模型提供了有价值的指标。</li>
</ul>

<h3>Title: Interpretable Company Similarity with Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Marco Molinari, Vladimir Tregubiak, Victor Shao, Abhimanyu Pandey, Mateusz Mikolajczak, Sebastião Kuznetsov Ryder Torres Pereira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02605">https://arxiv.org/abs/2412.02605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02605">https://arxiv.org/pdf/2412.02605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02605]] Interpretable Company Similarity with Sparse Autoencoders(https://arxiv.org/abs/2412.02605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Determining company similarity is a vital task in finance, underpinning hedging, risk management, portfolio diversification, and more. Practitioners often rely on sector and industry classifications to gauge similarity, such as SIC-codes and GICS-codes, the former being used by the U.S. Securities and Exchange Commission (SEC), and the latter widely used by the investment community. Clustering embeddings of company descriptions has been proposed as a potential technique for determining company similarity, but the lack of interpretability in token embeddings poses a significant barrier to adoption in high-stakes contexts. Sparse Autoencoders have shown promise in enhancing the interpretability of Large Language Models by decomposing LLM activations into interpretable features. In this paper, we explore the use of SAE features in measuring company similarity and benchmark them against (1) SIC codes and (2) Major Group codes. We conclude that SAE features can reproduce and even surpass sector classifications in quantifying fundamental characteristics of companies, evaluated by the correlation of monthly returns, a proxy for similarity, and PnL from cointegration.</li>
<li><strong>摘要：</strong>确定公司相似性是金融领域的一项重要任务，是套期保值、风险管理、投资组合多元化等的基础。从业者通常依靠行业分类来衡量相似性，例如 SIC 代码和 GICS 代码，前者由美国证券交易委员会 (SEC) 使用，后者被投资界广泛使用。公司描述的聚类嵌入已被提议作为一种确定公司相似性的潜在技术，但 token 嵌入缺乏可解释性，这对在高风险环境中采用构成了重大障碍。稀疏自动编码器已显示出通过将 LLM 激活分解为可解释特征来增强大型语言模型的可解释性的潜力。在本文中，我们探讨了 SAE 特征在衡量公司相似性方面的应用，并根据 (1) SIC 代码和 (2) 主要组代码对其进行了基准测试。我们得出的结论是，SAE 特征在量化公司基本特征方面可以重现甚至超越行业分类，通过月度回报的相关性、相似性的代理和协整的 PnL 进行评估。</li>
</ul>

<h3>Title: GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot</h3>
<ul>
<li><strong>Authors: </strong>Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02612">https://arxiv.org/abs/2412.02612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02612">https://arxiv.org/pdf/2412.02612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02612]] GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot(https://arxiv.org/abs/2412.02612)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through this https URL and this https URL.</li>
<li><strong>摘要：</strong>我们推出了 GLM-4-Voice，这是一款智能且像人类一样的端到端语音聊天机器人。它支持中文和英文，可进行实时语音对话，并根据用户指令改变声音的细微差别，例如情绪、语调、语速和方言。GLM-4-Voice 使用超低比特率（175bps）、单码本语音标记器，帧率为 12.5Hz，该标记器源自自动语音识别 (ASR) 模型，该模型通过将矢量量化瓶颈合并到编码器中而得出。为了有效地将知识从文本转移到语音模态，我们使用文本到标记模型从现有文本预训练语料库中合成语音文本交错数据。我们继续使用预训练文本语言模型 GLM-4-9B 进行预训练，结合使用无监督语音数据、交错语音文本数据和监督语音文本数据，扩展到 1 万亿个标记，在语音语言建模和口头问答方面均实现了一流的性能。然后，我们使用高质量的对话语音数据对预训练模型进行微调，在对话能力和语音质量方面均实现了优于现有基线的性能。可以通过此 https URL 和此 https URL 访问开放模型。</li>
</ul>

<h3>Title: Time-Reversal Provides Unsupervised Feedback to LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yerram Varun, Rahul Madhavan, Sravanti Addepalli, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02626">https://arxiv.org/abs/2412.02626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02626">https://arxiv.org/pdf/2412.02626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02626]] Time-Reversal Provides Unsupervised Feedback to LLMs(https://arxiv.org/abs/2412.02626)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常经过训练以预测时间的正向方向。然而，最近的研究表明，促使这些模型回顾和批评自己的世代可以产生有用的反馈。受此启发，我们探索了这样一个问题：LLM 是否可以被赋予向后思考（预测和评分）的能力，以提供补充前向 LLM 的无监督反馈。为此，我们引入了时间反转语言模型 (TRLM)，它可以在以响应为条件时对查询进行评分和生成查询，有效地在时间的反方向上发挥作用。此外，为了有效地推断查询方向的响应，我们从头开始以反向标记顺序预训练和微调语言模型 (TRLM-Ba)。我们通过经验（并在风格化的设置中从理论上）证明，时间反转模型在用于对给定响应的查询进行评分以重新排名多个前向代时确实可以补充前向模型预测。我们在广泛使用的 AlpacaEval 排行榜上获得了高达 5\% 的改进，超过了使用自对数困惑度分数的最佳 N 重排序的胜任基线。我们进一步表明，TRLM 评分优于给定查询的响应的传统前向评分，从而在引用生成和段落检索等应用中取得了显着的进步。接下来，我们利用 TRLM 的生成能力来增强或向 LLM 的输入安全过滤器提供无监督反馈，结果显示假阴性率大幅降低，而对流行的 JailbreakBench 排行榜上发布的几次攻击的假阳性率影响微乎其微。</li>
</ul>

<h3>Title: QA-TOOLBOX: Conversational Question-Answering for process task guidance in manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Ramesh Manuvinakurike, Elizabeth Watkins, Celal Savur, Anthony Rhodes, Sovan Biswas, Gesem Gudino Mejia, Richard Beckwith, Saurav Sahay, Giuseppe Raffa, Lama Nachman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02638">https://arxiv.org/abs/2412.02638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02638">https://arxiv.org/pdf/2412.02638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02638]] QA-TOOLBOX: Conversational Question-Answering for process task guidance in manufacturing(https://arxiv.org/abs/2412.02638)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In this work we explore utilizing LLMs for data augmentation for manufacturing task guidance system. The dataset consists of representative samples of interactions with technicians working in an advanced manufacturing setting. The purpose of this work to explore the task, data augmentation for the supported tasks and evaluating the performance of the existing LLMs. We observe that that task is complex requiring understanding from procedure specification documents, actions and objects sequenced temporally. The dataset consists of 200,000+ question/answer pairs that refer to the spec document and are grounded in narrations and/or video demonstrations. We compared the performance of several popular open-sourced LLMs by developing a baseline using each LLM and then compared the responses in a reference-free setting using LLM-as-a-judge and compared the ratings with crowd-workers whilst validating the ratings with experts.</li>
<li><strong>摘要：</strong>在这项工作中，我们探索利用 LLM 为制造任务指导系统进行数据增强。数据集包括在先进制造环境中工作的技术人员的互动代表性样本。这项工作的目的是探索任务、支持任务的数据增强以及评估现有 LLM 的性能。我们观察到该任务很复杂，需要理解按时间顺序排列的程序规范文档、操作和对象。数据集包含 200,000 多个问题/答案对，它们引用了规范文档并以叙述和/或视频演示为基础。我们通过使用每个 LLM 开发基线来比较几种流行的开源 LLM 的性能，然后使用 LLM-as-a-judge 在无参考设置中比较响应，并与众包工作者进行比较，同时与专家验证评级。</li>
</ul>

<h3>Title: Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, Udaya Ghai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02674">https://arxiv.org/abs/2412.02674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02674">https://arxiv.org/pdf/2412.02674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02674]] Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models(https://arxiv.org/abs/2412.02674)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the generation-verification gap. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries.</li>
<li><strong>摘要：</strong>自我改进是大型语言模型 (LLM) 预训练、后训练和测试时推理的一种机制。我们探索了一个框架，其中模型验证自己的输出，根据此验证过滤或重新加权数据，并提取过滤后的数据。尽管取得了一些经验上的成功，但仍然缺乏基本的理解。在这项工作中，我们启动了一项关于 LLM 自我改进的全面、模块化和受控的研究。我们为自我改进提供了一个数学公式，它主要由我们形式化为代际验证差距的量所控制。通过对各种模型系列和任务的实验，我们发现了一种自我改进的扩展现象——代际验证差距的变体与模型预训练失败单调扩展。我们还研究了何时可以进行自我改进、迭代自我改进程序以及提高其性能的方法。我们的研究结果不仅增进了对法学硕士自我提升的理解并具有实际意义，而且为未来研究其能力和界限开辟了众多途径。</li>
</ul>

<h3>Title: T-REG: Preference Optimization with Token-Level Reward Regularization</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Zhou, Shujian Zhang, Lingxiao Zhao, Tao Meng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.02685">https://arxiv.org/abs/2412.02685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.02685">https://arxiv.org/pdf/2412.02685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.02685]] T-REG: Preference Optimization with Token-Level Reward Regularization(https://arxiv.org/abs/2412.02685)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has been crucial in aligning large language models (LLMs) with human values. Traditionally, RLHF involves generating responses to a query and using a reward model to assign a reward to the entire response. However, this approach faces challenges due to its reliance on a single, sparse reward, which makes it challenging for the model to identify which parts of the sequence contribute most significantly to the final reward. Recent methods have attempted to address this limitation by introducing token-level rewards. However, these methods often rely on either a trained credit assignment model or AI annotators, raising concerns about the quality and reliability of the rewards. In this paper, we propose token-level reward regularization (T-REG), a novel approach that leverages both sequence-level and token-level rewards for preference optimization. Harnessing the self-refinement capabilities of LLMs, our method uses contrastive prompting to enable LLMs to self-generate token-level rewards. These self-generated rewards then act as reward regularization, guiding the model to more effectively distribute sequence-level rewards across tokens. This facilitates better token-level credit assignment and enhances alignment performance. Experiments on the instruction following benchmarks, including Alpaca Eval 2 and Arena-Hard, show that our method consistently outperforms baseline methods by up to 3.8% and 4.4%, respectively. We will release the code and models at this https URL.</li>
<li><strong>摘要：</strong>从人类反馈中进行强化学习 (RLHF) 对于将大型语言模型 (LLM) 与人类价值观相结合至关重要。传统上，RLHF 涉及生成对查询的响应并使用奖励模型为整个响应分配奖励。然而，这种方法面临着挑战，因为它依赖于单一、稀疏的奖励，这使得模型很难确定序列的哪些部分对最终奖励的贡献最大。最近的方法试图通过引入 token 级奖励来解决这一限制。然而，这些方法通常依赖于经过训练的信用分配模型或 AI 注释器，这引发了人们对奖励质量和可靠性的担忧。在本文中，我们提出了 token 级奖励正则化 (T-REG)，这是一种利用序列级和 token 级奖励进行偏好优化的新方法。利用 LLM 的自我改进能力，我们的方法使用对比提示使 LLM 能够自我生成 token 级奖励。这些自生成的奖励随后充当奖励正则化，引导模型更有效地在 token 之间分配序列级奖励。这有助于更好地分配 token 级信用并提高对齐性能。在包括 Alpaca Eval 2 和 Arena-Hard 在内的指令基准上进行的实验表明，我们的方法始终比基线方法分别高出 3.8% 和 4.4%。我们将在此 https URL 上发布代码和模型。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
