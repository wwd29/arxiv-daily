<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>language model</h2>
<h3>Title: Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06681">http://arxiv.org/abs/2312.06681</a></li>
<li>Code URL: https://github.com/wusche1/caa_hallucination</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06681]] Steering Llama 2 via Contrastive Activation Addition(http://arxiv.org/abs/2312.06681)</code></li>
<li>Summary: <p>We introduce Contrastive Activation Addition (CAA), an innovative method for
steering language models by modifying activations during their forward passes.
CAA computes ``steering vectors'' by averaging the difference in residual
stream activations between pairs of positive and negative examples of a
particular behavior such as factual versus hallucinatory responses. During
inference, these steering vectors are added at all token positions after the
user's prompt with either a positive or negative coefficient, allowing precise
control over the degree of the targeted behavior. We evaluate CAA's
effectiveness on Llama 2 Chat using both multiple-choice behavioral question
datasets and open-ended generation tasks. We demonstrate that CAA significantly
alters model behavior, outperforms traditional methods like finetuning and
few-shot prompting, and minimally reduces capabilities. Moreover, by employing
various activation space interpretation methods, we gain deeper insights into
CAA's mechanisms. CAA both accurately steers model outputs and also sheds light
on how high-level concepts are represented in Large Language Models (LLMs).
</p></li>
</ul>

<h3>Title: Enhanced E-Commerce Attribute Extraction: Innovating with Decorative Relation Correction and LLAMA 2.0-Based Annotation. (arXiv:2312.06684v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06684">http://arxiv.org/abs/2312.06684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06684]] Enhanced E-Commerce Attribute Extraction: Innovating with Decorative Relation Correction and LLAMA 2(http://arxiv.org/abs/2312.06684)</code></li>
<li>Summary: <p>The rapid proliferation of e-commerce platforms accentuates the need for
advanced search and retrieval systems to foster a superior user experience.
Central to this endeavor is the precise extraction of product attributes from
customer queries, enabling refined search, comparison, and other crucial
e-commerce functionalities. Unlike traditional Named Entity Recognition (NER)
tasks, e-commerce queries present a unique challenge owing to the intrinsic
decorative relationship between product types and attributes. In this study, we
propose a pioneering framework that integrates BERT for classification, a
Conditional Random Fields (CRFs) layer for attribute value extraction, and
Large Language Models (LLMs) for data annotation, significantly advancing
attribute recognition from customer inquiries. Our approach capitalizes on the
robust representation learning of BERT, synergized with the sequence decoding
prowess of CRFs, to adeptly identify and extract attribute values. We introduce
a novel decorative relation correction mechanism to further refine the
extraction process based on the nuanced relationships between product types and
attributes inherent in e-commerce data. Employing LLMs, we annotate additional
data to expand the model's grasp and coverage of diverse attributes. Our
methodology is rigorously validated on various datasets, including Walmart,
BestBuy's e-commerce NER dataset, and the CoNLL dataset, demonstrating
substantial improvements in attribute recognition performance. Particularly,
the model showcased promising results during a two-month deployment in
Walmart's Sponsor Product Search, underscoring its practical utility and
effectiveness.
</p></li>
</ul>

<h3>Title: Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models. (arXiv:2312.06685v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06685">http://arxiv.org/abs/2312.06685</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06685]] Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models(http://arxiv.org/abs/2312.06685)</code></li>
<li>Summary: <p>While Multi-modal Language Models (MLMs) demonstrate impressive multimodal
ability, they still struggle on providing factual and precise responses for
tasks like visual question answering (VQA). In this paper, we address this
challenge from the perspective of contextual information. We propose Causal
Context Generation, Causal-CoG, which is a prompting strategy that engages
contextual information to enhance precise VQA during inference. Specifically,
we prompt MLMs to generate contexts, i.e, text description of an image, and
engage the generated contexts for question answering. Moreover, we investigate
the advantage of contexts on VQA from a causality perspective, introducing
causality filtering to select samples for which contextual information is
helpful. To show the effectiveness of Causal-CoG, we run extensive experiments
on 10 multimodal benchmarks and show consistent improvements, e.g., +6.30% on
POPE, +13.69% on Vizwiz and +6.43% on VQAv2 compared to direct decoding,
surpassing existing methods. We hope Casual-CoG inspires explorations of
context knowledge in multimodal models, and serves as a plug-and-play strategy
for MLM decoding.
</p></li>
</ul>

<h3>Title: Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06717">http://arxiv.org/abs/2312.06717</a></li>
<li>Code URL: https://github.com/safr-ml-lab/survey-llm</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06717]] Privacy Issues in Large Language Models: A Survey(http://arxiv.org/abs/2312.06717)</code></li>
<li>Summary: <p>This is the first survey of the active area of AI research that focuses on
privacy issues in Large Language Models (LLMs). Specifically, we focus on work
that red-teams models to highlight privacy risks, attempts to build privacy
into the training or inference process, enables efficient data deletion from
trained models to comply with existing privacy regulations, and tries to
mitigate copyright issues. Our focus is on summarizing technical research that
develops algorithms, proves theorems, and runs empirical evaluations. While
there is an extensive body of legal and policy work addressing these challenges
from a different angle, that is not the focus of our survey. Nevertheless,
these works, along with recent legal developments do inform how these technical
problems are formalized, and so we discuss them briefly in Section 1. While we
have made our best effort to include all the relevant work, due to the fast
moving nature of this research we may have missed some recent work. If we have
missed some of your work please contact us, as we will attempt to keep this
survey relatively up to date. We are maintaining a repository with the list of
papers covered in this survey and any relevant code that was publicly available
at https://github.com/safr-ml-lab/survey-llm.
</p></li>
</ul>

<h3>Title: Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety. (arXiv:2312.06798v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06798">http://arxiv.org/abs/2312.06798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06798]] Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety(http://arxiv.org/abs/2312.06798)</code></li>
<li>Summary: <p>Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.
</p></li>
</ul>

<h3>Title: LLF-Bench: Benchmark for Interactive Learning from Language Feedback. (arXiv:2312.06853v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06853">http://arxiv.org/abs/2312.06853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06853]] LLF-Bench: Benchmark for Interactive Learning from Language Feedback(http://arxiv.org/abs/2312.06853)</code></li>
<li>Summary: <p>We introduce a new benchmark, LLF-Bench (Learning from Language Feedback
Benchmark; pronounced as "elf-bench"), to evaluate the ability of AI agents to
interactively learn from natural language feedback and instructions. Learning
from language feedback (LLF) is essential for people, largely because the rich
information this feedback provides can help a learner avoid much of trial and
error and thereby speed up the learning process. Large Language Models (LLMs)
have recently enabled AI agents to comprehend natural language -- and hence AI
agents can potentially benefit from language feedback during learning like
humans do. But existing interactive benchmarks do not assess this crucial
capability: they either use numeric reward feedback or require no learning at
all (only planning or information retrieval). LLF-Bench is designed to fill
this omission. LLF-Bench is a diverse collection of sequential decision-making
tasks that includes user recommendation, poem writing, navigation, and robot
control. The objective of an agent is to interactively solve these tasks based
on their natural-language instructions and the feedback received after taking
actions. Crucially, to ensure that the agent actually "learns" from the
feedback, LLF-Bench implements several randomization techniques (such as
paraphrasing and environment randomization) to ensure that the task isn't
familiar to the agent and that the agent is robust to various verbalizations.
In addition, LLF-Bench provides a unified OpenAI Gym interface for all its
tasks and allows the users to easily configure the information the feedback
conveys (among suggestion, explanation, and instantaneous performance) to study
how agents respond to different types of feedback. Together, these features
make LLF-Bench a unique research platform for developing and testing LLF
agents.
</p></li>
</ul>

<h3>Title: SM70: A Large Language Model for Medical Devices. (arXiv:2312.06974v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06974">http://arxiv.org/abs/2312.06974</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06974]] SM70: A Large Language Model for Medical Devices(http://arxiv.org/abs/2312.06974)</code></li>
<li>Summary: <p>We are introducing SM70, a 70 billion-parameter Large Language Model that is
specifically designed for SpassMed's medical devices under the brand name
'JEE1' (pronounced as G1 and means 'Life'). This large language model provides
more accurate and safe responses to medical-domain questions. To fine-tune
SM70, we used around 800K data entries from the publicly available dataset
MedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,
and we employed the QLoRA technique for fine-tuning. The evaluation is
conducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE
- each representing a unique aspect of medical knowledge and reasoning. The
performance of SM70 is contrasted with other notable LLMs, including Llama2
70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a
comparative understanding of its capabilities within the medical domain. Our
results indicate that SM70 outperforms several established models in these
datasets, showcasing its proficiency in handling a range of medical queries,
from fact-based questions derived from PubMed abstracts to complex clinical
decision-making scenarios. The robust performance of SM70, particularly in the
USMLE and PUBMEDQA datasets, suggests its potential as an effective tool in
clinical decision support and medical information retrieval. Despite its
promising results, the paper also acknowledges the areas where SM70 lags behind
the most advanced model, GPT 4, thereby highlighting the need for further
development, especially in tasks demanding extensive medical knowledge and
intricate reasoning.
</p></li>
</ul>

<h3>Title: Alignment for Honesty. (arXiv:2312.07000v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07000">http://arxiv.org/abs/2312.07000</a></li>
<li>Code URL: https://github.com/gair-nlp/alignment-for-honesty</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07000]] Alignment for Honesty(http://arxiv.org/abs/2312.07000)</code></li>
<li>Summary: <p>Recent research has made significant strides in applying alignment techniques
to enhance the helpfulness and harmlessness of large language models (LLMs) in
accordance with human intentions. In this paper, we argue for the importance of
alignment for honesty, ensuring that LLMs proactively refuse to answer
questions when they lack knowledge, while still not being overly conservative.
However, a pivotal aspect of alignment for honesty involves discerning the
limits of an LLM's knowledge, which is far from straightforward. This challenge
demands comprehensive solutions in terms of metric development, benchmark
creation, and training methodologies. In this paper, we address these
challenges by first establishing a precise problem definition and defining
``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone
for developing metrics that effectively measure an LLM's honesty by quantifying
its progress post-alignment. Furthermore, we introduce a flexible training
framework which is further instantiated by several efficient fine-tuning
techniques that emphasize honesty without sacrificing performance on other
tasks. Our extensive experiments reveal that these aligned models show a marked
increase in honesty, as indicated by our proposed metrics. We open-source a
wealth of resources to facilitate future research at
https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned
models, training and evaluation datasets for honesty alignment, concept
glossary, as well as all relevant source code.
</p></li>
</ul>

<h3>Title: Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models. (arXiv:2312.07028v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07028">http://arxiv.org/abs/2312.07028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07028]] Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models(http://arxiv.org/abs/2312.07028)</code></li>
<li>Summary: <p>We tackle the challenging issue of aggressive fine-tuning encountered during
the process of transfer learning of pre-trained language models (PLMs) with
limited labeled downstream data. This problem primarily results in a decline in
performance on the subsequent task. Inspired by the adaptive boosting method in
traditional machine learning, we present an effective dynamic corrective
self-distillation (DCS) approach to improve the fine-tuning of the PLMs. Our
technique involves performing a self-distillation mechanism where, at each
iteration, the student model actively adapts and corrects itself by dynamically
adjusting the weights assigned to individual data points. This iterative
self-correcting process significantly enhances the overall fine-tuning
capability of PLMs, leading to improved performance and robustness. We
conducted comprehensive evaluations using the GLUE benchmark demonstrating the
efficacy of our method in enhancing the fine-tuning process for various PLMs
across diverse downstream tasks.
</p></li>
</ul>

<h3>Title: HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts. (arXiv:2312.07035v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07035">http://arxiv.org/abs/2312.07035</a></li>
<li>Code URL: https://github.com/giangdip2410/hyperrouter</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07035]] HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts(http://arxiv.org/abs/2312.07035)</code></li>
<li>Summary: <p>By routing input tokens to only a few split experts, Sparse
Mixture-of-Experts has enabled efficient training of large language models.
Recent findings suggest that fixing the routers can achieve competitive
performance by alleviating the collapsing problem, where all experts eventually
learn similar representations. However, this strategy has two key limitations:
(i) the policy derived from random routers might be sub-optimal, and (ii) it
requires extensive resources during training and evaluation, leading to limited
efficiency gains. This work introduces \HyperRout, which dynamically generates
the router's parameters through a fixed hypernetwork and trainable embeddings
to achieve a balance between training the routers and freezing them to learn an
improved routing policy. Extensive experiments across a wide range of tasks
demonstrate the superior performance and efficiency gains of \HyperRouter
compared to existing routing methods. Our implementation is publicly available
at {\url{{https://github.com/giangdip2410/HyperRouter}}}.
</p></li>
</ul>

<h3>Title: Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications. (arXiv:2312.07069v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07069">http://arxiv.org/abs/2312.07069</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07069]] Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications(http://arxiv.org/abs/2312.07069)</code></li>
<li>Summary: <p>In this paper, we explore the challenges inherent to Large Language Models
(LLMs) like GPT-4, particularly their propensity for hallucinations, logic
mistakes, and incorrect conclusions when tasked with answering complex
questions. The capacity of LLMs to present erroneous answers in a coherent and
semantically rigorous manner further complicates the detection of factual
inaccuracies. This issue is especially pronounced in fields that require
specialized expertise. Our work delves into these challenges, aiming to enhance
the understanding and mitigation of such errors, thereby contributing to the
improvement of LLM accuracy and reliability in scientific and other specialized
domains. Our findings reveal a non-linear relationship between the context's
relevancy and the answers' measured quality. In addition, we demonstrate that
with the correct calibration, it is possible to automate the grading procedure
-- a finding suggesting that, at least to some degree, the LLMs can be used to
self-examine the quality of their own performance. Finally, we describe an
experimental platform that can be seen as a proof-of-concept of the techniques
described in this work.
</p></li>
</ul>

<h3>Title: Efficiently Programming Large Language Models using SGLang. (arXiv:2312.07104v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07104">http://arxiv.org/abs/2312.07104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07104]] Efficiently Programming Large Language Models using SGLang(http://arxiv.org/abs/2312.07104)</code></li>
<li>Summary: <p>Large language models (LLMs) are increasingly used for complex tasks
requiring multiple chained generation calls, advanced prompting techniques,
control flow, and interaction with external environments. However, efficient
systems for programming and executing these applications are lacking. To bridge
this gap, we introduce SGLang, a Structured Generation Language for LLMs.
SGLang is designed for the efficient programming of LLMs and incorporates
primitives for common LLM programming patterns. We have implemented SGLang as a
domain-specific language embedded in Python, and we developed an interpreter, a
compiler, and a high-performance runtime for SGLang. These components work
together to enable optimizations such as parallelism, batching, caching,
sharing, and other compilation techniques. Additionally, we propose
RadixAttention, a novel technique that maintains a Least Recently Used (LRU)
cache of the Key-Value (KV) cache for all requests in a radix tree, enabling
automatic KV cache reuse across multiple generation calls at runtime. SGLang
simplifies the writing of LLM programs and boosts execution efficiency. Our
experiments demonstrate that SGLang can speed up common LLM tasks by up to 5x,
while reducing code complexity and enhancing control.
</p></li>
</ul>

<h3>Title: Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning. (arXiv:2312.07250v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07250">http://arxiv.org/abs/2312.07250</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07250]] Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning(http://arxiv.org/abs/2312.07250)</code></li>
<li>Summary: <p>We conduct investigations on clinical text machine translation by examining
multilingual neural network models using deep learning such as Transformer
based structures. Furthermore, to address the language resource imbalance
issue, we also carry out experiments using a transfer learning methodology
based on massive multilingual pre-trained language models (MMPLMs). The
experimental results on three subtasks including 1) clinical case (CC), 2)
clinical terminology (CT), and 3) ontological concept (OC) show that our models
achieved top-level performances in the ClinSpEn-2022 shared task on
English-Spanish clinical domain data. Furthermore, our expert-based human
evaluations demonstrate that the small-sized pre-trained language model (PLM)
won over the other two extra-large language models by a large margin, in the
clinical domain fine-tuning, which finding was never reported in the field.
Finally, the transfer learning method works well in our experimental setting
using the WMT21fb model to accommodate a new language space Spanish that was
not seen at the pre-training stage within WMT21fb itself, which deserves more
exploitation for clinical knowledge transformation, e.g. to investigate into
more languages. These research findings can shed some light on domain-specific
machine translation development, especially in clinical and healthcare fields.
Further research projects can be carried out based on our work to improve
healthcare text analytics and knowledge transformation.
</p></li>
</ul>

<h3>Title: LLMEval: A Preliminary Study on How to Evaluate Large Language Models. (arXiv:2312.07398v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07398">http://arxiv.org/abs/2312.07398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07398]] LLMEval: A Preliminary Study on How to Evaluate Large Language Models(http://arxiv.org/abs/2312.07398)</code></li>
<li>Summary: <p>Recently, the evaluation of Large Language Models has emerged as a popular
area of research. The three crucial questions for LLM evaluation are ``what,
where, and how to evaluate''. However, the existing research mainly focuses on
the first two questions, which are basically what tasks to give the LLM during
testing and what kind of knowledge it should deal with. As for the third
question, which is about what standards to use, the types of evaluators, how to
score, and how to rank, there hasn't been much discussion. In this paper, we
analyze evaluation methods by comparing various criteria with both manual and
automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and
GPT-4, with different scoring methods and ranking systems. We propose a new
dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186
individuals participated, leading to the generation of 243,337 manual
annotations and 57,511 automatic evaluation results. We perform comparisons and
analyses of different settings and conduct 10 conclusions that can provide some
insights for evaluating LLM in the future. The dataset and the results are
publicly available at https://github.com/llmeval .
</p></li>
</ul>

<h3>Title: Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales. (arXiv:2312.07399v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07399">http://arxiv.org/abs/2312.07399</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07399]] Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales(http://arxiv.org/abs/2312.07399)</code></li>
<li>Summary: <p>Machine reasoning has made great progress in recent years owing to large
language models (LLMs). In the clinical domain, however, most NLP-driven
projects mainly focus on clinical classification or reading comprehension, and
under-explore clinical reasoning for disease diagnosis due to the expensive
rationale annotation with clinicians. In this work, we present a
``reasoning-aware'' diagnosis framework that rationalizes the diagnostic
process via prompt-based learning in a time- and labor-efficient manner, and
learns to reason over the prompt-generated rationales. Specifically, we address
the clinical reasoning for disease diagnosis, where the LLM generates
diagnostic rationales providing its insight on presented patient data and the
reasoning path towards the diagnosis, namely Clinical Chain-of-Thought
(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical
reasoning via extensive experiments and analyses on both rationale generation
and disease diagnosis in various settings. We further propose a novel set of
criteria for evaluating machine-generated rationales' potential for real-world
clinical settings, facilitating and benefiting future research in this area.
</p></li>
</ul>

<h3>Title: On Diverse Preferences for Large Language Model Alignment. (arXiv:2312.07401v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07401">http://arxiv.org/abs/2312.07401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07401]] On Diverse Preferences for Large Language Model Alignment(http://arxiv.org/abs/2312.07401)</code></li>
<li>Summary: <p>The alignment of large language models (LLMs) with human values is crucial
for the development of artificial general intelligence (AGI). One promising
approach to achieve this alignment is reinforcement learning from human
feedback, which employs a reward model (RM) learned from human preference
datasets to guide LLMs in generating text that aligns with human preferences.
Through intensive experiments and analysis of reward distribution, this paper
finds that preference datasets are diverse from each other, even though they
are all proposed to align human preference. Hence, mixing diverse human
preference datasets to increase data size for enhancing reward modeling could
fail. To address the issue and capture the shared human values from diverse
preferences, a new training policy called MORE is introduced, which minimizes
preference bias by adaptively adjusting the preference objective across diverse
preferences. Experiments with the Pythia-1.4B model and five mixed preference
datasets show that MORE achieves superior reward accuracy and lower calibration
error, highlighting its ability to leverage diverse human preference data.
</p></li>
</ul>

<h3>Title: Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07476">http://arxiv.org/abs/2312.07476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07476]] Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection(http://arxiv.org/abs/2312.07476)</code></li>
<li>Summary: <p>In-Context Learning (ICL) is an important paradigm for adapting Large
Language Models (LLMs) to downstream tasks through a few demonstrations.
Despite the great success of ICL, the limitation of the demonstration number
may lead to demonstration bias, i.e. the input-label mapping induced by LLMs
misunderstands the task's essence. Inspired by human experience, we attempt to
mitigate such bias through the perspective of the inter-demonstration
relationship. Specifically, we construct Comparable Demonstrations (CDs) by
minimally editing the texts to flip the corresponding labels, in order to
highlight the task's essence and eliminate potential spurious correlations
through the inter-demonstration comparison. Through a series of experiments on
CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can
significantly reduce such bias; (2) CDs exhibit good performance in ICL,
especially in out-of-distribution scenarios. In summary, this study explores
the ICL mechanisms from a novel perspective, providing a deeper insight into
the demonstration selection strategy for ICL.
</p></li>
</ul>

<h3>Title: SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07492">http://arxiv.org/abs/2312.07492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07492]] SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models(http://arxiv.org/abs/2312.07492)</code></li>
<li>Summary: <p>Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. We start with a
comprehensive list of 93 stigmas documented in social science literature and
curate a question-answering (QA) dataset which involves simple social
situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a
variety of prompt styles, carefully constructed to systematically test for both
social bias and model robustness. We present results for SocialStigmaQA with
two widely used open source generative language models and we demonstrate that
the output generated by these models considerably amplifies existing social
bias against stigmatized groups. Specifically, we find that the proportion of
socially biased output ranges from 45% to 59% across a variety of decoding
strategies and prompting styles. We discover that the deliberate design of the
templates in our benchmark (e.g., by adding biasing text to the prompt or
varying the answer that indicates bias) impact the model tendencies to generate
socially biased output. Additionally, we report on patterns in the generated
chain-of-thought output, finding a variety of problems from subtle bias to
evidence of a lack of reasoning.
</p>
<p>Warning: This paper contains examples of text which is toxic, biased, and
harmful.
</p></li>
</ul>

<h3>Title: Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models. (arXiv:2312.07046v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07046">http://arxiv.org/abs/2312.07046</a></li>
<li>Code URL: https://github.com/transmuteai/trailmet</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07046]] Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models(http://arxiv.org/abs/2312.07046)</code></li>
<li>Summary: <p>Due to the substantial scale of Large Language Models (LLMs), the direct
application of conventional compression methodologies proves impractical. The
computational demands associated with even minimal gradient updates present
challenges, particularly on consumer-grade hardware. This paper introduces an
innovative approach for the parametric and practical compression of LLMs based
on reduced order modelling, which entails low-rank decomposition within the
feature space and re-parameterization in the weight space. Notably, this
compression technique operates in a layer-wise manner, obviating the need for a
GPU device and enabling the compression of billion-scale models within
stringent constraints of both memory and time. Our method represents a
significant advancement in model compression by leveraging matrix
decomposition, demonstrating superior efficacy compared to the prevailing
state-of-the-art structured pruning method.
</p></li>
</ul>

<h3>Title: Improving Factual Error Correction by Learning to Inject Factual Errors. (arXiv:2312.07049v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07049">http://arxiv.org/abs/2312.07049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07049]] Improving Factual Error Correction by Learning to Inject Factual Errors(http://arxiv.org/abs/2312.07049)</code></li>
<li>Summary: <p>Factual error correction (FEC) aims to revise factual errors in false claims
with minimal editing, making them faithful to the provided evidence. This task
is crucial for alleviating the hallucination problem encountered by large
language models. Given the lack of paired data (i.e., false claims and their
corresponding correct claims), existing methods typically adopt the
mask-then-correct paradigm. This paradigm relies solely on unpaired false
claims and correct claims, thus being referred to as distantly supervised
methods. These methods require a masker to explicitly identify factual errors
within false claims before revising with a corrector. However, the absence of
paired data to train the masker makes accurately pinpointing factual errors
within claims challenging. To mitigate this, we propose to improve FEC by
Learning to Inject Factual Errors (LIFE), a three-step distantly supervised
method: mask-corrupt-correct. Specifically, we first train a corruptor using
the mask-then-corrupt procedure, allowing it to deliberately introduce factual
errors into correct text. The corruptor is then applied to correct claims,
generating a substantial amount of paired data. After that, we filter out
low-quality data, and use the remaining data to train a corrector. Notably, our
corrector does not require a masker, thus circumventing the bottleneck
associated with explicit factual error identification. Our experiments on a
public dataset verify the effectiveness of LIFE in two key aspects: Firstly, it
outperforms the previous best-performing distantly supervised method by a
notable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,
even compared to ChatGPT prompted with in-context examples, LIFE achieves a
superiority of 7.16 points in SARI Final.
</p></li>
</ul>

<h3>Title: Multilingual large language models leak human stereotypes across language boundaries. (arXiv:2312.07141v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07141">http://arxiv.org/abs/2312.07141</a></li>
<li>Code URL: https://github.com/annasou/stereotype_leakage</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07141]] Multilingual large language models leak human stereotypes across language boundaries(http://arxiv.org/abs/2312.07141)</code></li>
<li>Summary: <p>Multilingual large language models have been increasingly popular for their
proficiency in comprehending and generating text across various languages.
Previous research has shown that the presence of stereotypes and biases in
monolingual large language models can be attributed to the nature of their
training data, which is collected from humans and reflects societal biases.
Multilingual language models undergo the same training procedure as monolingual
ones, albeit with training data sourced from various languages. This raises the
question: do stereotypes present in one social context leak across languages
within the model? In our work, we first define the term ``stereotype leakage''
and propose a framework for its measurement. With this framework, we
investigate how stereotypical associations leak across four languages: English,
Russian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an
approach from social psychology, measuring stereotypes via group-trait
associations. We evaluate human stereotypes and stereotypical associations
manifested in multilingual large language models such as mBERT, mT5, and
ChatGPT. Our findings show a noticeable leakage of positive, negative, and
non-polar associations across all languages. Notably, Hindi within multilingual
models appears to be the most susceptible to influence from other languages,
while Chinese is the least. Additionally, ChatGPT exhibits a better alignment
with human scores than other models.
</p></li>
</ul>

<h3>Title: Classifying complex documents: comparing bespoke solutions to large language models. (arXiv:2312.07182v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07182">http://arxiv.org/abs/2312.07182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07182]] Classifying complex documents: comparing bespoke solutions to large language models(http://arxiv.org/abs/2312.07182)</code></li>
<li>Summary: <p>Here we search for the best automated classification approach for a set of
complex legal documents. Our classification task is not trivial: our aim is to
classify ca 30,000 public courthouse records from 12 states and 267 counties at
two different levels using nine sub-categories. Specifically, we investigated
whether a fine-tuned large language model (LLM) can achieve the accuracy of a
bespoke custom-trained model, and what is the amount of fine-tuning necessary.
</p></li>
</ul>

<h3>Title: The GUA-Speech System Description for CNVSRC Challenge 2023. (arXiv:2312.07254v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07254">http://arxiv.org/abs/2312.07254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07254]] The GUA-Speech System Description for CNVSRC Challenge 2023(http://arxiv.org/abs/2312.07254)</code></li>
<li>Summary: <p>This study describes our system for Task 1 Single-speaker Visual Speech
Recognition (VSR) fixed track in the Chinese Continuous Visual Speech
Recognition Challenge (CNVSRC) 2023. Specifically, we use intermediate
connectionist temporal classification (Inter CTC) residual modules to relax the
conditional independence assumption of CTC in our model. Then we use a
bi-transformer decoder to enable the model to capture both past and future
contextual information. In addition, we use Chinese characters as the modeling
units to improve the recognition accuracy of our model. Finally, we use a
recurrent neural network language model (RNNLM) for shallow fusion in the
inference stage. Experiments show that our system achieves a character error
rate (CER) of 38.09% on the Eval set which reaches a relative CER reduction of
21.63% over the official baseline, and obtains a second place in the challenge.
</p></li>
</ul>

<h3>Title: ICL Markup: Structuring In-Context Learning using Soft-Token Tags. (arXiv:2312.07405v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07405">http://arxiv.org/abs/2312.07405</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07405]] ICL Markup: Structuring In-Context Learning using Soft-Token Tags(http://arxiv.org/abs/2312.07405)</code></li>
<li>Summary: <p>Large pretrained language models (LLMs) can be rapidly adapted to a wide
variety of tasks via a text-to-text approach, where the instruction and input
are fed to the model in natural language. Combined with in-context learning
(ICL), this paradigm is impressively flexible and powerful. However, it also
burdens users with an overwhelming number of choices, many of them arbitrary.
Inspired by markup languages like HTML, we contribute a method of using
soft-token tags to compose prompt templates. This approach reduces arbitrary
decisions and streamlines the application of ICL. Our method is a form of
meta-learning for ICL; it learns these tags in advance during a
parameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently
be used in templates for ICL on new, unseen tasks without any additional
fine-tuning. Our experiments with this approach yield promising initial
results, improving LLM performance on important enterprise applications such as
few-shot and open-world intent detection, as well as text classification in
news and legal domains.
</p></li>
</ul>

<h3>Title: Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI. (arXiv:2312.06941v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06941">http://arxiv.org/abs/2312.06941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06941]] Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI(http://arxiv.org/abs/2312.06941)</code></li>
<li>Summary: <p>This study investigates the forecasting accuracy of human experts versus
Large Language Models (LLMs) in the retail sector, particularly during standard
and promotional sales periods. Utilizing a controlled experimental setup with
123 human forecasters and five LLMs, including ChatGPT4, ChatGPT3.5, Bard,
Bing, and Llama2, we evaluated forecasting precision through Mean Absolute
Percentage Error. Our analysis centered on the effect of the following factors
on forecasters performance: the supporting statistical model (baseline and
advanced), whether the product was on promotion, and the nature of external
impact. The findings indicate that LLMs do not consistently outperform humans
in forecasting accuracy and that advanced statistical forecasting models do not
uniformly enhance the performance of either human forecasters or LLMs. Both
human and LLM forecasters exhibited increased forecasting errors, particularly
during promotional periods and under the influence of positive external
impacts. Our findings call for careful consideration when integrating LLMs into
practical forecasting processes.
</p></li>
</ul>

<h3>Title: AI Control: Improving Safety Despite Intentional Subversion. (arXiv:2312.06942v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06942">http://arxiv.org/abs/2312.06942</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06942]] AI Control: Improving Safety Despite Intentional Subversion(http://arxiv.org/abs/2312.06942)</code></li>
<li>Summary: <p>As large language models (LLMs) become more powerful and are deployed more
autonomously, it will be increasingly important to prevent them from causing
harmful outcomes. Researchers have investigated a variety of safety techniques
for this purpose, e.g. using models to review the outputs of other models, or
red-teaming techniques to surface subtle failure modes. However, researchers
have not evaluated whether such techniques still ensure safety if the model is
itself intentionally trying to subvert them. In this paper, we develop and
evaluate pipelines of safety techniques ("protocols") that are robust to
intentional subversion.
</p>
<p>We investigate a scenario in which we want to solve a sequence of programming
problems, using access to a powerful but untrusted model (in our case, GPT-4),
access to a less powerful trusted model (in our case, GPT-3.5), and limited
access to human contractors who provide high-quality trusted labor. We
investigate protocols that aim to never submit solutions containing backdoors,
which we operationalize here as logical errors that are not caught by test
cases.
</p>
<p>We investigate a range of protocols and test each against strategies that the
untrusted model could use to subvert them. One protocol is what we call trusted
editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to
rate the suspiciousness of that code. If the code is below some suspiciousness
threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove
parts that seem suspicious and then submits the edited code. Another protocol
is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks
another instance of GPT-4 whether the code is backdoored, using various
techniques to prevent the GPT-4 instances from colluding. These protocols
improve substantially on simple baselines.
</p></li>
</ul>

<h2>gpt</h2>
<h3>Title: How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation. (arXiv:2312.07424v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07424">http://arxiv.org/abs/2312.07424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07424]] How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation(http://arxiv.org/abs/2312.07424)</code></li>
<li>Summary: <p>In machine learning, generalization against distribution shifts -- where
deployment conditions diverge from the training scenarios -- is crucial,
particularly in fields like climate modeling, biomedicine, and autonomous
driving. The emergence of foundation models, distinguished by their extensive
pretraining and task versatility, has led to an increased interest in their
adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced
publicly accessible multimodal foundation model, with extensive applications
across various domains, including anomaly detection, video understanding, image
generation, and medical diagnosis. However, its robustness against data
distributions remains largely underexplored. Addressing this gap, this study
rigorously evaluates GPT-4V's adaptability and generalization capabilities in
dynamic environments, benchmarking against prominent models like CLIP and
LLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse
datasets spanning natural, medical, and molecular domains. We further
investigate its adaptability to controlled data perturbations and examine the
efficacy of in-context learning as a tool to enhance its adaptation. Our
findings delineate GPT-4V's capability boundaries in distribution shifts,
shedding light on its strengths and limitations across various scenarios.
Importantly, this investigation contributes to our understanding of how AI
foundation models generalize to distribution shifts, offering pivotal insights
into their adaptability and robustness. Code is publicly available at
https://github.com/jameszhou-gl/gpt-4v-distribution-shift.
</p></li>
</ul>

<h3>Title: Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack. (arXiv:2312.06924v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06924">http://arxiv.org/abs/2312.06924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06924]] Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack(http://arxiv.org/abs/2312.06924)</code></li>
<li>Summary: <p>Recent developments in balancing the usefulness and safety of Large Language
Models (LLMs) have raised a critical question: Are mainstream NLP tasks
adequately aligned with safety consideration? Our study, focusing on
safety-sensitive documents obtained through adversarial attacks, reveals
significant disparities in the safety alignment of various NLP tasks. For
instance, LLMs can effectively summarize malicious long documents but often
refuse to translate them. This discrepancy highlights a previously unidentified
vulnerability: attacks exploiting tasks with weaker safety alignment, like
summarization, can potentially compromise the integraty of tasks traditionally
deemed more robust, such as translation and question-answering (QA). Moreover,
the concurrent use of multiple NLP tasks with lesser safety alignment increases
the risk of LLMs inadvertently processing harmful content. We demonstrate these
vulnerabilities in various safety-aligned LLMs, particularly Llama2 models and
GPT-4, indicating an urgent need for strengthening safety alignments across a
broad spectrum of NLP tasks.
</p></li>
</ul>

<h3>Title: Perseus: Removing Energy Bloat from Large Model Training. (arXiv:2312.06902v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06902">http://arxiv.org/abs/2312.06902</a></li>
<li>Code URL: https://github.com/ml-energy/zeus</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06902]] Perseus: Removing Energy Bloat from Large Model Training(http://arxiv.org/abs/2312.06902)</code></li>
<li>Summary: <p>Training large AI models on numerous GPUs consumes a massive amount of
energy. We observe that not all energy consumed during training directly
contributes to end-to-end training throughput, and a significant portion can be
removed without slowing down training, which we call energy bloat.
</p>
<p>In this work, we identify two independent sources of energy bloat in large
model training, intrinsic and extrinsic, and propose Perseus, a unified
optimization framework that mitigates both. Perseus obtains the "iteration
time-energy" Pareto frontier of any large model training job using an efficient
iterative graph cut-based algorithm and schedules energy consumption of its
forward and backward computations across time to remove intrinsic and extrinsic
energy bloat. Evaluation on large models like GPT-3 and Bloom shows that
Perseus reduces energy consumption of large model training by up to 30%,
enabling savings otherwise unobtainable before.
</p></li>
</ul>

<h2>llm</h2>
<h3>Title: Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. (arXiv:2312.06674v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06674">http://arxiv.org/abs/2312.06674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06674]] Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations(http://arxiv.org/abs/2312.06674)</code></li>
<li>Summary: <p>We introduce Llama Guard, an LLM-based input-output safeguard model geared
towards Human-AI conversation use cases. Our model incorporates a safety risk
taxonomy, a valuable tool for categorizing a specific set of safety risks found
in LLM prompts (i.e., prompt classification). This taxonomy is also
instrumental in classifying the responses generated by LLMs to these prompts, a
process we refer to as response classification. For the purpose of both prompt
and response classification, we have meticulously gathered a dataset of high
quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our
collected dataset, albeit low in volume, demonstrates strong performance on
existing benchmarks such as the OpenAI Moderation Evaluation dataset and
ToxicChat, where its performance matches or exceeds that of currently available
content moderation tools. Llama Guard functions as a language model, carrying
out multi-class classification and generating binary decision scores.
Furthermore, the instruction fine-tuning of Llama Guard allows for the
customization of tasks and the adaptation of output formats. This feature
enhances the model's capabilities, such as enabling the adjustment of taxonomy
categories to align with specific use cases, and facilitating zero-shot or
few-shot prompting with diverse taxonomies at the input. We are making Llama
Guard model weights available and we encourage researchers to further develop
and adapt them to meet the evolving needs of the community for AI safety.
</p></li>
</ul>

<h3>Title: Intelligent Virtual Assistants with LLM-based Process Automation. (arXiv:2312.06677v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06677">http://arxiv.org/abs/2312.06677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06677]] Intelligent Virtual Assistants with LLM-based Process Automation(http://arxiv.org/abs/2312.06677)</code></li>
<li>Summary: <p>While intelligent virtual assistants like Siri, Alexa, and Google Assistant
have become ubiquitous in modern life, they still face limitations in their
ability to follow multi-step instructions and accomplish complex goals
articulated in natural language. However, recent breakthroughs in large
language models (LLMs) show promise for overcoming existing barriers by
enhancing natural language processing and reasoning capabilities. Though
promising, applying LLMs to create more advanced virtual assistants still faces
challenges like ensuring robust performance and handling variability in
real-world user commands. This paper proposes a novel LLM-based virtual
assistant that can automatically perform multi-step operations within mobile
apps based on high-level user requests. The system represents an advance in
assistants by providing an end-to-end solution for parsing instructions,
reasoning about goals, and executing actions. LLM-based Process Automation
(LLMPA) has modules for decomposing instructions, generating descriptions,
detecting interface elements, predicting next actions, and error checking.
Experiments demonstrate the system completing complex mobile operation tasks in
Alipay based on natural language instructions. This showcases how large
language models can enable automated assistants to accomplish real-world tasks.
The main contributions are the novel LLMPA architecture optimized for app
process automation, the methodology for applying LLMs to mobile apps, and
demonstrations of multi-step task completion in a real-world environment.
Notably, this work represents the first real-world deployment and extensive
evaluation of a large language model-based virtual assistant in a widely used
mobile application with an enormous user base numbering in the hundreds of
millions.
</p></li>
</ul>

<h3>Title: Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning. (arXiv:2312.06820v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06820">http://arxiv.org/abs/2312.06820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06820]] Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning(http://arxiv.org/abs/2312.06820)</code></li>
<li>Summary: <p>Microsoft Windows Feedback Hub is designed to receive customer feedback on a
wide variety of subjects including critical topics such as power and battery.
Feedback is one of the most effective ways to have a grasp of users' experience
with Windows and its ecosystem. However, the sheer volume of feedback received
by Feedback Hub makes it immensely challenging to diagnose the actual cause of
reported issues. To better understand and triage issues, we leverage Double
Machine Learning (DML) to associate users' feedback with telemetry signals. One
of the main challenges we face in the DML pipeline is the necessity of domain
knowledge for model design (e.g., causal graph), which sometimes is either not
available or hard to obtain. In this work, we take advantage of reasoning
capabilities in Large Language Models (LLMs) to generate a prior model that
which to some extent compensates for the lack of domain knowledge and could be
used as a heuristic for measuring feedback informativeness. Our LLM-based
approach is able to extract previously known issues, uncover new bugs, and
identify sequences of events that lead to a bug, while minimizing out-of-domain
outputs.
</p></li>
</ul>

<h3>Title: User Friendly and Adaptable Discriminative AI: Using the Lessons from the Success of LLMs and Image Generation Models. (arXiv:2312.06826v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06826">http://arxiv.org/abs/2312.06826</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06826]] User Friendly and Adaptable Discriminative AI: Using the Lessons from the Success of LLMs and Image Generation Models(http://arxiv.org/abs/2312.06826)</code></li>
<li>Summary: <p>While there is significant interest in using generative AI tools as
general-purpose models for specific ML applications, discriminative models are
much more widely deployed currently. One of the key shortcomings of these
discriminative AI tools that have been already deployed is that they are not
adaptable and user-friendly compared to generative AI tools (e.g., GPT4, Stable
Diffusion, Bard, etc.), where a non-expert user can iteratively refine model
inputs and give real-time feedback that can be accounted for immediately,
allowing users to build trust from the start. Inspired by this emerging
collaborative workflow, we develop a new system architecture that enables users
to work with discriminative models (such as for object detection, sentiment
classification, etc.) in a fashion similar to generative AI tools, where they
can easily provide immediate feedback as well as adapt the deployed models as
desired. Our approach has implications on improving trust, user-friendliness,
and adaptability of these versatile but traditional prediction models.
</p></li>
</ul>

<h3>Title: Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass the Censorship of Text-to-Image Generation Model. (arXiv:2312.07130v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07130">http://arxiv.org/abs/2312.07130</a></li>
<li>Code URL: https://github.com/researchcode001/divide-and-conquer-attack</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07130]] Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass the Censorship of Text-to-Image Generation Model(http://arxiv.org/abs/2312.07130)</code></li>
<li>Summary: <p>Text-to-image generative models offer many innovative services but also raise
ethical concerns due to their potential to generate unethical images. Most
publicly available text-to-image models employ safety filters to prevent
unintended generation intents. In this work, we introduce the
Divide-and-Conquer Attack to circumvent the safety filters of state-of-the-art
text-to-image models. Our attack leverages LLMs as agents for text
transformation, creating adversarial prompts from sensitive ones. We have
developed effective helper prompts that enable LLMs to break down sensitive
drawing prompts into multiple harmless descriptions, allowing them to bypass
safety filters while still generating sensitive images. This means that the
latent harmful meaning only becomes apparent when all individual elements are
drawn together. Our evaluation demonstrates that our attack successfully
circumvents the closed-box safety filter of SOTA DALLE-3 integrated natively
into ChatGPT to generate unethical images. This approach, which essentially
uses LLM-generated adversarial prompts against GPT-4-assisted DALLE-3, is akin
to using one's own spear to breach their shield. It could have more severe
security implications than previous manual crafting or iterative model querying
methods, and we hope it stimulates more attention towards similar efforts. Our
code and data are available at:
https://github.com/researchcode001/Divide-and-Conquer-Attack
</p></li>
</ul>

<h3>Title: Sequential Planning in Large Partially Observable Environments guided by LLMs. (arXiv:2312.07368v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07368">http://arxiv.org/abs/2312.07368</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07368]] Sequential Planning in Large Partially Observable Environments guided by LLMs(http://arxiv.org/abs/2312.07368)</code></li>
<li>Summary: <p>Sequential planning in large state space and action space quickly becomes
intractable due to combinatorial explosion of the search space. Heuristic
methods, like monte-carlo tree search, though effective for large state space,
but struggle if action space is large. Pure reinforcement learning methods,
relying only on reward signals, needs prohibitively large interactions with the
environment to device a viable plan. If the state space, observations and
actions can be represented in natural language then Large Language models (LLM)
can be used to generate action plans. Recently several such goal-directed
agents like Reflexion, CLIN, SayCan were able to surpass the performance of
other state-of-the-art methods with minimum or no task specific training. But
they still struggle with exploration and get stuck in local optima. Their
planning capabilities are limited by the limited reasoning capability of the
foundational LLMs on text data. We propose a hybrid agent "neoplanner", that
synergizes both state space search with queries to foundational LLM to get the
best action plan. The reward signals are quantitatively used to drive the
search. A balance of exploration and exploitation is maintained by maximizing
upper confidence bounds of values of states. In places where random exploration
is needed, the LLM is queried to generate an action plan. Learnings from each
trial are stored as entity relationships in text format. Those are used in
future queries to the LLM for continual improvement. Experiments in the
Scienceworld environment reveals a 124% improvement from the current best
method in terms of average reward gained across multiple tasks.
</p></li>
</ul>

<h3>Title: LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature. (arXiv:2312.07110v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07110">http://arxiv.org/abs/2312.07110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07110]] LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature(http://arxiv.org/abs/2312.07110)</code></li>
<li>Summary: <p>The cybersecurity landscape evolves rapidly and poses threats to
organizations. To enhance resilience, one needs to track the latest
developments and trends in the domain. It has been demonstrated that standard
bibliometrics approaches show their limits in such a fast-evolving domain. For
this purpose, we use large language models (LLMs) to extract relevant knowledge
entities from cybersecurity-related texts. We use a subset of arXiv preprints
on cybersecurity as our data and compare different LLMs in terms of entity
recognition (ER) and relevance. The results suggest that LLMs do not produce
good knowledge entities that reflect the cybersecurity context, but our results
show some potential for noun extractors. For this reason, we developed a noun
extractor boosted with some statistical analysis to extract specific and
relevant compound nouns from the domain. Later, we tested our model to identify
trends in the LLM domain. We observe some limitations, but it offers promising
results to monitor the evolution of emergent trends.
</p></li>
</ul>

<h3>Title: FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs. (arXiv:2312.07420v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07420">http://arxiv.org/abs/2312.07420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07420]] FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs(http://arxiv.org/abs/2312.07420)</code></li>
<li>Summary: <p>Training large language models (LLMs) is a costly endeavour in terms of time
and computational resources. The large amount of training data used during the
unsupervised pre-training phase makes it difficult to verify all data and,
unfortunately, undesirable data may be ingested during training. Re-training
from scratch is impractical and has led to the creation of the 'unlearning'
discipline where models are modified to "unlearn" undesirable information
without retraining. However, any modification can alter the behaviour of LLMs,
especially on key dimensions such as fairness. This is the first work that
examines this interplay between unlearning and fairness for LLMs. In
particular, we focus on a popular unlearning framework known as SISA [Bourtoule
et al., 2021], which creates an ensemble of models trained on disjoint shards.
We evaluate the performance-fairness trade-off for SISA, and empirically
demsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we
propose post-processing bias mitigation techniques for ensemble models produced
by SISA. We adapt the post-processing fairness improvement technique from
[Hardt et al., 2016] to design three methods that can handle model ensembles,
and prove that one of the methods is an optimal fair predictor for ensemble of
models. Through experimental results, we demonstrate the efficacy of our
post-processing framework called 'FairSISA'.
</p></li>
</ul>

<h2>long context</h2>
<h3>Title: SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion. (arXiv:2312.07305v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07305">http://arxiv.org/abs/2312.07305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07305]] SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion(http://arxiv.org/abs/2312.07305)</code></li>
<li>Summary: <p>Sparse attention as a efficient method can significantly decrease the
computation cost, but current sparse attention tend to rely on window self
attention which block the global information flow. For this problem, we present
Shifted Cross Chunk Attention (SCCA), using different KV shifting strategy to
extend respective field in each attention layer. Except, we combine Dilated
Attention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted
Dilated Attention(SDA). Both SCCA and SDA can accumulate attention results in
multi head attention to obtain approximate respective field in full attention.
In this paper, we conduct language modeling experiments using different pattern
of SCCA and combination of SCCA and SDA. The proposed shifted cross chunk
attention (SCCA) can effectively extend large language models (LLMs) to longer
context combined with Positional interpolation(PI) and LoRA than current sparse
attention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.
This attention pattern can provide a Plug-and-play fine-tuning method to extend
model context while retaining their original architectures, and is compatible
with most existing techniques.
</p></li>
</ul>

<h2>lora</h2>
<h3>Title: Perceiving University Student's Opinions from Google App Reviews. (arXiv:2312.06705v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06705">http://arxiv.org/abs/2312.06705</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06705]] Perceiving University Student's Opinions from Google App Reviews(http://arxiv.org/abs/2312.06705)</code></li>
<li>Summary: <p>Google app market captures the school of thought of users from every corner
of the globe via ratings and text reviews, in a multilinguistic arena. The
potential information from the reviews cannot be extracted manually, due to its
exponential growth. So, Sentiment analysis, by machine learning and deep
learning algorithms employing NLP, explicitly uncovers and interprets the
emotions. This study performs the sentiment classification of the app reviews
and identifies the university student's behavior towards the app market via
exploratory analysis. We applied machine learning algorithms using the TP, TF,
and TF IDF text representation scheme and evaluated its performance on Bagging,
an ensemble learning method. We used word embedding, Glove, on the deep
learning paradigms. Our model was trained on Google app reviews and tested on
Student's App Reviews(SAR). The various combinations of these algorithms were
compared amongst each other using F score and accuracy and inferences were
highlighted graphically. SVM, amongst other classifiers, gave fruitful
accuracy(93.41%), F score(89%) on bigram and TF IDF scheme. Bagging enhanced
the performance of LR and NB with accuracy of 87.88% and 86.69% and F score of
86% and 78% respectively. Overall, LSTM on Glove embedding recorded the highest
accuracy(95.2%) and F score(88%).
</p></li>
</ul>

<h3>Title: Forced Exploration in Bandit Problems. (arXiv:2312.07285v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07285">http://arxiv.org/abs/2312.07285</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07285]] Forced Exploration in Bandit Problems(http://arxiv.org/abs/2312.07285)</code></li>
<li>Summary: <p>The multi-armed bandit(MAB) is a classical sequential decision problem. Most
work requires assumptions about the reward distribution (e.g., bounded), while
practitioners may have difficulty obtaining information about these
distributions to design models for their problems, especially in non-stationary
MAB problems. This paper aims to design a multi-armed bandit algorithm that can
be implemented without using information about the reward distribution while
still achieving substantial regret upper bounds. To this end, we propose a
novel algorithm alternating between greedy rule and forced exploration. Our
method can be applied to Gaussian, Bernoulli and other subgaussian
distributions, and its implementation does not require additional information.
We employ a unified analysis method for different forced exploration strategies
and provide problem-dependent regret upper bounds for stationary and
piecewise-stationary settings. Furthermore, we compare our algorithm with
popular bandit algorithms on different reward distributions.
</p></li>
</ul>

<h3>Title: Complex Recurrent Spectral Network. (arXiv:2312.07296v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07296">http://arxiv.org/abs/2312.07296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07296]] Complex Recurrent Spectral Network(http://arxiv.org/abs/2312.07296)</code></li>
<li>Summary: <p>This paper presents a novel approach to advancing artificial intelligence
(AI) through the development of the Complex Recurrent Spectral Network
($\mathbb{C}$-RSN), an innovative variant of the Recurrent Spectral Network
(RSN) model. The $\mathbb{C}$-RSN is designed to address a critical limitation
in existing neural network models: their inability to emulate the complex
processes of biological neural networks dynamically and accurately. By
integrating key concepts from dynamical systems theory and leveraging
principles from statistical mechanics, the $\mathbb{C}$-RSN model introduces
localized non-linearity, complex fixed eigenvalues, and a distinct separation
of memory and input processing functionalities. These features collectively
enable the $\mathbb{C}$-RSN evolving towards a dynamic, oscillating final state
that more closely mirrors biological cognition. Central to this work is the
exploration of how the $\mathbb{C}$-RSN manages to capture the rhythmic,
oscillatory dynamics intrinsic to biological systems, thanks to its complex
eigenvalue structure and the innovative segregation of its linear and
non-linear components. The model's ability to classify data through a
time-dependent function, and the localization of information processing, is
demonstrated with an empirical evaluation using the MNIST dataset. Remarkably,
distinct items supplied as a sequential input yield patterns in time which bear
the indirect imprint of the insertion order (and of the time of separation
between contiguous insertions).
</p></li>
</ul>

<h2>hallucination</h2>
<h2>prompt</h2>
<h3>Title: AI capabilities can be significantly improved without expensive retraining. (arXiv:2312.07413v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07413">http://arxiv.org/abs/2312.07413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07413]] AI capabilities can be significantly improved without expensive retraining(http://arxiv.org/abs/2312.07413)</code></li>
<li>Summary: <p>State-of-the-art AI systems can be significantly improved without expensive
retraining via "post-training enhancements"-techniques applied after initial
training like fine-tuning the system to use a web browser. We review recent
post-training enhancements, categorizing them into five types: tool-use,
prompting methods, scaffolding, solution selection, and data generation.
Different enhancements improve performance on different tasks, making it hard
to compare their significance. So we translate improvements from different
enhancements into a common currency, the compute-equivalent gain: how much
additional training compute would be needed to improve performance by the same
amount as the enhancement. Our non-experimental work shows that post-training
enhancements have significant benefits: most surveyed enhancements improve
benchmark performance by more than a 5x increase in training compute, some by
more than 20x. Post-training enhancements are relatively cheap to develop:
fine-tuning costs are typically &lt;1% of the original training cost. Governing
the development of capable post-training enhancements may be challenging
because frontier models could be enhanced by a wide range of actors.
</p></li>
</ul>

<h3>Title: Get an A in Math: Progressive Rectification Prompting. (arXiv:2312.06867v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06867">http://arxiv.org/abs/2312.06867</a></li>
<li>Code URL: https://github.com/wzy6642/PRP</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06867]] Get an A in Math: Progressive Rectification Prompting(http://arxiv.org/abs/2312.06867)</code></li>
<li>Summary: <p>Chain-of-Thought (CoT) prompting methods have enabled large language models
(LLMs) to generate reasoning paths and solve math word problems (MWPs).
However, they are sensitive to mistakes in the paths, as any mistake can result
in an incorrect answer. We propose a novel method named Progressive
Rectification Prompting (PRP) to improve average accuracy on eight MWP datasets
from 77.3 to 90.5. Given an initial answer from CoT, PRP iterates a
verify-then-rectify process to progressively identify incorrect answers and
rectify the reasoning paths. With the most likely correct answer, the LLM
predicts a masked numerical value in the question; if the prediction does not
match the masked value, the answer is likely incorrect. Then the LLM is
prompted to re-generate the reasoning path hinted with a set of incorrect
answers to prevent itself from repeating previous mistakes. PRP achieves the
best performance compared against the CoT methods. Our implementation is made
publicly available at https://wzy6642.github.io/prp.github.io/.
</p></li>
</ul>

<h2>code</h2>
<h3>Title: Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06695">http://arxiv.org/abs/2312.06695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06695]] Evolving Reservoirs for Meta Reinforcement Learning(http://arxiv.org/abs/2312.06695)</code></li>
<li>Summary: <p>Animals often demonstrate a remarkable ability to adapt to their environments
during their lifetime. They do so partly due to the evolution of morphological
and neural structures. These structures capture features of environments shared
between generations to bias and speed up lifetime learning. In this work, we
propose a computational model for studying a mechanism that can enable such a
process. We adopt a computational framework based on meta reinforcement
learning as a model of the interplay between evolution and development. At the
evolutionary scale, we evolve reservoirs, a family of recurrent neural networks
that differ from conventional networks in that one optimizes not the weight
values but hyperparameters of the architecture: the later control macro-level
properties, such as memory and dynamics. At the developmental scale, we employ
these evolved reservoirs to facilitate the learning of a behavioral policy
through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes
the environment state before providing it to an action policy. We evaluate our
approach on several 2D and 3D simulated environments. Our results show that the
evolution of reservoirs can improve the learning of diverse challenging tasks.
We study in particular three hypotheses: the use of an architecture combining
reservoirs and reinforcement learning could enable (1) solving tasks with
partial observability, (2) generating oscillatory dynamics that facilitate the
learning of locomotion tasks, and (3) facilitating the generalization of
learned behaviors to new tasks unknown during the evolution phase.
</p></li>
</ul>

<h3>Title: A method for recovery of multidimensional time series based on the detection of behavioral patterns and the use of autoencoders. (arXiv:2312.06727v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06727">http://arxiv.org/abs/2312.06727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06727]] A method for recovery of multidimensional time series based on the detection of behavioral patterns and the use of autoencoders(http://arxiv.org/abs/2312.06727)</code></li>
<li>Summary: <p>This article presents a method for recovering missing values in
multidimensional time series. The method combines neural network technologies
and an algorithm for searching snippets (behavioral patterns of a time series).
It includes the stages of data preprocessing, recognition and reconstruction,
using convolutional and recurrent neural networks. Experiments have shown high
accuracy of recovery and the advantage of the method over SOTA methods.
</p></li>
</ul>

<h3>Title: Unsupervised Extractive Summarization with Learnable Length Control Strategies. (arXiv:2312.06901v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06901">http://arxiv.org/abs/2312.06901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06901]] Unsupervised Extractive Summarization with Learnable Length Control Strategies(http://arxiv.org/abs/2312.06901)</code></li>
<li>Summary: <p>Unsupervised extractive summarization is an important technique in
information extraction and retrieval. Compared with supervised method, it does
not require high-quality human-labelled summaries for training and thus can be
easily applied for documents with different types, domains or languages. Most
of existing unsupervised methods including TextRank and PACSUM rely on
graph-based ranking on sentence centrality. However, this scorer can not be
directly applied in end-to-end training, and the positional-related prior
assumption is often needed for achieving good summaries. In addition, less
attention is paid to length-controllable extractor, where users can decide to
summarize texts under particular length constraint. This paper introduces an
unsupervised extractive summarization model based on a siamese network, for
which we develop a trainable bidirectional prediction objective between the
selected summary and the original document. Different from the centrality-based
ranking methods, our extractive scorer can be trained in an end-to-end manner,
with no other requirement of positional assumption. In addition, we introduce a
differentiable length control module by approximating 0-1 knapsack solver for
end-to-end length-controllable extracting. Experiments show that our
unsupervised method largely outperforms the centrality-based baseline using a
same sentence encoder. In terms of length control ability, via our trainable
knapsack module, the performance consistently outperforms the strong baseline
without utilizing end-to-end training. Human evaluation further evidences that
our method performs the best among baselines in terms of relevance and
consistency.
</p></li>
</ul>

<h3>Title: Patch-MI: Enhancing Model Inversion Attacks via Patch-Based Reconstruction. (arXiv:2312.07040v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07040">http://arxiv.org/abs/2312.07040</a></li>
<li>Code URL: https://github.com/jonggyujang0123/patch-attack</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07040]] Patch-MI: Enhancing Model Inversion Attacks via Patch-Based Reconstruction(http://arxiv.org/abs/2312.07040)</code></li>
<li>Summary: <p>Model inversion (MI) attacks aim to reveal sensitive information in training
datasets by solely accessing model weights. Generative MI attacks, a prominent
strand in this field, utilize auxiliary datasets to recreate target data
attributes, restricting the images to remain photo-realistic, but their success
often depends on the similarity between auxiliary and target datasets. If the
distributions are dissimilar, existing MI attack attempts frequently fail,
yielding unrealistic or target-unrelated results. In response to these
challenges, we introduce a groundbreaking approach named Patch-MI, inspired by
jigsaw puzzle assembly. To this end, we build upon a new probabilistic
interpretation of MI attacks, employing a generative adversarial network
(GAN)-like framework with a patch-based discriminator. This approach allows the
synthesis of images that are similar to the target dataset distribution, even
in cases of dissimilar auxiliary dataset distribution. Moreover, we artfully
employ a random transformation block, a sophisticated maneuver that crafts
generalized images, thus enhancing the efficacy of the target classifier. Our
numerical and graphical findings demonstrate that Patch-MI surpasses existing
generative MI methods in terms of accuracy, marking significant advancements
while preserving comparable statistical dataset quality. For reproducibility of
our results, we make our source code publicly available in
https://github.com/jonggyujang0123/Patch-Attack.
</p></li>
</ul>

<h3>Title: Toward Robustness in Multi-label Classification: A Data Augmentation Strategy against Imbalance and Noise. (arXiv:2312.07087v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07087">http://arxiv.org/abs/2312.07087</a></li>
<li>Code URL: https://github.com/disl-lab/balancemix</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07087]] Toward Robustness in Multi-label Classification: A Data Augmentation Strategy against Imbalance and Noise(http://arxiv.org/abs/2312.07087)</code></li>
<li>Summary: <p>Multi-label classification poses challenges due to imbalanced and noisy
labels in training data. We propose a unified data augmentation method, named
BalanceMix, to address these challenges. Our approach includes two samplers for
imbalanced labels, generating minority-augmented instances with high diversity.
It also refines multi-labels at the label-wise granularity, categorizing noisy
labels as clean, re-labeled, or ambiguous for robust optimization. Extensive
experiments on three benchmark datasets demonstrate that BalanceMix outperforms
existing state-of-the-art methods. We release the code at
https://github.com/DISL-Lab/BalanceMix.
</p></li>
</ul>

<h3>Title: BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction. (arXiv:2312.07088v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07088">http://arxiv.org/abs/2312.07088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07088]] BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction(http://arxiv.org/abs/2312.07088)</code></li>
<li>Summary: <p>Canonical relation extraction aims to extract relational triples from
sentences, where the triple elements (entity pairs and their relationship) are
mapped to the knowledge base. Recently, methods based on the encoder-decoder
architecture are proposed and achieve promising results. However, these methods
cannot well utilize the entity information, which is merely used as augmented
training data. Moreover, they are incapable of representing novel entities,
since no embeddings have been learned for them. In this paper, we propose a
novel framework, Bi-Encoder-Decoder (BED), to solve the above issues.
Specifically, to fully utilize entity information, we employ an encoder to
encode semantics of this information, leading to high-quality entity
representations. For novel entities, given a trained entity encoder, their
representations can be easily generated. Experimental results on two datasets
show that, our method achieves a significant performance improvement over the
previous state-of-the-art and handle novel entities well without retraining.
</p></li>
</ul>

<h3>Title: Neural Reasoning About Agents' Goals, Preferences, and Actions. (arXiv:2312.07122v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07122">http://arxiv.org/abs/2312.07122</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07122]] Neural Reasoning About Agents' Goals, Preferences, and Actions(http://arxiv.org/abs/2312.07122)</code></li>
<li>Summary: <p>We propose the Intuitive Reasoning Network (IRENE) - a novel neural model for
intuitive psychological reasoning about agents' goals, preferences, and actions
that can generalise previous experiences to new situations. IRENE combines a
graph neural network for learning agent and world state representations with a
transformer to encode the task context. When evaluated on the challenging Baby
Intuitions Benchmark, IRENE achieves new state-of-the-art performance on three
out of its five tasks - with up to 48.9% improvement. In contrast to existing
methods, IRENE is able to bind preferences to specific agents, to better
distinguish between rational and irrational agents, and to better understand
the role of blocking obstacles. We also investigate, for the first time, the
influence of the training tasks on test performance. Our analyses demonstrate
the effectiveness of IRENE in combining prior knowledge gained during training
for unseen evaluation tasks.
</p></li>
</ul>

<h3>Title: Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting. (arXiv:2312.06874v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06874">http://arxiv.org/abs/2312.06874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06874]] Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting(http://arxiv.org/abs/2312.06874)</code></li>
<li>Summary: <p>Transformers have achieved remarkable performance in multivariate time
series(MTS) forecasting due to their capability to capture long-term
dependencies. However, the canonical attention mechanism has two key
limitations: (1) its quadratic time complexity limits the sequence length, and
(2) it generates future values from the entire historical sequence. To address
this, we propose a Dozer Attention mechanism consisting of three sparse
components: (1) Local, each query exclusively attends to keys within a
localized window of neighboring time steps. (2) Stride, enables each query to
attend to keys at predefined intervals. (3) Vary, allows queries to selectively
attend to keys from a subset of the historical sequence. Notably, the size of
this subset dynamically expands as forecasting horizons extend. Those three
components are designed to capture essential attributes of MTS data, including
locality, seasonality, and global temporal dependencies. Additionally, we
present the Dozerformer Framework, incorporating the Dozer Attention mechanism
for the MTS forecasting task. We evaluated the proposed Dozerformer framework
with recent state-of-the-art methods on nine benchmark datasets and confirmed
its superior performance. The code will be released after the manuscript is
accepted.
</p></li>
</ul>

<h3>Title: DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models. (arXiv:2312.07066v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07066">http://arxiv.org/abs/2312.07066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07066]] DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models(http://arxiv.org/abs/2312.07066)</code></li>
<li>Summary: <p>Recent advances in image and video creation, especially AI-based image
synthesis, have led to the production of numerous visual scenes that exhibit a
high level of abstractness and diversity. Consequently, Visual Storytelling
(VST), a task that involves generating meaningful and coherent narratives from
a collection of images, has become even more challenging and is increasingly
desired beyond real-world imagery. While existing VST techniques, which
typically use autoregressive decoders, have made significant progress, they
suffer from low inference speed and are not well-suited for synthetic scenes.
To this end, we propose a novel diffusion-based system DiffuVST, which models
the generation of a series of visual descriptions as a single conditional
denoising process. The stochastic and non-autoregressive nature of DiffuVST at
inference time allows it to generate highly diverse narratives more
efficiently. In addition, DiffuVST features a unique design with bi-directional
text history guidance and multimodal adapter modules, which effectively improve
inter-sentence coherence and image-to-text fidelity. Extensive experiments on
the story generation task covering four fictional visual-story datasets
demonstrate the superiority of DiffuVST over traditional autoregressive models
in terms of both text quality and inference speed.
</p></li>
</ul>

<h3>Title: GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction. (arXiv:2312.07255v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07255">http://arxiv.org/abs/2312.07255</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07255]] GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction(http://arxiv.org/abs/2312.07255)</code></li>
<li>Summary: <p>The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or
introduces fewer trainable parameters to calibrate pre-trained models on
downstream tasks, has become a recent research interest. However, existing PEFT
methods within the traditional fine-tiuning framework have two main
shortcomings: 1) They overlook the explicit association between trainable
parameters and downstream task knowledge. 2) They neglect the interaction
between the intrinsic task-agnostic knowledge of pre-trained models and the
task-specific knowledge in downstream tasks. To address this gap, we propose a
novel fine-tuning framework, named GIST, in a plug-and-play manner.
Specifically, our framework first introduces a trainable token, called the Gist
token, when applying PEFT methods on downstream tasks. This token serves as an
aggregator of the task-specific knowledge learned by the PEFT methods and forms
an explicit association with downstream knowledge. Furthermore, to facilitate
explicit interaction between task-agnostic and task-specific knowledge, we
introduce the concept of Knowledge Interaction via a Bidirectional
Kullback-Leibler Divergence objective. As a result, PEFT methods within our
framework can make the pre-trained model understand downstream tasks more
comprehensively by leveraging the knowledge interaction. Extensive experiments
demonstrate the universality and scalability of our framework. Notably, on the
VTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our
GIST framework and achieve a performance boost of 2.25%, with an increase of
only 0.8K parameters. The Code will be released.
</p></li>
</ul>

<h3>Title: Predictive variational autoencoder for learning robust representations of time-series data. (arXiv:2312.06932v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06932">http://arxiv.org/abs/2312.06932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06932]] Predictive variational autoencoder for learning robust representations of time-series data(http://arxiv.org/abs/2312.06932)</code></li>
<li>Summary: <p>Variational autoencoders (VAEs) have been used extensively to discover
low-dimensional latent factors governing neural activity and animal behavior.
However, without careful model selection, the uncovered latent factors may
reflect noise in the data rather than true underlying features, rendering such
representations unsuitable for scientific interpretation. Existing solutions to
this problem involve introducing additional measured variables or data
augmentations specific to a particular data type. We propose a VAE architecture
that predicts the next point in time and show that it mitigates the learning of
spurious features. In addition, we introduce a model selection metric based on
smoothness over time in the latent space. We show that together these two
constraints on VAEs to be smooth over time produce robust latent
representations and faithfully recover latent factors on synthetic datasets.
</p></li>
</ul>

<h2>chat</h2>
<h2>retrieval augmented generation</h2>
<h2>rag</h2>
<h3>Title: Adversarial Estimation of Topological Dimension with Harmonic Score Maps. (arXiv:2312.06869v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06869">http://arxiv.org/abs/2312.06869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06869]] Adversarial Estimation of Topological Dimension with Harmonic Score Maps(http://arxiv.org/abs/2312.06869)</code></li>
<li>Summary: <p>Quantification of the number of variables needed to locally explain complex
data is often the first step to better understanding it. Existing techniques
from intrinsic dimension estimation leverage statistical models to glean this
information from samples within a neighborhood. However, existing methods often
rely on well-picked hyperparameters and ample data as manifold dimension and
curvature increases. Leveraging insight into the fixed point of the score
matching objective as the score map is regularized by its Dirichlet energy, we
show that it is possible to retrieve the topological dimension of the manifold
learned by the score map. We then introduce a novel method to measure the
learned manifold's topological dimension (i.e., local intrinsic dimension)
using adversarial attacks, thereby generating useful interpretations of the
learned manifold.
</p></li>
</ul>

<h3>Title: Understanding and Leveraging the Learning Phases of Neural Networks. (arXiv:2312.06887v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06887">http://arxiv.org/abs/2312.06887</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06887]] Understanding and Leveraging the Learning Phases of Neural Networks(http://arxiv.org/abs/2312.06887)</code></li>
<li>Summary: <p>The learning dynamics of deep neural networks are not well understood. The
information bottleneck (IB) theory proclaimed separate fitting and compression
phases. But they have since been heavily debated. We comprehensively analyze
the learning dynamics by investigating a layer's reconstruction ability of the
input and prediction performance based on the evolution of parameters during
training. We empirically show the existence of three phases using common
datasets and architectures such as ResNet and VGG: (i) near constant
reconstruction loss, (ii) decrease, and (iii) increase. We also derive an
empirically grounded data model and prove the existence of phases for
single-layer networks. Technically, our approach leverages classical complexity
analysis. It differs from IB by relying on measuring reconstruction loss rather
than information theoretic measures to relate information of intermediate
layers and inputs. Our work implies a new best practice for transfer learning:
We show empirically that the pre-training of a classifier should stop well
before its performance is optimal.
</p></li>
</ul>

<h3>Title: Noise Distribution Decomposition based Multi-Agent Distributional Reinforcement Learning. (arXiv:2312.07025v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07025">http://arxiv.org/abs/2312.07025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07025]] Noise Distribution Decomposition based Multi-Agent Distributional Reinforcement Learning(http://arxiv.org/abs/2312.07025)</code></li>
<li>Summary: <p>Generally, Reinforcement Learning (RL) agent updates its policy by
repetitively interacting with the environment, contingent on the received
rewards to observed states and undertaken actions. However, the environmental
disturbance, commonly leading to noisy observations (e.g., rewards and states),
could significantly shape the performance of agent. Furthermore, the learning
performance of Multi-Agent Reinforcement Learning (MARL) is more susceptible to
noise due to the interference among intelligent agents. Therefore, it becomes
imperative to revolutionize the design of MARL, so as to capably ameliorate the
annoying impact of noisy rewards. In this paper, we propose a novel
decomposition-based multi-agent distributional RL method by approximating the
globally shared noisy reward by a Gaussian mixture model (GMM) and decomposing
it into the combination of individual distributional local rewards, with which
each agent can be updated locally through distributional RL. Moreover, a
diffusion model (DM) is leveraged for reward generation in order to mitigate
the issue of costly interaction expenditure for learning distributions.
Furthermore, the optimality of the distribution decomposition is theoretically
validated, while the design of loss function is carefully calibrated to avoid
the decomposition ambiguity. We also verify the effectiveness of the proposed
method through extensive simulation experiments with noisy rewards. Besides,
different risk-sensitive policies are evaluated in order to demonstrate the
superiority of distributional RL in different MARL tasks.
</p></li>
</ul>

<h3>Title: Meta-survey on outlier and anomaly detection. (arXiv:2312.07101v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07101">http://arxiv.org/abs/2312.07101</a></li>
<li>Code URL: https://github.com/fabrice-rossi/outlier-anomaly-detection</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07101]] Meta-survey on outlier and anomaly detection(http://arxiv.org/abs/2312.07101)</code></li>
<li>Summary: <p>The impact of outliers and anomalies on model estimation and data processing
is of paramount importance, as evidenced by the extensive body of research
spanning various fields over several decades: thousands of research papers have
been published on the subject. As a consequence, numerous reviews, surveys, and
textbooks have sought to summarize the existing literature, encompassing a wide
range of methods from both the statistical and data mining communities. While
these endeavors to organize and summarize the research are invaluable, they
face inherent challenges due to the pervasive nature of outliers and anomalies
in all data-intensive applications, irrespective of the specific application
field or scientific discipline. As a result, the resulting collection of papers
remains voluminous and somewhat heterogeneous. To address the need for
knowledge organization in this domain, this paper implements the first
systematic meta-survey of general surveys and reviews on outlier and anomaly
detection. Employing a classical systematic survey approach, the study collects
nearly 500 papers using two specialized scientific search engines. From this
comprehensive collection, a subset of 56 papers that claim to be general
surveys on outlier detection is selected using a snowball search technique to
enhance field coverage. A meticulous quality assessment phase further refines
the selection to a subset of 25 high-quality general surveys. Using this
curated collection, the paper investigates the evolution of the outlier
detection field over a 20-year period, revealing emerging themes and methods.
Furthermore, an analysis of the surveys sheds light on the survey writing
practices adopted by scholars from different communities who have contributed
to this field. Finally, the paper delves into several topics where consensus
has emerged from the literature. These include taxonomies of outlier types,
challenges posed by high-dimensional data, the importance of anomaly scores,
the impact of learning conditions, difficulties in benchmarking, and the
significance of neural networks. Non-consensual aspects are also discussed,
particularly the distinction between local and global outliers and the
challenges in organizing detection methods into meaningful taxonomies.
</p></li>
</ul>

<h3>Title: Equivariant Flow Matching with Hybrid Probability Transport. (arXiv:2312.07168v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07168">http://arxiv.org/abs/2312.07168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07168]] Equivariant Flow Matching with Hybrid Probability Transport(http://arxiv.org/abs/2312.07168)</code></li>
<li>Summary: <p>The generation of 3D molecules requires simultaneously deciding the
categorical features~(atom types) and continuous features~(atom coordinates).
Deep generative models, especially Diffusion Models (DMs), have demonstrated
effectiveness in generating feature-rich geometries. However, existing DMs
typically suffer from unstable probability dynamics with inefficient sampling
speed. In this paper, we introduce geometric flow matching, which enjoys the
advantages of both equivariant modeling and stabilized probability dynamics.
More specifically, we propose a hybrid probability path where the coordinates
probability path is regularized by an equivariant optimal transport, and the
information between different modalities is aligned. Experimentally, the
proposed method could consistently achieve better performance on multiple
molecule generation benchmarks with 4.75$\times$ speed up of sampling on
average.
</p></li>
</ul>

<h3>Title: Verbreitungsmechanismen sch\"adigender Sprache im Netz: Anatomie zweier Shitstorms. (arXiv:2312.07194v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07194">http://arxiv.org/abs/2312.07194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07194]] Verbreitungsmechanismen sch\"adigender Sprache im Netz: Anatomie zweier Shitstorms(http://arxiv.org/abs/2312.07194)</code></li>
<li>Summary: <p>In this working paper, we turn our attention to two exemplary, cross-media
shitstorms directed against well-known individuals from the business world.
Both have in common, first, the trigger, a controversial statement by the
person who thereby becomes the target of the shitstorm, and second, the
identity of this target as relatively privileged: cis-male, white, successful.
We examine the spread of the outrage wave across two media at a time and test
the applicability of computational linguistic methods for analyzing its time
course. Assuming that harmful language spreads like a virus in digital space,
we are primarily interested in the events and constellations that lead to the
use of harmful language, and whether and how a linguistic formation of "tribes"
occurs. Our research therefore focuses, first, on the distribution of
linguistic features within the overall shitstorm: are individual words or
phrases increasingly used after their introduction, and through which pathways
they spread. Second, we ask whether "tribes," for example, one group of
supporters and one of opponents of the target, have a distinguished linguistic
form. Our hypothesis is that supporters remain equally active over time, while
the dynamic "ripple" effect of the shitstorm is based on the varying
participation of opponents.
</p></li>
</ul>

<h3>Title: A Novel Differentiable Loss Function for Unsupervised Graph Neural Networks in Graph Partitioning. (arXiv:2312.06877v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06877">http://arxiv.org/abs/2312.06877</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06877]] A Novel Differentiable Loss Function for Unsupervised Graph Neural Networks in Graph Partitioning(http://arxiv.org/abs/2312.06877)</code></li>
<li>Summary: <p>In this paper, we explore the graph partitioning problem, a pivotal
combina-torial optimization challenge with extensive applications in various
fields such as science, technology, and business. Recognized as an NP-hard
prob-lem, graph partitioning lacks polynomial-time algorithms for its
resolution. Recently, there has been a burgeoning interest in leveraging
machine learn-ing, particularly approaches like supervised, unsupervised, and
reinforce-ment learning, to tackle such NP-hard problems. However, these
methods face significant hurdles: supervised learning is constrained by the
necessity of labeled solution instances, which are often computationally
impractical to obtain; reinforcement learning grapples with instability in the
learning pro-cess; and unsupervised learning contends with the absence of a
differentia-ble loss function, a consequence of the discrete nature of most
combinatorial optimization problems. Addressing these challenges, our research
introduces a novel pipeline employing an unsupervised graph neural network to
solve the graph partitioning problem. The core innovation of this study is the
for-mulation of a differentiable loss function tailored for this purpose. We
rigor-ously evaluate our methodology against contemporary state-of-the-art
tech-niques, focusing on metrics: cuts and balance, and our findings reveal
that our is competitive with these leading methods.
</p></li>
</ul>

<h3>Title: Feature Norm Regularized Federated Learning: Transforming Skewed Distributions into Global Insights. (arXiv:2312.06951v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06951">http://arxiv.org/abs/2312.06951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06951]] Feature Norm Regularized Federated Learning: Transforming Skewed Distributions into Global Insights(http://arxiv.org/abs/2312.06951)</code></li>
<li>Summary: <p>In the field of federated learning, addressing non-independent and
identically distributed (non-i.i.d.) data remains a quintessential challenge
for improving global model performance. This work introduces the Feature Norm
Regularized Federated Learning (FNR-FL) algorithm, which uniquely incorporates
class average feature norms to enhance model accuracy and convergence in
non-i.i.d. scenarios. Our comprehensive analysis reveals that FNR-FL not only
accelerates convergence but also significantly surpasses other contemporary
federated learning algorithms in test accuracy, particularly under feature
distribution skew scenarios. The novel modular design of FNR-FL facilitates
seamless integration with existing federated learning frameworks, reinforcing
its adaptability and potential for widespread application. We substantiate our
claims through rigorous empirical evaluations, demonstrating FNR-FL's
exceptional performance across various skewed data distributions. Relative to
FedAvg, FNR-FL exhibits a substantial 66.24\% improvement in accuracy and a
significant 11.40\% reduction in training time, underscoring its enhanced
effectiveness and efficiency.
</p></li>
</ul>

<h3>Title: General Tail Bounds for Non-Smooth Stochastic Mirror Descent. (arXiv:2312.07142v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07142">http://arxiv.org/abs/2312.07142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07142]] General Tail Bounds for Non-Smooth Stochastic Mirror Descent(http://arxiv.org/abs/2312.07142)</code></li>
<li>Summary: <p>In this paper, we provide novel tail bounds on the optimization error of
Stochastic Mirror Descent for convex and Lipschitz objectives. Our analysis
extends the existing tail bounds from the classical light-tailed Sub-Gaussian
noise case to heavier-tailed noise regimes. We study the optimization error of
the last iterate as well as the average of the iterates. We instantiate our
results in two important cases: a class of noise with exponential tails and one
with polynomial tails. A remarkable feature of our results is that they do not
require an upper bound on the diameter of the domain. Finally, we support our
theory with illustrative experiments that compare the behavior of the average
of the iterates with that of the last iterate in heavy-tailed noise regimes.
</p></li>
</ul>

<h3>Title: Coupled Confusion Correction: Learning from Crowds with Sparse Annotations. (arXiv:2312.07331v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07331">http://arxiv.org/abs/2312.07331</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07331]] Coupled Confusion Correction: Learning from Crowds with Sparse Annotations(http://arxiv.org/abs/2312.07331)</code></li>
<li>Summary: <p>As the size of the datasets getting larger, accurately annotating such
datasets is becoming more impractical due to the expensiveness on both time and
economy. Therefore, crowd-sourcing has been widely adopted to alleviate the
cost of collecting labels, which also inevitably introduces label noise and
eventually degrades the performance of the model. To learn from crowd-sourcing
annotations, modeling the expertise of each annotator is a common but
challenging paradigm, because the annotations collected by crowd-sourcing are
usually highly-sparse. To alleviate this problem, we propose Coupled Confusion
Correction (CCC), where two models are simultaneously trained to correct the
confusion matrices learned by each other. Via bi-level optimization, the
confusion matrices learned by one model can be corrected by the distilled data
from the other. Moreover, we cluster the ``annotator groups'' who share similar
expertise so that their confusion matrices could be corrected together. In this
way, the expertise of the annotators, especially of those who provide seldom
labels, could be better captured. Remarkably, we point out that the annotation
sparsity not only means the average number of labels is low, but also there are
always some annotators who provide very few labels, which is neglected by
previous works when constructing synthetic crowd-sourcing annotations. Based on
that, we propose to use Beta distribution to control the generation of the
crowd-sourcing labels so that the synthetic annotations could be more
consistent with the real-world ones. Extensive experiments are conducted on two
types of synthetic datasets and three real-world datasets, the results of which
demonstrate that CCC significantly outperforms state-of-the-art approaches.
</p></li>
</ul>

<h2>multi-run</h2>
<h2>chain-of-thought</h2>
<h2>tree-of-thought</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
